<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A survey on sentiment analysis and opinion mining for social multimedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zuhe</forename><surname>Li</surname></persName>
							<email>zuheli@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Engineering</orgName>
								<orgName type="institution">Zhengzhou University of Light Industry</orgName>
								<address>
									<postCode>450002</postCode>
									<settlement>Zhengzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyu</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics and Information</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Engineering</orgName>
								<orgName type="institution">Zhengzhou University of Light Industry</orgName>
								<address>
									<postCode>450002</postCode>
									<settlement>Zhengzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Electronical and Information Engineering</orgName>
								<orgName type="institution">Shaanxi University of Science and Technology</orgName>
								<address>
									<postCode>710021</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weihua</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A survey on sentiment analysis and opinion mining for social multimedia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32F591EA2A21F76E43764695932B5886</idno>
					<idno type="DOI">10.1007/s11042-018-6445-z</idno>
					<note type="submission">Received: 1 September 2017 / Revised: 15 May 2018 / Accepted: 20 July 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sentiment analysis</term>
					<term>Opinion mining</term>
					<term>Social media</term>
					<term>Multimedia sentiment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social media sentiment analysis (also known as opinion mining) which aims to extract people's opinions, attitudes and emotions from social networks has become a research hotspot. Conventional sentiment analysis concentrates primarily on the textual content. However, multimedia sentiment analysis has begun to receive attention since visual content such as images and videos is becoming a new medium for self-expression in social networks. In order to provide a reference for the researchers in this active area, we give an overview of this topic and describe the algorithms of sentiment analysis and opinion mining for social multimedia. Having conducted a brief review on textual sentiment analysis for social media, we present a comprehensive survey of visual sentiment analysis on the basis of a thorough investigation of the existing literature. We further give a summary of existing studies on multimodal sentiment analysis which combines multiple media channels. We finally summarize the existing benchmark datasets in this area, and discuss the future research trends and potential directions for multimedia sentiment analysis. This survey covers 100 articles during 2008-2018 and categorizes existing studies according to the approaches they adopt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As an important platform for information exchange, social media has become a major source of information covering a wide range of topics. Social multimedia refers to the online multimedia resources posted in social media platforms, which promotes personal engagement and community curation <ref type="bibr" target="#b67">[68]</ref>. In the era of big data, a huge amount of multimedia data is generated in all kinds of social networks per minute. Discovering the knowledge embedded in social multimedia is of great importance since it is vital for many promising applications such as user behavior analysis and prediction <ref type="bibr">[23, 58-62, 64, 72]</ref>. For example, sentiment analysis (also called opinion mining) which intends to extract people's opinions and sentiments embedded in user-generated content can enable a wide variety of applications such as stock market prediction, box-office prediction, political voting forecasts and public opinion monitoring. In today's era, sentiment analysis for the diverse multimedia contents in social networks has proved to be of great significance since it plays an important role in the activities of perception, planning, reasoning, creativity and decision making.</p><p>Sentiment analysis originally refers to the task of detecting, analyzing and extracting attitudes, sentiments and opinions expressed in text <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b72">73]</ref>. With the popularity of camera-equipped mobile terminals and social network platforms (e.g. Facebook, Twitter and Weibo), multimedia content including images and videos plays an important role in conveying information about people's sentiments and opinions in social networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b93">94]</ref>. Sentiment analysis for social multimedia is therefore no longer just a topic in natural language processing and it also has a relationship with computer vision, pattern recognition and other issues in artificial intelligence. Correspondingly, the concept of sentiment analysis should be extended and the knowledge of sentiment analysis should be refreshed. Although social multimedia sentiment analysis is still in its initial stage and this topic is somewhat controversial, the multimedialization of social networks is an indisputable fact and social multimedia sentiment analysis has been paid more and more attention.</p><p>In recent years, a variety of new ideas have emerged in this promising area, especially for visual sentiment analysis. For example, deep learning, which has achieved great success in the field of artificial intelligence <ref type="bibr" target="#b83">[84]</ref>, has begun to be applied to sentiment analysis for different types of social media data. In this background, a comprehensive survey on social multimedia sentiment analysis is really needed because it will be of great significance, especially for those who are new to visual sentiment analysis and multimodal sentiment analysis. Considering the fact that there is a lack of detailed surveys on social multimedia sentiment analysis, we summarize the existing literature related to this topic and give an overview of existing approaches from the macroscopic point of view.</p><p>In general, extensive efforts have been dedicated to sentiment extraction of social media text. Although some progress has been made in visual sentiment analysis, multimodal sentiment analysis is still in its infancy. Audio-only sentiment analysis for social media is not common because audio data usually does not exist independently in social networks, but is included in videos. Therefore, the integration of audio-visual information is utilized for multimodal sentiment prediction. Considering the fact that there have already been a few review papers concerning textual sentiment analysis for social media <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b78">79]</ref>, we will focus on visual sentiment analysis and multimodal sentiment analysis in this paper. We first give a brief survey on textual sentiment analysis for social media in Section 2. We then conduct an in-depth investigation of visual sentiment analysis and categorize the approaches in Section 3. We next give an overview of multimodal sentiment analysis in Section 4. Existing benchmark datasets will be summarized in Section 5. The future trends and potential directions in this area will be discussed in Section 6. Conclusions will be given in the last section.</p><p>Sentiment analysis of social media text intends to extract the sentiment information embedded in messages posted on social media websites. Compared with the sentiment extraction from Internet websites such as blogs and forums, sentiment analysis for social networks is more challenging because of the unique characteristics of social media data. The length limitation and the informal nature of the textual content in social media bring difficulties and challenges. Conventional textual sentiment analysis has been applied at three levels (i.e. document level, sentence level, and entity level). Because of the length limitation, social media messages are often delivered in one or two sentences. In this case, there is no need to distinguish between the document level and the sentence level. Therefore, textual sentiment analysis for social media can be applied at two levels (i.e. message/sentence level and entity level) <ref type="bibr" target="#b30">[31]</ref>. As is illustrated in Fig. <ref type="figure">1</ref>, existing sentiment analysis approaches for social media text can be divided into five classes: Machine Learning, Lexicon-Based, Hybrid, Graph-Based and Others <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b78">79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine learning approaches</head><p>Machine learning methods are widely used to produce sentiment classification models in the field of sentiment analysis. These methods first build a training set and label the training data by sentiment. A set of features are then extracted from the training data and forwarded to a classifier model such as Naive Bayes (NB), Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), and so on. After training with the sentiment labels, the classifier can be utilized to predict the sentiment orientation of a sample which is not annotated.</p><p>Some studies employed emoticons and hash tags to build the training set and used emoticons as class labels to identify the polarity of social media messages. For example, Go et al. <ref type="bibr" target="#b31">[32]</ref> treated twitter sentiment analysis as a binary classification problem which classifies the tweets as positive or negative. Taking into account the difficulty of labeling the training samples, they built a classifier for classifying the sentiment of twitter messages using distant supervision. Kiritchenko et al. <ref type="bibr" target="#b46">[47]</ref> proposed a linear-kernel SVM method for sentiment analysis of short informal text based on supervised learning. They leveraged a variety of surface-form, semantic, and sentiment features which were primarily derived from tweet-specific sentiment lexicons.</p><p>Recently, ensemble classifiers (the combination of multiple classifiers) have been employed to generate more accurate system results in textual sentiment analysis for social media. For example, Lin and Kolcz <ref type="bibr" target="#b55">[56]</ref> combined the evidence from individual classifier Fig. <ref type="figure">1</ref> Overview of sentiment analysis approaches for social media text instances to produce a final prediction and managed to improve the accuracy of sentiment analysis. What is more, da Silva et al. <ref type="bibr" target="#b23">[24]</ref> employed more classifiers such as RF, SVM and LR to automatically classify the sentiment of tweets, and explored different feature representation approaches like bag-of-words and feature hashing. These studies demonstrate the feasibility of using ensemble classifiers to improve classification accuracy.</p><p>As a hotspot in the field of machine learning, deep learning has also been adopted to solve the problem of understanding natural languages. For textual sentiment analysis, word embeddings are first learned from large amounts of text data using deep-learning approaches and then utilized to produce the representations of documents <ref type="bibr" target="#b64">[65]</ref>. Deep learning has recently been applied to document level sentiment classification and review rating prediction <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>. Deep learning approaches have also been used in sentiment analysis of social media text. For example, Tang et al. <ref type="bibr" target="#b82">[83]</ref> collected tweets using distant supervision and learned sentiment specific word embeddings (SSWE) from them. They employed three neural networks to learn SSWE features for twitter sentiment analysis. Dong et al. <ref type="bibr" target="#b26">[27]</ref> developed an Adaptive Recursive Neural Network (AdaRNN) for sentiment analysis of social media text. A dependency tree was adopted to establish the mapping between the sentiment words and the targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lexicon-based approaches</head><p>Lexicon-based approaches predict the overall sentiment orientation of textual messages based on the words that are annotated by polarity or polarity scores. They have been widely adopted in conventional textual sentiment analysis because of the advantage that they do not need training data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b81">82]</ref>. Lexicon-based approaches have also been utilized in textual sentiment analysis for social media. A well-known lexicon-based algorithm for social media is SentiStrength <ref type="bibr" target="#b86">[87]</ref>, which can be used to identify the sentiment of informal text effectively. It is based on a human-coded lexicon containing words and phrases which are frequently used in social media. Recently, Saif et al. <ref type="bibr" target="#b75">[76]</ref> developed a lexicon-based platform called SentiCircles for twitter sentiment analysis. Considering the patterns of words co-occurring in different contexts, they updated the sentiment polarity and scores of the words in lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid approaches</head><p>Machine learning and lexicon-based approaches can be combined to solve the problem of textual sentiment analysis for social media. For example, Zhang et al. <ref type="bibr" target="#b99">[100]</ref> proposed a hybrid approach for entity-based twitter sentiment analysis. Having adopted a lexiconbased method to perform entity-level sentiment analysis, they exploited the information of the result obtained by the lexicon-based method and trained a classifier for sentiment analysis of the newly identified tweets. Ghiassi et al. <ref type="bibr" target="#b29">[30]</ref> presented another interesting hybrid approach by combining dynamic artificial neural networks with n-gram. They used emoticons and tweets as features to build two classifiers: an SVM model and an Artificial Neural Network. Khan et al. <ref type="bibr" target="#b45">[46]</ref> also presented a method including several preprocessing steps before sending the text to the classifier. In the last pre-processing step, a hybrid twitter sentiment analysis method was adopted. These studies demonstrate the feasibility of combining machine learning methods and lexicon-based methods in textual sentiment analysis for social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph-based approaches</head><p>Considering the fact that machine-learning methods require a large number of annotated samples, researchers utilize label propagation to reduce the demand of annotated data. Social graphs are used to distribute labels to nodes in label propagation based semi-supervised learning. Speriosu et al. <ref type="bibr" target="#b79">[80]</ref> are some of the researchers who first introduced label propagation methods to textual sentiment analysis for social media. Assuming that people are affected by each other, they leveraged the Twitter follower graph for sentiment analysis. Cui et al. <ref type="bibr" target="#b21">[22]</ref> also utilized a label propagation method for twitter sentiment analysis by analyzing emotion tokens. They first extracted emotion tokens from tweets and then adopted a graph propagation method to assign polarities to these emotion tokens. Wang et al. <ref type="bibr" target="#b89">[90]</ref> presented a graph-based hashtag-level sentiment classification model using the cooccurrence of hashtags. They also proposed different algorithms such as Loopy Belief Propagation and Relaxation Labeling to make a comparison with SVM voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Other approaches</head><p>There are some sentiment analysis approaches which cannot be classified into the above categories. For example, Kontopoulos et al. <ref type="bibr" target="#b47">[48]</ref> built an ontology domain model using concept analysis, and proposed a Formal Concept Analysis (FCA) technique for twitter sentiment analysis by breaking down tweets into a set of aspects relevant to the subject. Korenek and Šimko <ref type="bibr" target="#b48">[49]</ref> predicted the sentiment polarity of the entity in a tweet using appraisal theory. They used a list of annotated terms to create an appraisal dictionary. Hu et al. <ref type="bibr" target="#b32">[33]</ref> also proposed a sociological approach to handle noisy short texts based on the characteristics of Twitter data. They particularly put forward an approach to incorporate the theories of sentiment consistency and emotional contagion into supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Discussion</head><p>From the existing research, it can be observed that machine-learning approaches are most commonly used in textual sentiment analysis for social media. Beside traditional machine learning methods, ensemble classifiers are adopted to obtain more precise results. In addition, deep learning methods have received more and more attention. However, the limitations of machine learning methods are mainly reflected in two aspects. One is that the performance of machine learning methods depends on the number of annotated samples. The other is that they are domain dependent. Lexicon-based methods are therefore utilized for the reason that they do not require annotated data. However, lexicons are context independent and these methods depend on static lists of words. Hybrid approaches are therefore proposed to compensate for the shortcomings of machine learning approaches and lexicon-based approaches. What is more, graph-based methods are proposed to utilize the social graph and its attributes. These methods do not need large amounts of annotated data. However, they are domain dependent because the sentiment lexicons and the connection graphs are domain specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual sentiment analysis</head><p>Visual sentiment analysis for social media intends to extract sentiment related information from the visual content shared by social media users. Fig. <ref type="figure">2</ref> describes the delivery process of visual sentiment information. It can be seen from Fig. <ref type="figure">2</ref> that how to establish a reliable mapping between visual content and sentiment orientation is the most important issue in visual sentiment analysis, although visual sentiment analysis aims to predict the sentiment orientation of the transmitters in social media. In this sense, visual sentiment analysis for social media can be seen as an extension of emotional semantic analysis for images and videos.</p><p>Traditional studies on emotional semantic analysis for visual content are generally application-specific. These studies focus on the influence of visual content on receivers, and most of them directly establish a mapping between visual low-level features and emotional semantics <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b88">89]</ref>. Researchers have tried to establish a direct mapping between visual features and sentiment orientation in the initial studies on visual sentiment analysis for social media <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b77">78]</ref>. However, most of the images and videos in social media are concrete, and their emotional semantics are driven indirectly by cognitive semantics. Therefore, the visual lowlevel features based methods do not apply to sentiment analysis of visual content in social media. In addition, the visual content in social media is freely shared. The relationship between the diverse data and its sentiment orientation is extremely complex, and the semantic gap problem is very serious.</p><p>In order to fill the semantic gap, researchers endeavor to use mid-level representations as a bridge between visual low-level features and the sentiment orientation. In recent years, researchers have begun to apply deep learning techniques to sentiment analysis and opinion mining for visual content because deep learning has achieved great success in the field of computer vision. Therefore, existing sentiment analysis methods for visual content in social media can be divided into two categories: mid-level representation-based approaches and deep learning-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mid-level representation-based approaches</head><p>Yuan et al. <ref type="bibr" target="#b98">[99]</ref> defined a mid-level representation consisting of 102 attributes for image sentiment analysis. Compared with the method which uses visual low-level features directly, this method makes the sentiment prediction results more interpretable. Borth et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> presented another representative mid-level framework for visual sentiment analysis. They used Adjective Noun Pairs (ANPs) to build a large-scale Visual Sentiment Ontology (VSO) as mid-Fig. <ref type="figure">2</ref> Schematic diagram for the delivery process of visual sentiment information level descriptors for visual sentiment analysis. They also presented a set of ANP concept detectors called SentiBank to detect the presence of 1200 ANPs in visual content. The responses of ANPs can be used as mid-level features for visual sentiment prediction. Fig. <ref type="figure" target="#fig_0">3</ref> gives the schematic diagram of visual sentiment prediction using VSO and SentiBank.</p><p>VSO and SentiBank have been widely used in visual sentiment analysis for social media because they have been made available online. However, as seen in Fig. <ref type="figure" target="#fig_1">4</ref>, the emotional information of visual content is mainly conveyed by the objects in images and videos. Therefore, Chen et al. <ref type="bibr" target="#b13">[14]</ref> proposed an object-based method for visual sentiment concept analysis based on VSO and SentiBank. They first located the objects of visual content and then used adjectives to describe their associated attributes. In this way, they decomposed the problem of ANP detection into object localization and concept modeling. Experimental results show that this method can improve sentiment classification performance, but increase computational complexity. This method combines emotional semantic analysis with object detection, and provides a new perspective for visual sentiment analysis.</p><p>To solve the problem that VSO based models cannot indicate which ANP is highly correlated with the sentiment orientation of visual content, Cao et al. <ref type="bibr" target="#b11">[12]</ref> proposed a Visual Sentiment Topic Model (VSTM) for visual sentiment analysis. The main advantage of VSTM is that it incorporates a macro description of the topic of visual content. Existing VSO and SentiBank-based applications, which use the responses of ANP concepts as mid-level features, ignore the emotional information of these ANP concepts. Li et al. <ref type="bibr" target="#b54">[55]</ref> therefore proposed a method to make full use of the textual sentiment information of the ANPs. They calculated the overall sentiment value of an image according to the textual sentiment values of ANPs and corresponding responses in this image, and then used the image sentiment value as a onedimensional feature for image sentiment prediction. Experimental results indicate that it is feasible to improve the performance of image sentiment analysis by the aid of textual sentiment analysis.</p><p>Because of good openness and versatility, VSO and SentiBank are widely used in all kinds of applications, such as viewer emotion prediction for images and Animated GIFs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>. However, most of existing mid-level representation-based approaches list the concepts together to form a mid-level sentiment ontology, but ignore the distinction and connection between the ontology concepts. In addition, existing methods mainly use visual low-level features for concept detection and extraction. Because of the great successes that have been made by deep learning in the field of computer vision, it is a natural thing to apply deep learning technologies to visual sentiment analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep learning-based approaches</head><p>In recent years, deep learning has become a research hotspot because of its outstanding performance in the field of artificial intelligence. It uses multi-layer models to transform low-level features into an abstract feature space, which can better describe the intrinsic information of input data compared with artificial features <ref type="bibr" target="#b50">[51]</ref>. This brings new opportunities for artificial intelligence and computer vision, and may also benefit emotional semantic analysis for visual content <ref type="bibr" target="#b28">[29]</ref>. What is more, large amounts of visual data in social media can provide sufficient training samples for deep learning.</p><p>Existing deep learning-based approaches for visual sentiment analysis can be categorized into two classes: End-to-end mode and Pipeline mode. As shown in Fig. <ref type="figure">5</ref>, the End-to-end methods attempt to establish a mapping between image pixels and visual sentiment orientation using deep models like Convolutional Neural Networks (CNN). The Pipeline methods try to use deep learning models to establish a mapping between visual content and cognitive semantics, and then conduct sentiment analysis based on the cognitive semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">End-to-end mode</head><p>You et al. <ref type="bibr" target="#b94">[95]</ref> proposed an end-to-end scheme for image sentient prediction based on deep convolutional neural networks, and filtered the noisy training data by fine-tuning. After initial training, the trained model was employed to select a subset of the training samples, which are potentially cleaner. The trained model was further fine-tuned using the newly selected samples. Fig. <ref type="figure">6</ref> shows the scheme of progressive CNN-based image sentiment prediction. Experimental results show that this method can optimize the deep model and improve prediction accuracy.</p><p>In addition, Jindal et al. <ref type="bibr" target="#b37">[38]</ref> conducted image sentiment analysis using deep convolutional neural networks, and made full use of the image data from other areas to improve prediction performance through transfer learning. Campos Víctor et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> have also done a lot of work on visual emotion prediction based on deep convolutional neural networks, and employed fine-tuning to improve the performance of convolutional networks for visual sentiment analysis. Li et al. <ref type="bibr" target="#b53">[54]</ref> constructed a deep CNN model for visual sentiment analysis. They used sentiment-related information to pre-annotate the unlabeled samples from other areas, and proposed a hierarchical fine-tuning strategy for model training. Table <ref type="table" target="#tab_0">1</ref> summarizes the articles using end-to-end deep learning approaches in visual sentiment analysis. It can be found that the CNN model is widely used for visual sentiment prediction, and transfer learning is commonly employed in the training process.</p><p>In addition, there is a deep learning-based study which is different from the studies shown in Table <ref type="table" target="#tab_0">1</ref>. Wang et al. <ref type="bibr" target="#b91">[92]</ref> proposed a model called Deep Coupled Adjective and Noun neural networks (DCAN). They first used an adjective deep neural network and a noun deep neural network to learn a shared middle-level representation. On this basis, they further optimized a prediction network to deal with the subtle differences existing in the fine-grained image categories. Recently, Vadicamo et al. <ref type="bibr" target="#b87">[88]</ref> made full use of the sentiment information of textual comments associated with images to train a deep learning-based image sentiment classifier, on the basis of a large quantity of unlabeled data generated by social media users. Experimental results reveal that the textual descriptions of images can be utilized to train the deep learning models for image sentiment polarity prediction, even though the textual contents may be weakly relevant or irrelevant to corresponding images.</p><p>It can be found that these studies attempt to use neural networks like CNN to establish a mapping between image pixels and sentiment orientation, and employ transfer learning to improve performance with limited labeled samples in the training process. From the point of view of visual perception, a viewer has an emotional response only after recognizing the Fig. <ref type="figure">6</ref> Progressive CNN-based image sentiment prediction <ref type="bibr" target="#b94">[95]</ref> cognitive semantics in concrete images and videos. Therefore, the end-to-end model is flawed. Moreover, the mapping between the diverse visual content and emotional semantics is complicated, and the sentiment labels of training data are unreliable. These studies which leverage the noisily labeled data to train the end-to-end deep neural networks for visual sentiment analysis will be caught in a bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pipeline mode</head><p>In pipeline mode-based visual sentiment analysis, researchers first use deep learning models to establish a mapping from visual content to cognitive semantics, and then predict the sentiment orientation of visual content based on cognitive semantics. For example, Chen et al. <ref type="bibr" target="#b15">[16]</ref> upgraded the SentiBank presented in <ref type="bibr" target="#b5">[6]</ref>. Rather than low-level features, they employed deep convolutional neural networks to detect the presence of ontology concepts in images. Considering the cultural differences, Jou et al. <ref type="bibr" target="#b40">[41]</ref> established a Multilingual Visual Sentiment Ontology (MVSO), and trained the concept detectors for visual sentiment analysis based on deep convolutional neural networks. Liu et al. <ref type="bibr" target="#b62">[63]</ref> established a cross-lingual image sentiment analyzer and a culturally-coherent image query expansion engine based on MVSO. Recently, Ahsan et al. <ref type="bibr" target="#b1">[2]</ref> proposed a pipeline mode-based deep learning scheme to analyze the visual sentiment of social event images. They first generated a series of social event concepts and computed corresponding concept scores using the CNN architecture, and then predicted the sentiment orientation of social event images based on these concept scores. This study brings inspiration for visual sentiment analysis, although it focuses primarily on social events.</p><p>There are also some studies that apply the deep learning-based pipeline mode to sentiment analysis for dynamic visual content. Cai et al. <ref type="bibr" target="#b7">[8]</ref> established a GIF Sentiment Ontology (GSO) for sentiment analysis of dynamic images and short videos in social media, and detected the SentiPair Sequences in GSO using convolutional neural networks. This is an initial attempt for sentiment analysis of the diverse GIFs and videos in social media based on deep learning and ontology description. Similarly, Lin et al. <ref type="bibr" target="#b56">[57]</ref> also presented a SentiPair Sequence-based method for GIF sentiment prediction. Having built a SentiPair label set on the basis of the semantic concepts in WordNet, they took advantage of the CNN architecture to detect SentiPair features in GIFs. They finally utilized the Long Short-Term Memory (LSTM) network to predict the sentiment orientation of GIF videos. In general, the pipeline mode-based deep learning approaches are more interpretable because they mimic human visual perception. However, the concept (or event) detection performance is a key factor affecting the performance of these methods.  <ref type="bibr" target="#b37">[38]</ref> C N N Y e s N o Campos Víctor et al. <ref type="bibr" target="#b9">[10]</ref> C N N Y e s N o Campos Víctor et al. <ref type="bibr" target="#b10">[11]</ref> C N N Y e s N o Li et al. <ref type="bibr" target="#b53">[54]</ref> C N N Y e s Y e s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Other modes</head><p>There are some interesting deep learning-based studies which cannot be simply classified into these two previously mentioned types. As seen in Fig. <ref type="figure">7</ref>, Narihira et al. <ref type="bibr" target="#b68">[69]</ref> used a factorized CNN model to learn the representations for adjectives and nouns separately, and optimized the network over their product. They further developed three CNN models for image sentiment prediction. The first model treats each ANP as an independent class. The second employs a shared-architecture with forked output layers, and the third incorporates an explicit factorization layer for adjectives and nouns.</p><p>Inspired by the object-based visual sentiment analysis, Sun et al. <ref type="bibr" target="#b80">[81]</ref> proposed a method for visual sentiment prediction using deep learning models to detect the emotion-related local regions in images. Given a query image, they first generated candidate object proposals from it. They then ranked these proposals according to the objectness scores, and computed the sentiment score of each proposal using a CNN model. The objectness and sentiment scores are combined to select the most affective regions in the input image from the candidates. They finally extracted features from the whole image and the selected affective regions to predict the sentiment orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>In short, there has been some progress in sentiment analysis for visual content in social media. Mid-level representation-based and deep learning-based approaches are the main solutions to bridge the semantic gap between visual content and sentiment orientation. Although these methods have achieved some success, the complexity of social media data and the unreliability of sentiment labels still affect the performance of these models for visual sentiment prediction. How to establish a reliable mapping between the visual content covering a wide range of topics and the subjective sentiment labels still puzzles the researchers in this area. The problems to be studied and solved are mainly reflected in the following aspects:</p><p>(1) Mid-level representation-based methods detect the presence of cognitive concepts in visual content, and use the responses of these concepts as middle-level features to predict the sentiment orientation through machine learning. Most of the studies use single-layer sentiment ontologies, ignoring not only the holistic semantics of visual content but also the emotional information of ontology concepts. In addition, the unreliability of sentiment labels also creates obstacles to machine learning-based sentiment prediction.</p><p>Fig. <ref type="figure">7</ref> Overview of the factorized CNN model for visual sentiment analysis <ref type="bibr" target="#b68">[69]</ref> (2) Deep learning-based methods, which create a direct mapping between image pixels and sentiment orientation, ignore the process of visual perception. The unreliability of sentiment labels also increases the difficulty of network training. Those deep learning-based methods, which predict the sentiment of visual content after detecting the presence of ontology concepts using deep learning models, also ignore the emotional meaning of ontology concepts and the relationship between them. What is more, these methods do not pay sufficient attention to the objects in images and videos.</p><p>(3) Although visual content has a natural advantage in intercultural communication, there are still differences in the way of expressing emotions among people in different cultural contexts. Therefore, the relationship between visual content and sentiment orientation varies with cultural differences. Most of existing studies in this area are based on the Anglo-American culture, ignoring the cultural uniqueness. However, social networks in various countries in the world are developing rapidly with each passing day. Existing research on culture differences in visual sentiment analysis for social media is still extremely limited. (4) On today's social media platforms, dynamic images and short videos are also important media types expressing sentiments and opinions. However, existing studies have focused mainly on static images. Sentiment analysis for dynamic images and short videos is still to be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multimodal sentiment analysis</head><p>Recently, multimedia content including text, images, GIFs and short videos has become prevalent over all kinds of social networks. In addition to single modality sentiment analysis, there have been a number of studies on multimodal sentiment analysis, which combine multiple sources of data to improve the performance of sentiment prediction.</p><p>There are two kinds of multimodal sentiment analysis. Joint visual-textual sentiment analysis utilizes both the visual and textual sentiment analysis technologies to predict the sentiment orientation of social media messages consisting of text and images. Another kind of multimodal sentiment analysis aims to harvest opinions from web videos leveraging multi-channel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Joint visual-textual sentiment analysis</head><p>The main purpose of joint visual-textual sentiment analysis is to predict the sentiment orientation of social media messages including both text and images. Early fusion and late fusion are the two basic methods of solving this problem. For example, Cao et al. <ref type="bibr" target="#b12">[13]</ref> carried out textual and visual sentiment analysis respectively, and then employed a late fusion scheme to integrate the single results for cross-media sentiment analysis. Niu et al. <ref type="bibr" target="#b69">[70]</ref> extracted textual and visual features from multi-view social data, and adopted fusion strategies like early fusion, late fusion, and multi-model learning to conduct multi-view sentiment analysis. Considering the application of deep learning techniques in textual sentiment analysis, researchers have begun to conduct joint visual-textual sentiment analysis based on deep learning models. In addition, there are also some interesting methods proposed for cross-modality sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Deep learning-based joint visual-textual sentiment analysis</head><p>Deep learning-based joint visual-textual sentiment analysis mainly uses deep neural networks to extract features from text and images, and then employs different fusion methods for joint sentiment analysis. You et al. <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b96">97]</ref> have done a lot of work on deep learning-based visualtextual sentiment analysis. As shown in Fig. <ref type="figure">8</ref>, they employed a convolutional neural network to extract visual features from images, and trained a distributed paragraph vector model to learn textual features from the titles and descriptions of images. They then employed different fusion methods like early fusion, late fusion and Cross-modality Consistent Regression (CCR) to implement joint visual-textual sentiment analysis. For early fusion, visual and textual features were concatenated and a logistic regression model was built on these concatenated features. For late fusion, the average score of visual and textual sentiment prediction was used as the final prediction score. The CCR model was trained to learn the final sentiment classifier by imposing consistent constraints across related modalities (i.e. image and text). Experimental results reveal that the joint visual-textual model outperforms the single modality models, and the CCR model outperforms the single modality models and the fusion model. Similarly, Cai et al. <ref type="bibr" target="#b6">[7]</ref> proposed a CNN-based sentiment prediction framework for multimedia content (tweets including text and images). They used two separate CNN networks to learn textual features and visual features, and sent the output of the two networks into another CNN network to fully explore the relationship between text and images. Yu et al. <ref type="bibr" target="#b97">[98]</ref> also proposed a framework for joint visual-textual sentiment analysis based on convolutional neural networks. They constructed CNN models to integrate image features and textual features, so as to carry out sentiment prediction for Chinese micro-blog. These studies mainly rely on various fusion strategies to take into account both the textual and visual information. However, the relationship between the textual and visual content is often neglected.</p><p>Recently, some researchers started to notice the correlation between images and text in deep learning-based joint visual-textual sentiment analysis. For example, Chen et al. <ref type="bibr" target="#b17">[18]</ref> employed the Alexnet network to learn visual features from images, and utilized the Bag of Glove Vector (BoGV) to represent textual features. They finally presented a method, named as Supervised Collective Matrix Factorization (SCMF), to obtain a unified representation for sentiment prediction. In this study, the label information was taken into consideration in the process of collective matrix factorization. In addition, Chen et al. <ref type="bibr" target="#b18">[19]</ref> trained separate classifiers for images and textual comments simultaneously, and presented a weighted co-training approach for joint visual-textual sentiment analysis. Taking into account the text/image similarity, they made the sentiment classifiers for images and textual content benefit each other by means of Fig. <ref type="figure">8</ref> The framework for deep learning-based joint visual-textual sentiment analysis <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b96">97]</ref> weighted co-training. These studies deserve attention because they take into consideration the interrelationships between images and textual data while using deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Other methods</head><p>In addition to those methods which first extract features and then predict the sentiment orientation through machine learning, there are some interesting studies that concentrate mainly on the difference and semantic relations between textual content and visual content <ref type="bibr" target="#b33">[34]</ref>. They consider multimodal sentiment analysis from the perspective of correlation and hypergraph. For example, Li et al. <ref type="bibr" target="#b51">[52]</ref> proposed a Multimodal Correlation Model (MCM) to combine text and images for sentiment analysis. They fully utilized the hierarchical correlations among modalities and that between modalities and sentiments in this model. They further built a Probabilistic Graphical Model (PGM) upon the MCM model to preserve the classification ability of each single modality. Experimental results reveal the importance of hierarchical correlations in multimodal sentiment analysis.</p><p>Chen et al. <ref type="bibr" target="#b16">[17]</ref> proposed a multimodal hypergraph learning model for joint visual-textual sentiment analysis, taking into consideration both the multi-modality and the independence of each modality. The similarities of tweets on different modalities were captured by the constructed hypergraph. For each modality, the hyperedge is formed by the center vertex and its nearest neighbors. The relevance score among tweets was finally learned through transductive inference to carry out sentiment prediction. In addition, Wang et al. <ref type="bibr" target="#b90">[91]</ref> proposed an Unsupervised SEntiment Analysis (USEA) model to solve the problem of semantic gap and that of limited labeled data. They fused visual and textual information to share a common label space, and extracted noisy contextual information from image comments and captions to infer sentiments based on the USEA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multimodal sentiment analysis for web videos</head><p>In addition to joint visual-textual sentiment analysis, there is another kind of multimodal sentiment analysis which aims to extract sentiment-related information from web videos. Researchers integrate visual, audio, and even textual features to effectively predict the sentiment orientation of audio-visual content. Taking into account that the audio content in social media does not exist independently, we introduce the audio sentient analysis in this chapter. On this basis, we summarize the studies on multimodal sentiment analysis for audiovisual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Audio sentiment analysis</head><p>With the explosive increase of online videos shared on social media platforms, audio sentiment analysis becomes undoubtedly useful in data mining tasks. However, limited research has been conducted on audio sentiment detection. It should be noted that audio sentiment analysis, which focuses on the detection of user opinions, is not equivalent to speech emotion recognition. For speech based sentiment analysis, a common method is to employ Automatic Speech Recognition (ASR) technologies to convert speech into text. The conventional textual sentiment analysis methods can be then adopted for speech sentiment prediction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. In this way, the prediction performance depends mainly on the performance of automatic speech recognition and textual sentiment analysis.</p><p>Existing studies on ASR-based speech sentiment analysis are not the focus of this paper. In spite of this, there are some speech sentiment analysis studies worthy of attention. Kaushik et al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> developed a single Keyword Spotting (KWS) system for audio sentiment detection, rather than simply converted speech to text. They utilized the textual sentiment classifier to determine the most powerful sentiment-bearing terms for KWS, and proposed a method to reduce the complexity of textual sentiment classification model. They finally utilized the term list information to build a language model which is more focused, and evaluated the proposed model on videos collected from YouTube and UT-Opinion corpus. Recently, Amiriparian et al. <ref type="bibr" target="#b3">[4]</ref> tried to use deep spectrum features, rather than conventional features like Mel Frequency Cepstral Coefficients (MFCC), in speech sentiment analysis. They took the spectrograms of audio files as the input of a CNN model, and used the output of the fully connected layer as deep spectrum features for sentiment prediction. In addition, Abburi et al. <ref type="bibr" target="#b0">[1]</ref> presented an approach to detect the sentiment information in online spoken reviews. They extracted MFCC features only at the stressed regions of audio data, rather than the whole input, and detected the sentiment of spoken reviews based on both audio and textual features.</p><p>Beside speech sentiment analysis, some studies have begun to perform audio sentiment analysis for web videos, which include more than just speech data. Chisholm et al. <ref type="bibr" target="#b19">[20]</ref> proposed a solution to extract affective information from the audio signal in highly variable web videos. They focused primarily on affective concepts in audio content, and developed new classifiers for arousal levels in speech, crowd noise and music. Considering the computational challenge of quick classification, they further developed a novel audio sequence segmentation method to rapidly classify sequence audio subsections into audio classes. Fig. <ref type="figure">9</ref> demonstrates an overview of this approach, which consists of feature extraction, codebook construction, feature encoding, and model training.</p><p>Inspired by VSO and SentiBank used for visual sentiment analysis, Sager et al. <ref type="bibr" target="#b74">[75]</ref> recently explored audio content at the semantic level using concept pairs, which can complement other audio and multimedia applications because of the unique information conveyed by concept pairs. They presented a large-scale folksology containing more than 1123 adjective noun pairs and verb noun pairs (called AudioSentiBank corpus). They further presented a benchmark for acoustic concept pair classification. This is the first attempt to explore the classification of acoustic concepts pairs (e.g. happy crowd and angry crowd). The most important aspect of this research is that it is oriented toward the audio content in broad topic areas. This is of great significance to audio sentiment analysis for social media, which covers a wide range of topics.</p><p>Fig. <ref type="figure">9</ref> Overview of the approach for audio concept detection <ref type="bibr" target="#b19">[20]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multimodal sentiment analysis for audio-visual content</head><p>It should be noted that existing studies on multimodal sentiment analysis for audio-visual content focus primarily on self-timer videos containing human faces. Unlike visual sentiment analysis for broad topic areas, the visual features used in these studies are mainly facial features. For example, Yadav et al. <ref type="bibr" target="#b92">[93]</ref> extracted emotion-related information from both the video channel and the audio channel in audiovisual content. For visual analysis, they detected facial feature points from face images to identify facial expression. They also used audio features like pause, pitch, voice intensity and loudness to classify emotions in the audio data. They finally predicted the sentiment orientation of the overall review according to the detected emotions.</p><p>Morency et al. <ref type="bibr" target="#b66">[67]</ref> proposed a joint model integrating visual, audio, and textual features to extract sentiment information from web videos. They identified a subset of audio-visual features relevant to sentiment analysis and presented guidelines on how to integrate these features. For example, they used two series of summary features (i.e. smile duration and look-away duration) to identify facial expression. Correspondingly, two summary features including pause duration and pitch were employed as audio features. Similarly, Rosas et al. <ref type="bibr" target="#b73">[74]</ref> presented a method integrating linguistic, audio, and visual features for multimodal sentiment analysis of Spanish online videos. The audio-visual features they used are similar to that used in <ref type="bibr" target="#b66">[67]</ref>. Experimental results show that using multi-channel features can improve prediction performance. Poria et al. <ref type="bibr" target="#b70">[71]</ref> also proposed a novel method for multimodal sentiment analysis to harvest sentiments from web videos. They presented a model using multiple modalities (i.e. audio, visual and textual modalities) as information sources, and merged the multi-channel affective information based on both feature-level and decision-level fusion algorithms.</p><p>There are some studies which no longer focus only on facial features in visual analysis. For example, a representative study has been done to classify politically persuasive web videos by exploiting multimodal affect <ref type="bibr" target="#b76">[77]</ref>. As seen in Fig. <ref type="figure">10</ref>, Siddiquie et al. <ref type="bibr" target="#b76">[77]</ref> extracted audio, visual, and textual features in politically persuasive web videos to capture their affective semantics, and the Fig. <ref type="figure">10</ref> Overview of the framework for identification of politically persuasive web videos <ref type="bibr" target="#b76">[77]</ref> sentiment related information in the comments of viewers. In video analysis, they used deep learning methods to detect ImageNet concepts and VSO concepts. Experimental results reveal that each feature modality can be used for politically persuasive content classification, and the best performance can be obtained by fusing multiple modalities.</p><p>In addition, Jiang et al. <ref type="bibr" target="#b36">[37]</ref> presented a comprehensive framework for sentiment analysis of user-generated videos in social media. They extracted features including audio features, lowlevel visual descriptors, and semantic attributes to conduct multimodal sentiment analysis based on kernel-level multimodal fusion. Recently, Chu and Roy <ref type="bibr" target="#b20">[21]</ref> proposed an audiovisual analysis method to generate emotional arcs for movies including short videos on the web. They first trained models for audio sentiment analysis and video sentiment analysis respectively, and then used them to construct separate emotional arcs for audio and visual content. They next conducted experiments to evaluate the micro-level performance and synthesize the prediction results from audio and visual content. Although this study is not just about short videos in social networks, it can still provide inspiration for multimodal sentiment analysis of social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>On the whole, initial results have been obtained in multimodal sentiment analysis for social media. There has been a breakthrough in deep learning-based joint visual-textual sentiment analysis and multimodal sentiment analysis for audio-visual content. Even though, there are still many problems to be solved in multimodal sentiment analysis for the visual-textual data and web videos in social networks.</p><p>(1) Some progress has been made in joint visual-textual sentiment analysis for social media messages including both text and images, especially in deep learning-based sentiment analysis. However, most of existing studies on joint visual-textual sentiment analysis mainly employ different fusion methods to integrate the textual and visual information, ignoring the correlation between the textual and visual content. Although some researchers have begun to pay attention to this problem, there is still much room for further research. Additionally, deep learning models are often employed in existing studies on joint visual-textual sentiment analysis. In this case, the fruitful results in textual sentiment analysis for social media are often neglected. As described in Section 2, machine learning is only one of the solutions for textual sentiment analysis. Therefore, how to apply existing research results in textual sentiment analysis to joint visual-textual sentiment analysis is still worth studying. (2) Research on multimodal sentiment analysis for audio-visual content in social media is particularly limited. This is because the research foundation of audio sentiment analysis is relatively weak, and research on sentiment analysis for the video data in social media has just started. Most of existing studies on this topic focus primarily on self-timer web videos containing human faces. It should be noted that the sentiment orientation of selftimer web videos depends primarily on the emotions of the characters therein. There are similarities between this kind of research and that on affective computing in humancomputer interaction. However, the audio-visual content in social media is far more diverse and self-timer web videos are just one type of video data in social networks. Therefore, how to utilize multi-channel information to improve the performance of sentiment analysis for different kinds of videos in social networks remains to be studied.</p><p>(3) Although audio data mainly exists in videos and audio-only sentiment analysis for social media is not common, audio data can still provide supplementary information for multimodal sentiment prediction. It is noteworthy that the audio data in social networks is not limited to human speech. How to extract sentiment information from the audio data in user-generated content covering a broad range of topics has yet to be studied in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Benchmark datasets and evaluation measures</head><p>We discuss the benchmark datasets and evaluation measures for multimedia sentiment analysis in this section. Taking into account that there have been some review papers which summarized the datasets for textual sentiment analysis <ref type="bibr" target="#b30">[31]</ref>, we focus primarily on the benchmark datasets for visual analysis and multimedia analysis. In general, there are a limited number of benchmark datasets for sentiment analysis of visual content and multimedia content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark datasets</head><p>A small dataset including 603 image tweets (with textual data) covering a wide range of topics was first presented in <ref type="bibr" target="#b4">[5]</ref>, together with the proposed VSO and SentiBank. The image tweets were collected using popular hashtags, and labeled by Amazon Mechanical Turk (AMT) workers. Three labeling runs (i.e. image only inspection, text only inspection, and joint textimage inspection) were conducted to assign sentiment labels for the collected tweets. Each tweet was annotated by three randomly assigned Turkers in each run, and was defined as Bagreed^only if it was assigned the same label by more than two Turkers. Only 470 positive image tweets and 133 negative image tweets receiving unanimously agreed labels in the three labeling runs were finally selected.</p><p>A slightly larger dataset for image sentiment analysis was presented in <ref type="bibr" target="#b94">[95]</ref>. 1269 image tweets were collected from social media, and five AMT workers were recruited to generate binary sentiment labels for each candidate image. There are three subsets called BFive agree^, BAt Least Four Agree^, and BAt Least Three Agree^in this dataset. In the BFive agree^subset, all the five AMT workers gave the same sentiment label to each image sample. In the BAt Least Four Agree^subset, at least four of the five AMT workers agreed on the labeling of each sample. At least three of the five AMT workers gave the same sentiment label to each sample in the BAt Least Three Agree^subset. As seen in Table <ref type="table" target="#tab_1">2</ref>, there are 882 BFive agree^images, 1116 BAt Least Four Agree^images and 1269 BAt Least Three Agree^images in this dataset.</p><p>Recently, a dataset for GIF sentiment analysis, called GSO-2015, was built in <ref type="bibr" target="#b7">[8]</ref>. 40,000+ GIF videos were collected from social networks and manually labeled by crowd intelligence. Seven AMT workers were recruited to accomplish two tasks. One is to describe each sample using SentiPairs, each of which consists of either an Adjective Noun Pair (ANP) or a Verb There is a dataset for predicting emotions in user-generated videos <ref type="bibr" target="#b36">[37]</ref>. 4486 videos from YouTube and 3215 videos from Flickr were collected and annotated based on the eight emotion categories in Plutchik's wheel. This is somewhat different from sentiment analysis because more emotion categories were considered. However, it can be utilized in sentiment analysis because the eight emotion categories can be easily classified into positive and negative sentiment orientations. Ten annotators (5 males and 5 females) were asked to filter the videos, and 1101 videos were finally kept to build the dataset. This dataset can be used for multimodal sentiment analysis of audio-visual content because these videos contain audio data.</p><p>Recently, a Multi-view Sentiment Analysis Dataset (MVSA), which can be used for joint visual-textual sentiment analysis, was presented in <ref type="bibr" target="#b69">[70]</ref>. This dataset includes a set of imagetext tweets collected from Twitter. A public streaming Twitter API, called Twitter4J, was adopted to collect representative tweets based on 406 emotional words. Only those text-image tweets whose images can be accessed were finally kept for annotation. An interface was developed for annotation based on three sentiments (positive, negative and neutral). 4869 messages have been annotated in the benchmark dataset. The details of MVSA and the dataset presented in <ref type="bibr" target="#b4">[5]</ref> are given in Table <ref type="table" target="#tab_2">3</ref> to make a comparison. MVSA is the largest dataset for joint visual-textual sentiment analysis up to now.</p><p>Existing benchmark datasets for multimedia sentiment analysis are still very scarce, especially in multimodal sentiment analysis for audio-visual content. Moreover, there are two main problems encountered in construction and annotation of evaluation datasets. One is that there are more positive samples than negative samples in existing datasets. The unevenness of training data will affect the performance of machine learning-based sentiment prediction. The other is the unreliability of labels, which is caused by the subjectivity of human emotion. Sentiment related labels are not completely reliable even if they are obtained by manual annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation measures</head><p>Existing studies on social multimedia sentiment analysis mainly evaluate the performance of sentiment prediction on benchmark datasets. Having given sentiment labels to the samples in benchmark datasets, researchers usually conduct cross-validation experiments to evaluate the performance of different algorithms. As shown in Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_2">4</ref>), accuracy, precision, recall and F-score are the commonly adopted evaluation measures for performance comparison. </p><formula xml:id="formula_0">accuracy ¼ true positives þ true negatives true positives þ false positives þ true negatives þ false negatives<label>ð1Þ</label></formula><formula xml:id="formula_1">F-score ¼ 1 þ α 2 À Á Â precision Â recall α 2 Â precision þ recall<label>ð3Þ</label></formula><p>where Btrue positives^denotes the number of positive samples which are correctly classified, Bfalse positives^denotes the number of negative samples which are wrongly classified, Btrue negatives^denotes the number of negative samples which are correctly classified, Bfalse negatives^denotes the number of positive samples which are wrongly classified, and α is the weight parameter for F-score.</p><p>In general, accuracy is the most commonly used measure for assessing the performance of different algorithms because it is adopted in almost all the studies in this area. However, taking into account the unevenness of the samples in benchmark datasets, it is unfair to use only this indicator for performance evaluation. The number of positive samples is usually larger than that of negative samples in benchmark datasets. To solve this problem, researchers have adopted other measures (e.g. precision, recall and F-score) to make a comprehensive evaluation. For example, You et al. <ref type="bibr" target="#b94">[95]</ref>, who present a benchmark dataset for image sentiment analysis, evaluate the prediction performance in terms of accuracy, precision, recall and Fscore (α = 1) on the three subsets of this dataset. Niu et al. <ref type="bibr" target="#b69">[70]</ref>, who present the MVSA Dataset for joint visual-textual sentiment analysis, use accuracy and F-score as evaluation measures. In spite of this, quite a lot of researchers only use Baccuracy^as the evaluation measure to illustrate the superiority of their algorithms. This is somewhat problematic due to the limited sample size and the unevenness of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Research trends and potential directions</head><p>After analyzing the studies on sentiment analysis for social media, we will discuss the research trends and potential directions in this section. Because the research trends of textual sentiment analysis for social media have already been fully discussed in a series of recent review papers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b78">79]</ref>, we only discuss the research trends and potential directions for visual sentiment analysis and multimodal sentiment analysis in detail.</p><p>On the whole, the application of deep learning techniques in multimedia sentiment analysis is a general trend. How to use the noisy big data in deep models-based sentiment analysis is still worth studying. For example, existing research on multimedia sentiment analysis based on end-to-end deep neural networks has encountered a bottleneck because of the unreliability of sentiment labels. The pipeline mode-based method, which predicts the sentiment orientation indirectly using mid-level representations, is therefore more promising in deep learning-based multimedia sentiment analysis. Also, sentiment analysis, which intends to extract high-level semantics from all kinds of media, is inextricably linked to the studies on semantic analysis in natural language processing and computer vision. Since there have been fruitful research results in these fields, how to make full use of existing results in related areas is also important for sentiment analysis. The most important research directions include the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-angle ontology description and concept detection for visual sentiment analysis</head><p>Existing research on pipeline mode-based visual sentiment analysis often uses a set of concepts (e.g. adjectives and nouns) to construct an ontology describing the visual content. In the process of concept detection, either the whole image or only the local objects therein are focused on. However, according to the psychological theory, human beings on the one hand understand the visual content as a whole, and on the other hand also recognize it locally. In other words, the human perception of visual content is multi-dimensional. Therefore, in the construction of sentiment ontology, the ontology concepts can be divided into two levels: macroscopic and microscopic. Accordingly, the concepts in visual content can be detected both in whole and in part, thus forming a multi-angle and multi-level description of visual content to improve the performance of sentiment analysis. However, how to describe the diverse visual content in social media in a comprehensive and unified manner and how to effectively detect the sentiment related semantics are still open issues. This is especially true for dynamic visual content containing semantic information about activities and behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Reasoning technology-based sentiment prediction</head><p>Existing mid-level representation-based multimedia sentiment analysis first detects the responses of cognitive concepts in multimedia content like images and videos, and then employs these response features and sentiment labels to train machine learning models for sentiment prediction. In fact, it is feasible to predict the sentiment orientation by means of reasoning technologies after detecting the presence of cognitive concepts therein, because cognitive concepts themselves have sentiment information. These direct reasoning methods are often neglected in existing studies, and should be taken into consideration in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation and utilization of the correlation between textual and visual content</head><p>Nowadays, it is very common for social media users to post visual content like images and videos together with textual descriptions. In most cases, there is a correlation between the textual content and the visual content. On the one hand, textual descriptions can be used to generate sentiment labels for corresponding visual content <ref type="bibr" target="#b87">[88]</ref>. On the other hand, both the visual and textual content can be comprehensively used to obtain more reliable predictions in joint visual-textual sentiment analysis. However, the textual descriptions of social media messages (e.g. tweets) may be noisy or misleading because the comments may be irrelevant to corresponding image content. In this case, these cross-media approaches will have a negative effect. Therefore, how to evaluate the correlation between textual and visual content and take advantage of it is a key issue, no matter for visual sentiment analysis or for joint visual-textual sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Sentiment related semantic concept detection for audio content in social media</head><p>Audio sentiment analysis cannot be ignored because audio data is an indispensable part of social multimedia content, even if audio content often does not appear alone. Future research should focus more on audio content in broad topic areas than on speech data. From the existing research, it can be found that it is of significance to apply the mid-level representation approaches (e.g. VSO) to sentiment analysis for audio data covering a wide range of topics. For example, a recent study <ref type="bibr" target="#b74">[75]</ref> shows that it is feasible to explore audio content at the semantic level using concept pairs. However, how to describe the diverse audio content in social media in a unified manner and how to effectively detect the sentiment related concepts are still problems. In particular, for sentiment analysis, we not only need to detect entity concepts like Bcar^and Bbaby^, but also need to detect more abstract concepts like Bquiet^and Bangry^. It is worthwhile to dedicate more efforts to this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Technologies of multi-source information fusion for sentiment analysis</head><p>With the popularity of multimedia content in social media platforms, researchers have started to leverage multi-channel information including visual, audio, and even textual content to harvest sentiments and opinions from social networks. In essence, this is a problem of multisource information fusion. However, existing studies on this issue mainly employ simple fusion methods (e.g. early fusion and late fusion) to take multi-channel information into account. This is far from enough. It should be pointed out that considerable progress has been made in the field of multi-source information fusion, and it is likely to promote the studies on sentiment analysis for multimedia content by fully exploring these results. Therefore, it is of great value to conduct research on the technologies of multi-source information fusion for multimedia sentiment analysis in the future. Given the complexity and variability of social media data, how to effectively integrate multiple sources of information will be a key issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Construction of reliable benchmark datasets</head><p>The lack of benchmark datasets is an important problem in multimedia sentiment analysis, especially in visual analysis and multimodal analysis. What is more, the unevenness of samples and the unreliability of sentiment labels increase the difficulty of making a fair comparison between different methods. For example, quite a lot of researchers conduct experiments on their own datasets, many of which contain only a limited number of samples whose labels are not completely correct. Even worse, there is often a big difference between the number of positive samples and that of negative ones. Experimental results on these datasets are not convincing because performance evaluation does not make sense without trustworthy datasets. However, existing studies in this area rarely pay attention to this issue. Therefore, it is also a meaningful task to collect enough samples, assign reliable sentiment labels to them, and make them public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>With the rapid development of social media, multimedia data has become an important carrier of human sentiments and opinions. Sentiment analysis for the multimedia content in social networks becomes a promising research topic. On the basis of a brief review on textual sentiment analysis for social media, we reviewed the most prominent methods for visual sentiment analysis, and discussed the prevalent approaches for multimodal sentiment analysis. In addition, we summarized the benchmark datasets that have been built for social multimedia sentiment analysis, and indicated the promising future directions in this area.</p><p>From the analysis, we can draw the following conclusions. (1) Deep Learning-based multimedia sentiment analysis will still be a hot topic. <ref type="bibr" target="#b1">(2)</ref> The utilization of existing semantic analysis technologies (e.g. reasoning technology) in related areas will be beneficial to multimedia sentiment analysis, especially for visual sentiment analysis. (3) The application of existing techniques for textual sentiment analysis to joint visual-textual sentiment analysis deserves our attention. Also, the correlation between the textual and visual content should be taken into consideration. (4) More effort should be dedicated to audio sentiment analysis, video sentiment analysis and multimodal sentiment analysis for the audio-visual content (not restricted to self-timer videos) in social networks. In short, this survey gave a comprehensive review on the literature in social multimedia sentiment analysis, identified the interesting open issues, and discussed the potential directions and research trends. It will be a useful reference for those researchers interested in this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>Fig.3Schematic diagram of visual sentiment prediction using VSO and SentiBank<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> </figDesc><graphic coords="7,46.77,488.41,346.48,115.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Example images (from the Flickr images collected in [5]) for object-based visual sentiment analysis</figDesc><graphic coords="8,77.73,53.12,283.87,140.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,46.77,393.51,346.48,209.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of the articles employing end-to-end deep learning approaches in visual sentiment analysis</figDesc><table><row><cell>Study</cell><cell>Model</cell><cell>Transfer learning</cell><cell>Sample selection</cell></row><row><cell>You et al. [95]</cell><cell>C N N</cell><cell>Y e s</cell><cell>Y e s</cell></row><row><cell>Jindal et al.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Summary of the labeling results for the Twitter dataset presented in<ref type="bibr" target="#b94">[95]</ref> VNP). The other task is to assign sentiment labels (Positive, Negative, Neutral, or Can't Judge) to candidate GIF videos. 1239 positive samples, 207 negative samples and 630 neutral ones were finally kept for training.</figDesc><table><row><cell>Sentiment</cell><cell>Five agree</cell><cell>At Least Four Agree</cell><cell>At Least Three Agree</cell></row><row><cell>Positive</cell><cell>581</cell><cell>689</cell><cell>769</cell></row><row><cell>Negative</cell><cell>301</cell><cell>427</cell><cell>500</cell></row><row><cell>Sum</cell><cell>882</cell><cell>1116</cell><cell>1269</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Statistics of the annotated datasets can be used for joint visual-textual sentiment analysis</figDesc><table><row><cell>Dataset</cell><cell>Positive</cell><cell>Negative</cell><cell>Neutral</cell></row><row><cell>SentiBank [5]</cell><cell>4 7 0</cell><cell>1 3 3</cell><cell>/</cell></row><row><cell>MVSA [70]</cell><cell>1398</cell><cell>724</cell><cell>470</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the National Natural Science Foundation of China under Grant 61702462, 61702464 and 61461025, the Scientific and Technological Project of Henan Province under Grant 182102210607, and the Science and Technology Innovation Engineering Program for Shaanxi Provincial Key Laboratories under Grant 2013SZS15-K02.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved multimodal sentiment detection using stressed regions of audio</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abburi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gangashetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Region 10 Conference (TENCON)</title>
		<meeting>IEEE Region 10 Conference (TENCON)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2834" to="2837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards using visual attributes to infer image sentiment of social events</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Essa</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1372" to="1379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Color-based visual sentiment for social communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amencherla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th Canadian Workshop on Information Theory (CWIT)</title>
		<meeting>15th Canadian Workshop on Information Theory (CWIT)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">7994829</biblScope>
		</imprint>
	</monogr>
	<note>Article number</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentiment analysis using image-based deep spectrum features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ottl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</title>
		<meeting>7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="26" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SentiBank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="459" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for multimedia sentiment analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A spatial-temporal visual mid-level ontology for GIF sentiment analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Congress on Evolutionary Computation</title>
		<meeting>IEEE Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4860" to="4865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Affective computing and sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diving deep into sentiment: Understanding fine-tuned CNNs for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giró-I-Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Affect &amp; Sentiment in Multimedia</title>
		<meeting>the 1st International Workshop on Affect &amp; Sentiment in Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From pixels to sentiment: fine-tuning CNNs for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giró-I-Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual sentiment topic model based microblog image sentiment analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="8955" to="8968" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A cross-media public sentiment analysis system for microblog</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="486" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-based visual sentiment concept analysis and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Conference on Multimedia</title>
		<meeting>the 2014 ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting viewer affective comments based on image content in social media</title>
		<author>
			<persName><forename type="first">Y Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia Retrieval</title>
		<meeting>the ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DeepSentiBank: visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8586</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal hypergraph learning for microblog sentiment prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Cao D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE International Conference on Multimedia and Expo</title>
		<meeting>2015 IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image sentiment analysis using supervised collective matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 12th IEEE Conference on Industrial Electronics and Applications (ICIEA)</title>
		<meeting>12th IEEE Conference on Industrial Electronics and Applications (ICIEA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted co-training for cross-domain image sentiment classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Sci Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="714" to="725" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio-based affect detection in web videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio-visual sentiment analysis for learning emotional arcs in movies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th IEEE International Conference on Data Mining (ICDMW)</title>
		<meeting>17th IEEE International Conference on Data Mining (ICDMW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="829" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion tokens: Bridging the gap among multilingual twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Asia Conference on Information Retrieval Technology (AIRS&apos;11)</title>
		<meeting>the 7th Asia Conference on Information Retrieval Technology (AIRS&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tracking generic human motion via fusion of low-and high-dimensional approaches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst, Man, Cybernetics: Syst</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1002" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tweet sentiment analysis with classifier ensembles</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nff</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis Support Syst</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="170" to="179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: state of the art and independent comparison of techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dashtipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="757" to="771" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Web Search and Data Mining (WSDM&apos;08)</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining (WSDM&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentiment analysis of call Centre audio conversations using text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Inform Syst Indust Manag Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="619" to="627" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Affective abstract image classification based on convolutional sparse autoencoders across different domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Electron Inf Technol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Twitter brand sentiment analysis: a hybrid system using n-gram analysis and dynamic artificial neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghiassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zimbra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="6266" to="6282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Like it or not: a survey of twitter sentiment analysis methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009">2009</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Project Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting social relations for sentiment analysis in microblogging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM&apos;13)</title>
		<meeting>the 6th ACM International Conference on Web Search and Data Mining (WSDM&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-modality sentiment analysis for social multimedia</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia Big Data</title>
		<meeting>IEEE International Conference on Multimedia Big Data</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Survey of visual sentiment prediction for social media analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Comput Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="602" to="611" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Can we understand van Gogh&apos;s mood? Learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimedia</title>
		<meeting>the 20th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting emotions in user-generated videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence and the 26th Innovative Applications of Artificial Intelligence Conference and the 5th Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence and the 26th Innovative Applications of Artificial Intelligence Conference and the 5th Symposium on Educational Advances in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image sentiment analysis using deep convolutional neural networks with domain specific fine tuning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 International Conference on Information Processing</title>
		<meeting>2015 International Conference on Information Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting viewer perceived emotions in animated GIFs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual affect around the world: A large-scale multilingual visual sentiment ontology</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentiment extraction from natural audio streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8485" to="8489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic sentiment extraction from YouTube videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>2013 IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="239" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic audio sentiment extraction using keyword spotting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the International Speech Communication Association</title>
		<meeting>the 16th Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2709" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic sentiment detection in naturalistic audio</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-ACM Trans Audio, Speech, Language Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1668" to="1679" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TOM: twitter opinion mining framework using hybrid classification scheme</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Qamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis Support Syst</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ontology-based sentiment analysis of twitter posts</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kontopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berberidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dergiades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4065" to="4074" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sentiment analysis on microblog utilizing appraisal theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Korenek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šimko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="847" to="867" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Survey on visual sentiment analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Res Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The effect of whitening transformation on pooling operations in convolutional autoencoders</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J Adv Sign Proc</title>
		<imprint>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sentiment analysis of Chinese micro-blog based on multi-modal correlation model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Image Processing</title>
		<meeting>the 2015 IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4798" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Emotional textile image classification based on cross-domain convolutional sparse autoencoders with feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Electron Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13022</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SentiNet: Mining visual sentiment from scratch</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computational Intelligence Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image sentiment prediction based on textual descriptions with adjective noun pairs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1115" to="1132" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Large-scale machine learning at twitter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="793" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">GIF video sentiment detection using semantic sequence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Probl Eng</title>
		<imprint>
			<biblScope unit="page">6863174</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fusion of low-and high-dimensional approaches by trackers sampling for generic human motion tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition</title>
		<meeting>the 21st International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="898" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Action2Activity: Recognizing complex activities from sensor data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1617" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recognizing complex activities by a probabilistic interval-based model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence, AAAI 2016</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence, AAAI 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1266" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">From action to activity: sensor-based activity recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="108" to="115" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fortune teller: Predicting your career path</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence, AAAI 2016</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence, AAAI 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Complura: Exploring and leveraging a large-scale multilingual visual sentiment ontology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="417" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards unsupervised physical activity recognition using smartphone accelerometers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10701" to="10719" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM International Conference on Multimedia</title>
		<meeting>the 2010 ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: harvesting opinions from the web</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM International Conference on Multimodal Interaction</title>
		<meeting>the 2011 ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Social multimedia: highlighting opportunities for search and mining of multimedia data in social media applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="34" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Mapping images to sentiment adjective noun pairs with factorized neural nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06838</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sentiment analysis on multi-view social data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Modeling</title>
		<meeting>International Conference on Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fusing audio, visual and textual clues for sentiment analysis from multimodal content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Beyond binary labels: political ideology prediction of twitter users</title>
		<author>
			<persName><forename type="first">D</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="729" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A survey on opinion mining and sentiment analysis: tasks, approaches and applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="14" to="46" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis of Spanish online videos</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">AudioSentibank: Large-scale semantic ontology of acoustic concepts for audio content analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03766</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Contextual semantics for sentiment analysis of twitter</title>
		<author>
			<persName><forename type="first">H</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Process Manag</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploiting multimodal affect and semantics to identify politically persuasive web videos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM International Conference on Multimedia</title>
		<meeting>the 2010 ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A survey and comparative study of tweet sentiment analysis via semi-supervised learning</title>
		<author>
			<persName><forename type="first">Nffd</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lfs</forename><surname>Coletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Twitter polarity classification with label propagation over lexical links and the follower graph</title>
		<author>
			<persName><forename type="first">M</forename><surname>Speriosu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sudan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First workshop on Unsupervised Learning in NLP</title>
		<meeting>the First workshop on Unsupervised Learning in NLP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Discovering affective regions in deep convolutional neural networks for visual sentiment prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE International Conference on Multimedia and Expo</title>
		<meeting>2016 IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tofiloski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;14)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: successful approaches and future challenges</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="292" to="303" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">User modeling with neural network for review rating prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI&apos;15)</title>
		<meeting>the 24th International Conference on Artificial Intelligence (IJCAI&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1340" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Assoc Inform Sci Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Cross-media learning for image sentiment analysis in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vadicamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cimino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>16th IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A survey on emotional semantic image retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th IEEE International Conference on Image Processing</title>
		<meeting>15th IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis for social media images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2378" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Beyond object recognition: Visual sentiment analysis with deep coupled adjective and noun neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3484" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis: Sentiment analysis using audiovisual format</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhushan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd International Conference on Computing for Sustainable Global Development</title>
		<meeting>2nd International Conference on Computing for Sustainable Global Development</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1415" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Sentiment and emotion analysis for social multimedia: Methodologies and applications</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1445" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Joint visual-textual sentiment analysis with deep neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1071" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Web Search and Data Mining</title>
		<meeting>ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Visual and textual sentiment analysis of a microblog using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<idno>number: 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<meeting>the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Combining lexicon-based and learning-based methods for twitter sentiment analysis. Hp Laboratories Technical Report Zuhe Li is a lecturer at the Zhengzhou University of Light Industry. He received his BS degree in electronic information science and technology from the Zhengzhou University of Light Industry in 2004</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dekhil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">his MS degree in communication and information system from the Huazhong University of Science and Technology in 2008, and his Ph.D. degree in information and communication engineering from the Northwestern Polytechnical University in 2017. His current research interests include computer vision and machine learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">He is currently a postdoctor at school of computer science, Northwestern Polytechnical University. He is also a professor at college of electrical and information Engineering, Shaanxi University of science &amp; technology. His current research interests include image processing, computer vision and pattern recognition</title>
	</analytic>
	<monogr>
		<title level="m">Tao Lei received his Ph.D. degree in information and communication engineering from Northwestern Polytechnical University, Xi&apos;an</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>CCF</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Weihua Liu is a researcher at the Xi&apos;an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences. He received his BS degree from the Xi&apos;an Technological University in 2009, and his Ph.D. degree from the Northwestern Polytechnical University in 2016</title>
	</analytic>
	<monogr>
		<title level="m">His major research interests include Computer Vision, Human gesture recognition and Machine Lerning</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
