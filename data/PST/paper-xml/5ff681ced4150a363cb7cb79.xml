<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Science Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Harshvardhan</forename><surname>Gm</surname></persName>
							<email>harrshvardhan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kalinga Institute of Industrial Technology</orgName>
								<orgName type="institution" key="instit2">Deemed to be University</orgName>
								<address>
									<postCode>751024</postCode>
									<settlement>Bhubaneswar</settlement>
									<region>Odisha</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kumar</forename><surname>Mahendra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kalinga Institute of Industrial Technology</orgName>
								<orgName type="institution" key="instit2">Deemed to be University</orgName>
								<address>
									<postCode>751024</postCode>
									<settlement>Bhubaneswar</settlement>
									<region>Odisha</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manjusha</forename><surname>Gourisaria</surname></persName>
							<email>mkgourisaria2010@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kalinga Institute of Industrial Technology</orgName>
								<orgName type="institution" key="instit2">Deemed to be University</orgName>
								<address>
									<postCode>751024</postCode>
									<settlement>Bhubaneswar</settlement>
									<region>Odisha</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Siddharth</roleName><forename type="first">Swarup</forename><surname>Pandey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kalinga Institute of Industrial Technology</orgName>
								<orgName type="institution" key="instit2">Deemed to be University</orgName>
								<address>
									<postCode>751024</postCode>
									<settlement>Bhubaneswar</settlement>
									<region>Odisha</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Rautaray</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kalinga Institute of Industrial Technology</orgName>
								<orgName type="institution" key="instit2">Deemed to be University</orgName>
								<address>
									<postCode>751024</postCode>
									<settlement>Bhubaneswar</settlement>
									<region>Odisha</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Science Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F38F44EFC212517153D635867D0EBD7D</idno>
					<idno type="DOI">10.1016/j.cosrev.2020.100285</idno>
					<note type="submission">Received 18 April 2020 Received in revised form 28 June 2020 Accepted 11 July 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative models Machine learning Deep learning Neural networks Bayesian inference</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Modern day machine learning classifiers mainly include logistic regression, support vector machine (SVM), supervised feedforward deep neural networks, nearest neighbour, conditional random fields (CRFs), etc. All of these models focus on the discriminative classification process -where they only model the decision boundary between the classes by learning directly from the training data. However, with generative models, this is not the case as these models assume that data is created by a probability distribution which is then estimated and a distribution very similar to the original one is generated. The probability of the target variable conditioned on the given input variable is then calculated based upon this generated distribution. Fundamentally, both discriminative and generative classifiers perform the same task because of the last step of calculating the conditional probability of the target variable. Mathematically speaking, if we have two variables X and Y as the independent and target variable respectively, discriminative classifiers only estimate the parameters of P(Y |X) whereas generative models estimate the distribution given by P(X |Y ) and P (Y ) with particular algorithms, finally using Bayes' rule to calculate P (Y |X). The Naïve Bayes' classifier is based on this principle, which is also considered to be generative. One may arbitrarily sample from P(Y |X), compute the modes of the distribution argmax Y P(Y |X), or find expectation of the distribution P(Y |X). Before estimation, this joint distribution may be constrained to having lower degrees of freedom which may be done by structurally studying the conditional independencies between all the variables of the joint distribution as illustrated by Fig. <ref type="figure" target="#fig_0">1</ref> for n variables X 1 , X 2 , . . . , X n of the joint distribution P(X 1 , X 2 , . . . , X n ). Fig. <ref type="figure" target="#fig_0">1</ref> demonstrates the factorization product of conditional distributions over variables conditioned on parents π i for variable</p><formula xml:id="formula_0">X i as, P (X 1 , X 2 , . . . , X n ) = n ∏ i=1 P(X i |X π i ) (1)</formula><p>In many cases, it is difficult to directly estimate the probability of Y conditioned on X hence we use generative models to produce a distribution which resembles the original distribution of the probability of X conditioned on Y to then run a classifier similar approach and calculate the probability of the target Y conditioned on X . Training generative models, especially deep generative models, takes longer than discriminative models since creating a probability distribution resembling the original involves a substantially higher number of correlations to learn as opposed to simply labelling instances to their most probable classes as discriminative models do. For example, a convolutional neural network (CNN) classifier only has to spot a few tell-tale differences between the images of cats and dogs to differentiate them as opposed to a deep convolutional generative adversarial network (DCGAN) which has to generate images of cats and dogs by learning all the features, even the ones that the CNN may have skipped. Fundamentally, discriminative models only draw the decision boundary in a data space, whereas generative models learn the overall distribution of the data. Generative models, as we shall see, have their contemporary uses as powerful feature extraction tools <ref type="bibr" target="#b0">[1]</ref>, or in regression, clustering and classification <ref type="bibr" target="#b1">[2]</ref>, pattern recognition followed by generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, recommendations <ref type="bibr" target="#b4">[5]</ref>, topic modelling, text generation, etc. Classifiers have recently found useful applications in object tracking and detection through algorithms like the Single Shot Detection <ref type="bibr" target="#b5">[6]</ref>, YOLO (You Only Look Once) <ref type="bibr" target="#b6">[7]</ref>, Faster R-CNN <ref type="bibr" target="#b7">[8]</ref> and Masked R-CNN <ref type="bibr" target="#b8">[9]</ref> which take the normal classification of visual data one step further by detecting objects within a picture with apt bounding boxes. These approaches are used in cutting-edge research in the field of self-driving cars and computer vision based automation in the industry. Classifiers also find extensive use in biological applications in diagnosis of diseases <ref type="bibr" target="#b9">[10]</ref>, weather prediction <ref type="bibr" target="#b10">[11]</ref>, assessment of potential high-risk loan applicants for banks <ref type="bibr" target="#b11">[12]</ref>, and even used in sorting important and spam mail in every modern day emailing services like Gmail, Yahoo!, etc.</p><p>We notice that there is a huge focus on discriminative modelling, in other words, classifier-based approaches in problem solving while generative models have not yet sought comparable prominence. The use of generative models is of paramount importance, the reason being two-fold: (a) they may be used to select indicative features and act as feature selection tools to facilitate classification and increase model accuracy, and (b) they can be applied to generate realistic data samples which is not something the discriminative models are capable of. The motivation of this survey arises from these two pertinent aspects of generative models, combined with the fact that the amount of research done on them does not do justice to their capabilities.</p><p>In this paper, we introduce all the generative models (and thus put forward an exhaustive list) section-wise and review the work done on them as: 1. Gaussian Mixture Models (GMM), 2. Hidden Markov Models (HMM), 3. Latent Dirichlet Allocation (LDA), 4. Boltzmann Machines (BM), 5. Variational Autoencoders (VAE), 6. Generative Adversarial Networks (GAN), and 7. Analysis and Discussion, where we analyse and implement all the models discussed in this paper and compare them so the reader can gain  insights on which model (if they are looking to implement one of the generative models for a task) to pick from to best suit their needs. The models discussed in this paper are differently classified under machine learning as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Gaussian mixture models</head><p>A Gaussian distribution is simply another name for a normal distribution which is continuous for a real valued random variable and is symmetric about its mean. The probability of occurrence of data is more likely in the vicinity (left and right sides) of the mean and tapers off at the edges tending to be zero or becoming asymptotic to a real value like zero. In certain situations, we deal with different Gaussians (or curves that are normally distributed) in a single graph (Fig. <ref type="figure" target="#fig_2">3</ref>). The magnitude of these different Gaussians can be represented by weights π = {π 1 , π 2 , π 3 , . . . , π n }, mean µ = {µ 1 , µ 2 , µ 3 , . . . , µ n }, variance ρ = {ρ 1 , ρ 2 , ρ 3 , . . . , ρ n }, for a mixture of n Gaussians. In our case, n = 3 and 0 ≤ π i ≤ 1, where i ∈ {1, 2, 3}. The weights are such that,</p><formula xml:id="formula_1">n ∑ i=1 π i = 1 (2)</formula><p>The magnitudes of the weights associated to each Gaussian sum to 1 as they represent the prior probability of finding that cluster i when taking into consideration the set of all the data.</p><p>Since our Gaussian mixture model is in one dimension, each unique cluster can be identified by the 3-tuple format given by: {π i , µ i , ρ 2 i }. In 2D, π i is a vector and ρ i a covariance matrix. The prior probability of any random data point z k from a dataset of d points (where 1 ≤ k ≤ d) to belong to a particular cluster i is given by the equation,</p><formula xml:id="formula_2">P (z k = i) = π i ,<label>(3)</label></formula><p>where the argument of the probability P is the cluster assignment for observation z k . However, if we are given that z k is from a cluster i, the likelihood of observing x k is given as,</p><formula xml:id="formula_3">P( x k | z k = i, µ i , ρ i )<label>(4)</label></formula><p>= N( x k | µ i , ρ i ) N( x k | µ i , ρ i ) is the likelihood which is a single Gaussian with a mean µ i and variance ρ i . Another equation that represents a weighted Gaussian mixture model with N components can be written as,</p><formula xml:id="formula_4">P( x | ϑ) = N ∑ k=1 w i b( x | u k , ρ k ), (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where ϑ is the 3-tuple format used to identify clusters as discussed before and b( x | u k , ρ k ) are the component Gaussian densities for k = 1, 2, . . . , N defined by, <ref type="bibr" target="#b5">(6)</ref> for a mixture of Gaussians in an M dimensional space.</p><formula xml:id="formula_6">b( x | u k , ρ k ) = 1 (2π ) M/2 |ρ k | 1/2 e -1 2 (x-µ k )( ∑ k (x-µ k ) -1 )</formula><p>GMMs are considered a generalization of the K-means clustering algorithm <ref type="bibr" target="#b12">[13]</ref> because the latter is only prominent at detecting clusters of a circular shape in 2D (and hyper-sphere in higher dimensions). However, GMMs can form clusters of oblong nature as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Albeit GMM can be termed as a clustering algorithm, it is more correct to call it an algorithm for density estimation. When the GMM fits on a data, it is a generative probabilistic model giving us the recipe to generate new data distributed similar to the distribution to which the GMM was fit. <ref type="bibr" target="#b13">[14]</ref> mentions the Kullback-Leibler (KL) Divergence technique which is a tool used for statistics and pattern recognition. The KL divergence is used to measure the similarity between two density functions p(x) and q (x) defined by:</p><formula xml:id="formula_7">D(p ∥ q) = ∫ p (x) log p (x) q (x) dx (7)</formula><p>D(p ∥ q) is always ≥ 0 and is 0 iff p = q. This method is not analytically tractable and hence <ref type="bibr" target="#b13">[14]</ref> puts forward two methods namely variational approximation and variational upper bound to measure the similarity between GMMs. GMMs are also used in language identification systems as described in <ref type="bibr" target="#b14">[15]</ref> where the two approaches mentioned use shifted delta cepstra (SDC) feature vectors. The first approach is based on acoustic scoring which is an identification system composed of a pre-processor to extract features, a backend classifier and a Gaussian mixture of all the target languages. The second approach is done through GMM tokenization as shown in Fig. <ref type="figure" target="#fig_4">5</ref> which comprises a parallel sequence of GMM tokenizers which feed a bank of tokenizer dependent interpolated (unigram and bigram) language models. A sequence of symbols is generated by each tokenizer which corresponds to the per frame indices of the Gaussian component having the highest score. Now, the likelihood of each of these sequences is fed to the language models which generate scores for the corresponding language. These scores are given to the final backend classifier to get the results. GMMs have been used for speech recognition <ref type="bibr" target="#b14">[15]</ref> and even more sophisticated tasks like accent recognition <ref type="bibr" target="#b15">[16]</ref>. The implementation for GMMs is demonstrated in Section 7.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hidden Markov models</head><p>Hidden Markov models are used to generate sequences named as Markov chains that are a series of states having certain statetransition probabilities which generate state sequences having corresponding symbol-emission probabilities (Fig. <ref type="figure" target="#fig_5">6</ref>). The series of states are said to be Markov chains because the probability of reaching the next state is dependent on the transition function of the current state. HMMs are used in statistical modelling for linear problems involving time series or sequences and have many common links with probabilistic non-deterministic finite automata. In fact, as proven in <ref type="bibr" target="#b16">[17]</ref>, HMMs are equivalent to probabilistic automata with no final probabilities which can be converted into equivalent probabilistic non-deterministic finite automata. Hidden Markov models describe a probability distribution over a non-finite number of possible sequences.</p><p>The probabilistic automata as shown in Fig. <ref type="figure" target="#fig_5">6</ref> generate hidden state sequences based on the state transition probabilities. Each state emits residues or symbols based on their symbolemission probabilities and finally we have an observable symbol sequence. On the contrary, the Markov chain (state sequence) that led to these emissions are not observable; they are hidden and hence the name hidden Markov models. HMMs are a probabilistic representation of a system they model, so these states and their transitions need to be constructed which aptly describe the behaviour of the system.</p><p>Hidden Markov models have been extensively used for speech recognition <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> and in other instances, for biological sequence modelling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and optical character recognition (OCR) <ref type="bibr" target="#b21">[22]</ref>. The implementation for HMMs is demonstrated in Section 7.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Latent Dirichlet allocation</head><p>Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b22">[23]</ref> is a generative technique mainly used for topic modelling, although, more broadly, is considered a dimensionality reduction technique. Topic modelling is the process of making a machine predict the relevant topics associated with the input text. We next describe the process followed by LDA to achieve this.</p><p>LDA assumes there to be a vocabulary with W distinct words, each word described by W j where 0 ≤ j ≤ W -1 and T different topics. Each topic T i , where 0 ≤ i ≤ T -1, is described by a probability distribution ω T i over all the W words each with Dirichlet prior β. Then ω T i ,W j denotes the probability of topic T i being represented by word W j . If there are D documents (by documents, we do not mean the entirety of an actual document containing many paragraphs, instead, it is implied that a document can be seen as a block of text or a paragraph) then essentially β is defined as the distribution of words W over all documents D. Similarly, another Dirichlet prior α is defined as the distribution of topics T over all documents D. Let z notate each topic which is assigned to each word thereby making each document a mixture of these topics. Let there be N D k words in each document D k where 0 ≤ k ≤ D -1 and probability distribution of each document over all the topics given by ϕ D k drawn from the Dirichlet distribution with parameter α. So, we can say ϕ D k ,T i is the probability that document D k is associated with topic T i . Assuming α, β as scalars (which, however, we take as vectors in Fig. <ref type="figure" target="#fig_6">7</ref> and while defining the Dirichlet distribution later), LDA involves iterating through each document D k having N D k words. For word W j , a topic assignment is drawn z D k ,W j from the categorical distribution ϕ D k , and then a word V D k ,W j is drawn from the categorical distribution ω z D k ,W j . The algorithm is summarized as follows:</p><formula xml:id="formula_8">1. Draw ϕ T i ∼ Dir(β) for each 0 ≤ i ≤ T -1. 2. Consider D k for each 0 ≤ k ≤ D -1: (i) Draw ω T ∼ Dir(α) (ii) Draw z D k ,W j ∼ Cat ( ϕ D k ) for each 0 ≤ k ≤ D -1 (iii) Draw V D k ,W j ∼ Cat(ω z D k ,W j ) for each 0 ≤ k ≤ D -1,</formula><p>where Cat denotes the categorical distribution and Dir denotes the Dirichlet distribution whose argument is either one of the Dirichlet priors α or β. The Dirichlet distribution parameterized by vector α is given by:  where, µ is the Beta distribution function given by:</p><formula xml:id="formula_9">Dir ( ϕ D k |α ) = 1 µ(α) D-1 ∏ k=0 D α k -1 k ,<label>(8)</label></formula><formula xml:id="formula_10">µ (α) = ∏ D-1 k=0 τ (α k ) τ ( ∑ D-1 k=0 α k ) , α = (α 0 , α 1 , α 2 , . . . , α D-1 )<label>(9)</label></formula><p>τ (x) in Eq. ( <ref type="formula" target="#formula_10">9</ref>) is the complete gamma function defined by</p><formula xml:id="formula_11">τ (x) = (x -1)! (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>This method can be used for various tasks like web-spam filtering <ref type="bibr" target="#b23">[24]</ref>, tag recommendation <ref type="bibr" target="#b4">[5]</ref>, bug localization <ref type="bibr" target="#b24">[25]</ref> which involves locating potential buggy files in a software. LDA, apart from tasks revolving topic modelling, has also been used for annotating satellite images into various regions like residential regions, golf courses, deserts, commercial and urban areas as</p><p>shown in <ref type="bibr" target="#b25">[26]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Boltzmann machines</head><p>In this section, we will be discussing about three different kinds of Boltzmann machines as 4.1 Restricted Boltzmann Machines (RBM), 4.2 Deep Belief Networks (DBNs), and 4.3 Deep Boltzmann machines (DBMs). Why Boltzmann machines (BMs) on their own are not discussed here is because BMs are impractical to deploy due to various computational constraints.</p><p>Boltzmann machines are undirected networks composed of many nodes linked with each other via weighted connections. BMs represent a class of unsupervised neural networks which do not try to minimize a loss or achieve a target, instead, they generate data to form a system (which usually is a probability distribution) that closely resembles the original system. There are certain visible and hidden nodes chosen for our convenience where visible nodes are used as the input and output, shown in Fig. <ref type="figure" target="#fig_7">8</ref>. This is because after feeding the visible nodes, through contrastive divergence <ref type="bibr" target="#b26">[27]</ref> which involves Gibbs' sampling (a Monte Carlo algorithm), the visible nodes iteratively feed the hidden nodes through weights, and in return, the hidden nodes feed the visible nodes. This one iteration can also be called a single Monte Carlo Markov Chain walk as a Markov chain is generated at the visible nodes layer.</p><p>Pragmatically, it is not easy to sample each iteration when all the nodes are connected to every other node. Hence, the Restricted Boltzmann Machine was proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Restricted Boltzmann machine</head><p>In RBMs, connections between visible-visible and hiddenhidden (feature detector) nodes are forbidden and hence we end up with a structure as shown in Fig. <ref type="figure" target="#fig_8">9</ref>.</p><p>RBMs (and BMs), in general, are energy based models <ref type="bibr" target="#b27">[28]</ref> where the energy of the joint configuration of visible and hidden nodes E(v, h) is given as:</p><formula xml:id="formula_13">E (v, h) = - ∑ i∈visible p i v i - ∑ j∈hidden q j h j (11) - ∑ i ∑ j v i w i,j h j ,</formula><p>where v i and h j are the states of the visible node i and hidden node j, p i , q j are their biases and weights between them is denoted by w i,j . The RBM makes use of the following formula to assign a probability between each hidden and visible vector pair,</p><formula xml:id="formula_14">p (v, h) = e -E(v,h) ∑ v,h e -E(v,h) , (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where the denominator of the RHS is also called as the partition function. Contrastive divergence follows the gradient for the learning as given by</p><formula xml:id="formula_16">δ log p ( v initial ) δw i,j = ⟨v initial i h initial j ⟩ -⟨v final i h final j ⟩ (13)</formula><p>Through this gradient formula, the lowest energy state is achieved by adjusting the weights.</p><p>RBMs have been used for collaborative filtering in the field of recommender systems <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, facial recognition <ref type="bibr" target="#b30">[31]</ref>, phone recognition <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> where <ref type="bibr" target="#b32">[33]</ref> replaces each Gaussian mixture in a traditional spectral model with an RBM to outperform the former by modelling the joint probability distributions of the source and target spectral features. <ref type="bibr" target="#b33">[34]</ref> uses conditional RBMs (cRBMs) to outperform previously made attempts at phone recognition through HMMs, and sentiment analysis and aspect extraction <ref type="bibr" target="#b34">[35]</ref> among various other applications. The implementation for RBMs is demonstrated in Section 7.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep belief networks</head><p>Extending the idea of RBMs further, DBNs can be called as a network of stacked up RBMs (an RBM is a single level DBN). However, training DBNs is not very simple as there is a phenomenon called ''explaining away'' in Bayesian networks that takes place while inferring the hidden variables in the hidden layers as the posterior distribution over the hidden variables is intractable. Monte Carlo Markov Chains (MCMC) can be used to sample from these intractable posterior distributions, however, they are very time consuming. Explaining away occurs when one of the causes of an effect explains the effect entirely thereby reducing the probability of other causes to be responsible. Another problem with training DBNs is when the prior assumes independence at the deepest hidden layer with initial randomized weights. It would be convenient to eliminate the explaining away effect and the independence of the prior both to train the DBNs more quickly and efficiently.</p><p>We take a logistic belief network with stochastic binary units <ref type="bibr" target="#b35">[36]</ref> to generate data, where the probability of activating a unit i is a logistic function of its immediate prior neighbours j and the weights between them w i,j (the connections are directed to i from j).</p><formula xml:id="formula_17">P (s i = 1) = 1 1 + e (-b i - ∑ j s j w i,j )<label>(14)</label></formula><p>The bias of unit i is given by b i . Considering the belief network to have only one hidden layer, we can say that this hidden layer is factorial (which means that the hidden units are conditionally independent). However, the posterior is not independent because of the likelihood term which comes from the input vectors fed to the visible layer. <ref type="bibr" target="#b36">[37]</ref> proposes that the explaining away in the hidden layer can be eliminated by creating a complementary prior having exactly the opposite correlations to those in the likelihood term originating from the data. This is done so that the product of the prior and the likelihood term yields us a factorial posterior.</p><p>The way data is generated in an infinite belief net as shown in Fig. <ref type="figure" target="#fig_9">10</ref>; left panel and explained in <ref type="bibr" target="#b36">[37]</ref> is the same as using an RBM to generate data. Notice that the alternating connections between hidden and visible layers with tied weights can be reduced down to a two-layer (v, h) RBM consisting of infinite walks of contrastive divergence. Finally, in both cases, the generation process converges at a stationary point also called as the equilibrium point of the Markov chain. The main contribution done by <ref type="bibr" target="#b36">[37]</ref> was to put forward a greedy algorithm to make the DBN learn layer-by-layer.</p><p>First we train an RBM which models the data x. Let R(c 1 |c 0 ) be the posterior over c 1 where c 0 is the input data vector x. What we can achieve from this is an empirical distribution d1 over the layer c 1 when we sample c 0 from d <ref type="bibr" target="#b37">[38]</ref>.</p><formula xml:id="formula_18">d1 ( c 1 ) = ∑ c 0 d ( c 0 ) R ( c 1 |c 0 )<label>(15)</label></formula><p>After training the RBM, we insert this RBM on top of the DBN where the number of layers is (n + 1). Thus, R(c n-1 |c n ) corresponds to D(c n-1</p><p>|c n ) where D(.) is the posterior distribution associated with the DBN. Hence, we are effectively using R(c n-1</p><p>|c n ) to be an approximation of the posterior D</p><formula xml:id="formula_19">( c n-1 |c n )</formula><p>. This eliminates the deepest hidden layer to have an absolutely independent prior. Using Eq. ( <ref type="formula" target="#formula_18">15</ref>), we can write,</p><formula xml:id="formula_20">dn ( c n ) = ∑ c n-1 dn-1 ( c n-1 ) R(c n |c n-1 )<label>(16)</label></formula><p>Through Eq. ( <ref type="formula" target="#formula_20">16</ref>) the samples c n-1 with distribution dn-1 are stochastically transformed to c n with distribution dn . We can now sample in an unbiased manner from c n and feed it downward stochastically by R(c i |c i-1 ). By doing so, all the lower hidden units acquire an approximated posterior of data x clamped at c 0 . Further, using mean-field approximation (as optionally proposed in <ref type="bibr" target="#b36">[37]</ref>), we can approximate posteriors D</p><formula xml:id="formula_21">( c i |c 0 )</formula><p>by transforming each individual samples c i-1 j from level i -1 where j ∈ {0, 1, . . . , m i -1} as m i is the number of units in layer i. Each sample can be transformed by their mean-field expected value</p><formula xml:id="formula_22">E i-1 j</formula><p>where,</p><formula xml:id="formula_23">E i = σ (b i j + W i E i-1 ), where<label>(17)</label></formula><formula xml:id="formula_24">σ (x) = 1 1 + e -x<label>(18)</label></formula><p>Eq. ( <ref type="formula" target="#formula_24">18</ref>) is called the sigmoid function used in logistic models. W i is the weight matrix of ith layer, b i j are biases for unit j existing in the layer i. The general equation of deep belief networks put forward by <ref type="bibr" target="#b36">[37]</ref> is as follows:</p><formula xml:id="formula_25">D ( c i ⏐ ⏐ c i+1 ) = m i ∏ j=1 D(c i j |c i+1 ),<label>(19) and D</label></formula><p>(</p><formula xml:id="formula_26">c i j = 1 ⏐ ⏐ c i+1 ) = σ ⎛ ⎝ b i j + m i+1 ∑ k=1 W i kj c i+1 k ⎞ ⎠<label>(20)</label></formula><p>As they are models with a powerful feature extraction ability, DBNs have been employed for a large spectrum of different tasks viz. breast cancer classification <ref type="bibr" target="#b38">[39]</ref>, time-series forecasting <ref type="bibr" target="#b39">[40]</ref>, classifying audio from different sources (voice activity detection) <ref type="bibr" target="#b40">[41]</ref>, and through convolutional DBNs <ref type="bibr" target="#b41">[42]</ref>, hyperspectral spatial data classification <ref type="bibr" target="#b1">[2]</ref>, facial expression recognition through boosted DBNs <ref type="bibr" target="#b0">[1]</ref>, etc. The implementation for DBNs is demonstrated in Section 7.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Deep Boltzmann machines</head><p>The main difference between DBMs <ref type="bibr" target="#b42">[43]</ref> and DBNs is that in the former all the connections are undirected (see Fig. <ref type="figure" target="#fig_10">11</ref>). DBMs are also used to capture hidden complex underlying features in the data making it suitable for tasks like speech and object recognition. DBMs, as opposed to DBNs, use an approximate inference procedure with an additional bottom-up pass initially to accelerate learning and incorporate top-down feedback which makes the DBM deal well with ambiguous inputs.</p><p>DBMs can also be pretrained layer-wise in a greedy fashion. Having a DBM with 3 layers, the pretraining is done by learning a stack of RBMs with the modification that the bottom-most vector of inputs and the top-most layer are doubled (see Fig. <ref type="figure" target="#fig_11">12</ref>; left panel). This is done to respectively compensate for the scarce top-down input on h 1 and the scarce bottom-up input on h 2 . Lastly, all the weights in the intermediate RBMs are doubled.  These three components when composed together form a single deep Boltzmann machine (Fig. <ref type="figure" target="#fig_11">12</ref>; right panel). An algorithm for the procedure may be written as:</p><p>1. Duplicate visible vector and tie W 1 . Fit this RBM comprising v and h to the data.  3 and tie W 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Freeze W 1 . Use 1st layer of features h 1 through P(h</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Utilize</head><formula xml:id="formula_27">W 1 , W 2 , W 3 to compose a DBM.</formula><p>The traditional process of training BMs uses random initialization to approximate gradients of the likelihood function <ref type="bibr" target="#b43">[44]</ref> for the input data which is not the quickest approach. To tackle this, <ref type="bibr" target="#b42">[43]</ref> proposed a variational technique making use of meanfield inference to approximate expectations correlated to data with a Markov chain based estimation procedure to estimate the model's expected required statistics. This procedure involves Markov chains initializing the weights to appropriate values (as in solving the mean-field fixed point equations for each update in the parameters of the DBM) to facilitate joint learning of all layers. However, this is very expensive when compared to the pretraining of DBNs where inference is done through a single bottom-up pass. Thus, inference of DBMs is accelerated, as proposed in <ref type="bibr" target="#b44">[45]</ref>, with the use of recognition weights. The set of recognition weights {R 1 , R 2 , R 3  } are initialized to the weights</p><formula xml:id="formula_28">{ W 1 , W 2 , W 3 }</formula><p>which are obtained after the greedy pretraining process. With an input clamped on v the recognition weights are applied to reconstruct v = {v 1 , v 2 , v 3  } of the approximating posterior distribution which is fully factorized: =</p><formula xml:id="formula_29">D rec (h|v; µ)</formula><formula xml:id="formula_30">m 1 ∏ p=1 m 2 ∏ q=1 m 3 ∏ r=1 d rec (h 1 p )d rec (h 2 q )d rec (h 3 r ),<label>(21)</label></formula><p>where, d rec (</p><formula xml:id="formula_31">h l i = 1 ) = v l i for l = {1, 2, 3} and µ = {µ 1 , µ 2 , µ 3 }</formula><p>are the mean field parameters. Each hidden layer is activated with a bottom-up pass by (D denotes the number of visible units):</p><formula xml:id="formula_32">v 1 p = σ ( D ∑ i=1 2R 1 ip v i ) ,<label>(22)</label></formula><formula xml:id="formula_33">v 2 q = σ ⎛ ⎝ m 1 ∑ p=1 2R 2 pq v 1 p ⎞ ⎠ ,<label>(23)</label></formula><formula xml:id="formula_34">v 3 r = σ ⎛ ⎝ m 3 ∑ q=1 R 3 qr v 2 q ⎞ ⎠<label>(24)</label></formula><p>As noticed in Eqs. ( <ref type="formula" target="#formula_32">22</ref>) and ( <ref type="formula" target="#formula_33">23</ref>) the recognition weights are doubled to compensate for the lack of top-down feedback (as also shown in Fig. <ref type="figure" target="#fig_11">12</ref>; right panel). However, in the top layer there is no top-down feedback, hence the recognition weights are not doubled. After this step, k iterations of mean-field approximation are applied which initialize at µ = v. These mean-field parameters thus obtained are used in the training updates for the DBM. Finally, the recognition weights are updated in such a manner that the Kullback-Leibler divergence (defined by Eq. ( <ref type="formula">7</ref>))</p><p>between mean-field posterior D mf (h|v; µ) and factorial posterior D rec (h|v; v) is minimized as illustrated in Eq. <ref type="bibr" target="#b24">(25)</ref>.</p><formula xml:id="formula_35">KL(D mf (h|v; µ) ∥ D rec (h|v; v)) = - ∑ i µ i logv i - ∑ i (1 -µ i ) log (1 -v i ) + C (const.)<label>(25)</label></formula><p>Further on, fine-tuning of the DBM may be done discriminatively (in a supervised manner) by feeding a few samples of labelled data. DBMs have been applied on topic modelling to outperform LDA <ref type="bibr" target="#b45">[46]</ref>, multimodal learning <ref type="bibr" target="#b46">[47]</ref>, spoken query detection <ref type="bibr" target="#b47">[48]</ref>, state-of-the-art 3D model recognition <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, face modelling <ref type="bibr" target="#b50">[51]</ref>, etc. The implementation for DBMs is demonstrated in Section 7.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Variational autoencoders</head><p>In this section, we first describe what autoencoders are in 5.1 Autoencoders, and then move on to 5.2 Variational Autoencoders to describe how these models generate data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Autoencoders</head><p>Normal autoencoders comprise three layers (Fig. <ref type="figure" target="#fig_12">13</ref>). The input layer, where {x i } N i=1 ∈ X , the middle layer also known as the coding or the bottleneck layer Z , and lastly the output layer X.</p><p>Autoencoders are an unsupervised approach to learning lower dimensional feature representations from unlabelled data. The inputs are encoded into feature extracted representations and stored in Z through weights. Similarly, an output similar to X at X is generated after decoding of vector Z . There is a mapping function for encoding which may be stated as</p><formula xml:id="formula_36">Z = f (WX + b) (26)</formula><p>where b is the bias and W is the vector of weights. The loss is calculated through a simple L2 loss function at the end after one epoch,</p><formula xml:id="formula_37">L = ∥x -x∥ 2 (27)</formula><p>After calculating the loss, the error is backpropagated through the network and the weights are adjusted like in a normal artificial neural network. Autoencoders can be used to initialize supervised classification models where the decoder is replaced with a classifier and this classifier runs on the extracted feature vector Z to classify only based on the important encoded features. Autoencoders are mainly thought to be used for compression tasks as the cardinality of vector Z is very low as compared to that of vector X and hence data is converted into a compressed format with all the important features still intact. Autoencoders however, are used for more purposes such as collaborative filtering <ref type="bibr" target="#b51">[52]</ref>, hashing for fast image search through binary autoencoders <ref type="bibr" target="#b53">[53]</ref>, and audio generation <ref type="bibr" target="#b54">[54]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Variational autoencoders</head><p>In order to generate data, we must be allowed to sample from the autoencoder model. We first assume that our data</p><formula xml:id="formula_38">{ x i } N</formula><p>i=1 is generated by a true prior latent distribution z (which is assumed to be a Gaussian) given by p θ (z) where θ are the parameters of our model. To generate data, we must now sample from x by the true conditional p θ ( x | z i ) and estimate the true parameters of θ.</p><p>This conditional can be represented by a neural network. Generally, for training generative models we maximize the likelihood of training data,</p><formula xml:id="formula_39">p θ (x) = ∫ p θ (z) p θ (x|z) dz<label>(28)</label></formula><p>The problem with Eq. ( <ref type="formula" target="#formula_39">28</ref>) is that the conditional p θ (x|z) over the integral is intractable. Moreover, the posterior density given by,</p><formula xml:id="formula_40">p θ (z|x) = p θ (x|z)p θ (z) p θ (x)<label>(29)</label></formula><p>is also intractable due to the denominator p θ (x) which we know from Eq. ( <ref type="formula" target="#formula_39">28</ref>) is intractable. As a solution, we can approximate p θ (z|x) through an inference network q ϕ (z|x) which allows us to derive a tractable lower bound which can be maximized by proper optimization. Fig. <ref type="figure" target="#fig_13">14</ref> describes this new probabilistic model where p θ (x|z) may be called the generator network. The logarithm of the data likelihood can then be expressed as the expectation E with respect to z sampled from q ϕ</p><formula xml:id="formula_41">( z|x i ) . log p θ ( x i ) (30) = E z∼qϕ (z|x i ) [ log p θ ( x i )] p θ ( x i ) is independent of z = E z [ log p θ (x i |z)p θ (z) p θ (z|x i )</formula><p>] ,</p><p>from Bayes' rule <ref type="bibr" target="#b30">(31)</ref> Multiplying with a constant, we get,</p><formula xml:id="formula_42">E z [ log p θ (x i |z)p θ (z) p θ (z|x i ) q ϕ (z|x i ) q ϕ (z|x i ) ] (32) = E z [ log p θ (x i |z) ] -E z [ log q ϕ ( z|x i ) p θ (z) ] + E z [ log q ϕ (z|x i ) p θ (z|x i ) ] (33) = E z [ log p θ (x i |z) ]</formula><p>-KL(q ϕ (z|x i ) ∥ p θ (z))</p><formula xml:id="formula_43">+ KL(q ϕ (z|x i ) ∥ p θ ( z|x i ) ) (<label>34</label></formula><formula xml:id="formula_44">)</formula><p>As we can see, in Eq. ( <ref type="formula" target="#formula_43">34</ref>), term III comprises p θ (z|x i ) which we know is intractable. However, we also know that the KL divergence is always ≥ 0 which is useful for optimizing the likelihood.</p><p>Also, term I can be estimated by sampling that is differentiable when we use the reparameterization trick described in <ref type="bibr" target="#b55">[55]</ref>, term II is composed of two Gaussians and their KL divergence gives us a closed-form solution. To maximize the likelihood we have to maximize I and minimize II. Term I describes the reconstruction of input data which requires the expectation to be high (or reconstruct the data very well), whereas term II needs to be minimized, which essentially means the posterior distribution must be as similar as possible to the prior, as by doing this, the Kullback-Leibler divergence is minimized.</p><p>We define a tractable lower bound (because term III is ≥ 0), ε ( x i , θ, ϕ )</p><p>whose gradient can be acquired and optimized, ε</p><formula xml:id="formula_45">( x i , θ, ϕ ) (35) = E z [ log p θ (x i |z) ] -KL(q ϕ (z|x i ) ∥ p θ (z))</formula><p>Therefore, we get a variational lower bound,</p><formula xml:id="formula_46">ε ( x i , θ, ϕ ) ≤ log p θ ( x i )<label>(36)</label></formula><p>While training, we attempt to estimate the parameters θ ′ and ϕ ′ by maximizing ε(x i , θ, ϕ) as</p><formula xml:id="formula_47">θ ′ , ϕ ′ = arg max θ,ϕ N ∑ i=1 ε(x i , θ, ϕ)<label>(37)</label></formula><p>Fig. <ref type="figure" target="#fig_14">15</ref> shows the entire variational autoencoder network.</p><p>VAEs have been used for trajectory prediction from static images <ref type="bibr" target="#b56">[56]</ref>, collaborative filtering <ref type="bibr" target="#b57">[57]</ref>, recreation of music <ref type="bibr" target="#b58">[58]</ref>, modelling frame-wise spectral envelopes in speech processing <ref type="bibr" target="#b59">[59]</ref>, speech emotion classification <ref type="bibr" target="#b60">[60]</ref>, molecule generation <ref type="bibr" target="#b61">[61]</ref>, etc. Later, in Section 7.1.4, we demonstrate how CVAEs (Convolutional Variational Autoencoders) can be applied to generate artificial images from a base distribution of similar images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Generative adversarial networks</head><p>VAEs generate data similar to the original data to an extent, however, they are not the most accurate. In the case of generating images, one can notice blurriness in the generated images. This is overcome by GANs <ref type="bibr" target="#b62">[62]</ref>, proposed by <ref type="bibr" target="#b62">Goodfellow et al. (2014)</ref>, which are the most recent addition to the modern generative models and also achieve high accuracy in generating data. GANs offer a solution of training generative models without the usual procedure of maximizing a log likelihood (as we saw earlier with VAEs) which are usually intractable and required numerous approximations. Neither do GANs require any Markov chains, as in the case of Boltzmann machines.</p><p>In this section, we shall first discuss GANs and the basic building blocks that they comprise along with their ideology of operation. Then, we discuss some of the most relevant and popular derivatives of GANs (although, in practice, there are literally more than a thousand derivatives invented so far) viz. GANs have two components namely the discriminator and the generator. Both these components work in tandem (contrary to the term 'adversarial' -which gives the idea of a competitive environment) and learn features together rather than one of them being pretrained. Fig. <ref type="figure" target="#fig_17">16</ref> describes the whole training process and the ideology of operation in a pedagogical manner.</p><p>Formally, let us denote the distribution of generator G by p G over genuine data x. We define a prior of input noise (latent random variable) p z (z). Note that G is a differentiable function as it operates on non-discrete data z with parameters θ G whose data space is represented by G θ G (z). Similarly, we represent the discriminator D's data space as D θ D (x) having parameters θ D which is the probability that the data came from genuine data x and is not fake (that is, not from p G ). One can think of D as a simple Convolutional Neural Network (CNN) which discriminates between real and fake images and outputs softmax probabilities between 0 and 1 regarding whether the data is real or fake. The general equation of GANs is given by a value function f (G, D) or a minimax objective function given by:  The output of D is subtracted by 1 (to train G appropriately) and backpropagated through G and its weights are adjusted. Further, G generates images with noise input again with readjusted weights, resulting in more realistic looking images. These images are fed to D along with real images and we get outputs which are backpropagated through D so it can better discriminate next time. The outputs for fake images are also backpropagated through G for the betterment of quality of generated images. These steps are repeated for many epochs until D cannot discriminate between generated and real images.</p><formula xml:id="formula_48">min θ G min θ D f (D, G) = E x∼p data (x) log D θ D (x) + E z∼pz (z) log(1 -D θ D (G θ G (z))) (38)</formula><p>that no player can reach their equilibrium by changing only their own strategy. Further, the training of GANs alternates between k steps of optimizing D and one step of optimizing G as follows:</p><formula xml:id="formula_49">D GradAsc = argmax θ D [E x∼p data (x) log D θ D (x) + E z∼pz (z) log(1 -D θ D (G θ G (z)))],<label>(39)</label></formula><p>and,</p><formula xml:id="formula_50">G GradDesc = argmin θ G [E z∼pz (z) log(1 -D θ D (G θ G (z)))]<label>(40)</label></formula><p>Eq. ( <ref type="formula" target="#formula_49">39</ref>) describes the gradient ascent on D while Eq. (40) describes the gradient descent on G.</p><p>However, as reported in <ref type="bibr" target="#b62">[62]</ref>, in practice, initially it may be difficult to train GANs due to insufficient gradient for G. This happens because initially when G is weak, it is easy for D to detect generated data with high confidence thus making</p><formula xml:id="formula_51">log(1 -D θ D (G θ G (z))) saturate. Instead, we choose to maximize log D θ D (G θ G (z)), minLog (41) = argmax θ G [E z∼pz (z) log D θ D (G θ G (z))]</formula><p>This deals with the problem of a flat gradient early on in the training for G by increasing the gradient which helps G to train properly. Intuitively, this modification serves the same purpose as before when G was tending to minimize the objective. We now discuss a few major contributions done on the field of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Deep convolutional GANs (DCGAN)</head><p>DCGANs <ref type="bibr" target="#b63">[63]</ref> were proposed with some structural changes to the original GANs which were unstable to train in the sense that the network could collapse after certain epochs, where the generator produced nonsensical outputs. There were five significant modifications done, which we shall discuss now.</p><p>The first change was to use only convolutional layers, instead of the traditional alternating convolution and max-pooling layers followed by full connection with ANNs. This was inspired by <ref type="bibr" target="#b65">[65]</ref> which found that a convolutional layer with augmented stride can replace a max-pooling layer without any loss in accuracy. This allowed the generator to learn its own spatial upsampling (through transposed convolutional layers or fractionally strided convolutional layers) and similarly for the discriminator to learn its own spatial downsampling.</p><p>Secondly, the full connection on top of the highest convolutional features was eliminated. The highest convolutional features were connected to the input of the generator and the output of the discriminator. Generator G takes a noise distribution z as input which can be called a full connection (because it is only a matrix multiplication) and as for discriminator D, the last convolutional layer is flattened to be fed into a single sigmoid output. Fig. <ref type="figure" target="#fig_18">17</ref> sheds light on the DCGAN architecture of G.</p><p>The third change was to use batch normalization in both networks (except for the G output layer and D input layer to avoid sample oscillation and model instability). Batch normalization normalizes data to have zero mean and unit variance, which averts training problems arising from poor initialization and helps to have strong gradients in deep networks. This change showed that G could initially learn better with stronger gradients and also avoided it to collapse later on where all the generated samples would be the same and not make any sense (however, this can still happen with DCGANs demonstrated in Fig. <ref type="figure" target="#fig_19">18</ref>).</p><p>The fourth and fifth changes made respectively were to use rectified linear unit (reLU) as activation function <ref type="bibr" target="#b66">[66]</ref> for the G (except the final layer which had tanh activation, allowing the model to learn quicker to convergence and utilize the whole spectrum of the colours from the training data) and to use leaky reLU <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b68">68]</ref> for D for higher resolution modelling which incorporates a negative slope in the negative domain of the function to achieve better results in neural networks. Fig. <ref type="figure" target="#fig_20">19</ref> shows the comparison between reLU and leaky reLU (left panel) along with an example of a DCGAN generated image sample as compared to real data (right panel). DCGANs have been used for CNN-based image recognition <ref type="bibr" target="#b69">[69]</ref>, automatic sketch colourization <ref type="bibr" target="#b70">[70]</ref>, gesture recognition <ref type="bibr" target="#b71">[71]</ref>, object regeneration <ref type="bibr" target="#b72">[72]</ref>, infrared image colourization <ref type="bibr" target="#b73">[73]</ref>, etc. The implementation for DCGANs is demonstrated in Section 7.1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Fully connected and convolutional GANs (FCC-GAN)</head><p>[74] puts forward a fully connected and convolutional GAN and argues that having full connection along with max pooling layers in both components of the GAN can outperform the traditional DCGANs. FCC-GAN demonstrates improvement over the traditional DCGAN in sample quality, learning speed and stability.</p><p>The strided convolution layers in D are replaced by pooling layers having unit stride convolution (Fig. <ref type="figure" target="#fig_22">21</ref>).   The FC network of the generator maps noise to intermediate image features which has many advantages. It allows learning of essential non-spatial mapping from the noise vector to intermediate features. It also captures the underlying relationship between different noise vectors that should be mapped to similar features of the same class of images. Finally, it solves the problem of shared weights in the DCGAN architecture which did not allow the generation of slight variations with respect to different spatial zones in the same convolution filter, thus giving more realistic results. The problem with the discriminator of the DCGAN is that it extracts high dimensional features from an input image which makes it easier to make a decision boundary between real and fake. This implies that the discriminator loss converges quickly to small values. If D becomes too powerful as compared to G, then the learning gradients for G would become too weak or flat and may even vanish completely. Secondly, the distance between the decision boundary and category regions in high dimensional space may be high, and thus the gradients may point to random directions which is not appropriate to train G. FCC-GANs solve the above problems by reducing the high dimensional features to lower dimensions, thus bringing the decision boundary closer to category regions, and since it is harder to build the decision boundary in lower dimensions, D cannot easily discriminate between real and fake data points. This in turn slows down the convergence rate.</p><p>Finally, the last advantage of FCC-GAN over traditional DC-GANs is the average pooling in D which boosts performance and acts as a regularization in feature extraction process which is very useful in deep fully connected networks. Fig. <ref type="figure" target="#fig_1">20</ref> shows results of DCGAN (denoted by conventional CNN), FCC-GAN-S (strided convolution), and FCC-GAN-P (pooling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Conditional GANs (CGAN)</head><p>CGANs <ref type="bibr" target="#b75">[75]</ref> provide a control over the data being generated by conditioning it on additional data (which could be class labels or associated data from a different modality). Let this additional -D θ D (G θ G (z|y)))</p><p>CGANs have been used for many purposes such as face generation <ref type="bibr" target="#b76">[76]</ref>, transforming face by age <ref type="bibr" target="#b77">[77]</ref>, removal of rain from images <ref type="bibr" target="#b78">[78]</ref>, speech enhancement <ref type="bibr" target="#b79">[79]</ref>, classifying spoken language <ref type="bibr" target="#b80">[80]</ref>, however, they do not promise to always outperform their non-conditional counterparts. With hyperparameter tuning and changes to the architecture it may be possible to do so, yet CGANs on their own are not always more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Stack GANs (SGAN)</head><p>Stacked GANs <ref type="bibr" target="#b81">[81]</ref> comprise stacks of top-down generators, each of which generate lower level representations while being conditioned on corresponding higher level representations of the data. Similarly, in a bottom-up manner, a series of deep neural networks (DNNs) are connected which form the encoder. The adversarial lossω adv is introduced which is given by each of the intermediate representations of the units (DNNs) in the encoder which forces these intermediate representations to lie on the manifold of the DNN's representation space. There are two more types of losses, namely, conditional lossω cond and entropy lossω ent . Conditional loss arises due to the conditional nature of the generators used which are conditioned on class labels y with noise input z which forces the generators to better utilize the high level conditional information. Entropy loss forces the generators to make the generated representations more varied and diverse. Fig. <ref type="figure" target="#fig_23">23</ref> shows the architecture of SGAN in its entirety.</p><p>The encoder E is pretrained initially for classification. Let there be a stack of bottom up deterministic non-linear mappings h i+1 = E i (h i ), i ∈ {0, 1, . . . , N -1} where N is the total number of stacks and h i (i ̸ = 0, N} are the intermediate representations between each encoding unit (it is worthy of mention that h 0 = x (original data) and h N = y which are the predictions). E i refers to the ith unit of E which is made up of neural network layers like convolutions, pooling, etc. Each G i then takes the higher level feature and the noise vector to produce a lower level feature ĥi . As described in Fig. <ref type="figure" target="#fig_23">23</ref>, the generators G i , whose function is to invert a bottom-up mapping of the corresponding E i , is trained independently first with the conditional input provided by E as ĥi = G i (h i+1 , z i ), and then jointly when each G i (except the top-most, which takes input from the prediction y) gets input from upper generators as ĥi = G i ( ĥi+1 , z i ). All of the loss functions described henceforth are for independent training which can be represented the same way for joint training by replacing h i+1 with ĥi+1 wherever applicable.</p><p>The total loss is calculated as the linear combination of the three losses described earlier for each G i as:</p><formula xml:id="formula_52">ω G i = C 1 ω adv G i + C 2 ω cond G i (<label>43</label></formula><formula xml:id="formula_53">)</formula><formula xml:id="formula_54">+ C 3 ω ent G i ,</formula><p>where C 1 , C 2 and C 3 are the weights corresponding to each of the respective losses, however, these weights are not learnt. <ref type="bibr" target="#b81">[81]</ref> specifies that in practical use, these weights are set in such a magnitude that all the losses fall in similar scales. For each G i there exists a corresponding discriminator D i that judges the generated sample ĥi based on real representations given by h i whose loss function may be given as:</p><formula xml:id="formula_55">ω D i = E h i ∼P data ,E [-log D i (h i )] + E z i ∼Pz i , h i+1 ∼P data ,E [-log(1<label>(44)</label></formula><formula xml:id="formula_56">-D i (G i (h i+1 , z i )))]</formula><p>Each G i 's adversarial loss is given by the equation,</p><formula xml:id="formula_57">ω adv G i = E h i+1 ∼P data ,E, z i ∼Pz i [-log(D i (G i (h i+1 , z i )))]<label>(45)</label></formula><p>Conditional loss ω cond serves the purpose of preventing generators G i to generate data from the lower level representation inputs ĥi by completely ignoring high level representations h i+1 , which should not be the case as G i is supposed to be conditioned on h i+1 . This is a form of regularization applied on G i . The generated lowlevel distributions ĥi are fed back to corresponding encoder unit E i to compute recovered higher-level representations. ω cond is used to force these recovered representations to be close (in Euclidean distance defined by f ) to conditional representations as,</p><formula xml:id="formula_58">ω cond G i = E h i+1 ∼P data ,E, z i ∼Pz i [f (E i (G i (h i+1 , z i )) , h i+1 )]<label>(46)</label></formula><p>Another problem that arises with conditional loss is that G i tends to ignore the latent noise distribution z while determining ĥi con- ditioned on h i+1 . To solve this, it is necessary that the generated representations ĥi be sufficiently diverse being conditioned on h i+1 given by the conditional entropy function H( ĥi |h i+1 ). Higher entropy means higher diversity, thus desirably one would want to maximize H, but it is intractable. <ref type="bibr" target="#b81">[81]</ref> proposes a variational lower bound on H where an auxiliary distribution Q i (z i | ĥi ) is used to approximate the true posterior P i (z i | ĥi ) through the entropy loss ω ent G i given by,</p><formula xml:id="formula_59">ω ent G i = E z i ∼Pz i E ĥi ∼G i ( ĥi |z i ) [-log Q i (z i | ĥi )]<label>(47)</label></formula><p>ω ent G i is minimized to maximize the variational lower bound for </p><formula xml:id="formula_60">H( ĥi |h i+1 ). Q i is</formula><formula xml:id="formula_61">= P G ( ĥ0 | ĥN ) ∝ P G ( ĥ0 , ĥ1 , . . . , ĥN-1 ⏐ ⏐ ⏐ ĥN ) = N-1 ∏ i=0 P G i ( ĥi ⏐ ⏐ĥ i+1 )<label>(48)</label></formula><p>SGAN factorizes H(x) into smaller conditional entropy terms,</p><formula xml:id="formula_62">H (x) = H (h 0 , h 1 , . . . , h N ) = N-1 ∑ i=0 H (h i |h i+1 ) + H (y)<label>(49)</label></formula><p>SGANs achieve higher sample quality than the traditional DC-GANs with either independent or joint training separately. Fig. <ref type="figure" target="#fig_3">24</ref> shows some SGAN generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Analysis and discussion</head><p>Table <ref type="table" target="#tab_14">13</ref>, given later, describes the fields in which all the models discussed in this paper are used in. However, while putting together this survey, we noticed that there are common fields amongst two entirely different models that they are applied on. For example, we have seen that LDA is mainly meant for topic modelling, and still, DBNs have outperformed LDA in the same field <ref type="bibr" target="#b45">[46]</ref>. We saw that HMMs were good for pattern recognition like the case of speech recognition, which was also done effectively by VAEs <ref type="bibr" target="#b60">[60]</ref>. Virtually, one may think that any of the generative models can be applied in some form to tackle all the problems that specific classes of generative models usually do.</p><p>Having mentioned that, it is still useful to apply only a certain and relevant class of generative models for a specific problem for better efficiency of problem solving, simplicity of model design (in the sense that one should not arbitrarily throw a deep generative network with many layers at data which does not require feature extraction at various levels for better learning), for better cost minimization, among various other reasons. In this section, we analyse all the generative models discussed thus far to give the reader a better understanding of the advantages and    stands for Neural Autoregressive Distribution Estimation <ref type="bibr" target="#b83">[83]</ref>, MADE for Masked Autoencoders Distribution Estimation <ref type="bibr" target="#b84">[84]</ref>, Pix-elRNN for Pixel Recurrent Neural Networks <ref type="bibr" target="#b85">[85]</ref> which is a type of fully visible deep belief network (FVBN) <ref type="bibr" target="#b86">[86,</ref><ref type="bibr" target="#b87">87]</ref>, and GSN for Generative Stochastic Networks <ref type="bibr" target="#b88">[88]</ref>. The problem with explicit density models is capturing all the complexities in a given data and simultaneously not losing tractability. To tackle this, FVBNs consider careful construction of the model which guarantees tractability, and on the other hand, explicit models that approximate density like VAEs incorporate tractable approximations in the likelihood and its gradients.</p><p>We further describe all the explicit density models having tractable density distributions listed in Fig. <ref type="figure" target="#fig_26">25</ref>, which the paper does not formally address, in brief. FVBNs use the chain rule of probability to decompose a probability distribution over an M dimensional vector x, giving us a product of 1-D probability distributions as,</p><formula xml:id="formula_63">P model (x) = M ∏ k=1 P model (x k |x 1 , . . . , x k-1 )<label>(50)</label></formula><p>The order of generating a sample in FVBNs is O(n) because each sample has to be generated sequentially as x 1 , then x 2 and so on. The advantage that GANs have over FVBNs is in the respect that all the samples are generated in parallel yielding a very high generator speed. NADE, similar to FVBNs, assume that a product of 1D distributions represent a factorized M dimensional distribution p(x) in any order o, or permutation of integers ∈</p><formula xml:id="formula_64">[1, M], p (x) = M ∏ m=1 p(x om |x o&lt;m ) (51)</formula><p>where o &lt;m represent dimensions ∈ [1, m -1] and x o&lt;m is the corresponding set of dimensions (a subvector) in the order o. NADE specifies a parameterization of all these M conditionals p(x om |x o&lt;m ) using a feed-forward multilayer perceptron given by, p</p><formula xml:id="formula_65">( x om = 1|x o&lt;m ) = sigm(V om , h m + y om ) (<label>52</label></formula><formula xml:id="formula_66">)</formula><formula xml:id="formula_67">h m = sigm(W o&lt;m x o&lt;m + z) (<label>53</label></formula><formula xml:id="formula_68">)</formula><p>where sigm(x) is the logistic sigmoid function defined by sigm (x) 1</p><formula xml:id="formula_69">= 1/(1 + e -x ) with a total of H hidden units, V ∈ R M×H , y ∈ R M , W ∈ R H×M , z ∈ R H ,</formula><formula xml:id="formula_70">N N ∑ n=1 -log p ( x (n) ) = 1 N N ∑ n=1 M ∑ m=1 -log p(x (n) om |x (n) o&lt;m )<label>(54)</label></formula><p>MADEs are modified autoencoders which satisfy the autoregressive property, which is referred as such because of the sequential prediction (regression of) each dimension of x while computing the negative log-likelihood which we define below by Eq. ( <ref type="formula" target="#formula_74">56</ref>). Let l(x) denote the binary cross-entropy loss function given by,</p><formula xml:id="formula_71">l (x) = M ∑ m=1 [-x om log xom - ( 1 -x om ) log(1 -xom )]<label>(55)</label></formula><p>As before, we define p</p><formula xml:id="formula_72">( x om = 1|x o&lt;m ) = xom</formula><p>, along with the fundamental rule of probability that implies p <ref type="formula" target="#formula_71">55</ref>) into a negative log-likelihood, x = sigm(a + Vh (x))</p><formula xml:id="formula_73">( x om = 0|x o &lt;d ) = 1 -xom we transform Eq. (</formula><formula xml:id="formula_74">-log p (x) = M ∑ m=1 -log p(x om |x o&lt;m ) = M ∑ m=1 [-x om log p ( x om = 1|x o&lt;m ) - ( 1 -x om ) log p(x om = 0|x o&lt;m )]<label>(56)</label></formula><formula xml:id="formula_75">(58)</formula><p>where a, b are vectors and W and V are matrices representing connections from input-to-hidden and hidden-to-output respectively, with f as a non-linear activation function. Transforming this simple autoencoder to satisfy autoregressiveness can be done by knowing that since any outputs xom depend upon subvec- tor x o&lt;m , there is no computational waypoint between xom and x om , . . . , x o M . Hence, it can be said that at least one of these paths represented by matrices W or V equals to null. To nullify connections, a binary mask matrix is applied element-wise and multiplied, the values of which are 0 for the entries that correspond to paths that are null (or non-existent). Using the autoencoder defined by Eqs. ( <ref type="formula">57</ref>) and ( <ref type="formula">58</ref>),</p><formula xml:id="formula_76">h (x) = f (b + ( W ⊙ K W )</formula><p>x) <ref type="bibr" target="#b59">(59)</ref> x = sigm(a +</p><formula xml:id="formula_77">( V ⊙ K V ) h (x))<label>(60)</label></formula><p>where K W and K V are defined to be the masks for matrices W and V respectively which help in transforming the autoencoder satisfy the autoregressive property.</p><p>PixelRNN is another neural autoregressive model which models distributions over pixel values which optimizes weights by maximization of likelihood. Given the fragmentation of a joint distribution p(x) defined by Eq. ( <ref type="formula" target="#formula_63">50</ref>), each conditional is considered a multinomial and parameterized through a softmax activation layer. Long Short-Term Memory (LSTM) networks are used to model the product of the conditionals (which are also used in recurrent neural networks, RNNs) to learn long-term dependencies between pixels. For a given pixel, the contextual dependencies are learned by iterating the images row by row from top to bottom demonstrated by Fig. <ref type="figure" target="#fig_28">26</ref>.</p><p>Explicit density estimation models which require approximations provide an intractable explicit density function which needs approximations to maximize likelihood. VAEs are the variational categorization of approximating density models. In general, the lower bound in variational methods is given by,</p><formula xml:id="formula_78">ε (x; θ) ≤ log P model (x; θ) (61)</formula><p>The problem with VAEs is that when a weak approximation of the posterior distribution or prior distribution is used, no matter how good the optimization of the parameters or how big and diverse the dataset, distance between ε and the true likelihood can make the distribution P model be not similar to P data at all; the model can learn an undesired distribution. Variational methods obtain a very high likelihood but still fail to generate samples of quality as high as that of GANs, and when compared to FVBNs, they are difficult to optimize. Generative models often are evaluated based on the quality of samples they produce which is a subjective opinion and not an empirical fact (we discuss more about evaluation of generative models later in this section).</p><p>The other class of generative models which estimate density explicitly by approximation are BMs which use Markov chains to  Running the Markov chain until it converges is also referred to as burning in the Markov chain in literature. However, achieving this convergence is not simple. In literature, the number of steps the Markov chain must run before reaching equilibrium is called the mixing time. There is no proper theory regarding predicting when a Markov chain will reach convergence, although it is clear that it definitely will converge at some point. This is the reason why usually Markov chains, wherever used, are made to run for a sufficient amount of time so one can say that it has roughly converged. Mixing time can be very long and thus in practice, x is put to use a long time before it can actually converge to be a sample from P model . Also, efficiency of Markov chains in higher dimensions diminishes. BMs, more specifically DBNs brought the focus back on deep learning when the official DBN paper was released, however, BMs altogether have lost relevance and are moving along the downward curve in terms of popularity. The reason for this could be that the MC approximation techniques fail to outperform their supervised, directed graph model-centric counterparts in terms of accuracy, and another could be the high cost associated with burning in MCs. Finally, the implicit density models positioned on the right branch of the taxonomy interact indirectly with P model while training by sampling from it. This sampling can be done directly as in the case of GANs, or a Markov chain could be run several times before reaching convergence which translates to sampling from P model itself, an example of which is the GSN. However, as discussed earlier, burning in Markov chains is cost intensive and GANs are better in those regards. Moreover, GANs can generate samples in parallel when put in comparison against FVBNs which only generate samples one by one. There are no difficult approximations of intractable probabilistic computations in maximum likelihood estimation, and, the added advantage GANs possess is better sample quality. Yet, GANs come with their own disadvantages; they are difficult to optimize and are very susceptible to collapsing as discussed in Section 6. Section 7.4 also discusses the difficulty in properly analysing whether the GAN has overfit or underfit the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Implementation of Restricted Boltzmann Machine 1</head><p>For this experiment, we demonstrate how RBMs can be used as recommender systems by generation of a set of recommended movie titles for a particular user given their past history of movies that they liked or disliked. We use the MovieLens ml-100k dataset <ref type="bibr" target="#b90">[90]</ref> as our training data. The data comprises 100 000 ratings of 943 users (in rows) on 1682 movies (in columns). The ratings fall in the range of 1∼5 which, at the time of preprocessing, was binarized so that if the rating was over 3, it would be replaced by a 1, otherwise a 0. The data is binarized as RBMs generally are trained on binary data and are suitable for such cases. The data is fed to the visible layer of the RBM row-wise during training which then goes through contrastive divergence stochastically and a regenerated output is acquired from the visible layer. The regenerated outputs are compared to the original ratings of the users and the model's performance is evaluated based on RMSE metrics (root mean square error) defined as,</p><formula xml:id="formula_79">RMSE = √ 1 N ∑ u,i∈N (r ui -rui ) 2 (62)</formula><p>In Eq. ( <ref type="formula">62</ref>), u denotes a user, i denotes a movie, r ui denotes the rating of user u on movie i, rui denotes user u's predicted rating on movie i and N is the number of movies. The configuration of the network is as follows: the number of visible layers is set to be equal to N (which, in our case, is 1682). This is done so that the input layer can assign one node to each individual rating made by the user for every movie. Obviously, each user cannot rate all 1682 movies and hence these nodes receive a -1 instead of 0. During training, the negative values are frozen and hence these values are not trained. The configuration settings for RBM training are listed in Table <ref type="table" target="#tab_3">1</ref>. After training, we acquire the reconstruction loss per each epoch as illustrated by Fig. <ref type="figure" target="#fig_29">27</ref> and attain a satisfactory loss on the test set of 0.2439 based on RMSE. Hence, we may conclude that RBMs can be used as recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Implementation of deep belief network</head><p>In the previous subsection we demonstrated how RBMs can be used for generating data close to the original distribution when applied as recommender systems. Here, we use DBNs for feature extraction and pipeline a logistic regression model after extracting features to classify a dataset of digits <ref type="bibr" target="#b91">[91]</ref> ranging from 0 to 9. We compare results of classification when logistic regression is applied on raw pixel data versus when the features are extracted from the same data using DBN followed by the application of logistic regression. The dataset is augmented 5fold by shifting the 8 × 8 images 1 px (single pixel) to the left, right, down and up followed by splitting of data into training and testing set of proportions 8:2. The DBN was constructed with specifications provided by Table <ref type="table" target="#tab_4">2</ref>, where {256, 512} implies that the first hidden layer comprises 256 nodes and the second 512 nodes. As mentioned earlier in Section 4.2 the training of DBNs is done by initially training an RBM followed by its insertion at the top of the DBN to eliminate independence of the prior at the top-most layer.</p><p>1 All source code for each implementation further on is provided in <ref type="bibr" target="#b89">[89]</ref>.   It is observed after our experiments that feature extraction using DBNs yields superior classification performance as compared to classification without feature extraction. Reconstruction error of the RBM by training on 20 epochs is illustrated by Fig. <ref type="figure" target="#fig_30">28</ref>. The evaluation metrics for evaluation of both approaches we use are namely precision, recall, and F1 score. Precision is defined as the ratio of positive observations that the model correctly predicted to the total number of predicted positive observations. Recall (also referred to as sensitivity) is the ratio of positive observations that the model correctly predicted to all the observations in the actual class of 'positive'. Finally, F1-score is defined as the weighted average of both precision and recall. Mathematically,</p><formula xml:id="formula_80">Precision = TP /TP+FN (63) Recall = TP /TP+FN (64) F 1 Score = 2(Recall * Precision) (Recall + Precision)<label>(65)</label></formula><p>We illustrate the results in terms of the above metrics by Fig. <ref type="figure" target="#fig_31">29</ref> for classification by feature selection using DBNs and by Fig. <ref type="figure" target="#fig_32">30</ref> for classification without feature selection (in both cases, using logistic regression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3.">Implementation of Deep Boltzmann Machine</head><p>We saw how RBMs can be applied as recommender systems and how DBNs may be used as powerful feature extractors. In  this implementation, we demonstrate how DBMs can be used to regenerate data in form of images by taking the popular MNIST dataset of digits <ref type="bibr" target="#b92">[92]</ref>. Once the data is imported, the pixel values, ranging from 0∼255 are divided by 255 for normalization purposes, scaling all pixel values to lie in [0, 1]. The configuration settings used for the training of the DBM is given by Table <ref type="table" target="#tab_5">3</ref>. The generated samples are illustrated by Fig. <ref type="figure" target="#fig_33">31</ref>. The dimensions {784, 500, 784} refer to the consecutive enumeration of visible and hidden layers separated by commas. We notice that DBMs can reconstruct simple image data such as the MNIST dataset to a decent extent of good sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4.">Implementation of variational autoencoder</head><p>In Convolutional Variational Autoencoders (CVAEs), the encoder and decoder networks comprise convolutional and fractionally strided (or transpose convolutional) layers respectively which help in generating images similar to DCGANs. For this experiment, we use the same dataset as used for DBM-based image generation. The images are resized to 28 × 28 × 1 where 1 represents the channels or the colours, and since our data is black and white, there is only a single channel that ranges between 0∼255. Next all the pixel values are normalized with the division of 255 so the values now lie between [0, 1]. Here, we take 60 000 training images as opposed to 10 000 test images (6:1 split for training size to testing size) with a batch size of 32. All the configuration  settings are provided by Table <ref type="table" target="#tab_6">4</ref>. Latent dimensions refers to the central latent dimensions in CVAE which is the dimensions of data being reconstructed between the encoder and decoder networks. Conv filters refer to the number of filters applied on each convolutional layer. The plotted output image of the CVAE is illustrated by Fig. <ref type="figure" target="#fig_34">32</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.5.">Implementation of generative adversarial network</head><p>In this implementation we make use of DCGANs to reconstruct images from the CIFAR-10 dataset <ref type="bibr" target="#b64">[64]</ref>. All input images are set to have the dimensions 64 × 64 × 3 where 3 refers to the number of channels, and thus 3 refers to channels R, G and B for red, green and blue which when combined can form any colour in the visual spectrum. The configuration settings for training of DCGAN is given by Table <ref type="table" target="#tab_7">5</ref>. Note that learning rate for both, discriminator and generator networks (D, G) as given by Table <ref type="table" target="#tab_7">5</ref> is set at 0.0002. We notice that while training, the network collapses randomly at epoch 15 (by randomly, we mean that it is not mandatory that this DCGAN with its specific hyperparameters would always collapse at epoch 15 on the same dataset), the effects of which start to become visible from epoch 14 as illustrated by Fig. <ref type="figure" target="#fig_35">33</ref> (bottom). The initial epoch results (epochs 1, 2, 3) are shown by Fig. <ref type="figure" target="#fig_35">33</ref> (top) to illustrate how the DCGAN learns the input distribution (as shown in Fig. <ref type="figure" target="#fig_36">34</ref>) and corrects images initially. DCGANs, as mentioned earlier, are susceptible to collapsing, and this experiment verifies that they are indeed unstable and difficult to train.  We now shift our analysis to focus on the generative models discussed in this paper that do not fall in the category of DL (as shown in Fig. <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Analysis of pure ML-based generative models</head><p>When dealing with problems that require clustering, if the distribution to be modelled has hidden, non observable parameters, a viable option is to use GMM. This is because the model essentially assigns probabilities of each point being in some cluster k and does not assume, as in the case of K-means clustering, that the probability of a certain data point to belong to a certain cluster is 1. GMMs are a lot more flexible in terms of cluster covariance and thus we get more flexible clustering regions (elliptical instead of circular) as observed in Fig. <ref type="figure" target="#fig_3">4</ref>. Another advantage of GMMs is that they accommodate mixed membership in the sense that data points can probabilistically belong to more than one cluster, as opposed to K-means which forcefully assigns one cluster for each data point. In GMMs, a data point belongs to each cluster to a different extent or degree. Mixed membership can be beneficial in certain cases, for example, when trying to cluster news articles based on their tags. In this case, a news article may contain many tags, hence it may belong to many clusters, however, some tags are more prominent and some other may not be so prominent. This prominence reflects in the probability assignments accordingly. In other cases, mixed membership may not be so beneficial which can be considered a disadvantage of GMMs, for example, when clustering items that cannot fall under more than one cluster (say, organisms, which can only belong to one species). In this case, K-means does a better job at hard defining a single cluster for each data point.</p><p>LDA, as discussed in Section 2 is a dimensionality reduction technique in actuality. However, we confine our focus to LDA when applied on natural language processing (NLP) tasks (though it can be applied on many other forms of data) as it has a more intuitive use in this field for topic modelling. It assumes that the corpora given as input contain documents which belong to a number of topics. Each topic is associated with a vocabulary of words, and that each document is the result of a mixture of probabilistic samplings -first over distribution of possible topics of the input documents, and then over the amalgamation of possible words in the selected topic. This general assumption gives LDA the biggest advantage as opposed to other previously employed topic modelling approaches viz. latent semantic indexing (LSI) <ref type="bibr" target="#b93">[93]</ref> and probabilistic LSI (pLSI) <ref type="bibr" target="#b94">[94]</ref>. This advantage is that LDA can be extended to separate documents into topics for documents that do not belong to the corpora it was trained on. LDA generalizes the model it uses for dimensionality reduction for new corpora. For example, if the LDA model was trained on news articles corpora to group the news articles into categories like politics, crime, sports, etc. it would be allowed to use the same model to categorize newly published articles. This is not possible in LSI. Another advantage of LDA over pLSI is that the number of parameters grow linearly with the number of documents in the corpora in pLSI, whereas on the other hand, the number of parameters in LDA goes linearly with the number of topics which is much lower than pLSI. Hence, LDA can outperform pLSI in terms of speed on much bigger datasets.</p><p>There are, however, many disadvantages of using LDA. First there must exist prior knowledge about the number of topics; the number of topics K is fixed. Secondly, it is not suitable to apply LDA on short texts as found in <ref type="bibr" target="#b95">[95]</ref> because LDA exploits statistical inference to discover hidden patterns in the data. Thus, when the data is very less (as in the case of tweets on Twitter), there are very few observations to infer the parameters of the model which hinder with the accuracy of the whole affair. Thirdly, LDA cannot capture correlations between topics. For example, LDA will not be able to use to its advantage that a tag, say, computer science is highly correlated with another tag namely programming language. Even with many more disadvantages, LDA has revolutionalized the field of topic modelling and has become the central idea associated with it.</p><p>HMMs have been considered as a specific form of dynamic Bayesian networks and since 1980s have been used for modelling biological sequences (e.g DNA). HMMs assume that the system they try to model is generated by a Markov process with hidden or unknown parameters. Thus, these unknown parameters are tried to be learnt based on the given observable parameters. Not only biological sequence modelling, but they have been used successfully for pattern recognition in the form of speech recognition, OCR, data mining, classification and structural analysis. More generally, one can infer that an HMM can be applied if the system to be modelled is made up of different stages or states that exist in definite or typical orders. HMMs have strong statistical foundation, are simple conceptually, and are very flexible -they are the most flexible generalization of sequence profiling methods as they can handle inputs of variable length. The problem with these models, however, is that the number of unstructured parameters can be very high quite often. Another problem that arises in sequence labelling with HMMs is that at times, it is required to know the correlation between states that are not immediate neighbours as sequences can have contextual property among states and are also of certain length, all of which is not taken into account by HMMs which only take the immediate next state in the transition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Implementation of Gaussian mixture models</head><p>In the experiment, we randomly generate and plot 400 data points gathered around 4 different centres; 100 data points per centre with a standard deviation of 0.6 and initialization of random state by 0. Next, we make another version of this scatter plot by stretching each of the four distributions unidirectionally to better demonstrate results of GMMs and prove how effective they are, as discussed, when the distribution is oblongated. Fig. <ref type="figure" target="#fig_38">35</ref> shows the two randomly generated data distributions. Further, we apply GMM to both these distribution and as an addition, also apply K-means (note that K-means implementation is not provided in <ref type="bibr" target="#b89">[89]</ref>) to compare our results. It is noticed that results shown by Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">Implementation of latent Dirichlet allocation</head><p>For this experiment, we take a textual dataset which, after appropriate preprocessing, contains a raw mixture of author names, abstracts, titles the text corresponding to every research paper presented in NIPS (Neural Information Processing Systems) conferences from the first conference in 1987 till the 2016 conference. To get an estimate of the most popular phrases used in the papers, we generate a word cloud as represented by Fig. <ref type="figure" target="#fig_40">37</ref> (left) and also a bar graph for the most popular words represented by Fig. <ref type="figure" target="#fig_40">37</ref> (right). We apply LDA on this dataset for finding 5 topics and print the 10 most prominent words and the results attained are enlisted in Table <ref type="table" target="#tab_8">6</ref>. Further, introduce two terms saliency <ref type="bibr" target="#b96">[96]</ref> and relevance <ref type="bibr" target="#b97">[97]</ref> in LDAs defined by,</p><formula xml:id="formula_81">saliency (w) = f (w) ∑ T P (T |w) log P(T |w) P(T )<label>(66)</label></formula><p>relevance (w|T )</p><p>= λP (w|T ) + (1 -λ)</p><formula xml:id="formula_82">P(w|T ) P(w)<label>(67)</label></formula><p>where, w denotes a word from the data vocabulary, T denotes a topic from the set of topics, P(E) denotes the probability of event E and λ denotes a weight parameter (0 ≤ λ ≤ 1). Saliency is a measure proposed by <ref type="bibr" target="#b96">Chuang et al. (2012)</ref>  <ref type="bibr" target="#b96">[96]</ref> that aids rapid classification and disambiguation of topics. Relevance is a measure proposed by Sievert et al. <ref type="bibr" target="#b97">[97]</ref> (2014) of a term that provides users with an understanding of how useful the word is in describing the topic. Based on these two parameters, we use LDAvis, a tool made by <ref type="bibr" target="#b97">[97]</ref> to visualize inter-topic distances through multidimensional scaling projected on principal component axes PC1 and PC2 between the 5 topics. Moreover, we show ranking of the top 30 most salient and relevant words (decorated by light blue and red bars, respectively) in any selected topic with λ = 1 as demonstrated by Fig. <ref type="figure" target="#fig_7">38</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3.">Implementation of hidden Markov models</head><p>In this implementation, we assume there to be a particle x that travels to different locations (four) given by the set L = {L 1 , L 2 , L 3 , L 4 } and can be found at one of the locations in L at any    and the probabilities of transitions between the hidden states is given by Table <ref type="table" target="#tab_9">7</ref>. Next, we define the probabilities of particle x to emit a certain type of radiation R i while at any given location L j by Table <ref type="table" target="#tab_10">8</ref>. Finally, we present the initial state probability distribution matrix by Table <ref type="table" target="#tab_11">9</ref> which specifies where x resides initially. Note that Table <ref type="table" target="#tab_9">7</ref>, Table <ref type="table" target="#tab_10">8</ref>, and Table <ref type="table" target="#tab_11">9</ref> specify the true distribution of the system and are provided to demonstrate the likelihood of x to emit a certain event when in a certain state. We apply our HMM on an arbitrary input of observed radiations without the model's prior knowledge of Tables <ref type="table" target="#tab_9">7</ref> and<ref type="table" target="#tab_10">8</ref> and train it on 100 epochs (Fig. <ref type="figure" target="#fig_8">39</ref>) to come up with predictions of likelihood of the particle's location as given by Table <ref type="table" target="#tab_12">10</ref>.</p><p>Note that since the model was initialized from a random state, there is no control over the naming of hidden states of L which is a convention of the model. In other words, the HMM model does not really know which locations we mean by L j . In reality, we shall have to swap the output label names which correctly explain the true emission probability distribution that exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Comparisons</head><p>Table <ref type="table" target="#tab_13">11</ref> gives an application-oriented comparison of all the generative models discussed in this paper to give the readers a quick overview on how the models compare with each other based on their pros and cons. Additionally, we formulated Table 12 to make users aware of the positive and negative aspects that we came across while implementing all the models. A summary and comparison of all the models discussed is given below in Table <ref type="table" target="#tab_14">13</ref> where MC, VI, LAI, AIS, CF, and PDE respectively stand for Markov chain, variational inference, learnt approximate inference, annealed importance sampling, collaborative filtering and Parzen density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Difficulty of analysing generated samples</head><p>We have seen how GANs produce sharp generated images as opposed to VAEs. One may be tempted to say that GANs are better at generating images but that is a wide misconception. Usually, generative models are evaluated on the basis of how realistic the generated samples appear to be when compared with the data distribution; a visual inspection. However, it is possible for a very weak probabilistic model to generate very good samples.</p><p>One way to evaluate the models is to map the nearest neighbour of the generated sample to the data distribution by Euclidean distance in data space x. Then it would be known if the model is overfitting as the generated sample would appear simply copied from the sample in the training data. In this case, visual inspection would definitely fail as the generated sample can still look very sharp. In another case, the model can also underfit. For example, one can train a generative model on a dataset of trees and houses. However, the model while generation can create samples only showing trees. To the normal person, this is a successful generative model as it generates high quality samples of trees. However, it is required to know by the evaluator that the dataset that the model was trained on also contained images of houses which are not being generated at all. The model does not assign any probability to training images of houses. Realistically, when the model is trained on thousands and thousands of modes </p><formula xml:id="formula_83">(Location) 1 R 6 L 2 2 R 6 L 2 3 R 6 L 2 4 R 9 L 2 5 R 6 L 2 6 R 6 L 2 7 R 6 L 2 8 R 6 L 2 9 R 3 L 1 10 R 5 L 1 11 R 5 L 1 12 R 5 L 1 13 R 5 L 1 14 R 5 L 1 15 R 4 L 3 16 R 4 L 3 17 R 8 L 4 18 R 8 L 4 19 R 8 L 4 20 R 1 L 4 21 R 7 L 4 22 R 7 L 4 23 R 7 L 4 24 R 2 L 4</formula><p>(statistically speaking), it may ignore a few modes which would be very difficult to detect by human observation as one cannot remember so many images to detect missing variations in the generated samples. These reasons also partially explain why VAEs can achieve high data likelihood and be very close to the true posterior distribution and still generate blurry samples as opposed to GANs.</p><p>Even if we turn to the evaluation of the log-likelihood that the model assigns to test data we see that this method is not perfect either. Sometimes the log-likelihood may measure unimportant attributes and leave out the ones we want to be measured. Some models may achieve high log-likelihood due to the assignment of low variance to certain portions of the training data (say, background of the images) which will never change. We say it is a good thing to achieve high likelihood, but clearly, in this case, it is not.</p><p>Therefore there is a need in the field of generative models in machine learning to not just strive for better generated samples but also to devise new ways of unbaisedly evaluating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Future directions</head><p>Future directions of generative modelling may point to mixing up two or more generative models (HMM and LDA have been jointly used for stem cell research, topic modelling, and speech emotion recognition) <ref type="bibr" target="#b98">[98]</ref><ref type="bibr" target="#b99">[99]</ref><ref type="bibr" target="#b100">[100]</ref> for a problem to overcome the drawbacks faced when deploying models individually. More interesting uses of generative models are text-to-image generation <ref type="bibr" target="#b101">[101]</ref> using SGAN and stacking VAE and GAN for context aware text-to-image generation <ref type="bibr" target="#b102">[102]</ref>, with more instances of exploiting the VAE-GAN combination <ref type="bibr" target="#b103">[103,</ref><ref type="bibr" target="#b104">104]</ref>, which are more examples of combining two generative models.</p><p>More research could be done on combining unsupervised generative models with supervised models (semi-supervision) to fine-tune (like we saw in Section 4.3 in DBMs) generation process and make it more efficient. It is noticed that while energy based models like BMs have been used in the field of recommender systems, the newest addition GANs have not been employed except in a few cases <ref type="bibr" target="#b105">[105]</ref> and so applying these generative Training requires finding Nash equilibrium -harder problem than optimizing an objective function. Difficult to train due to instability and possible collapse. Difficult to evaluate empirically (easy subjectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 12</head><p>Encountered positive and negative aspects of each model while experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Pros Cons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMM</head><p>Strong Python 3 library support. Very powerful for complex shaped clusters and easily visualized.</p><p>We noticed mixed membership or intersection of different cluster regions which may not always be desirable.</p><p>LDA Over a corpora of text documents, we were able to detect n number of topics and the top m words related to each topic where m and n were decided by us. With LDAvis, some very useful insights were drawn.</p><p>Computation of LDAvis to generate the final report took extensive memory and hours of processing. So did LDA in generating the topics and the top 10 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMM</head><p>The HMM trained and gave an output prediction of the latent state sequence with only the input of a single observable sequence; the training prerequisite data (input sequence) was very simple.</p><p>Implementing HMM in our instance took memory of &gt;8 GB for a very simple model with not many observable and hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RBM</head><p>We provided a lengthy visible layer of 1682 nodes with only a single hidden layer responsible for detecting 100 features (having 100 nodes) with a test loss of 0.249 which is considered optimal for recommender systems.</p><p>The data had to be preprocessed into binary form for proper functioning of the RBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBN</head><p>Use of DBNs for classification had a huge impact on metrics like precision, recall and F1 score.</p><p>We went through a complex and computationally expensive pretraining process due to training the top level RBM for 20 epochs.</p><p>DBM Satisfactory sample quality on MNIST digit recognition dataset.</p><p>Similarly complex training procedure as with DBNs.</p><p>CVAE Satisfactory sample quality on the same data as used for DBM, the MNIST digit recognition dataset.</p><p>Library support for constructing CVAE was scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCGAN</head><p>We noticed that up to 13 epochs the DCGAN had started generating very realistic models and with some hypertuning it may have avoided a collapse.</p><p>Each epoch was computationally expensive and time-taking. Additionally, the GAN collapsed, the signs of which crystallized by the 15th epoch. models (not only GANs) to recommendation systems is another future direction that can be taken by researchers. However, perhaps the biggest development required in generative modelling is a better way of interpretation of the generated samples as discussed in Section 7.4. The problem with VAEs is that it may spread probability mass where it may not make sense whereas GANs may miss modes of the true distribution. There have been initial approaches <ref type="bibr" target="#b106">[106,</ref><ref type="bibr" target="#b107">107]</ref> that try to improve this and there has been work that shows that directly optimizing likelihood can also generate high quality samples <ref type="bibr" target="#b108">[108]</ref> which show the direction to future research in generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we provide a high level overview and analysis of all the generative models used in modern day applications by studying their ideology of operation, properties, advantages and disadvantages. We compare all the models juxtaposed with each other based on their inference, sampling, probability evaluation, design and looked at the fields they are employed in to bring out some of the major differences between them. In addition, we implemented each discussed model with details of the results and our findings. It is worthy to mention that all the methods described in this paper are fields of active research in the literature and every day we see these generative models put to newer applications. This paper also points out the flaws in the evaluation of generated samples and provides future directions to the field of generative models. We hope that this survey provides readers a comprehensive, high-level and exhaustive read on all the generative models that exist and give them a fundamental along with a practical understanding of them. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A directed graph model.</figDesc><graphic coords="2,318.75,183.88,216.96,96.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Classification of different generative models discussed in this paper with respect to ML and DL (Machine Learning and Deep Learning).</figDesc><graphic coords="3,109.54,55.32,386.21,137.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Mixture of three Gaussians where π signifies the weight associated to the Gaussian and hence also the probability of the data belonging to the ith cluster or Gaussian, µ specifies the position of the Gaussian with the mean, ρ signifies the 'spread' of the Gaussian over the overall distribution by the variance.</figDesc><graphic coords="3,66.26,226.05,203.72,91.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (Left): Data clustering done by GMMs for oblong data (oval in this case). If visualized in 3D, these datapoints may be coplanar and lying on the surfaces of a circular planes inclined at different angles. (Right): Data clustering done by K-means algorithm which is a special case of GMM clusters for circular clustering. It is clearly visible that GMMs outperform K-means clustering algorithms for a generalized dataset.</figDesc><graphic coords="4,135.95,55.32,313.77,142.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. P. A. Torres-Carrasquillo et al. The implementation of GMM tokenization system.</figDesc><graphic coords="5,145.94,55.32,313.28,181.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. A simplified HMM with no initial and final states for the sake of simplicity. Let there be a set of symbols defined by S = {S 1 , S 2 , S 3 , S 4 }. The two states that generate the Markov chain are labelled as I and II. State I generates sequences comprising S 1 and S 4 more frequently whereas state II generates sequences comprising S 2 and S 3 more frequently (each state's symbol emission probabilities are stated below the respective state). All the state-transitions are implemented through arrows with their corresponding probabilities. Finally, the probability of the observable symbol sequence is the product of state-transition and symbol emission probabilities.</figDesc><graphic coords="5,121.70,270.88,361.69,312.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Graphical plate notation for Latent Dirichlet Allocation. The grey shaded portion signifies an observable entity.</figDesc><graphic coords="6,37.64,55.32,241.28,269.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The Boltzmann machine where blue-grey nodes are hidden and maroon nodes are visible. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="6,38.60,367.53,239.36,201.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The restricted Boltzmann machine.</figDesc><graphic coords="6,327.06,55.32,200.35,77.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Hinton et al. (Left): An infinite logistic belief net having tied weights. The upward arrows are not part of the generative model as they are only used for sampling inference from the posterior distribution at every hidden layer when the data vector is given to v o . Whereas, the downward arrows represent the generative model. (Right): The hybrid model with undirected connections between the top two layers representing an RBM and directed top-down connections below representing the generative model whereas the bottom-up connections infer a factorial representation in the layer from the layer below it. In the greedy learning process initially the top-down and bottom-up weights are tied.</figDesc><graphic coords="8,172.13,55.32,241.28,219.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (Left): Deep belief network with 3 hidden layers and two undirected roof layers. (Right): Deep Boltzmann machine with no directed connections.</figDesc><graphic coords="8,172.13,342.89,241.28,177.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (Left): Pretraining of DBM involves individual training of stacked RBMs with the lowest and topmost layers doubled and the intermediate weights doubled. (Right): The final structure done through composition of the weights obtained through modified pretraining process as described on the left.</figDesc><graphic coords="9,181.94,55.32,241.28,186.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (Left): An autoencoder network. (Right): Training process of the autoencoder with an L2 loss function ∥x -x∥ 2 .</figDesc><graphic coords="10,172.13,55.32,241.28,178.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. (Left): The inference network represented by q ϕ (z|x) which outputs mean and diagonal covariance vectors of z|x from which we sample z|x. (Right): The generator network (or decoder) represented by p θ (x|z) which also output the mean and diagonal covariance vectors of x|z from which the distribution x|z can be sampled.</figDesc><graphic coords="11,91.40,55.32,422.37,135.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The Variational Autoencoder (VAE) when all the parts are pieced together. From the bottom-up, it comprises of the inference network and the generator network which gives us the output X.</figDesc><graphic coords="11,59.57,241.15,216.96,334.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>6 . 1</head><label>61</label><figDesc>Deep Convolutional GANs (DCGANs), 6.2 Fully Connected and Convolutional GANs (FCC-GANs), 6.3 Conditional GANs (CGANs), 6.4 Stack GANs (SGAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>D θ D tends to maximize objective by increasing D θ D (x) ≈ 1 and decreasing D θ D (G θ G (z)) ≈ 0, while on the other hand, G θ G tends to minimize objective by increasing D θ D (G θ G (z)) ≈ 1. In other words, D attempts to discriminate more properly between real and fake data, and conversely, G attempts to make D output higher values close to 1 for generated data in order to fool D. This minimax game terminates at a ''saddle point'' (the Nash equilibrium) where f (D, G) is minimum with respect to G's strategy and maximum with respect to D's strategy. The Nash equilibrium is a solution concept in game theory which involves two players competing each other (non-cooperative) where each player knows the equilibrium strategies of other players, with the given constraint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Generative adversarial network. The working procedure is as follows: Step 0: The discriminator D (a multilayer perceptron (MLP)) is trained first by feeding it real images. Step 1: The error is backpropagated through D and its weights are adjusted. Step 2: D is trained further by feeding it some random primitive stage generations by generator G (note that G takes noise input and is also an MLP). Step 3: D outputs values close to 0 because of pretraining of real images. Step 4:The output of D is subtracted by 1 (to train G appropriately) and backpropagated through G and its weights are adjusted. Further, G generates images with noise input again with readjusted weights, resulting in more realistic looking images. These images are fed to D along with real images and we get outputs which are backpropagated through D so it can better discriminate next time. The outputs for fake images are also backpropagated through G for the betterment of quality of generated images. These steps are repeated for many epochs until D cannot discriminate between generated and real images.</figDesc><graphic coords="12,111.80,55.32,362.10,134.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Radford et al. The generator of DCGAN with four sequential fractionally strided convolutional layers.</figDesc><graphic coords="13,122.54,55.32,360.00,143.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Collapse of a DCGAN to a single point, particularly visible after 12 epochs with certain parameters on the 32 × 32 CIFAR-10 [64] dataset.</figDesc><graphic coords="13,86.63,232.66,432.00,148.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. (Left): Rectified Linear Unit (reLU) activation function on the left and leaky reLU on the right. (Right): DCGAN generated samples after 11 epochs on 32 × 32 CIFAR-10 dataset on the left and real samples on the right.</figDesc><graphic coords="13,79.16,415.40,446.93,95.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 20 . 1 Fig. 22 .</head><label>20122</label><figDesc>Fig. 20. Barua et al. Image generation performance on 32 ×32 CIFAR-10 image dataset, compared between CNN, FCC-GAN-S and FCC-GAN-P after 1 epoch (a-c), 35 epochs (d-f), and 150 epochs (g-i).</figDesc><graphic coords="14,38.27,55.32,240.08,259.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Barua et al. (a) The FCC-GAN discriminator having multiple deep fully connected layers mapping high dimensional features extracted by convolutional layers to a lower dimensional space before max pooling with unit stride convolution, and (b) the FCC-GAN generator having multiple deep fully connected layers before convolutional layers to convert low dimensional noise distribution to a high dimensional representation of features of an image.</figDesc><graphic coords="14,172.13,467.82,241.28,239.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Huang et al. Stack GAN model. (a) The training procedure where each generator G i is trained individually conditioned on the input given by the encoder and then jointly when receiving input from other upper generators. (b) While testing, x is the final generated data which is a result of stacked top-down generators sequentially generating features one-by-one.</figDesc><graphic coords="16,99.77,55.32,386.01,366.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 24 . 3 Comparisons, 7 . 4</head><label>24374</label><figDesc>Fig. 24. Huang et al. Left: (a) SGAN samples conditioned on class labels, (b) ground truth (real) images from MNIST dataset. Right (a) SGAN samples on conditioned on class labels, (b) ground truth (real) images from SVHN dataset.</figDesc><graphic coords="16,69.68,475.81,446.21,119.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>7. 1 .</head><label>1</label><figDesc>Analysis of deep generative modelsDeep generative models can be classified as whether they perform explicit density estimation of P data (x) or implicit. Explicit density estimation simply means defining and solving for P model (x), whereas implicit density estimation refers to learning a model that can sample from P model (x) without explicitly defining it. Fig.25describes the flowchart of deep generative models based on the tractabilities of their density distributions where NADE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 25 .</head><label>25</label><figDesc>Fig. 25. Adapted from Goodfellow et al. [82], the taxonomy of generative models based on the tractability of their density distribution.</figDesc><graphic coords="17,181.94,55.32,241.28,236.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>which are the parameters of NADE. As the hidden layer matrix W and bias z are shared among all hidden layers h m , NADE features parameters of the order O(HM) as opposed to O(HM 2 ) as in the case of individual neural networks. Training of NADE can be done through minimization of the average negative log-likelihood or by maximum likelihood through SGD (stochastic gradient descent),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 26 .</head><label>26</label><figDesc>Fig. 26. Autoregressive LSTM-based image modelling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 27 .</head><label>27</label><figDesc>Fig. 27. Reconstruction RMSE loss encountered per epoch while training the RBM.</figDesc><graphic coords="18,306.75,111.54,241.04,138.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Fig. 28 .</head><label>28</label><figDesc>Fig. 28. RBM pre-training reconstruction error at each epoch.</figDesc><graphic coords="19,317.04,55.32,240.08,137.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Fig. 29 .</head><label>29</label><figDesc>Fig. 29. Metrics precision, recall and F1 score for each of the digit classes as classified by a logistic regressor on DBN extracted features.</figDesc><graphic coords="19,316.44,226.24,241.28,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Fig. 30 .</head><label>30</label><figDesc>Fig. 30. Metrics precision, recall and F1 score for each of the digit classes as classified by a logistic regressor on raw pixel-value data without feature selection. Clearly, these values are diminished due to absence of feature selection.</figDesc><graphic coords="20,37.64,55.32,241.28,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. 31 .</head><label>31</label><figDesc>Fig. 31. Regenerated samples of MNIST digit recognition dataset by DBM-based feature selection and plotting.</figDesc><graphic coords="20,37.64,244.10,241.28,183.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Fig. 32 .</head><label>32</label><figDesc>Fig. 32. CVAE generated image of the MNIST digit recognition dataset.</figDesc><graphic coords="20,307.11,55.32,240.32,241.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Fig. 33 .</head><label>33</label><figDesc>Fig. 33. (Top): Output of the DCGAN when initialized from epochs 1 till 3. (Bottom): Output of the DCGAN from epochs 13 till 15 where a collapse can be seen in epochs 14 and 15.</figDesc><graphic coords="21,91.46,55.32,422.37,313.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Fig. 34 .</head><label>34</label><figDesc>Fig. 34. Real samples of the CIFAR-10 dataset that the DCGAN tries to model.</figDesc><graphic coords="21,48.17,423.91,239.84,239.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>36 (Top) (a) and Fig. 36 (Bottom) (a) are quite similar because of a somewhat circular nature of the data distribution. However, the true advantage of GMMs over K-means is realized by Fig. 36 (Top) (b) and Fig. 36 (Bottom) (b) as we notice the clusters generated by GMM is more precise due to its flexible clustering capability as discussed in Section 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Fig. 35 .</head><label>35</label><figDesc>Fig. 35. (Left): Randomly generated quad-centric data distribution. (Right): Randomly generated quad-centric stretched (oblong) data distribution.</figDesc><graphic coords="23,91.46,55.32,422.37,150.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Fig. 36 .</head><label>36</label><figDesc>Fig. 36. (Top): (a) GMM applied for clustering on randomly generated quad-centric data distribution. (b) GMM applied for clustering on randomly generated quadcentric stretched (oblong) data distribution. (Bottom): (a) K-means used on the same distribution for clustering as displayed by Fig. 36 (Top) (a). (b) K-means applied on the same distribution for clustering as displayed by Fig. 36 (Top) (b).</figDesc><graphic coords="23,94.10,239.44,417.07,318.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Fig. 37 .</head><label>37</label><figDesc>Fig. 37. (Left): Generated wordcloud. (Right): 10 most common words.</figDesc><graphic coords="24,81.65,57.26,422.37,156.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Fig. 38 .Fig. 39 .</head><label>3839</label><figDesc>Fig. 38. Inter-topic distance map through multidimensional scaling along with the top 30 salient words (light blue bar) and their relevance in selected topic 1 (red bar) with λ = 1. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="24,75.56,247.73,434.41,274.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a DNN that estimates the posterior distribution z i conditioned on ĥi which estimates only the posterior mean</figDesc><table><row><cell cols="4">(which is treated to be a diagonal Gaussian distribution) which</cell></row><row><cell cols="4">implies that ω ent G i error. G i and Q i are updated in each iteration to minimize ω ent becomes similar to a Euclidean reconstruction . G i One thing to note is that G i of the SGAN generate distributions</cell></row><row><cell cols="4">conditioned on class labels y as,</cell></row><row><cell>P G</cell><cell>(</cell><cell>x|y</cell><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Configuration settings for training the RBM.</figDesc><table><row><cell># visible nodes</cell><cell># hidden nodes</cell><cell># epochs</cell><cell>Batch size</cell></row><row><cell>1682</cell><cell>100</cell><cell>10</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>DBN configuration settings.</figDesc><table><row><cell>Hidden layer</cell><cell>Batch size</cell><cell>Learning rate</cell><cell># epochs</cell><cell>Activation</cell></row><row><cell>structure</cell><cell></cell><cell>(RBM)</cell><cell></cell><cell>function</cell></row><row><cell>{256, 512}</cell><cell>10</cell><cell>0.06</cell><cell>20</cell><cell>Sigmoid</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Configuration settings for training of DBM in MNIST digit recognition dataset.</figDesc><table><row><cell>Dimensions</cell><cell># epochs</cell><cell>Learning rate</cell><cell>Batch size</cell></row><row><cell>{784, 500, 784}</cell><cell>2</cell><cell>0.01</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Configuration settings for training of CVAE on MNIST digit recognition dataset.</figDesc><table><row><cell>Latent dimensions</cell><cell># epochs</cell><cell>Learning rate</cell><cell>Batch size</cell><cell>Conv filters</cell></row><row><cell>2</cell><cell>10</cell><cell>0.001</cell><cell>32</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Configuration settings for training of DCGAN.</figDesc><table><row><cell>Input dimensions</cell><cell>Batch size</cell><cell>Learning rate (D, G)</cell><cell># epochs</cell></row><row><cell>64 × 64 × 3</cell><cell>64</cell><cell>0.0002</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Top 10 most prominent words generated by LDA over 5 topics.</figDesc><table><row><cell>Topics</cell><cell>Prominent words (top 10)</cell></row><row><cell>1</cell><cell>Model, network, image, neural, figure, input, time, using,</cell></row><row><cell></cell><cell>images, neurons</cell></row><row><cell>2</cell><cell>State, learning, policy, time, function, value, action, algorithm,</cell></row><row><cell></cell><cell>optimal, reward</cell></row><row><cell>3</cell><cell>Data, model, models, distribution, using, 10, set, algorithm,</cell></row><row><cell></cell><cell>Gaussian, number</cell></row><row><cell>4</cell><cell>Algorithm, matrix, problem, function, theorem, set, 10, let, log,</cell></row><row><cell></cell><cell>learning</cell></row><row><cell>5</cell><cell>Learning, training, data, set, networks, neural, network,</cell></row><row><cell></cell><cell>classification, using, model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Hidden state transition matrix.</figDesc><table><row><cell></cell><cell>L 1</cell><cell>L 2</cell><cell>L 3</cell><cell>L 4</cell></row><row><cell>L 1</cell><cell>0.90</cell><cell>0.08</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>L 2</cell><cell>0.01</cell><cell>0.90</cell><cell>0.05</cell><cell>0.04</cell></row><row><cell>L 3</cell><cell>0.03</cell><cell>0.02</cell><cell>0.85</cell><cell>0.10</cell></row><row><cell>L 4</cell><cell>0.05</cell><cell>0.02</cell><cell>0.23</cell><cell>0.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>State emission probability matrix.</figDesc><table><row><cell></cell><cell>R 1</cell><cell>R 2</cell><cell>R 3</cell><cell>R 4</cell><cell>R 5</cell><cell>R 6</cell><cell>R 7</cell><cell>R 8</cell><cell>R 9</cell></row><row><cell>L 1</cell><cell>0.01</cell><cell>0.01</cell><cell>0.20</cell><cell>0.01</cell><cell>0.30</cell><cell>0.05</cell><cell>0.01</cell><cell>0.40</cell><cell>0.01</cell></row><row><cell>L 2</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.10</cell><cell>0.01</cell><cell>0.30</cell><cell>0.05</cell><cell>0.01</cell><cell>0.50</cell></row><row><cell>L 3</cell><cell>0.30</cell><cell>0.20</cell><cell>0.01</cell><cell>0.10</cell><cell>0.01</cell><cell>0.30</cell><cell>0.05</cell><cell>0.02</cell><cell>0.01</cell></row><row><cell>L 4</cell><cell>0.03</cell><cell>0.01</cell><cell>0.01</cell><cell>0.19</cell><cell>0.01</cell><cell>0.39</cell><cell>0.39</cell><cell>0.01</cell><cell>0.03</cell></row></table><note><p>particular time t. We further assume that particle x emits certain (nine) types of radiations defined by the set R = {R 1 , R 2 , R 3 , . . . , R 9 } corresponding to the location it resides in at any time t. We define set R to be the set of the observables and L to be the set</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Initial state probability distribution matrix.</figDesc><table><row><cell>L 1</cell><cell>L 2</cell><cell>L 3</cell><cell>L 3</cell></row><row><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell cols="4">of hidden states in the HMM. At every time update, the particle transitions from one hidden state to the other, L (t)</cell></row></table><note><p>i → L (t+1) j</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc>Results of the trained HMM on input sequence of emissions (radiations, in our case) based on the predictions of the hidden state (locations).</figDesc><table><row><cell>Time (t)</cell><cell>Input observable emission</cell><cell>Output state</cell></row><row><cell></cell><cell>sequence (Radiation)</cell><cell>prediction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11</head><label>11</label><figDesc>Application-oriented comparison of all generative models. Mixed membership not always the best choice. Fails to perform in higher dimensional data (e.g &gt; 6-D).</figDesc><table><row><cell>Model</cell><cell>Density estimation</cell><cell>Advantages</cell><cell>Disadvantages</cell></row><row><cell>GMM</cell><cell></cell><cell>Flexible cluster covariance. Mixed membership of data</cell><cell></cell></row><row><cell></cell><cell></cell><cell>points.</cell><cell></cell></row><row><cell>HMM</cell><cell>Explicit</cell><cell>Strong statistical foundation, simple concept, very flexible.</cell><cell>Large number of unknown parameters in bigger sequencing problems. Fails to understand correlations in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sequence.</cell></row><row><cell>LDA</cell><cell></cell><cell>Extensible and applicable on new data points.</cell><cell>Requires prior knowledge of number of topics. Not</cell></row><row><cell></cell><cell></cell><cell>Parameters increase linearly with number of topics</cell><cell>suitable for short texts. Fails to capture correlation</cell></row><row><cell></cell><cell></cell><cell>-not documents (as in pLSI).</cell><cell>among topics.</cell></row><row><cell>RBM</cell><cell></cell><cell>Expressive at encoding higher-order correlations due to</cell><cell>Requires Markov chains -computationally expensive</cell></row><row><cell></cell><cell></cell><cell>hidden layers. Allows for feature extraction to train</cell><cell>and uncertain convergence time. Partition function</cell></row><row><cell></cell><cell></cell><cell>other models on top of.</cell><cell>estimation is not easy.</cell></row><row><cell>DBN</cell><cell></cell><cell>Powerful feature extractor for pattern recognition, can</cell><cell>Complex training procedure. Requires Markov chains.</cell></row><row><cell></cell><cell></cell><cell>be fine-tuned with a small labelled dataset.</cell><cell></cell></row><row><cell>DBM</cell><cell></cell><cell>Efficient feature extraction. Higher performance gain by</cell><cell>Requires Markov chains, mean field approximations.</cell></row><row><cell></cell><cell></cell><cell>adding layers.</cell><cell>Complex training procedure.</cell></row><row><cell>VAE</cell><cell></cell><cell>Achieves high data likelihood; precise control over</cell><cell>Generated samples are not sharp; blurry. Limited</cell></row><row><cell></cell><cell></cell><cell>latent representations. Objective is measurable (lower</cell><cell>approximation to true posterior.</cell></row><row><cell></cell><cell></cell><cell>bound on goodness of model)</cell><cell></cell></row><row><cell>(DC, FCC, C,</cell><cell>Implicit</cell><cell>Parallel sample generation; fast. High sample quality. No</cell><cell></cell></row><row><cell>S) GANs</cell><cell></cell><cell>Markov chains needed.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13</head><label>13</label><figDesc>Comparison of all generative models discussed in this paper.</figDesc><table><row><cell>Model</cell><cell cols="2">Inference</cell><cell></cell><cell>Sampling</cell><cell>P(x) evaluation</cell><cell>Design</cell><cell>Deployed in fields</cell></row><row><cell></cell><cell>MCI</cell><cell>VI</cell><cell>LAI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GMM</cell><cell>✖</cell><cell>✖</cell><cell>✖</cell><cell>No difficulties</cell><cell>Tractable</cell><cell>Simple; mixture of</cell><cell>Clustering, acoustics analysis, etc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>probability distributions</cell><cell></cell></row><row><cell>HMM</cell><cell>✖</cell><cell>✖</cell><cell>✖</cell><cell>No difficulties</cell><cell>Tractable</cell><cell>Involves hidden states;</cell><cell>DNA sequence analysis, pattern recognition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>level-wise simple</cell><cell>in sound/speech, etc.</cell></row><row><cell>LDA</cell><cell>✖</cell><cell>✖</cell><cell>✖</cell><cell>No difficulties</cell><cell>Tractable</cell><cell>Many variables in play; to be</cell><cell>Dimensionality reduction, topic modelling,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>estimated</cell><cell>etc.</cell></row><row><cell>RBM</cell><cell>✖</cell><cell>✔</cell><cell>✖</cell><cell>Requires Markov</cell><cell>May be approximated via</cell><cell>Complex, designed carefully</cell><cell>Dimensionality reduction,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chain</cell><cell>AIS; intractable</cell><cell>to ensure various parameters</cell><cell>regression/classification, CF, topic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>modelling, etc.</cell></row><row><cell>DBN</cell><cell>✖</cell><cell>✔</cell><cell>✖</cell><cell>Requires Markov</cell><cell>May be approximated via</cell><cell>Complex, designed carefully</cell><cell>Classification, speech recognition,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chain</cell><cell>AIS; intractable</cell><cell>to ensure various parameters</cell><cell>information retrieval, drug discovery, etc.</cell></row><row><cell>DBM</cell><cell>✖</cell><cell>✔</cell><cell>✖</cell><cell>Requires Markov</cell><cell>May be approximated via</cell><cell>Complex, designed carefully</cell><cell>Pattern recognition, information retrieval,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chain</cell><cell>AIS; intractable</cell><cell>to ensure various parameters</cell><cell>regression/classification, etc.</cell></row><row><cell>VAE</cell><cell>✔</cell><cell>✖</cell><cell>✖</cell><cell>Requires Markov</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>Image forecasting, CF, modelling acoustic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chain</cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell>features and molecule design, etc.</cell></row><row><cell>GAN</cell><cell>✖</cell><cell>✖</cell><cell>✔</cell><cell>No difficulties</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>Drug discovery, anomaly detection, image</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell>analysis and transformations, etc.</cell></row><row><cell>DC-</cell><cell>✖</cell><cell>✖</cell><cell>✔</cell><cell>No difficulties</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>Image generation</cell></row><row><cell>GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell></cell></row><row><cell>FCC-</cell><cell>✖</cell><cell>✖</cell><cell>✔</cell><cell>No difficulties</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>High quality image generation, etc.</cell></row><row><cell>GAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell></cell></row><row><cell>CGAN</cell><cell>✖</cell><cell>✖</cell><cell>✔</cell><cell>No difficulties</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>Face generation and ageing simulation,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell>image feature editing, speech enhancement,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>language identification, etc.</cell></row><row><cell>SGAN</cell><cell>✖</cell><cell>✖</cell><cell>✔</cell><cell>No difficulties</cell><cell>Not represented clearly,</cell><cell>Any differentiable function</cell><cell>Text-to-image generation, image analysis</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>may be estimated via PDE</cell><cell>permitted</cell><cell>and transformations, etc.</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.233</idno>
		<ptr target="http://dx.doi.org/10.1109/cvpr.2014.233" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/jstars.2015.2388577</idno>
		<ptr target="http://dx.doi.org/10.1109/jstars.2015.2388577" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mitchison</surname></persName>
		</author>
		<title level="m">Biological Sequence Analysis</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
		<title level="m">Bioinformatics: A Machine Learning Approach</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation for tag recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
		<idno type="DOI">10.1145/1639714.1639726</idno>
		<ptr target="http://dx.doi.org/10.1145/1639714.1639726" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Conference on Recommender Systems -RecSys &apos;09</title>
		<meeting>the Third ACM Conference on Recommender Systems -RecSys &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Ssd: Single shot multibox detector</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask. r cnn, Mask r-cnn</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prediction of extreme rainfall event using weather pattern recognition and support vector machine classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Appl. Climatol</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="583" to="603" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An early warning system for loan risk assessment using artificial neural networks</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="303" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The global k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0031-3203(02)00060-2</idno>
		<ptr target="http://dx.doi.org/10.1016/s0031-3203" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="62" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximating the kullback leibler divergence between Gaussian mixture models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2007.366913</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2007.366913" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subspace Gaussian mixture models for speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Akyazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2010.5495662</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2010.5495662" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic accent identification using Gaussian mixture models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/asru.2001.1034657</idno>
		<ptr target="http://dx.doi.org/10.1109/asru.2001.1034657" />
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding. ASRU &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Links between probabilistic automata and hidden Markov models: probability distributions, learning models and induction algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Esposito</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2004.03.020</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2004.03.020" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1349" to="1371" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating hidden Markov model parameters so as to maximize speech recognition accuracy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<idno type="DOI">10.1109/89.221369</idno>
		<ptr target="http://dx.doi.org/10.1109/89.221369" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On large-vocabulary speaker-independent continuous speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-6393(88)90053-2</idno>
		<ptr target="http://dx.doi.org/10.1016/0167-6393" />
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="90053" to="90055" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Methods for Speech Recognition</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hidden markov model based optical character recognition in the presence of deterministic transformations</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Agazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="DOI">10.1016/0031-3203(93)90178-y</idno>
		<ptr target="http://dx.doi.org/10.1016/0031-3203(93)90178-y" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1813" to="1826" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.1162/jmlr.2003.3.4-5.993</idno>
		<ptr target="http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation in web spam filtering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bíró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
		<idno type="DOI">10.1145/1451983.1451991</idno>
		<ptr target="http://dx.doi.org/10.1145/1451983.1451991" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Adversarial Information Retrieval on the Web -AIRWeb &apos;08</title>
		<meeting>the 4th International Workshop on Adversarial Information Retrieval on the Web -AIRWeb &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bug localization using latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lukins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Etzkorn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.infsof.2010.04.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.infsof.2010.04.002" />
	</analytic>
	<monogr>
		<title level="j">Inf. Softw. Technol</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="972" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic annotation of satellite images using latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lienou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<idno type="DOI">10.1109/lgrs.2009.2023536</idno>
		<ptr target="http://dx.doi.org/10.1109/lgrs.2009.2023536" />
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="32" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<ptr target="http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf" />
		<title level="m">Notes on contrastive divergence</title>
		<imprint>
			<date type="published" when="2006-03">2006. March 2020</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf" />
		<imprint>
			<date type="published" when="2006-03">2006. March 2020</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273596</idno>
		<ptr target="http://dx.doi.org/10.1145/1273496.1273596" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning -ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning -ICML &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<title level="m">A non-iid framework for collaborative filtering with restricted boltzmann machines, ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1148" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rate-coded restricted Boltzmann machines for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="908" to="914" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a better representation of speech soundwaves using restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2011.5947700</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2011.5947700" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Joint spectral distribution modeling using restricted boltzmann machines for voice conversion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dai</surname></persName>
		</author>
		<idno>INTERSPEECH-2013</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3052" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phone recognition using restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2010.5495651</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2010.5495651" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentiment-aspect extraction based on restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Connectionist learning of belief networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702(92)90065-6</idno>
		<ptr target="http://dx.doi.org/10.1016/0004-3702(92)90065-6" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="113" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Neural Information Processing Systems (NIPS&apos;07)</title>
		<meeting>the 20th International Conference on Neural Information Processing Systems (NIPS&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Breast cancer classification using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Abdel-Zaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Eldeib</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2015.10.015</idno>
		<ptr target="http://dx.doi.org/10.1016/j.eswa.2015.10.015" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Time series forecasting using a deep belief network with restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kuremoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Obayashi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2013.03.047</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2013.03.047" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep belief networks based voice activity detection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tasl.2012.2229986</idno>
		<ptr target="http://dx.doi.org/10.1109/tasl.2012.2229986" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="697" to="710" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Neural Information Processing Systems (NIPS&apos;09)</title>
		<meeting>the 22nd International Conference on Neural Information Processing Systems (NIPS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimal perceptual inference</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient learning of deep boltzmann machines AISTATS</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page" from="693" to="700" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling documents with a deep Boltzmann machine</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI&apos;13)</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI&apos;13)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2949" to="2980" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Resource configurable spoken query detection using deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2012.6289082</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2012.6289082" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A 3D model recognition mechanism based on deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2014.06.084</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2014.06.084" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="593" to="602" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298920</idno>
		<ptr target="http://dx.doi.org/10.1109/cvpr.2015.7298920" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond principal components: Deep Boltzmann machines for face modeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299111</idno>
		<ptr target="http://dx.doi.org/10.1109/cvpr.2015.7299111" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Autorec</forename></persName>
		</author>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web -WWW &apos;15</title>
		<meeting>the 24th International Conference on World Wide Web -WWW &apos;15</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<idno type="DOI">10.1145/2740908.2742726</idno>
		<ptr target="http://dx.doi.org/10.1145/2740908.2742726" />
	</analytic>
	<monogr>
		<title level="j">Companion</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hashing with binary autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raziperchikolaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML&apos;17)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auto-encoding variational Bayes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An Uncertain Future: Forecasting from Static Images using Variational Autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_51</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46478-7_51" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="835" to="851" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186150</idno>
		<ptr target="http://dx.doi.org/10.1145/3178876.3186150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web -WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web -WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hierarchical variational autoencoders for music</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning for Creativity and Design</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
		<title level="m">Modeling and Transforming Speech using Variational Autoencoders, Interspeech; ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1770" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Variational autoencoders for learning latent representations of speech emotion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<idno>ArXiv, abs/1712.08708</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="7795" to="7804" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generative.adversarial. nets, Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS&apos;14)</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems (NIPS&apos;14)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML&apos;10)</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning (ICML&apos;10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A method for improving CNNbased image recognition using DCGAN CMC</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput, Mater. Continua</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Automatic sketch colorization using DCGAN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Control, Automation and Systems (ICCAS)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1316" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gesture recognition based on CNN and DCGAN for calculation and text output</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="28230" to="28237" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generating pedestrian training dataset using DCGAN</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piccialli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 3rd International Conference on Advances in Image Processing</title>
		<meeting>the 2019 3rd International Conference on Advances in Image Processing</meeting>
		<imprint>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Infrared image colorization based on a triplet dcgan architecture</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Vintimilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">FCC-gan: A fully connected and convolutional net architecture for GANs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02417</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Face aging with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2089" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcsvt.2019.2920407</idno>
		<ptr target="http://dx.doi.org/10.1109/tcsvt.2019.2920407" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01703</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
		<title level="m">Conditional Generative Adversarial Nets Classifier for Spoken Language Identification</title>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2814" to="2818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Generative adversarial networks NIPS tutorial</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Neural autoregressive distribution estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7184" to="7220" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Does the wake-sleep algorithm produce good density estimators?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="661" to="667" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<title level="m">Graphical Models for Machine Learning and Digital Communication</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<title level="m">Deep generative stochastic networks trainable by backprop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G M</forename><surname>Generativemodels</surname></persName>
		</author>
		<ptr target="https://github.com/GM-git-dotcom/GenerativeModels" />
		<title level="m">Github repository</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst. (TiiS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Irvine, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>UCI Machine Learning Repository, University of California, School of Information and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<title level="m">MNIST Handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on social media analytics</title>
		<meeting>the first workshop on social media analytics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international working conference on advanced visual interfaces</title>
		<meeting>the international working conference on advanced visual interfaces</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">LDAvis: A method for visualizing and interpreting topics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on interactive language learning, visualization, and interfaces</title>
		<meeting>the workshop on interactive language learning, visualization, and interfaces</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Topic evolution based on LDA and HMM and its application in stem cell research</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165551514540565</idno>
		<ptr target="http://dx.doi.org/10.1177/0165551514540565" />
	</analytic>
	<monogr>
		<title level="j">J. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="611" to="620" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">topic language model adaptation using HMM-LDA</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Style</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A novel LDA and HMM-based technique for emotion recognition from facial expressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-37081-6_3</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-37081-6_3" />
	</analytic>
	<monogr>
		<title level="m">Multimodal Pattern Recognition of Social Signals in Human-Computer-Interaction</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><surname>Stackgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Stacking VAE and GAN for context-aware text-to-image generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1109/bigmm.2018.8499439</idno>
		<ptr target="http://dx.doi.org/10.1109/bigmm.2018.8499439" />
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on Multimedia Big Data (BigMM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">GAN for recommendation system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Prosvetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Conf. Ser. 1405</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12005</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">F-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
