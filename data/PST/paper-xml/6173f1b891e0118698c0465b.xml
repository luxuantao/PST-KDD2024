<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conversational Group Detection with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sydney</forename><surname>Thompson</surname></persName>
							<email>sydney.thompson@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Abhijit</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anjali</forename><forename type="middle">W</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Austin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marynel</forename><surname>VÃ¡zquez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhijit</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anjali</forename><forename type="middle">W</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conversational Group Detection with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3462244.3479963</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>F-formation</term>
					<term>clustering</term>
					<term>graph neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Scene frames b) Interaction graph c) GNN + Dominant Sets Features GNN Pairwise a nities A nity matrix Figure 1: Proposed approach for conversational group detection ((a) includes an example frame from MatchNMingle [5]).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conversational group detection has a wide range of applications, including video surveillance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>, displays and exhibits <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, co-located collaboration <ref type="bibr" target="#b20">[21]</ref>, and interactive playgrounds <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. Group detection can also enable better spoken language interaction with situated agents <ref type="bibr" target="#b3">[4]</ref>, non-verbal robot behavior generation <ref type="bibr" target="#b33">[34]</ref>, and socially aware robot navigation in human environments <ref type="bibr" target="#b25">[26]</ref>.</p><p>Similar to prior work, we approach the problem of conversational group detection by reasoning about human proxemics <ref type="bibr" target="#b11">[12]</ref> and conversational formations. During free-standing conversations, people tend to form certain spatial patterns with each other, known as Face Formations or F-Formations in short <ref type="bibr" target="#b17">[18]</ref>. F-Formations are varied, adapting to factors such as density and physical environmental constraints. They characterize conversational groups.</p><p>We describe a social scene as an interaction graph and explore using a Graph Neural Network (GNN) <ref type="bibr" target="#b1">[2]</ref> for conversational group detection. Inspired by Swofford and colleagues <ref type="bibr" target="#b29">[30]</ref>, we use the GNN to predict pairwise affinities for the graph, which encode the likelihood that two people are part of an F-Formation. Then, we use the affinities to cluster people into conversational groups, as shown in Figure <ref type="figure">1</ref>. While Swofford and colleagues <ref type="bibr" target="#b29">[30]</ref> used a Deep Set <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> architecture to aggregate context from graph nodes when predicting an affinity value, this work advocates in favor of a more general message-passing architecture for reasoning about information in both the nodes and edges. This allows us to reduce feature engineering and more explicitly leverage relational features.</p><p>In summary, our main contributions are threefold. First, we propose a novel approach for group detection which relies on a GNN. Second, we conduct experiments on two datasets with varied input features such as position, orientation, and top-down images of participants to demonstrate the efficacy of the proposed model. Third, we open-source our code to facilitate future reproducibility. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of conversational group detection has traditionally been approached by hand-crafted heuristics and mathematical models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. However, advancements in machine learning have enabled improved social awareness with greater generalization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. In particular, the approach by Swofford et al. <ref type="bibr" target="#b29">[30]</ref>, called DANTE, outperformed several traditional approaches. DANTE receives spatial features for people in a scene and constructs a fullyconnected interaction graph, using the input data as node features. It then computes pairwise affinities by combining the dyad node features with context aggregated using a Deep Set architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>. These affinities are used to partition the graph with the Dominant Sets algorithm <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. Because DANTE computes context in tandem with a dyad, it relies heavily on hand-crafted feature transformations to preserve rotation and translation invariance.</p><p>While DANTE <ref type="bibr" target="#b29">[30]</ref> mainly reasons about information encoded in the nodes of a graph, we propose to use a more general messagepassing GNN architecture <ref type="bibr" target="#b1">[2]</ref> for affinity prediction. The GNN consists of a collection of update and aggregate functions that allow for node and edge information consolidation in a graph. This GNN architecture is a superset of Deep Sets, as discussed in <ref type="bibr" target="#b1">[2]</ref>.</p><p>While GNNs have previously been used for node clustering <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>, our problem differs in several key ways. Methods such as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> require an input affinity matrix, while our GNN must calculate the affinities itself. Also, several prior models for clustering with deep learning require information about the number of clusters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref>; however, we do not know the number of conversational groups in a scene in advance. Lastly, many models (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>) for social interaction analysis are designed and evaluated on large graphs (see <ref type="bibr" target="#b35">[36]</ref> for five such datasets with an average number of nodes ranging from 13 to 500). In our case, we make predictions over smaller graphs (2-16 nodes) as there is a physical limit to how many people can interact simultaneously in a given place <ref type="bibr" target="#b17">[18]</ref>.</p><p>Three reasons motivate us to predict an affinity matrix with a message-passing GNN. First, the values in an affinity matrix can be thought of as unidimensional edge features and, by design, GNNs are well suited to predict this type of data. Second, a single GNNs can work on graphs with numbers of nodes, which is important when reasoning about varied environments. Third, strategic choices about what features are encoded in the nodes and edges of a graph can make GNNs invariant to spatial rotations and translations. This reduces the amount of pre-processing transformations needed to analyze a scene in comparison to DANTE.</p><p>Prior benchmarks in conversational group detection from still images <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> commonly consider datasets with a limited number of people, e.g., the Cocktail Party dataset <ref type="bibr" target="#b40">[41]</ref> considers six people. Given the relative simplicity of these datasets, we study group detection performance using the recent MatchNMingle dataset <ref type="bibr" target="#b4">[5]</ref> made available by the Delft University of Technology. MatchNMingle is a multi-sensor dataset of in-the-wild conversations for the analysis of social interactions. It contains 4446 images of a scene with up to 15 people per frame, as shown in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>This paper studies conversational group detection: partitioning a set of people in a scene into non-overlapping clusters representing interacting groups. Formally, assume that there are ğ‘› people in the scene and let ğ‘ƒ be a set of individual feature vectors, ğ‘ƒ = {ğ’‡ ğ‘˜ | 1 â‰¤ ğ‘˜ â‰¤ ğ‘›}. Then, the groups can be expressed via a clustering schema ğ¶ : ğ‘ƒ â†’ {1, . . . , ğ‘› ğ‘ }, with ğ‘› ğ‘ the number of clusters.</p><p>Given a dataset D of ğ‘ examples, D = {(ğ‘ƒ 1 , ğ¶ 1 ), . . . , (ğ‘ƒ ğ‘ , ğ¶ ğ‘ )}, we frame the group detection problem from a supervised learning perspective as computing a function â„(ğ‘ƒ ğ‘– ) = Äˆğ‘– that estimates cluster assignments. Each predicted Äˆğ‘– should be as close as possible to the true ğ¶ ğ‘– for all the examples ğ‘– in D. Note that in this problem the number of clusters and people may differ across examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clustering Conversational Interactants</head><p>We propose to construct the function â„(ğ‘ƒ ğ‘– ) = Äˆğ‘– using a Graph Neural Network (GNN), followed by the application of the Dominant Sets (DS) algorithm <ref type="bibr" target="#b22">[23]</ref>. To this end, we first create a fullyconnected interaction graph that describes the scene, ğº 0 ğ‘– = (ğ‘ 0 ğ‘– , ğ¸ 0 ğ‘– ), as illustrated in Figure <ref type="figure">1</ref>. We assign each feature vector ğ’‡ ğ‘˜ to node features ğ’ 0 ğ‘˜ and edge features ğ’† 0 ğ‘—ğ‘˜ in the graph, such that:</p><formula xml:id="formula_0">ğ‘ 0 ğ‘– = ğ’ 0 ğ‘˜ | 1 â‰¤ ğ‘˜ â‰¤ |ğ‘ƒ ğ‘– | , ğ¸ 0 ğ‘– = ğ’† 0 ğ‘—ğ‘˜ | 1 â‰¤ ğ‘—, ğ‘˜ â‰¤ |ğ‘ƒ ğ‘– |, ğ‘— â‰  ğ‘˜</formula><p>The proposed GNN is composed of two graph computation layers, Based on the pairwise affinities output by the GNN, we construct an affinity matrix ğ´ ğ‘– for the graph corresponding to the set ğ‘ƒ ğ‘– . ğ´ ğ‘– is then passed through the DS algorithm <ref type="bibr" target="#b22">[23]</ref>, which iteratively groups graph nodes into clusters by maximizing the quadratic program max</p><formula xml:id="formula_1">ğ‘”(â€¢) = ğ‘” 2 (ğ‘” 1 (â€¢)). Each</formula><formula xml:id="formula_2">ğ’™ âˆˆğ‘† |ğ‘ƒ ğ‘– | ğ’™ ğ‘‡ ğ´ ğ‘– ğ’™, where ğ‘† |ğ‘ƒ ğ‘– | is the standard simplex in R |ğ‘ƒ ğ‘– | .</formula><p>Here, solutions to the quadratic program represent a group of people, the dominant set in the input ğ´ ğ‘– . Note that every iteration of DS reduces the size of the affinity matrix by removing the data corresponding to the last group that was predicted by the algorithm.</p><p>Oftentimes, there will be individuals in a scene that are not in a group conversation. However, the peeling-off strategy employed by DS tends to group together these individuals. To combat this problem, we use the DS stopping criteria from <ref type="bibr" target="#b14">[15]</ref> to consider the global context of the complete graph when grouping people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Graph Neural Network.</head><p>Each computation layer of the proposed GNN is a graph network block comprised of two updates: one for the edges and one for the nodes, following the message-passing architecture described in <ref type="bibr" target="#b1">[2]</ref>. If we define the node and edge features for layer ğ‘™ as ğ’ ğ‘™ ğ‘˜ and ğ’† ğ‘™ ğ‘—ğ‘˜ , respectively, then the graph network block operates as follows:</p><formula xml:id="formula_3">ğ’† ğ‘™ +1 ğ‘—ğ‘˜ = edge_update ğ’† ğ‘™ ğ‘—ğ‘˜ , ğ’ ğ‘™ ğ‘— , ğ’ ğ‘™ ğ‘˜ (<label>1</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">ğ’ ğ‘™ +1 ğ‘˜ = node_update ğ’ ğ‘™ ğ‘˜ , agg {ğ’† ğ‘™ +1 ğ‘—ğ‘˜ | ğ‘— â‰  ğ‘˜ }<label>(2)</label></formula><p>The edge_update(â€¢) and node_update(â€¢) functions are neural networks that reason about edge or node features in relation to the information in their neighborhood in the graph, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The agg(â€¢) function is a symmetric function that summarizes information in the edge features connected to a given node.</p><p>Our motivation for designing our GNN with two graph network blocks stems from the fact that we consider fully-connected interaction graphs with no self loops in this work. Thus, two graph network blocks suffice to make the output affinity values dependent on the information encoded in all the nodes and edges in the graph. 3.1.2 Implementation Details. When creating an interaction graph ğº 0 , each person in the scene is associated to a node and edges are created between all individuals. The edge features ğ’† 0 are derived from a subset of the person features ğ’‡ in order to describe pairwise relationships. For example, if the person features include position information, then we compute an edge feature that corresponds to the distance between people. The remaining individual features are used as node features ğ’ 0 in the interaction graph.</p><p>Our model uses averaging for agg(â€¢) in eq. ( <ref type="formula" target="#formula_5">2</ref>), and multi-layer perceptrons (MLPs) for the edge_update(â€¢) and node_update(â€¢) functions in eq. ( <ref type="formula" target="#formula_3">1</ref>) and ( <ref type="formula" target="#formula_5">2</ref>), respectively. We also apply MLPs to both the initial node and edge features in ğº 0 , before the update functions, in order to balance their relative number of features. For example, if the nodes include image-derived features and the edges contain only distance, these MLPs can embed the image into a smaller feature and the distance into a bigger feature to increase their relative importance. The final edge embeddings in the GNN have a size of 1, corresponding to pairwise affinities. We train the GNN using binary cross-entropy on these affinities, as in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Finally, we aggregate the pairwise affinities output by the GNN into a matrix, Ã‚ğ‘– , and then compute a symmetric affinity matrix ğ´ ğ‘– = 1 2 (ğ´ ğ‘– +ğ´ ğ‘‡ ğ‘– ). The latter matrix is used by DS to compute groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We compare the proposed approach for group detection against baselines on two datasets with different person-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Cocktail Party Dataset <ref type="bibr" target="#b40">[41]</ref>. The dataset contains 30 minutes of interactions among six people in a lab environment. The dataset provides position and head orientation information for each individual, and we also consider their body orientation from <ref type="bibr" target="#b34">[35]</ref>. Conversational groups are labeled for 320 frames. We use the first 64 frames for testing, the next 64 for validation, and the rest for training. We chose this dataset for our evaluation because of its relative simplicity, high quality features, and popularity <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>MatchNMingle Dataset <ref type="bibr" target="#b4">[5]</ref>. The dataset was recorded over 3 days with a total of 92 participants. We used the "mingle" data, a subset of MatchNMingle where participants engaged in a cocktail party. For each day of recording, this subset includes 10/30 minutes of video from 3 cameras with annotated bounding boxes and "social actions" in 9 categories for each participant. Triaxial acceleration and binary proximity data is provided for 71/92 participants, all labeled at 20Hz. Also, manually-annotated conversational groups are given at 1 Hz. We aggregate these annotations into 600 frames per camera per day. Because there was high variability in group sizes, spacing, and environment obstacles between recordings, we partition each recording individually. The first and last 10% of frames from each recording were used for test, the next 10% of frames from beginning and end were used for validation, and the middle 60% were used for training. We consider 4 types of features for individuals in MatchNMingle:</p><p>-Position features (pos) include ğ‘¥, ğ‘¦ coordinates for the corresponding person on a video recording. -Acceleration features (accel) are the last 10 accelerometer readings for a person, covering a time window of 0.5 seconds. -Image features (img) are visual embeddings for the person. We compute the embedding by passing a 32 Ã— 32 cropped section of the recorded image around the person to ResNet <ref type="bibr" target="#b19">[20]</ref> and extracting the 512 features in the penultimate layer of the network. -Semantic features (label) encode person actions. The features are computed by aggregating the actions into a 9-dimensional vector that indicates their occurrence per type over the last 0.5 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Group Detection Methods</head><p>We consider three methods in our evaluation:</p><p>(1) Dist. Hand-crafted baseline inspired by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39]</ref>. The method computes an affinity matrix as ğ´ ğ‘– ğ‘— = exp âˆ’ ğ‘‘ ğ‘– ğ‘— /2ğœ 2 , where ğ‘‘ ğ‘– ğ‘— is the distance between two participants and ğœ = 2 meters, following <ref type="bibr" target="#b14">[15]</ref>. DS is then applied to obtain groupings, as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>(2) DANTE. We implement the DANTE neural network <ref type="bibr" target="#b29">[30]</ref> in PyTorch and use DS for clustering, as in <ref type="bibr" target="#b14">[15]</ref>. The dyad and context MLPs of DANTE had two layers with 32 and 64 units. The final MLP had 64 and 32 units. All but the last layer used a ReLU activation followed by batch normalization. For Cocktail Party, DANTE uses position and the orientations as node features, transforming each feature into a coordinate frame centered between each dyad, as in <ref type="bibr" target="#b29">[30]</ref>. For MatchNMingle, it uses the position, transformed by the dyadic coordinate frame, and applies the rest of the features without additional transformations.</p><p>(3) GNN. Our proposed combination of a GNN with DS for group detection. We implement the GNN using PyTorch Geometric to leverage sparse tensor computations. For the edge updates, we use two MLPs of dimensions 128, 64 and 32, 16 for each graph network block. The node updates use MLPs of dimensions <ref type="bibr">32, 16 and 16, 16.</ref> As in DANTE, we use ReLU activations and batch norm. These dimensions were chosen to produce a similar number of parameters to the DANTE models for all input feature combinations.</p><p>For Cocktail Party, the GNN uses distance and both angles, transformed into point pair features <ref type="bibr" target="#b8">[9]</ref>, as edge features. For MatchN-Mingle, it uses distance for edge features and all other features as node features. When considering position-only features, however, we do not use any node features.</p><p>Both DANTE and the GNN were trained using a learning rate of 1e-4 that decays to 1e-6 over 1000 epochs, a batch size of 512, and the Adam optimizer. Early stopping halted training if there was no decrease in the cross-entropy loss after 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Our main evaluation metric is the Group F1 metric <ref type="bibr" target="#b31">[32]</ref>. For a threshold ğ‘‡ , the Group F1 metric considers a ground truth cluster with ğ‘› ğ‘” people to be correctly identified if at least âŒˆğ‘‡ â€¢ ğ‘› ğ‘” âŒ‰ members are grouped together by the algorithm and no more than âŒˆ(1âˆ’ğ‘‡ )â€¢ğ‘› ğ‘” âŒ‰  <ref type="table" target="#tab_1">1</ref> displays the F1T1 and F1T2/3 scores on the Cocktail Party dataset. For both metrics, a Kruskall-Wallis nonparametric test indicated that there was a significant difference between scores by method, with p &lt; 0.0001 for F1T1 and p = 0.0003 for F1T2/3. Further, Steel-Dwass post-hoc tests showed that in both cases DANTE and the proposed GNN method led to significantly higher performance than the Dist baseline, but the scores for DANTE and the GNN were not significantly different. Given the high F1T2/3 scores for the data-driven methods, we proceeded to evaluate performance on the more complex MatchNMingle dataset, which contains almost 14 times as many frames as Cocktail Party, 2.5 times the maximum number of people per frame, and, unlike Cocktail Party, a variable number of people per frame.</p><p>MatchNMingle. Table <ref type="table" target="#tab_2">2</ref> shows the results based on the features available for detection. When only position was available, a Kruskal-Wallis test resulted in significant differences for F1T1 (p = 0.02) and a Steel-Dwass post-hoc test indicated that DANTE had significantly higher results than the Dist baseline. No other significant pairwise differences were found. For T1T2/3, the Kruskall-Wallis test also showed significant differences (p = 0.0003). In this case, DANTE was significantly better than the other two methods.</p><p>Wilcoxon tests showed significant differences by method for the combinations of pos, accel, img, and label features in Table <ref type="table" target="#tab_2">2</ref>. The proposed GNN outperformed DANTE in all these cases. Interestingly, the GNN shows increased performance the more features were provided to it, especially including accel and label. However, DANTE is not as effective at incorporating additional features. For example, the F1T1 score for DANTE using all MatchNMingle features is about 5% lower than the score for using only position data, where the proposed GNN results in a 6% increase in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS &amp; FUTURE WORK</head><p>We demonstrated the successful application of GNNs to group detection. In principle, the inductive nature of the proposed approach allows our method to run in an online fashion, processing streams of data. However, more tests are needed to verify this in practice. Future work could also evaluate the GNN on other group detection datasets, like CoffeeBreak <ref type="bibr" target="#b6">[7]</ref> or Salsa <ref type="bibr" target="#b0">[1]</ref>.</p><p>Unexpectedly, DANTE and the proposed GNN did not benefit from the added image features in the MatchNMingle dataset. Further, in the case of DANTE, performance tended to decrease with more features. There are several possible explanations for this phenomenon. First, the image features from the ResNet <ref type="bibr" target="#b19">[20]</ref> model could have been too deep in the network. Low-level features from earlier in the network could be used to fix this issue. Second, the MatchNMingle cameras have visible radial distortion, which we did not correct for because the intrinsic camera parameters are not public. Lastly, the performance drop could be due to challenges combining feature modalities. In this respect, future work could explore using attention mechanisms to fuse data, e.g., as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>We processed the data from different cameras in the MatchN-Mingle dataset as independent samples, although some of them contained information captured at the same time from different views. Likewise, we did not consider the temporal correlation of data across dataset samples, but this information could improve model prediction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. Thus, future work could explore detecting groups across multiple camera views to understand more holistically the environment, and combining GNNs with recurrent neural networks to take advantage of temporal correlations, e.g., as in <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented an approach to predict conversational clusters in social scenes, where the number of clusters is unknown a priori. Our results indicate that GNNs can better take advantage of multi-modal data for group detection in comparison to baselines. In particular, the proposed GNN-based model outperformed the previous stateof-the-art approach <ref type="bibr" target="#b29">[30]</ref> on the complex MatchNMingle dataset with all types of data except position-only, while requiring less data pre-processing. This suggests that leveraging relational inductive biases in data-driven methods for group detection is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Portions of the research in this paper used the MatchNMingle Dataset made available by the Delft University of Technology, Delft, The Netherlands. This work was supported by the National Science Foundation (NSF), Grant No. <ref type="bibr">(IIS-1924802)</ref>. The findings and conclusions in this paper are those of the authors and do not necessarily reflect the views of the NSF. The authors are also thankful to the Yale Hahn Scholars program for supporting A. W. Gupta.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Edge update (left) and node update (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>At the last layer of the GNN, ğ’† 2 ğ‘—ğ‘˜ âˆˆ ğ¸ 2 ğ‘– represents the pairwise affinity from node ğ‘— to node ğ‘˜ in the graph.</figDesc><table /><note>layer transforms an input graph ğº ğ‘™âˆ’1 ğ‘– into another graph ğº ğ‘™ ğ‘– , with ğ‘™ indicating the ğ‘™-th layer without loss of generality.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on Cocktail Party, including average results and std. deviation (ğœ‡ Â± ğœ) over the test examples. Results in bold are significantly better than those with regular font.</figDesc><table><row><cell>Metric</cell><cell>Dist</cell><cell>DANTE</cell><cell>GNN</cell></row><row><cell>F1T1</cell><cell cols="3">0.24 Â± 0.34 0.58 Â± 0.43 0.62 Â± 0.41</cell></row><row><cell cols="4">F1T2/3 0.53 Â± 0.32 0.71 Â± 0.35 0.70 Â± 0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on MatchNMingle, including average results and std. deviation (ğœ‡ Â± ğœ) over the test examples. Results in bold are significantly better than those with regular font. We consider two values for ğ‘‡ : 2/3 and 1. For example, let the true group be ğ‘” = {1, 2, 4} and the predicted group Ä = {1, 2, 5}. Here, ğ‘› ğ‘” = 3. For T=1, âŒˆğ‘‡ â€¢ ğ‘› ğ‘” âŒ‰ = âŒˆ1 â€¢ 3âŒ‰ = 3 &gt; |ğ‘” âˆ© Ä|, so the group is not correctly identified. For ğ‘‡ =2  3 ,âŒˆğ‘‡ â€¢ğ‘› ğ‘” âŒ‰ = âŒˆ 2 3 â€¢3âŒ‰ = 2 â‰¤ |ğ‘”âˆ© Ä| and âŒˆ(1âˆ’ğ‘‡ ) â€¢ğ‘› ğ‘” âŒ‰ = âŒˆ 1 3 â€¢3âŒ‰ = 1 â‰¤ | Ä\ğ‘”|, so the group is correctly identified.</figDesc><table><row><cell>Features</cell><cell>Metric</cell><cell>Dist</cell><cell>DANTE</cell><cell>GNN</cell></row><row><cell>pos</cell><cell>F1T1</cell><cell cols="3">0.28 Â± 0.26 0.32 Â± 0.28 0.30 Â± 0.26</cell></row><row><cell></cell><cell cols="4">F1T2/3 0.38 Â± 0.29 0.43 Â± 0.30 0.40 Â± 0.28</cell></row><row><cell>pos+accel</cell><cell>F1T1</cell><cell>-</cell><cell cols="2">0.24 Â± 0.25 0.34 Â± 0.27</cell></row><row><cell></cell><cell>F1T2/3</cell><cell>-</cell><cell cols="2">0.30 Â± 0.28 0.43 Â± 0.28</cell></row><row><cell>pos+img</cell><cell>F1T1</cell><cell>-</cell><cell cols="2">0.28 Â± 0.29 0.31 Â± 0.28</cell></row><row><cell></cell><cell>F1T2/3</cell><cell>-</cell><cell cols="2">0.34 Â± 0.29 0.40 Â± 0.29</cell></row><row><cell>pos+accel</cell><cell>F1T1</cell><cell>-</cell><cell cols="2">0.23 Â± 0.24 0.32 Â± 0.28</cell></row><row><cell>+img</cell><cell>F1T2/3</cell><cell>-</cell><cell cols="2">0.28 Â± 0.26 0.42 Â± 0.29</cell></row><row><cell>pos+accel</cell><cell>F1T1</cell><cell>-</cell><cell cols="2">0.27 Â± 0.26 0.36 Â± 0.29</cell></row><row><cell cols="2">+img+label F1T2/3</cell><cell>-</cell><cell cols="2">0.35 Â± 0.28 0.46 Â± 0.29</cell></row><row><cell cols="3">false subjects are identified. Cocktail Party. Table</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://gitlab.com/interactive-machines/perception/group_gnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SALSA: A Novel Dataset for Multimodal Group Behavior Analysis</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligia</forename><surname>Batrinca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1707" to="1720" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261[cs.LG]</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral Clustering with Graph Neural Networks for Graph Pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>DaumÃ©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directions Robot: In-the-Wild Experiences and Lessons Learned</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chit</forename><forename type="middle">W</forename><surname>Saw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems</title>
				<meeting>the 2014 International Conference on Autonomous Agents and Multi-Agent Systems<address><addrLine>Paris, France; Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="637" to="644" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The MatchNMingle Dataset: A Novel Multi-Sensor Resource for the Analysis of Social Interactions and Group Dynamics In-the-Wild During Free-Standing Conversations and Speed Dates</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Cabrera-Quiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Demetriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Leander Van Der Meij</surname></persName>
		</author>
		<author>
			<persName><surname>Hung</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2018.2848914</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2018.2848914" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="113" to="130" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>KDD &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social Interaction Discovery by Statistical Analysis of F-formations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Paggetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Tosato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gloria</forename><surname>Menegaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
				<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>Citeseer, BMVA Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human behavior analysis in video surveillance: A Social Signal Processing perspective</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramachandra</forename><surname>Raghavendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Alessio Del Bue, and Vittorio Murino</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic Detection of Social Behavior of Museum Visitor Pairs</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsvi</forename><surname>Kuflik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting Conversing Groups Using Social Dynamics From Wearable Acceleration: Group Size Awareness</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
				<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Hidden Dimension</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hall</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Doubleday</publisher>
			<biblScope unit="volume">609</biblScope>
			<pubPlace>Garden City, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing F-Formations in the Open World</title>
		<author>
			<persName><forename type="first">Hooman</forename><surname>Hedayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Andrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="558" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting F-Formations as Dominant Sets</title>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>KrÃ¶se</surname></persName>
		</author>
		<idno type="DOI">10.1145/2070481.2070525</idno>
		<ptr target="https://doi.org/10.1145/2070481.2070525" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Multimodal Interfaces</title>
				<meeting>the 13th International Conference on Multimodal Interfaces<address><addrLine>Alicante, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
	<note>ICMI &apos;11)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effects of the Display Angle on Social Behaviors of the People around the Display: A Field Study at a Museum</title>
		<author>
			<persName><forename type="first">Junko</forename><surname>Ichino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuo</forename><surname>Isoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reimi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</title>
				<meeting>the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
	<note>CSCW &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial Play Effects in a Tangible Game with an F-Formation of Multiple Players</title>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Jungmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldine</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Australasian User Interface Conference -Volume</title>
				<meeting>the Fifteenth Australasian User Interface Conference -Volume<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<publisher>AUS</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conducting interaction: Patterns of behavior in focused encounters</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CUP Archive</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907[cs.LG]</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FPGA accelerates deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Xuelei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="837" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-Device Interaction via Micro-Mobility and f-Formations</title>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Marquardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saul</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 25th Annual ACM Symposium on User Interface Software and Technology<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Socially Aware Interactive Playgrounds</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robby</forename><surname>Van Delden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Reidsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dominant sets and pairwise clustering</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Local Trajectory Replanning and Control for Robot Navigation</title>
		<author>
			<persName><forename type="first">Ashwini</forename><surname>Pokle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>MartÃ­n-MartÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Hans M Ewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5815" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From Proxemics Theory to Socially-Aware Navigation: A Survey</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Rios-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Spalanzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Networks as Learnable Physics Engines for Inference and Control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<meeting>the 35th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Group detection in still images by F-formation modeling: A comparative study</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fformation detection: Individuating free-standing conversational groups in images</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Bassetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">e0123783</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving Social Awareness Through DANTE: Deep Affinity Network for Clustering Conversational Interactants</title>
		<author>
			<persName><forename type="first">Mason</forename><surname>Swofford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Peruzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sydney</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>MartÃ­n-MartÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marynel</forename><surname>VÃ¡zquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Hum.-Comput. Interact. 4, CSCW1, Article 020</title>
				<meeting>ACM Hum.-Comput. Interact. 4, CSCW1, Article 020</meeting>
		<imprint>
			<date type="published" when="2020-05">2020. May 2020</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16904[cs.LG]</idno>
		<title level="m">Graph Clustering with Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting conversational groups in images and sequences: A robust game-theoretic approach</title>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eyasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mequanint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><surname>Murino</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2015.09.012</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2015.09.012" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Åukasz Kai ser, and Illia Polosukhin</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
	<note>Attention is All You Need</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards Robot Autonomy in Group Conversations: Understanding the Effects of Body Orientation and Gaze</title>
		<author>
			<persName><forename type="first">Marynel</forename><surname>VÃ¡zquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Braden</forename><surname>Mcdorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th ACM/IEEE International Conference on Human-Robot Interaction</title>
				<imprint>
			<publisher>HRI. IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parallel detection of conversational groups of free-standing people and tracking of their lower-body orientation</title>
		<author>
			<persName><forename type="first">Marynel</forename><surname>VÃ¡zquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2015.7353792</idno>
		<ptr target="https://doi.org/10.1109/IROS.2015.7353792" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3010" to="3017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826[cs.LG]</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cross-Modal Attention With Semantic Consistence for Image-Text Matching</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5412" to="5425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems<address><addrLine>MontrÃ©al, Canada; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
	<note>NIPS&apos;18)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monitoring, recognizing and discovering social networks</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kedar</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Krahnstoever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbhakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BarnabÃ¡s</forename><surname>PÃ³czos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Space speaks: towards socially and personality aware visual surveillance</title>
		<author>
			<persName><forename type="first">Gloria</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Workshop on Multimodal Pervasive Video Analysis</title>
				<meeting>the 1st ACM International Workshop on Multimodal Pervasive Video Analysis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
