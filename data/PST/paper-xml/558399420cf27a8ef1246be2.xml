<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Source-Channel Separation Theorem Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sridhar</forename><surname>Vembu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Verd6</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">International Symposium on Information Theory</orgName>
								<address>
									<postCode>1994, 1994</postCode>
									<settlement>Trondheim, Norway</settlement>
									<region>June</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Qualcom Inc</orgName>
								<address>
									<postCode>92121</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Ben Gurion University of the Negev</orgName>
								<address>
									<postCode>84105</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Source-Channel Separation Theorem Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">19ED1FEF8DACCC3A3FC416499C7A9099</idno>
					<note type="submission">received July 20, 1993; revised January 31, 1994. This research has been partially supported by the National Science Foundation under PYI</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Shannon Theory</term>
					<term>channel capacity, source coding</term>
					<term>joint source-channel coding</term>
					<term>separation theorem</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The single-user separation theorem source-channel coding has been D r o v e d D W V ~O U S ~V Verdd. Fellow, IEEE, and Yossef Steinberg of joint for wide classes of sources and channels. We find ab informition-stable sourcdchannel pair which does not satisfy the separation theorem. New necessary and sufficient conditions for the transmissibility of a source through a channel are found, and we characterize the class of channels for which the separation theorem holds regardless of the source statistics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE MEETING point of the two main branches of the T Shannon theory is the joint sourcexhannel coding theorem. This theorem has two parts: a direct part that states that if the minimum achievable source coding rate of a given source is strictly below the capacity of a channel, then the source can be reliably transmitted through the channel by appropriate encoding and decoding operations; and a converse part stating that if the source coding rate is strictly greater than capacity, then reliable transmission is impossible. Implicit in the direct sourcexhannel coding theorem is the fact that reliable transmission can be accomplished by separate source and channel coding, where the source (resp., channel) encoder and decoder need not take into account the channel (resp., source) statistics. Because of the converse theorem (and except for the residual uncertainty in the case when the minimum source coding rate is equal to the channel capacity) it follows that either reliable transmission is possible by separate source-channel coding or it is not possible at all. This is the reason why the joint sourcexhannel coding theorem is commonly referred to as the separation theorem.</p><p>Ever since Claude Shannon's 1948 paper <ref type="bibr">[l]</ref>, where the result was stated for stationary memoryless sources and channels, the separation theorem has received considerable attention, with a number of researchers proving versions that apply to more and more general classes of sources and channels. Dobrushin [ 2 ] and Hu <ref type="bibr" target="#b3">[4]</ref> considered the separation theorem in the context of information stable sources and channels, i.e., situations where, essentially, the Asymptotic Equipartition Property (AEP) is satisfied and minimum achievable source coding rate and channel capacity are equal to the entropy rate and the limit of maximal mutual information rates, respectively. In addition, joint source-channel coding has been a main focus of ergodic-theoretic researchers in Shannon theory, who have obtained general expressions for the maximal entropy rate of those sources that can be transmitted reliably through a given channel. In the foregoing discussion, reliable transmission of the source through the channel means that the probability of correctly decoding a block of n transmitted symbols goes to 1 as n -+ 00. Since the works by Kolmogorov <ref type="bibr" target="#b6">[6]</ref>, Shannon <ref type="bibr" target="#b7">[7]</ref>, and Dobrushin [ 2 ] , the separation theorem has also been investigated in the context of transmission with a distortion measure. In this paper we focus exclusively on the aforementioned reliability criterion of "almost noiseless" fixed-length block coding.</p><p>Even though most analytically tractable channels and sources are encompassed by previous versions of the separation theorem, it is of considerable theoretical interest to study the validity of this theorem in the context of very general sources and channels. In particular, we do not impose restrictions such as memorylessness, stationarity, ergodicity, causality, information stability, etc. This is motivated by the recent papers <ref type="bibr" target="#b13">[13]</ref> and <ref type="bibr">[14]</ref>, which find general expressions for the minimal source coding rate and channel capacity that apply without those restrictions. A result [14, Theorem 41 which leads to a new general converse to the channel coding theorem in <ref type="bibr">[14]</ref>, proves to be a key tool in our investigation of the sourcexhannel coding theorem. Despite the generality of <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr">[14]</ref> and the present paper, the proofs are, in fact, conceptually simple.</p><p>After a review of definitions and previous results in Section 11, we show an example in Section I11 where the converse to the separation theorem fails to hold: a memoryless information stable sourcekhannel pair such that the source is transmissible through the channel (with zero error), yet its minimum achievable source coding rate is twice the channel capacity. We note that previous instances where the separation theorem was known to fail were always within the context of multiterminal sources and channels (e.g., <ref type="bibr" target="#b8">[8]</ref>). The example in Section I11 reveals that, in general, the channel capacity and the minimum source coding rate do not provide sufficient knowledge in order to determine whether the source can be transmitted reliably through the channel. A finer look at the statistical structure of the channel and source is necessary. This is done in Section IV where two similar conditions, domination and strict domination, are shown to be necessary and sufficient, respectively, for reliable transmissibility. In Section V we characterize those channels for which the classical statement of the separation theorem holds for every source. It turns out that those are the channels whose definition of capacity is insensitive to whether good codes are required for all sufficiently long blocklengths or for only infinitely many blocklengths. We also characterize those sources for which the separation theorem holds for every channel. This class of sources includes but is not restricted to stationary sources. A conclusion to be drawn from our results is that when dealing with nonstationary probabilistic models, care should be exercised before applying the separation theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dejinitions and Classical Results</head><p>Let F , A, and B be finite sets. A source Z with alphabet F is the sequence {€'Zn},Z1, where Pzn is a probability distribution on F". Similarly, a channel W with input alphabet A and output alphabet' B is a sequence of conditional distributions {Wn(.l.)}n&gt;l such that Wn(.lnn) is a probability distribution on B" for every an E A". Given an A-valued source X, and a channel W , we denote the joint source whose finite-dimensional distributions are W" Pxn by (X, Y ) .</p><p>Dejinition I : Given a joint distribution P Z p ~~. on A" x B" with marginals PAyn, Pyn the information density is the function</p><formula xml:id="formula_0">W"(b" la") PY-(b") . = log</formula><p>The distribution of the random variable ( l / n ) i x n w n (a"; b") is referred to as the information spectrum of Pxnwn, <ref type="bibr" target="#b13">[13]</ref>,</p><p>and the expected value of the information spectrum is the normalized mutual information ( l / n ) I ( X " ; Y " ) . The mutual information rate of ( X , Y ) is defined as</p><formula xml:id="formula_1">' 1 I ( X ; Y ) = lim -I ( X n ; Y " )</formula><p>n-cc n provided that the limit exists. In case that X is equal to Y (i.e., W is an identity channel), the information density I ~~~~I I -~~ (0": b") is referred to as the entropy density, which is given by The expected value of (l/n)h,yn ( X " ) is the normalized entropy ( l / n ) H ( X " ) , and the entropy rate of X is 1</p><formula xml:id="formula_2">H ( X ) = lirri -H ( X n ) 11-00 71.</formula><p>provided that the limit exists. We proceed now to the classical definition of information stability of sources and channels. The concept of information stability was first introduced by Dobrushin [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'</head><p>The assumption that the channel alphabets are finite is made for notational convenience. It can be readily lifted without impacting our results or their proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 [2], [3/:</head><p>A source Z is said to be informationstable if H ( Z n ) &gt; 0 for all sufficiently large n, and hz.. (2") / H ( 2") converges in probability to one as n + m, i.e., for every t &gt; 0.</p><p>This definition is slightly more general than the AEP <ref type="bibr">[16]</ref> which requires that (l/n)hzn (2") converges in probability to limn+m ( l / n ) H ( Z " ) . The Shannon-McMillan theorem implies that stationary and ergodic finite alphabet sources are information-stable.</p><p>Dejinition 3 [2], <ref type="bibr" target="#b3">[4]</ref>: A channel W is called informationstable if there exists an input process X such that</p><formula xml:id="formula_3">i x n w -( X " ; Y " ) --f ~ nCn ( W " )</formula><p>where the convergence is in probability, and</p><formula xml:id="formula_4">Cn(W") = A 1 sup -I ( X " ; Y " ) . X " n</formula><p>The reason for defining information stability as in Definition 2 lies in the fact that information stability is precisely the condition under which C, has an operational meaning asymptotically. Before stating the classical results on source coding, channel capacity, and source-channel separation theorem, we record the standard definitions of the minimal achievable (fixed-length) source coding rate and channel capacity. Definition 4: An ( n , M , t) fixed-length source code for 2" is a collection of M n-tuples {a;". . . . , u g f } such that P(Z" 6 {a?, . ' . , &amp;}) 5 t.</p><p>R is and t-achievable source coding rate for Z if for every y &gt; 0 there exist, for all sufficiently large 7 ~, ( n , M , f ) codes with 1 -1ogM &lt; R f y . n R is an achievable (fixed-length) source coding rate for Z if it is €-achievable for all E &gt; 0. The minimal achievable source coding rate of Z is denoted by T(2). Definition 5: An ( n , M , E ) code for a random transformation W" with input alphabet A and output alphabet B is a pair of mappings such that</p><p>The mappings f and g are referred to as the encoder and decoder, respectively. Dejinition 6: R is an t-achievable rate for W if for every y &gt; 0 there exists, for all sufficiently large n, an ( n , MI t ) code for W" with 1 -l o g M &gt; R -7 . n The maximum t-achievable rate for a channel W is called the t-capacity, C, ( W ) , of the channel. The channel capacity*, C ( W ) , is defined as the maximal rate that is t-achievable for every E &gt; 0. The definition implies that</p><formula xml:id="formula_5">C ( W ) = lim C,(W) t-0</formula><p>and that C ( W ) is the supremum of all the rates R for which there exist a sequence of ( n , M , E , ) codes such that log M n &gt; R and lim t, = 0.</p><p>In the definition of capacity we have used the average probability of error. However, capacity remains unchanged if we use the maximal probability of error. This is due to the fact that the existence of ( n , M , t ) codes according to the average probability of error criterion implies the existence of ( n , M/2. where 2" is the output due to 2" of the cascade encoder-c hannel-decoder. We briefly reiiew the well-known results on source coding and transmission. The following theorems are not the most general statements that can be deduced from the results in [2], <ref type="bibr" target="#b3">[4]</ref>, but give a flavor of what is implied by the results in these works with regard to a separation theorem.</p><p>Theorem 1: Every information stable source 2 satisfies 1</p><formula xml:id="formula_6">T ( Z ) = limsup -H ( Z " ) . n-o3</formula><p>Theorem 2: Every information stable channel W satisfies</p><formula xml:id="formula_7">1 n-oo n C ( W ) = lirriinf slip -I ( X " ; Y " ) .</formula><p>If the liminf in Theorem 2 is actually a limit, the following joint sourcekhannel coding theorem follows.</p><p>*The explicit dependence on W will be omitted when convenient. The restriction on the classes of sources and channels in <ref type="bibr" target="#b3">[4]</ref> are somewhat more general than those in Theorem 3; however, they are hard to verify. A summary and further discussion of those conditions can be found in <ref type="bibr" target="#b5">[5]</ref>. Another well-known body of results on the source-channel transmission problem is ergodic theoretic in nature. Tutorial discussions on the ergodic theoretic approach to the separation theorem can be found in C ( W ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~91, [lo].</head><p>For every channel we can define </p><formula xml:id="formula_8">CE(W) 2 S U P H ( 2 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General Sources and Channels</head><p>In this subsection we describe the results on source coding and channel capacity obtained in [ 131, [ 141 for general sources and channels. We start with a few definitions.</p><p>Definition 8 <ref type="bibr" target="#b13">[13]</ref>: The limsup in probability of a sequence of random variables { A n } is defined as the smallest extended real number P such that for all t &gt; 0 lim P[A, 2 + t] = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n-o=</head><p>Analogously, the liminf in probability is the largest extended real number cy such that for all E &gt; 0 lim P [ A , 5 cyt] = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n-oo</head><p>Note that a sequence of random variables converges in probability to a constant if and only if its limsup in probability is equal to its liminf in probability. The limsup in probability (resp., liminf in probability) of the sequence of random variables { ( l / n ) i p w -n ( X " , Yn))r=._, is referred to as the sup-information rate (resp., infinformation rate) of the pair ( X , Y ) and is denoted as T ( X ; Y ) (resp., l ( X ; Y ) ) . The limsup (resp., liminf) in probability of the sequence of random variables { ( l / n ) h x 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( X T ' ) } r = l</head><p>is referred to as the sup (resp., inA-entropy rate of X and is denoted by H ( X ) (resp., H ( X ) ) .</p><p>We are in a position to state the general source coding and channel capacity results of <ref type="bibr" target="#b13">[13]</ref>, [141.</p><p>Theorem 4 <ref type="bibr" target="#b13">[13]</ref>: For any finite alphabet source Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T ( 2 ) = H ( Z ) .</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOURCE CHANNEL</head><p>Fig. <ref type="figure">1</ref>. A source and channel that do not satisfy the separation theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5 [14]:</head><p>The capacity C ( W ) of any finite alphabet channel W is given by</p><formula xml:id="formula_9">C(W) = supL(X; Y ) X where Y is the output of W to X.</formula><p>In view of Theorems 4 and 5 , we have the direct part of the separation theorem that holds for general sources and channels: Theorem 6: If a given source 2 and channel W satisfy T ( 2 ) &lt; C ( W then 2 can be reliably transmitted over W . Proofi Let us assume that T + S &lt; C for some S &gt; 0.</p><p>From Theorem 4, we can find a sequence of ( n , exp {n(T + (t1/2))}, t , ) source codes for 2 with E , --f 0. Furthermore, Theorem 5 implies that we can find a sequence of ( n , exp { n ( T + (6/2))}, yn) channel codes for W with yn ---f 0.</p><p>We define the encoder and decoder pair needed in Definition 7 as follows: the encoder consists of the cascade of the source encoder and the channel encoder. The decoder is the cascade of the channel decoder and the source decoder. The overall probability of error is upper bounded by t , + yn which goes to zero. Note that we have proved not just that 2 is transmissible over W , but also that the encoder can be split into a source encoder and a channel encoder and these functions can be position right before times 2', i = 1, 2, 3;'. . Note that both the source and channel are memoryless.</p><p>Let J denote the set of times at which the switch is in the "up" position. Assuming that the switch is "down" at time i = 1, we have J = (2, 3, 8, 9. 10, 11, 12, 13, 14, 15, 32, 33,. .. , 62, 63, 128, 129,. . .}.</p><p>At times i E J , the binary source is independently equally likely (0, l}, and the channel is noiseless, whereas at times i 6 J , the source is deterministic and the channel output is independent of the input. Since the set J is deterministic and known to both transmitter and receiver, 2 can be transmitted over W with zero probability of error. However, as we shall</p><formula xml:id="formula_10">show, T(2) = 2/3 and C ( W ) = 1/3.</formula><p>TO evaluate H(z), write and observe that log Pz, (2;) is deterministic, attaining the value -1 bit for i E J and 0 for i 6 J . (For convenience, the logarithms in this section have base 2.) Thus it is straightforward to verify that (2)</p><formula xml:id="formula_11">J ( n &gt; 2 n-oo n 3 - H ( 2 ) = limsup- = -</formula><p>where J ( n ) stands for the cardinality of the intersection of J with the set (1, 2 , . . . , n } , i.e., performed independently. This is the crux of the separation ' principle.</p><p>A completely general separation theorem would follow if we could prove that ~( n ) e I J n (1, 2 , . . . , .&gt;I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T ( 2 ) I C ( W )</head><p>We proceed to the evaluation of the capacity of W . It was is a necessary condition for reliable transmission of 2 over W to be possible. This, however, is not true: in Section 111 we give an example that disproves such a statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">AN EXAMPLE WHERE SEPARATION DOES NOT HOLD</head><p>In this section we construct an example of a nonstationary source 2 and a nonstationary channel W over which 2 can be reliably transmitted, yet H ( 2 ) &gt; C ( W ) .</p><p>The source and channel pair is given in Fig. <ref type="figure">1</ref>. Note that the switches move in synchronism+ither both are "up" position or both are "down." The switches deterministically change shown in [ 14, sec. 71 that the capacity of a memoryless binary symmetric channel is given by</p><formula xml:id="formula_12">C ( W ) = 1 -H(N)</formula><p>where N denotes the random process of errors. Following the arguments leading to (2), we conclude that</p><formula xml:id="formula_13">H ( N ) = limsup 1 --. - J ( n ) . n-oo n, Thus J ( n ) 1 C ( W ) = liminf- = - n-oo n 3 '<label>(3)</label></formula><p>It is interesting to note that both the source and the channel in this example are not only memoryless but informationstable. To check this, note that nonzero probability source strings of a given length all have the same probability; thus (1) follows immediately. Analogously, the information stability of the channel can be checked by noticing that Bernoulli ( i) inputs achieve maximal input-output mutual information, and with that choice, every pair of input-output strings of a given length has the same probability or cannot occur at all. Separation need not hold because C,(Wn) does not have a limit (cf. Theorem 3).</p><p>IV. A GENERAL TRANSMISSION THEOREM In view of the example above, it is clear that the minimum source-coding rate and the channel capacity are inadequate to completely characterize the problem of reliable transmission of a source over a channel. In this section we present a general transmission theorem that serves to give an answer to this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notions of Domination</head><p>We will now define two related notions, which we call strict domination and domination. Dejnition 9: A channel W is said to strictly dominate a source 2 if there exists a S &gt; 0 and a channel input process X such that 1</p><formula xml:id="formula_14">+ P -i x -w -( X n ; Y " ) 5 e, + s = 0.</formula><p>[:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I1</head><p>The above definition captures the notion that asymptotically the information spectrum of the channel lies entirely above the spectrum of the source. For example, if the sup-entropy rate of the source and the channel capacity satisfy H ( Z ) &lt; C ( W ) , strict domination holds. Note that strict domination does not necessarily mean that there exists a fixed scalar such that the channel information spectrum is asymptotically above it while the source spectrum is asymptotically below it. Such a case would be included in this definition but that is not the only way a channel can strictly dominate a source.</p><p>We will define a closely related notion called domination. Definition 10: A channel W is said to dominate a source Z if for any S &gt; 0 and any sequence of nonnegative numbers {c,}:=~, there exists X such that</p><p>The gist of both definitions is that the upper tail of the source information spectrum (i.e., the distribution of the normalized entropy density) has vanishing overlap with the lower tail of the channel information spectrum (distribution of the normalized information density) evaluated with a favorable input process. Domination compares the tails in nonoverlapping intervals [c,, +oo) and (-w, cn -SI, and forbids that both tails be nonvanishing for any sequence e,. Strict domination compares the tails in overlapping intervals [c,, +00) and (-00, c, + SI, and dictates that both tails vanish provided {c,} is suitably selected.</p><p>As we will see, domination (resp., strict domination) will take the role of the condition that the minimum source coding rate is greater than or equal to (resp., strictly greater than) the capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transmission Theorem</head><p>The following result3 is a generalization of the well-known source-channel separation theorem. In the classical theory, the case when capacity equals the minimum source coding rate is left unresolved. We have the same ambiguity here and that is the reason for two definitions in Section IV-A.</p><p>Theorem 7: i) Reliable transmission for a source-channel pair is possible ii) If reliable transmission is possible for a source-channel if the channel strictly dominates the source.</p><p>pair, then the channel dominates the source. Proof:</p><p>i) Let us assume that channel W strictly dominates source 2. From the definition of strict domination, we can find a S &gt; 0 and a channel input process X such that This means that there exists a sequence {en},",l such that Y " ) &lt; C , + S 5 7,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">and</head><p>where T~ goes to zero as n goes to infinity. Now we will apply Feinstein's Lemma (see, for example,</p><p>[17]) to claim that there exists a sequence of ( n , MI E , ) codes for the channel where M = exp{n(c, + ( S / 2 ) ) } and E , is upper bounded by T, + exp ( -n 6 / 2 ) which goes to zero with n.</p><p>We will only encode the source words that fall within the set whose probability goes to 1 with n. When an outcome in E;</p><p>occurs we declare error.</p><p>Clearly, E, contains fewer than exp(nc,) elements and they can be transmitted with probability of error at most e,.</p><p>Hence the overall probability of error is upper-bounded by t, + T , which goes to zero. This establishes the direct part.</p><p>3With suitable modifications to the definitions of strict domination and domination, it is possible to prove a straightforward extension of Theorem 7 to the case where the source words and the channel codewords do not have the same blocklength; see 151.</p><p>ii) Recall from Definition 7 that reliable transmission of 2 over W implies the existence of a sequence of encoders { f,}</p><p>We know that there exists a sequence of encoder-decoder pairs such that the probability of error and decoders {g,} such that where we have used the notation t , = Ce(,")Pz.(z") Z n goes to zero with increasing n E K . We have used e ( z n ) to denote the probability of error when the input is z n , i.e.,</p><p>V" (Y" 12") = W"(Y" I f n ( z " ) ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e(,%)</head><formula xml:id="formula_15">= P[Z" # z n p n = z"]</formula><p>Furthermore, given a channel and a sequence of encoders, we will denote the cascade encoder channel by V. The following result gives a very simple property satisfied by the information spectrum of the cascade of a deterministic transformation and a random transformation.</p><p>Lemma 1: Fix n. For any Pz-, deterministic transformation f i l and random transformation W " , the information densities X" = f,(Zn) and Vn(.lzn) = W n ( . l f n ( z n ) ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that</head><p>where Zn is the decoder output.</p><p>Let us define the following set:</p><p>D, = z" E B,: e(z") &gt; -.</p><formula xml:id="formula_16">I 2tn N 1 Since ~[ ~( z n ) ~{ z n E B,}] 5 (6) Zz.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~~-(Z"; Y " ) and i A p ~7 L ( X n ;</head><p>U " ) are identical, where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PZ. 1 ( D n ) 5 2'</head><p>Proof of Lemma I : Let 0 stand for the indicator function. It follows from ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>) that the set Gn = Bn\Dn has probability at least a/2. Moreover, for each zn E G,,, e(z") 5 2 4 a . Let us fix an arbitrary S &gt; 0. The cardinality of G, is bounded as</p><formula xml:id="formula_17">N (GnI &gt; 2 exp {nc,} &gt; exp {n(c, -6)) = y,F;y,Pzn ( Z ~) O { Z " = f ( z n ) } ~( y n l z n ) provided n is large. 2- yn z"</formula><p>Therefore, we have found a set G, which has no fewer</p><formula xml:id="formula_18">. U { log -} = P [ i X * W . ( X " ; Y n ) 5 c]</formula><p>where the second equation is due to the fact that V(ynlzn) = Now we will show that the reliable transmission of 2 implies that V dominates 2. Due to Lemma 1 it follows immediately that 2 is dominated by W as well, which is the desired result. Lemma 2: Reliable transmissibility of 2 over W implies that the cascade channel V dominates 2. for every y &gt; 0, where X" places probability mass 1 / M on each codeword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of</head><p>Taking A4 = exp {n(c, -S)}, we know that there exist a sequence of ( n , M , 2t,/a) codebooks for large n E K for the channel V . Let us denote the distribution that puts equal probability mass on each of these codewords as Z:. Applying Lemma 3 with y = S to this sequence of codebooks, we get lim Pz-(B,) &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n-o3, n E K</head><p>Hence we can find some a &gt; 0 such that for all large n E K Pz-(B,) &gt; a.</p><p>( 5 )</p><p>Henceforth we will concentrate on those n E K for which <ref type="bibr" target="#b5">( 5 )</ref> is true.</p><p>where Y," is the output of V due to input Z:. Since t , goes to zero as n goes to infinity in K , we get Y,") 5 e, -26 = 0. (8)</p><p>1</p><p>Since the choice of S &gt; 0 and the subsequence K satisfying ( <ref type="formula">4</ref>) are arbitrary we have shown that the channel V dominates 2. This concludes the proof of Lemma 2 and Theorem 7.</p><p>Remark: Theorem 7 can be strengthened by replacing S in the definition of strict domination and domination by a vanishing sequence <ref type="bibr">(6,)</ref> such that nSn -+ CO.</p><p>To illustrate an application of Theorem 7, we now state the following lemma, which immediately leads to Theorem 3 whose proof is in the Appendix. In Section V, we derive more general results, dispensing with the information stability requirement.</p><p>Lemma 4: Under the conditions of Theorem 3</p><formula xml:id="formula_19">i) If T(2) &lt; C ( W ) , channel W strictly dominates source ii) If channel W dominates source 2, then T(2) 5 C ( W ) .</formula><p>To conclude this section we point out that for the example in Section I11 the channel dominates the source, as it should from the converse part of Theorem 7. As we noted earlier the source 2 and the channel with Bernoulli (+) input process X , for each n, have their respective information spectra concentrated at J ( n ) / n . Hence for any n, 6, and en, at least one of the probabilities in Definition 10 is identically zero. We now change the example in Section 111 slightly to illustrate a case of strict domination of a source by a channel whose capacity is strictly smaller than the minimum sourcecoding rate. The channel remains as before. But the source 2 is defined slightly differently: The Bernoulli subsource now has probability p &lt; l/2. The minimum source coding rate of 2 is (2/3)h(p), whereas the channel capacity is 1/3. Here h ( p ) denotes the binary entropy function in bits. To see that the channel strictly dominates 2, we will select the sequence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>From the law of large numbers it is easy to check that 2 cn = 0. 1 On the other hand, if we let X be Bernoulli (i), we know that, with probability 1, which implies that as long as In the special case of discrete memoryless channels and sources with a finite number of states, such as the example in Section 111, the notions of strict domination and domination boil down to the asymptotic comparison sequences of the deterministic (9) and It can be checked that in that special case, strict domination is equivalent to the existence of S &gt; 0 such that n for all sufficiently large n, whereas domination is equivalent to</p><formula xml:id="formula_20">n n i = l i=l</formula><p>Note that in this special case of nonstationary discrete memoryless channels, the question of transmissibility depends only on the behavior of the deterministic sequences ( <ref type="formula">9</ref>) and (10). Thus in the case, it is sensible to describe the source and the channel by those respective sequences, instead of the conventional scalar definitions of maximum source coding rate and capacity, which as we showed in Section 111 fail to predict whether transmissibility is possible. However, in the context of general (possibly information-unstable) sources and channels, we do not believe that the notions of domination can be substituted by simpler tests based on the comparison of a pair of deterministic scalar sequences characterizing the source and the channel, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. OPTIMISTIC CAPACITY AND THE SEPARATION THEOREM</head><p>We saw in Theorem 6 that the direct part of the separation theorem holds in complete generality, namely, if the minimum achievable source coding rate is strictly less than the channel capacity, then the source is reliably transmissible through the channel. Furthermore, we saw in Section 111 that the converse part of the separation theorem fails to hold for some source&lt;hannel pairs. In this section, we characterize those channels (and sources) for which the converse separation theorem always holds.</p><p>In this respect, it is of interest to define for any channel the source capacity CS, as the supremum of the minimum achievable source coding rates of the sources that can be reliably transmitted through the ~h a n n e l . ~ It follows from Theorem 4 that -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS(W) =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S U P</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ( S ) .</head><p>(1 1)</p><formula xml:id="formula_21">s transmissible over W</formula><p>The next result is an immediate consequence of the definitions and reveals the key role of the source capacity in checking the validity of the converse separation theorem.</p><p>Theorem 8: For any channel W the following are equiva-</p><formula xml:id="formula_22">lent: i) C s ( W ) = C ( W ) .</formula><p>ii) For every source 2 transmissible through W , we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T ( 2 ) 5 C ( W ) .</head><p>4A similar quantity, C E . where the supremum is taken over the subset of transmissible ergodic sources was mentioned in Section 11.</p><p>Proot First we show Cs 3 C for every channel. and According to Definition 6, for all y &gt; 0 there exists a sequence of ( 7 ~, M , E,) codes such that t, -+ 0 and lirn E, = 0,.</p><formula xml:id="formula_23">n-co, n € K log M C -r &lt; -. n</formula><p>Let X" be the distribution that puts mass Z/M on each of the M / 2 codewords with the lowest conditional probability of error. Clearly, all those codewords are different (for large enough 71), for otherwise, E" does not vanish. The process X is reliably transmissible through the channel (with an identity encoder and the decoder of the corresponding channel code). Its minimum achievable source coding rate satisfies, which enables us to conclude that CS 2 C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finally, it follows from Theorem 4 and the definition of Cs</head><p>We would like to give a characterization of those channels whose capacity is equal to the source-capacity, without recourse to source-channel transmission proper tie^.^ The main result of this section is that Cs is always equal to the socalled optimistic channel capacity, a concept which is closely connected with the conventional definition of channel capacity (Definition 6). The conventional definition [ 151 requires that good codes exist for all sufficiently large blocklengths; alternatively, we could require that good codes exist for infinitely many blocklengths (cf. Definition 6):</p><p>Definition 11: The optimistic capacity of channel W , C ( W ) , is the supremum of all the rates R for which there exist a sequence of ( n , M , e n ) codes such that that property ii) is equivalent to CS 5 C. Unlike C, c does not admit a simple expression such as that in Theorem 5 (cf. <ref type="bibr" target="#b14">[ 141)</ref>. For further discussion on the optimistic definition versus the conventional one see <ref type="bibr">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>.</p><p>The next result is a consequence of our main result in Section IV (Theorem 7).</p><p>Theorem 9: For any channel W , the source capacity is equal to the optimistic capacity C S ( W ) = E ( W ) .</p><p>Proof We first prove that Cs 2 E. To prove this inequality, we have to show that for every 6 &gt; 0 there exists a transmissible source X with a(X) 2 ?? -6.</p><p>By definition of optimistic capacity, for every 6 &gt; 0 there exists a subsequence K c N and a sequence of ( n , M , 'Note that if Cs &gt; C, then there may exist a source that cannot be transmitted reliably through the channel but whose sup-entropy rate is less than or equal C'S.</p><p>We construct X as follows: for n E K , X" is uniformly distributed over the codewords of the corresponding (AL M , f n ) code, and for n 6 K , X r L is deterministic. Clearly, X can be reliably transmitted over W and p ( X ) 3 ?? -6. This shows that CS 2 C.</p><p>We proceed now to show that Cs 5 ??. Pick 6 &gt; 0 and let Z be a transmissible source with a(2) 2 CS -(S/4). Such a source always exists by definition of CS. Thus there exists a subsequence K c N such that and since 2 is transmissible, there exists a channel input process X satisfying In particular, there exists a sequence rrL ---f 0 such that P -Z,ynl+rn(X"; Y " ) 5 Cs -6 5 r,, for all n, E K .</p><p>We now apply Feinstein's Lemma to conclude that there exists a sequence of ( n , M , t,) channel code, such that for 71 E K M = exp {n(Cs -26)) (t,-) and t, 2 7 , + exp ( -7 4 .</p><p>This proves that ?? 2 CS -26. Since 6 is arbitrary, c 2 Cs, Theorem 9 reveals an important operational characterization for the optimistic capacity which was unknown up to now. It implies that the classical statement of the separation theorem holds for a given channel (and every source) if and only if its optimistic capacity is equal to its conventional capacity. This property is indeed satisfied for most channels of interest. It is tantamount to requiring that no matter which subsequence of blocklengths we concentrate on, we cannot achieve a higher capacity. Another way to characterize this property in terms of the channel statistical description is given by Theorem 11 below.</p><p>Theorems 6 and 9 allow us to state the following generalized form of the joint sourcexhannel coding theorem.</p><p>Theorem 10: i) A source 2 is reliably transmissible through channel W ii) A source 2 is not reliably transmissible through W if Thus it is only in the case where the source-coding rate lies between the conventional and the optimistic channel capacities that recourse need be made to the conditions derived in Section IV. Note that it is not possible to strengthen the direct part by replacing C by E. To check that, the reader may easily and the proof is complete.</p><p>if T(2) &lt; C ( W ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T(2) &gt; C(W).</head><p>construct a suitable source which is not reliably transmissible through the channel of Section 111.</p><p>We now give an equivalent characterization of the condition C = based on the information spectrum of the channel. The proof is relegated to the Appendix.</p><p>Theorem 11: For any channel W , c ( W ) = C ( W ) if an only if for all S &gt; 0, and for all channel input processes X the following holds:</p><p>lirninf P -t X n W n ( X " ; Y " ) I C ( W ) + 6 &gt; 0. ( <ref type="formula">12</ref>)</p><formula xml:id="formula_24">n-oo [i.</formula><p>1</p><p>To conclude this section, we consider the dual problem; namely, under what condition on a given source can we guarantee that the converse to the separation theorem holds for any channel? Not surprisingly, the sought-after condition is that the optimistic and pessimistic (i.e., conventional) definitions of minimum achievable source coding rate coincide for that source.</p><p>Analogously to Cs(W), define for any source 2</p><formula xml:id="formula_25">T c ( 2 ) = infC(V)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V</head><p>where the infimum is over all channels over which 2 can be reliably transmitted. Analogously to Theorem 8 we have, Theorem 12: For any source 2, the following are equiva-</p><formula xml:id="formula_26">lent: i) Tc(2) = T ( 2 ) .</formula><p>ii) For every channel W over which the source is transmissible, we have T ( 2 ) 5 C ( W ) .</p><p>Proof: We first show that T c ( 2 ) 5 T ( 2 ) . Select b &gt; 0.</p><p>There exists a sequence of ( n , M ; e a ) source codes for 2 such that</p><formula xml:id="formula_27">I T(2) + s log M n and lim t , =<label>0</label></formula><p>Let W be a channel with the same input and output alphabets with cardinality exp{T(Z) + 6 ) . Let W" be such that each of the elements of a subset of size M of A" is mapped to itself with probability 1, and every other element is mapped to a common element, Clearly, C ( W ) 5 T ( 2 ) + 6 and 2 can be reliably transmitted over W . Since S is arbitrary, Finally, it is obvious that property ii) is equivalent to</p><p>Analogously to the phenomenon we saw for channels, Tc(2) is equal to T ( Z ) , the optimistic minimum achievable source-coding rate defined as in Definition 4, substituting for all suflciently large n by for infinitely many n. The proof of the following result can be found in the Appendix.</p><p>Theorem 13: For any source 2, T c ( 2 ) = r(2).</p><p>As an example, if the source 2 is stationary, then it satisfies the property that the optimistic and pessimistic definitions of the minimum source-coding rate coincide. To prove this, note that the minimum source-coding rate of any stationary source will be the essential infimum of the minimum source-coding n-cc TC(2) I T ( 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TC(2) 2 T ( Z ) .</head><p>rate over all the ergodic modes that comprise this source-this is a simple consequence of the ergodic decomposition of the limit of ((l/n)hzn(Z")}. Hence for any real number a, exists. The equality of optimistic and pessimistic definitions of the minimum source-coding rates follows immediately and hence the separation theorem is true whenever the source is stationary regardless of the channel. This result cannot be shown from the classical results of [21, [41.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 4</head><p>Using the fact that the channel is information stable and lim Cn(W") = C ( W ) we will show that (l/n)ixnwn ( X " ; Y " ) converges in probability to C ( W ) for some input process X . Indeed, information stability of the channel (Definition 3) implies that there exists an input process X such that for any A &gt; 0 and 7 &gt; 0, and for sufficiently large n n-m 1 . n P (I -.)C,(W") 5 -2,ynwn(X"; Y " ) 5 (1 + .)Cn(Wn) 2 1 -A. <ref type="bibr" target="#b13">(13)</ref> [ I We also know that for any 8 &gt; 0 and for all sufficiently large n, C ( W ) -8 5 Cn(Wn) I C ( W ) + 8. Using this and (13), and taking n suitably large we conclude that</p><formula xml:id="formula_28">L I (1 + .)(C(W) + 0) 2 1 -A.</formula><p>Since A, 7, and 0 are all arbitrary positive numbers, we have proved the convergence in probability of 1 (l/n)ipw-(X"; Y " ) to C ( W ) .</p><p>From [14], any pair ( X , Y ) satisfies This property can be applied for a particular subsequence of N too; the inf-information rate defined over any subsequence is greater than or equal to the inf-information rate defined over the whole sequence.</p><p>i) Let S = C -T , and c, = C -26 for all n. We see that the definition of strict domination (Definition 9) is satisfied with this choice, using Theorem 4.</p><p>ii) Fix y &gt; 0. In the definition of domination (Definition IO), choose c, = H ( 2 ) -7, for all n. It follows by definition of a(2) that there exists a subsequence K c N such that -2 c, 1 &gt; 0 Hence, for any S &gt; 0 there exists a channel input process X such that which contradicts our assumption that for all 6 2 0 and x <ref type="bibr" target="#b12">(12)</ref> is satisfied. This establishes the if part.</p><p>We proceed now to prove the other direction, again by a contradiction argument. Thus assume that c = c  Y " ) 5 C ( W ) -6</p><p>We will use a similar characterization of C (replacing the limit in (15) by liminf).</p><p>Lemma 5: For any 6 &gt; 0, there exists a channel input process X such that liminfP -z,ynn.n(X".; Y " ) 5 c ( W ) -5 = 0. (16) Proof: Construct the process X which is transmissible over W exactly as in the proof of Theorem 9. This process satisfies (cf. proof of Theorem 8), I n-m ['. n By Theorem 7, W dominates X and hence from Definition 10, there exists an input process X such that We now proceed with the proof of Theorem 11. Observe that by definition, c 2 C . Hence to show that condition <ref type="bibr" target="#b12">(12)</ref> implies ?? = C , it is enough to show that it implies 5 C. By way of contradiction, assume that ( <ref type="formula">12</ref>) is satisfied for every S &gt; 0 and every X, yet C &gt; C + y for some y &gt; 0. Now from Lemma 5 , we know that there exists an input process X such that Since by assumption c 2 C + y, (18) implies that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 13</head><p>We start by proving that TC(Z) I a z ) . Let W be a channel with input and output alphabets both equal to F satisfying</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W"(PIZ")</head><p>for .zn E G,,</p><formula xml:id="formula_29">1 for Z" G, IFI" ' Z" = zn ={l,</formula><p>for n E K and W n = identity channel on F" for n 6 K .</p><p>Clearly 2 can be reliably transmitted over W , and C ( W ) = -T ( 2 ) + 6. Since 6 is arbitrary, we have proved (20).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3 :</head><label>3</label><figDesc>Let 2 and W be an information-stable source and an information-stable channel, respectively. Assume that lim,,,C,(W") exists (cf. Definition 3). Then a) T ( 2 ) &lt; C ( W ) implies that 2 can be reliably transmitted over W .b) If 2 can be reliably transmitted over W , then T ( 2 ) 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 2 :</head><label>2</label><figDesc>We will verify the condition of domination in Definition 10. Let us fix an arbitrary subsequence K of the sequence of positive integers. Let us consider a sequence { c , } , ~K of nonnegative real numbers such that the sequence of sets { B n } n E ~ W ( Y I f n ( zn 1 ).satisfies the condition than exp{n(c, -S)} elements and the probability of error when any of these elements is used as a codeword is at most 2tn/a, which goes to zero as n increases in K . In other words, for the cascade channel V , we have found a sequence of ( n . exp {c(c, -S)}, 2c,/a) codes for large n E K . Now we are in a position to invoke the central result in the new converse to the coding theorem proved in[14]. This result gives a simple lower bound to the average probability of error of any ( n , M , f ) code.Lemma 3 [14]: Every ( n , M , t) code for a conditional distribution @, -z s n ~, . ( X n ;Y " ) 5 -1 o g M -y -exp(-yn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>zl;-nwn(Xin; Y n ) 5 c, -s = 0.inf-information rate of ( X , Y ) , defined over the have, using subsequence is greater than Or to rr(z&gt; -7. we yet there exist &gt; 0, an input process X , a subsequence K c N and a sequence r, -+ 0 such that lim Cn = C n-m that ( X " ; Y " ) &lt; C + S for all n E K.1 Using Feinstein's Lemma we conclude that there exists a sequence of ( n , M , e n ) codes for the channel W satisfying, on the subsequence K M = exp { n(c + :)}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>inequality we have to show that for every S &gt; 0 there exists a channel W over which Z is transmissible, with C ( W ) 5 r(2) + S. By definition of r ( Z ) , there exist a subsequence K c N and a sequence of ( n , M , c n ) source codes for 2 such that and lim tn = 0. n-m, n E K Let G, be any subset of F n (where F is the alphabet of 2) which includes the ( n , 111, f ) source codebook, and has cardinality lGnl = exp {n[T(z) + 611.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 ergodic, block transmissible It was proved by Kieffer [ 111 that for a weakly continuous ergodic channel (see [ l 11 for definitions) CE equals the channel capacity C , and that C equals the supremum of mutual information rate over all stationary inputs. This amounts to the separation theorem for this class of channels and ergodic sources. Kieffer [ 1 I] also shows that for the class of weakly continuous stationary channels, CE equals the information quantile capacity introduced by Winkelbauer [ 121.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(14)  over subsequence K , we see that infinformation rate defined over K is less than or equal to C, and hence C 2 a(2) -y, where the choice is y is arbitrary. Observe from Theorem 5 that if a channel has capacity C,</figDesc><table><row><cell>l i m s u p -I ( P ; Y n ) 5 C.</cell></row><row><cell>n-cc</cell></row><row><cell>This concludes the proof.</cell></row></table><note><p><p>Applying W and Proof of Theorem I 1 t , 5 7 , + exp (-n:)</p>implying that ?? 2 C + (S/2), in contradiction with (19). W then for any S &gt; 0 there exists a channel input process X such that</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We proceed to prove that Tc(2) 2 T ( 2 ) . We have to show that the capacity of every channel over which 2 is transmissible, is at least T ( 2 ) . By definition of r(2) and the source coding results of [13], for every 6 &gt; 0 we have</p><p>Now let W be a channel over which 2 is transmissible. Since W dominates 2, (21) implies that there exists an input process X such that Using again Feinstein's Lemma as in the proofs of Theorem 9 and 11, we conclude that T ( 2 ) -6 is an achievable rate for the channel W , whenever 2 is transmissible over W . <ref type="bibr">Since 6</ref> w is arbitrary, the proof is complete.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">379423</biblScope>
			<date type="published" when="1948-07">July 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">General formulation of Shannon&apos;s basic theorems of information theory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dobrushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMS Transl</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="323" to="438" />
			<date type="published" when="1963">1963</date>
			<publisher>AMs</publisher>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pinsker</surname></persName>
		</author>
		<title level="m">Information and Information Stability of Random Variables and Processes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On Shannon theorem and its converse for sequence of communication schemes in the case of abstract random variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<publisher>Holden-Day</publisher>
			<pubPlace>Oakland, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">Trans. 3rd Prague Con$ on Information Theory, Statistical Decision Functions, Random Processes (Czechoslovak Academy of Sciences</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="page" from="285" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Information theory without ergodicity assumptions: Some new results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vembu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Shannon theory of information in the case of continuous signals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="1956-09">Sept. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coding theorems for a discrete source with a fidelity criterion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Nut. Conv. Rec</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="163" />
			<date type="published" when="1959-03">Mar. 1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple access channels with arbitrary correlated sources</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="648" to="657" />
			<date type="published" when="1980-11">Nov. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
		<title level="m">Ergodic and Information Theory (Benchmark Papers in Electrical Engineering and Computer Science)</title>
		<imprint>
			<publisher>Dowden, Hutchinson and Ross, Inc</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information theory and ergodic theory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Contr. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Block coding for weakly continuous channels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kieffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="721" to="727" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the asymptotic rate of non-ergodic information sources</title>
		<author>
			<persName><forename type="first">K</forename><surname>Winkelbauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybemetika</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="127" to="148" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximation theory of output statistics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="752" to="772" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general formula for channel capacity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1147" to="1157" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Information Theory: Coding Theorems for Discrete Memoryless Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Informarion Theory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Ash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965. 1990. 1981</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
