<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 SIMPLE SPECTRAL GRAPH CONVOLUTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 SIMPLE SPECTRAL GRAPH CONVOLUTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have drawn significant attention and become leading methods for learning graph representations. The most GCNs suffer the performance loss when the depth of the model increases. Similarly to CNNs, without specially designed architectures, the performance of a network degrades quickly with increased depth. Some researchers argue that the required neighbourhood size and neural network depth are two completely orthogonal aspects of graph representation. Thus, several methods extend the neighbourhood by aggregating k-hop neighbourhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, high computation and storage costs. In this paper, we use a modified Markov Diffusion Kernel to derive a variant of GCN called Simple Spectral Graph Convolution (S 2 GC). Our spectral analysis shows that our simple spectral graph convolution used in S 2 GC is a trade-off of low-pass and high-pass filter which captures the global and local contexts of each node. We provide two theoretical claims which demonstrate that we can aggregate over a sequence of increasingly larger neighborhoods compared to competitors while limiting severe oversmoothing. Our experimental evaluation demonstrates that S 2 GC with a linear learner is competitive in text, node and graph classification tasks. Moreover, S 2 GC is comparable to other state-of-the-art methods for node clustering and community prediction tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the past few years, the rise and application of deep learning have successfully promoted the research of computer vision and data mining. Although these deep learning methods have been applied to extract the features on the Euclidean lattice (spatial data) with great success, the data in many practical scenarios lies on non-Euclidean structures. Processing non-Euclidean structures is a challenge for deep learning methods. By defining a convolution operator between the graph and signal, Graph Convolutional Networks (GCNs) generalize Convolutional Neural Networks (CNNs) to graph-structured inputs which contain attributes. Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b9">(Gilmer et al., 2017)</ref> unify the graph convolution as two functions: the transformation function and the aggregation function. MPNN iteratively propagates node features based on the adjacency structure of graph in a number of rounds.</p><p>Despite their enormous success in many applications like social media, traffic analysis, biology, recommendation systems and even computer vision, most of the current GCN models use shallow architectures because many of the recent models such as <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref> achieve their best performance with 2-layer models. In other words, 2-layer GCN models aggregate nodes in twohops neighbourhood and thus have no ability to extract information in k-hops neighbourhoods for k &gt; 2. Moreover, stacking more layers and adding a non-linearity tends to degrade the performance of these models. Such a phenomenon is called oversmoothing <ref type="bibr">(Li et al., 2018a)</ref>, which suggests that as the number of layers increases, the representations of the nodes in GCNs are inclined to converge to a certain value and become less distinct from one another. Even adding residual connections, an effective trick for training very deep neural networks in computer vision, merely slows down the oversmoothing issue <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref> in GCNs. It appears that deep GCN models gain nothing but the performance degradation from the deep architecture.</p><p>One solution for that is only widening the aggregation function but keeping the same transformation function because the required neighbourhood size and neural network depth can be regarded as two orthogonal aspects of design. SGC <ref type="bibr" target="#b30">(Wu et al., 2019)</ref> attempts to capture the context from K-hops neighbours in the graph by applying the K-th power of the graph convolution matrix in a single neural network layer. This scheme is also used for attributed graph clustering <ref type="bibr" target="#b35">(Zhang et al., 2019)</ref>. However, SGC is also suffering from oversmoothing as K ? ? as shown in Theorem 1. PPNP and APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref> replace the power of the graph convolution matrix with the Personalized PageRank matrix to solve the oversmoothing problem. Although APPNP relieve the oversmoothing problem, it employs a non-linear operation which requires costly computation of the derivative of the filter due to the non-linearity over the multiplication of feature matrix with learnable weights. In contrast, we show that our approach enjoys a free derivative computed in the feedforward step thanks to the use of linear model. Furthermore, APPNP aggregates over multiple k-hop neighborhoods but the weighting scheme favors either global or local context making it difficult if not impossible to find the balancing parameter. In contrast, our approach does aggregate over k-hop neighborhoods in a balanced manner as shown later in the text.</p><p>GDC <ref type="bibr">(Klicpera et al., 2019b)</ref> further extends APPNP by generalizing Personalized PageRank <ref type="bibr" target="#b21">(Page et al., 1999)</ref> to an arbitrary graph diffusion process. GDC, technically, has stronger expressive power than SGC <ref type="bibr" target="#b30">(Wu et al., 2019)</ref>, PPNP and APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref> but it leads to a dense transition matrix which makes the computation and space storage intractable for large graphs although authors suggest that the shrinkage method can be used to sparsify the generated transition matrix by ignoring small values.</p><p>To solve the above issues, we propose a Simple Spectral Graph Convolution (S 2 GC) network for node clustering (semi-supervised and unsupervised setting), node classification and graph classification. By analyzing Markov Diffusion <ref type="bibr">Kernel (Fouss et al., 2012)</ref>, we obtain a very simple and effective filter: we aggregate k-step diffusion matrices over k = 0, ? ? ? , K steps which is equivalent to aggregating over neighborhoods of various sizes. Moreover, we show that our design incorporates larger neighborhoods compared to SGC thus coping better with oversmoothing. We explain that limiting over-dominance of the largest neighborhoods in the aggregation step is a desired approach to limit oversmoothing while preserving large context of each node. We also show that in spectral analysis that S 2 GC is a trade-off between the low-and high-pass filters which leads to capturing the global and local contexts of each node. Moreover, we show how S 2 GC and APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref> are related and explain why S 2 GC captures a range of neighborhoods better than APPNP. Our experimental results include node clustering, unsupervised and semi-supervised node classification, node property prediction and supervised text classification. We show that S 2 GC is highly competitive often significantly outperforming state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Notations. Let G = (V, E) be a simple and connected undirected graph with n nodes and m edges. We use {1, ? ? ? , n} to denote the node index of G, and d j denote the degree of node j in G. Let A be the adjacency matrix and D the diagonal degree matrix. Let A = A + I n denote the adjacency matrix with added self-loops and the corresponding diagonal degree matrix D where</p><formula xml:id="formula_0">I n ? S n</formula><p>++ is an identity matrix. Finally, let X ? R n?d denote the node feature matrix and each node v is associated with a d-dimensional feature vector X v . The normalized graph Laplacian matrix is defined as</p><formula xml:id="formula_1">L = I n -D -1/2 AD -1/2 ? S n</formula><p>+ , a symmetric positive semidefinite matrix with eigendecomposition U?U . Here ? is a diagonal matrix of the eigenvalues of L, and U ? R n?n is a unitary matrix that consists of the eigenvectors of L. <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>. We consider spectral convolutions on graphs defined as the multiplication of a signal x ? R n with a filter g ? parameterized by ? ? R n in the Fourier domain:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral Graph Convolution</head><formula xml:id="formula_2">g ? (L) * x = Ug * ? (?)U x,<label>(1)</label></formula><p>where the parameter ? ? R n is a vector of spectral filter coefficients. We can understand g ? as a function operating on eigenvalues of L, that is g * ? (?). To avoid eigendecomposition, g ? (?) can be approximated by a truncated expansion in terms of Chebyshev polynomials T k (?) up to the K-th order <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>:</p><formula xml:id="formula_3">g * ? (?) ? K-1 k=0 ? k T k ( ?),<label>(2)</label></formula><p>with a rescaled ? = 1 2?max ? -I n where ? max denotes the largest eigenvalue of L and ? ? R K is now a vector of Chebyshev coefficients.</p><p>Vanila Graph Convolutional Network (GCN) <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref>. The vanilla GCN is a first-order approximation of spectral graph convolutions. If one sets K = 1, ? 0 = 2, and ? 1 = -1 for Eq. 2 they obtain the convolution operation g ? (L) * x = (I + D -1/2 AD -1/2 )x. Finally, by the renormalization trick, replacing the matrix I + D -1/2 AD -1/2 by a normalized version</p><formula xml:id="formula_4">T = D -1/2 A D -1/2 = (D + I n ) -1/2 (A + I n )(D + I n ) -1/2</formula><p>leads to the GCN layer with ?, a non-linear function, e.g. ReLU:</p><formula xml:id="formula_5">H (l+1) = ?( TH (l) W (l) ),<label>(3)</label></formula><p>Graph Diffusion Convolution (GDC) <ref type="bibr">(Klicpera et al., 2019b)</ref>. A generalized graph diffusion is given by the diffusion matrix:</p><formula xml:id="formula_6">S = ? k=0 ? k T k ,<label>(4)</label></formula><p>with the weighting coefficients ? k and the generalized transition matrix T. Eq. 4 can be regarded as related to the Taylor expansion of matrix-valued functions. Thus, the choice of ? k and T k must at least ensure that Eq. 4 converges. <ref type="bibr">Klicpera et al. (2019b)</ref> provide two special cases as low-pass filters, ie. heat kernel and the kernel based on random walk with restarts. If S denote the adjacency matrix and D be the diagonal degree matrix of S then the corresponding graph diffusion convolution is defined as D -1/2 SD -1/2 x. Note that ? k can be a learnable parameter, or it can be chosen in one or another way. Many works use expansion in Eq. 4 but different choices of ? k realise very different filters making each method unique. One example may be Chebynet <ref type="bibr" target="#b35">(Zhang et al., 2019)</ref>.</p><p>Simple Graph Convolution (SGC) <ref type="bibr" target="#b30">(Wu et al., 2019)</ref>. A classical MPNN <ref type="bibr" target="#b9">(Gilmer et al., 2017)</ref> averages in each layer the hidden representations among 1-hop neighbors. This implies that each node in the k-th layer obtains feature information from all nodes that are k-hops away in the graph. By hypothesizing that the non-linearity between GCN layers is not critical, SGC captures information from k-hops neighbourhood in the graph by applying the K-th power of the transition matrix in a single neural network layer. The SGC can be regarded as a special case of GDC without nonlinearity and without the normalization by D -1/2 if we set ? k = 1 and ? i&lt;K = 0 for Eq. 5, and T = T: ? = softmax( T K XW).</p><p>(5) Although SGC is an efficient and effective method, increasing Kleads to oversmoothing. Thus, SGC uses similar K number of layers as GCN. We provide Theorem 1 to demonstrate that oversmoothing is a result of convergence to the stationary distribution in graph diffusion as time t ? ?. Theorem 1. (Chung &amp; Graham, 1997) Let ? 2 denote second largest eigenvalue of transition matrix T = D -1 A of a non-bipartite graph, p(t) be the probability distribution vector and ? the stationary distribution. If walk starts from the vertex i , p i (0) = 1, then after t steps for every vertex:</p><formula xml:id="formula_7">|p j (t) -? j | ? d j d i ? t 2 ,<label>(6)</label></formula><p>APPNP. <ref type="bibr">Klicpera et al. (2019a)</ref> proposed APPNP which uses Personalized PageRank to derive a fixed filter of order K. Let f ? (X) denote the output of a two-layer fully connected neural network on the feature matrix X, the PPNP model is defined as</p><formula xml:id="formula_8">H = ?I n -(1 -?) T -1 f ? (X).</formula><p>To avoid calculating the inverse of matrix T, <ref type="bibr">Klicpera et al. (2019a)</ref> also proposes Approximate PPNP (APPNP) which replaces the costly inverse with an approximation derived by the truncated power iteration:</p><formula xml:id="formula_9">H (l+1) = (1 -?) TH (l) + ?H (0) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">H (0) = f ? (X) = ReLU(XW)</formula><p>. By decoupling the feature transformation and propagation steps, PPNP and APPNP aggregate information from multi-hop neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, firstly we briefly discuss the spectral analysis and graph partitioning, and conclude the small eigenvalues of a Laplacian matrix control global clustering which partitions the graph into </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SPECTRAL ANALYSIS AND GRAPH PARTITIONING</head><p>Our design follows Claims I and II described in Section A.3.1 which includes their proofs.</p><p>Claim I. Our filter, by design, will give the highest weight to the closest neighborhood of a node as neighborhoods</p><formula xml:id="formula_11">N of diffusion steps k = 0, ? ? ? , K obey: N ( T 0 ) ? N ( T 1 ) ? ? ? ? ? N ( T K ) ? N ( T ? ).</formula><p>That is, smaller neighbourhoods belong to larger neighbourhoods too.</p><p>Claim II. As K ? ?, the ratio of energies contributed by S 2 GC to SGC is 0. Thus, the energy of infinite-dimensional receptive field (largest k) will not dominate the sum energy of our filter. Thus, S 2 GC can incorporate larger receptive fields without overbearing contributions of smaller receptive fields. This is substantiated by Table <ref type="table" target="#tab_5">8</ref> where we achieve K = 16 while SGC achieves K = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MARKOV DIFFUSION KERNEL</head><p>Two nodes are considered similar when they are diffused in a similar way through the graph, and therefore when they influence the other nodes in a similar manner <ref type="bibr" target="#b8">(Fouss et al., 2012)</ref>. In other words, two nodes are close if they are in the same cluster which has a consistent local structure. More precisely, the diffusion distance at time K between nodes i and j is defined as follows:</p><formula xml:id="formula_12">d ij (K) = x i (K) -x j (K) 2 2 ,<label>(8)</label></formula><p>where the average visiting rate x i (K) after K steps for a process that started at time k = 0 is computed as follows:</p><formula xml:id="formula_13">x i (K) = 1 K K k=1 T k x i (0).<label>(9)</label></formula><p>By defining</p><formula xml:id="formula_14">Z(K) = 1 K K k=1</formula><p>T k , we reformulate Eq. 8 as a metric given as:</p><formula xml:id="formula_15">d ij (K) = Z(K)(x i (0) -x j (0)) 2 2 .</formula><p>(10) The underlying feature map of Markov Diffusion Kernel (MDK) is given as Z(K)x i (0) for node i.</p><p>The effect of the linear projection Z(K) (filter) acting on spectrum as f (?) = 1 K K k=0 ? k (we sum from 0 to include self-loops) is plotted in Figure <ref type="figure">1</ref>, from which we observe the following properties: (i) Z(K) preserves leading (large) eigenvalues of T and (ii) the higher K is the stricter the low-pass filter becomes but the filter also preserves the high frequency. In other words, as K grows, this filter includes larger and larger neighborhood but also maintains the closest locality of nodes. Note that L = I -T where L is the normalized Laplacian matrix and T is the normalized adjacency matrix. Thus keeping large positive eigenvalues for T equals keeping small eigenvalues for L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SIMPLE SPECTRAL GRAPH CONVOLUTION</head><p>Based on the aforementioned Markov Diffusion Kernel, we include self-loops and we propose the Simple Spectral Graph Convolution (S 2 GC) network with a softmax after a linear layer:</p><formula xml:id="formula_16">? = softmax( 1 K K k=0 T k XW). (11) As K ? ?, H = ? k=1</formula><p>T k is the optimal diffused representation of the normalized Laplacian Regularization problem <ref type="bibr" target="#b3">(Chapelle et al., 2006)</ref>:</p><formula xml:id="formula_17">min H 1 2 n i,j=1 A ij h i ? d i - h j d j 2 2 + h i -x i 2 2 , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where each vector h i denotes the i-th row of H. However, the infinite expansion resulting from Eq. 12 in fact is suboptimal due to oversmoothing (Table <ref type="table" target="#tab_5">8</ref> shows this). In contrast, we include in Eq. 11 a self-loop T 0 = I, the ? ? [0, 1] parameter (Table <ref type="table" target="#tab_6">9</ref> evaluates its impact) to balance the node's self information vs. consecutive neighborhoods, and we consider finite K. We generalize the Eq. 11 as:</p><formula xml:id="formula_19">? = softmax 1 K K k=1 (1 -?) T k X + ?X W . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>Relation of S 2 GC to GDC. GDC uses the entire filter matrix S (n ? n) as S is then re-normalized by its degree. <ref type="bibr">Klicpera et al. (2019b)</ref> explain that 'Most graph diffusions result in a dense matrix S.</p><p>In contrast, our approach is simply computed as (</p><formula xml:id="formula_21">K k=1 T k X)W (plus self-loop)</formula><p>where X is of size (n ? d) where d n, n and d being the number of nodes and features, respectively. The T ? X step is computed as T</p><formula xml:id="formula_22">? ( T ? (? ? ? ( TX) ? ? ? )</formula><p>) which requires t matrix-matrix multiplications between matrices of size n ? n and n ? d. Thus, S 2 GC can handle extremely large graphs as it does not need to sparsify dense filter matrices as GDC.</p><p>Relation of S 2 GC to APPNP. Let us define H 0 = XW as we use the linear step in our S 2 GC. Then and only then, for l = 0 and H 0 = XW, APPNP expansion yields</p><formula xml:id="formula_23">H 1 = (1 -?) TXW + ?XW = ((1 -?) T + ?I)XW which is equal to our Z(1)XW = ( K k=0 T k )XW = TX + X = ( T + I)XW if ? = 0.5, K = 1, except for scaling (constant) of H 1 .</formula><p>In contrast, for l = 1 and general case H</p><formula xml:id="formula_24">0 = f (X; W), APPNP yields H 2 = (1-?) 2 T 2 f (X; W)+ (1-?)? Tf (X; W)+?f (X; W) from which is is easy to note specific weight coefficients (1-?) 2 ,</formula><p>(1 -?)? and ? associated with 2-, 1-, and 0-hops. This shows that the APPNP expansion is very different to S 2 GC expansion in Eq. 13. In fact, S 2 G and APPNP are only equivalent if ? = 0.5, K = 1 and a linear transformation f is used.</p><p>Moreover, APPNP assumes H 0 = f (X; W) = ReLU(XW), thus their optimizer has to backpropagate through f (X; W) to obtain ?f ?W and multiply this with the above expansion e.g.,</p><formula xml:id="formula_25">?H 2 ?W = (1 -?) 2 T 2 f (X; W) + (1 -?)? Tf (X; W) + ?f (X; W).</formula><p>In contrast, we use the linear function XW. Thus, ?XW ?W yields X. Thus, the multiplication of our expansion with X for the backprop step is in fact obtained in the forward pass which makes our approx very fast for large graphs.</p><p>Relation of S 2 GC to AR. The AR filter <ref type="bibr" target="#b18">(Li et al., 2019)</ref> uses the regularized Laplacian kernel <ref type="bibr" target="#b26">(Smola &amp; Kondor, 2003)</ref> which differs from used by us (modified) Markov diffusion kernel. Specifically, the regularized Laplacian kernel uses the negated Laplacian matrix yielding -L as follows: </p><formula xml:id="formula_26">K L = ? k=0 ? k (-L) k = (I + ?L) -1 where L = I-T which is related to the von Neumann diffusion kernel K vN = ? k=0 ? k A k . In contrast, the Markov diffusion kernel K MD (K) = Z(K)Z T (K) where Z(K) = 1 K K k=1 T k , where T = D -1/2 AD -1/2 .</formula><formula xml:id="formula_27">? ( T ? (? ? ? ( TX) ? ? ? ))</formula><p>. The computational cost is the O(K|E|d + Knd). Each sparse matrix multiplication TX costs |E|d and we need K such multiplications while Knd realises summation over filters and nd is the cost of adding features X.</p><p>In contrast, the storage cost of GDC is approximately O(n 2 ) and the computational cost is approximately O(K|E|n) where n is the node numbers, K is the order of terms and |E| is the number of graph edges. APPNP, SGC and S 2 GC have much lower cost than GDC. Kindly note K|E|d Knd and n d. We found that APPNP, SGC and S 2 GC have similar computational and storage costs in the forward stage. Note that the d in APPNP is not the dimension of features X but f (X), which is the number of categories.</p><p>For the backward stage including computing the gradient of the classification step, the computational costs of GDC, SGC and S 2 GC are independent of K and |E| because the graph convolution for these methods does not require backprop (grad. is computed in the forward step). In contrast, APPNP requires backprop as explained earlier.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the computational and storage costs of several methods. Table <ref type="table">2</ref> demonstrates that APPNP is over 66? slower than S 2 GC on large scale Products dataset (OGB benchmark) despite, for fairness, we use the same basic building blocks of PyTorch among compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed method on four different tasks: node clustering, community prediction, semi-supervised node classification and text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NODE CLUSTERING</head><p>We compare S 2 GC with three kinds of clustering methods: (i) Methods that only use node features: k-means and spectral clustering (spectral-f) that constructs a similarity matrix with the node features by a linear kernel. (ii) Structural clustering methods that only use graph structures: spectral clustering (spectral-g) that takes the node adjacency matrix as the similarity matrix, DeepWalk <ref type="bibr" target="#b24">(Perozzi et al., 2014)</ref>, and (iii) Attributed graph clustering methods that utilize both node features and graph structures: Graph Autoencoder (GAE) and Graph Variational Autoencoder (VGAE) <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref>, and Adversarially Regularized Graph Autoencoder (ARGE), Variational Graph Autoencoder (ARVGE) <ref type="bibr" target="#b22">(Pan et al., 2018)</ref> and AGC <ref type="bibr" target="#b35">(Zhang et al., 2019)</ref>. To evaluate the clustering performance, three performance measures are adopted: clustering Accuracy (Acc), Normalized Mutual Information (NMI) and macro F1-score (F1). We run each method 10 times on four datasets: Cora, CiteSeer, PubMed, and Wiki, and we report the average clustering results in Table <ref type="table" target="#tab_1">3</ref>, where top-1 results are highlighted in bold. To adaptively select the order k, we use the clustering performance metric: internal criteria based on the information intrinsic to the data alone <ref type="bibr" target="#b35">Zhang et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMMUNITY PREDICTION</head><p>We supplement our social network analysis by using S 2 GC to inductively predict the community structure on Reddit, a large scale dataset as shown in Table <ref type="table" target="#tab_8">11</ref> which cannot be processed by the vanilla GCN Kipf &amp; Welling (2016) and GDC <ref type="bibr">(Klicpera et al., 2019b)</ref> due to the memory issues.</p><p>On the Reddit dataset, we train S 2 GC with L-BFGS using no regularization, and we set K = 5 and ? = 0.05. We evaluate S 2 GC inductively according to protocol <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>. We train S 2 GC on a subgraph comprising only training nodes and test on the original graph. On all datasets, we tune the number of epochs based on both convergence behavior and the obtained validation accuracy.</p><p>For Reddit, we compare S 2 GC to the reported performance of supervised and unsupervised variants of GraphSAGE <ref type="bibr" target="#b10">(Hamilton et al., 2017)</ref>, FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, SGC <ref type="bibr" target="#b30">(Wu et al., 2019)</ref> and DGI <ref type="bibr" target="#b29">(Velickovic et al., 2019)</ref>. Table <ref type="table" target="#tab_2">4</ref> also highlights the setting of the feature extraction step for each method. Note that S 2 GC and SGC involve no learning because they do not learn any parameters to extract features. The logistic regression is used as a classifier for both unsupervised and no-learning approaches to train with labels afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NODE CLASSIFICATION</head><p>For the semi-supervised node classification task, we apply the standard fixed training/validation/testing split <ref type="bibr" target="#b33">(Yang et al., 2016)</ref> on the Cora, Citeseer, and Pubmed datasets, with 20 nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. For baselines, We include three state-of-the-art shallow models: GCN (Kipf &amp; Welling, 2016), GAT <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref>, FastGCN <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref>, Mixhop (Abu-El-Haija et al., 2019), SGC <ref type="bibr" target="#b30">(Wu et al., 2019)</ref>, DGI <ref type="bibr" target="#b29">(Velickovic et al., 2019)</ref> and GIN <ref type="bibr">(Xu et al., 2018a)</ref>. We use the Adam SGD optimizer (Kingma &amp; Ba, 2014) with a learning rate of 0.02 to train S 2 GC. We set ? = 0.05 and K = 16 on all datasets. To determine K and ?, we used the MetaOpt package <ref type="bibr" target="#b2">Bergstra et al. (2015)</ref> with 20 steps to meta-optimize hyperparameters on the validation set of Cora.</p><p>Following that, we fixed K = 16 and ? = 0.05 across all datasets so K and ? are not tuned to individual datasets at all. We will discuss the influence of ? and K later.</p><p>To evaluate the proposed method on large scale benchmarks, we use ogbn-arxiv, ogbn-mag and ogbn-products to demonstrate the comparison among the proposed method, SGC, GraphSage, GCN, MLP and Softmax (multinomial Regression), as shown in Table <ref type="table" target="#tab_3">6</ref>. In these three datasets, our method outperforms SGC consistently. In ogbn-arxiv and ogbn-products, we can observe our method cannot outperforms GCN and GraphSage while MLP outperforms softmax classifier significantly. Thus we argue in these two datasets, MLP plays a more important role than graph convolution. To prove this point, we also conduct the experiment (S 2 GC+MLP) that we use MLP to replace the linear classifier and obtain a more powerful S 2 GC. In ogbn-mag, MLP barely helps our method because the performance of MLP is close to the one of Softmax. In other two datasets, the significant improvements are easy to observe that S 2 GC with MLP is more close to GCN and even outperform it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TEXT CLASSIFICATION</head><p>Text classification predicts the labels of documents. <ref type="bibr" target="#b34">Yao et al. (2019)</ref> use a 2-layer GCN to achieve state-of-the-art results by creating a corpus-level graph which treats both documents and words as nodes in a graph. Word-to-word edge weights are given by Point-wise Mutual Information (PMI) and word-document edge weights are given by normalized TF-IDF scores.</p><p>We ran our experiments on five widely used benchmark corpora including movie review (MR), 20-Newsgroups (20NG), Ohsumed, R52 and R8 of Reuters 21578. We first preprocessed all the datasets by cleaning and tokenizing text as <ref type="bibr" target="#b11">(Kim, 2014)</ref>. We then removed stop words defined in NLTK6 and low frequency words appearing less than 5 times for 20NG, R8, R52 and Ohsumed. We compare two state-of-the-art models with our method: GCN (Kipf &amp; Welling, 2016) and SGC <ref type="bibr" target="#b30">(Wu et al., 2019)</ref>. The statistics of the preprocessed datasets are summarized in Table <ref type="table" target="#tab_10">12</ref>. Table <ref type="table" target="#tab_4">7</ref> shows that an S 2 GC rivals their models on 5 benchmark datasets. We give the parameters setting in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">A DETAILED COMPARISON WITH VARIOUS NUMBERS OF LAYERS AND ?</head><p>Table <ref type="table" target="#tab_5">8</ref> summaries the results for the deep models with various numbers of layers (or K for SGC and our methods). We observe that on Cora, Citeseer and Pubmed that our method consistently obtains the best performance with K = 16 equivalent of 16 layers. Overall, the results suggest that with S 2 GC, we can aggregate over larger neighborhood than SGC thus we suffer less from oversmoothing. On the other hand, the performance of GCN and SGC drops rapidly as the number of layers exceeds 32 due to oversmoothing.</p><p>Table <ref type="table" target="#tab_6">9</ref> summaries the results for the proposed method for various ? ranged from 0 to 0.15. As we can observe, ? only slightly improves the performance of S 2 GC. Thus. balancing self-loop by ? with other filters of consecutively larger receptive fields is useful but the self-loop is not mandatory. 83.4 ? 7.5 72.6 ? 4.9 79.8 ? 2.4 72.1 ? 5.1 DiffPool 85.0 ? 10.3 75.1 ? 3.5 78.9 ? 2.3 72.6 ? 3.9 S 2 GC 85.1 ? 7.4 75.5 ? 4.1 80.2 ? 1.3 72.9 ? 4.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">GRAPH CLASSIFICATION</head><p>We report the average accuracy of 10-fold cross validation on a number of common benchmark datasets (as shown in Table <ref type="table" target="#tab_7">10</ref>), where we randomly sample a training fold to serve as a validation set. We only make use of discrete node features. In case they are not given, we use one-hot encodings of node degrees as feature input. We note that graph classification is a task highly dependent on the global pooling strategy. There exist methods that apply sophisticated mechanisms for this step. However, with a simple average pooling and a highly scalable S 2 GC model, we comfortably outperform all methods on MUTAG, Proteins and IMDB-Binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have proposed Simple Spectral Graph Convolution (S 2 GC), a method based on the Markov Diffusion Kernel (Section 3.2) whose feature maps emerge from the normalized Laplacian Regularization problem (Section 3.3). The theoretical analysis shows that S 2 GC benefits from the right level of trade-off in the aggregation over consecutively larger receptive fields. We have shown there exist a connection between S 2 GC and other methods such as SGC, APPNP and JKN by analyzing spectral properties and implementation of each model. However, ac our Claims I and II show, we have designed a filter with unique properties to capture larger context while limiting oversmoothing. S 2 GC inherits many of the strengths of spectral methods and copes better with oversmoothing (larger K si achieved). We have conducted extensive and rigorous experiments that have shown that S 2 GC is competitive and frequently outperforms many state-of-the-art methods on unsupervised, semi-supervised and supervised tasks and several popular datasets. A SUPPLEMENTARY MATERIAL</p><p>A.1 NODE CLUSTERING For S 2 GC and AGC, we set max iterations to 60. For other baselines, we follow the parameter settings in the original papers. In particular, for DeepWalk, the number of random walks is 10, the number of latent dimensions for each node is 128, and the path length of each random walk is 80. For DNGR, the autoencoder is of three layers with 512 neurons and 256 neurons in the hidden layers respectively. For GAE and VGAE, we construct encoders with a 32-neuron hidden layer and a 16-neuron embedding layer, and train the encoders for 200 iterations using the Adam optimizer with learning rate equal 0.01. For ARGE and ARVGE, we construct encoders with a 32-neuron hidden layer and a 16-neuron embedding layer. The discriminators are built by two hidden layers with 16 and 64 neurons respectively. On Cora, Citeseer and Wiki, we train the autoencoder-related models of ARGE and ARVGE for 200 iterations with the Adam optimizer, with the encoder and discriminator learning rates both set as 0.001; on Pubmed, we train them for 2000 iterations with the encoder learning rate 0.001 and the discriminator learning rate 0.008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TEXT CLASSIFICATION</head><p>The 20NG dataset1 (bydate version) contains 18,846 documents evenly categorized into 20 different categories. In total, 11,314 documents are in the training set and 7,532 documents are in the test set.</p><p>The Ohsumed corpus2 is from the MEDLINE database, which is a bibliographic database of important medical literature maintained by the National Library of Medicine. In this work, we used the 13,929 unique cardiovascular diseases abstracts in the first 20,000 abstracts of the year 1991. Each document in the set has one or more associated categories from the 23 disease categories.</p><p>As we focus on single-label text classification, the documents belonging to multiple categories are excluded so that 7,400 documents belonging to only one category remain. MR is a movie review dataset for binary sentiment classification, in which each review only contains one sentence <ref type="bibr" target="#b23">(Pang &amp; Lee, 2005)</ref> The corpus has 5,331 positive and 5,331 negative reviews. We used the training/test split in <ref type="bibr" target="#b27">(Tang et al., 2015)</ref>.</p><p>A.3 OPEN GRAPH BENCHMARK: NODE PROPERTY PREDICTION A.3.1 TEXT CLASSIFICATION Parameters. We follow the setting of Text GCN <ref type="bibr" target="#b34">(Yao et al., 2019)</ref> that includes experiments on four widely used benchmark corpora such as 20-Newsgroups (20NG), Ohsumed, R52 and R8 of Reuters 21578. For Text GCN, SGC, and our approach, the embedding size of the first convolution layer is 200 and the window size is 20. We set the learning rate to 0.02, dropout rate to 0.5 and the decay rate to 0. The 10% of training set is randomly selected for validation. Following <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref>, we trained our method and Text GCN for a maximum of 200 epochs using the To convert text classification into the node classification on graph, there are two relationships considered when forming graphs: (i) the relation between documents and words and (ii) the connection between words. For the first type of relations, we build edges among word nodes and document nodes based on the word occurrence in documents. The weight of the edge between a document node and a word node is the Term Frequency-Inverse Document Frequency (Rajaraman &amp; Ullman, 2011) (TF-IDF) of the word in the document applied to build the Docs-words graph. For the second type of relations, we build edges in graph among word co-occurrences across the whole corpus. To utilize the global word co-occurrence information, we use a fixed-size sliding window on all documents in the corpus to gather co-occurrence statistics. Point-wise Mutual Information <ref type="bibr" target="#b6">(Church &amp; Hanks, 1990</ref>) (PMI), a popular measure for word associations, is used to calculate weights between two word nodes according to the following definition:</p><formula xml:id="formula_28">PMI(i, j) = log p(i, j) p(i)p(j)<label>(14)</label></formula><p>where</p><formula xml:id="formula_29">p(i, j) = W (i,j) W , p(i) = W (i) W . #W (i)</formula><p>is the number of sliding windows in a corpus that contain word i, #W (i, j) is the number of sliding windows that contain both word i and word j, and #W is the total number of sliding windows in the corpus. A positive PMI value implies a high semantic correlation of words in a corpus, while a negative PMI value indicates little or no semantic correlation in the corpus. Therefore, we only add edges between word pairs with positive PMI values:</p><formula xml:id="formula_30">A = W 1 W 2 W 2 I or A ij = ? ? ? ? ? ? ? PMI(i, j) if i, j are words, PMI(i, j) &gt; 0, TF-IDF ij if i is document, j is word, 1 if i = j, 0 otherwise.<label>(15)</label></formula><p>A.4 THEORETICAL ANALYSIS Below we show that we can reduce oversmoothing compared to SGC while incorporating larger receptive fields.</p><p>Our design contains a sum of consecutive diffusion matrices T k , k = 0, ? ? ? , K. As k increases, so does the neighbourhood of each node visited in diffusion T k (analogy to random walks).</p><p>This means that:</p><p>Claim I. Our filter, by design, will give the highest weight to the closest neighborhood of a node as neighborhoods N of diffusion steps k = 0, ? ? ? , K obey:</p><formula xml:id="formula_31">N ( T 0 ) ? N ( T 1 ) ? ? ? ? ? N ( T K ) ? N ( T ? ).</formula><p>That is, smaller neighbourhoods belong to larger neighbourhoods too.</p><p>To see this clearer, for the q-dimensional Euclidean lattice graph with infinite number of nodes, after t steps of random walk, the estimate of absolute distance the walk moves from the source to its current position is given as:</p><formula xml:id="formula_32">r(t, q) = 2t q ? ? q+1 2 ? (q + 1) ,<label>(16)</label></formula><p>where r(t, q) is the absolute distance walked from the source to the current point and ?(?) is the Gamma function. Moreover, if the number of dimensions q ? ?, we have r(t, q) ? ? t. It is clear then that the receptive field associated with the random walk (and thus diffusion at time t) obeys the monotonically increasing radius r, that is r(0</p><formula xml:id="formula_33">) ? r(1) ? ? ? ? ? r(K) ? ? ? ? ? r(?).</formula><p>To see that, simply plot ? t (and/or the more complicated expression).</p><p>This proves Claim I for the Euclidean lattice graph. That is, for consecutive diffusion steps T k , k = 0, ? ? ? , K, our receptive field grows.</p><p>Moreover, note that our filter is realized as the sum of consecutive diffusion steps, that is</p><formula xml:id="formula_34">1 t t ? =0 diff(s, ? )</formula><p>where s is the source of walk. It is easy to see then that even if each walked distance was to contribute the energy proportional with r(t) to the summation term, we have:</p><formula xml:id="formula_35">lim t?? 1 t t t =0 ? t ? t = 0,<label>(17)</label></formula><p>where the enumerator is the model of the total energy when aggregating over receptive fields from size 0 to ? in S 2 GC while the denominator is the total energy of SGC (filter is given by T K , that is by diff(s, t)).</p><p>Claim II. The above proof shows that the above ratio of energies is 0 tells that the energy of infinitedimensional receptive field (when t ? ?) is not going to dominate the sum energy of our filter. Thus, according to this model S 2 GC can incorporate larger receptive fields without overbearing contributions of smaller receptive fields compared to SGC as t ? ? on the Euclidean lattice graph.</p><p>However, in practice, we work with finite-dimensional non-Euclidean graphs. Obtaining the absolute distance r(t) walked from the source is a difficult topic. Kindly see for instance Eq. 184 in <ref type="bibr" target="#b20">Masuda et al. (2017)</ref>.</p><p>For this reason, below we use a simple approximation. We use Theorem 1 (main paper) as the proxy for the walked radius. That is to say the error of convergence to the stationary distribution is indicative of the absolute distance walked from the source/node. Specifically, we have:</p><p>Recall Theorem 1. That is, let ? 2 denote second largest eigenvalue of transition matrix T = D -1 A, p(t) be the probability distribution vector and ? the stationary distribution. If walk starts from the vertex i , p i (0) = 1, then after t steps for every vertex:</p><formula xml:id="formula_36">|p j (t) -? j | ? d j d i ? t 2 ,<label>(18)</label></formula><p>Then, the average walked distance r from node i over t steps in a graph with n nodes and connectivity given by the second largest eigenvalue ? 2 , denoted by r(i, t, n) is lower-bounded by r(i, t, n) as follows:</p><formula xml:id="formula_37">r(i, t, n) ? 1 1 n j =i |p j (t) -? j | ? r(i, t, n) = n ? t 2 j =i ? dj ? di = n ? d i ? t 2 (2|E| -d i ) , (<label>19</label></formula><formula xml:id="formula_38">)</formula><p>where n is the number of nodes, t is the number of diffusion steps (think T k ), d i and d j are degrees of nodes i and j, ? 2 being the second largest eigenvalue intuitively denotes the graph connectivity (large ? 2 ? 1 indicates low connectivity while low ? 2 indicates high connectivity in graph), and |E| is the total number of edges in the graph.</p><p>While the above approximations may be loose for very small/large t, the important property to note is that r(i, 0, n) ? r(i, 1, n) ? ? ? ? ? r(i, t, n) which indicates that our filter indeed realises the sum over increasingly larger receptive fields. As smaller receptive fields are a subset of larger receptive fields given node i, that is N ( T 0 ) ? N ( T 1 ) ? ? ? ? ? N ( T K ) ? N ( T ? ), this proves our Claim I.</p><p>To prove Claim II for general graphs, we have:</p><formula xml:id="formula_39">lim t?? 1 t t t =0</formula><p>r(i, t , n)</p><formula xml:id="formula_40">r(i, t, n) = 0,<label>(20)</label></formula><p>Similar findings are highlighted by carefully considering the meaning of so-called Cheeger constant introduced in Section A.5. More on spectral analysis of filters in GCNs can be found in the studies of <ref type="bibr">Li et al. (2018a)</ref> and <ref type="bibr" target="#b19">Li et al. (2018b)</ref>.</p><p>A.5 GRAPH PARTITIONING Below we introduce the definitions of expansion and k-way Cheeger constant. According to the definitions, the expansion in Def. A.1 describes the effect of graph partitioning according to subset S while the k-way Cheeger constant reflects the effect of the graph partitioning into k parts-the smaller the value the better the partitioning is. Higher-order Cheeger's inequality <ref type="bibr" target="#b1">(Bandeira et al., 2013;</ref><ref type="bibr" target="#b16">Lee et al., 2014)</ref> bridges the gap between the network spectral analysis and graph partitioning by controlling the bounds of k-way Cheeger constant:</p><formula xml:id="formula_41">? k 2 ? ? G (k) ? O k 2 ? k ,<label>(21)</label></formula><p>where ? k is the k-th eigenvalue of the normalized Laplacian matrix and 0 = ? 1 ? ? 2 ? ? ? ? ? ? n .</p><p>From inequality 21, we can conclude that small (large) eigenvalues control global clustering (local smoothing) effect of the graph partitioned into a few large parts (many small parts).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: (a) Function f (?) = 1 K</figDesc><graphic url="image-1.png" coords="4,163.66,86.85,138.60,105.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Definition A.1. Expansion: For a node subset S ? V, ?(S) = |E(S)| min{vol(S),vol(V \S)} , where E(S) is the set of edges with one node in S and vol(S) is the sum of degree of nodes in set S. Definition A.2. The k-way Cheeger constant is given as:? G (k) = min S1,S2,??? ,S k max{?(S i ) : i = {1, ? ? ? , k}}where the minimum is over all collections of k non-empty disjoint subsets S 1 , S 2 , ? ? ? , S k ? V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Computational and storage complexities O(?).Relation of S 2 GC to Jumping Knowledge Network (JKN).Xu et al. (2018b)  combine intermediate node representations from each layer by concatenating them in the final layer. However,(Xu  et al., 2018b) use non-linear layers which results in a completely different network architecture and the usual slower processing due to complex backpropagation chain.</figDesc><table><row><cell>Stage</cell><cell>Complexity</cell><cell>APPNP</cell><cell>GDC</cell><cell>SGC</cell><cell>S 2 GC</cell></row><row><cell>Forward</cell><cell cols="4">Computation Cost K|E|d + Knd ? K|E|n K|E|d</cell><cell>K|E|d + Knd</cell></row><row><cell cols="2">Propagation Storage Cost</cell><cell>nd + |E|</cell><cell>? n 2</cell><cell cols="2">nd + |E| nd + |E|</cell></row><row><cell>Backward</cell><cell cols="2">Computation Cost K|E|d</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Propagation Storage Cost</cell><cell>nd + |E|</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="6">Table 2: Timing (seconds) on Cora, Citeseer, Pubmed and the large scale Open Graph Benchmark</cell></row><row><cell cols="2">(OGB) which includes Products.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">methods Cora Citeseer Pubmed Products</cell><cell></cell></row><row><cell></cell><cell>SGC</cell><cell>0.45 0.55</cell><cell>0.78</cell><cell>9.8</cell><cell></cell></row><row><cell></cell><cell cols="2">APPNP 1.08 1.44</cell><cell>1.32</cell><cell>748</cell><cell></cell></row><row><cell></cell><cell>S 2 GC</cell><cell>0.67 0.81</cell><cell>0.79</cell><cell>11.4</cell><cell></cell></row></table><note><p><p>3.4 COMPLEXITY ANALYSIS</p>For S 2 GC, the storage costs is O(|E| + nd), where |E| is the total edge count, nd relates to saving the T k X during intermediate multiplications T</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Clustering performance with three different metrics on four datasets.</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>Cora</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell>Pubmed</cell><cell>Wiki</cell></row><row><cell></cell><cell></cell><cell cols="2">Acc% NMI% F1%</cell><cell>Acc% NMI%</cell><cell>F1%</cell><cell>Acc% NMI%</cell><cell>F1%</cell><cell>Acc% NMI% F1%</cell></row><row><cell>k-means</cell><cell cols="2">Feature 34.65 16.73</cell><cell cols="2">25.42 38.49 17.02</cell><cell cols="2">30.47 57.32 29.12</cell><cell>57.35 33.37 30.20</cell><cell>24.51</cell></row><row><cell cols="3">Spectral-f Feature 36.26 15.09</cell><cell cols="2">25.64 46.23 21.19</cell><cell cols="2">33.70 59.91 32.55</cell><cell>58.61 41.28 43.99</cell><cell>25.20</cell></row><row><cell cols="2">Spectral-g Graph</cell><cell>34.19 19.49</cell><cell cols="2">30.17 25.91 11.84</cell><cell cols="2">29.48 39.74 3.46</cell><cell>51.97 23.58 19.28</cell><cell>17.21</cell></row><row><cell cols="2">DeepWalk Graph</cell><cell>46.74 31.75</cell><cell cols="2">38.06 36.15 9.66</cell><cell cols="2">26.70 61.86 16.71</cell><cell>47.06 38.46 32.38</cell><cell>25.74</cell></row><row><cell>GAE</cell><cell>Both</cell><cell>53.25 40.69</cell><cell cols="2">41.97 41.26 18.34</cell><cell cols="2">29.13 64.08 22.97</cell><cell>49.26 17.33 11.93</cell><cell>15.35</cell></row><row><cell>VGAE</cell><cell>Both</cell><cell>55.95 38.45</cell><cell cols="2">41.50 44.38 22.71</cell><cell cols="2">31.88 65.48 25.09</cell><cell>50.95 28.67 30.28</cell><cell>20.49</cell></row><row><cell>ARGE</cell><cell>Both</cell><cell>64.00 44.90</cell><cell cols="2">61.90 57.30 35.00</cell><cell cols="2">54.60 59.12 23.17</cell><cell>58.41 41.40 39.50</cell><cell>38.27</cell></row><row><cell>ARVGE</cell><cell>Both</cell><cell>62.66 45.28</cell><cell cols="2">62.15 54.40 26.10</cell><cell cols="2">52.90 58.22 20.62</cell><cell>23.04 41.55 40.01</cell><cell>37.80</cell></row><row><cell>AGC</cell><cell>Both</cell><cell>68.92 53.68</cell><cell cols="2">65.61 67.00 41.13</cell><cell cols="2">62.48 69.78 31.59</cell><cell>68.72 47.65 45.28</cell><cell>40.36</cell></row><row><cell>S 2 GC</cell><cell>Both</cell><cell>69.60 54.71</cell><cell cols="2">65.83 69.11 42.87</cell><cell cols="2">64.65 70.98 33.21</cell><cell>70.28 52.67 49.62</cell><cell>44.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Test Micro F1 Score (%) averaged over 10 runs on Reddit. Performance of other models are cited from their original papers.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 5: Test accuracy (%) averaged over 10 runs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">on citation networks.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>methods</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Setting Supervised</cell><cell cols="2">Model SAGE-mean SAGE-LSTM 95.4 Test F1 95.0 SAGE-GCN 93.0</cell><cell cols="2">GCN GAT FastGCN 79.8? 0.3 81.4? 0.4 83.3? 0.7 GIN 77.6? 1.1 DGI 82.5? 0.7</cell><cell>70.9? 0.5 72.6? 0.6 68.8? 0.6 66.1? 0.9 71.6? 0.7</cell><cell>79.0? 0.4 78.5? 0.3 77.4? 0.3 77.0? 1.2 78.4? 0.7</cell></row><row><cell cols="2">Unsupervised FastGCN</cell><cell>93.7</cell><cell>SGC</cell><cell cols="2">81.0? 0.02 71.9? 0.08 78.9? 0.03</cell></row><row><cell>No Learning</cell><cell>SAGE-GCN DGI SGC S 2 GC</cell><cell>90.8 94.0?0.001 94.9?0.001 95.3?0.001</cell><cell cols="3">MixHop APPNP Chebynet 78.0? 0.4 81.8?0.6 83.3?0.5 AR filter 80.8? 0.02 69.3? 0.15 78.1? 0.01 71.4?0.8 80.0?1.1 71.7?0.6 80.1?0.2 70.1? 0.5 78.0? 0.4 Ours 83.0? 0.02 73.6? 0.09 80.4? 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) averaged over 10 runs on the large-scale OGB node property prediction.</figDesc><table><row><cell>methods</cell><cell>Products</cell><cell>Mag</cell><cell>Arxiv</cell></row><row><cell>MLP</cell><cell cols="3">61.06?0.08 26.92?0.26 55.50?0.23</cell></row><row><cell>GCN</cell><cell cols="3">75.64?0.21 30.43?0.25 71.74?0.29</cell></row><row><cell>GraphSage</cell><cell cols="3">78.29?0.16 31.53?0.15 71.49?0.27</cell></row><row><cell>Softmax</cell><cell cols="3">47.70?0.03 24.13?0.03 52.77?0.56</cell></row><row><cell>SGC</cell><cell cols="3">68.87? 0.01 29.47?0.03 68.78?0.02</cell></row><row><cell>S 2 GC</cell><cell cols="3">70.22? 0.01 32.47?0.11 70.15?0.13</cell></row><row><cell cols="4">S 2 GC+MLP 74.84?0.20 32.72?0.23 72.01?0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy on the document classification task.</figDesc><table><row><cell>Model</cell><cell>20NG</cell><cell>R8</cell><cell>R52</cell><cell>Ohsumed</cell><cell>MR</cell></row><row><cell cols="6">Text GCN 87.9 ? 0.2 97.0 ? 0.2 93.8 ? 0.2 68.2 ? 0.4 76.3 ? 0.3</cell></row><row><cell>SGC</cell><cell cols="5">88.5 ? 0.1 97.2 ? 0.2 94.0 ? 0.2 68.5 ? 0.3 75.9 ? 0.3</cell></row><row><cell>S 2 GC</cell><cell cols="5">88.6? 0.1 97.4 ? 0.1 94.5 ? 0.2 68.5 ? 0.1 76.7 ? 0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Summary of classification accuracy (%) results with various depths. In the linear model, filter parameter K is equivalent with the number of layers.</figDesc><table><row><cell cols="2">Dataset Method</cell><cell></cell><cell></cell><cell cols="2">Layers (K)</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell>Cora</cell><cell>GCN</cell><cell cols="5">81.1 80.4 69.5 64.9 60.3 28.7</cell></row><row><cell></cell><cell>SGC</cell><cell cols="5">80.8 81.5 80.7 79.0 75.9 66.8</cell></row><row><cell></cell><cell>S 2 GC</cell><cell cols="5">76.5 79.8 82.5 83.0 82.2 80.0</cell></row><row><cell cols="2">Citeseer GCN</cell><cell cols="5">70.8 67.6 30.2 18.3 25.0 20.0</cell></row><row><cell></cell><cell>SGC</cell><cell cols="5">71.9 72.6 73.1 72.2 70.6 69.2</cell></row><row><cell></cell><cell>S 2 GC</cell><cell cols="5">70.9 72.7 72.7 73.4 73.1 73.2</cell></row><row><cell cols="2">Pubmed GCN</cell><cell cols="5">79.0 76.5 61.2 40.9 22.4 35.3</cell></row><row><cell></cell><cell>SGC</cell><cell cols="5">79.2 79.7 78.4 76.4 71.6 68.6</cell></row><row><cell></cell><cell>S 2 GC</cell><cell cols="5">77.6 78.7 79.4 80.6 78.0 74.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Classification accuracy (%) results with different ?.</figDesc><table><row><cell cols="2">Dataset 0.0</cell><cell>0.05 0.1</cell><cell>0.15</cell></row><row><cell>cora</cell><cell cols="3">82.9 83.0 82.6 81.9</cell></row><row><cell cols="2">citeseer 73</cell><cell cols="2">73.4 73.3 72.9</cell></row><row><cell cols="4">pubmed 80.4 80.6 79.7 79.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Graph classification.</figDesc><table><row><cell>Method</cell><cell>MUTAG</cell><cell cols="2">PROTEINS COLLAB IMDB-BINARY</cell></row><row><cell>GCN</cell><cell>74.6 ? 7.7</cell><cell>73.1 ? 3.8</cell><cell>80.6 ? 2.1 72.6 ? 4.5</cell></row><row><cell>SAGE</cell><cell>74.9 ? 8.7</cell><cell>73.8 ? 3.6</cell><cell>79.7 ? 1.7 72.4 ? 3.6</cell></row><row><cell>GIN-0</cell><cell>85.7 ? 7.7</cell><cell>72.1 ? 5.1</cell><cell>79.3 ? 2.7 72.8 ? 4.5</cell></row><row><cell>GIN-</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>The statistics of datasets used for node classification and clustering.</figDesc><table><row><cell cols="3">Dataset # Nodes # Edges</cell><cell cols="3">class feature Train/Dev/Test Nodes</cell></row><row><cell>Cora</cell><cell>2, 708</cell><cell>5, 429</cell><cell>7</cell><cell>1433</cell><cell>140/500/1, 000</cell></row><row><cell cols="2">Citeseer 3, 327</cell><cell>4, 732</cell><cell>6</cell><cell>3703</cell><cell>120/500/1, 000</cell></row><row><cell cols="2">Pubmed 19, 717</cell><cell>44, 338</cell><cell>3</cell><cell>500</cell><cell>60/500/1, 000</cell></row><row><cell>Reddit</cell><cell cols="3">232, 965 11, 606, 919 41</cell><cell>602</cell><cell>152K/24K/55K</cell></row><row><cell>wiki</cell><cell>2405</cell><cell>17981</cell><cell>17</cell><cell>4973</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>3,357 documents are in the training set and 4,043 documents are in the test set.</figDesc><table><row><cell>R52 and R8 (all-terms version) are two subsets of the Reuters 21578 dataset. R8 has 8 categories,</cell></row><row><cell>and was split to 5,485 training and 2,189 test documents. R52 has 52 categories, and was split to</cell></row><row><cell>6,532 training and 2,568 test documents.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>The statistics of datasets for text classification.</figDesc><table><row><cell>Dataset</cell><cell cols="7"># Docs # Training # Test # Words # Nodes # Classes Average Length</cell></row><row><cell>20NG</cell><cell>18,846</cell><cell>11,314</cell><cell>7,532</cell><cell>42,757</cell><cell>61,603</cell><cell>20</cell><cell>221.26</cell></row><row><cell>R8</cell><cell>7,674</cell><cell>5,485</cell><cell>2,189</cell><cell>7,688</cell><cell>15,362</cell><cell>8</cell><cell>65.72</cell></row><row><cell>R52</cell><cell>9,100</cell><cell>6,532</cell><cell>2,568</cell><cell>8,892</cell><cell>17,992</cell><cell>52</cell><cell>69.82</cell></row><row><cell cols="2">Ohsumed 7,400</cell><cell>3,357</cell><cell>4,043</cell><cell>14,157</cell><cell>21,557</cell><cell>23</cell><cell>135.82</cell></row><row><cell>MR</cell><cell>10,662</cell><cell>7,108</cell><cell>3,554</cell><cell>18,764</cell><cell>29,426</cell><cell>2</cell><cell>20.39</cell></row><row><cell cols="8">Adam (Kingma &amp; Ba, 2014) optimizer, and we stop training if the validation loss does not decrease</cell></row><row><cell cols="8">for 10 consecutive epochs. The text graph was built according to steps detailed in the supplementary</cell></row><row><cell>material.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A cheeger inequality for the graph connection laplacian</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Afonso S Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1611" to="1630" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hyperopt: a python library for model selection and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<ptr target="http://stacks.iop.org/1749-4699/8/i=1/a=014008" />
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14008</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (adaptive computation and machine learning)</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Franc ?ois Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luh</forename><surname>Francoisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13354" to="13366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiway spectral partitioning and higherorder cheeger inequalities</title>
		<author>
			<persName><forename type="first">Shayan</forename><surname>James R Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Oveis Gharan</surname></persName>
		</author>
		<author>
			<persName><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Label efficient semisupervised learning via graph filtering</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03226</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random walks and diffusion on networks</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Lambiotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">716</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv preprint cs/0506075</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mining of massive datasets</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>David Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kernels and regularization on graphs</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01210</idno>
		<title level="m">Attributed graph clustering via adaptive graph convolution</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
