<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SGPT: GPT Sentence Embeddings for Semantic Search</title>
				<funder>
					<orgName type="full">OpenAI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-17">17 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
							<email>muennighoff@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SGPT: GPT Sentence Embeddings for Semantic Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-17">17 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.08904v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GPT transformers are the largest language models available, yet semantic search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric search. SGPT-BE produces semantically meaningful sentence embeddings by contrastive fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion parameter SGPT-BE outperforms the best available sentence embeddings by 6% setting a new state-of-the-art on BEIR. It outperforms the concurrently proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes 250,000 times more parameters. SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1 billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It beats the supervised state-of-the-art on 7 datasets, but significantly loses on other datasets. We show how this can be alleviated by adapting the prompt. SGPT-BE and SGPT-CE performance scales with model size. Yet, increased latency, storage and compute costs should be considered. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic search consists of two parts: Search refers to finding the top k answers from a document corpus given a query. Semantic refers to understanding the documents and queries beyond keywords. Transformers <ref type="bibr" target="#b25">[26]</ref> are the dominant semantic architecture <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> competing with non-semantic models like BM25 <ref type="bibr" target="#b22">[23]</ref>. However, they have been limited to BERT-like encoder transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>. Meanwhile, GPT-like decoder transformers <ref type="bibr" target="#b18">[19]</ref> have been the focus of recent scaling efforts of up to 580 billion parameters <ref type="bibr" target="#b11">[12]</ref>. Yet, it remains unclear how to extract competitive GPT sentence embeddings and use them for semantic search. In this work, we investigate how to apply decoder transformers to semantic search and make use of their scale to outperform current methods. We distinguish four settings: Cross-Encoder vs Bi-Encoder, Symmetric vs Asymmetric. See Figure <ref type="figure" target="#fig_0">1</ref> and Section 2.</p><p>In the Bi-Encoder setting, we propose SGPT-BE using position-weighted mean pooling and contrastive fine-tuning of only bias tensors (BitFit <ref type="bibr" target="#b30">[31]</ref>). BitFit is within +2 to -2% of full fine-tuning performance for SBERT <ref type="bibr" target="#b21">[22]</ref> and SGPT despite changing &lt;0.1% of pre-trained parameters. When controlling for size, SGPT is within +1 to -3% of SBERT performance. When scaling up, SGPT-BE-5.8B sets state-of-the-art results on BEIR and USEB for asymmetric and symmetric search.</p><p>In the Cross-Encoder setting, we propose SGPT-CE using log probability extraction of pre-trained GPT models. It can be used for symmetric or asymmetric search by changing the prompt. Unsupervised SGPT-CE-6.1B is 5% worse than supervised SGPT-BE-5.8B on BEIR.</p><p>In summary, our contributions are three-fold:</p><p>? For SGPT-BE in Section 4, we develop a new pooling method and show the usefulness of bias-only fine-tuning for embeddings. At 5.8B parameters, it produces the best natural language embeddings available by a margin of 6% for the example of semantic search.</p><p>? For SGPT-CE in Section 3, we show how to use GPT for search via log probabilities without fine-tuning. At 6.1B parameters, it has the best unsupervised performance on BEIR by a margin of 8%.</p><p>? We provide free, more performant alternatives to OpenAI Search or Similarity Embeddings and the OpenAI Search endpoint available at https://github.com/Muennighoff/sgpt 2 Background</p><p>In this section, we explain two dimensions fundamental to our work: Cross-Encoders vs Bi-Encoders and Symmetric vs Asymmetric Search.</p><p>Cross-Encoders encode query and document at the same time. BERT is used as a Cross-Encoder by separating the query from the document with a [SEP ] token <ref type="bibr" target="#b4">[5]</ref>. They are then passed through the transformer network together. Each new query requires k forward passes given a corpus of k documents.</p><p>Bi-Encoders encode query and document separately. SBERT <ref type="bibr" target="#b21">[22]</ref> extends BERT to the Bi-Encoder setting via supervised fine-tuning and a pooling operation across the sequence output. The resulting document vectors can be cached. A new query requires only one forward pass through the transformer to produce the query vector. The query vector can then be scored against the cached document vectors with a similarity function. Embeddings from Bi-Encoders can be used for non-search tasks such as clustering or as input features of machine learning models.</p><p>Cross-Encoders tend to outperform Bi-Encoders <ref type="bibr" target="#b23">[24]</ref>, but are slower as vectors cannot be cached.</p><p>To balance the trade-offs, multi-stage architectures have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. In a two-stage re-ranking setup, the first model processes the entire corpus and the second model is only used on the top k documents returned by the first. In Section 3, we use Bi-Encoder (BM25) + Cross-Encoder re-ranking.</p><p>Asymmetric Search means queries and documents are not interchangeable. Finding answers given a question is an asymmetric search problem. Commonly, documents are much longer than queries <ref type="bibr" target="#b24">[25]</ref>. We evaluate asymmetric search experiments on BEIR <ref type="bibr" target="#b24">[25]</ref>, a recently proposed benchmark consisting of 19 asymmetric search datasets.</p><p>Symmetric Search means queries and documents are interchangeable. Finding duplicate questions, where both queries and documents are questions, is a symmetric search problem. We evaluate symmetric search experiments on USEB <ref type="bibr" target="#b27">[28]</ref>, Quora from BEIR <ref type="bibr" target="#b24">[25]</ref> and STS-B. In Quora, queries are question titles and documents are question texts. They are often the same with average word lengths of 9.53 and 11.44, respectively <ref type="bibr" target="#b24">[25]</ref>. Hence, we consider it more of a symmetric search task. We include Quora in both symmetric and asymmetric experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SGPT Cross-Encoder</head><p>3.1 Asymmetric Search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Method</head><p>Given a query q, and a document corpus D, we are interested in the most likely document d * . Using Bayes' Theorem this can be expressed as:</p><formula xml:id="formula_0">d * = arg max d?D P (d|q) = arg max d?D P (q|d)P (d) P (q) = arg max d?D P (q|d)P (d)<label>(1)</label></formula><p>Note that P (q) is irrelevant as it is always the same when taking the arg max over D. Due to variable document lengths and contents it is easier to compare P (q|d) than P (d|q). We hence compute the joint probability of the query tokens q i,..,n given the document tokens embedded in a prompt P as p(q i , ..., q n |p 1 , ..., p i-1 ). As long as P (d) does not vary excessively across the corpus D, this simplification should produce reasonable scores.</p><p>In practice, we use log probabilities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, computed via the log of the softmax of the model output.</p><p>To have a constant query length n + 1 -i and avoid abrupt text changes, documents are truncated from the left until the input fits the model's maximum sequence length. We apply these methods to re-rank top k documents returned by BM25 <ref type="bibr" target="#b22">[23]</ref>. While re-ranking with BM25 bottlenecks performance, it speeds up experiments. It is not a necessary part of the architecture and therefore not depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We experiment with publicly available pre-trained decoder transformers with 125M, 1.3B, 2.7B and 6.1B parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>. Table <ref type="table">2</ref>: OpenAI model parameter estimates. Based on comparing the embedding sizes from the OpenAI docs with the dimensions provided in <ref type="bibr" target="#b2">[3]</ref>. In brackets are numbers for cpt-text models recently provided in <ref type="bibr" target="#b15">[16]</ref>. They differ likely due to removing the language modeling head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Results</head><p>We sub-select 6 small datasets from BEIR <ref type="bibr" target="#b24">[25]</ref> and perform a search over 12 prompts. The prompts and results are in Table <ref type="table" target="#tab_7">7</ref> and Table <ref type="table" target="#tab_8">8</ref>. We select the prompt with the best average score, P G .</p><p>In Table <ref type="table" target="#tab_0">1</ref>, we benchmark the resulting SGPT-CE (SGPT-Cross-Encoder). We compare with OpenAI's Search endpoint, which is to be distinguished from their Embeddings endpoint. Please refer to Table <ref type="table" target="#tab_5">6</ref> in the Bi-Encoder section for a benchmark with the OpenAI Embeddings endpoint. We provide parameter estimates for the OpenAI model names in Table <ref type="table">2</ref>. We also compare with the current stateof-the-art on BEIR <ref type="bibr" target="#b24">[25]</ref>, a BERT-based Cross-Encoder. BM25+CE consists of a pre-trained BERT model that is further fine-tuned on MS-MARCO <ref type="bibr" target="#b16">[17]</ref> in a supervised fashion <ref type="bibr" target="#b24">[25]</ref>. SGPT-CE consists solely of the pre-trained GPT model. However, SGPT-CE-6.1B has almost 15x more parameters than BM25+CE significantly increasing latency. In the Re-rank Top 100 setting, the top 100 documents as returned by BM25 are re-ranked by the respective model. While SGPT-CE-6.1B wins on more datasets than the encoder-based state-of-the-art, its average score is worse. This can be alleviated by not using the same prompt P G for all datasets. We show in Section 3.2 that SGPT-CE-6.1B can beat BM25+CE on Quora by changing the prompt. Table <ref type="table">3</ref>: SGPT-CE symmetric search results on Quora. The sum of log probabilities from {query} is used as the re-rank score. Overflowing tokens are truncated from the left of {doc}. Scores are nDCG@10.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, we investigate how performance scales with model size. As we are in a re-ranking setup, the Cross-Encoder performance is bounded by the documents returned by BM25. We provide the BM25 bounds and additional model results in Table <ref type="table" target="#tab_9">9</ref>. In a Re-rank Top 10 setting, the model is significantly bottlenecked by BM25. SGPT-CE-6.1B reaches around 80% of the maximum possible performance. We hence observe high jumps in performance for datasets like HotpotQA or TREC-COVID as we move to top 100. In fact, the 0.791 nDCG@10 on TREC-COVID in Table <ref type="table" target="#tab_0">1</ref> is not possible in a Re-rank Top 10 setting as the bound is at 0.750, see Table <ref type="table" target="#tab_9">9</ref>. From the results, we infer that performance scales both as we re-rank more documents or increase model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symmetric Search</head><p>We use the same methods outlined in Section 3.1.1, but adapt the prompt for symmetric search. We show this on the example of Quora in Table <ref type="table">3</ref>. In Section 2, we have explained why Quora is closer to symmetric search than asymmetric search. By adapting the prompt, SGPT-CE-6.1B improves by 6% outperforming all Quora results in Table <ref type="table" target="#tab_0">1</ref>.  <ref type="table" target="#tab_4">4</ref>: SGPT parameter overview. Due to the removal of the final language modeling head SGPT-BE-5.8B has 206M parameters less than SGPT-CE-6.1B or GPT-J-6.1B. GPT-Neo models tie the language modeling head weights with the input embeddings, hence there is no weight difference. Like in Section 3.1.1, we first experiment with decoder transformers that have only gone through unsupervised pre-training. In the Bi-Encoder setting, a pooling operation is commonly applied to the model's hidden states to reduce them to a vector whose size is irrespective of sequence length. SBERT <ref type="bibr" target="#b21">[22]</ref> showed that a MEAN pooling mechanism outperforms [CLS] and MAX strategies for a BERT encoder. Due to the causal attention mask in an auto-regressive decoder transformer, tokens do not attend to future tokens like in an encoder transformer. Hence, only the last token has attended to all tokens in a sequence. To account for this information mismatch, we propose to give later tokens a higher weight using a position-weighted mean pooling method:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SGPT Bi-Encoder</head><formula xml:id="formula_1">v = S i=1 w i h i where w i = i S i=1 i<label>(2)</label></formula><p>where S is the sequence length, h i the ith hidden state and v the query or document embedding. We compare weighted mean pooling with last token pooling, where the hidden state of the final token is the embedding, and regular mean pooling.</p><p>We follow recent work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> and perform supervised contrastive learning with in-batch negatives.</p><p>Given matching query-doc pairs {q (i) , d (i) } M i=1 , we optimize the cost function:</p><formula xml:id="formula_2">J CL (?) = 1 M M i=1 log exp(? ? ?(f ? (q (i) ), f ? (d (i) ))) M j=1 exp(? ? ?(f ? (q (i) ), f ? (d (j) )))<label>(3)</label></formula><p>where f ? is the SGPT model outputting a fixed-size vector, ? cosine similarity and ? a temperature parameter set to 20 in our experiments. We train on SNLI <ref type="bibr" target="#b1">[2]</ref> and MNLI <ref type="bibr" target="#b29">[30]</ref>. We limit the model sequence length to 75 tokens during both training and inference.</p><p>For large models, we fine-tune only bias parameters and freeze the rest of the model. This has been recently proposed as BitFit <ref type="bibr" target="#b30">[31]</ref> for BERT encoders. It has been shown to be competitive with full fine-tuning in various scenarios <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref>. Table <ref type="table" target="#tab_4">4</ref> shows the number of parameters trained for BitFit models. Due to fewer gradient updates, BitFit significantly reduces GPU memory and time required per step. Further, adding a BitFit checkpoint to an instance with an existing full model will only require storing the different biases. An instance already serving a 22.5GB fp32 GPT-J-6B model requires an additional 22MB of storage to serve an SGPT-5.8B-bitfit model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>Figure <ref type="figure" target="#fig_3">3</ref> shows average precisions on USEB <ref type="bibr" target="#b27">[28]</ref> across different methods and layers. In the unsupervised setting, decoder transformers strongly underperform encoders. However, after finetuning on the same dataset with the same hyperparameters, decoders (SGPT) with 125M parameters closely trail the 110M parameter encoder (SBERT) for the 12th layer. When increasing SGPT size    <ref type="bibr" target="#b27">[28]</ref>. However, fragments may be in-domain due to the large pre-training data of the transformer models. SGPT-0.1B-weightedmean-nli performs 2% worse than SBERT-base-nli-v2 on USEB, but improves on Quora by 1%. Note that there is still a size difference of 14% between the two models. ?: Results from <ref type="bibr" target="#b27">[28]</ref> except when marked with ?. CQADupstack and SciDocs differ from the same-name datasets in BEIR. We provide the code for running OpenAI similarity endpoints on USEB. Feel free to message the author if you would like to fund them (around 300 USD for Curie).</p><p>ten-fold, the last layer performance increases beyond that of SBERT models. Weighted mean pooling outperforms other pooling mechanisms for decoders.</p><p>Table <ref type="table" target="#tab_3">5</ref> provides performance on the individual USEB datasets, Quora and STS-B. STS-B scores should not be the focus of comparison due to the drawbacks highlighted in <ref type="bibr" target="#b27">[28]</ref>. Despite training on less than 0.1% of parameters BitFit models are within +2 to -2% of fully fine-tuned ones. We investigate gradients in Figure <ref type="figure" target="#fig_4">4</ref>. BitFit degrades performance more for decoders than encoders. This could be due to the missing bias parameters, see bias vector for BERT, which is not present for SGPT models. SGPT-5.8B-weightedmean-nli-bitfit sets an out-of-domain state-of-the-art on USEB, but is outperformed by models trained in-domain in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Asymmetric Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Method</head><p>If not otherwise specified, we follow the same setup as in Section 4.1.1. For asymmetric search, we train on MS-MARCO <ref type="bibr" target="#b16">[17]</ref>. We limit the model sequence length to 300 tokens during both training and inference. We follow concurrent work <ref type="bibr" target="#b15">[16]</ref> and add enclosing brackets to help the model distinguish between query and document. We embed the tokens of query q in two brackets as [q 0-n ].</p><p>For documents, we use curly brackets: {d 0-n }. We add the token ids of the brackets to the already tokenized text to avoid the tokens intermingling. We refer to these special brackets as specb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>Table <ref type="table" target="#tab_5">6</ref> benchmarks SGPT-BE-5.8B (SGPT-5.8B-weightedmean-msmarco-specb-bitfit) on BEIR <ref type="bibr" target="#b24">[25]</ref> with: (a) BM25 <ref type="bibr" target="#b22">[23]</ref>, a non-semantic fast baseline (b) SGPT-CE-6.1B from Section 3 (c) BM25+CE <ref type="bibr" target="#b24">[25]</ref>, the current overall state-of-the-art on BEIR (d) TAS-B <ref type="bibr" target="#b8">[9]</ref>, the original Bi-Encoder state-of-the-art on BEIR (e) Contriever <ref type="bibr" target="#b10">[11]</ref>, a similar training scheme as <ref type="bibr" target="#b15">[16]</ref> but using an encoder transformer encoder (f ) GTR-XXL <ref type="bibr" target="#b17">[18]</ref>, the current Bi-Encoder state-of-the-art on BEIR with 4.8 billion parameters using T5 <ref type="bibr" target="#b20">[21]</ref>, a BERT-like encoder transformer (g) cpt-text, a GPT-like decoder transformer architecture concurrently proposed in <ref type="bibr" target="#b15">[16]</ref>. Corresponding parameter estimates are in Table <ref type="table">2</ref>.</p><p>SGPT-5.8B achieves the best average nDCG@10 both on the BEIR subset selected in <ref type="bibr" target="#b15">[16]</ref> and on the full BEIR benchmark. It outperforms the roughly same-sized cpt-text-L and the 30x larger cpt-text-XL by 8.6% and 3.2%, respectively. Yet, cpt-text models have gone through an additional unsupervised training stage <ref type="bibr" target="#b15">[16]</ref> and are fully trained. SGPT-BE-5.8B fine-tunes just 700K parameters, 0.0004% of the parameters fine-tuned for cpt-text-XL <ref type="bibr" target="#b15">[16]</ref>. See Table <ref type="table">2</ref> for sizes. We suspect much of the difference to come from the cpt-text model's inferior last token pooling as shown in Figure <ref type="figure" target="#fig_3">3</ref>. SGPT-BE-5.8B improves on the overall state-of-the-art, a Cross-Encoder, by 2.3%. It improves on the previously best sentence embeddings (Bi-Encoder) on BEIR, GTR-XXL, by 6.3%. However, these improvements come at a significant cost. GTR-XXL has 20% fewer parameters and its embeddings have 768 dimensions. SGPT-BE-5.8B produces embeddings with 4096 dimensions, hence requiring about 5x more storage. It took the model six days on one Nvidia A100 GPU to encode the entire BioASQ corpus with 15M documents and an average 200 words each <ref type="bibr" target="#b24">[25]</ref>. Its comparatively low performance on BioASQ may be improved by increasing the sequence length limit beyond 300, however, requiring additional compute. For SGPT-CE-6.1B, the sequence length limit was 2048 for the combined prompt on all datasets. The high performance on TREC-COVID for SGPT models could be due to the different pre-training datasets. The SGPT pre-training dataset, The Pile <ref type="bibr" target="#b6">[7]</ref>, contains data until mid-2020. This may give the models an information advantage on Covid-19.</p><p>Lastly, we highlight that on Quora SGPT-BE-5.8B-msmarco is outperformed by SGPT-BE-5.8B-nli from Table <ref type="table" target="#tab_3">5</ref>. Given our classification of Quora as a symmetric search task in Section 2, this supports our overall distinction between asymmetric and symmetric search. We advise users of our models to classify their tasks as symmetric or asymmetric and use the appropriate model. For non-classifiable embedding tasks, both may work, but we recommend experimenting with embeddings from the symmetric models in Section 4.1 first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This work presented SGPT. Building on SBERT, we proposed modifications to GPT models to use them as Cross-or Bi-Encoders for semantic search.</p><p>SGPT-BE uses position-weighted mean pooling and fine-tuning of only bias tensors. At scale, it produces new state-of-the-art sentence embeddings. The model can be used for semantic search or other embedding tasks. We recommend using SGPT-BE-5.8B when compute and storage are of high availability and maximum performance is desired.</p><p>SGPT-CE extracts log probabilities of pre-trained GPT models to produce unsupervised state-ofthe-art search results. The setup presented can only be used for semantic search. Storage can be limited, but compute should be of high availability for SGPT-CE-6.1B. The prompt and max re-rank parameter can be adjusted depending on performance and latency requirements.</p><p>Future research could fine-tune a GPT Cross-Encoder on MSMARCO similar to the BM25+CE model. We suspect that this should outperform the presented non-fine-tuned SGPT-CE model as well as SGPT-BE if enough documents are re-ranked. Further, the combination of SGPT with GPT for generative search results could be interesting. Possibly, SGPT embeddings could be injected into GPT models to generate answers. Lastly, a detailed study of the disadvantages of the missing biases in large GPT models could be helpful to consider their inclusion in the training of future large language models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><note type="other">Id</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking (?)</head><p>Re-rank   <ref type="table" target="#tab_0">13</ref>: Document similarity scores with the query "deep learning". The higher the more similar. BM25 is keyword-based, hence does not assign any scores to "artificial intelligence" or "artificial snow" in this scenario. Depending on the user, this may or may not result in undesirable search results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Given a query q, documents d1-3, SGPT ranks the documents with scores s1-3. (a) The Cross-Encoder concatenates queries and documents and encodes them together. Scores are extracted log probabilities. (b) The Bi-Encoder separately encodes queries and documents. Resulting document vectors v1-3 can be cached and retrieved at time tc, when a new query comes in. Scores are cosine similarities.</figDesc><graphic url="image-1.png" coords="3,108.00,72.00,395.99,173.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scaling behavior across parameters and re-ranking for SGPT-CE on BEIR. Scores are rescaled nDCG@10 based on bounds defined in Table9. Dataset labels are ordered by the Max Re-rank=100 6.1B performance. The higher on the y-axis, the more bottlenecked is the Cross-Encoder by BM25's performance. Average scores include MS MARCO.</figDesc><graphic url="image-2.png" coords="6,157.50,72.00,297.00,201.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Performance on USEB<ref type="bibr" target="#b27">[28]</ref> by taking the embeddings from certain layers. S models are fine-tuned on the same data with the same hyperparameters. Dashed, solid and dotted lines are last token, mean and weighted mean pooling, respectively. Shades of red are transformer encoders, while shades of blue are decoders. The 0th layer is the embeddings prior to the first transformer layer.</figDesc><graphic url="image-3.png" coords="8,108.00,72.00,396.00,248.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Weight and bias gradients of the attention out projection in the first transformer layer. Weight gradients are less variable than bias gradients during training in the upper graphs. Bias gradients show slightly more activity when weights are freezed in the BitFit model in the lower graph.</figDesc><graphic url="image-6.png" coords="14,216.90,547.86,178.12,93.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Re-ranking performances on BEIR<ref type="bibr" target="#b24">[25]</ref>. OpenAI Search is to be distinguished from the OpenAI Embeddings endpoint. Please refer to Table6in the Bi-Encoder section for a benchmark with the OpenAI Embeddings endpoint. Results on the Search endpoint were produced in October 2021. Scores are nDCG@10.</figDesc><table><row><cell>Training (?)</cell><cell></cell><cell></cell><cell cols="2">Unsupervised</cell><cell></cell><cell></cell><cell cols="2">Unsupervised + Supervised</cell></row><row><cell>Ranking (?)</cell><cell cols="3">Re-rank Top 0 Re-rank Top 10</cell><cell></cell><cell></cell><cell cols="2">Re-rank Top 100</cell><cell></cell></row><row><cell>Model (?)</cell><cell>[23]</cell><cell cols="2">SGPT-CE</cell><cell cols="2">OpenAI Search</cell><cell cols="2">SGPT-CE</cell><cell>[25]</cell></row><row><cell>Dataset (?)</cell><cell>BM25</cell><cell>2.7B</cell><cell>6.1B</cell><cell>Ada</cell><cell>Davinci</cell><cell>2.7B</cell><cell>6.1B</cell><cell>BM25+CE?</cell></row><row><cell>MS MARCO</cell><cell>0.228</cell><cell>0.249</cell><cell>0.253</cell><cell>L</cell><cell>L</cell><cell cols="2">0.278 0.290</cell><cell>0.413  ?</cell></row><row><cell>TREC-COVID</cell><cell>0.688</cell><cell>0.708</cell><cell>0.705</cell><cell>0.616</cell><cell>0.627</cell><cell cols="2">0.762 0.791</cell><cell>0.757</cell></row><row><cell>BioASQ</cell><cell>0.488</cell><cell>0.517</cell><cell>0.518</cell><cell>L</cell><cell>L</cell><cell cols="2">0.546 0.547</cell><cell>0.523</cell></row><row><cell>NFCorpus</cell><cell>0.306</cell><cell>0.319</cell><cell>0.323</cell><cell>0.336</cell><cell>0.358</cell><cell cols="2">0.333 0.347</cell><cell>0.350</cell></row><row><cell>NQ</cell><cell>0.326</cell><cell>0.358</cell><cell>0.366</cell><cell>L</cell><cell>L</cell><cell cols="2">0.384 0.401</cell><cell>0.533</cell></row><row><cell>HotpotQA</cell><cell>0.602</cell><cell>0.647</cell><cell>0.649</cell><cell>L</cell><cell>L</cell><cell cols="2">0.691 0.699</cell><cell>0.707</cell></row><row><cell>FiQA-2018</cell><cell>0.254</cell><cell>0.305</cell><cell>0.313</cell><cell>0.320</cell><cell></cell><cell cols="2">0.369 0.401</cell><cell>0.347</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.330</cell><cell>0.343</cell><cell>0.342</cell><cell>0.313</cell><cell></cell><cell cols="2">0.320 0.323</cell><cell>0.338</cell></row><row><cell>TREC-NEWS</cell><cell>0.405</cell><cell>0.409</cell><cell>0.418</cell><cell>L</cell><cell>L</cell><cell cols="2">0.434 0.466</cell><cell>0.431</cell></row><row><cell>Robust04</cell><cell>0.425</cell><cell>0.438</cell><cell>0.444</cell><cell>L</cell><cell>L</cell><cell cols="2">0.449 0.480</cell><cell>0.475</cell></row><row><cell>ArguAna</cell><cell>0.472</cell><cell>0.379</cell><cell>0.376</cell><cell></cell><cell></cell><cell cols="2">0.293 0.286</cell><cell>0.311</cell></row><row><cell>Touch?-2020</cell><cell>0.347</cell><cell>0.335</cell><cell>0.335</cell><cell>0.332</cell><cell></cell><cell cols="2">0.256 0.234</cell><cell>0.271</cell></row><row><cell>CQADupStack</cell><cell>0.326</cell><cell>0.364</cell><cell>0.367</cell><cell>0.328</cell><cell></cell><cell cols="2">0.405 0.420</cell><cell>0.370</cell></row><row><cell>Quora</cell><cell>0.808</cell><cell>0.810</cell><cell>0.810</cell><cell>0.786</cell><cell></cell><cell cols="2">0.792 0.794</cell><cell>0.825</cell></row><row><cell>DBPedia</cell><cell>0.320</cell><cell>0.341</cell><cell>0.340</cell><cell>L</cell><cell>L</cell><cell cols="2">0.367 0.370</cell><cell>0.409</cell></row><row><cell>SCIDOCS</cell><cell>0.165</cell><cell>0.176</cell><cell>0.177</cell><cell>0.161</cell><cell></cell><cell cols="2">0.186 0.196</cell><cell>0.166</cell></row><row><cell>FEVER</cell><cell>0.649</cell><cell>0.706</cell><cell>0.723</cell><cell>L</cell><cell>L</cell><cell cols="2">0.698 0.725</cell><cell>0.819</cell></row><row><cell>Climate-FEVER</cell><cell>0.186</cell><cell>0.179</cell><cell>0.189</cell><cell>L</cell><cell>L</cell><cell cols="2">0.138 0.161</cell><cell>0.253</cell></row><row><cell>SciFact</cell><cell>0.611</cell><cell>0.653</cell><cell>0.657</cell><cell>0.727</cell><cell></cell><cell cols="2">0.676 0.682</cell><cell>0.688</cell></row><row><cell>Average</cell><cell>0.428</cell><cell>0.444</cell><cell>0.447</cell><cell></cell><cell></cell><cell cols="2">0.450 0.462</cell><cell>0.476</cell></row><row><cell>Best on</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>7</cell><cell>5</cell></row><row><cell cols="9">L: Dataset is too large for OpenAI's endpoint.  ?: In-domain performance. ?: Results from [25]. Other scores</cell></row><row><cell cols="5">are from us. Average scores do not include MS MARCO.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Model Name</cell><cell cols="2">Ada (S)</cell><cell cols="5">Babbage (M) Curie (L) Davinci (XL)</cell></row><row><cell cols="2">Parameters</cell><cell cols="2">350M (300M)</cell><cell cols="2">1.3B (1.2B)</cell><cell cols="3">6.7B (6B) 175B (175B)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10 for Quora and Spearman correlation for STS-B. bf=BitFit. OOD=Out-of-domain, to contrast these numbers from in-domain numbers in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b30">[31]</ref> highlights the importance of the query</figDesc><table><row><cell>Training (?)</cell><cell cols="2">Unsupervised</cell><cell>U. + U.</cell><cell cols="3">Unsupervised + Supervised</cell><cell></cell><cell cols="2">Unsupervised + Unsupervised + Supervised</cell><cell></cell></row><row><cell>Model (?)</cell><cell>[23]</cell><cell>SGPT-CE</cell><cell>[16]</cell><cell>[25]</cell><cell>[9]</cell><cell>SGPT-BE</cell><cell>[11]</cell><cell>[18]</cell><cell cols="2">OpenAI Embeddings [16]</cell></row><row><cell>Dataset (?)</cell><cell cols="10">BM25 SGPT-6.1B cpt-text-L? BM25+CE? TAS-B? SGPT-5.8B Contriever? GTR-XXL? cpt-text-L? cpt-text-XL?</cell></row><row><cell>MS MARCO</cell><cell>0.228</cell><cell>0.290</cell><cell></cell><cell>0.413  ?</cell><cell>0.408  ?</cell><cell>0.405  ?</cell><cell></cell><cell>0.442  ?</cell><cell></cell><cell></cell></row><row><cell>TREC-COVID</cell><cell>0.688</cell><cell>0.791</cell><cell>0.427</cell><cell>0.757</cell><cell>0.481</cell><cell>0.863</cell><cell>0.596</cell><cell>0.501</cell><cell>0.562</cell><cell>0.649</cell></row><row><cell>BioASQ</cell><cell>0.488</cell><cell>0.547</cell><cell></cell><cell>0.523</cell><cell>0.383</cell><cell>0.423</cell><cell></cell><cell>0.324</cell><cell></cell><cell></cell></row><row><cell>NFCorpus</cell><cell>0.306</cell><cell>0.347</cell><cell>0.369</cell><cell>0.350</cell><cell>0.319</cell><cell>0.353</cell><cell>0.328</cell><cell>0.342</cell><cell>0.380</cell><cell>0.407</cell></row><row><cell>NQ</cell><cell>0.326</cell><cell>0.401</cell><cell></cell><cell>0.533</cell><cell>0.463</cell><cell>0.528</cell><cell>0.498</cell><cell>0.568</cell><cell></cell><cell></cell></row><row><cell>HotpotQA</cell><cell>0.602</cell><cell>0.699</cell><cell>0.543</cell><cell>0.707</cell><cell>0.584</cell><cell>0.607</cell><cell>0.638</cell><cell>0.599</cell><cell>0.648</cell><cell>0.688</cell></row><row><cell>FiQA-2018</cell><cell>0.254</cell><cell>0.401</cell><cell>0.397</cell><cell>0.347</cell><cell>0.300</cell><cell>0.371</cell><cell>0.329</cell><cell>0.467</cell><cell>0.452</cell><cell>0.512</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.330</cell><cell>0.323</cell><cell></cell><cell>0.338</cell><cell>0.289</cell><cell>0.267</cell><cell></cell><cell>0.273</cell><cell></cell><cell></cell></row><row><cell>TREC-NEWS</cell><cell>0.405</cell><cell>0.466</cell><cell></cell><cell>0.431</cell><cell>0.377</cell><cell>0.463</cell><cell></cell><cell>0.346</cell><cell></cell><cell></cell></row><row><cell>Robust04</cell><cell>0.425</cell><cell>0.480</cell><cell></cell><cell>0.475</cell><cell>0.427</cell><cell>0.493</cell><cell></cell><cell>0.506</cell><cell></cell><cell></cell></row><row><cell>ArguAna</cell><cell>0.472</cell><cell>0.286</cell><cell>0.392</cell><cell>0.311</cell><cell>0.429</cell><cell>0.499</cell><cell>0.446</cell><cell>0.540</cell><cell>0.469</cell><cell>0.435</cell></row><row><cell>Touch?-2020</cell><cell>0.347</cell><cell>0.234</cell><cell>0.228</cell><cell>0.271</cell><cell>0.162</cell><cell>0.251</cell><cell>0.230</cell><cell>0.256</cell><cell>0.309</cell><cell>0.291</cell></row><row><cell>CQADupStack</cell><cell>0.326</cell><cell>0.420</cell><cell></cell><cell>0.370</cell><cell>0.314</cell><cell>0.376</cell><cell>0.345</cell><cell>0.399</cell><cell></cell><cell></cell></row><row><cell>Quora</cell><cell>0.808</cell><cell>0.794</cell><cell>0.687</cell><cell>0.825</cell><cell>0.835</cell><cell>0.825</cell><cell>0.865</cell><cell>0.892</cell><cell>0.677</cell><cell>0.638</cell></row><row><cell>DBPedia</cell><cell>0.320</cell><cell>0.370</cell><cell>0.312</cell><cell>0.409</cell><cell>0.384</cell><cell>0.393</cell><cell>0.413</cell><cell>0.408</cell><cell>0.412</cell><cell>0.432</cell></row><row><cell>SCIDOCS</cell><cell>0.165</cell><cell>0.196</cell><cell></cell><cell>0.166</cell><cell>0.149</cell><cell>0.194</cell><cell>0.165</cell><cell>0.161</cell><cell>0.177  ?</cell><cell></cell></row><row><cell>FEVER</cell><cell>0.649</cell><cell>0.725</cell><cell>0.638</cell><cell>0.819</cell><cell>0.700</cell><cell>0.789</cell><cell>0.758</cell><cell>0.740</cell><cell>0.756</cell><cell>0.775</cell></row><row><cell>Climate-FEVER</cell><cell>0.186</cell><cell>0.161</cell><cell>0.161</cell><cell>0.253</cell><cell>0.228</cell><cell>0.304</cell><cell>0.237</cell><cell>0.267</cell><cell>0.194</cell><cell>0.223</cell></row><row><cell>SciFact</cell><cell>0.611</cell><cell>0.682</cell><cell>0.712</cell><cell>0.688</cell><cell>0.643</cell><cell>0.742</cell><cell>0.677</cell><cell>0.662</cell><cell>0.744</cell><cell>0.754</cell></row><row><cell>Sub-Average</cell><cell>0.477</cell><cell>0.499</cell><cell>0.442</cell><cell>0.520</cell><cell>0.460</cell><cell>0.545</cell><cell>0.502</cell><cell>0.516</cell><cell>0.509</cell><cell>0.528</cell></row><row><cell>Average</cell><cell>0.428</cell><cell>0.462</cell><cell></cell><cell>0.476</cell><cell>0.395</cell><cell>0.487</cell><cell></cell><cell>0.458</cell><cell></cell><cell></cell></row><row><cell>Best on</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of BEIR state-of-the-art models. Keep model size, latency and training time in mind when inspecting this table. Further, this table compares 2 Cross-Encoders and 8 Bi-Encoders, whose respective trade-offs should be considered. Scores are nDCG@10.</figDesc><table /><note><p><p><p><p><p><p><p><p><p>?: In-domain performance. ?: Results from</p><ref type="bibr" target="#b24">[25]</ref></p>. ?: Results from</p><ref type="bibr" target="#b10">[11]</ref></p>. ?: Results from</p><ref type="bibr" target="#b17">[18]</ref></p>. ?: Results from</p><ref type="bibr" target="#b15">[16]</ref> </p>except when marked with ?. Other scores are from us. Average scores do not include MS MARCO.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Prompts searched over ordered by increasing complexity. The example is the shortest query-doc match from FiQA<ref type="bibr" target="#b14">[15]</ref>. The sum of log probabilities from {query} is used as the re-rank score. Overflowing tokens are truncated from the left of {doc}.</figDesc><table><row><cell>Id</cell><cell cols="2">TREC-Covid NFCorpus</cell><cell>FiQA</cell><cell cols="2">Touch?-2020 DBPedia SciFact</cell><cell>Avg</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>0.68803</cell><cell>0.30630</cell><cell>0.25407</cell><cell>0.34707</cell><cell cols="2">0.32016 0.61100 0.42111</cell></row><row><cell cols="3">SGPT-Neo2.7B -Base Prompts</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>0.76104</cell><cell>0.30014</cell><cell>0.32123</cell><cell>0.23390</cell><cell cols="2">0.33272 0.61284 0.42698</cell></row><row><cell>B</cell><cell>0.67918</cell><cell>0.29101</cell><cell>0.32468</cell><cell>0.26027</cell><cell cols="2">0.33584 0.61114 0.41702</cell></row><row><cell>C</cell><cell>0.75144</cell><cell>0.32429</cell><cell>0.35669</cell><cell>0.25349</cell><cell cols="2">0.34872 0.64377 0.44640</cell></row><row><cell>D</cell><cell>0.74194</cell><cell>0.32671</cell><cell>0.33099</cell><cell>0.25514</cell><cell cols="2">0.34126 0.63182 0.43798</cell></row><row><cell>E</cell><cell>0.71957</cell><cell>0.32502</cell><cell>0.36201</cell><cell>0.26621</cell><cell cols="2">0.35977 0.63839 0.44516</cell></row><row><cell>F</cell><cell>0.75060</cell><cell>0.32509</cell><cell>0.35246</cell><cell>0.24975</cell><cell cols="2">0.35814 0.64710 0.44719</cell></row><row><cell>G</cell><cell>0.76223</cell><cell>0.33283</cell><cell>0.36866</cell><cell>0.25546</cell><cell cols="2">0.36684 0.67555 0.46026</cell></row><row><cell>H</cell><cell>0.76880</cell><cell>0.33457</cell><cell>0.37940</cell><cell>0.25135</cell><cell cols="2">0.36218 0.66105 0.45956</cell></row><row><cell>I</cell><cell>0.71819</cell><cell>0.31729</cell><cell>0.33234</cell><cell>0.28724</cell><cell cols="2">0.33688 0.65875 0.44178</cell></row><row><cell cols="3">SGPT-Neo2.7B -One-shot prompts</cell><cell></cell><cell></cell><cell></cell></row><row><cell>J</cell><cell>0.67664</cell><cell>0.32655</cell><cell>0.33772</cell><cell>0.25806</cell><cell cols="2">0.34739 0.62981 0.42936</cell></row><row><cell>K</cell><cell>0.66603</cell><cell>0.32612</cell><cell>0.34823</cell><cell>0.25869</cell><cell cols="2">0.34768 0.63656 0.43055</cell></row><row><cell cols="5">SGPT-Neo2.7B -Only Yes/No token probabilities are compared</cell><cell></cell></row><row><cell>L</cell><cell>0.53819</cell><cell>0.18379</cell><cell>0.05593</cell><cell>0.20737</cell><cell cols="2">0.10268 0.40389 0.24864</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Prompt</figDesc><table /><note><p><p><p>search results. Ids correspond to Table</p>7</p>. For prompt L, softmax was only computed over the token ids for Yes and No. Scores are nDCG@10.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Additional SGPT Cross-Encoder scores on BEIR. Bounds are the maximum achievable score, given the first-stage BM25 results. We report additional Max Re-rank=10 scores using OpenAI's search endpoint: TREC-COVID: 0.545 (Ada), 0.539 (Davinci); SciFact: 0.670 (Ada), 0.658 (Davinci). Scores are nDCG@10. Average scores do not include MS MARCO.</figDesc><table><row><cell></cell><cell>Top 0</cell><cell cols="2">Re-rank Top 10</cell><cell cols="2">Re-rank Top 100</cell></row><row><cell>Model (?)</cell><cell>[23]</cell><cell>SGPT-P G</cell><cell></cell><cell>SGPT-P G</cell><cell></cell></row><row><cell>Dataset (?)</cell><cell>BM25</cell><cell cols="4">125M 1.3B Bound 125M 1.3B Bound</cell></row><row><cell>MS MARCO</cell><cell>0.228</cell><cell>0.237 0.245</cell><cell>0.383</cell><cell>0.232 0.267</cell><cell>0.664</cell></row><row><cell>TREC-COVID</cell><cell>0.688</cell><cell>0.695 0.694</cell><cell>0.750</cell><cell>0.693 0.735</cell><cell>0.988</cell></row><row><cell>BioASQ</cell><cell>0.488</cell><cell>0.507 0.514</cell><cell>0.588</cell><cell>0.511 0.528</cell><cell>0.798</cell></row><row><cell>NFCorpus</cell><cell>0.306</cell><cell>0.314 0.316</cell><cell>0.364</cell><cell>0.298 0.327</cell><cell>0.513</cell></row><row><cell>NQ</cell><cell>0.326</cell><cell>0.336 0.354</cell><cell>0.514</cell><cell>0.319 0.367</cell><cell>0.788</cell></row><row><cell>HotpotQA</cell><cell>0.602</cell><cell>0.633 0.645</cell><cell>0.690</cell><cell>0.658 0.688</cell><cell>0.808</cell></row><row><cell>FiQA-2018</cell><cell>0.254</cell><cell>0.281 0.297</cell><cell>0.363</cell><cell>0.280 0.340</cell><cell>0.595</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.330</cell><cell>0.339 0.343</cell><cell>0.390</cell><cell>0.307 0.322</cell><cell>0.619</cell></row><row><cell>TREC-NEWS</cell><cell>0.405</cell><cell>0.400 0.402</cell><cell>0.492</cell><cell>0.393 0.443</cell><cell>0.831</cell></row><row><cell>Robust04</cell><cell>0.425</cell><cell>0.419 0.434</cell><cell>0.508</cell><cell>0.382 0.427</cell><cell>0.854</cell></row><row><cell>ArguAna</cell><cell>0.472</cell><cell>0.394 0.383</cell><cell>0.754</cell><cell>0.315 0.299</cell><cell>0.952</cell></row><row><cell>Touch?-2020</cell><cell>0.347</cell><cell>0.340 0.335</cell><cell>0.467</cell><cell>0.268 0.261</cell><cell>0.881</cell></row><row><cell>CQADupStack</cell><cell>0.326</cell><cell>0.348 0.360</cell><cell>0.426</cell><cell>0.357 0.394</cell><cell>0.637</cell></row><row><cell>Quora</cell><cell>0.808</cell><cell>0.794 0.809</cell><cell>0.914</cell><cell>0.764 0.791</cell><cell>0.982</cell></row><row><cell>DBPedia</cell><cell>0.320</cell><cell>0.328 0.336</cell><cell>0.397</cell><cell>0.329 0.356</cell><cell>0.651</cell></row><row><cell>SCIDOCS</cell><cell>0.165</cell><cell>0.173 0.177</cell><cell>0.245</cell><cell>0.171 0.185</cell><cell>0.465</cell></row><row><cell>FEVER</cell><cell>0.649</cell><cell>0.735 0.718</cell><cell>0.824</cell><cell>0.762 0.729</cell><cell>0.931</cell></row><row><cell>Climate-FEVER</cell><cell>0.186</cell><cell>0.194 0.191</cell><cell>0.281</cell><cell>0.179 0.167</cell><cell>0.474</cell></row><row><cell>SciFact</cell><cell>0.611</cell><cell>0.626 0.645</cell><cell>0.729</cell><cell>0.621 0.652</cell><cell>0.825</cell></row><row><cell>Average</cell><cell>0.428</cell><cell>0.436 0.442</cell><cell>0.539</cell><cell>0.423 0.445</cell><cell>0.755</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Additional results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10 for Quora and Spearman correlation for STS-B. bf=BitFit. OOD=Out-of-domain, to contrast these numbers from in-domain numbers in<ref type="bibr" target="#b27">[28]</ref>. However, fragments may be in-domain due to the large pre-training data of the transformer models. CQADupstack and SciDocs differ from the same-name datasets in BEIR.</figDesc><table><row><cell>Models (?)</cell><cell>[23]</cell><cell cols="2">SGPT-BE-nli SGPT-BE-nli</cell></row><row><cell cols="3">1 Query &amp; 3 Documents (?) BM25 SGPT-125M</cell><cell>SGPT-5.8B</cell></row><row><cell>deep learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>artificial intelligence</cell><cell>0</cell><cell>0.692</cell><cell>0.579</cell></row><row><cell>deep throating</cell><cell>1.08</cell><cell>0.460</cell><cell>0.347</cell></row><row><cell>artificial snow</cell><cell>0</cell><cell>0.422</cell><cell>0.247</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Constantin Eichenberg</rs> and <rs type="person">Samuel Weinbach</rs> for insightful discussions and valuable feedback throughout the project. We thank <rs type="person">Robert Baldock</rs>, <rs type="person">Marco Bellagente</rs> and <rs type="person">Koen Oostermeijer</rs> for reading drafts of this paper. This work has been supported by <rs type="funder">OpenAI</rs> under the academic access program.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model (?)</head><note type="other">OpenAI</note><p>The idea was to help the model learn the special role of the brackets. It did not help. asym=Two-tower model with separate transformers for queries and documents. SGPT-125M-weightedmean-msmarco-specb performs 3% worse than SBERT-base-msmarco on average. SGPT-125M-weightedmean-msmarco-specb-bitfit performs 1% worse than SBERT-base-msmarco-bitfit on average. Interestingly, Curie beats SGPT-5.8B on this subset, but does not on the bigger subset in Table <ref type="table">6</ref>. Scores are nDCG@10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preetham</forename><surname>Gali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levy-Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kip</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Songz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Weinbach</surname></persName>
		</author>
		<title level="m">GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326.7</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175.3</idno>
		<title level="m">Chris Tar, et al. 2018. Universal sentence encoder</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">SPLADE v2: Sparse lexical and expansion model for information retrieval</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086.3</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027.10</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821.7</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685.7</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09118</idno>
		<title level="m">Towards Unsupervised Dense Information Retrieval with Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World&apos;s Largest and Most Powerful Generative Language Model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alvi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353.7</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Www&apos;18 open challenge: financial opinion mining and question answering</title>
		<author>
			<persName><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1941" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10005.5</idno>
		<title level="m">Text and Code Embeddings by Contrastive Pre-Training</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hern?ndez ?brego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07899</idno>
		<title level="m">Large Dual Encoders Are Generalizable Retrievers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno>training. 3</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="9" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683.9</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084.3</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 3, 4, 5, 9, 15</date>
			<publisher>Now Publishers Inc</publisher>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08240.4</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08663.3</idno>
		<title level="m">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax.4" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06979</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06274.7</idno>
		<title level="m">Lite Self-training Makes Efficient Few-shot Learners</title>
		<meeting><address><addrLine>LiST</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426.7</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199.3</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
