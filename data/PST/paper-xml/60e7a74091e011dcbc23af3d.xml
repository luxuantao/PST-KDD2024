<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELFCF: A Simple Framework for Self-supervised Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-07">7 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
						</author>
						<title level="a" type="main">SELFCF: A Simple Framework for Self-supervised Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-07">7 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.03019v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Self-supervised Learning</term>
					<term>Recommender Systems</term>
					<term>Siamese Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering (CF) is widely used to learn an informative latent representation of a user or item from observed interactions. Existing CF-based methods commonly adopt negative sampling to discriminate different items. That is, observed user-item pairs are treated as positive instances; unobserved pairs are considered as negative instances and are sampled under a defined distribution for training. Training with negative sampling on large datasets is computationally expensive. Further, negative items should be carefully sampled under the defined distribution, in order to avoid selecting an observed positive item in the training dataset. Unavoidably, some negative items sampled from the training dataset could be positive in the test set. Recently, self-supervised learning (SSL) has emerged as a powerful tool to learn a model without negative samples. In this paper, we propose a self-supervised collaborative filtering framework (SELFCF), that is specially designed for recommender scenario with implicit feedback. The proposed SELFCF framework simplifies the Siamese networks and can be easily applied to existing deep-learning based CF models, which we refer to as backbone networks. The main idea of SELFCF is to augment the output embeddings generated by backbone networks, because it is infeasible to augment raw input of user/item ids. We propose and study three output perturbation techniques that can be applied to different types of backbone networks including both traditional CF models and graph-based models. The framework enables learning informative representations of users and items without negative samples, and is agnostic to the encapsulated backbones. By encapsulating two popular recommendation models into the framework, our experiments on three datasets show that the best performance of our framework is comparable or better than the supervised counterpart. We also show that SELFCF can boost up the performance by up to 8.93% on average, compared with another self-supervised framework as the baseline. Source codes are available at: https://github.com/enoche/SelfCF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECOMMENDER systems aim to provide users with per- sonalized products or services. They can help handle the increasing online information overload problem and improve customer relationship management. Collaborative Filtering (CF) is a canonical recommendation technique, which predicts interests of a user by aggregating information from similar users or items. In detail, existing CF-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> learn latent representations of users and items, by first factorizing the observed interaction matrix, then predicting the potential interests of user-item pairs based on the dot-product of learned embeddings. However, existing CF models rely heavily on negative sampling techniques to discriminate against different items, because negative samples are not naturally available.</p><p>Nevertheless, the negative sampling techniques suffer from the following limitations. Firstly, they introduce additional computation and memory costs. In existing CFbased methods, the negative sampling algorithm should be carefully designed in order to not select the observed positive user-item pairs. Specifically, to sample one negative user-item pair for a specific user, the algorithm is required to check its conflicts with all the observed positive items interacted by this user. As a result, much computation is needed for users who have a large number of interactions.</p><p>‚Ä¢ X. Zhou, A. Sun, Y. Liu, J. <ref type="bibr">Zhang</ref>  Secondly, even if non-conflicted negative samples are selected for a user, the samples may fall into the future positive items of the user. The reason is that the unobserved useritem pairs can be either true negative instances (i.e., the user is not interested in interacting with these items) or missing values (e.g., positive pairs in the test set) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Although another line of work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have get rid of negative sampling and take the full unobserved interactions as negative samples, they may still treat a future positive sample as negative.</p><p>Self-supervised learning (SSL) models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, that are proposed recently, provide us a possible solution to tackle the aforementioned limitations. SSL enables training a model by iteratively updating network parameters without using negative samples. Thus, it presents a way to scale recommender systems into big data scenarios. Research in various domains ranging from Computer Vision (CV) to Natural Language Processing (NLP), has shown that SSL is possible to achieve competitive or even better results than supervised learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The underlying idea is to maximize the similarity of representations obtained from different distorted versions of a sample using a variant of Siamese networks <ref type="bibr" target="#b13">[14]</ref>. Siamese networks usually include two symmetric networks (i.e., online network and target network) for inputs comparing. The problem with only positive sampling in model training is that, the Siamese networks collapse to a trivial constant solution <ref type="bibr" target="#b10">[11]</ref>. Thus, in recent work, BYOL <ref type="bibr" target="#b9">[10]</ref> and SIMSIAM <ref type="bibr" target="#b10">[11]</ref> introduce asymmetry to the network architecture by adding parameter update technique. Specifically, in the network architecture, an additional "predictor" network is stacked onto the online encoder. For parameter update, a special "stop gradient" operation is highlighted to prevent solution collapsing. The difference between BYOL and SIMSIAM is that SIMSIAM does not need an additional momentum encoder, and its target network shares parameters with the online network. We will illustrate the architectures in detail (see Fig. <ref type="figure">1</ref>) in the related work section.</p><p>To the best of our knowledge, BUIR <ref type="bibr" target="#b5">[6]</ref> is the only framework for CF to learn user and item latent representations without negative samples. BUIR is derived from BYOL <ref type="bibr" target="#b9">[10]</ref>. Similar to BYOL, BUIR employs two distinct encoder networks (i.e., online and target networks) to address the recurring of trivial constant solutions in SSL. In BUIR, the parameters of the online network are optimized towards that of the target network. At the same time, parameters of the target network are updated based on momentum-based moving average <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> to slowly approximate the online network <ref type="bibr" target="#b5">[6]</ref>. As BUIR is built upon BYOL, which stems from vision domain, its architecture is redundant and difficult to fuse information from high-order neighbors for efficient recommendation, because of the design of the momentum-based parameter updating. The SIMSIAM network is originally proposed in vision domain as well. The input is an image, and techniques for data augmentation on images are relatively mature <ref type="bibr" target="#b16">[17]</ref>, such as random cropping, resizing, horizontal flipping, color jittering, converting to grayscale, Gaussian blurring, and solarization. As for a pair of user id and item id that is observed in implicit feedback, there is no standard solution on how to distort it while keep its representation invariant.</p><p>In this paper, we propose a self-supervised collaborative filtering framework, which performs posterior perturbation on user and item output embeddings, to obtain a contrastive pair. On architecture design, our framework uses only one encoder that is shared by the online network and the target network. This design makes our framework fundamentally different from that of SIMSIAM and BYOL. In addition, we do not rely on a momentum encoder to update the target network. Instead, we use three posterior embedding perturbation techniques (that are analogous to input data augmentation of an image) to generate different but invariant views from the two networks. An additional benefit of posterior embedding perturbation is that the framework can take the internal implementation of the encapsulated backbones as black-box. Conversely, BUIR adds momentumbased parameter updating to the backbones in order to generate different views. Our experiments on three realworld datasets validate that the proposed SSL framework is able to learn informative representation solely based on positive user-item pairs. In our experiments, we encapsulate two popular CF-based models into the framework, and the results on Top-K item recommendation are competitive or even better than their supervised counterparts.</p><p>We summarize our contributions as follows: ‚Ä¢ We propose a novel framework, SELFCF, that learns latent representations of users/items solely based on positively observed interactions. The framework uses posterior output perturbation to generate different augmented views of the same user/item embeddings for contrastive learning.</p><p>‚Ä¢ We design three output perturbation techniques: historical embedding, embedding dropout, and edge pruning to distort the output of the backbone. The techniques are applicable to all existing CF-based models as long as their outputs are embedding-like.</p><p>‚Ä¢ We investigate the underlying mechanisms of the framework by performing ablation study on each component. We find the presentations of user/item can be learnt even without the "stop gradient" operator, which shows different behaviors from previous SSL frameworks (e.g., BYOL <ref type="bibr" target="#b9">[10]</ref> and SIMSIAM <ref type="bibr" target="#b10">[11]</ref>). ‚Ä¢ Finally, we conduct experiments on three public datasets by encapsulating two popular backbones. Results show SELFCF is competitive or better than their supervised counterpart and outperforms existing SSL framework by up to 8.93% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first review the CF technique, then summarize the current progress of SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Collaborative Filtering</head><p>To tackle information overload and information asymmetry problems in e-commerce platforms, recommender systems are proposed to provide personalized item recommendations. CF is a typical and prevalent technique adopted in modern recommender systems <ref type="bibr" target="#b17">[18]</ref>. The core concept is that similar users tend to have similar tastes on items. To tackle the data sparsity and scalability of CF, more advanced method, Matrix Factorization (MF), decomposes the original sparse matrix to low-dimensional matrices with latent factors/features and less sparsity. To learn informative and compressed latent features, deep learning based models are further proposed for recommendation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>With the emerge of graph convolutional networks (GCNs), which generalize convolutional neural networks (CNNs) on graph-structured data, GCN-based CF is widely researched recently <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The user-item interaction matrix naturally can be treated as a bipartite graph. GCNbased CF takes advantage of fusing both high-order information and the inherent graph structure. GCNs are used to propagate information using the normalized adjacency matrix and aggregate information from neighbors via the nonlinear activation and linear transformation layers. He et al. <ref type="bibr" target="#b3">[4]</ref> simplified the GCNs architecture by removing the feature transformation as well as nonlinear activation layers as they impose negative effect on recommendation performance. In <ref type="bibr" target="#b22">[23]</ref>, the authors added a residual preference learning on GCN and obtained better recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Learning</head><p>SSL has achieved competitive results on various tasks in vision and natural language processing domains. We review two lines of work on SSL.</p><p>Contrastive learning. Contrastive approaches learn representations by attracting the positive sample pairs and repulsing the negative sample pairs <ref type="bibr" target="#b13">[14]</ref>. A line of work <ref type="bibr" target="#b12">[13]</ref>, (c) The SELFCF framework.</p><p>Fig. <ref type="figure">1</ref>: Comparison of Siamese network architectures. BYOL's an SIMSIAM's input x is an image. The input to SELFCF is the interaction pairs of users (u) and items (i).</p><p>[16], <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> are developed based on this concept. These work benefit from a large number of negative samples, which require a memory bank <ref type="bibr" target="#b24">[25]</ref> or a queue <ref type="bibr" target="#b15">[16]</ref> to store negative samples. In <ref type="bibr" target="#b27">[28]</ref>, the authors integrate supplemental signal into supervised baselines for contrastive learning, and show that it performs better than their baselines.</p><p>Siamese networks. Siamese networks <ref type="bibr" target="#b28">[29]</ref> are general models for comparing entities. BYOL <ref type="bibr" target="#b9">[10]</ref> and SIMSIAM <ref type="bibr" target="#b10">[11]</ref> are two specializations of the Siamese network that achieve remarkable results by only using positive samples. BYOL proposed two coupled networks (i.e., online and target networks) that are optimized and updated iteratively. In detail, the online network is optimized towards the target network, while the target network is updated with a moving average of the online network to avoid collapse. On the contrary, SIMSIAM verified that a "stop gradient" operator is crucial in preventing collapse. We summarize the architectures of Siamese networks in Fig. <ref type="figure">1</ref>. For BYOL, the parameter Œ∏ of f Œ∏ is different from the parameter œÜ of f œÜ , as shown in Fig. <ref type="figure">1a</ref>. In SIMSIAM, the encoder of online and target networks share the same weights. Furthermore, BYOL requires an additional momentum encoder to update the parameters of the target network based on the optimized online network. Compared with SIMSIAM, the dash line between the online and target encoders in Fig. <ref type="figure">1a</ref> can be omitted, resulting in Fig. <ref type="figure">1b</ref>. Derived from BYOL, the recently proposed selfsupervised framework, BUIR, learns the representation of users and items solely on positive interactions. It introduces different views by differentiating parameters of online and target networks. However, the framework modifies the underlying logic of the encapsulated graph-based CF models for the sake of introducing contrastive user-item pairs. In our solution, we choose to augment the output of encoder f to generate two different but related embeddings for representation learning. For comparison, we present our proposed framework specialized for CF, SELFCF, in Fig. <ref type="figure">1c</ref>. The framework shares the same encoder between online and target networks, thus reduces the unnecessary memory and computational resources for storing and executing an additional encoder of the target network. We elaborate our framework in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE SELFCF FRAMEWORK</head><p>Our framework (shown in Fig. <ref type="figure">1c</ref> ) partially inherits the Siamese network architecture of SIMSIAM, as shown in Fig. <ref type="figure">1</ref>. In our framework, SELFCF, the goal is to learn informative representations of users and items based on positive user-item interactions only. The latent embeddings of users and items are learnt from the online network. Analogous to convolutions <ref type="bibr" target="#b29">[30]</ref>, which is a successful inductive bias via weight-sharing for modeling translation-invariance, the weight-sharing Siamese networks can model invariance with regard to more complicated transformations (e.g., data augmentations) <ref type="bibr" target="#b10">[11]</ref>. The online and target networks in SELFCF use a same copy of the parameters as well as the backbone for modeling representation invariance. In addition, we drop the momentum encoder as used in BYOL and BUIR. As a result, with the same input, the online and target networks will generate the same output which makes the loss totally vanished. We will discuss how to tackle this issue in the following section.</p><p>When considering data augmentations of input in CF, it is not a trivial task to distort the positive samples. In vision domain, where SSL is popularly applied, images can be easily distorted under a wide range of transformations. However, positive user-item pairs are difficult to be distorted while preserving their representation invariance. We use the following embedding perturbation techniques to achieve the same effect. For reasons of clarity, we denote bold value E as the embedding matrix of users and items within a batch, and differentiate the embedding matrix of users with E u , vice visa. The value e in lowercase denotes the embedding of a user or item, specified as e u or e i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Augmentation via Output Perturbation</head><p>In vision, researchers use image transformations to augment input data and generate two different but relative reviews. Instead, our framework augments the output embeddings of users and items to generate two contrastive reviews. We propose three methods to introduce embedding perturbation in our framework, shown in Fig. <ref type="figure">2</ref>. The historical</p><formula xml:id="formula_0">User-Item interaction User Item ùë¢ 0 ùë¢ 1 ùë¢ 2 ùë¢ 3 ùëñ 1 ùëñ 0 ùëñ 2 Mini-Batch: ùîπ ùë° ùë¨ ùë° ùë¨ ùë°‚àí1 ‡∑© ùë¨ ùë° ‡∑© ùë¨ ùë° = ùë¨ ùë°‚àí1 ‚Ä¢ ùúè + ùë¨ ùë° ‚Ä¢ (1 ‚àí ùúè) ‡∑© ùë¨ ùë° ‡∑© ùë¨ ùë° = ùë¨ ùë° ‚Ä¢ ùêµùëíùëüùëõùëúùë¢ùëôùëôùëñ(ùëù) ‡∑© ùë¨ ùë° ùê¥ ùëùùëüùë¢ùëõùëíùëë ‡∑© ùë¨ ùë° = ùë¨ ùë° ‚Ä¢ ùê¥ ùëùùëüùë¢ùëõùëíùëë</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Historical Embedding Embedding Dropout Edge Pruning</head><p>Embedding generated by backbone encoder ùëì Fig. <ref type="figure">2</ref>: Illustration of output perturbation performed on a mini-batch. The perturbed embedding is denoted as ·∫º.</p><p>embedding and embedding dropout are general techniques for output augmentation in our framework, while the edge pruning is specially designed for graph-based CF models.</p><p>Historical embedding. First, we introduce embedding perturbation by utilizing historical embeddings <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> from prior training iterations. Specifically, we use a momentum update to generate the contrastive embeddings in the target network. Suppose E t is the embeddings generated by a backbone encoder f in a mini-batch B t . The perturbed embeddings ·∫ºt is calculated by combining of the output embeddings E t with the historical embedding E t‚àí1 :</p><formula xml:id="formula_1">·∫ºt = E t‚àí1 œÑ + E t (1 ‚àí œÑ )<label>(1)</label></formula><p>where œÑ is a parameter controls the proportion of information preserved from a prior iteration.</p><p>Embedding dropout. Second, we apply the embedding dropout scheme to perturb the embeddings of users and items from the target network. In classical CF models, the parameters are not modified until the loss is backpropagated. With the same input, to avoid null loss resulted from these models, our framework adopts embedding dropout on the resulted users' and items' vectors, which is analogous to node dropout <ref type="bibr" target="#b32">[33]</ref>. In this way, the framework is able to generate two different but related views on the output, which are then feed into the loss function for optimization. The resulted embeddings under a dropout ratio p is calculated as:</p><formula xml:id="formula_2">·∫ºt = E t ‚Ä¢ Bernoulli(p)<label>(2)</label></formula><p>Edge pruning. As for graph-based CF models, the edge pruning method used in <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> provides an alternative way to augment the output embeddings. With the user-item bipartite graph, we randomly prune a certain proportion of edges from the graph in each batch. The output embeddings are updated by aggregating the embeddings of neighbors.</p><p>With the same positive user-item pair, the output is distorted with different adjacency matrix (neighbors). Let A pruned be the pruned adjacency matrix, then the resulted embeddings with edge pruning denote as:</p><formula xml:id="formula_3">·∫ºt = E t ‚Ä¢ A pruned<label>(3)</label></formula><p>Note that, in implementation, edge pruning would require to calculate the adjacency matrix of users and items, which is more expensive in computation than the embedding dropout technique.</p><p>To summarize, our framework augments the output via embedding perturbation in the target network instead of distorting the input directly as commonly used in vision domain. It is worth noting that the historical embedding perturbation performs on embeddings from prior and current iteration, the embedding dropout perturbs the current embedding with noise, and the edge pruning method operates on future embeddings generated by stacking one more convolutional layer on current embeddings. We will discuss their performance with regard to this perspective in experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Loss Function</head><p>Our framework, as shown in Fig. <ref type="figure">1c</ref>, takes a positive useritem pair (u, i) as input. The (u, i) pair is initially processed by an encoder network f in a backbone (e.g. LightGCN <ref type="bibr" target="#b3">[4]</ref>). The output of the encoder f is then copied to the target network for embedding perturbation. Formally, we denote the output of the encoder from the online network as (e u , e i ) = f (u, i). Finally, the linear predictor in our framework transforms the output (e u , e i ) with ( ƒóu , ƒói ) = h(e u , e i ) and matches it to the perturbed embeddings (·∫Ω u , ·∫Ωi ) = g(e u , e i ) in other view like in BYOL <ref type="bibr" target="#b9">[10]</ref> and SIMSIAM <ref type="bibr" target="#b10">[11]</ref>.</p><p>We define a symmetrized loss function as the negative cosine similarity between ( ƒóu , ·∫Ωi ) and (·∫Ω u , ƒói ):</p><formula xml:id="formula_4">L = 1 2 C( ƒóu , ·∫Ωi ) + 1 2 C(·∫Ω u , ƒói )<label>(4)</label></formula><p>Function C(‚Ä¢, ‚Ä¢) in the above equation is defined as:</p><formula xml:id="formula_5">C(e u , e i ) = ‚àí (e u ) T e i ||e u || 2 ||e i || 2<label>(5)</label></formula><p>where || ‚Ä¢ || is l 2 -norm. The total loss is averaged over all user-item pairs in a mini-batch. Finally, we stop gradient on the target network and force the backpropagation of loss over the online network only. We follow the stop gradient (sg) operator as in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and implement the operator by updating Equation 4 as:</p><formula xml:id="formula_6">L = 1 2 (C( ƒóu , sg(·∫Ω i )) + C(sg(·∫Ω u ), ƒói ))<label>(6)</label></formula><p>With the stop gradient operator, the target network receives no gradient from (·∫Ω u , ·∫Ωi ). However, the encoder f in the online network receives gradients from user-item pair ( ƒóu , ƒói ), and optimizes its parameters towards the global optimum.</p><p>The pseudo-code of SELFCF is in Algorithm 1.</p><p>Algorithm 1 PyTorch-style pseudo-code for SELFCF.</p><p>Require: user-item interaction set B Require: f, h, g encoder, predictor, output perturbation 1: for B t in B do load a batch 2:</p><formula xml:id="formula_7">(E t u , E t i ) = f (B t ) output of encoder 3: ( ƒñt u , ƒñt i ) = h(E t u , E t i ) output of predictor 4: ( ·∫ºt u , ·∫ºt i ) = g(E t u , E t i ) output of perturbation 5: L = 1 2 C( ƒñt u , sg( ·∫ºt i )) + C(sg( ·∫ºt u ), ƒñt i ) Equation<label>6</label></formula><p>6:</p><p>L.backward() back-propagate return s(e u , e i ) Equation 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Top-K Recommendation</head><p>Classical CF methods recommend top-K items by ranking scores of the inner product of a user embedding with all candidate item embeddings. However, in SSL, we minimize the predicted loss between u and i for each positive interaction (u, i). Intuitively, we predict the future interaction score based on a cross-prediction task <ref type="bibr" target="#b5">[6]</ref>. That is, we both predict the interaction probability of item i with u and the probability of user u with i. Given (e u , e i ) being the output of the encoder f , the recommendation score is calculated as:</p><formula xml:id="formula_8">s(e u , e i ) = h(e u ) ‚Ä¢ (e i ) T + e u ‚Ä¢ h(e i ) T<label>(7)</label></formula><p>It is worth noting that since the encoder f is shared between both online and target networks, we use the representations obtained from the online network to predict top-K items for each user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the framework on three publicly available datasets and compare its performance with BUIR <ref type="bibr" target="#b5">[6]</ref> by encapsulating two popular CF-based baselines. The CF baselines serve as a supervised counterpart compared with our self-supervised framework. All baselines as well as the frameworks are trained on a single GeForce RTX 2080 Ti (11 GB).</p><p>We list the research questions addressed in our evaluation as follows: RQ1: Whether the self-supervised baselines that only leverage positive user-item interactions can outperform their supervised counterparts? RQ2: How SELFCF shapes the recommendation results for cold-start and loyal users? RQ3: Why SELFCF works, and which component is essential in preventing collapsing? We address the first research question by evaluating our framework against supervised baselines with 3 datasets under 4 evaluation metrics. Next, we dive into the recommendation results of the baselines under both supervised and self-supervised settings and analyze their performance on users with different number of interactions. Finally, to investigate the underlying mechanisms of SELFCF, we perform ablation study on the components of SELFCF, such as the linear predictor, the loss function etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description.</head><p>We choose the evaluated datasets carefully by considering the following principles in order to introduce as much as diversity.</p><p>Sparsity: Data sparsity <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> is measured as the number of interactions divided by the product of the number of users and the number of items. Classic CF methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> usually show better performance on dense datasets. Domain: Interactions within the same domain may exhibit similar patterns across the datasets. Hence, we choose evaluation datasets from three different domains ranging from education to e-commerce. Released Date: Existing recommender systems usually evaluated on out-dated datasets nearly collected 10 years ago. With the rapid growth of e-commerce platforms, user behaviors are gradually shaped with online purchasing.</p><p>With the aforementioned considerations, our framework is evaluated on one classic dataset in recommender systems and two recently collected public datasets. We describe each of the datasets with regard to the above selection principles and summarize the statistics of the datasets in Table <ref type="table" target="#tab_3">1</ref>.</p><p>‚Ä¢ MovieLens-1M: This is a widely used movie rating dataset <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In MovieLens, each entry records the rating value of a user and film pair. Similar to <ref type="bibr" target="#b41">[42]</ref>, we transform the rating scores into binary values, so that each entry is either 1 or 0 indicating whether the user rated the movie. As shown in Table Our framework is mainly compared with BUIR <ref type="bibr" target="#b5">[6]</ref>, a selfsupervised framework that is derived from BYOL <ref type="bibr" target="#b9">[10]</ref>. Its architecture follows the Siamese network in Fig. <ref type="figure">1a</ref>. To compare the performance of our proposed framework, we encapsulate two representative and state-of-the-art models into the frameworks. The state-of-the-art models ranges from traditional matrix factorization (BPR) to graph-based models (LightGCN).</p><p>‚Ä¢ BPR <ref type="bibr" target="#b4">[5]</ref>: A matrix factorization model optimized by a pairwise ranking loss in a Bayesian way.</p><p>‚Ä¢ <ref type="bibr">LightGCN [4]</ref>: This is a simplified graph convolution network that only performs linear propagation and aggregation between neighbors. The hidden layer embeddings are averaged to calculate the final user/item embeddings for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics.</head><p>We use Recall@K and N DCG@K computed by the allranking protocol as the evaluation metrics for recommendation performance comparison. In the recommendation phase, all items that have not been interacted with a specific user are regarded as candidates. That is, we do not use sampled evaluation. Formally, we define I r u (i) as the i-th ranked item recommended for u, I[‚Ä¢] is the indicator function, and I t u is the set of items that user u interacted in the testing data.</p><p>Recall@K for users u is:</p><formula xml:id="formula_9">Recall@K(u) = K i=1 I[I r u (i) ‚àà I t u ] |I t u |<label>(8)</label></formula><p>The Discounted Cumulative Gain (DCG@K) is:</p><formula xml:id="formula_10">DCG@K(u) = K i=1 2 I[I r u (i)‚ààI t u ] ‚àí 1 log(i + 1)<label>(9)</label></formula><p>N DCG@K is normalized to [0, 1] with N DCG@K = DCG@K/IDCG@K, where IDCG@K is calculated by sorting the interacted items in the testing data at top and then use the formula for DCG@K. We set K = 20 and K = 50 in our experimental comparison, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyper-parameters Settings.</head><p>Same as other work <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we fix the embedding size of both users and items to 64 for all models, initialize the embedding parameters with the Xavier method <ref type="bibr" target="#b43">[44]</ref>, and use Adam <ref type="bibr" target="#b44">[45]</ref> as the optimizer. For a fair comparison, we carefully tune the parameters of each model. To be specific, a grid search is conducted to confirm the optimal settings, the number of layers is searched in {1, 2, 3, 4}, and the coefficients Œª of L 2 regularization term is tuned in {1e ‚àí2 , 1e ‚àí3 , 1e ‚àí4 , 1e ‚àí5 } for all baselines. For converge consideration, the early stopping and total epochs are fixed at 50 and 1000, respectively. To perform ablation studies on our proposed framework, we compare three variants of SELFCF based on the proposed three output perturbation techniques, namely: SELFCF (he) , SELFCF (ed) and SELFCF (ep) , respectively. Specifically, SELFCF (he) is built with historical embedding perturbation, SELFCF (ed) is with embedding dropout and SELFCF (ep) is with edge pruning. The search space for the nodes dropout and the ratio of edges pruned both tuned in {0.05, 0.1, 0.15, 0.2}. The momentum œÑ in historical embedding is tuned from 0.1 to 0.9 with a step of 0.1. We implement our model in PyTorch 4 at: https://github.com/enoche/SelfCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overall Comparison</head><p>For a fair comparison, we follow the same evaluation setting of <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> with a chronological data splitting 8:1:1 for training, validation and testing to avoid data leakage. We define the global comparison perspective as the comparison across supervised and self-supervised baselines, while the local comparison perspective as the comparison between self-supervised frameworks BUIR and SELFCF. We analyze the comparison results with regard to recommendation accuracy (Table <ref type="table" target="#tab_5">2</ref>) and framework complexity (Table <ref type="table" target="#tab_6">3</ref>) under the following perspectives:</p><p>‚Ä¢ Data sparsity. The dataset statistics in Table <ref type="table" target="#tab_3">1</ref> show the MovieLens-1M data with the highest density, and the Amazon-Video-Games data the most sparse. From the global comparison between supervised and selfsupervised methods, we observe the performance of methods can be significantly improved in the selfsupervised framework with higher data sparsity. All the best results are observed under the self-supervised version across all datasets. In addition, the graph-based model (i.e. LightGCN), achieves better results under self-supervised signal than the supervised counterpart.</p><p>The results indicate the self-supervised frameworks is able to alleviate the cold-start issue in recommendation systems. However, the self-supervised frameworks that filled with BPR show remarkable performance but still worse than its supervised one. Compared with discriminative models that differentiate positive and negative samples, the self-supervised version is better at capturing the similarity between the positive samples.</p><p>From local comparison, our framework, both SELFCF (ed) and SELFCF (ep) have better performance under every evaluation metric. ‚Ä¢ Classic CF vs. Graph-based CF. We observe the performance of BPR always dominant under supervised over self-supervised settings. Conversely, the performance of LightGCN under self-supervised learning is on par or better than that of under supervised learning. We speculate the graph-based CF model naturally encodes structural embedding that preferred for contrastive learning. Classic CF models, e.g. BPR, use pairwise learning to differentiate positive and negative useritem samples which encode less information between positive instances, resulting in a worse performance under self-supervised frameworks. ‚Ä¢ Comparison between self-supervised frameworks.</p><p>From local perspective, our proposed framework almost improve BUIR on mean performance ‚àÜ across every dataset except SELFCF (ep) on MovieLens-1M. The proposed framework with three output perturbations takes significantly improvement with dataset in high sparsity, as high as 13.9% with SELFCF (ed) on Amazon dataset.</p><p>4. https://pytorch.org/  ‚Ä¢ Output perturbation techniques in SELFCF. Within the three proposed output perturbation techniques, Table <ref type="table" target="#tab_5">2</ref> shows the history embedding technique is more appropriate with dense datasets. SELFCF (he) shows best performance in both supervised and self-supervised baselines. The embedding dropout technique is more preferable with sparse data. Among the three output perturbation techniques, the history embedding technique integrates the embedding from previous training iteration; the embedding dropout technique introduces noise on the current output embedding; and the edge pruning technique achieves embedding augmentation by merging embedding from neighbors. In dense dataset, the historical embedding technique is able to prevent the user/item embedding from smoothing with neighbor embeddings. In sparse dataset, the embedding dropout and edge pruning techniques not only can remove the noise information and preserve the salient features in the embeddings, but also is capable of integrating high-order information from multi-hop neighbors.</p><p>‚Ä¢ Parameters and run time of models. We state our framework neither adding other parameters above the encapsulated model nor adding additional networks to the model other than a linear predictor. In Table <ref type="table" target="#tab_6">3</ref>, the parameters of our framework is comparable to the original models. To the contrary, the BUIR framework uses two encoder networks with different parameters. Hence, the parameters of BUIR is doubled in all cases.</p><p>When considering the run time of the frameworks, our framework retain the same computational complexity with the original model. Besides, in case of dense dataset, much computation cost should be paid for negative sampling, our proposed framework greatly cut off the time required for training. In local view, the proposed SELFCF removes the target encoder of BUIR, and reduces the run time of graph-based models.</p><p>We conclude our analysis to address research question RQ1: graph-based model can benefit from SELFCF with up to 4.36% improvement across all evaluation metrics, while classic model encodes no structural information in output and show worse performance than its supervised counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hyper-parameter Sensitivity</head><p>To guide the selection of parameters of our framework, we preform hyper-parameter study on the performance of SELFCF. In implementation, we use the Amazon-Video-Games as the evaluated dataset and LightGCN as the backbone of SELFCF. We investigate the performance changes of our framework with regard to hyper-parameters on the momentum in historical embedding, the number of layers, the ratio of embedding dropout and the proportion of edges pruned in SELFCF (ep) .  We set the momentum value in the range of [0.1, 0.9] with a step of 0.1. From Fig. <ref type="figure" target="#fig_0">3</ref>, we observe the performance of our framework under historical embedding technique is less sensitive to the momentum. We also consider the ratio of embedding dropout p within the range of (0.00, 1) with a step of 0.05. Fig. <ref type="figure" target="#fig_0">3</ref> shows the performance of our framework is relatively more sensitive to the ratio of embedding dropout than edge pruned. Best result is observed with a low embedding dropout setting, in which the output from online and target views are much related to each other. On the contrary, the performance of SELFCF (ep) is not sensitive to the ratio of edge pruned.</p><p>For the number of layers in the graph-based models, LightGCN, we select its range from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> and plot the results in Fig. <ref type="figure" target="#fig_1">4</ref>. The changes of SELFCF (ep) is similar with that of SELFCF (ed) . Therefore, we plot the performance of the original LightGCN for comparison instead of SELFCF (ep) and attach the plots of SELFCF (he) and (ep) in Fig. <ref type="figure" target="#fig_3">6</ref> of Appendix. The performance of SELFCF declines with the increment of layers in LightGCN. The performance show different patterns to the original LightGCN, which increases within certain layers and is unable to be improved when stacked too many layers.</p><p>From the above observations, we conclude that our proposed framework is capable of boosting up the performance of recommendation for the graph-based models within few layers. And usually it is practical to fix the dropout of embedding and ratio of edge pruned at 0.05.</p><p>The hyper-parameter studies also show that three variants of SELFCF exhibit similar behaviors. Hence, we analyze recommendation result and perform ablation study on SELFCF (ed) in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Diving into the Recommendation Results</head><p>In our framework, we recommend top-K items to users relying solely on positive user-item interaction pairs. We further study how our framework differentiate with the supervised models in recommendation results. Specifically, we encapsulate LightGCN into our framework, and compare the recommendation results between SELFCF and LightGCN with regard to users' degree on Amazon-Video-Games. We plot the results in Fig. <ref type="figure" target="#fig_2">5</ref>. On metrics Recall@50 and NDCG@50, we see SELFCF outperforms LightGCN in every categories. Our proposed framework is able to alleviate the cold-start issue to certain extent. However, the most significantly improvement is observed on users with a high degree, occupied about 0.89% in the testing dataset. From our data analysis, we find users with a high degree of interactions in the training are prefer to select items with a low degree in the testing. Thus, it is difficult to recommend the right items to these users. Our framework doubles the recommendation performance on these active users, which can improve the user retention rate of the platform.</p><p>We speculate the underlying reason is that for these users the supervised models sample a large number of unobserved but potentially positive items for training, which makes the models unable to consider the negatively sampled items in recommendation list.</p><p>Regarding research question RQ2, SELFCF can boost up the recommendation performance of all users. Especially for loyal users, it doubles the recommendation accuracy of LightGCN compared with its supervised counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY</head><p>We investigate each component in SELFCF to study its contribution to the recommendation performance. All ablation studies are performed on SELFCF (ed) trained on Amazon-Video-Games (2018). The encapsulated baseline is Light-GCN with one convolutional layer. The dropout of embedding for SELFCF (ed) is set as 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predictor</head><p>We study the recommendation performance considering predictor h under several variants. Table <ref type="table" target="#tab_7">4</ref> summarizes the variants and their recommendation performance.  Different from the predictor in SIMSIAM <ref type="bibr" target="#b10">[11]</ref>, our framework still works by removing the predictor h, but resulting in performance degeneration to some extent. A fixed random initialization with the predictor makes the selfsupervised framework difficult to learn good representations of users/items. On the contrary, a 2-layer MLP suffers from over-fitting and worsens the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss function</head><p>In contrastive learning, it is a common practice for losses measuring a cosine similarity <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b47">[48]</ref>. We substitute the loss function with cross-entropy similarity by modifying C with:</p><p>C(e u , e i ) = ‚àísof tmax(e i ) ‚Ä¢ log(sof tmax(e u ))</p><p>Table <ref type="table" target="#tab_8">5</ref> shows the results compared with cosine similarity. The cross-entropy similarity can prevent the solution collapsing to some extent. Cosine similarity captures the interaction preference between user and item directly, hence shows better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Stop-gradient</head><p>Existing researches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b48">[49]</ref> on SSL highlight the crucial role of stop-gradient in preventing solution collapsing. We evaluate with adding or removing the stop-gradient operator with/without a linear predictor. The results in Table <ref type="table" target="#tab_9">6</ref> show that our self-supervised framework works even under a completely symmetry setting. The loss function of Equation 6 is able to capture the invariant and salient features in the embeddings of users/items by dropping the noise signal. However, without the "stop gradient" operator, the performance of SELFCF decreases greatly. We speculate the loss backpropagated to both directions (online and target networks) leads to the framework unable to learn the optimal parameters of the baseline. Based on our ablation studies with regard to research question RQ3, we observe that SELFCF does not rely on a single component for preventing solution collapsing. It shows a different behavior from other self-supervised models, in which the "stop gradient" operator is identified as a crucial component to prevent solution collapsing <ref type="bibr" target="#b10">[11]</ref>. The underlying reason is that our loss function is designed as the similarity between latent embeddings of user and item, hence it can capture the preference of user to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a framework on top of Siamese networks to learn representation of users and items without negative samples or labels. We argue the self-supervised learning techniques that widely used in vision cannot be directly applied in recommendation domain. Hence we design a Siamese network architecture that perturbs the output of backbone instead of augmenting the input data. By encapsulating two popular recommendation models into the framework, our experiments on three datasets show the proposed framework is on par or better than other self-supervised framework, BUIR. The performance is also competitive to the supervised counterpart, especially on high sparse data. We hope our study will shed light on further researches on self-supervised collaborative filtering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX PERFORMANCE VARIES WITH</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Performance changes of SELFCF with respect to the embedding dropout and ratio of edge pruned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Performance changes of SELFCF (ed) and LightGCN with regard to the number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The recommendation results on different degree of users. User' degree indicates the number of interactions of a user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Performance changes of SELFCF (he) and SELFCF (ep) with regard to the number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the experimented data. Amazon dataset refers to Amazon-Video-Games (2018).</figDesc><table><row><cell>Dataset</cell><cell cols="3">Users Items Interactions Sparsity</cell></row><row><cell cols="2">MovieLens-1M 6,040 3,706</cell><cell cols="2">1,000,209 95.5316%</cell></row><row><cell>MOOC</cell><cell>82,536 1,303</cell><cell>458,453</cell><cell>99.5737%</cell></row><row><cell cols="2">Amazon(5-core) 50,677 16,897</cell><cell>454,529</cell><cell>99.9469%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Overall performance comparison. We mark the global best results on each dataset under each metric in bold face, and the second best underlined. We also calculate the mean performance improvement by SELFCF on BUIR over all evaluation metrics as ‚àÜ.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Self-supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Frameworks</cell><cell>-</cell><cell></cell><cell></cell><cell>BUIR</cell><cell></cell><cell>SELFCF (he)</cell><cell></cell><cell></cell><cell>SELFCF (ed)</cell><cell></cell><cell cols="2">SELFCF (ep)</cell></row><row><cell>Datasets</cell><cell cols="7">Metrics BPR LightGCN BPR LightGCN BPR LightGCN</cell><cell>‚àÜ</cell><cell cols="2">BPR LightGCN</cell><cell>‚àÜ</cell><cell>LightGCN</cell><cell>‚àÜ</cell></row><row><cell></cell><cell>R@20</cell><cell>0.0800</cell><cell>0.0838</cell><cell>0.0672</cell><cell>0.0827</cell><cell>0.0737</cell><cell>0.0886</cell><cell></cell><cell>0.0718</cell><cell>0.0833</cell><cell></cell><cell>0.0729</cell></row><row><cell>MovieLens-1M</cell><cell cols="2">N@20 0.2477 R@50 0.1596</cell><cell>0.2463 0.1592</cell><cell>0.1760 0.1270</cell><cell>0.2462 0.1611</cell><cell>0.2185 0.1473</cell><cell>0.2487 0.1661</cell><cell>+0.106</cell><cell>0.2167 0.1439</cell><cell>0.2410 0.1642</cell><cell>+0.079</cell><cell>0.2323 0.1538</cell><cell>-0.066</cell></row><row><cell></cell><cell cols="2">N@50 0.2380</cell><cell>0.2377</cell><cell>0.1749</cell><cell>0.2366</cell><cell>0.2147</cell><cell>0.2396</cell><cell></cell><cell>0.2107</cell><cell>0.2342</cell><cell></cell><cell>0.2267</cell></row><row><cell></cell><cell>R@20</cell><cell>0.3559</cell><cell>0.3784</cell><cell>0.3352</cell><cell>0.3714</cell><cell>0.3597</cell><cell>0.3963</cell><cell></cell><cell>0.3487</cell><cell>0.3880</cell><cell></cell><cell>0.3748</cell></row><row><cell>MOOC</cell><cell cols="2">N@20 0.2130 R@50 0.5028</cell><cell>0.2138 0.5187</cell><cell>0.1943 0.4654</cell><cell>0.2059 0.5038</cell><cell>0.2108 0.4943</cell><cell>0.2143 0.5461</cell><cell>+0.068</cell><cell>0.1882 0.4810</cell><cell>0.2125 0.5381</cell><cell>+0.026</cell><cell>0.2057 0.5126</cell><cell>+0.007</cell></row><row><cell></cell><cell cols="2">N@50 0.2477</cell><cell>0.2470</cell><cell>0.2249</cell><cell>0.2373</cell><cell>0.2427</cell><cell>0.2496</cell><cell></cell><cell>0.2194</cell><cell>0.2480</cell><cell></cell><cell>0.2382</cell></row><row><cell></cell><cell>R@20</cell><cell>0.0320</cell><cell>0.0440</cell><cell>0.0211</cell><cell>0.0454</cell><cell>0.0252</cell><cell>0.0454</cell><cell></cell><cell>0.0250</cell><cell>0.0509</cell><cell></cell><cell>0.0480</cell></row><row><cell>Amazon</cell><cell cols="2">N@20 0.0156 R@50 0.0581</cell><cell>0.0227 0.0819</cell><cell>0.0086 0.0474</cell><cell>0.0224 0.0828</cell><cell>0.0110 0.0516</cell><cell>0.0225 0.0838</cell><cell>+0.094</cell><cell>0.0108 0.0514</cell><cell>0.0250 0.0913</cell><cell>+0.139</cell><cell>0.0235 0.0873</cell><cell>+0.052</cell></row><row><cell></cell><cell cols="2">N@50 0.0219</cell><cell>0.0318</cell><cell>0.0150</cell><cell>0.0316</cell><cell>0.0174</cell><cell>0.0320</cell><cell></cell><cell>0.0171</cell><cell>0.0350</cell><cell></cell><cell>0.0331</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison of number of parameters and time cost for the baselines under supervised and self-supervised settings.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Supervised</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Self-supervised</cell><cell></cell><cell></cell></row><row><cell cols="2">Frameworks</cell><cell></cell><cell>-</cell><cell></cell><cell>BUIR</cell><cell cols="2">SELFCF (he)</cell><cell cols="2">SELFCF (ed)</cell><cell>SELFCF (ep)</cell></row><row><cell>Datasets</cell><cell>Metrics</cell><cell>BPR</cell><cell>LightGCN</cell><cell>BPR</cell><cell>LightGCN</cell><cell>BPR</cell><cell>LightGCN</cell><cell>BPR</cell><cell>LightGCN</cell><cell>LightGCN</cell></row><row><cell>MovieLens-1M</cell><cell>Param. Time/epoch</cell><cell>0.58M 9.74s</cell><cell>0.58M 26.29s</cell><cell>1.16M 3.15s</cell><cell>1.16M 20.37s</cell><cell>0.58M 2.74s</cell><cell>0.58M 16.58s</cell><cell>0.58M 3.40s</cell><cell>0.58M 14.06s</cell><cell>0.58M 19.45s</cell></row><row><cell>MOOC</cell><cell>Param. Time/epoch</cell><cell>5.15M 6.32s</cell><cell>5.15M 9.68s</cell><cell>10.30M 3.35s</cell><cell>10.30M 9.91s</cell><cell>5.15M 4.66s</cell><cell>5.15M 8.60s</cell><cell>5.15M 5.21s</cell><cell>5.15M 8.38s</cell><cell>5.15M 8.54s</cell></row><row><cell>Amazon</cell><cell>Param. Time/epoch</cell><cell>3.98M 9.25s</cell><cell>3.98M 7.62s</cell><cell>7.96M 4.53s</cell><cell>7.96M 9.63s</cell><cell>3.98M 2.66s</cell><cell>3.98M 7.81s</cell><cell>3.98M 5.25s</cell><cell>3.98M 7.75s</cell><cell>3.98M 8.39s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Impact of predictor h.</figDesc><table><row><cell>MLP h</cell><cell cols="4">Recall@20 NDCG@20 Recall@50 NDCG@50</cell></row><row><cell>Baseline(1-layer MLP)</cell><cell>0.0509</cell><cell>0.0250</cell><cell>0.0913</cell><cell>0.0350</cell></row><row><cell>No predictor</cell><cell>0.0282</cell><cell>0.0148</cell><cell>0.0536</cell><cell>0.0209</cell></row><row><cell>Fixed random init.</cell><cell>0.0198</cell><cell>0.0107</cell><cell>0.0420</cell><cell>0.0162</cell></row><row><cell>2-layer MLP</cell><cell>0.0397</cell><cell>0.0202</cell><cell>0.0768</cell><cell>0.0290</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Effectiveness of loss function.</figDesc><table><row><cell cols="5">Similarity loss Recall@20 NDCG@20 Recall@50 NDCG@50</cell></row><row><cell>Cosine</cell><cell>0.0509</cell><cell>0.0250</cell><cell>0.0913</cell><cell>0.0350</cell></row><row><cell>Cross-entropy</cell><cell>0.0222</cell><cell>0.0138</cell><cell>0.0500</cell><cell>0.0206</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Effectiveness of stop-gradient operator.</figDesc><table><row><cell>Case</cell><cell cols="6">sg Predictor Recall@20 NDCG@20 Recall@50 NDCG@50</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell>0.0509</cell><cell>0.0250</cell><cell>0.0913</cell><cell>0.0350</cell></row><row><cell>(a)</cell><cell>-</cell><cell></cell><cell>0.0286</cell><cell>0.0124</cell><cell>0.0546</cell><cell>0.0186</cell></row><row><cell>(b)</cell><cell>-</cell><cell>-</cell><cell>0.0282</cell><cell>0.0148</cell><cell>0.0546</cell><cell>0.0211</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Professor Toru Ishida from School of Science and Engineering of Waseda University for his valuable advices on shaping our paper, especially with the Abstract and Introduction sections.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
				<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bootstrapping user and item representations for one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient nonsampling factorization machines for optimal context-aware recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2400" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient heterogeneous collaborative filtering without negative sampling for recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly nonsampling learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altch√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Conference on Neural Information Processing Systems</title>
				<meeting>the 34th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">284</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 24th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selfsupervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>S√§ckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to drop: Robust graph neural network via topological denoising</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analysis of recommendation algorithms for e-commerce</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Conference on Electronic Commerce</title>
				<meeting>the 2nd ACM Conference on Electronic Commerce</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating reputation of web services under rating scarcity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Services Computing (SCC</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
				<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nais: Neural attentive item similarity model for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic connection-based social group recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="467" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cffnn: Cross feature fusion neural network for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Oguti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kurdahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to hash with graph neural networks for recommender systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1988" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling dynamic user preference via dictionary learning for sequential recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Conference on Neural Information Processing Systems</title>
				<meeting>the 34th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
