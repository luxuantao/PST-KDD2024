<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Document Cross-Lingual Summarization</title>
				<funder ref="#_jVyZtb6 #_T3dKKdM">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_MyAfXYK">
					<orgName type="full">Shanghai Science and Technology Innovation Action Plan</orgName>
				</funder>
				<funder ref="#_5UWXqUk">
					<orgName type="full">Natural Science Foundation of Jiangsu Province</orgName>
				</funder>
				<funder ref="#_YDUetez">
					<orgName type="full">Natural Science Foundation of Educational Commission of Jiangsu Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-01">1 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaohui</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
							<email>zhixuli@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
							<email>jawang1@stu.suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
							<email>zhaol@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
							<email>zgchen@iflytek.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Jilin Kexun Information Technology Co., Ltd</orgName>
								<address>
									<settlement>Jilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Jilin Kexun Information Technology Co., Ltd</orgName>
								<address>
									<settlement>Jilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Document Cross-Lingual Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-01">1 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.00586v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dataset</term>
					<term>cross-lingual summarization</term>
					<term>long-document cross-lingual summarization ??????????????</term>
					<term>??????????????????????????????</term>
					<term>?? ?????????????</term>
					<term>??????????????????????????????</term>
					<term>???</term>
					<term>????????????????????????????????GIS??????</term>
					<term>??????? ????????????????????????</term>
					<term>?????????????????????? ???????????????????????????</term>
					<term>??????????????????? ?????????????????? ??????(Lakeside Area)????????????????</term>
					<term>?????????????????? ??????????????</term>
					<term>?????????</term>
					<term>?????????</term>
					<term>??????????????</term>
					<term>???????????????</term>
					<term>????????????????????[...] ?????????(GeometricCorrection)?????????????(Resample)?????? (ResolutionMerge)?????(RadiometricEnhancement)??????????????????????? ????????????????????3?????????????</term>
					<term>????????????? ??????????????????????????[...] ???????????????</term>
					<term>???????????????????????????</term>
					<term>???? ?????????????</term>
					<term>???3?????????????????????1??2?????? ??1???</term>
					<term>????????????????????</term>
					<term>????&quot;??&quot;????</term>
					<term>????????? ??????????????????????</term>
					<term>???????????????????????? ???????????????????????</term>
					<term>??????????????????????? ???????????????????????????</term>
					<term>???????????</term>
					<term>???????? ????????????&quot;??&quot;????</term>
					<term>??????????1/3??????????????? ????????????????????????</term>
					<term>??????</term>
					<term>????????????[...] ?????????</term>
					<term>??????</term>
					<term>??????????????????????????????? ??????????????????????</term>
					<term>???????????????????????? ??????????????????????????</term>
					<term>????????????</term>
					<term>???????? ???</term>
					<term>?????????????????</term>
					<term>???????????????</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-Lingual Summarization (CLS) aims at generating summaries in one language for the given documents in another language. CLS has attracted wide research attention due to its practical significance in the multi-lingual world. Though great contributions have been made, existing CLS works typically focus on short documents, such as news articles, short dialogues and guides. Different from these short texts, long documents such as academic articles and business reports usually discuss complicated subjects and consist of thousands of words, making them non-trivial to process and summarize. To promote CLS research on long documents, we construct Perseus, the first long-document CLS dataset which collects about 94K Chinese scientific documents paired with English summaries. The average length of documents in Perseus is more than two thousand tokens. As a preliminary study on long-document CLS, we build and evaluate various CLS baselines, including pipeline and end-to-end methods. Experimental results on Perseus show the superiority of the end-to-end baseline, outperforming the strong pipeline models equipped with sophisticated machine translation systems. Furthermore, to provide a deeper understanding, we manually analyze the model outputs and discuss specific challenges faced by current approaches. We hope that our work could benchmark long-document CLS and benefit future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Summarization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Given documents in a source language, Cross-Lingual Summarization (CLS) aims to generate the corresponding summaries in a different target language. Under the background of globalization, CLS could help people obtain key information from documents in their unfamiliar languages, making information acquisition more efficient. Consequently, this task becomes more important and has attracted wide research attention <ref type="bibr" target="#b32">[33]</ref>.</p><p>Nevertheless, current CLS works generally focus on short texts. For example, Zhu et al. <ref type="bibr" target="#b40">[41]</ref> propose two CLS datasets, En2ZhSum and Zh2EnSum, and Bai et al. <ref type="bibr" target="#b0">[1]</ref> propose En2DeSum. These three widely-used CLS datasets are all collected from English and Chinese news reports with the scales of 371K, 1.7M, and 438K, respectively. The average lengths of source documents in En2ZhSum and En2DeSum are 755.0 and 31.0 words (in English), respectively, while the counterpart in Zh2EnSum is 103.7 characters (in Chinese). Besides, Ladhak et al. <ref type="bibr" target="#b8">[9]</ref> construct WikiLingua with an average of 45K CLS samples per cross-lingual direction <ref type="foot" target="#foot_0">1</ref> , and the average length of their source documents is 391 words. Perez-Beltrachini and Lapata <ref type="bibr" target="#b19">[20]</ref> construct XWikis which involves 214K documents per direction and the average length of their source documents is 945 words. Recently, Wang et al. <ref type="bibr" target="#b31">[32]</ref> propose a dialogue-oriented CLS dataset named XSAMSum which contains 16K dialogue documents whose average length is 83.9 words. Different from these short texts, long documents usually provide detailed discussions of multiple topics and involve more than thousands of words. Building a long-document CLS system has practical significance since it can save a lot of reading time for people who are not familiar with the source language. However, this task is still under-explored due to the lack of corresponding datasets.</p><p>In this paper, we construct Perseus<ref type="foot" target="#foot_1">2</ref> (Scientific Papers Online for Cross-Lingual Summarization), the first long-document CLS dataset which contains about 94K Chinese scientific papers paired with English summaries (an example is shown in Figure <ref type="figure" target="#fig_0">1</ref>). Our dataset covers four disciplines in total, including engineering applications, natural science, agricultural science and medical science. The average length of its source documents is 2872.9 Chinese characters, which is significantly larger than those of previous English Summary: Wuhan has long been known for its abundant lakes and waters. And it has the most number of intro-city lakes in China Wuhan's lakeside which are important ecological areas for linking lakes and land not only carry Wuhan's historical and cultural contents but also Wuhan's key economic growth areas. Under the background of increasingly prominent conflicts between people and land, land shortage and increasing population in Wuhan cause huge changes in Wuhan's lakeside land use. This article uses GIS data extraction to analyzes the evolution of the land use along Wuhan's lake boundaries in different years and seeks to identify the characteristics and inner drive mechanisms for evolution in Wuhan's lakeside land use. This article provides database for improving land use and for adjusting development modes of Wuhan's lakeside reuse and presents first-hand data reference for exploring future trends in Wuhan's lakeside land use during Wuhan's high-speed urbanization. datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41]</ref>. To evaluate the generalization of long document CLS models, we also provide an out-of-domain test set that contains 500 Chinese documents together with their English summaries pairs in the sports domain. In view of pre-trained language models, most of them can only handle hundreds of tokens (e.g., 512-token limitation in BERT-style NLU models and 1,024-token limitation in BART-style NLG models) due to the quadratic memory and computational consumption, thus it is a great challenge to model the long-distance dependencies within long documents. Besides, long documents usually need long summaries to convey their core ideas, leading to more difficulty in generating comprehensive and accurate summaries.</p><p>Existing CLS methods generally follow three paradigms, i.e., translate-then-summarize <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, summarize-thentranslate <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, and end-to-end <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. Specifically, the translate-then-summarize methods first translate the source documents into the target language and then summarize the translated documents. In contrast, summarize-then-translate methods first summarize the source-language documents and further translate the summaries into the target language. End-to-end methods directly generate summaries in the target language from the given documents in the source language. Among them, the translatethen-summarize paradigm is not suitable for our scenario due to the high costs caused by machine translation in long documents (it needs to translate the whole documents instead of brief summaries). Thus, we build and evaluate various summarize-then-translate and end-to-end methods based on Perseus. In addition, we divide the summarize-then-translate methods into extract-then-translate methods and abstract-then-translate methods depending on whether the summarization methods is extractive (directly select sentences from documents as summaries) or abstractive (use sequence-tosequence models to generate summaries). Experimental results on Perseus show that the end-to-end baseline performs best in terms of all metrics (including automatic and human evaluation metrics), demonstrating its superiority in generating logical, informative and concise summaries.</p><p>Moreover, to provide a deeper understanding of long-document CLS, we manually analyze the model outputs and summarize the main challenges brought by this new task, including (i) missing information, (ii) redundancy, (iii) wrong references and (iv) semantically unclear generation. We hope that our work could prompt the CLS research on long documents and inspire future studies.</p><p>Our main contributions are concluded as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Cross-Lingual Summarization</head><p>Cross-Lingual Summarization (CLS) has received a lot of research attention these years, and many valuable datasets and methods have been proposed one after another. Based on existing monolingual summarization datasets, Zhu et al. <ref type="bibr" target="#b40">[41]</ref> design a round-trip translation strategy with a machine translation service to construct the first large-scale CLS datasets, i.e., En2ZhSum and Zh2EnSum. Later, Bai et al. <ref type="bibr" target="#b0">[1]</ref> construct En2DeSum in the same way. The target summaries of these three datasets are all machine translated from other languages. Recently, some CLS datasets are constructed through manually translating the summaries of existing monolingual summarization datasets to different target languages, e.g., ClidSum <ref type="bibr" target="#b31">[32]</ref> employs professional translators to translate the summaries of two English monolingual dialogue summarization to German and Chinese. GOAL <ref type="bibr" target="#b33">[34]</ref> manually translates the summaries of its collected English monolingual sports summarization data to Chinese. In another way, researchers also attempt to collect CLS data from multilingual online resources. Global Voice <ref type="bibr" target="#b15">[16]</ref>, WikiLingua <ref type="bibr" target="#b8">[9]</ref> and XWikis <ref type="bibr" target="#b19">[20]</ref> crawl multi-lingual document-summary pairs from Global Voice, WikiHow, and Wikipedia websites, respectively.</p><p>Early CLS methods typically focus on pipeline paradigms, i.e., translate-then-summarize <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> and summarize-thentranslate <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. The former first translates the source documents to the target language and then summarizes the translated documents. And the latter first generates the summaries of the source documents and further translates them to the target language. Though straightforward, these pipeline methods suffer from severe propagation errors. Recently, many efforts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref> are given to prompt end-to-end CLS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long-Document Summarization</head><p>Long-document summarization aims to generate a summary from a given long document containing thousands of tokens. Cohan et al. <ref type="bibr" target="#b4">[5]</ref> present two large-scale long-document summarization datasets, arXiv and PubMed, whose data are collected from the corresponding scientific paper websites, arXiv.org and PubMed.com. In addition, there are many other long-document summarization datasets collected from various resources, including patents <ref type="bibr" target="#b21">[22]</ref>, government reports <ref type="bibr" target="#b6">[7]</ref> and sports games <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. It is difficult for traditional summarization methods to perform long documents due to their limited capability of modeling long-distance dependencies. Transformer-based pre-trained models can only take no more than 1024 tokens as input because the memory and compute consumption of self-attention scales quadratically with input length. To enlarge the maximum acceptable input length of pre-trained models, longformer encoder-decoder <ref type="bibr" target="#b2">[3]</ref> uses a local windowed attention mechanism with a task-guided global attention to make the consumption scales linearly as input length. Besides, there are many other sparse attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> that can efficiently process long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>In this section, we first introduce the construction process of our dataset ( ? 3.1). To verify the generalization of long-document CLS models, we also present an out-of-domain test set ( ? 3.2). Finally, we give data statistics to provide deeper analyses ( ? 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Construction</head><p>Resource. We crawl long-document CLS data from Sciencepaper Online<ref type="foot" target="#foot_2">3</ref> , which records large amounts of Chinese scientific papers. We choose these Chinese scientific papers due to: (1) Scientific papers usually reach thousands of words and could be regarded as long documents. There are many monolingual long-document summarization datasets leveraging scientific papers as their research object <ref type="bibr" target="#b4">[5]</ref>. (2) Many Chinese academic journals require researchers to write abstracts in both Chinese and English <ref type="bibr" target="#b32">[33]</ref>. Thus, the English summaries and the corresponding Chinese paper contents could naturally form CLS samples. We crawl 652 journals from 2000 to 2021, covering four subjects, i.e., engineering applications, natural science, agricultural science, and medical science. As a result, 418.7k papers (in PDF format) are collected.</p><p>Pre-Processing. To extract the Chinese documents, Chinese summaries and English summaries from the science papers in PDF format, we employ the following data pre-processing:</p><p>We first utilize PyMuPDF toolkit <ref type="foot" target="#foot_3">4</ref> to translate PDF papers to plain texts, and then extract Chinese summaries and English summaries via the rule-based methods. The texts extracted by the toolkit contain a lot of noise such as journal information that is irrelevant to the articles. Thus, we design a lot of regular expression templates to filter this noise. Following ArXiv dataset <ref type="bibr" target="#b4">[5]</ref>, we remove the reference information and replace reference endnotes with a special token, we also use another special token to replace the math formulations in the documents. We also find that the toolkit works well with Chinese characters, but suffers from a problem where there is no space between English words when parsing English contents. To address this problem, we transform the first pages of papers into pictures and employ tesseract-OCR <ref type="foot" target="#foot_4">5</ref> to extract their English summaries.</p><p>After the pre-processing, we remove the documents whose lengths are less than 1,000 or over 8,000, and discard papers that do not contain either Chinese or English summaries. Finally, we obtain 94K triples of ?Chinese document, Chinese summary, English summary?, which constitute our Perseus. We split our dataset into 82K/6K/6K w.r.t training/validation/test set.</p><p>Quality. We randomly select 200 samples from the test set and employ three graduate students as evaluators to check these samples. The evaluators are asked to give one point for a sample if there is no noise, zero otherwise. Finally, we receive 176 points on average, achieving a noise-free rate of 88%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Out-of-Domain Test</head><p>We also provide an out-of-domain test set (OOD set) to evaluate the model's generalization. To this end, we choose K-SportsSum <ref type="bibr" target="#b30">[31]</ref>, a Chinese long-document summarization dataset in the sports domain. To adapt this dataset to the CLS task, we manually translate the summaries in its test set (with 500 samples) from Chinese to English. In detail, we have three graduate students who are all native Chinese speakers with fluent English to translate these summaries. After manually translating, a data expert will check the translating results to make sure the translations are qualified. In this way, the original Chinese documents paired with the translated English summaries could form the out-of-domain test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistics</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the data statistics of our Perseus as well as previous CLS datasets. The average lengths of the documents in Perseus are 2871.2, 2880.5 and 2883.4 w.r.t training, validation and test sets, respectively, longer than all previous CLS datasets. We also calculate compression ratio for these datasets, which is the result of the average length of source-langauge documents divided by the average length of source-language summaries. It reveals how much the summary refines the content of the document. Our compression ratio of our dataset is 14.3/14.4/14.3 (training/validation/test), larger than all previous CLS datasets, which also means the documents in our dataset contain more redundant information and the distribution of their key information is sparser. The out-of-domain test set is in the sports domain, and we do not limit their lengths to the same level as those of Perseus, thus, we can evaluate the generalization of CLS models trained on Perseus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BASELINES</head><p>In this section, we first formally definite the long-document CLS task ( ? 4.1), then we introduce the details of various baselines including extract-then-translate ( ? 4.2), abstract-then-translate ( ? 4.3) and end-to-end ( ? 4.4) baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Definition</head><p>Long-document cross-lingual summarization aims to generate a brief summary ? = {? 1 , ? 2 , ..., ? |? | } in a target language given a long document ? = {? 1 , ? 2 , ..., ? |? | } in a different source language, where ? ? denotes the ?-th token and ? ? denotes the ?-th sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extract-then-Translate</head><p>Extract-then-Translate (Ext-Trans) is a pipeline paradigm that directly extracts Chinese sentences from a document and translates these sentences into English to obtain the target summaries. A key component lies in Ext-Trans is extractor which directly selects sentences from original documents as their summaries. In this way, the summaries involve few grammatical errors, but lose flexibility. We adopt the following four extractors in the Ext-Trans methods:</p><p>? Longest is a heuristic way to directly select the longest ? sentences from each document as its summary.</p><p>? TextRank <ref type="bibr" target="#b13">[14]</ref> is an unsupervised sentence-level ranking algorithm based on undirected graph. ? PacSum <ref type="bibr" target="#b39">[40]</ref> could be regarded as an upgraded version of Tex-tRank. It uses the position information in the graph network to judge the pointing relationship between sentences, so as to convert the traditional undirected graph into a directed graph and improve the model performance of selecting key sentences. ? SummaRu. <ref type="bibr" target="#b14">[15]</ref> is a supervised RNN-based extracting method.</p><p>Given the source-language documents, we select the top-5 sentences based on each extractor to form their source-language summaries. Next, we adopt the following machine translation (MT) methods (including sophisticated MT service and open source MT model) to translate the summaries to the target language:</p><p>? Baidu Translation<ref type="foot" target="#foot_5">6</ref> is a sophisticated MT service.</p><p>? OPUS-MT <ref type="bibr" target="#b25">[26]</ref> releases many MT models with the architecture of transformer. we utilize the pre-trained OPUS-MT-zh-en model <ref type="foot" target="#foot_6">7</ref>to translate summaries from Chinese into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Abstract-then-Translate</head><p>Abstract-then-Translate (Abs-Trans) first employs a sequence-tosequence (seq2seq) model to generate the summaries of the given source-language documents and then translates the summaries from the source to the target language. Abstractive methods can flexibly generate summaries conditioned on the key information of the document. A key component lies in Abs-Trans is abstractor with the architecture of seq2seq. Specifically, we adopt the following two seq2seq models as abstractors, respectively:</p><p>? PGN <ref type="bibr" target="#b20">[21]</ref> is a LSTM-based seq2seq model, introducing copy mechanism and coverage mechanism to alleviate the problems of out-of-vocabulary and repeated generation. PGN can take long documents as inputs but cannot efficiently model the longdistance dependencies due to its LSTM-based architecture. ? LED (Longformer-Encoder-Decoder) <ref type="bibr" target="#b2">[3]</ref> is a seq2seq model with sparse attention mechanism. The weights of LED is initialized by BART <ref type="bibr" target="#b10">[11]</ref> (a pre-trained transformer-based seq2seq model).</p><p>LED is suitable for processing long documents due to its sparse attention (based on sliding window attention). The above abstractors are trained with monolingual documentsummary pairs (in Chinese). Next, we adopt the same MT methods as Ext-Trans to translate the summaries from Chinese to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">End-to-End</head><p>The end-to-end method directly generates a target-language summary given a source-language document in a seq2seq manner. To build the end-to-end baseline, we modify mBART-50 <ref type="bibr" target="#b23">[24]</ref>, a stateof-the-art multilingual generative model which is pre-trained on a large-scale multi-lingual corpus involving 50 languages, to support the inputs of long documents. In detail, we replace the dense selfattention in vanilla mBART-50 with LED-style sparse self-attention. It could also be regarded as a multi-lingual version of LED (denoted as mLED).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Implementation Details</head><p>The pre-trained models in our experiments are provided by the Huggingface Transformers Library<ref type="foot" target="#foot_7">8</ref> , i.e., BART-base-chinese <ref type="foot" target="#foot_8">9</ref> and mBART-50 <ref type="foot" target="#foot_9">10</ref> . During fine-tuning, we set the batch size to 2 and 1 for BART and mBART, respectively. All models are fine-tuned for 5 epochs with 5e-5 learning rates. We initialize LED with the weights of BART via an official script <ref type="foot" target="#foot_10">11</ref> . To initialize mLED with the weights of mBART, we utilize another script <ref type="foot" target="#foot_11">12</ref>  and Tgt Lang. denote the source language and the target language, respectively (En: English, Zh: Chinese, De: German). Doc. Length, Src Summ. Length and Tgt Summ. Length indicate the average lengths of source documents, source-language summaries and target-language summaries, respectively. The lengths are counted in word level for English and character level for Chinese. Comp. Ratio represents compression ratio. WikiLingua and XWikis have multiple source-target language pairs and their average lengths are averaged over CLS samples in all cross-lingual directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>Automatic Evaluation.</p><p>To comprehensively evaluate model performance, we adopt multiple automatic metrics as follows:</p><p>? ROUGE <ref type="bibr" target="#b12">[13]</ref>. ROUGE-N (R-N) evaluates the recall based on N-gram overlaps between the generated summaries and the corresponding references. ROUGE-L (R-L) is designed to find the length of the longest common subsequence. ? BLEU <ref type="bibr" target="#b18">[19]</ref>. BLEU-N computes the precision based on N-gram overlaps between the generated summaries and the references. ? METEOR <ref type="bibr" target="#b1">[2]</ref>. METEOR evaluates the harmonic mean of precision and recall, and recall weights more than precision. ? CIDEr <ref type="bibr" target="#b26">[27]</ref> introduces TF-IDF <ref type="bibr" target="#b7">[8]</ref> to assign weights to n-grams and low frequency words are given higher weights than high frequency words. ? BertScore (B-S) <ref type="bibr" target="#b38">[39]</ref> evaluates the semantic similarity between the generated summaries and the references. Human Evaluation. For further evaluation of baselines' performance, we conduct human evaluation from three aspects: ? Coherence (Cohe.) evaluates the quality of the generated summaries' logic and consistency. ? Relevance (Rel.) evaluates the relevance between the generated summaries and the reference. ? Conciseness (Conci.) evaluates how brief but comprehensive the generated summaries are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the experimental results. We first analyze the performance of pipeline baselines and then compare them with the end-to-end baseline.</p><p>Pipelines. There are two paradigms in pipeline baselines, which are extract-then-translate (Ext-Trans) and abstract-then-translate (Abs-Trans). We find that the Abs-Trans methods generally outperform the Ext-Trans methods. It is because abstractors are more flexible to generate new words or phrases based on the important sentences in documents while the extractors cannot make any modifications to the extracted sentences. Besides, the pipeline methods' performance is highly related to the adopted MT methods. Specifically, we equip every extractor or abstractor with Baidu and OPUS-MT MT methods, respectively. Based on the same extractor/abstractor, the performance of using the Baidu MT service is much better than that of using the OPUS-MT model. End-to-End vs Pipelines. The end-to-end model achieves the best performance among all baselines. The mLED model is trained with both translation and summarization in an end-to-end manner, and thus, does not suffer from the error propagation issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generalization</head><p>To evaluate the generalization of long-document CLS models, we test the trained SummaRu., LED and mLED on the OOD test set.</p><p>As the results shown in Table <ref type="table" target="#tab_3">3</ref>, we find that models trained on Perseus do not perform as well on the OOD set, revealing the limited generalization capability of current models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Human Study</head><p>We conduct human studies under SummaRu.+Baidu, LED+Baidu and mLED correspond to extract-trans, abstract-trans, and endto-end paradigms, respectively. We randomly select 50 samples from the in-domain test set and employ four crowd workers who are fluent in both English and Chinese to evaluate the generated summaries. The scoring adopts a 3-point scale. The final average scores are shown in Table <ref type="table" target="#tab_4">4</ref>. mLED performs better than other methods in all three aspects, indicating its strong ability to generate logical, informative and concise summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><formula xml:id="formula_0">R-1 R-2 R-L B-1 B-2 B-3 B-4 M C B-S Ext-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>We give an example from our dataset and show the generated summaries of several strong baselines. The example is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>For pipeline methods, the Chinese summaries are directly generated by monolingual summarization models, and the English summaries are translated from the corresponding Chinese summaries. The endto-end model (mLED) directly generates the English summary, and the Chinese summary is translated from the English one. For Sum-maRu., the source summary is obtained by concatenating several key sentences that are directly extracted from the source document.</p><p>In this case, we find that the two bold sentences are the same, although they are from different parts of the document, revealing the problem that extractive approaches could cause semantic duplication. For LED, it generates Chinese summaries from Chinese documents. Due to its monolingual characteristics, the presence of content in other languages in the source documents may result in generation errors. As the purple parts showed in Figure <ref type="figure" target="#fig_1">2</ref>, LED misses the English part of the professional term, leading to semantic errors, while mLED performs well in this aspect because of its strong ability to deal with the cross-lingual setting. For mLED, it generates error messages that do not conform to the source document. For example, the green sentence is not contained in the source document and there is no content in the source document expressing the meaning of this sentence. Moreover, the orange sentence in LED's summary means improving the response speed of the system and overcome the influence of parameter variation and external voltage fluctuation is the purpose of proposing a higher-order sliding-mode variable structure control strategy with instantaneous power feed-forward compensation, while in both of the source document and the reference summary, the former is a consequence of the latter. The red part in the reference summary does not appear directly in the source document, and it needs to be inferred from the source document. However, both LED and mLED miss the information, indicating their limited understanding and reasoning abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>To further understand the challenges of our long-document CLS dataset, we randomly select 200 samples from the test set and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Summaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SummaRu. +Baidu-MT</head><p>In this paper, the structure and control principle of Dual PWM converter are analyzed, and it is proposed that the grid side converter adopts grid voltage oriented vector control to realize the AC side unity power factor and DC link voltage control. Then, the limitations of traditional PI control are analyzed when the grid voltage is in fault, and a high-order sliding mode variable structure control strategy with instantaneous power feed-forward compensation is proposed to improve the system response speed and overcome the influence of parameter changes and external voltage fluctuations. Conclusions this paper analyzes the structure and control principle of the dual PWM converter, and proposes that the grid side converter adopts the grid voltage oriented vector control to realize the AC side unit power factor and DC link voltage control. On the basis of the traditional PI control, a high-order sliding mode variable structure control strategy with instantaneous power feedforward compensation is proposed to improve the system response speed and overcome the influence of parameter changes and external voltage fluctuations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LED +Baidu-MT</head><p>The structure and control principle of dual type converter are analyzed. It is proposed that the grid side converter adopts grid voltage oriented vector control to realize AC side unit power factor and DC link voltage control; Based on the traditional control, a high-order sliding mode variable structure control strategy with instantaneous power feedforward compensation is proposed, which improves the response speed of the system and overcomes the effects of parameter changes and external voltage fluctuations. Simulation results show the effectiveness of the strategy. ????????????????, ???????????????????, ?????????????????????; ?????? ????, ??????????????????????????, ?????????, ???????????????. ??????? ???????.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mLED</head><p>The structure and control principle of a double-PWM converter are analyzed. A grid voltage-oriented vector control strategy is proposed for the grid-side converter, in which the input current is controlled by the grid-side unit power factor and the DC link voltage is controlled by the proportional-resonant(P-R) controller. In order to improve the response speed of the system and overcome the influence of parameter variation and external voltage fluctuation, a higherorder sliding-mode variable structure control strategy with instantaneous power feed-forward compensation is proposed. Simulation results show the effectiveness of the proposed method.</p><formula xml:id="formula_1">???? PWM ???????????????????????????????????????????????????????? ?????????? (P-R) ?????????????????????????????????????????????????? ??????????????????????????</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>For the grid side converter, this paper proposes a higher-order sliding mode control (SMC) plus a transient power feedforward PI control to deal with the grid failure and to keep the direct current (DC) voltage stable. Firstly, We analyze the structure and control principle of the double pulse width module (PWM) converter in double-fed induction generator (DIG). According to the mathematical model of the grid side converter, the paper adopts the voltage orientation vector control principle to realize the unity power-factor for the alternating current (AC) side and the voltage control in the DC loop. The proposed method speeds up the system tracking, and stabilizes the system under the influence of the external disturbances. Simulation results validate the accuracy and effectiveness of the control strategy proposed in this paper.</p><p>???????????????????????, ?????????????, ?????????????????????????? ?, ??????????????????????? (PWM) ????????????, ??????????????, ???????? ????????????, ?????????????????????. ??????????????, ???????????????. ??????????????. analyze the wrong generation of mLED. We list four main errors that occur in the generated summaries as follows:</p><p>? Missing information: the generated summaries neglect some information involved in the references.</p><p>? Redundancy: the generated summaries have additional information that does not exist in the references. ? Wrong references: some information in generated summaries is not faithful to the source documents.</p><p>? Semantically unclear generation: the generated summaries contain information that is incomprehensible. Table <ref type="table" target="#tab_8">5</ref> shows the proportion of each error type. We find that (1) the proportions of missing information and redundancy are higher than others, indicating that it is difficult for mLED to grasp key information from long documents. (2) The wrong references problem also occupies a certain proportion. Being faithful to the source documents is very important in the summarization task, especially in the field of scientific papers, but mLED still has some problems in this regard. (3) Semantically unclear generation problem shows that mLED has insufficient ability to generate long sequences.</p><p>Missing information and Redundancy. This is mainly caused by the long sequences of the input documents. Besides, the compression ratio of CLS samples in Perseus is also at a high level, which also means the information in the summaries is sparsely distributed across documents. There, it is non-trivial for models to generate informative and accurate summaries for documents. In addition, although mLED can process long sequences, its attention mechanism is essentially local attention (windowed local attention with global attention), which has a limited capacity of interacting between long distant content. As a result, mLED cannot fully incorporate information from the entire document and extract the truly important information. In severe cases, it will extract a series of content that is internally correlated but irrelevant to the ground truth summary. To alleviate the problems of missing information and redundancy, the model needs to integrate the information of the entire document and distinguish between core and non-core information. Future work could integrate the information of documents by introducing hierarchical structures, e.g., dividing and linking different parts of documents.</p><p>Wrong references. mLED generates information that does not conform to the references. For example, a reference summary says "Methods: Fifty-eight patients with subaortic stenosis were treated surgically in our center from December 1996 to October 2019. ", but the generated summary is "Methods: The clinical data of 13 patients with congenital heart disease were retrospectively analyzed. ". The wrong reference is mainly caused by the long-distance dependencies problem. When generating summaries, the model needs to fuse information across long distances. However, as the distance grows, the long-distance information becomes more and more blurred, leading to generating wrong information.</p><p>Semantically unclear generation. mLED generates sentences like "data mining is an important content of data mining. " and "the results provide us with a basis to judge whether the sub-time series of time series with increasing and decreasing is the sub-time series with great increasing and decreasing." that involve wrong syntax and are difficult to understand. The long-distance dependencies problem is one of the reasons for this error. In addition, this error also exposes the inadequacy of the current generative models in generating long texts.</p><p>For wrong references and semantically unclear generation, it is important to address the problem of long-distance dependencies. The best way is to create a pre-trained language model that can efficiently incorporate long document information, but it is nontrivial to do so <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref>. Instead, we think it is possible to transform long-document CLS task into short-document CLS task by combining extractive methods with abstractive methods. For example, an extractive method is used to extract sufficient key sentences, and then a well-performed multilingual seq2seq model, such as mBART, is adopted to generate a summary based on the extracted sentences. Although it is a pipeline method and there is error propagation problem, but it can avoid the problem of long-distance dependencies. Moreover, the first step of this method can filter out irrelevant information to a large extent, and the self-attention of mBART can be fully utilized to fuse the entire content.</p><p>To summarize, our long-document CLS dataset, Perseus, brings a lot of new challenges to the CLS task: (1) its documents are too long for current CLS models to process; (2) the summaries are also relatively long and sparsely distributed across documents, making it difficult to generate; (3) the length of the document makes it difficult for the model to incorporate the entire document information; (4) the problem of long-distance dependencies makes it hard for seq2seq models to generate correct summaries; <ref type="bibr" target="#b4">(5)</ref> it is hard to process the professional terms correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduce the long-document cross-lingual summarization (long-document CLS) task and propose the first longdocument CLS dataset, Perseus. We conduct multiple experiments on our dataset and analyze the advantages and disadvantages of different summarization methods. To evaluate the generalization of long-document CLS models trained on our dataset, we also provide an out-of-domain test set which is in the sports domain. To further understand the challenges brought by Perseus, we manually analyze the generated summaries of mLED, take a deep dive into the reasons behind these errors and discuss the possible solutions. In the future, we would like to focus on expanding the multilingual version of Perseus to meet the needs of different languages and explore a more efficient method for long-document CLS tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of Chinese long document and the target English summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of Perseus and the generated summary of strong baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>To the best of our knowledge, we are the first CLS work on long documents. We construct the first long-document CLS dataset named Perseus, containing 94K Chinese long documents and the corresponding English summaries. An out-of-domain test set is also provided to evaluate the model's generalization.</figDesc><table><row><cell>? Based on the Perseus dataset, we build and evaluate various</cell></row><row><cell>baselines from different paradigms and manually analyze the</cell></row><row><cell>model outputs to provide deeper analyses.</cell></row><row><cell>? We conduct thorough analyses of this task based on Perseus and</cell></row><row><cell>discuss the promising directions for future work.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>. Data statistics of Perseus and previous CLS datasets. Doc Num. is the number of samples in each dataset. Src Lang.</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell>Doc Num.</cell><cell>Src Lang.</cell><cell>Tgt Lang.</cell><cell>Doc. Length</cell><cell>Src Summ. Length</cell><cell>Tgt Summ. Length</cell><cell>Comp. Ratio</cell></row><row><cell></cell><cell></cell><cell cols="3">Previous CLS Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eh2ZnSum [41]</cell><cell>News Report</cell><cell>371K</cell><cell>En</cell><cell>Zh</cell><cell>755.0</cell><cell>55.2</cell><cell>96.0</cell><cell>13.7</cell></row><row><cell>Zn2EhSum [41]</cell><cell>News Report</cell><cell>1.7M</cell><cell>Zh</cell><cell>En</cell><cell>103.7</cell><cell>17.9</cell><cell>13.7</cell><cell>5.8</cell></row><row><cell>En2DeSum [1]</cell><cell>News Report</cell><cell>438K</cell><cell>En</cell><cell>De</cell><cell>31.0</cell><cell>8.5</cell><cell>7.5</cell><cell>3.6</cell></row><row><cell>XSAMSum [32]</cell><cell>Dialogue</cell><cell>16K</cell><cell>En</cell><cell>De/Zh</cell><cell>83.9</cell><cell>20.3</cell><cell>19.9/33.0</cell><cell>4.1</cell></row><row><cell>WikiLingua [9]</cell><cell>How-to Guide</cell><cell>46K</cell><cell cols="2">Multi Multi</cell><cell>391.0</cell><cell>/</cell><cell>39.0</cell><cell>/</cell></row><row><cell>XWikis [20]</cell><cell cols="4">Encyclopedia Article 214K Multi Multi</cell><cell>945.0</cell><cell>/</cell><cell>77.0</cell><cell>/</cell></row><row><cell></cell><cell></cell><cell cols="2">Perseus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell>Scientific Paper</cell><cell>82K</cell><cell>Zh</cell><cell>En</cell><cell>2871.2</cell><cell>201.2</cell><cell>124.1</cell><cell>14.3</cell></row><row><cell>Validate</cell><cell>Scientific Paper</cell><cell>6K</cell><cell>Zh</cell><cell>En</cell><cell>2880.5</cell><cell>199.7</cell><cell>122.8</cell><cell>14.4</cell></row><row><cell>Test (in-domain)</cell><cell>Scientific Paper</cell><cell>6K</cell><cell>Zh</cell><cell>En</cell><cell>2883.4</cell><cell>202.3</cell><cell>124.7</cell><cell>14.3</cell></row><row><cell>Test (out-of-domain)</cell><cell>Sports Game</cell><cell>0.5K</cell><cell>Zh</cell><cell>En</cell><cell>3970.9</cell><cell>612.1</cell><cell>456.3</cell><cell>6.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Trans</cell></row></table><note><p>Experimental results (R-1/2/L: ROUGE-1/2/L; B-1/2/3/4: BLEU-1/2/3/4; M:METEOR; C:CIDEr; B-S: BertScore.).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Out-of-domain testing results.</figDesc><table><row><cell>Method</cell><cell cols="3">Cohe. Rel. Conci.</cell></row><row><cell>SummaRu. + Baidu-MT</cell><cell>0.82</cell><cell>1.24</cell><cell>1.12</cell></row><row><cell>LED + Baidu-MT</cell><cell>1.48</cell><cell>1.64</cell><cell>1.54</cell></row><row><cell>mLED</cell><cell>1.62</cell><cell>1.72</cell><cell>1.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of human study on Perseus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>?</head><label></label><figDesc>Other: the errors do not belong to any of the former error types.</figDesc><table><row><cell>Error</cell><cell>%</cell></row><row><cell>Missing information</cell><cell>87.5</cell></row><row><cell>Redundancy</cell><cell>40.5</cell></row><row><cell>Wrong references</cell><cell>24.5</cell></row><row><cell cols="2">Semantically unclear generation 22.0</cell></row><row><cell>Other</cell><cell>17.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Main error types of mLED's outputs. Note that each generated summary might contain multiple errors.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use "direction" to denote the summarization direction from the source to the target languages, e.g., English (documents) ? Chinese (summaries). Some CLS datasets contain more than one direction, thus we show the average samples per direction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The dataset has been released at https://github.com/LearnItBoy/Perseus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.paper.edu.cn/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/pymupdf/PyMuPDF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/tesseract-ocr/tesseract</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://fanyi-api.baidu.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://huggingface.co/Helsinki-NLP/opus-mt-zh-en</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/huggingface/transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://huggingface.co/fnlp/bart-base-chinese</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>https://github.com/allenai/longformer/blob/master/scripts/convert_bart_to_ longformerencoderdecoder.py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://github.com/SCNUJackyChen/mBART50Long</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We would like to thank anonymous reviewers for their suggestions and comments. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62072323</rs>, <rs type="grantNumber">62102276</rs>), <rs type="funder">Shanghai Science and Technology Innovation Action Plan</rs> (No. <rs type="grantNumber">22511104700</rs>), the <rs type="funder">Natural Science Foundation of Jiangsu Province</rs> (Grant No. <rs type="grantNumber">BK20210705</rs>), and the <rs type="funder">Natural Science Foundation of Educational Commission of Jiangsu Province, China</rs> (Grant No. <rs type="grantNumber">21KJD520005</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jVyZtb6">
					<idno type="grant-number">62072323</idno>
				</org>
				<org type="funding" xml:id="_T3dKKdM">
					<idno type="grant-number">62102276</idno>
				</org>
				<org type="funding" xml:id="_MyAfXYK">
					<idno type="grant-number">22511104700</idno>
				</org>
				<org type="funding" xml:id="_5UWXqUk">
					<idno type="grant-number">BK20210705</idno>
				</org>
				<org type="funding" xml:id="_YDUetez">
					<idno type="grant-number">21KJD520005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-Lingual Abstractive Summarization with Limited Parallel Resources</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6910" to="6924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W05-0909" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6220" to="6231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MSAMSum: Towards Benchmarking Multi-lingual Dialogue Summarization</title>
		<author>
			<persName><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering</title>
		<meeting>the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient Attentions for Long Document Summarization</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jones</forename></persName>
		</author>
		<title level="m">Index term weighting. Information storage and retrieval</title>
		<imprint>
			<date type="published" when="1973">1973. 1973</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="619" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wik-iLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In Findings of the Association for Computational Linguistics: EMNLP 2020. 4034-4048</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-lingual C*ST*RD: English access to Hindi information</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Asian Lang. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="245" to="269" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Variational Hierarchical Model for Neural Cross-Lingual Summarization</title>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2088" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global Voices: Crossing Borders in Automatic News Summarization</title>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5411</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-5411" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of a Crosslingual Romanian-English Multi-document Summariser</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Or?san</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Andreea Chiorean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Robust Abstractive System for Cross-Lingual Summarization</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1204</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1204" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2025" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Models and Datasets for Cross-Lingual Summarisation</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.742</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.emnlp-main.742" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9408" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1099" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1212</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1212" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency</title>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.276" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3120" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilingual Translation from Denoising Pre-Training</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.304</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-acl.304" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3450" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient Transformers: A Survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3530811</idno>
		<ptr target="https://doi.org/10.1145/3530811JustAccepted" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2022-04">2022. apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.eamt-1.61" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</title>
		<meeting>the 22nd Annual Conference of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="479" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using Bilingual Information for Cross-Language Document Summarization</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P11-1155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-Language Document Summarization Based on Machine Translation Quality Prediction</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P10-1094" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary</title>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Sports Game Summarization</title>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3488560.3498405</idno>
		<ptr target="https://doi.org/10.1145/3488560.3498405" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1045" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05599</idno>
		<title level="m">Clidsum: A benchmark dataset for cross-lingual dialogue summarization</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12515</idno>
		<title level="m">A survey on cross-lingual summarization</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08635</idno>
		<title level="m">GOAL: Towards Benchmarking Few-Shot Sports Game Summarization</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks</title>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beiqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database Systems for Advanced Applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixed-Lingual Pre-training for Cross-lingual Summarization</title>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.aacl-main.53" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Phrase-based Compressive Cross-Language Summarization</title>
		<author>
			<persName><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1012</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Abstractive Cross-Language Summarization via Translation Model Enhanced Predicate Argument Structure Fusing</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2016.2586608</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2016.2586608" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1842" to="1853" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sentence Centrality Revisited for Unsupervised Summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1628</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1628" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">NCLS: Neural Cross-Lingual Summarization</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1302</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3054" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
