<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FL-Tuning: Layer Tuning for Feed-Forward Network in Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-30">30 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingping</forename><surname>Liu</surname></persName>
							<email>jingpingliu@ecust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kui</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongli</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihan</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<address>
									<country>Fudan University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Ruan</surname></persName>
							<email>ruantong@ecust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FL-Tuning: Layer Tuning for Feed-Forward Network in Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-30">30 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.15312v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt tuning is an emerging way of adapting pre-trained language models to downstream tasks. However, the existing studies are mainly to add prompts to the input sequence. This way would not work as expected due to the intermediate multi-head self-attention and feed-forward network computation, making model optimization not very smooth. Hence, we propose a novel tuning way called layer tuning, aiming to add learnable parameters in Transformer layers. Specifically, we focus on layer tuning for feed-forward network in the Transformer, namely FL-tuning. It introduces additional units into the hidden layer of each feed-forward network. We conduct extensive experiments on the public CLUE benchmark. The results show that: 1) Our FL-tuning outperforms prompt tuning methods under both full-data and few-shot settings in almost all cases. In particular, it improves accuracy by 17.93% (full-data setting) on WSC 1.0 and F1 by 16.142% (few-shot setting) on CLUENER over P-tuning v2. 2) Our FL-tuning is more stable and converges about 1.17 times faster than P-tuning v2. 3) With only about 3% of Transformer's parameters to be trained, FL-tuning is comparable with fine-tuning on most datasets, and significantly outperforms fine-tuning (e.g., accuracy improved by 12.9% on WSC 1.1) on several datasets. The source codes are available at https://github.com/genggui001/FL-Tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (PLMs), such as ELMo <ref type="bibr" target="#b0">[1]</ref> and BERT <ref type="bibr" target="#b1">[2]</ref>, have become increasingly important in natural language processing. The popular way to accommodate general-purpose PLMs to specific downstream tasks is to FINE TUNE them by updating all the parameters. As a result, it is necessary to store a modified copy of full-size model parameters for each task <ref type="bibr" target="#b2">[3]</ref>. However, this would be prohibitively expensive when applying the model to a large number of tasks <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p><p>PROMPT TUNING is an emerging way to adapt PLMs to downstream tasks, which adds prompts to the input sequence and feeds the new input to PLMs in the pre-training task. In this way, all PLM parameters are frozen and only the task-specific prompt is updated. Discrete prompt tuning is the first method that leverages text tokens as the prompt to the original input <ref type="bibr" target="#b6">[7]</ref>. For example, in text classification, a common prompt tuning method is to concatenate an input (e.g., "I haven't published a paper.") with the prompt "I felt <ref type="bibr">[MASK]</ref>" and ask PLMs to fill the masked token with "happy" or "sad". Since the PLMs are continuous from an optimization point of view, it is difficult to achieve the optimum with discrete prompts <ref type="bibr" target="#b7">[8]</ref>. Continuous prompt tuning is thus proposed to replace text </p><formula xml:id="formula_0">?? Q K V ? ! ? " ? ???? ? ? ? ?? ? ? ?? Figure 1:</formula><p>The difference between FL-tuning and prompt tuning. The former introduces learnable parameters to each FFN in the Transformer, while the latter adds tokens/embeddings to the input. tokens with trainable embeddings, which outperforms discrete prompt tuning on many tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. However, the impact of input prompts before the first transformer layer will gradually weaken due to the multiple intermediate layers' computation <ref type="bibr" target="#b7">[8]</ref>. To address this problem, deep prompt tuning <ref type="bibr" target="#b7">[8]</ref> is proposed, which takes continuous embeddings as the prompt to the input sequence of each layer in PLMs. Although this method has shown better performance on many tasks, its essence is still only adding a few parameters as prompts to the input. These prompts would not work as expected due to the intermediate multi-head self-attention and feed-forward network computation, making model optimization not very smooth. This situation leads to limited performance gain, unstable training, and slow convergence.</p><p>To address the above problem, we propose a novel tuning way, namely LAYER TUNING, for adapting PLMs to downstream tasks. Different from prompt tuning, layer tuning is to add learnable parameters to the Transformer layers with original PLM parameters frozen. Transformer <ref type="bibr" target="#b9">[10]</ref> mainly contains two sub-layers: multi-head self-attention and feed-forward network (FFN). In this paper, we mainly focus on Layer Tuning for Feed-forward network in the Transformer, namely FL-tuning. As shown in Figure <ref type="figure">1</ref>, it aims at introducing additional hidden units into each FFN layer. In other words, FL-tuning expands the dimensions of the weight matrices (W 1 and W 2 in Eq. ( <ref type="formula" target="#formula_1">1</ref>)) and bias (b 1 ) of the linear layers in FFN. The reason for tuning on FFN is that it accounts for about 2/3 of the number of parameters in the Transformer encoder <ref type="bibr" target="#b10">[11]</ref>, and it is expected to obtain better results. Due to the inconvenience of directly operating on the expanded weight matrices and biases, we split them into fixed and learnable parts. Hence, we can perform independent operations on the two parts to simplify the implementation process. The feasibility of this "split" idea is rigorously proved theoretically in Section 4.2. In addition, we also prove that the influence of the trainable weight matrices and bias on model performance is independent of their positions in the expanded ones. That is, the learnable part can be placed at any position in the expanded weight matrices and bias.</p><p>Contributions. The contributions in this paper are summarized as follows:</p><p>? To the best of our knowledge, we are the first to propose layer tuning for adapting PLMs to downstream task. The most distinguish characteristic is that it adds learnable parameters in Transformer layers.</p><p>? We propose a method of layer tuning for feed-forward network in the Transformer, namely FL-tuning. Our tuning method is more stable and converges faster than P-tuning v2.</p><p>? We conduct extensive experiments on 7 downstream tasks and 11 NLU datasets. The results show that our FL-tuning outperforms prompt tuning in almost all cases. In addition, with only about 3% of Transformer's parameters to be trained, it is comparable with fine-tuning on most datasets, and significantly outperforms fine-tuning on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prompt tuning adapts PLMs to downstream tasks by adding prompts to the input sequence and has been verified effective in many NLP applications <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. These methods can be divided into three categories. First, discrete prompt tuning directly generates the results without changing the pre-trained model parameters based only on the text tokens. LM-BFF <ref type="bibr" target="#b15">[16]</ref> introduces T5 <ref type="bibr" target="#b16">[17]</ref> into the template search process to generate tokens. Typical studies also include GPT-3 <ref type="bibr" target="#b11">[12]</ref> and LAMA <ref type="bibr" target="#b17">[18]</ref>. Second, the appearance of P-tuning <ref type="bibr" target="#b8">[9]</ref> turns on continuous prompt tuning work, which replaces text tokens with trainable embeddings to overcome the limitation of sub-optimal performance in the previous methods. P-tuning <ref type="bibr" target="#b8">[9]</ref> introduces continuous embeddings learned by LSTM to the original sequence of input word embeddings. Prefix-tuning <ref type="bibr" target="#b2">[3]</ref> designs task-specific trainable prefixes for natural language generation tasks. Third, deep prompt tuning is proposed to solve the challenges of continuous prompt tuning, which are the lack of universality across scales and tasks. P-tuning v2 <ref type="bibr" target="#b7">[8]</ref> applies continuous embeddings as the prompts to the input sequence of each layer in PLMs. Unlike the above studies, we aim to propose a novel PLMs' tuning way, namely layer tuning, which adds learnable parameters to the Transformer layers.</p><p>Transformer has been proved to be an effective architecture for PLMs <ref type="bibr" target="#b9">[10]</ref>. It is an encoder-decoder structure, which is composed of multi-head self-attention and FFN. Much effort is dedicated to improve the Transformer's capability <ref type="bibr" target="#b18">[19]</ref> and can be divided into three categories. First, some studies attempt to design a novel self-attention architecture in the Transformer <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Linformer <ref type="bibr" target="#b24">[25]</ref> proposes a new self-attention mechanism, which reduces the complexity of self-attention to O(n) in both time and space. Second, although FFN is just a multi-layer perceptron, it accounts for about 2/3 of the number of parameters in the Transformer encoder. Kformer <ref type="bibr" target="#b25">[26]</ref> improves the ability of PLMs by incorporating external knowledge into the FFN in the Transformer. Third, in addition to the separate researches on the two sub-layers, there are also many studies to modify the Transformer structure. Fan et al. <ref type="bibr" target="#b18">[19]</ref> strength the Transformer with a dynamic mask attention network before the multi-head self-attention. Press et al. <ref type="bibr" target="#b26">[27]</ref> study the impact of the combined order of the two sub-layers in the Transformer on model performance. Different from the above work on sub-layers or adding new sub-layers to the Transformer, this paper focuses on introducing hidden units to each FFN layer for PLMs' tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this section, we briefly review the multi-head self-attention and feed-forward network in the Transformer architecture. Besides, we give the definitions of three kinds of prompt tuning, including discrete prompt tuning, continuous prompt tuning, and deep prompt tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer</head><p>The Transformer model is based on an encoder-decoder structure. Each encoder (decoder) is composed of a stack of identical blocks, containing multi-head self-attention and feed-forward network. These two sub-layers are formulated as follows:</p><formula xml:id="formula_1">Attention(Q, K, V ) = Sof tmax( QK T ? d k )V, F F N (X) = ReLU (XW 1 + b 1 )W 2 + b 2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">X ? R du?dm , Q = XW Q , K = XW K , V = XW V , W Q ? R dm?d k , W K ? R dm?d k , W V ? R dm?dv , W 1 ? R dm?do , W 2 ? R do?dm are weight matrices, b 1 ? R 1?do and b 2 ? R 1?dm</formula><p>are bias, and ReLU (x) = max(0, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt tuning</head><p>Discrete prompt tuning leverages text tokens as prompts to adapt PLMs to downstream tasks. For example, to classify the sentiment of S="I haven't published a paper." as happy or sad, we design a prompt "I felt [MASK]" for S. The input embedding for PLMs is defined as: </p><p>Continuous prompt tuning adds the trainable embeddings to the input ones. Continuing the previous example, the prompt "I felt [MASK]" is replaced with continuous embeddings [h 0 , ..., h l ], where l is the prompt length. The input sequence is thus formulated as: Deep prompt tuning, also known as P-tuning v2 <ref type="bibr" target="#b7">[8]</ref>, takes the prefix embeddings as the prompt to the input sequence in each Transformer layer. P-tuning v2 essentially concatenates two matrices E 0 and E 1 to the key matrix K and the value matrix V respectively in each multi-head attention layer, which is defined as:</p><formula xml:id="formula_4">[e(S), h 0 , ..., h l , e([M ASK])].<label>(3)</label></formula><formula xml:id="formula_5">Attention P V 2 (Q, K, V, E 0 , E 1 ) = Sof tmax( Q[E 0 ?K] T ? d k )[E 1 ?V ],<label>(4)</label></formula><p>where the operation "?" means the row-wise concatenation, E 0 and E 1 are trainable and other parameters are frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we first introduce the details of FL-tuning. Then, we give the theoretical proofs of its equivalent implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FL-tuning</head><p>In the Transformer <ref type="bibr" target="#b9">[10]</ref> architecture, FFN is a multi-layer perceptron with one hidden layer, consisting of two linear transformations with an ReLU activation in their middle <ref type="bibr" target="#b9">[10]</ref>. In essence, FL-tuning adds a number of hidden units in each FFN layer, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Note that the added hidden units in different layers are independent to each other. Formally, we change W 1 and W 2 in Eq. ( <ref type="formula" target="#formula_1">1</ref>) to [W 1 :W 1 ] and [W 2 ?W 2 ] as follows:</p><formula xml:id="formula_6">F F N F L (X) = ReLU (X[W 1 : W 1 ] + [b 1 : b 1 ])[W 2 ?W 2 ] + b 2 ,<label>(5)</label></formula><p>where the operations ":" and "?" stand for the column-and row-wise concatenation, respectively. Only the parameters of</p><formula xml:id="formula_7">W 1 ? R dm?da , b 1 ? R 1?da</formula><p>, and W 2 ? R da?dm can be trained and all parameters of the Transformer-based PLMs are frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>However, it is inconvenient to directly utilize Eq. ( <ref type="formula" target="#formula_6">5</ref>) to implement FL-tuning because the learnable and fixed parameters are mixed in matrices. Therefore, we keep the original FFN hidden layer and add another small FFN hidden layer (referred to as addFFN). The equivalence is established by the following theorem.</p><p>Theorem 1. In FL-tuning, FFN can be split into:</p><formula xml:id="formula_8">F F N F L (X) = addF F N (X) + F F N (X),<label>(6)</label></formula><p>where addF F N (X) = ReLU (XW</p><formula xml:id="formula_9">1 + b 1 )W 2 .</formula><p>Proof. Based on the distributive property of matrix multiplication, we make a further derivation of Eq. ( <ref type="formula" target="#formula_6">5</ref>) as below:</p><formula xml:id="formula_10">F F N F L (X) = ReLU ([XW 1 + b 1 : XW 1 + b 1 ])[W 2 ?W 2 ] + b 2 .<label>(7)</label></formula><p>Let H be the output of the hidden layer in FFN and H be the output of the addFFN's hidden layer, i.e.,</p><formula xml:id="formula_11">H = ReLU (XW 1 + b 1 ), H = ReLU (XW 1 + b 1 ).<label>(8)</label></formula><p>Eq. ( <ref type="formula" target="#formula_10">7</ref>) can be rewritten as: <ref type="bibr" target="#b8">(9)</ref> which proves the theorem.</p><formula xml:id="formula_12">F F N F L (X) = [H : H][W 2 ?W 2 ]+b 2 = H W 2 +HW 2 +b 2 = addF F N (X)+F F N (X),</formula><p>In the proof of Theorem 1, additional hidden units are introduced into FFN as the prefix (at the beginning of the FFN hidden layer). Alternatively, we also have the choices of the infix (in the middle of the FFN hidden layer) and suffix (at the end of the FFN hidden layer) to add hidden units. However, these three cases are equivalent, as stated in the following theorem. Theorem 2. In FL-tuning, Eq. ( <ref type="formula" target="#formula_8">6</ref>) is not affected by the position of addF F N (?). That is, Eq. ( <ref type="formula" target="#formula_8">6</ref>) holds regardless of whether addF F N (?) is prefix, infix, or suffix.</p><p>Proof. The proof of this theorem is divided into the following three cases:</p><formula xml:id="formula_13">Prefix: In the proof of Theorem 1, we have F F N F L (X) = addF F N (X) + F F N (X). Infix: In this case, W 1 , b 1 , and W 2 can be split into [W 1p : W 1s ], [b 1p : b 1s ], and [W 2p ?W 2s ].</formula><p>Then, we formulate FL-tuning in the infix form as follows:</p><formula xml:id="formula_14">F F N F L (X) =ReLU (X[W 1p : W 1 : W 1s ] + [b 1p : b 1 : b 1s ])[W 2p ?W 2 ?W 2s ] + b 2 . =[H 1p : H : H 1s ][W 2p ?W 2 ?W 2s ] + b 2 = H 1p W 2p + H W 2 + H 1s W 2s + b 2 =H W 2 + [H 1p : H 1s ][W 2p ?W 2s ] + b 2 = H W 2 + HW 2 + b 2 =addF F N (X) + F F N (X).</formula><p>(10) Suffix: The proof of this case is similar to that of Theorem 1. That is,</p><formula xml:id="formula_15">F F N F L (X) =ReLU (X[W 1 : W 1 ][b 1 : b 1 ])[W 2 ?W 2 ] + b 2 =[H : H ][W 2 ?W 2 ] + b 2 = HW 2 + H W 2 + b 2 =F F N (X) + addF F N (X)<label>(11)</label></formula><p>In summary, Eq. ( <ref type="formula" target="#formula_8">6</ref>) is not influenced by the position of addF F N (?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we investigate our tuning method over three PLMs and seven NLU tasks. The experimental results and detailed analysis demonstrate the superior performance, stable training, and faster convergence of FL-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Backbones. We perform experiments on three Transformer-based PLMs, including RoBERTa <ref type="bibr" target="#b27">[28]</ref>, NEZHA <ref type="bibr" target="#b28">[29]</ref>, and RoFormer <ref type="bibr" target="#b29">[30]</ref>. We carefully tune the hyperparameters in experiments based on RoBERTa and apply the optimal ones to NEZHA and RoFormer. Hyperparameters used in fine-tuning, P-tuning v1 <ref type="bibr" target="#b8">[9]</ref>, P-tuning v2 <ref type="bibr" target="#b7">[8]</ref>, and FL-tuning over the three PLMs are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Baselines. We compare our FL-tuning (FL) with fine-tuning (FT), P-tuning v1 (PV1) <ref type="bibr" target="#b8">[9]</ref>, and P-tuning v2 (PV2) <ref type="bibr" target="#b7">[8]</ref>. The prompt length in both PV1 and PV2 are set to 160, and the number of added hidden units of each FFN layer in FL is also pre-defined as 160. FT's results are obtained by tuning all the Transformer's parameters without prompts. Results of PV1, PV2, and FL are obtained by freezing the original parameters of the Transformer and only tuning the introduced trainable parameters.  Evaluation metrics. We report F1 (%) as the evaluation metric for the NER task and Accuracy (%) for other NLU tasks. All comparison results are obtained after submitting to CLUE and the detailed analysis is based on the parameters of the submitted models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Results Across Tasks</head><p>Similar to CLUE <ref type="bibr" target="#b30">[31]</ref>, we divide the selected seven kinds of NLU tasks into four categories: Single-Sentence Tasks, Sentence Pair Tasks, Name Entity Recognition Tasks, and Machine Reading Comprehension Tasks.</p><p>Single-Sentence Tasks include TC and PD. We adopt the IFLYTEK dataset for long TC, TNEWS for short TC, and WSC for PD. The experimental results on TC and PD tasks are shown in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>, respectively. From the table, we observe that FL-tuning outperforms fine-tuning and   prompt tuning in most cases, indicating the effectiveness of our method on TC and PD tasks. In particular, the improvement of NEZHA-based FL-tuning on PD task is very obvious, and its accuracy is 12.76% and 17.93% higher than that of fine-tuning and P-tuning v2. The reason may be that the training dataset of WSC is relatively small and FL-tuning is more suitable for this scenario than other methods. To verify this argument, we supplement the experiments under few-shot settings later in this subsection.</p><p>Sentence Pair Tasks aim to predict relations between sentence pairs, or abstract-keyword pairs, including SS (AFQMC), NLI (CMNLI and OCNLI), and KR (CSL). Table <ref type="table" target="#tab_3">4</ref> shows the results on SS, NLI, and KR. According to the results, we conclude that FL-tuning is better than prompt tuning in almost all cases. In particular, it achieves a 2.15% improvement over 78.77% obtained by RoFormer-based PV2. Although FT's results are slightly better than ours, the number of learnable parameters in FL-tuning is only about 3% of Transformer's parameters, which greatly reduces the training cost.</p><p>Name Entity Recognition Tasks experiment on the CLUE Fine-Grain NER (CLUENER) dataset and the results are shown in Table <ref type="table" target="#tab_2">3</ref>. From the table, we observe that FL-tuning performs best among tuning methods in all cases, which further demonstrates the effectiveness of our PLMs' tuning method.</p><p>Machine Reading Comprehension Tasks include C3, CHID, and CMRC2018 datasets. The results are shown in Table <ref type="table" target="#tab_4">5</ref>. According to the results, we find that the performance of FL-tuning on MRC tasks is similar to that of Sentence Pair tasks. Although it is not as good as fine-tuning in most cases, it significantly outperforms prompt tuning methods. This verifies that layer tuning is more effective than prompt tuning.    (d) CMRC2018</p><p>Figure <ref type="figure">5</ref>: The impact of the number of hidden units on model performance. "[x-y]" refers to the layer interval where we add hidden units. The optimal value of the number of hidden unit varies from task to task.</p><p>Few-shot Setting. In addition to the comparison on the full data, we use RoBERTa as the backbone to compare FL-tuning with prompt tuning methods under few-shot setting. We set the training set to be {20, 40, 60, 80, 100}. The experimental results on TNEWS 1.0, CMNLI, CLUENER, and CHID datasets are reported in Table <ref type="table" target="#tab_5">6</ref>. It can be found that FL-tuning is better than PV1 and PV2 in almost all cases, indicating that our method is also suitable for few-shot learning. In particular, our FL-tuning improves F1 by more than 10% over PV2 in almost all training sample size settings on the CLUENER dataset.</p><p>Summary: Combing the above results, we conclude that: 1) The performance of FL-tuning is much better than that of prompt tuning methods. 2) With only about 3% of Transformer's parameters to be trained, FL-tuning is comparable with fine-tuning on most datasets, and significantly outperforms fine-tuning on several datasets. 3) Compared with the full-data setting, FL-tuning's performance gain over prompt tuning methods is larger under the few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Analysis</head><p>Next, we take RoBERTa as the backbone to analyze our FL-tuning in detail from the perspectives of convergence, hyperparameter sensitivity as well as the impact of the number and depth of added hidden units on model performance.  Figure <ref type="figure">6</ref>: The impact of the depth of hidden units on model performance. The performance of our FL-tuning is positively correlated with the number of added layers. In addition, the deeper the layer of hidden units, the better the model performance.</p><p>Faster convergence and more stable training. We compare our FL-tuning with PV2 in terms of convergence speed and training stability. The results on WSC, OCNLI, CLUENER, and CMRC2018 are shown in Figure <ref type="figure" target="#fig_3">3</ref>. From the results, we observe that the loss value of FL-tuning decreases faster than that of PV2, which demonstrates that the former is better than the latter in terms of convergence speed. We further record their running time and find that our FL-tuning converges about 1.17 times faster than PV2 on average. In addition, the loss curve of FL-tuning is flatter than that of PV2 during the training process, indicating that FL-tuning is more stable.</p><p>Hyperparameter insensitive. The prediction results of deep models are often sensitive to hyperparameters, especially the learning rate. Even two learning rates with a small gap would make a great difference in predictions. Hence, we analyze the impact of different learning rates on model performance. The results on WSC and CLUENER datasets are reported in Figure <ref type="figure" target="#fig_5">4</ref>. From the results, we find that in our FL-tuning, the results obtained with different learning rates are more concentrated, while the results are more scattered in PV2. This verifies that our model is more insensitive to the learning rate than PV2.</p><p>The number of hidden units. The number of added hidden units is an influential hyperparameter for our FL-tuning. We adjust the values to analyze its impact on the model performance. The results on four datasets are shown in Figure <ref type="figure">5</ref>. The model performance improves with the increase of the number of hidden units on CLUENER, while there is no apparent regularity on the other three datasets. As a result, the optimal value of the number of hidden units varies from task to task.</p><p>Depth of hidden units. To analyze the influence of adding hidden units to different layers on model performance, we select k layers in both ascending and descending order to introduce hidden units. The results on TNEWS, WSC, OCNLI, and CLUENER are shown in Figure <ref type="figure">6</ref>. According to the results, we observe that the performance of our FL-tuning is not only positively correlated to the number of added layers, but also related to the layer in which the added hidden units are located. The deeper the layer, the better the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>In the Transformer, there are mainly two sub-layers: multi-head self-attention and FFN. The idea of FL-tuning can also be transferred to self-attention. Thus, we perform layer tuning on multi-head self-attention called MA-tuning to compare its performance with FL-tuning. That is, we increase the hidden size of the matrices in multi-head self-attention. The comparison results are listed in Table <ref type="table" target="#tab_7">7</ref>. We find that FL-tuning slightly outperforms MA-tuning in most cases. The possible reason is that, as mentioned in the Introduction, FFN occupies about 2/3 of the number of parameters in the model and tuning it may perform better. In addition, the results also show the feasibility of layer tuning on self-attention and FFN at the same time. Hence, we will implement this idea in the future and further analyze its possible optimal combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel tuning way, namely layer tuning, to accommodate PLMs to downstream tasks, which aims to add learnable parameters to the Transformer layers. Specifically, we mainly focus on layer tuning on FFN called FL-tuning in the Transformer. It introduces additional units into the hidden layer of each FFN. We conduct extensive experiments on the CLUE benchmark and the results show that: 1) With only about 3% of Transformer's parameters to be trained, our FL-tuning is comparable with fine-tuning on most datasets, and significantly outperforms fine-tuning on several datasets. 2) FL-tuning is better than prompt tuning methods under both full-data and few-shot settings in almost all cases. 3) FL-tuning is more stable and converges about 1.17 times faster than P-tuning v2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FL-tuning (Layer Tuning)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[e(S), e(I), e(f elt), e([M ASK])].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The model architecture of FL-tuning. FL-tuning introduces additional hidden units to each FFN layer, and the units of each layer are independent. The orange circles are hidden units in addFFN, the blue circles are hidden units in original FFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence comparison of FL-tuning and P-tuning v2. The curves in dark color are obtained by smoothing the loss curves (light color). The smoothing function is ? * previousStep_smoothed_value + (1 -?) * current_value, where ? = 0.99 is the smooth weight.The models run on Ubuntu 20.04 with Intel Xeon Gold 6248R*2, NVIDIA V100 and 512G of RAM. FL-tuning is more stable and converges faster than PV2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hyperparameter sensitivity of FL-tuning and P-tuning v2. The former is more insensitive to the learning rate than the latter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters (BS: batch size; SL: sequence length; LR: learning rate; Epo: epoch) used in PLMs' tuning methods on CLUE benchmark and the statistics of CLUE.</figDesc><table><row><cell></cell><cell cols="4">Fine-tuning BS SL LR Epo BS</cell><cell cols="2">P-tuning v1 SL LR</cell><cell cols="7">P-tuning v2 Epo BS SL LR Epo BS SL FL-tuning LR Epo</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>IFLYTEK</cell><cell cols="2">32 128 1e-5</cell><cell>30</cell><cell cols="3">128 128 6e-4</cell><cell>40</cell><cell cols="2">32 128 2e-4</cell><cell>50</cell><cell cols="2">32 128 5e-5</cell><cell>30</cell><cell>12.1K</cell><cell>2.5K</cell><cell>2.6K</cell></row><row><cell>TNEWS</cell><cell cols="2">32 128 1e-5</cell><cell>40</cell><cell cols="3">128 128 4e-4</cell><cell>30</cell><cell cols="2">32 128 1e-3</cell><cell>80</cell><cell cols="2">32 128 5e-5</cell><cell>30</cell><cell>266K</cell><cell>57K</cell><cell>57K</cell></row><row><cell>WSC</cell><cell cols="2">32 128 5e-5</cell><cell>50</cell><cell cols="3">128 128 6e-4</cell><cell>20</cell><cell cols="2">32 128 3e-2</cell><cell>80</cell><cell cols="2">32 128 3e-4</cell><cell>40</cell><cell>532</cell><cell>104</cell><cell>143</cell></row><row><cell>AFQMC</cell><cell cols="2">32 128 1e-5</cell><cell>15</cell><cell cols="3">128 128 6e-4</cell><cell>30</cell><cell cols="2">32 128 8e-3</cell><cell>30</cell><cell cols="2">32 128 2e-4</cell><cell>10</cell><cell>34.3K</cell><cell>4.3K</cell><cell>3.8K</cell></row><row><cell>CMNLI</cell><cell cols="2">32 128 1e-5</cell><cell>10</cell><cell cols="3">128 128 8e-4</cell><cell>40</cell><cell cols="5">32 128 1e-2 100 32 128 1e-4</cell><cell>30</cell><cell cols="2">391.7K 12.4K 13.8K</cell></row><row><cell>OCNLI</cell><cell cols="2">32 128 1e-5</cell><cell>30</cell><cell cols="3">128 128 4e-4</cell><cell>80</cell><cell cols="5">32 128 1e-2 150 32 128 5e-4</cell><cell>30</cell><cell>50K</cell><cell>3K</cell><cell>3K</cell></row><row><cell>CSL</cell><cell cols="2">32 128 1e-5</cell><cell>30</cell><cell cols="6">32 128 128 6e-4 50 128 9e-3</cell><cell>50</cell><cell cols="3">32 128 8e-5 100</cell><cell>532</cell><cell>104</cell><cell>143</cell></row><row><cell>CLUENER</cell><cell cols="2">32 256 2e-5</cell><cell>30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">32 256 8e-2</cell><cell>50</cell><cell cols="2">32 256 1e-4</cell><cell>30</cell><cell>10.7K</cell><cell>1.3K</cell><cell>1.3K</cell></row><row><cell>C3</cell><cell>4</cell><cell>512 1e-5</cell><cell>50</cell><cell cols="3">16 512 1e-3</cell><cell>50</cell><cell>4</cell><cell cols="2">512 1e-2 100</cell><cell>4</cell><cell>512 1e-4</cell><cell>50</cell><cell>11.8K</cell><cell>3.8K</cell><cell>3.8K</cell></row><row><cell>CHID</cell><cell>16</cell><cell>64 1e-5</cell><cell>30</cell><cell cols="3">128 64 4e-4</cell><cell>30</cell><cell>16</cell><cell>64 2e-3</cell><cell>80</cell><cell>16</cell><cell>64 8e-5</cell><cell>30</cell><cell>84.7K</cell><cell>3.2K</cell><cell>3.2K</cell></row><row><cell cols="3">CMRC2018 16 512 5e-5</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">16 512 3e-2</cell><cell>50</cell><cell cols="2">16 512 2e-4</cell><cell>20</cell><cell>12.5K</cell><cell>1.2K</cell><cell>4K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of different tuning methods on TC task. We highlight the best of FT, PV1, PV2, and FL in dark gray and use light gray for the next best. If FL outperforms PV2, the improvement over PV2 is reflected in red below. The difference between TNEWS 1.0 and TNEWS 1.1 is that their validation sets are different.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Text Classification (TC)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">IFLYTEK</cell><cell></cell><cell></cell><cell cols="2">TNEWS 1.0</cell><cell></cell><cell></cell><cell cols="2">TNEWS 1.1</cell></row><row><cell></cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell></row><row><cell cols="4">RoBERTa 61.000 54.310 61.730</cell><cell>62.150</cell><cell cols="3">57.170 55.700 56.770</cell><cell>57.510</cell><cell cols="3">56.960 55.370 58.130</cell><cell>59.750</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.420)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.740)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.620)</cell></row><row><cell>NEZHA</cell><cell cols="3">59.080 55.650 62.810</cell><cell>60.770</cell><cell cols="3">58.510 56.360 58.190</cell><cell>57.940</cell><cell cols="3">58.990 58.440 61.300</cell><cell>60.590</cell></row><row><cell cols="4">RoFormer 61.000 54.620 61.620</cell><cell>62.000</cell><cell cols="3">57.840 55.370 57.380</cell><cell>57.070</cell><cell cols="3">55.210 54.530 58.100</cell><cell>57.700</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.380)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different tuning methods on PD and NER tasks. FL-tuning achieves the best performance on the two tasks. In particular, it improves accuracy by 12.76% and 17.93% on WSC 1.0 over FT and PV2, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Pronoun Disambiguation (PD)</cell><cell></cell><cell></cell><cell cols="4">Name Entity Recognition (NER)</cell></row><row><cell></cell><cell></cell><cell cols="2">WSC 1.0</cell><cell></cell><cell></cell><cell cols="2">WSC 1.1</cell><cell></cell><cell></cell><cell cols="2">CLUENER</cell></row><row><cell></cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell></row><row><cell cols="4">RoBERTa 80.690 78.620 79.660</cell><cell>82.070</cell><cell cols="3">78.240 79.910 78.590</cell><cell>80.540</cell><cell>79.596</cell><cell>-</cell><cell>79.583</cell><cell>80.479</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+2.410)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.950)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.896)</cell></row><row><cell>NEZHA</cell><cell cols="3">68.620 61.380 63.450</cell><cell>81.380</cell><cell cols="3">66.080 51.630 69.810</cell><cell>78.980</cell><cell>80.636</cell><cell>-</cell><cell>80.296</cell><cell>80.866</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+17.930)</cell><cell></cell><cell></cell><cell></cell><cell>(+9.170)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.570)</cell></row><row><cell cols="4">RoFormer 71.720 62.410 75.170</cell><cell>78.970</cell><cell cols="3">74.510 56.370 71.520</cell><cell>81.240</cell><cell>79.395</cell><cell>-</cell><cell>79.229</cell><cell>80.839</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+3.800)</cell><cell></cell><cell></cell><cell></cell><cell>(+9.720)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.610)</cell></row></table><note><p><p><p><p><p><p><p><p><p>Datasets. We use NLU text datasets from the CLUE 2</p><ref type="bibr" target="#b30">[31]</ref> </p>benchmark as experimental data, which is similar to GLUE</p><ref type="bibr" target="#b31">[32]</ref> </p>and SuperGLUE</p><ref type="bibr" target="#b32">[33]</ref></p>. We select 11 NLU datasets from CLUE for the comparison experiments. The scale of each dataset is shown in Table</p>1</p>. The selected 11 text datasets are divided into 7 kinds of NLU tasks, including Text Classification (TC), Pronoun Disambiguation (PD), Semantic Similarity (SS), Natural Language Inference (NLI), Keyword Recognition (KR), Name Entity Recognition (NER), and Machine Reading Comprehension (MRC).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of different tuning methods on SS, NLI, and KR tasks. FL-tuning is better than prompt tuning methods. Although FT slightly outperforms FL, the number of trainable parameters in FL is only about 3% of Transformer's parameters.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Semantic Similarity (SS)</cell><cell></cell><cell></cell><cell cols="4">Natural Language Inference (NLI)</cell><cell></cell><cell></cell><cell cols="4">Keyword Recognition (KR)</cell></row><row><cell></cell><cell></cell><cell cols="2">AFQMC</cell><cell></cell><cell></cell><cell cols="2">CMNLI</cell><cell></cell><cell></cell><cell cols="2">OCNLI</cell><cell></cell><cell></cell><cell></cell><cell>CSL</cell></row><row><cell></cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell></row><row><cell cols="4">RoBERTa 72.570 69.850 72.030</cell><cell>73.370</cell><cell cols="3">80.430 67.470 79.990</cell><cell>80.370</cell><cell cols="3">72.900 64.630 72.370</cell><cell>73.530</cell><cell cols="3">85.330 81.000 84.000</cell><cell>84.930</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.340)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.380)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.160)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.930)</cell></row><row><cell>NEZHA</cell><cell cols="3">73.970 69.770 74.330</cell><cell>74.180</cell><cell cols="3">81.450 71.950 80.160</cell><cell>81.160</cell><cell cols="3">75.300 67.600 74.100</cell><cell>74.470</cell><cell cols="3">84.500 82.130 84.900</cell><cell>84.670</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.000)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.370)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">RoFormer 73.820 69.960 71.920</cell><cell>72.990</cell><cell cols="3">81.060 64.840 78.770</cell><cell>80.920</cell><cell cols="3">73.970 63.670 71.800</cell><cell>72.100</cell><cell cols="3">84.130 82.730 83.830</cell><cell>85.030</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.070)</cell><cell></cell><cell></cell><cell></cell><cell>(+2.150)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.300)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.200)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison results of different tuning methods on MRC task. FL significantly outperforms PV2 in almost all cases and is comparable with FT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Machine Reading Comprehension (MRC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>C3 1.0</cell><cell></cell><cell></cell><cell></cell><cell>C3 1.1</cell><cell></cell><cell></cell><cell cols="2">CHID</cell><cell></cell><cell></cell><cell cols="2">CMRC2018</cell></row><row><cell></cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>FT</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell></row><row><cell cols="4">RoBERTa 67.650 50.800 64.210</cell><cell>67.060</cell><cell cols="3">73.600 48.000 70.090</cell><cell>74.340</cell><cell cols="3">87.544 50.605 82.981</cell><cell>87.246</cell><cell>77.500</cell><cell>-</cell><cell>76.300</cell><cell>77.900</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+2.850)</cell><cell></cell><cell></cell><cell></cell><cell>(+4.250)</cell><cell></cell><cell></cell><cell></cell><cell>(+4.265)</cell><cell></cell><cell></cell><cell></cell><cell>(+1.600)</cell></row><row><cell>NEZHA</cell><cell cols="3">70.990 54.550 69.810</cell><cell>71.220</cell><cell cols="3">76.120 50.220 73.290</cell><cell>73.540</cell><cell cols="3">89.250 63.911 85.557</cell><cell>88.961</cell><cell>76.350</cell><cell>-</cell><cell>72.650</cell><cell>75.900</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.410)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.250)</cell><cell></cell><cell></cell><cell></cell><cell>(+3.404)</cell><cell></cell><cell></cell><cell></cell><cell>(+3.250)</cell></row><row><cell cols="4">RoFormer 66.650 53.030 60.380</cell><cell>65.930</cell><cell cols="3">72.550 51.320 67.880</cell><cell>73.720</cell><cell cols="3">86.759 54.276 81.916</cell><cell>86.953</cell><cell>73.500</cell><cell>-</cell><cell>74.600</cell><cell>72.050</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+5.550)</cell><cell></cell><cell></cell><cell></cell><cell>(+5.840)</cell><cell></cell><cell></cell><cell></cell><cell>(+5.037)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison results of layer/prompt tuning methods under few-shot setting. Our FL-tuning achieves the best results in almost all cases. In particular, it improves F1 of CLUENER by more than 10% on average over PV2.</figDesc><table><row><cell></cell><cell>TNEWS 1.0</cell><cell></cell><cell></cell><cell>CMNLI</cell><cell></cell><cell></cell><cell cols="2">CLUENER</cell><cell></cell><cell>CHID</cell><cell></cell></row><row><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell><cell>PV1</cell><cell>PV2</cell><cell>FL</cell></row><row><cell cols="2">20 42.990 48.730</cell><cell>49.150</cell><cell cols="2">34.060 44.790</cell><cell>44.900</cell><cell>-</cell><cell>23.211</cell><cell>27.525</cell><cell cols="2">51.226 52.131</cell><cell>52.811</cell></row><row><cell></cell><cell cols="2">(+0.420)</cell><cell></cell><cell></cell><cell>(+0.11)</cell><cell>-</cell><cell></cell><cell>(+4.314)</cell><cell></cell><cell></cell><cell>(+0.680)</cell></row><row><cell cols="2">40 45.410 49.440</cell><cell>49.720</cell><cell cols="2">34.250 50.040</cell><cell>51.000</cell><cell>-</cell><cell>25.347</cell><cell>41.489</cell><cell cols="2">51.937 52.674</cell><cell>52.842</cell></row><row><cell></cell><cell cols="2">(+0.280)</cell><cell></cell><cell></cell><cell>(+0.960)</cell><cell>-</cell><cell></cell><cell>(+16.142)</cell><cell></cell><cell></cell><cell>(+0.168)</cell></row><row><cell cols="2">60 43.350 48.980</cell><cell>49.110</cell><cell cols="2">34.300 49.790</cell><cell>50.490</cell><cell>-</cell><cell>35.788</cell><cell>46.295</cell><cell cols="2">52.674 52.898</cell><cell>52.932</cell></row><row><cell></cell><cell cols="2">(+0.130)</cell><cell></cell><cell></cell><cell>(+0.700)</cell><cell>-</cell><cell></cell><cell>(+10.507)</cell><cell></cell><cell></cell><cell>(+0.034)</cell></row><row><cell cols="2">80 43.780 49.370</cell><cell>49.510</cell><cell cols="2">35.980 50.190</cell><cell>52.590</cell><cell>-</cell><cell>40.962</cell><cell>53.575</cell><cell cols="2">52.850 52.648</cell><cell>52.609</cell></row><row><cell></cell><cell cols="2">(+0.140)</cell><cell></cell><cell></cell><cell>(+2.400)</cell><cell>-</cell><cell></cell><cell>(+12.613)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">100 47.850 49.660</cell><cell>50.030</cell><cell cols="2">35.520 51.310</cell><cell>52.740</cell><cell>-</cell><cell>48.093</cell><cell>59.226</cell><cell cols="2">52.454 52.100</cell><cell>52.794</cell></row><row><cell></cell><cell cols="2">(+0.370)</cell><cell></cell><cell></cell><cell>(+1.430)</cell><cell>-</cell><cell></cell><cell>(+11.133)</cell><cell></cell><cell></cell><cell>(+0.694)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison results of FL-tuning and MA-tuning (MA) on RoBERTa. It is more effective to realize layer tuning on FFN than multi-head self-attention.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">IFLYTEK TNEWS 1.1 WSC 1.1 AFQMC CMNLI</cell><cell>CSL</cell><cell cols="2">CLUENER C3 1.0 CHID CMRC2018</cell></row><row><cell cols="2">FL-tuning</cell><cell>62.000</cell><cell></cell><cell>57.700</cell><cell>81.240</cell><cell>72.990</cell><cell>80.920</cell><cell>85.030</cell><cell>80.839</cell><cell>73.720 86.953</cell><cell>72.050</cell></row><row><cell cols="2">MA-tuning</cell><cell>61.540</cell><cell></cell><cell>57.140</cell><cell>81.590</cell><cell>73.740</cell><cell>80.570</cell><cell>84.400</cell><cell>80.319</cell><cell>73.170 85.868</cell><cell>71.850</cell></row><row><cell>1-2</cell><cell>1-4</cell><cell>1-6 ascend_order 1-8</cell><cell>1-10</cell><cell>1-12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.cluebenchmarks.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recall and learn: Fine-tuning deep pretrained language models with less forgetting</title>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzhan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7870" to="7881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to fine-tune BERT for text classification?</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics -18th China National Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11856</biblScope>
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Gpt understands, too</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5484" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Ptr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Prompt tuning with rules for text classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask attention networks: Rethinking and strengthen transformer</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1692" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast transformers with clustered attention</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21665" to="21674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Predictive attention transformer: Improving transformer with attention map prediction</title>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerating neural transformer via an average attention network</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02436</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Talking-heads attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Kformer: Knowledge injection in transformer feed-forward layers</title>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2201.05742</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00204</idno>
		<title level="m">Neural contextualized representation for chinese language understanding</title>
		<meeting><address><addrLine>Nezha</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CLUE: A chinese language understanding evaluation benchmark</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yechen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yina</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuoyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoweihua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4762" to="4772" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
