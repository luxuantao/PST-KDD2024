<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julian</forename><surname>Faraone</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney* Xilinx Research Labs #</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Fraser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney* Xilinx Research Labs #</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michaela</forename><surname>Blott</surname></persName>
							<email>mblott@xilinx.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney* Xilinx Research Labs #</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">#</forename><surname>Philip</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney* Xilinx Research Labs #</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Leong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney* Xilinx Research Labs #</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inference for state-of-the-art deep neural networks is computationally expensive, making them difficult to deploy on constrained hardware environments. An efficient way to reduce this complexity is to quantize the weight parameters and/or activations during training by approximating their distributions with a limited entry codebook. For very low-precisions, such as binary or ternary networks with 1-8-bit activations, the information loss from quantization leads to significant accuracy degradation due to large gradient mismatches between the forward and backward functions. In this paper, we introduce a quantization method to reduce this loss by learning a symmetric codebook for particular weight subgroups. These subgroups are determined based on their locality in the weight matrix, such that the hardware simplicity of the low-precision representations is preserved. Empirically, we show that symmetric quantization can substantially improve accuracy for networks with extremely low-precision weights and activations. We also demonstrate that this representation imposes minimal or no hardware implications to more coarse-grained approaches. Source code is available at https://www.github.com/julianfaraone/SYQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) have produced state-ofthe-art results in applications such as computer vision <ref type="bibr" target="#b16">[17]</ref>, natural language processing <ref type="bibr" target="#b3">[4]</ref> and object detection <ref type="bibr" target="#b30">[31]</ref>. As their size continues to grow to improve prediction capabilities, their memory and computational requirements also scales, making them increasingly difficult to deploy on embedded systems. For example, <ref type="bibr" target="#b16">[17]</ref> achieved stateof-art-results on the ImageNet challenge using AlexNet which required 240MB of storage and 1.45 billion operations to compute inference per image. Several methods of compression <ref type="bibr" target="#b11">[12]</ref>, quantization <ref type="bibr" target="#b2">[3]</ref> and dimensionality reduction <ref type="bibr" target="#b24">[25]</ref> have been applied to reduce these demands, with promising results. This demonstrates the over-parametrization and redundancies in DNNs and poses an opportunity for utilizing regularization to make their representations more amenable to hardware implementations.</p><p>In particular, low-precision neural networks reduce both memory and computational requirements whilst achieving accuracies comparable to floating point <ref type="bibr" target="#b9">[10]</ref>. For extremely low-precisions, such as binary and/or ternary weight representations and 1-8 bits for activations, most of the multiplyaccumulate (MAC) operations can be replaced by simple bitwise operations. This translates to massive reductions in storage requirements and spatial complexity in hardware. Additionally, large power savings and speed gains are achieved when networks can fit in on-chip memory. The issue is that a large reduction in precision, leads to large information loss which incurs significant accuracy degradation, especially for complex datasets such as ImageNet <ref type="bibr" target="#b25">[26]</ref>. Ideally, we can train networks which have both high prediction capabilities and minimal computational complexity.</p><p>DNN training is an iterative process which has a feedforward path to compute the output and a backpropagation path to calculate gradients and update its parameters for learning. Low-precision networks involve having a set of full-precision weights which are quantized before computing inference. As the quantization functions are piecewise and constant, the gradients of quantized weights are calculated and applied to update their corresponding fullprecision weights. Similarly, derivatives of quantized activations are calculated by using a non-constant differentiable approximation function. This type of training was first proposed as the Straight Through Estimator (STE) <ref type="bibr" target="#b0">[1]</ref> which suggested the use of a nonzero derivative approximation to functions which are non-differentiable or have zero derivatives everywhere. The problem is that without an accurate estimator for weights and activations, there exists a significant gradient mismatch which impinges on learning. Seemingly, as discussed in <ref type="bibr" target="#b21">[22]</ref>, activations are more robust to quantization than weights for image classification problems due to weight reuse in Convolutional (CONV) layers affecting multiple operations. To overcome this, methods such as increasing the weight codebook by applying a scaling co-efficient to all weights in a layer, provides better approximations for weight distributions and greater model capacity <ref type="bibr" target="#b18">[19]</ref>. This is computationally inexpensive and can be represented as multiplying each weight layer's matrix by a diagonal scalar matrix which only requires storage of one value. Applying fine-grained scaling coefficients has also been shown to improve accuracy by increasing model capacity <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The problem with all of these fine-grained approaches is either large storage requirements for the scaling coefficients or high computational complexity due to irregular codebook indices. In this paper we present Learning Symmetric Quantization (SYQ), a method to design binary/ternary networks with fine-grained scaling coefficients which preserve these complexities. We do this by learning a symmetric weight codebook via gradient-based optimizations which enables a minimally-sized square diagonal scalar matrix representation. To reduce the large information loss from CONV layer quantization, we use a more fine-grained pixel/row-wise scaling approach, rather than layer-wise scaling in Fully-Connected (FC) layers. In the process, we significantly close the accuracy gap for low-precision networks to their floating point counterpart, whilst preserving their efficient computational structures. Our work makes the following contributions:</p><p>• Our approach significantly improves the ability of convolutional weights to learn low-precision representations. This is useful as most layers in modern network architectures consist of convolutions which are typically the least redundant layers.</p><p>• The proposed method reduces the computational complexity of traditional fine-grained low-precision scaling and imposes minimal hardware costs to layer-wise scaling.</p><p>• On state-of-the-art networks such as AlexNet, ResNet and VGG, our method is empirically shown to improve accuracy for 1-2 bit weights and 2-8 bit activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most methods for training low-precision DNNs maintain a set of full precision weights that are deterministically or stochastically quantized during forward or backward propagation. Gradient updates computed with the quantized weights are then applied to the full precision weights <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>. To produce state-of-the-art results on larger models, <ref type="bibr" target="#b23">[24]</ref> proposed scaling the quantized weights by the expectation of real-valued weights to recover the dynamic range of each layer. <ref type="bibr" target="#b18">[19]</ref> also implemented a similar technique for ternary networks and optimised a non-zero quantization threshold as a function of the weight expectation. Other gradient-based optimization methods for the scaling coefficient have been introduced <ref type="bibr" target="#b33">[34]</ref>. Other methods of quantization have also been implemented, i.e. re-training networks using incremental weight subgrouping to produce no accuracy loss for 5 bit weights <ref type="bibr" target="#b31">[32]</ref>. Multiple binarizations and a scaling layer were described in <ref type="bibr" target="#b27">[28]</ref> to improve accuracy and binarize the last layer. Logarithmic data representations were used to approximate the non-uniform distribution of the weights, activations and gradients down to 3-bits with negligible accuracy loss <ref type="bibr" target="#b21">[22]</ref>. Activations quantization has also been investigated with frameworks created for varying activation bitwidths <ref type="bibr" target="#b32">[33]</ref> and both weights and activations <ref type="bibr" target="#b22">[23]</ref>. Improving the network learnability under low-precision weights and activations was analysed in <ref type="bibr" target="#b1">[2]</ref>. More fine-grained approaches of quantization have effectively clustered weights or grouped filters together and quantize differently based on their statistical distributions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref> . Increasing model capacity by applying scaling coefficients to positive and negative values separately was proposed in <ref type="bibr" target="#b33">[34]</ref>. Furthermore, sparse representations were used as regularization to make networks more amenable to hardware <ref type="bibr" target="#b6">[7]</ref>. Also, many low-precision DNN hardware implementations have been published <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b10">[11]</ref>. For example, FINN <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b28">[29]</ref> demonstrated the performance gains of being able to store all network weights in on-chip memory by implementing binarized neural networks on FPGAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Low-Precision Networks</head><p>In this section we discuss the motivations behind our work and fundamentals of low-precision neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Each layer of a DNN computes dot products between weight parameters and its input values. We can represent the output of each hidden unit h, as:</p><formula xml:id="formula_0">h = g(w T x)<label>(1)</label></formula><p>where g is an element-wise nonlinear activation function, x ∈ R i.w.h is the input vector, and w ∈ R i.w.h provides the weight vector of a linear transformation. This computation is repeated throughout the network, therefore overall model complexity is dependant on its structure. As modern networks continue to get deeper/wider, model complexity becomes problematic for their applicability on constrained hardware environments. A solution is to efficiently quantize both weights and activations to very low-precisions (1-8 bits) with negligible or no accuracy loss. In doing so, the arithmetic operations are greatly simplified, reducing both computational and resource complexity. In the binary/ternary weight case, MACs are replaced by bit operations. For example, Figure <ref type="figure" target="#fig_0">1</ref> shows average resource usage on Field Programmable Gate Array (FPGA) hardware to implement a MAC operation under different precisions, which scales quadratically with the multiplier size at O(k 2 ) where k is the number of bits 1 . As shown, no high precision multipliers (known as DSPs on an FPGA) are required for precisions less than or equal to ternary weights and 8-bit activations. Furthermore, the logic element (known as LUTs on an FPGA) requirement reduces proportionally with both weight and activation precisions. Additionally, the storage requirements for both weights and activations is reduced by 8 − 32×. This significantly improves the network's ability to fit in on-chip memory and constrained hardware environments, and broadens the applicability of DNNs. For a CONV layer, all weights are typically represented as a tensor W l ∈ R K×K×I×N where K is the filter size, I is the number of input feature maps and N , the number of output feature maps. In low-precision networks, each weight layer l can typically be represented by a diagonal scalar matrix α l multiplied by quantized weight matrix Q l and ideally W l ≈ α l Q l . Also, the activation function g can be approximated using a piecewise constant activation function G. In our proposed method, we observe that by ensuring quantization levels for W are symmetric around zero, we can construct efficient square diagonal matrix representations of α l , which enable fine-grained quantization whilst having minimal memory requirements (of size K or K 2 ). This translates to a reduction in overall model complexity and high prediction capabilities. Although, we restrict ourselves by structured matrices and low-precision weights and activations, the network efficiently captures information through our gradient-based symmetric quantizer which learns the diagonal elements of α l during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weight Quantization</head><p>For low-precision DNNs, the distribution of full precision weight matrices for each layer W l are approximated by a function f , resulting in a quantized weight matrix Q l :</p><formula xml:id="formula_1">Q li,j = f (W l ) i,j<label>(2)</label></formula><p>1 Results are obtained from instantiating MAC modules using Vivado for W li,j ∈ R and Q li,j ∈ C. The codebook C = c 1 , c 2 , ..., c r is a set of all possible values for Q li,j where c i ∈ R and i ∈ R + represent each codebook value and index respectively. For example, binary and ternary weight spaces have C = − 1, +1 and C = − 1, 0, +1 respectively. Efficient functions for binarizing and ternarizing weight parameters have been proposed as piecewise constant functions in <ref type="bibr" target="#b18">[19]</ref>, such that:</p><formula xml:id="formula_2">Q l = sign(W l ) ⊙ M l (3)</formula><p>with,</p><formula xml:id="formula_3">M li,j = 1 if W li,j ≥ η l 0 if − η l &lt; W li,j &lt; η l (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where M represents a masking matrix, η is the quantization threshold hyperparameter. η = 0 for binary networks and in our work we set η = 0.05×max(|W l |) for ternary networks as in <ref type="bibr" target="#b33">[34]</ref>. The issue with discretization of the weights, is that it leads to the vanishing gradients problem <ref type="bibr" target="#b0">[1]</ref>. To overcome this, an STE is defined to replace the zero derivatives from the piecewise constant function in (3), by a non-zero surrogate derivative <ref type="bibr" target="#b14">[15]</ref>. During training Q l is used for inference and backpropagation, and the corresponding elements in W l are updated based on these gradients. Hence the STE is defined as:</p><formula xml:id="formula_5">∂ Ê ∂W li,j = ∂ Ê ∂Q li,j<label>(5)</label></formula><p>where Ê is the error function for a network without scaling coefficients. After training, the full precision weights are discarded and we require only the quantized weights for deployment. Whilst these methods greatly reduce computational complexity by eliminating floating point MACs, they increase the difficulty of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling</head><p>The introduction of scaling coefficients improves learning capabilities by providing greater model capacity and compensating for the large information loss due to binary/ternary quantization. Scaling discrete weight representations requires multiplying all Q li,j by positive scaling coefficients α ∈ R + . We want to find optimal scaling coefficients for each layer, α l , which minimize our error function:</p><formula xml:id="formula_6">α * l = argmin α E(α, Q) s.t. α ≥ 0, Q li,j ∈ C (6)</formula><p>with E representing the error function with scaling coefficients. Finding the optimal α l is vital to reducing gradient mismatches in the forward and backward functions. It was proposed in <ref type="bibr" target="#b32">[33]</ref> as the mean of absolute weight values for each layer:</p><formula xml:id="formula_7">α l = W l 1 Z l (7)</formula><p>where Z l is the total number of layer weights. The codebook for each layer after scaling in ( <ref type="formula">7</ref>) is symmetric: Ĉl = − α l , +α l and the scalars become per-layer learning rate multipliers. Additionally, the STE in (8) reduces the gradient mismatch from (5) by including information from the full precision weights:</p><formula xml:id="formula_8">∂E ∂W li,j = ∂E ∂Q li,j = α l ∂ Ê ∂Q li,j<label>(8)</label></formula><p>Gradient-based optimizations for scaling coefficients were also introduced in <ref type="bibr" target="#b33">[34]</ref> which applied different scaling coefficients for positive and negative Q li,j to improve model capacity and accuracies. These are updated during backpropagation using gradients:</p><formula xml:id="formula_9">∂E ∂α p l = i,j∈S p l ∂E ∂W li,j , ∂E ∂α n l = i,j∈S n l ∂E ∂W li,j<label>(9)</label></formula><p>where initially α p l0 , α n l0 = 1 and S l is the codebook indices for each layer, i.e. S p l = i, j|W li,j ≥ η and S n l = i, j|W li,j ≤ −η . This allows each layer's codebook values to be asymmetric around zero, such that Ĉl = − α n l , +α p l . The codebook indices are then highly irregular and unordered which increases computational complexity as the matrices cannot be easily decomposed. Rather we have to check the sign of every element before computation, leading to extra branching instructions for conventional computing platforms such as CPUs/GPUs and additional logic for custom hardware. The difficulty of designing low-precision networks which have both high learning capabilities and computational efficiency can be solved by learning a symmetric codebook during training and exploiting structured matrix representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SYQ Structural Representations</head><p>We now propose matrix representations of SYQ by partitioning the quantization into weight subgroups. Diagonal matrix representations consist of mainly zeros and have non-zero entries along the main diagonal. For a matrix D to be diagonal, D = 0 if D i,j = 0 ∀ i = j, and square if D ∈ R m×m . A square diagonal matrix consisting of all equal main diagonal entries is a scalar matrix. A diagonal matrix α l is defined by the vector α l = α 1 l , ..., α m l :</p><formula xml:id="formula_10">α = diag(α) :=     α 1 0 .. 0 0 0 α 2 .. : 0 : : .. α m−1 : 0 0 .. 0 α m    </formula><p>Diagonal matrix multiplication is very computationally efficient as it can be easily decomposed and only the scalar vector requires storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Layers</head><p>CONV and FC layers have differing computational requirements and sensitivities to network redundancies. CONV weights are reused many times across the input feature map whereas FC weights are used only once per image. Hence, the quantization error of each weight in a CONV layer impacts the dot products across the entire input feature map volume rather than just once for FC weights. Thus, a fine-grained approach to CONV layers is effective at compensating for this error. Quantized CONV weights are represented as a tensor</p><formula xml:id="formula_11">Q l ∈ R Z with Z = K × K × I × N .</formula><p>As typically I, N ≫ K, it is optimal to have a diagonal scalar of size K × K or even K 2 × K 2 as only small scalar vectors are required for storage. By reshaping the tensor Q l , we form a matrix <ref type="figure">K</ref>) and represent our scalar matrix multiplication as diag(α l )Q T l with the square diagonal matrix, diag(α l ) ∈ R K 2 ×K 2 or diag(α l ) ∈ R K×K respectively. FC layers are represented as a matrix Q l ∈ R L×H where H is the number of hidden nodes and L the activation neurons. As FC layers are more robust to quantization, one learnable scaling coefficient (layer-wise) for the FC layer can sufficiently approximate the distribution and also can be represented with scalar matrix computation. All elements in α l are then equal and we only require storage of one value.</p><formula xml:id="formula_12">Q l ∈ R Ẑ where Ẑ = K 2 × (IN ) or Ẑ = K × (IN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Subgroups</head><p>More fine-grained quantization can improve approximations of the statistical distributions of weights. We implement pixel-wise scaling for CONV layers which involves grouping all spatially equivalent pixels along the I × N dimension. This results in different values for all the main diagonal elements in diag(α) ∈ R K 2 ×K 2 . With this representation, we can still decompose the matrix computation along each pixel dimension and exploit the parallel nature of convolutions as shown in Figure <ref type="figure" target="#fig_1">2</ref>. We do this by creating subgroups 1 ≤ i ≤ K 2 with codebook indices S i l = j|W li,j . Other granularities such as row-wise scaling involve grouping all pixels along a row or column (I × N × K), resulting in S j l = S i l ∪ S i+1 l ... ∪ S K l where 1 ≤ j ≤ K (as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>) and also layer-wise scaling: S l = S i l ∪ S i+1 l ... ∪ S K 2 l . Different granularities affect both accuracy and computation as further explored in Sections 6 &amp; 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SYQ Training</head><p>In this section, we now describe the methodology to efficiently train SYQ networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Symmetric Quantizer</head><p>When training low-precision inference networks, the aim is to have the smallest possible codebook. Typically, as the codebook size increases, a network will approach fullprecision performance but increase hardware cost. However, there are certain codebook representations which are significantly more hardware friendly than others and won't necessarily impose any hardware costs. Given a codebook C, and the nonzero codebooks C p = c i |c i &gt; 0 and C n = c j |c j &lt; 0 , a quantizer is denoted as symmetric if:</p><formula xml:id="formula_13">∀c i ∈ C p , ∃ |c j | ∈ C n where c i = |c j |<label>(10)</label></formula><p>Learning this type of codebook requires updating one scaling coefficient during training for two bi-polar codebook values. The gradient of each scaling coefficient for each subgroup becomes:</p><formula xml:id="formula_14">∂E ∂α i l = j∈S i l ∂E ∂W li,j<label>(11)</label></formula><p>When computing binary/ternary weight representations followed by a scale, it is ideal to have a codebook which is symmetric around zero, as the codebook storage requirements are almost halved. This is because only the absolute value of the two symmetric values needs to be stored. Additionally, codebook indices become highly regular and ordered for the scalar multiply which greatly reduces computational complexity. The nature of symmetric quantization enables the opportunity to implement fine-grained quantization (pixel/row-wise) whilst maintaining the scalar matrix multiplication structure used in layer-wise scaling. This is also advantageous as the scaling coefficients become finegrained adaptive learning rate multipliers for each pixel/row in a CONV layer, i.e. the STE becomes:</p><formula xml:id="formula_15">∂E ∂W li,j = ∂E ∂Q li,j = α i l ∂ Ê ∂Q li,j<label>(12)</label></formula><p>As the use of scaling coefficients can more accurately approximate subgroups and are gradient-based, the gradient mismatch is significantly reduced for weight quantization which enhances network learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Initialization</head><p>The solution to non-convex optimizations such as gradient descent depend heavily on parameter initialization to avoid vanishing or exploding activations/gradients and ensure network convergence <ref type="bibr" target="#b8">[9]</ref>. For low-precision networks, excessive gradient mismatches between the forward and backward functions must be minimized, otherwise the gradients will not propagate well. To deal with this concern, the scaling coefficients coefficients are initialized as the mean of full precision weights in it's corresponding subgroup. For example, the scaling coefficient in pixel-wise scaling is:</p><formula xml:id="formula_16">α i l0 = j∈S i l W li,j I × N<label>(13)</label></formula><p>Layer-wise scaling in FC layers has α l0 as the mean of all layer weights. By incorporating information from the full precision weights, we aim to reduce the mismatch initially and the scaling coefficients are then optimized during backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Activations Quantization</head><p>Our forward path approximation to g in ( <ref type="formula" target="#formula_0">1</ref>) uniformly quantizes a real number x ∈ [0, M ] to a k-bit number:</p><formula xml:id="formula_17">G(x) = 1 2 f f loor((2 f )x + 1 2 ) (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where f loor represents the round down operation and M is the upper bound. M itself is bounded by its arbitrary unsigned two's complement fixed point representation where f is the number of fractional bits and</p><formula xml:id="formula_19">M = 2 k−f − 2 −f .</formula><p>Uniform quantization translates to a reduction in hardware implementation complexity. To achieve this, we use the following STE for the activations: SYQ Forward: for l=1 to L do Q l = sign(W l ) ⊙ M l with η, using ( <ref type="formula">3</ref>) &amp; ( <ref type="formula" target="#formula_3">4</ref>) for ith subgroup in lth layer do Apply α i l to S i l end for end for Ŷ = SYQForward (I, Y, Q l , α l ) using ( <ref type="formula" target="#formula_17">14</ref>) SYQ Backward: <ref type="formula" target="#formula_15">12</ref>) &amp; ( <ref type="formula">15</ref>)</p><formula xml:id="formula_20">∂E ∂x = ∂E ∂G<label>(</label></formula><formula xml:id="formula_21">∂ Ê ∂Q l = WeightBackward(Q l , α l , ∂ Ê ∂ Ŷ ) using (</formula><formula xml:id="formula_22">∂ Ê ∂α l = ScalarBackward( ∂ Ê ∂Q l , α l , ∂ Ê ∂ Ŷ ) using (11) W t+1 = UpdateWeights(W t , ∂ Ê ∂Q l , γ) α t+1 = UpdateScalars(α t , ∂ Ê ∂α l , γ) γ t+1 = UpdateLearningRate(γ t , t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To demonstrate the versatility of SYQ, we applied it to several state-of-the-art benchmark models, all with different network topologies. We use binary/ternary weights and varying activation bitwidths for classification of the largescale ImageNet dataset. The ILSVRC-2012 ImageNet is a natural high resolution visual classification dataset consisting of 1000 classes, 1.28 million training images and 50K validation images. Inputs are resized to 256 × 256 before being randomly cropped to 224×224. We report our singlecrop evaluation results using Top-1 and Top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Networks</head><p>We compare our results to the full precision baseline and benchmark reference model accuracies in Table <ref type="table" target="#tab_1">1</ref> <ref type="foot" target="#foot_0">2</ref> , showing that SYQ training achieves similar accuracy to floating point. This suggests the noise induced from replacing floating point weight layers with SYQ versions, provides effective regularization during training. An AlexNet <ref type="bibr" target="#b17">[18]</ref> variant is implemented which eliminates dropout and includes batch normalization <ref type="bibr" target="#b15">[16]</ref>. A mini batch size of 64 is used, L2 weight decay of 5e-6, and our learning rate is initially 1e-4 with step decays of scale factor 0.2. For ResNet <ref type="bibr" target="#b12">[13]</ref>, we test on the 18, 34 and 50 layer variations. Our batch size is 128, learning rate is initially 1e-3 with step decay of Row-wise Layer-wise Weights Act. Top-1 Top-5 Top-1 Top-5</p><formula xml:id="formula_23">1 2 -0.7 -0.5 -1.4 -2.2 1 8 -0.1 -0.3 -0.4 -2.2 2 2 +0.1 -0.0 -1.3 -1.5 2 8 -0.1 -0.1 -1.9 -1.7</formula><p>factor 0.2. We also test on a variant of VGG-16 <ref type="bibr" target="#b26">[27]</ref>, using model-A in <ref type="bibr" target="#b13">[14]</ref> with the spp layer replaced by a max pool and only 3 CONV layers rather than 5 for input size blocks of 56, 28 and 14, as in <ref type="bibr" target="#b1">[2]</ref>. Batch sizes are set to 32 and our learning rate is initially 1e-4 with a step decay of factor 0.2. The VGG and ResNet models were initialized from floating point baseline weights. Full-precision weights are used for the first and last layer. All other CONV layers are quantized with SYQ pixel-wise scaling, FC layers with layerwise scaling and the activations of all layers using (14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Changing Granularity Via Weight Subgroups</head><p>Weight subgroups can be arbitrarily designed for a given hardware application. Table <ref type="table" target="#tab_2">2</ref> shows accuracy differences between using row/layer-wise vs pixel-wise scaling on AlexNet and suggests pixel-wise and row-wise are marginally different, especially for higher precisions, but both are considerably more accurate than layer-wise. This demonstrates the effectiveness of fine-grained quantization of CONV layers over layer-wise and promotes the exploration for efficient representations of scalar computation. It also shows the effectiveness of row-wise quantization as it typically incurs a smaller memory requirement with a small accuracy drop, for a significant gain in the potential parallelism of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparisons To Previous Work</head><p>We compare SYQ explicitly using AlexNet, ResNet-18 and ResNet-50 in Tables <ref type="table" target="#tab_4">3, 4</ref> &amp; 5 as they've been extensively studied in the literature. Our ternary results with 8 bit activations (2w-8act) improves on the state-of-the-art for all </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Weights Act. Top-1 Top-5 DoReFa-Net <ref type="bibr" target="#b32">[33]</ref>   three networks. Our 2w-4act for ResNet-50 also improves on the state-of-the-art FGQ. This is also the case for binary weights, such as 1w-8act ResNet-18 and AlexNet with 1w-2/4act. For extremely low 1w-2act representations, SYQ also has a 2.7% increase in Top-1 accuracy over the stateof-the-art HWGQ. This demonstrates SYQ's superiority for producing high accuracy. Additionally, it shows that multiple learnable scaling coefficients effectively reduce the gradient mismatch in the forward and backward paths, translating to efficient learning under low-precision constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Varying Activation Bitwidth</head><p>The most important result is that SYQ efficiently quantizes networks with low-precisions for both weights and activations. From Figure <ref type="figure" target="#fig_2">3</ref>, we can see that lowering the precision of the activations does not severely alter the training curve, suggesting that the gradient information from pixel-wise scaling coefficients in SYQ compensates well  for the loss of information. However, when quantizing down to 2-bits, the training error curve does become more volatile, demonstrating instabilities in network learning. We also report the classification accuracies for varying activations and bitwidths on AlexNet and ResNet-50 in Tables <ref type="table" target="#tab_5">3  &amp; 5</ref>, which shows that there is minimal discrepancy from the full-precision networks with as low as 4-bit activations. These results are extremely promising and have strong implications for specialized hardware implementations of lowpower DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Hardware Implications</head><p>In this section we discuss the computational implications of different scaling operations and present a design for specialized hardware implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Computational and Memory Complexity</head><p>Considering a CONV layer with Ops, P = K × K × I × N × F × F , where F is the IFM dimension. The layer-wise scaling, as in DoReFa-Net, requires one scaling coefficient per P operations. For channel-wise scaling in HWGQ and BWN, it requires N scaling coefficients as there is one per output feature map, where typically N ≫ 1. TTQ implements asymmetric layer-wise quantization which requires two scaling coefficients per layer and P + Z operations as we add a branching operation for each weight due to irregular codebook indices, as described in Section 3.3. FGQ uses pixel-wise scaling for every 4 filters, whereas SYQ uses pixel-wise scaling per N filters, hence it requires K 2 N/4 scaling coefficients and P operations. For pixel-wise SYQ scaling, K 2 scaling coefficients and P operations are required, where K = 3 for most CONV layers in modern networks. For row-wise SYQ scaling it requires K scaling coefficients and P operations. These results are displayed in Table <ref type="table" target="#tab_6">6</ref>, demonstrating the benefits of maintaining a diagonal representation for the scalar matrix multiplication of each layer as we either improve computational or memory complexity against all other fine-grained methods. Another key benefit of SYQ is its amenability to highly parallel processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Architectural Design</head><p>For the CONV layer, the operations are a sum of dot products between the input and kernel filter. In order to reduce compute complexity, we increase the number of operations in each dot product, while significantly decreasing the complexity of each operation. For example, the size of the input vector, in the calculation of each dot product is:</p><formula xml:id="formula_24">L v = K 2 I.</formula><p>The number of operations is Op L mul = L v for multiplies and Op L add = L v − 1 for additions. Given we have a limited codebook for our weights, we can break it into sub-dot products where we apply the scaling factor, α i , after we have computed the sub-dot product for that set of symmetrically constrained weights. For pixel-wise quantization, the total multiplies becomes Op P mul = L v + K 2 and the total adds become Op P add = K 2 (L v /K 2 − 1) + (K 2 − 1) = L v − 1. However, the first term in each of these calculations can be done at significantly lower precision. For multiplies this means a binary or ternary multiple -which can often be implemented as a bit-flip. To compute this in specialized hardware, for layer-wise scaling, we have a parallel MAC tree which consists of a multiply of an input and binary/ternary number (represented as a dot) followed by an adder tree to sum up the outputs. Outputs of these are fed into a multiplier to compute the scale, followed by an accumulator to store the outputs before being fed into the activation function. This architecture is shown in Figure <ref type="figure" target="#fig_3">4</ref>. For every hardware block of this type, our per-pixel/row scaling only requires one additional ring counter which stores scaling coefficients and shifts the input to the scaling multiplier through an index counter as each row/pixel is finished computing which is computationally inexpensive. As in the equivalent layer-wise scaling architecture, we can still maintain one multiplier in hardware and only increase memory slightly to store the scaling coefficients. Table <ref type="table" target="#tab_7">7</ref> shows the resource and performance estimates provided by Vivado HLS of the described hardware architecture for a target Xilinx ZU3 FPGA device at an estimated clock frequency of over 300 MHz. The main design is based on the MVTU described in FINN <ref type="bibr" target="#b28">[29]</ref>, with an extension to 2-bit activations and pixel-wise and row-wise  SYQ. The layer-wise baseline uses no multiplies, as these can absorb into quantization thresholds for activations <ref type="bibr" target="#b28">[29]</ref>.</p><p>The MVTU was configured for a convolution layer with I = 384, N = 256, K = 3, while scaling the size of the MAC tree (SIMD) and the number of parallel processors (PE). As shown, the BRAM (memory blocks on an FPGA (18k)) and LUT usage is almost identical, while the DSP usage increases proportionally with the number of parallel output channels which are processed. The increase in DSPs is not necessarily costly for the ZU3 as we are able to utilize more of the total available resources. Resource usage is only shown for pixel-wise SYQ, as row-wise only differed in LUT usage by less than 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>The problem of efficiently training large DNNs with lowprecision weights and activations is considered. We propose learning symmetric quantization for DNNs in order to maximize network learning whilst minimizing hardware complexity. This was achieved by constraining the solution to low-precision representations and learning a diagonal scalar matrix using gradient-based optimizations for efficient computation. As a result, we reduce the computational requirements of fine-grained quantization and achieve state-of-the-art accuracies on modern benchmark networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The average cost per MAC operation on an FPGA device for different bitwidths (weight-activation)</figDesc><graphic url="image-1.png" coords="3,50.11,54.07,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Computational structure of pixel-wise (Left) and row-wise (Right) subgrouping of a CONV layer (K, I = 3). The tensors represent the weight layer structure during training and the matrices represent the matrix decomposition for deployment.</figDesc><graphic url="image-2.png" coords="5,58.77,72.00,244.80,111.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Top-1 training and validation error for binary AlexNet with varying activation precisions</figDesc><graphic url="image-4.png" coords="7,326.19,72.00,201.60,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Hardware description of MAC for SYQ layers</figDesc><graphic url="image-5.png" coords="8,318.99,72.00,216.00,116.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 SYQ Training Summary For DNNs. Initialize: Set subgrouping granularity for S i l and set α i l0 . Inputs: Minibatch of inputs &amp; targets (I, Y ), Error function E(Y, Ŷ ), current weights W t and learning rate, γ t Outputs: Updated W t+1 , α t+1 and γ t+1</figDesc><table /><note><ref type="bibr" target="#b14">15)</ref> Differences in the forward and backward activation functions create a gradient mismatch which can result in unstable and inefficient learning. To minimize this issue, we adjust M as a hyperparameter. The overall SYQ training process is summarized in Algorithm 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of Results for 8-bit activations and binary (1-8) and ternary (2-8) weights</figDesc><table><row><cell>Model</cell><cell>1-8</cell><cell cols="2">2-8 Baseline Reference</cell></row><row><cell>AlexNet</cell><cell cols="2">Top-1 56.6 58.1 56.6 Top-5 79.4 80.8 80.2</cell><cell>57.1 80.2</cell></row><row><cell>VGG</cell><cell cols="2">Top-1 66.2 68.7 69.4 Top-5 87.0 88.5 89.1</cell><cell>--</cell></row><row><cell>ResNet-18</cell><cell cols="2">Top-1 62.9 67.7 69.1 Top-5 84.6 87.8 89.0</cell><cell>69.6 89.2</cell></row><row><cell>ResNet-34</cell><cell cols="2">Top-1 67.0 70.8 71.3 Top-5 87.6 89.8 89.1</cell><cell>73.3 91.3</cell></row><row><cell>ResNet-50</cell><cell cols="2">Top-1 70.6 72.3 76.0 Top-5 89.6 90.9 93.0</cell><cell>76.0 93.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>AlexNet accuracy differences between using row/layerwise and pixel-wise symmetric quantization</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison to previously published AlexNet results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison to previously published ResNet-18 results</figDesc><table><row><cell></cell><cell>1</cell><cell>2</cell><cell>49.8</cell><cell>-</cell></row><row><cell>QNN [15]</cell><cell>1</cell><cell>2</cell><cell>51.0</cell><cell>73.7</cell></row><row><cell>HWGQ [2]</cell><cell>1</cell><cell>2</cell><cell>52.7</cell><cell>76.3</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>2</cell><cell>55.4</cell><cell>78.6</cell></row><row><cell cols="2">DoReFa-Net [33] 1</cell><cell>4</cell><cell>53.0</cell><cell>-</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>4</cell><cell>56.2</cell><cell>79.4</cell></row><row><cell>BWN [24]</cell><cell>1</cell><cell>32</cell><cell>56.8</cell><cell>79.4</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>8</cell><cell>56.6</cell><cell>79.4</cell></row><row><cell>SYQ</cell><cell>2</cell><cell>2</cell><cell>55.8</cell><cell>79.2</cell></row><row><cell>FGQ [21]</cell><cell>2</cell><cell>8</cell><cell cols="2">49.04 -</cell></row><row><cell>TTQ [34]</cell><cell>2</cell><cell>32</cell><cell>57.5</cell><cell>79.7</cell></row><row><cell>SYQ</cell><cell>2</cell><cell>8</cell><cell>58.1</cell><cell>80.8</cell></row><row><cell>Model</cell><cell cols="4">Weights Act. Top-1 Top-5</cell></row><row><cell cols="2">BWN [24] 1</cell><cell>32</cell><cell>60.8</cell><cell>83.0</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>8</cell><cell>62.9</cell><cell>84.6</cell></row><row><cell cols="2">TWN [19] 2</cell><cell>32</cell><cell>65.3</cell><cell>86.2</cell></row><row><cell>INQ [32]</cell><cell>2</cell><cell>32</cell><cell>66.0</cell><cell>87.1</cell></row><row><cell>TTQ [34]</cell><cell>2</cell><cell>32</cell><cell>66.6</cell><cell>87.2</cell></row><row><cell>SYQ</cell><cell>2</cell><cell>8</cell><cell>67.7</cell><cell>87.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison to previously published ResNet-50 results</figDesc><table><row><cell>Model</cell><cell cols="4">Weights Act. Top-1 Top-5</cell></row><row><cell cols="2">HWGQ [2] 1</cell><cell>2</cell><cell>64.6</cell><cell>85.9</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>4</cell><cell>68.8</cell><cell>88.7</cell></row><row><cell>SYQ</cell><cell>1</cell><cell>8</cell><cell>70.6</cell><cell>89.6</cell></row><row><cell>FGQ [21]</cell><cell>2</cell><cell>4</cell><cell>68.4</cell><cell>-</cell></row><row><cell>SYQ</cell><cell>2</cell><cell>4</cell><cell>70.9</cell><cell>90.2</cell></row><row><cell>FGQ [21]</cell><cell>2</cell><cell>8</cell><cell>70.8</cell><cell>-</cell></row><row><cell>SYQ</cell><cell>2</cell><cell>8</cell><cell>72.3</cell><cell>90.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Number of scaling coefficients and operations per layer, for different techniques</figDesc><table><row><cell>Method</cell><cell>Scalars</cell><cell>Ops</cell></row><row><cell>Layer (DoReFa)</cell><cell>1</cell><cell>P</cell></row><row><cell>Row (SYQ)</cell><cell>K</cell><cell>P</cell></row><row><cell>Pixel (SYQ)</cell><cell>K 2</cell><cell>P</cell></row><row><cell>Asymmetric (TTQ)</cell><cell>2</cell><cell>P + Z</cell></row><row><cell>Grouping (FGQ)</cell><cell cols="2">K 2 N/4 P</cell></row><row><cell cols="2">Channel (HWGQ/BWN) N</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Resource Usage of a Matrix-Vector Processing Unit with Layer-wise and Pixel-wise Quantization for target Xilinx ZU3</figDesc><table><row><cell>Config</cell><cell cols="5">SIMD PE BRAMs LUTs (k) DSPs</cell></row><row><cell>Layer</cell><cell>32</cell><cell cols="2">32 64</cell><cell>29.8</cell><cell>4</cell></row><row><cell>Layer</cell><cell>64</cell><cell cols="2">32 64</cell><cell>56.5</cell><cell>4</cell></row><row><cell>Layer</cell><cell>32</cell><cell cols="2">64 64</cell><cell>58.9</cell><cell>4</cell></row><row><cell cols="2">SYQ(P) 32</cell><cell cols="2">32 64</cell><cell>29.4</cell><cell>36</cell></row><row><cell cols="2">SYQ(P) 64</cell><cell cols="2">32 64</cell><cell>56.1</cell><cell>36</cell></row><row><cell cols="2">SYQ(P) 32</cell><cell cols="2">64 64</cell><cell>57.7</cell><cell>68</cell></row><row><cell>ZU3</cell><cell>-</cell><cell>-</cell><cell>432</cell><cell>70.6</cell><cell>360</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Our ResNet and AlexNet reference results are obtained from https://github.com/facebook/fb.resnet.torch and https://github.com/BVLC/caffe, respectively</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partly supported under the Australian Research Councils Linkage Projects funding scheme (project number LP130101034) and Zomojo Pty Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>CoRR, abs/1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning with low precision by half-wave gaussian quantization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno>CoRR, abs/1702.00953</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1504.04788</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
		<idno>CoRR, abs/1103.0398</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<idno>CoRR, abs/1511.00363</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep binary descriptor with multi-quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compressing low precision deep neural networks using sparsity-induced regularization in ternary networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Faraone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gamberdella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
		</author>
		<idno>CoRR, abs/1709.06262</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scaling binarized neural networks reconfigurable logic</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Umuroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop and 6th Workshop on Parallel Programming and Run-Time Management Techniques for Many-core Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms, PARMA-DITAM &apos;17</title>
				<meeting>the 8th Workshop and 6th Workshop on Parallel Programming and Run-Time Management Techniques for Many-core Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms, PARMA-DITAM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</title>
				<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
				<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
		<respStmt>
			<orgName>JMLR.org</orgName>
		</respStmt>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">EIE: efficient inference engine on com</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>CoRR, abs/1602.01528</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>CoRR, abs/1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1609.07061</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
				<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ternary weight networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1605.04711</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural networks with few multiplications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1510.03009</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ternary neural networks with fine-grained quantization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno>CoRR, abs/1705.01462</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional neural networks using logarithmic data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
		<idno>CoRR, abs/1603.01025</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted-entropy-based quantization for deep neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>CoRR, abs/1603.05279</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Projectionnet: Learning efficient on-device deep networks using neural projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<idno>CoRR, abs/1708.00630</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How to train a compact binary neural network with high accuracy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">FINN: A framework for fast, scalable binarized neural network inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Umuroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Vissers</surname></persName>
		</author>
		<idno>CoRR, abs/1612.07119</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Accelerating deep convolutional networks using low-precision and sparsity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<idno>CoRR, abs/1610.00324</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Journal of Computer Vision</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with lowprecision weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1702.03044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno>CoRR, abs/1606.06160</idno>
		<title level="m">Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Trained ternary quantization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>CoRR, abs/1612.01064</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
