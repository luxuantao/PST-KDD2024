<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3-D Motion Estimation in Model-Based Facial Image Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Haibo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pertti</forename><surname>Roivainen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Forchheimer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Stucki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Roivainen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Forchheimer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linkoping University</orgName>
								<address>
									<settlement>Linkoping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3-D Motion Estimation in Model-Based Facial Image Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CD361866707331D5A7ACDE5704BA8B9</idno>
					<note type="submission">received October 10, 1991; revised December 1, 1992.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive prediction</term>
					<term>analysis-synthesis technique</term>
					<term>computer graphics</term>
					<term>computer vision</term>
					<term>facial image coding</term>
					<term>feedback technique</term>
					<term>model-based image coding</term>
					<term>motion tracking</term>
					<term>nonrigid motion estimation</term>
					<term>3-D modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the issue of 3-D motion estimation in model-based facial image coding. A new approach to estimating the motion of the head and the facial expressions is presented and has the following characteristics: 1) An affine nonrigid motion model is set up. The specific knowledge about facial shape and facial expression is formulated by this model in the form of parameters. This affine motion model is especially suitable to such a type of nonrigid motion as facial expressions.</p><p>2) Based on the affine model, we present a direct method of estimating the two-view motion parameters. Because this method neither necessitates solving the correspondence problem nor computing optical flow, motion parameters can be simply and reliably recovered. 3) Based on the reasonable assumption that the 3-D motion of the face is almost smooth in the time domain, we propose several approaches to predicting the motion of the next frame. In this way, the temporal motion information existing in the image sequence is fully exploited. With a good motion predictor the error arising from the treatment of motion by a linear method will be reduced. 4) Using a 3-D model, the new approach is characterized by a feedback loop connecting computer vision and computer graphics. Embedding the synthesis techniques into the analysis phase greatly improves the performance of motion estimation. Our simulations and experiments with long image sequences of real-world scenes indicate that the method developed in this paper not only greatly reduces computational complexity but also substantially improves estimation accuracy. The synthesized image sequence using the estimated motion parameters, a 3-D model of the face, and a frame of textured image looks very natural.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>head model is available. Because an explicit head model is not necessary, the second scheme can be applied to a more general class of objects. For a restricted scene, e.g., a typical headand-shoulder scene, the first scheme can give higher coding efficiency. The following discussion will only cover a facial image coding scheme in which an explicit facial model is available.</p><p>A block diagram of a 3-D model-based facial image coding scheme with an explicit facial model is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We assume that the transmitter and the receiver both possess the same 3-D facial model and texture image at the beginning of a visual communication session. During the session, it is required, at the transmitting side, to extract the facial motion parameters that contain two parts: The first part consists of the global motion parameters (the six degrees of freedom needed for describing rotation and translation of the head, and the second part consists of the local motion parameters describing the facial expression. Then, at the receiving side, the image is synthesized using the estimated motion parameters. Clearly, the performance of the 3-D motion estimation will directly determine the motion faithfulness of the synthesized image. Therefore, the 3-D motion estimation plays a very important role in model-based image coding <ref type="bibr">[ 121-[ 141.</ref> Motion estimation in the context of model-based image coding has been treated by many researchers <ref type="bibr">[13]</ref>, <ref type="bibr">[17]</ref>, <ref type="bibr">[18]</ref>. Earlier work on our part is described in <ref type="bibr">[12]</ref> and <ref type="bibr">[13]</ref>. Here, it is suggested that the motion parameters should be divided into two parts: global motion parameters such as rotation and translation of the head and local motion parameters such as facial expressions (action units). In the treatment of global motion, it is argued that without accurate information about the global movements, it is difficult to resolve local motion. To estimate local motion, we made an important hypothesis about it, namely, that any facial expression can be viewed as a weighted linear combination of a set of typical action units. This insight deeply influences the subsequent local motion estimation method, especially the work presented here. <ref type="bibr">Aizawa and Harashima in [18]</ref> present a method of estimating the 3-D motion parameters and depth information of the head. Their method performs iterative correction of the depth of the head. Without taking local motion into account, the global motion obtained will only be an approximation of the real value. <ref type="bibr">Welsh [17]</ref> gave a least-squares method to estimate motion parameters. Similar to <ref type="bibr">[18]</ref>, his method is only used to estimate global motion parameters. Roivainen and Forchheimer have studied the motion estimation problem in detail <ref type="bibr">[15]</ref>. They were first to present a new approach that can jointly estimate both global and local motion parameters. Inspired by the idea of Kalman filtering, they adopted an iterative style that can overcome the error-accumulation problem in sequential estimation. When motion between frames is not too large, this algorithm can give better tracking performance. However, when the tracking does not work well, deformation will occur in the synthesized face. A further drawback of this approach is that the computation of the displacement field is time consuming because of the correspondence problem that has to be solved. Another important work on the estimation of 3-D motion of rigid bodies was done by <ref type="bibr">Netravali and Salz in 1985 [16]</ref>. They may be considered to be the first to estimate 3-D rigid motion directly from the intensity values of the image. Although their work only handles rigid motion, which will limit its direct application in model-based image coding, they indeed provide a method for rapid computation of 3-D motion, which has special appeal in real-time applications. Works closer to our own, namely, direct motion estimation from grey-level images and analysis by synthesis techniques, have been used in object-oriented model-based coding [ 191, <ref type="bibr" target="#b30">[33]</ref>, <ref type="bibr" target="#b31">[34]</ref>. Although the primary idea of these techniques is similar to what we will describe, the implementation is very different. In <ref type="bibr">[19]</ref>, the motion of 3-D nonrigid objects is hierarchically estimated from image intensities by subdividing the nonrigid objects into quasi-rigid parts. This is in contrast with the approach to be described here, where both the rigid motion of the head and the nonrigid motion of the facial expression are estimated simultaneously, directly from image intensities. In our motion tracking system, the image synthesis module is embedded into the image analysis part in order to provide a feedback loop for the predicted motion parameters.</p><p>As already noted, this approach is directly influenced by the Kalman filtering technique.</p><p>In this contribution, we further develop our previous work on this topic in the following three directions: the first is that we set up an affine nonrigid motion model in which the knowledge about the facial shape and expression is contained in the form of parameters. With this model, we derive a direct and robust approach to obtain motion parameters. The second is that we introduce motion prediction. In this way, the temporal motion information, besides the spatial motion information, is also used. Finally, by means of a 3-D model, we build a feedback system using a combination of computer vision and computer graphics. The aid of computer graphics greatly improves the performance of the vision algorithm.</p><p>These developments enable us to obtain reasonable motion parameters.</p><p>This paper is organized as follows: After the Introduction, we first discuss the 3-D motion model of the face in Section 11. In Section 111, we develop a direct method to estimate twoview local motion that neither uses optical flow nor relies on solving the correspondence problem. Motion prediction and sequential motion estimation are dealt with in Section IV. In this Section, our emphasis will be on how to exploit temporal redundancy in the available image sequence and how to combine computer vision and computer graphics parts. The subsequent section demonstrates some experimental results on a real image sequence. Finally, the paper closes with concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">3-D MOTION MODEL OF A FACE</head><p>The estimation of 3-D motion of a face is a difficult problem. The difficulty arises partly from the lack of a flexible motion model that can characterize the nonrigid nature of the 3-D motion of a human face. Therefore, the first step in estimating 3-D motion is to build such a 3-D motion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nonrigid Motion Model</head><p>During a visual communication session, the nonrigid motion occurring in the face can be viewed as an affine mapping.</p><p>Assume a point p in the face is represented by a vector s = ( ~, y , z ) ~. After facial movement, it moves to a new position s ' = (z', y', z ' ) ~; the change in position of this point can be written as follows according to the Helmholtz theory on nonrigid motion <ref type="bibr">[3]</ref> where R is the rotation matrix. T is the translation matrix, and D is a deformation matrix that represents the deformation caused by facial expressions.</p><p>Unfortunately, although the mathematical form of the above motion model is simple, it is impossible to directly use the model (1) to estimate 3-D motion of a human face. This is because R,T, and D are, in general, point dependent.</p><p>Fortunately, face motion is a special type of nonrigid motion. It can be viewed as mainly rigid motion plus a slight nonrigid motion. Therefore, R and T can be considered as global motion parameters, which become point independent. They can be described by six unknown parameters. It is worth noticing that the deformation matrix also contains nine unknown parameters, which are point oriented. We should keep in mind, however, that the local nonrigid motion is caused by the facial expression. This implies that the motion of any point in a face is not free or independent; it is constrained by muscle and skin. If these inherent constraints are utilized, the number of unknown motion parameters will be dramatically reduced.</p><p>In order to find these inherent constraints, we must consider two aspects: The first is which points will influence a certain point, and the second is how much will it be influenced. To answer the two questions, we need a face model in which the position and action of points are given in parametric form. We will introduce such a parameterized face in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Parameterized Face Model: The Candide Model</head><p>The Candide model is a parameterized face model that contains both facial shape and facial expression information <ref type="bibr">[21]</ref>. The Candide model was first developed at Linkoping University by Rydfalk for the purpose of model-based image coding and computer animation. The model contains a full 3-D description of the face object as well as parameters for controling expressions. The 3-D shape is directly recorded in coordinate form. The facial expressions are described by "action units," which are based on the facial action coding system (FACS) by <ref type="bibr">Ekman and Friesen [23]</ref>. The action unit (AV) stands for a small change in the facial expression that is dependent on a small conscious activation of muscles <ref type="bibr">[21]</ref>. Obviously, the AU is a kind of knowledge that is expressed in parameter form. The Candide model is now widely used by groups around the world in their studies of model-based image coding.</p><p>Because of the purpose of real-time animation, the size of the Candide model is limited to only 100 triangles. Although synthesized images using the Candide model are quite good, the model is not capable of reproducing completely realistic images, especially in the mouth part. In order to raise the quality of the synthesized image, the Candide model was extented by <ref type="bibr">Welsh [17]</ref>. Further changes on the neck part were made within the framework of our study. The current version of Candide is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Facial Expression Movement Model</head><p>By observing real facial image sequences, it is seen that the assumption that any facial expression can be analyzed into a weighted linear combination of a set of typical AU's is reasonable <ref type="bibr">[13], [15]</ref>. This important assumption implies that facial expression can be compactly described with relatively few local motion parameters. Making use of the Candide model, the facial expression can be described by</p><formula xml:id="formula_0">Ds = E@ (2)</formula><p>where m facial expression movement parameters have been collected in the vector @ = ( + 1 , + 2 , . . . + ~) ~. These real valued movement parameters are computed from the larger set of AU's described in <ref type="bibr">[21]</ref>. The 3 x m matrix E determines how a certain point s is affected by @.</p><formula xml:id="formula_1">ell e12 . ' . elm [ e31 e32 e3m E = e21 e22 ...<label>(3)</label></formula><p>Equation (2) gives a good example of how to handle nonrigid motion. Making full use of the knowledge about the motion of the nonrigid object, we can induce the inherent constraints. With these constraints, we can reduce the freedom of the deformation coefficients and limit the solution space. This makes it possible to find a unique solution.</p><p>In the real visual communication case, considering that the frame rate is relatively high with respect to the motion of the face, rigid motion of the head can be viewed as an incremental rigid motion [l], <ref type="bibr">[16]</ref>. In this case, the motion model ( <ref type="formula">1</ref>) can be approximately represented as follows:</p><formula xml:id="formula_2">m 2' = 2 + eii+i + R z y -Ryz + Tx i=l m 9' = y + C e z i d i -+ R X Z +Ty i=l m 2' = z + C e 3 i + i + R,x -Oxy + Tz (4) i=l</formula><p>where R,, R,, and Rz are angular velocities about the 2, y, and z axes, respectively. Tx, T,, and T, are the three velocity components of the translation motion. z = dzo. d is the distance between the point p and the camera. zo is the depth value of the point p, which is given by the Candide model. Equation (4) can be written into a more compact form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s ' = s + M U (5)</head><p>where M will be called the model matrix whose coefficients are functions of e i j and the position of points in 3-D space. U is the motion vector that contains both the global motion vector and local motion vector, that is, U = Formula (5) is the affine nonrigid motion model we use for facial movement analysis and synthesis.</p><p>In (4), eij and zo come from the expression and shape information of the 3-D Candide model, which illustrates the role of the model in the image analysis stage.</p><p>[ $ I , $27 . * * 1 $mlRx, R y 7 Rz, Tx, Ty, TzIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">TWO-VIEW MOTION ESTIMATION</head><p>Estimation of 3-D motion from two successive frames has a long history. The existing literature on this subject can be roughly divided into two groups [6]: The first concerns "exact methods" [l] and the second the "approximation methods" <ref type="bibr">[2]</ref>. The choice of approach depends on the problem at hand. In our application, we will use an approximation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3-0 Motion Projection Model</head><p>It is assumed that the geometry projection from the 3-D space onto the 2-D image plane is the perspective projection and the focus length of the camera f = 1; then, the optical flow field ( U , w) induced by the 3-D motion of the face is <ref type="bibr" target="#b29">[32]</ref> </p><formula xml:id="formula_3">U = -C(e1i -~e 3 i ) + i + XYR, -(1 + x ~) R , l m i=] m+6 T z Tx + YR, -x -+ -= CUiUi z z i=l 1 " w = -C ( e 2 i -~e 3 ; ) 4 i + (1 + Y ~) R , -XYR, i=l m+6 (7) Tz -X R , -YT + 2 = c,;ua i=l</formula><p>where X,Y are the coordinates of the image plane.</p><p>Note that the depth z can be obtained from the Candide model. Since the depth z is known, the recovery of 3-D motion from ( U , v) is clearly a linear problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3 -0 Motion Estimation 1) Motion Estimation from Displacement Field:</head><p>Assuming a set of (ui, w i ) , i = 1,2, . . . , I is available, ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>) can be written in a more compact form CU = V, where V = [ u l , v1, 212,212, . . .IT, and C is a matrix that is made up of cui and cui.</p><p>Using the standard LS algorithm, we can obtain the motion parameter U as</p><formula xml:id="formula_4">U = (cTc)-lc*v (8)</formula><p>Equation ( <ref type="formula">8</ref>) shows that once the displacement vector V is obtained, the 3-D motion can be directly recovered.</p><p>2) The Direct Computation Approach: Inspired by the works of [ll], we present, in this section, a new method of estimating both the global motion parameters and the local motion parameters of the face directly from the intensity of the image. This method is based on "the differential approach," and no "correspondence problem" situation has to be resolved. This makes the method computationally fast. Because it avoids the recovery of optical flow, no constraints need to be imposed. Thus, it can provide more accurate motion information than an optical flow-based method. Meanwhile, because the depth information can be obtained from the Candide model, it is possible to estimate the motion parameters using an aggregation of data. This approach will thus be very robust to noise. Now, we discuss the direct motion estimation in the following two cases:</p><p>Small Motion Case: We assume that the intensity of a moving point remains constant, in which case, we have the normal optical flow constraint equation <ref type="bibr">[2]</ref> IXU + Iyw + It = 0 <ref type="bibr">(9)</ref> where I, = % , I y = $$, and It = E.</p><p>Inserting ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>) into (9) yields . We can thus estimate the motion parameters using the LS algorithm to yield</p><formula xml:id="formula_5">U = -(GTG)-lGTIt. (12)</formula><p>Large Motion Case: In some situations, the displacement ( u , ~) may be large, in which case, there exists a large linearizing error in (9). Clearly, that will affect the accuracy of the displacement estimation. In order to handle larger motion and motivated by the work of <ref type="bibr">[16]</ref>, we use the following prediction scheme.</p><p>Assume that the actual displacement is ( U , v). If we use 3-D motion parameters obtained in the previous frame to predict the present 3-D motion parameters, then we have the predicted value of the actual displacement m Tz Tx</p><formula xml:id="formula_6">-(1 + X 2 ) A , + YA* -x -+ -(13) z z m * 1 v = -C ( e 2 i -~e 3 i &gt; &amp; + (1 + y2)fiX Tz T, -X Y A , -xfiz -Y -+ -. i=l (14) z z</formula><p>Considering the fact that, in most cases, the motion smoothness assumption is valid, the absolute value of (U -fi) will be smaller than that of U. A similar result will hold for w.</p><p>Now, let us derive the displacement estimation algorithm.</p><p>Based on the assumption I ( . + U, y + w, t + 1) = I(%, y, t),</p><p>we have <ref type="bibr" target="#b29">[32]</ref> I ( x + u , y + v,t + 1) -I ( x , y , t ) w I x ( z + e, y + 6, t + 1)(u -G) + I Y ( z + G, y + 6 , t + l ) ( ~ -6) + A I = 0 (15) where A I = I ( z + G, y + 6, t + 1) -I(z, y, t), and the approximation is obtained as the first term in the Taylor expansion.</p><p>If we let i = I ( z + G, y + 6, t + l ) , then ( <ref type="formula">15</ref>) is simplified to j X u + j y u + A Ij x G -Iy6 = 0.</p><p>(16) Denoting it = A Ijxii -jYG, we have the optical flow Note that the difference between algorithm (18) and algorithm (12) appears to be just a change in notation, but actually, algorithm (18) contains the prediction information of the motion. Therefore, algorithm ( <ref type="formula">18</ref>) is able to handle a larger motion range than algorithm (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LONG SEQUENCE MOTION TRACKING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motion Estimation Framework</head><p>Three-dimensional motion estimation in model-based image coding is a long sequence motion tracking problem. We have to dynamically provide the motion parameters. 'Avo-view motion estimation provides only the relative motion between these two frames. The absolute motion is given in the following way: U;+l x U; + AU; <ref type="bibr">(19)</ref> where U;,U;+l are the absolute motion at time instant i and i + 1, respectively. AU; is the two-view motion that can be computed by ( <ref type="formula">12</ref>) or (18).</p><p>The motion parameters obtained from the two-view case are inherently noisy. Obviously, with the increase in number of frames, the error of the estimated motion parameters will be accumulated, which will result in divergence in the motion estimation. This is the so-called "error accumulation" problem associated with the long sequence motion tracking [14], <ref type="bibr">[15]</ref>. There are many works that have addressed this problem. These include the pioneer works of Brodia and Chellappa [lo] and recent works of Pentland et al. <ref type="bibr">[31]</ref>. The most basic strategy of these works lies in the combination of local techniques with Kalman filtering techniques. However, in our own application, it is not suitable to directly adopt Kalman filtering <ref type="bibr" target="#b29">[32]</ref>.</p><p>Inspired, however, by the idea of Kalman filtering, we propose a motion tracking system that uses a "prediction-andcorrection" strategy to handle the motion tracking problem. In this motion tracking system, there are four modules; the "motion prediction module," "two-view motion estimation module," the "image synthesis module," and the "motion correction module." The total system block diagram is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. From the block diagram, we find that this is a closed-loop feedback system. The feedback mechanism is a powerful tool for handling error accumulation. Now, let us examine how this system works. Assume that we want to estimate the 3-D motion parameters U of the current frame I(., y, t + 1). Using motion parameters of the previous frames, we obtain predicted motion parameters U of the actual motion parameters U by the motion prediction module. Due to the unmeasurability of the 3-D motion parameters, it is impossible to directly compare the actual motion parameters and their predicted value. However, image intensity provides us with an indirect measurement of the 3-D motion parameters. Therefore, in order to evaluate the closeness of the actual motion parameters and the predicted motion parameters, we synthesize an image frame I ( z , y, t) by the motion synthesis module using the predicted motion parameters. If the predicted motion parameters are equal to the actual motion parameters, then I(z, y, t + 1) = I ( z , y, t ) , disregarding for the moment optical tffects such as shading. If there is a difference between U and U, then I(z, y, t + 1) # I(z, y, t). In this case, we can use the two-view motion estimation module to estimate the relative motion 6U between the synthesized image I ( z , y, t) and the original image I ( z , y, t + l ) . With the predicted motion parameters U and the motion error 6U, we can obtain an accurate estimation of the actual motion parameters using the motion correction module.</p><p>The greatest advantage of this system is that it can provide satisfactory quality of a synthesized image due to the following three reasons: The first is that error accumulation is suppressed due to the use of the feedback loop; the second is that the criterion is directly set up between the synthesized image and the original one; the last is that motion prediction makes the error due to the linearity assumption smaller.</p><p>In the following section, we will discuss the individual modules of the motion tracking system in detail. The twoview motion estimation module has already been discussed in Section 111.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Prediction 1) Why predict?:</head><p>In the derivation of two-view motion estimation, we make an implicit assumption that the 3-D motion between successive frames is small. Based on this assumption, we can use the "approximation approach" to reduce the nonlinear problem to a linear one. For a real image sequence, however, larger 3-D motion may occur in successive frames, which will result in a degradation in the two-view motion estimation. One good approach to reduce this error is motion prediction since we believe that the difference between the real 3-D motion and its prediction is smaller than the real 3-D motion. Therefore, from the viewpoint of reducing two-view motion estimation error, motion prediction is necessary.</p><p>In the two-view motion estimation case, we only utilize spatial motion information. Since visual communication is a kind of dynamic behavior and 3-D motion parameters have to be estimated continuously, we should exploit the temporal motion information existing in the image sequence. In fact, it is the temporal redundancy in the available image sequence that provides a possibility to make motion prediction.</p><p>The motion of the human face is controled by the force of muscles. In addition, due to inertia, it is difficult for the head to change its motion instantaneously. Therefore, 3-D motion of a face can be considered to be smooth in the time domain. Although a face may change its motion abruptly in some extreme cases, we still consider the smoothness assumption to be reasonable [8]. This is because the assumption is almost always satisfied, and even if the constraint is violated at some instant, this will only affect the prediction of the 3-D motion, the effect of which will be removed by the feedback loop. Therefore, the motion smoothness constraint provides a solid theoretical foundation for the motion prediction.</p><p>We assume that the 3-D motion of a face during the observation period is smooth enough so that it can be represented by a dynamic model of relatively low dimensionality [lo] k= I In (20), the motion is assumed to be smooth up to order n. According to the problem at hand, we decide how large a value of n is suitable. For example, for constant velocity, we have = 0, and for a linear change in acceleration, we have = 0. a) Linear Prediction: In this section, we first discuss how to achieve the required linear prediction from the known motion parameters of the previous frames according to the motion smoothness assumption.</p><p>If we use the following motion smoothness constraint = 0, then we have the following linear predictive equation: = 0, for constant acceleration, we have 2) Some Prediction Approaches:</p><formula xml:id="formula_7">U ( t ) = U ( t -1).</formula><p>If the motion smoothness assumption the linear predictive equation becomes</p><formula xml:id="formula_8">O ( t ) = 2U(t -1) -U ( t (21) is = 0, then -2 ) . (<label>22</label></formula><formula xml:id="formula_9">)</formula><p>This can be called "two-oint prediction." equation is 9</p><p>If the assumption is = 0, then the linear predictive</p><formula xml:id="formula_10">U ( t ) = 3U(t -1) -3U(t -2 ) + V ( t -3 ) . (<label>23</label></formula><formula xml:id="formula_11">)</formula><p>From the above observation, we find that with the increase in the order of linear prediction, the number of previous frames used in the prediction scheme will increase linearly.</p><p>b) Adaptive Linear Prediction: A great advantage of the above approach is its computational simplicity. It will also work well most of the time. However, when the assumption made mismatches the actual one or there is a sudden change in motion, then the performance will deteriorate.</p><p>In order to cope with these special cases, we can make the coefficients in the prediction equation adapt to the performance of the motion prediction instead of being fixed. In this way, motion prediction can adaptively track the real motion behavior. For a detailed discussion of adaptive linear prediction, refer to <ref type="bibr" target="#b29">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Synthesis</head><p>The function of the image synthesis module in our motion tracking system is to synthesize a facial image that is as faithful to the original as possible using the given motion parameters. Assuming the predicted motion parameters are U , the "Candide model" can be used to obtain the new 3-D </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Motion Correction</head><p>The function of the motion correction module is to correct the predicted value of the 3-D motion parameters according to the prediction error, that is, the output of the two-view motion estimation module. { &amp; I =. . = E@."""' + R""'E@Pre-.</p><p>Equation (24) is the motion correction formula.</p><p>In many practical applications, the prediction error U-"' may be far smaller than Updid. In such a case, the motion correction can be further simplified as follows <ref type="bibr" target="#b29">[32]</ref>:</p><formula xml:id="formula_12">fi -Oerror + Opredict T -Terror + Tpredict (25) { : 4 -p r r o r + 4predict</formula><p>This simplified motion correction method has been used by <ref type="bibr">Roivainen in [15]</ref>. It is worth pointing out that this formula should be used only when the prediction error U""' is far smaller than Updid. When the prediction error is not smaller, the motion correction is expected to be more efficient if (24) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We have studied the efficiency of the proposed approach using the "Claire" sequence. The sequence used consists of 32 frames, where each frame consists of 256 x 256 pixels.</p><p>The tirst step in the model-based image coding is to fit the Candide model to the face of the person who will be on visual communication. However, fitting a general 3-D model to a special face is a difficult problem. The fitting requires the precise location of the facial features. One attempt to deal with this problem is given in [17]. In our experiments, we have manually adjusted the position of the feature points so that the Candidc model fits the second frame.</p><p>For the depth of the facial model, we initially provide an approximate value. If this rough approximation is used, there will be an inherent error in the motion estimation. In order to reduce the inherent error, we have to correct the approximated depth. A good method used in our experiments is to correct the approximate depth by the position correction method 1151 using multiple image frames. Certainly, it is difficult to obtain a precise depth using this method. However, our preliminary experiments show that promising results can be expected, given a reasonable approximation of the depth.</p><p>We have designed two groups of experiments used for evaluating the performance of the two-view motion estimation and the long sequence motion tracking, respectively. These experiments are described in the following sections. Every second frame of this sequence is shown in Fig. <ref type="figure" target="#fig_6">4</ref>. The second frame is used as the texture-mapping image. In our experiments, the extented Candide model is utilized, and 3-D motion of only the facial part is considered. The nonrigid motion of the neck and shoulder has not yet been considered. A good approach to handling this problem can be found in <ref type="bibr" target="#b31">[34]</ref>. As for the facial motion, we have confined ourselves to five AU's that correspond to the opening of the mouth, the closing of the eyes, and the raising of eyebrows, etc. Some synthesized frames with different expressions are shown in Fig. <ref type="figure" target="#fig_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Two-View Motion Estimation</head><p>In order to quantitatively evaluate the two-view motion estimation method, we have arranged one group of quantitative experiments in which two algorithms ((12) and ( <ref type="formula">18</ref>)) are used to estimate the 3-D motion from an artificially synthesized facial image sequence whose motion is known.</p><p>I ) Quantitative Experiments: The purpose of this group of experiments is to investigate the accuracy and efficiency of the two-view motion estimation. To do so, we need a facial image sequence whose global and local motion (facial expressions) are exactly known. Such a facial image sequence can be synthesized based on the "Claire" image sequence in the following way: We first take some frame of the "Claire" image sequence as the first frame of the artificially synthesized image sequence. Then, we give arbitrary values to the 3-D motion parameters. With the 3-D motion parameters thus given, we can obtain a synthesized facial image that will be used as the second frame. Since the 3-D motion is known, we can quantitatively evaluate the performance of the two-view motion estimation method.</p><p>Besides directly testing the accuracy of our method by comparing the estimated 3-D motion parameters with the real ones, we also use the following error criterion to evaluate the efficiency of the estimated motion parameters: where i ( s , y) is the synthesized I ( z . y) using the estimated 3-D motion parameters, and n is the number of image points used.</p><p>We have synthesized two image sequences with different motion parameters. One contains smaller motion, and the second contains relatively large motion. These two image     <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_1">11</ref>, respectively.</p><p>In the tables "method I" denotes the method using (12) to estimate motion parameters, and "method 11" denotes the method using (18). The synthesized image sequences using the estimated motion parameters are shown in Figs. <ref type="figure" target="#fig_8">6(c</ref>) and (d) and 7(c) and (d), respectively.</p><p>From Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_1">11</ref>, we find that for the smaller 3-D motion, both (18) and ( <ref type="formula">12</ref>) produce accurate estimates of the 3-D motion, and the performance of ( <ref type="formula">18</ref>) is just a shade better than that of (12). However, when the 3-D motion becomes larger, as shown in Table <ref type="table" target="#tab_1">11</ref>, and although both methods still work, the performance of ( <ref type="formula">12</ref>) is obviously not as good as that of (18). For example, a large error occurs in AU 1. The reason for this error is that there are inherent ambiguities in recovering the 3-D motion field from its projection on the 2-D image plane [9]. Especially for estimating facial expression and due to the AU's not being independent of each other, an image may be synthesized by many different linear combinations of the basic AU's. Therefore, the inherent ambiguities will result in motion parameter estimation errors. Another important reason is due to the error coming from (9), especially when ( u , ~)  is large. However, from the synthesized images, e.g., Figs. <ref type="figure" target="#fig_8">6(c</ref>) and (d) and 7(c) and (d), we find that there is almost no visual difference between the synthesized facial image and the original one, although there are errors between the estimated motion parameters and the actual ones. This phenomenon illustrates the fact that it is not so important that the motion parameters are correct, as long as the synthesized image looks natural and similar to the original. Therefore, we believe that in model-based image coding, the evaluation criterion of motion estimation should be the "efficiency" R instead of "accuracy"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experiments on a Real Image:</head><p>In this group of experiments, we test ( <ref type="formula">18</ref>) and ( <ref type="formula">12</ref>) using the real image frames from the "Claire" image sequence, Two experiments have been done. One uses Fig. <ref type="figure">8</ref> From Fig. <ref type="figure" target="#fig_0">10</ref>, it is seen that the sequential method gives an increasingly larger error over time. With the increase in frame number, the motion estimation will tend to diverge. Similar results were also obtained in <ref type="bibr">[15]</ref>. Clearly, the unstability of this method is because of error accumulation. By comparison, results provided by both motion prediction methods show that our motion tracking system has a good stability behavior. This is due to the feedback characteristic of the system. Using the estimated motion parameters, we then synthesize an artificial image sequence. For the purpose of comparison, every second frame of this sequence is shown in Fig. <ref type="figure" target="#fig_0">11</ref>.</p><p>We also find from Fig. <ref type="figure" target="#fig_0">10</ref> that the performance of the system with the adaptive motion predictor is superior to that of the linear predictor. This proves that the adaptive method does have better tracking behavior than the nonadaptive one.</p><p>Carefully analyzing the relationship between the R and the global rotation motion, we find that the R criterion will become larger with increasing 3-D rotation. This phenomenon is not surprising, as was pointed out earlier. The criterion R is based on the assumption that the intensity of a moving point will remain unchanged over time. When interframe rotation becomes larger, this assumption will be violated. This is because the value of intensity of a point depends on three factors: the microstructure of the point, the distribution of the incident light, and the orientation of the point with respect to the viewer and the light sources [2]. With an increase in rotation parameters, the former two factors could be considered unchanged, but the orientation of the point with respect to the viewer and light source will vary, which causes a change in intensity. If such a change in intensity cannot be compensated for, it will result in a larger error in the R criterion. Certainly, that will affect the estimation of 3-D motion to some degree. How to cope with such a problem is, therefore, worth further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUDING REMARKS</head><p>We have presented an approach to 3-D motion estimation in model-based facial image coding. Our approach can handle both global motion of the head and facial expressions simultaneously. Based on the "Candide" facial model, we have given an affine model that is designed to cope with such a kind of nonrigid motion as facial expression. The use of the affine model provides an example of how to handle nonrigid motion when some knowledge about the nonrigid object is available. It is impossible to directly compute nonrigid motion parameters using the Helmholtz formula due to too many unknown parameters. It is therefore important to extract the constraints behind the nonrigid behavior from the knowledge and use these constraints to limit the solution space.</p><p>We have also developed a two-view motion estimation method that can directly compute global and local motion parameters. Both quantitative and qualitative experiments show that the method is completely suitable to the application of model-based facial image coding. In the evaluation of the estimated motion parameters, we suggest that the estimated motion parameters should be efficient rather than accurate.</p><p>In the dynamic motion estimation case, we have also argued the necessity and possibility of 3-D motion prediction. The use of temporal motion information is of benefit for local motion estimation. Experimental results illustrate that our motion estimation system has rather stable tracking behavior, which can be contributed to the embedding of the synthesis technique into the analysis stage. We strongly believe that some kind of image feedback mechanism is necessary to obtain stable performance of 3-D motion estimation.</p><p>The success of the work presented in this paper confirms the fact that for some computer vision problems such as model-based image coding, the collaboration between computer vision and computer graphics will be very fruitful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model-based image coding-A 3-D object oriented system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Wire-frame model of the face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Let gi = Ixc,i + Iyc,i; equation (10) can be written as a simple linear equation as follows (11) GU = -It where G = ( g l , g 2 , . . . g m + 6 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. General structure of a long sequence motion tracking system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>position of the face by s' = s + MU. In order to synthesize a natural and realistic facial image, texture mapping [4], [5], [26] is done over triangles. Further details on the image synthesis module can be found in [32].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For a given predicted value Uprdid of the motion parameter and the prediction error U " " ' , an estimation U of the actual motion U is given by<ref type="bibr" target="#b29">[32]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Every second frame of the "Claire" image sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Synthesized facial images with different facial expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) First frame of the artificial image sequence; (b) second frame of the artificial image sequence; (c) synthesized second frame using motion parameters by algorithm I; (d) synthesized second frame using motion parameters by algorithm 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>sequences are shown in Figs. 6(a) and (b) and 7(a) and (b). The numerical results are given in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) First frame of the artificial image sequence; (b) second frame of the artificial image sequence; (c) synthesized second frame using motion parameters by algorithm I; (d) synthesized second frame using motion parameters by algorithm 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig. 11. Reconstructed "Claire" image sequence using only one frame of texture image and motion parameters according to the motion estimation system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>MOTION ESTIMATION RESULTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table><row><cell>MOTION ESTIMATION RESULTS</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to acknowledge the contributions from R. Nohre, A . Lundmark, and the other members of the Linkoping University Image Coding Group, to Q. Ye and S. Xu with the image processing group, and to 0. Fahlander at the computer animation group. The authors would also like to thank the anonymous referees for valuable comments and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Swedish Board for Technical Development (STU).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. Reconstruction error when using the sequential method and our feed-back method with a linear motion predictor and an adaptive motion predictor, respectively.</p><p>coding. In fact, the feasibility of these algorithms is further proven by the motion tracking tests. the two-view motion estimation algorithms (12) and (18), especially (18), are quite efficient. The synthesized images using the estimated motion parameters agree well with the original ones.</p><p>Through these experiments, we can draw a conclusion that the two-view motion estimation algorithms ( <ref type="formula">12</ref>) and ( <ref type="formula">18</ref>) are completely suitable to the application of model-based image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Motion Tracking</head><p>In this subsection, we mainly test the stable behavior of our motion tracking system shown in Fig. <ref type="figure">3</ref>. The 32 successive frames of the "Claire" image sequence are employed. In this part of the image sequence, Claire turns her head from the center position to the left with some typical facial expressions such as Blink, Mouth Stretch, etc. Our task is to track the motion of the head and extract the facial expressions. We have assumed that the acceleration of the motion of Claire's head is uniform. Thus, a two-point linear motion predictor and a twopoint adaptive linear motion predictor are used in our motion tracking system, respectively. For the purpose of comparison, we also test the sequential motion estimation method in which the motion of the current frame is obtained by directly adding the newly estimated change in motion between successive frames to the motion of the previous frame <ref type="bibr">[15]</ref>.</p><p>In order to come up with an objective measure of the performance of these methods, we still adopt the R criterion (26) as a measure of how well motion has been estimated. In Fig. <ref type="figure">10</ref>, we show plots of three methods. On the z axis, we have the frame number, and on the y axis, we have the R criterion. During the academic year 1979 to 1980, he was a visiting research scientist at University of Southern California, where he worked with optical circuits and computer architectures for image processing His research areas have involved data security, packet radio transmission, integrated circuits for image processing, and image coding. He has authored or coauthored more thdn 60 papers in these areas and holds several patents. As associate professor, he is currently in charge of the image coding group at Linkoping University His main work concerns algorithms for low bit-rate video transmission and smart sensors for image processing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image Sequence Processing and Dynamic Scene Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<publisher>Robor Hsion</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sommerfeld</surname></persName>
		</author>
		<title level="m">Mechanics of Deformable Bodies</title>
		<imprint>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Principles of Interactive Computer Graphics</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Sproull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image Synthesis: Theory and Practice</title>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the computation of motion from sequences of images-A review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Agganval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nandhakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1981-12">Aug. 1988. Dec. 1981. 1986</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1147" to="1153" />
		</imprint>
	</monogr>
	<note>Estimating 3-D motion parameters of a rigid planar patch</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding trajectories of feature points in a monocular image sequence</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Putt. Anal. Machine Intell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inherent ambiguities in recovering 3D motion and structure from a noisy flow field</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Putt. Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">477489</biblScope>
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of object motion parameters from noisy images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Broida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9G" to="99" />
			<date type="published" when="1986-01">Jan. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust algorithms for direct motion perception</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ICCV</title>
		<meeting>1st ICCV</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low bit-rate coding through animation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
		<author>
			<persName><surname>Fahlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Picture Coding Symp. (PCS-83) (Davis)</title>
		<meeting>Picture Coding Symp. (PCS-83) (Davis)</meeting>
		<imprint>
			<date type="published" when="1983-03">Mar. 1983</date>
			<biblScope unit="page" from="113" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The motion estimation problem in semantic image coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Picture Coding Symp. (PCS-87) (Stockholm)</title>
		<meeting>Picture Coding Symp. (PCS-87) (Stockholm)</meeting>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
			<biblScope unit="page" from="171" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image coding-from waveforms to animation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kronander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Motion estimation in model-based coding of human faces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Roivainen</surname></persName>
		</author>
		<idno>LIU-TEK-LIC-1990:25</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>ISY, Linkoping Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Licentiate Thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for estimation of threedimensional motion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AT&amp;T Techn. J</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1985-02">Feb. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-based coding of images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Telecom Res. Lab</title>
		<imprint>
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model-based synthesis image coding system-Modeling a person&apos;s face and synthesis of facial expressions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GLOBECOM-87</title>
		<meeting>GLOBECOM-87</meeting>
		<imprint>
			<date type="published" when="1987-11">Nov. 1987</date>
			<biblScope unit="page" from="4" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object-oriented analysissynthesis coding of moving images</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Musmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="138" />
			<date type="published" when="1989-10">Oct. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A texture mapping approach to 3D facial image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="129" to="134" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CANDIDE: A Parameterized face</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rydfalk</surname></persName>
		</author>
		<idno>ISY-1-0866</idno>
	</analytic>
	<monogr>
		<title level="j">Dep. Elec. Eng. Rep. LiTH</title>
		<imprint>
			<date type="published" when="1987-10">Oct. 1987</date>
		</imprint>
		<respStmt>
			<orgName>Linkoping Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human face and the mimical language</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hjortsjo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studentlitteratur</title>
		<imprint>
			<date type="published" when="1969">1969</date>
			<pubPlace>Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consulting Psychologists</title>
		<meeting><address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coding of facial image sequence based on a 3D model of the head and motion detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hatori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Visual Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="54" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analysis of dynamic facial images using physical and anatomical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Int. Conf Comput. Vision</title>
		<meeting>Third Int. Conf Comput. Vision<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="306" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture-mapping in model-based image coding</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Commun</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="395" />
			<date type="published" when="1990-12">Dec. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling, analysis, and visualization of nonrigid object motion</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf Patt. Recogn</title>
		<meeting>10th Int. Conf Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Constraints on deformable models: Recovering 3D shape and nonrigid motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopolous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="123" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonrigid motion interpretation: A regularized approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="730" to="742" />
			<date type="published" when="1988-07">1988. July 1991</date>
		</imprint>
	</monogr>
	<note>Proc. Roy. Soc. London</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cvgip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="175" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D motion estimation in model-based facial image coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roivainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LiTH-ISY-I-1278</title>
		<imprint>
			<date type="published" when="1991-10">Oct. 1991</date>
		</imprint>
		<respStmt>
			<orgName>Linkoping Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modelling of a natural 3D scene consisting of moving objects from a sequence of monocular TV images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kappei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-E</forename><surname>Liedtke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">860</biblScope>
			<biblScope unit="page">126</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Subdividing nonrigid 3D objects into quasi rigid parts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE 3rd Int. Conf Image Processing Applications</title>
		<meeting>IEE 3rd Int. Conf Image essing Applications<address><addrLine>Warwick, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
