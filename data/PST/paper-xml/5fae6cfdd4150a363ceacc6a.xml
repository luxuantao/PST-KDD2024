<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-to-Fine CNN for Image Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
							<email>chunweitian@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<email>yongxu@ymail.com</email>
						</author>
						<author>
							<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
							<email>cwlin@ee.nthu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Shenzhen Key Laboratory of Visual Object Detection and Recognition</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<region>Heilongjiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Guang-dong</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Institute of Communications Engineering</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-to-Fine CNN for Image Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6E872288A21C98C97E509802E7C886C3</idno>
					<idno type="DOI">10.1109/TMM.2020.2999182</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2999182, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 1 Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 03:24:05 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2999182, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 03:24:05 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2999182, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 3 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2020.2999182, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 4</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image super-resolution</term>
					<term>convolutional neural network</term>
					<term>cascaded structure</term>
					<term>feature fusion</term>
					<term>feature refinement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) have been popularly adopted in image super-resolution (SR). However, deep CNNs for SR often suffer from the instability of training, resulting in poor image SR performance. Gathering complementary contextual information can effectively overcome the problem. Along this line, we propose a coarse-to-fine SR CNN (CFSRCNN) to recover a high-resolution (HR) image from its low-resolution version. The proposed CFSRCNN consists of a stack of feature extraction blocks (FEBs), an enhancement block (EB), a construction block (CB) and, a feature refinement block (FRB) to learn a robust SR model. Specifically, the stack of FEBs learns the long-and short-path features, and then fuses the learned features by expending the effect of the shallower layers to the deeper layers to improve the representing power of learned features. A compression unit is then used in each FEB to distill important information of features so as to reduce the number of parameters. Subsequently, the EB utilizes residual learning to integrate the extracted features to prevent from losing edge information due to repeated distillation operations. After that, the CB applies the global and local LR features to obtain coarse features, followed by the FRB to refine the features to reconstruct a high-resolution image. Extensive experiments demonstrate the high efficiency and good performance of our CFSRCNN model on benchmark datasets compared with stateof-the-art SR models. The code of CFSRCNN is accessible on https://github.com/hellloxiaotian/CFSRCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>identification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Since SISR is an ill-posed inverse problem, prior information is important to guarantee the quality of reconstructed SR image. For example, a set of patterns through Bayesian knowledge was learned for SISR in <ref type="bibr" target="#b4">[5]</ref>. The method proposed in <ref type="bibr" target="#b5">[6]</ref> utilizes sparse representation to predict the SR counterparts of LR patches. The random forest-based method proposed in <ref type="bibr" target="#b6">[7]</ref> can directly map SR patches from LR patches to overcome the difficulty of training. In addition, non-local self-similarity (NLSS) <ref type="bibr" target="#b7">[8]</ref>, regression <ref type="bibr" target="#b8">[9]</ref>, dictionary learning <ref type="bibr" target="#b9">[10]</ref>, and gradient methods <ref type="bibr" target="#b10">[11]</ref> were shown to be effective for SISR.</p><p>Although these SISR methods can achieve impressive performances, most of them still suffer from two drawbacks. First, they usually rely on complex optimization methods to recover HR images, which are time-consuming. Second, they need manually-tuned parameters to obtain a good performance of SISR, making them inflexible.</p><p>Deep learning techniques have found wide applications in low-level vision, such as image denoising <ref type="bibr" target="#b11">[12]</ref>, rain removal <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, deblurring <ref type="bibr" target="#b14">[15]</ref> and image SR <ref type="bibr" target="#b15">[16]</ref>. Taking image SR as an example, the super-resolution CNN (SRCNN) proposed in <ref type="bibr" target="#b16">[17]</ref> utilizes three convolutional layers to predict the HR image in a pixel-mapping manner, which, however, leads to slow convergence and large training cost. To break the bottleneck of the SRCNN, a very deep SR (VDSR) network <ref type="bibr" target="#b17">[18]</ref> uses residual learning and small filter sizes to accelerate the speed of training while achieving good visual quality. Moreover, reducing the number of parameters is effective to overcome the difficulty in training a SR model. For example, the deeply-recursive convolutional network (DRCN) <ref type="bibr" target="#b18">[19]</ref> and the deep recursive residual network (DRRN) <ref type="bibr" target="#b19">[20]</ref> utilize recursive learning and residual learning techniques to improve training efficiency. Besides, using skip connections to fuse global and local features, such as the 30-layer convolutional residual encoder-decoder network (RED30) <ref type="bibr" target="#b20">[21]</ref>, is shown effective in enhancing the expressive ability of the SISR model. Moreover, the very deep persistent memory network (MemNet) in <ref type="bibr" target="#b21">[22]</ref> applies recursive and gate units to mine useful features for some low-level tasks. However, very deep networks are not easy to train. Additionally, some of these methods perform bicubic interpolation to upscale a LR image to the same size as the HR image on SISR, which resulted in low efficiency for training <ref type="bibr" target="#b22">[23]</ref>. Some existing methods only extract LR features for the SR task and magnify the obtained LR features in the final layer, which ignores the effect of HR features on SISR and can lead to the instability of training.</p><p>In this paper, we propose a coarse-to-fine super-resolution CNN (CFSRCNN) for SISR. It consists of a stack of feature extraction blocks (FEBs), an enhancement block (EB), a construction block (CB), and a feature refinement block (FRB) to train a robust SR model. The combination of the stacked FEBs, EB and CB can more effectively make use of hierarchical LR features extracted from a LR image with much fewer parameters to enhance LR features and infer better initial HR features. Specifically, the stack of FEBs learns the long-and short-path features, and then fuses these features by expending the effect of the shallower layers to the deeper layers to improve the representing power of the learned features. A compression unit is then used in each FEB to distill important information of features so as to reduce the number of parameters. Subsequently, the EB utilizes residual learning to integrate the learned features to prevent from losing edge information due to repeated distillation operations. After that, the CB applies global and local LR features to obtain initial HR features, followed by the FRB to refine the HR features to reconstruct the final SR image. The proposed CFSRCNN has the following contributions.</p><p>(1) We propose a cascaded network that combines LR and HR features to prevent possible training instability and performance degradation caused by upsampling operations.</p><p>(2) We propose a novel feature fusion scheme based on heterogeneous convolutions to well resolve the long-term dependency problem and prevent information loss so as to significantly improve the efficiency of SISR without sacrificing the visual quality of reconstructed SR images.</p><p>(3) The proposed network achieves both good performance and high computational efficiency for SISR.</p><p>The remainder of this paper is organized as follows. Section II provides related work. Section III presents the proposed method. Section IV shows extensive experimental results. Section V reports conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep CNNs based on cascaded structures for SISR</head><p>In image SR, using LR information, especially with a large upscaling factor to recover a HR image is very challenging. To address this problem, deep CNNs based on cascaded structures have been proposed to minimize the error between prediction results and their ground-truths. They can be divided into two categories in general. The first category applies bicubic interpolation to upscale a given LR image to the same size as the HR image, and then uses the upscaled image to predict a SR image. Specifically, a simple, effective, robust, and fast (SERF) method <ref type="bibr" target="#b23">[24]</ref> cascades several linear least squares functions to extract effective features and then compresses the model in a coarse-to-fine manner. The second category gradually uses upscaling in different stages to predict the SR image. Specifically, the deep network cascade (DNC) in <ref type="bibr" target="#b24">[25]</ref> magnifies a LR image layer by layer and utilizes NLSS in each sub-network to extract HR texture features. The cascaded multi-scale cross (CMSC) network in <ref type="bibr" target="#b25">[26]</ref> cascades different sub-networks to obtain SR features. Then, it uses a reconstruction stage to fuse the obtained features in a weighted way to reconstruct a SR image. To reduce computation, the cascading residual network (CARN) in <ref type="bibr" target="#b26">[27]</ref> cascades residual networks with small filter sizes to train a fast, accurate, and lightweight model. All the methods demonstrated the effectiveness of cascading operations in mitigating the discrepancy between a predicted SR image and its ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep CNNs based on modules for SISR</head><p>Due to their flexible end-to-end architectures, CNNs have been widely adopted in many fields, i.e., image processing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, video surveillance <ref type="bibr" target="#b30">[31]</ref> and speech processing <ref type="bibr" target="#b31">[32]</ref>, and text recognition <ref type="bibr" target="#b32">[33]</ref>. To facilitate more features, CNNs based on modules are developed for image SR. Specifically, these methods can be divided into two categories in general: high accuracy (also referred to as performance) and efficiency.</p><p>For improving the accuracy of SR, fusing multiple features has been found useful in enhancing the expressive ability of a SR model. For example, the multi-scale dense network (MSDN) in <ref type="bibr" target="#b33">[34]</ref> employs a multi-scale dense block to fuse intermediate features of different layers for SISR. Further, different views of an image are used as the inputs of a SR network to improve accuracy. The residual channel attention network (RCAN) in <ref type="bibr" target="#b34">[35]</ref> utilizes residual channel attention blocks to mine and integrate different features from the channels of a LR image. The deep networks in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> fuse color and depth information of a given LR image to enhance the expressive ability of learned features. Besides, to make better use of hierarchical features, the residual dense network (RDN) in <ref type="bibr" target="#b37">[38]</ref> combines local and global features via residual dense blocks to recover HR image details. Using residual learning techniques to learn hierarchical features is shown beneficial to build a depth map for SISR <ref type="bibr" target="#b38">[39]</ref>. To obtain more detailed information, the SR CliqueNet (SRCliqueNet) in <ref type="bibr" target="#b39">[40]</ref> utilizes a clique block and an up-sampling module to extract textural details useful for SISR. The recursively dilated residual network in <ref type="bibr" target="#b15">[16]</ref> increases the impact of local spatial information by using a recursion module, followed by a refinement module to learn more accurate LR features for recovering HR image details.</p><p>For improving efficiency, reducing the number of parameters is a common way. The information distillation network (IDN) in <ref type="bibr" target="#b40">[41]</ref> applies a feature extraction block, a stacked information distillation block, and a reconstruction block to recover HR details. The stacked information distillation block exploits part convolutional filters of size 1 × 1 to compress the model for improving the speed of training. To reduce complexity and computational cost, the block state-based recursive network (BSRN) in <ref type="bibr" target="#b41">[42]</ref>, comprising an initial feature extractor, a recursive residual block and an upscaling part, increases the resolution of input images at the last stage to reduce the model complexity. That was also extended to a deeper or wider network, the convolutional anchored regression network (CARN) <ref type="bibr" target="#b42">[43]</ref> employs regression blocks to convert a LR input image to an other domain, according to the regression and similarity so as to achieve a better trade-off between speed and accuracy in contrast to other SISR methods. The Laplacian pyramid distillation network (LapIDN) in <ref type="bibr" target="#b43">[44]</ref> first applies a Laplacian pyramid module to upscale extracted features gradually, followed by a distillation network to achieve high SR performance while compressing the network complexity. Moreover, there are other effective SISR methods, such as the lightweight feature fusion network (LFFN) in <ref type="bibr" target="#b44">[45]</ref> that aggregates different hierarchical features in an adaptively convex weighed manner to control the number of parameters through a spindle block and a softmax feature fusion module, and the adaptive weighted SR network (AWSRN) in <ref type="bibr" target="#b45">[46]</ref> that utilizes adaptive weighted residual unit and local residual fusion units, and an adaptive weighted multi-scale module to reduce parameters, according to the contribution of obtained features at different scales.</p><p>To achieve both high performance and efficiency in SISR, we propose a novel cascaded structure consisting of modular CNN blocks to learn accurate features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our proposed CFSRCNN is composed of a stack of Feature Extraction Blocks (FEBs), an Enhancement Block (EB), a Construction Block (CB) and a Feature Refinement Block (FRB). The combination of the stacked FEBs, EB and CB can make use of hierarchical LR features extracted from the LR image with fewer parameters to enhance obtained LR features and derive coarse SR features. Specifically, combining an FEU and a CU into an FEB obtains longand short-path features. Also, fusing the obtained features via the two closest FEUs can enlarge the effects of shallow layers on deep layers to improve the representing power of the SR model. The CU can distill more useful information and reduce the number of parameters. The EB fuses the features of all FEUs to offer complementary features for the stacked FEBs and prevent from the loss of edge information caused by the repeated distillation operations. Gathering several extra stacked FEUs into the EB removes over-enhanced pixel points from the previous stage of the EB. After that, the CB utilizes the global and local LR features to obtain coarse SR features. Finally, the FRB utilizes HR features to more effectively learn HR features and reconstruct a HR image. We introduce these techniques in the later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>The proposed 46-layer CFSRCNN is composed of four parts, a stacked FEBs, an EB, a CB and an FRB. Let I LR and I SR denote the input LR image and its corresponding SR output image of CFSRCNN, respectively. We divide the four blocks into two kinds, according to the obtained feature types (i.e., LR and HR features): the combination of the stack of FEBs, EB and CB, and FRB. For the combination of the three functional blocks, a 40-layer network is used to extract LR features from a given LR image and derive the coarse SR features, as formulated below:</p><formula xml:id="formula_0">O CB = f CB (f EB (f sF EBs (I LR ))),<label>(1)</label></formula><p>where f sF EBs , f EB and f CB denote the functions of the stack of FEBs, EB and CB, respectively, O CB is the output of the the combination of the stack of FEBs, EB and CB. Specifically, O CB is used as the input of a 6-layer FRB, which utilizes HR features to reduce the discrepancy between the predicted SR and target HR images, as formulated below:</p><formula xml:id="formula_1">O F RB =f F RB (O CB ) =f CF SRCN N (I LR ),<label>(2)</label></formula><p>where f F RB and O F RB denote the function and output of FR-B, respectively. f CF SRCN N is the function of the CFSRCNN. Also, O F RB = I SR . Finally, CFSRCNN is optimized by the loss function that will be explained in Section III.B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss function</head><p>We use a set of training pairs {I j LR , I j HR } N j=1 to train the model, where N is the size of training set, and I j LR and I j HR denote the j-th LR and HR training images, respectively. We choose mean square error (MSE) <ref type="bibr" target="#b46">[47]</ref> as the loss function to minimize the difference between the predicted SR and target HR images as follows:</p><formula xml:id="formula_2">l(θ) = 1 2N N j=1 f CF SRCN N (I j LR ) -I j HR 2 2 ,<label>(3)</label></formula><p>where θ denotes the parameter set of the trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The combination of stacked FEBs, EB, and CB</head><p>The 40-layer block consists of 33-layer stacked FEBs, 6layer EB and 1-layer CB. Specifically, FEBs aim to improve the efficiency and performance of SR by combining a FEU with a CU, where CU is used to distill more useful information. EB offers complementary features by fusing hierarchical LR features to address the loss of edge information from the CUs. Besides, EB also utilizes several additional stacked FEUs to learn finer LR features to mitigate over-enhanced pixel points caused by the previous stage of EB. Subsequently, CB can apply the global and local LR features to obtain coarse SR features. The detailed information is shown in latter subsections.</p><p>1) Stacked FEBs: As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the 33-layer stacked FEBs fuses obtained features from the pair of FEU and CU to enhance the performance and efficiency of the SR model. FEB of the stacked FEBs is composed of a pair of FEU and CU that respectively perform 3 × 3 and 1 × 1 heterogeneous convolutions, except for the last FEB that only has a FEU without CU. Each FEB (except the last one) first concatenates the output of the FEU from the previous FEB (aka the long-path features) and that of its own FEU (aka the short-path features) as the input of its own CU to enhance the representing power for SR. Subsequently, CU distills useful information from the enhanced LR features above, which can reduce the number of parameters and improve the training efficiency for a SR model. In practice, each FEU and CU comprise Conv+ReLU: a convolution filter followed by a rectified linear unit <ref type="bibr" target="#b47">[48]</ref>. As mentioned above, the filter sizes of the FEU and CU are 3 × 3 and 1 × 1, respectively. The ReLU is used to non-linearly transform the fused features, which is then used as the input of the following CU. Further, the sizes of the first FEU and CU are 3 × 3 × 3 × 64 and 64 × 1 × 1 × 64, respectively, where 3 and 64 are the channel numbers of the input and output of the first FEU, respectively, and 64 is the channel number of the input and output of the first CU, respectively. The sizes of FEU and CU of the other FEBs are 64×3×3×64 and 128×1×1×64, respectively, where 64 represents the channel numbers of input and output except  for the first FEU, and 128 and 64 are the channel numbers of input and output except the first CU, respectively. Due to the 1 × 1 convolution, CUs can better distill useful information as well as reduce the number of parameters <ref type="bibr" target="#b40">[41]</ref>, as will be shown in Section IV.D. We further clarify the formulation of the stacked FEBs in details. Let O i F EU and O i CU denote the output of the FEU and CU of the i-th FEB, respectively. According to the previous descriptions, the function of the i-th FEB can be expressed as</p><formula xml:id="formula_3">O i CU = CU (Cat(F EU (O i-1 CU ), O i-1 F EU )),<label>(4)</label></formula><p>where i = 2, 3, , ..., 16 and Cat represents a concatenation operation. Specifically, the first FEB can be represented as</p><formula xml:id="formula_4">O 1 CU = CU (F EU (I LR )).</formula><p>Because the last FEB (the 17th FEB) only has a FEU, it can be formulated as</p><formula xml:id="formula_5">O 17 CU = Cat(F EU (O 16 CU ), O 16 F EU ),<label>(5)</label></formula><p>where O sF EBs = O 17 CU and O sF EBs is the output of the stacked FEBs.</p><p>2) Enhancement block: It is known that as the depth of the network increases, the extracted features are more accurate. However, this will also lead to the information loss of shallow layers <ref type="bibr" target="#b48">[49]</ref>. Also, although the CUs can reduce the number of parameters and improve the efficiency for a SR model, repeated distillation operations lead to the loss of edge information extracted from the shallow layers. Taking into account these two factors, we propose a two-step feature enhancement block (EB) as demonstrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The first step of the EB, involving one Conv+ReLU with a filter size of 128×3×3×64, gathers hierarchical features extracted by the FEUs of all FEBs through residual learning to offer features complementary to the stacked FEBs. The second step, involving five Conv+ReLU with the same filter size of 64 × 3 × 3 × 64, fine-tunes the LR features to mitigate possible over-enhancement caused by the first step. The first-step operation of EB can be formulated as follows:</p><formula xml:id="formula_6">O 1 EB = 17 i=1 (F EU (O sF EBs ) + O i F EU ),<label>(6)</label></formula><p>where O 1 EB denotes the first-step output of EB. The EB's second-step can be expressed as:</p><formula xml:id="formula_7">O 2 EB = F EU (F EU (F EU (F EU (F EU (O 1 EB ))))),<label>(7)</label></formula><p>where O 2 EB represents the final output of EB. 3) Construction block: It is known that a given LR image uses bicubic interpolation to obtain the same size as the HR image as the input of a SR network for training the model, which can result in high computational cost and memory consumption <ref type="bibr" target="#b22">[23]</ref>. To address this problem, the up-sampling technique was proposed <ref type="bibr" target="#b49">[50]</ref>. For example, a fast SR convolutional neural network (FSRCNN) <ref type="bibr" target="#b22">[23]</ref> utilized deconvolution as the last layer to upscale the extracted LR features for SISR. FSRCNN directly utilized a LR image as input to extract LR features, then applied the deconvolution technique in the last layer to reconstruct the HR image. Although this approach can improve the efficiency of training, its performance in SISR is not satisfactory. In this paper, we use two steps in the construction block (CB) to overcome this problem. CB first performs a sub-pixel convolution with a filter size of 64 × 3 × 3 × 64 to obtain global-and local-features. Subsequently, CB fuses the obtained features by residual learning to derive coarse HR features. The operation of CB can be formulated as follows:</p><formula xml:id="formula_8">O CB = S(O 1 F EU ) + S(O 2 EB ),<label>(8)</label></formula><p>where S and + stand for the functions of the sub-pixel convolution and residual learning, respectively. In addition, the residual learning operation is denoted as in Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature refinement block</head><p>Feature refinement block (FRB) is used to reduce the discrepancy between the predicted SR and ground-truth HR images. It involves five cascaded FEUs followed by a convolutional filter, where each FEU consists of Conv+ReLU with the same filter size of 64 × 3 × 3 × 64, where 64 is the channel numbers of the input and output. This operation of cascaded FEUs is shown in Eq. <ref type="bibr" target="#b8">(9)</ref>.</p><formula xml:id="formula_9">O F RB1 = F EU (F EU (F EU (F EU (F EU (O CB ))))), (9)</formula><p>where O F RB1 is the output of five cascaded FEUs of FRB, which is then filtered by the final convolution with a filter size of 64 × 3 × 3 × 3 as follows:</p><formula xml:id="formula_10">I SR = C(O F RB1 ),<label>(10)</label></formula><p>where C denotes the final convolution function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training dataset</head><p>Following the state-of-the-art SR methods in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, we utilize the public DIV2K dataset <ref type="bibr" target="#b52">[53]</ref> to train our model. The DIV2K dataset contains 800 training images, 100 validation images, and 100 test images at three different scales: ×2, ×3, and ×4. We merge the training and validation datasets of the DIV2K to expand the training dataset. Further, to improve the efficiency of model training, we divide each LR image into patches of size 77 × 77. Besides, we use random horizontal flipping and 90 • rotation operations <ref type="bibr" target="#b26">[27]</ref> to augment the training patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Testing datasets</head><p>We utilize five benchmark datasets, including Set5 <ref type="bibr" target="#b53">[54]</ref>, Set14 <ref type="bibr" target="#b5">[6]</ref>, BSD100 <ref type="bibr" target="#b54">[55]</ref>, Urban100 <ref type="bibr" target="#b55">[56]</ref> and 720p, which are constructed at three different scales (×2, ×3, and ×4), to evaluate the performance of the trained SR model. Set5 and Set14 respectively contain 5 and 14 images from different scenes, and BSD100 (a.k.a. B100) and Urban100 (a.k.a. U100) both consist of 100 images. The 720p dataset is composed of three typical images from clean images in the PolyU dataset <ref type="bibr" target="#b56">[57]</ref>, which are cropped to 1280 × 720.</p><p>Note, existing methods, such as DnCNN <ref type="bibr" target="#b57">[58]</ref> and RED30 utilize YCbCr channel (also named Y channel) to conduct experiments. Thus, we covert the RGB image predicted by CFSRCNN into Y channel to evaluate the performance for SISR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>In the training process, the initial parameters are batch size of 64, beta 1 of 0.9, beta 2 of 0.999 and epsilon of 1e-8. All steps of the training are 6e+5. The initial learning rate is 1e-4 and halved every 4e+5 steps. Also, the initial weights and biases are the same as <ref type="bibr" target="#b26">[27]</ref>.</p><p>The propose CFSRCNN is implemented on Pytorch 0.41 and Python 2.7 for training and inference, respectively. Besides, all experiments are conducted on Ubuntu 16.04 on a PC equipped with an Intel Core i7-7800 CPU, 16G RAM, and two GPUs of Nvidia GeForce GTX 1080Ti with Nvidia CUDA 9.0, and CuDNN 7.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network analysis</head><p>Extracting suitable features has been shown useful to accelerate the training process and improve performance in many image applications <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. To this end, feature extraction for a SR network usually involves HR features, LR features, and the combination of HR and LR features. For HR features, using bicubic interpolation to upscale a LR image as the input for training a SR model was popular <ref type="bibr" target="#b60">[61]</ref>. This method, however, may lose some LR features, thereby resulting in performance degradation in SISR. Also, it leads to high computational cost as well. To address this issue, directly using LR features to train a SR model has been proposed. For example, FSRCNN <ref type="bibr" target="#b22">[23]</ref> accelerates the SR task by utilizing only LR features prior to upsampling LR features in the last layer to obtain the HR image. However, as reported in <ref type="bibr" target="#b61">[62]</ref>, the upsampling operation may lead to a sudden shock to the model, which makes the training procedure unstable. To address the problem, an additional refinement process was found useful in <ref type="bibr" target="#b61">[62]</ref> which refines the HR features obtained by the upsampling process by combining HR and LR information. Motivated by this, we also cascade a feature refinement block (FEB) to recover HR image details. Moreover, when the network depth goes deeper, the shallow-layer features would have weaker effect on deep-layer features. To address this problem, fusing hierarchical information has been proposed. Notably, the RDN in <ref type="bibr" target="#b37">[38]</ref> fuses hierarchical non-linear features extracted from all convolutional layers via a residual dense block (see Fig. <ref type="figure" target="#fig_2">3(a)</ref>) to enhance the memory ability of shallow layers. Besides, it employs global residual learning to learn global features complementary to local features obtained from the residual dense blocks, thereby achieving performance improvement in SISR. Moreover, the channel-wise and spatial feature modulation (CSFM) method proposed in <ref type="bibr" target="#b62">[63]</ref> applies channel-wise and spatial attentions as block to extract hierarchical features and fuse them for enhancing the expressive ability of the SR model, as illustrated in Fig. <ref type="figure" target="#fig_2">3(b</ref>). These methods achieve great performances for SISR. Similarly, our proposed CFSRCNN makes full use of hierarchical features to enhance LR features in the cascaded network for SISR. Nevertheless, as can be easily seen by comparing Figs. 1-2 and Fig. <ref type="figure" target="#fig_2">3</ref>, CFSRCNN is different from the RDN and CSFM in the following aspects:</p><p>(1) We concatenate the features of two neighboring FEBs, instead of using solely the current layer (used in RDB and CSFM), as the input of all the following layers to propagate the effect of shallower-layer features to deeper layers. Besides, we use a pair of heterogeneous (3×3 and 1×1) convolutions with two layers, rather than stacking multiple (3 × 3) convolutions, and fuse them into a block (FEB) to reduce the network depth and complexity (i.e., the numbers of parameters and ops). The above two changes together largely reduce the number of parameters to only 5.5% of RDB and 9.3% of CSFM, and the run-time, without severely sacrificing the visual quality of reconstructed SR images, as shown in the next subsection.</p><p>(2) The Enhancement Block (EB) is not simply a concatenation of multiple FEUs. Instead, it adopts residual learning, rather than concatenating FEUs, to integrate the hierarchical LR features obtained from FEUs for enhancing the robustness of obtained LR features, which are complementary to the sFEBs. To prevent possible over-enhancement caused by previous operations, inspired by VDSR <ref type="bibr" target="#b17">[18]</ref>, we stack several convolutional layers to smooth out sharp features.</p><p>(3) By gathering the global and local features via the residual learning and sub-pixel convolution to obtain coarse HR features, rather than using solely local features, CB can effectively address the long-term dependency problem.</p><p>(4) Different from most existing learning-based SR methods which utilize LR features to train their models, we additionally make use of HR features to boost the SR performance via FRB that learns more accurate HR features by stacking multiple convolutional layers to reduce the discrepancy between the predicted SR image and its ground-truth. This also can enhance the stability of the training process.</p><p>CFSRCNN is composed of a number of stacked feature extraction blocks (sFEBs), an enhancement block (EB), a construction block (CB), and a feature refinement block (FRB). Such a combination efficiently makes full use of extracted hierarchical features to enhance the LR features, that are then used to infer initial SR features. The feature refinement block further employs HR features to learn more robust SR features to reduce the discrepancy between the predicted SR and its ground-truth. Thus, the combination of the sFEBs, EB, CB, and FRB can enhance the stability of SR model training. These key modules constituting CFSRCNN are elaborated below.</p><p>1) Stacked Feature Extraction Blocks (sFEBs): The sFEBs well collaborate with the EB and CB to extract more robust LR features for training a SR model, where each FEB includes an FEU and a CU. The design of FEB breaks two rules: less parameters and better performance for a SR model. For the first aspect, we use partial heterogeneous convolutions with P = 2 <ref type="bibr" target="#b63">[64]</ref> to reduce computation and improve efficiency of training the SR network, where P denotes part. These heterogeneous convolutions consist of 16 standard convolutions with size 3×3 and 16 small convolutions with size 1 × 1. The convolutions of size 3 × 3 and 1 × 1 are integrated into FEU and CU, respectively. Since the convolution of size 1 × 1 can remove redundant information and distill important features <ref type="bibr" target="#b40">[41]</ref>, the proposed CU can effectively reduce the number of parameters and improve the efficiency of training CFSRCNN.</p><p>The higher diversity the network architecture is, the better performance the SR model achieves as revealed in <ref type="bibr" target="#b37">[38]</ref>. We therefore append a standard convolutional layer of 3 × 3 to heterogeneous convolutions. Given a LR input image, we consolidate the sub-pixel layer to learn SR features after heterogeneous convolutions by using a standard convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv ReLU</head><p>To reconstruct the final SR image, we employ a standard convolution with size 64 × 3 × 3 × 3 as the final layer. The depth of heterogeneous convolutional network (HCN) is set to be 35. For fair comparison, we compare HCN with a standard convolutional network (SCN) of the same depth as that of HCN. Table <ref type="table" target="#tab_0">I</ref> shows that HCN consumes significantly fewer computational cost and memory space than SCN for ×2 upscaling. Besides, HCN is also more computationally efficient than SCN in run-time complexity as shown in Table <ref type="table" target="#tab_1">II</ref>. These results verify that our method effectively reduces the number of parameters and complexity. Additionally, Table <ref type="table" target="#tab_2">III</ref> show that HCN achieves the same Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) performance as that of SCN on U100 for ×2 upscaling. Note, with the increase of network depth, shallow-layer features would make weaker effect on deep-layer ones, making a deep network suffer from SR performance degradation <ref type="bibr" target="#b48">[49]</ref>. To address this problem, gathering hierarchical features can offer complementary contextual information from shallower layers to deeper layers <ref type="bibr" target="#b37">[38]</ref>. Motivated by this, we propose a two-step enhancement mechanism in the sFEBs to enhance the expressive ability of the SR model. The first step utilizes the FEUs and CUs from two contiguous FEBs to extract longand short-path features, respectively. The second step exploits a concatenation operation to fuse the extracted long-and shortpath features to address the long-term dependency problem. Further, the effectiveness of the two-step enhancement mechanism for SR is illustrated in Table <ref type="table" target="#tab_2">III</ref>, where the sFEBs with long and short-path achieve higher PSNR and SSIM than that of HCN without long and short-path on B100 and U100, showing the effectiveness of the combination of long-and short-path.</p><p>2) Enhancement Block (EB): Although a convolution of size 1 × 1 can distill useful features <ref type="bibr" target="#b40">[41]</ref>, repeated distillation operations may lead to information loss of the original images. To address this issue, a two-step enhancement mechanism has been adopted in the sFEBs. However, the two-step enhancement mechanism can fuse long-and short-path features by concatenating their half feature points to enhance the generalization ability of a SR model. The features extracted from the deep layers only inherit partial features from shallow layers, which cannot completely address the above-mentioned problem. To better handle this problem, we propose a twophase EB. The first phase of EB (named EB1) applies the residual learning technique to integrate hierarchical features of all FEUs to offer complementary features for the sFEBs. The second phase uses several additional stacked FEUs to refine the learned LR features, which can mitigate over-enhanced pixels from EB1. The two phases not only provide extra information for SISR, but also enhance the diversity of the network architecture. These are useful to recover a latent HR image as verified in Table <ref type="table" target="#tab_2">III</ref>. That is, the 'combination of stacked FEBs and EB1' outperforms 'sFEBs' in PSRN and SSIM on B100 and U100, showing the effectiveness of EB1 for SISR. 'The combination of stacked FEBs and EB' achieves higher PSNR and SSIM performances than 'The combination of stacked FEBs and EB1', implies that the second phase of EB improves SR performance.</p><p>3) Construction Block (CB): Using bicubic interpolation to upscale a given LR image as the input of a SR model leads to high computational cost and memory consumption <ref type="bibr" target="#b22">[23]</ref>. Taking this into consideration, we use the sub-pixel technique to magnify the obtained LR features as SR features, which can improve the training efficiency by reducing the computational cost and memory consumption. However, local LR features benefit from multiple convolutions may ignore some useful information in the original LR images, making the extracted SR features anemic. It is known that global features are complementary with local features to promote the expressive ability for SISR <ref type="bibr" target="#b37">[38]</ref>. Inspired by that, we utilize the residual learning technique to fuse global and local   <ref type="table" target="#tab_2">III</ref>. That is, the 'The combination of sFEBs, EB and CB' has better results of PSNR and SSIM on B100 and U100 than that of 'The combination of sFEBs and EB' in SISR. 4) Feature Refinement Block (FRB): The combination of the sFEBs, EB and CB mainly extracts LR features to learn coarse SR features, which, however, cannot fully characterize the latent HR image. Also, training of a SISR model is unstable. To tackle these problems, a six-layer FRB is proposed to refine SR features. That is, FRB can use HR features to refine SR features so as to reduce the discrepancy between the predicted SR and its ground-truth, which is complementary with the combination of the sFEBs, EB and CB. As a consequence, combining the sFEBs, EB, CB and FRB can enhance the stability of training the SR model. This fact is verified by comparing 'CFSRCNN' with 'The combination of sFEBs, EB and CB' about PSNR and SSIM on U100 and B100 in Table <ref type="table" target="#tab_2">III</ref>. Also, we employ FRB in the LR space (called FRNet) to extract features with approximately same number of parameters to validate the effectiveness of FRB as shown in Table <ref type="table" target="#tab_2">III</ref>. However, solely using FRB in the LR space behalves like a VGG network with a very deep depth that usually leads to gradient vanishing/explosion, thereby significantly degrading performance. Since such validation method does not provide meaningful comparison, we do not include it in the comparison. Finally, we use a convolution of size 64×3×3×3 as the final layer to reconstruct a SR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons with state-of-the-arts</head><p>We conduct quantitative and qualitative analyses to evaluate the performance of CFSRCNN for SISR. Specifically, we evaluate quantitatively the average PSNR and SSIM performances and the run-time and model complexities of various SR methods on five benchmark datasets: Set5, Set14, B100, U100 and 720p. The compared methods include Bicubic, A+ <ref type="bibr" target="#b8">[9]</ref>, RFL <ref type="bibr" target="#b6">[7]</ref>, self-exemplars SR method (SelfEx) <ref type="bibr" target="#b55">[56]</ref>, cascade of sparse coding based network (CSCN) <ref type="bibr" target="#b66">[67]</ref>, RED30 <ref type="bibr" target="#b20">[21]</ref>, DnCNN <ref type="bibr" target="#b57">[58]</ref>, trainable nonlinear reaction diffusion (TNRD) <ref type="bibr" target="#b67">[68]</ref>, fast dilated SR method (FDSR) [69], SRCNN <ref type="bibr" target="#b16">[17]</ref>, FS-RCNN <ref type="bibr" target="#b22">[23]</ref>, residue context sub-network (RCN) <ref type="bibr" target="#b60">[61]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, DRCN <ref type="bibr" target="#b18">[19]</ref>, context-wise network fusion (CNF) <ref type="bibr" target="#b50">[51]</ref>, Laplacian SR network (LapSRN) <ref type="bibr" target="#b69">[70]</ref>, DRRN <ref type="bibr" target="#b19">[20]</ref>, balanced two-stage residual networks (BTSRN) <ref type="bibr" target="#b70">[71]</ref>, MemNet <ref type="bibr" target="#b21">[22]</ref>, CARN-M <ref type="bibr" target="#b26">[27]</ref>, CARN <ref type="bibr" target="#b26">[27]</ref>, end-to-end deep and shallow network (EEDS)+ <ref type="bibr" target="#b71">[72]</ref>, two-stage convolutional network (TSCN) <ref type="bibr" target="#b72">[73]</ref>, deep recurrent fusion network (DRFN) <ref type="bibr" target="#b73">[74]</ref>, RDN <ref type="bibr" target="#b37">[38]</ref>, CSFM <ref type="bibr" target="#b62">[63]</ref>, and super-resolution feedback network (SRFBN) <ref type="bibr" target="#b74">[75]</ref>. We also demonstrate a few reconstructed SR images for subjective visual comparisons. The average PSNR and SSIM performances of various SR methods on the Set5, Set14, B100, U100 and 720p datasets are demonstrated in Tables IV-VIII, respectively. As shown in Table <ref type="table" target="#tab_4">IV</ref>, our CFSRCNN with scaling factors of ×3 and ×4 outperforms the state-of-the-art SR methods, such as DRFN, TSCN, EEDS+ and CARN-M on Set5, and achieves a comparable performance with that of CARN-M for ×2 upscaling. Specifically, compared with CARN-M, CFSRCNN achieves a notable gain of 0.14dB in PSNR for ×4 upscaling. Moreover, as shown in Tables V-VIII, CFSRCNN achieves excellent performances for all the three scaling factors: ×2 , ×3 and ×4. For example, Table <ref type="table">V</ref>     412K 2.50G CARN <ref type="bibr" target="#b26">[27]</ref> 1,592K 10.13G CSFM <ref type="bibr" target="#b62">[63]</ref> 12,841K 76.82G RDN <ref type="bibr" target="#b37">[38]</ref> 21,937K 130.75G SRFBN <ref type="bibr" target="#b74">[75]</ref> 3,631K images of size 154 × 154. Table <ref type="table" target="#tab_7">X</ref> shows that CFSRCNN consumes the third fewest number of flops while faithfully high-quality SR images. Due to its shallower architecture and fewer concatenation operations, CFSRCNN does not outperform some deeper SR networks, such as RDN, CSFM and SRFBM. However, CFSRCNN offers an excellent trade-off among visual quality, computational efficiency, and model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a coarse-to-fine super-resolution CNN (CFSRCNN) for single-image super-resolution. CFSR-CNN combines low-resolution and high-resolution features by cascading several types of modular blocks to prevent possible training instability and performance degradation caused by upsampling operations. We have also proposed a novel feature fusion scheme based on heterogeneous convolutions to address the long-term dependency problem as well as prevent information loss so as to significantly improve the computational efficiency of super-resolution without sacrificing the visual quality of reconstructed images. Comprehensive evaluations on four benchmark datasets demonstrate that CFSRCNN offers an excellent trade-off among visual quality, computational efficiency, and model complexity. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Network architecture of CFSRCNN.</figDesc><graphic coords="4,165.14,251.91,126.57,85.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the CFSRCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) The residual dense block (RDB) architecture proposed in [38]; (b) The FMM module in the CFSM [63].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 4 .Fig. 5 .</head><label>645</label><figDesc>Fig. 6. Subjective visual quality comparison of various SR methods for ×4 upscaling on U100: (a) HR image (PSNR/SSIM), (b) Bicubic (22.10/0.7862), (c) SRCNN (26.08/0.8547), (d) SelfEx (28.02/0.9026), (e) CARN-M (31.80/0.9324) and (f) CFSRCNN (33.21/0.9377).</figDesc><graphic coords="10,313.43,249.89,124.55,81.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPLEXITY</head><label>I</label><figDesc>COMPARISON OF TWO DIFFERENT NETWORKS.</figDesc><table><row><cell>Methods</cell><cell>Parameters</cell><cell>Flops</cell></row><row><cell>HCN</cell><cell>757k</cell><cell>5.18G</cell></row><row><cell>SCN</cell><cell>1257K</cell><cell>8.14G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RUN</head><label>II</label><figDesc>-TIME PERFORMANCE COMPARISON OF HCN AND SCN FOR ×2 UPSCALING ON IMAGES OF SIZES 256 × 256, 512 × 512, AND 1024 × 1024. First, the outputs of the FEU from the first FEB and the EB are treated as global and local SR features, respectively. The global and local features are then upscaled by the sub-pixel technique as the global and local SR features, respectively. Second, residual learning is employed to fuse the obtained global and local SR features to derive the coarse SR features. This phase makes full use of global and local information from LR and SR features to improve the expressive ability of a SR model, where is tested by Table</figDesc><table><row><cell></cell><cell cols="2">Methods</cell></row><row><cell>Sizes</cell><cell>HCN</cell><cell>SCN</cell></row><row><cell></cell><cell>×2</cell><cell></cell></row><row><cell>256 × 256</cell><cell>0.009466</cell><cell>0.009536</cell></row><row><cell>512 × 512</cell><cell>0.011093</cell><cell>0.011369</cell></row><row><cell>1024 × 1024</cell><cell>0.019960</cell><cell>0.026624</cell></row><row><cell cols="3">features for enhancing the robustness of the extracted SR</cell></row><row><cell>features as follows.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III AVERAGE</head><label>III</label><figDesc>PSNR AND SSIM PERFORMANCES OF VARIOUS SR METHODS FOR ×2 UPSCALING ON TWO BENCHMARK DATASETS: B100 AND U100.</figDesc><table><row><cell>Methods</cell><cell>B100 PSNR/SSIM</cell><cell>U100 PSNR/SSIM</cell></row><row><cell>HCN</cell><cell cols="2">14.64/0.4132 12.86/0.3788</cell></row><row><cell>SCN</cell><cell cols="2">14.64/0.4132 12.86/0.3788</cell></row><row><cell>sFEBs</cell><cell cols="2">31.83/0.8954 31.07/0.9169</cell></row><row><cell>The combination of sFEBs and EB1</cell><cell cols="2">32.03/0.8974 31.77/0.9243</cell></row><row><cell>The combination of sFEBs and EB</cell><cell cols="2">32.05/0.8976 31.80/0.9247</cell></row><row><cell cols="3">The combination of sFEBs, EB and CB 32.06/0.8981 31.91/0.9261</cell></row><row><cell>FRNet</cell><cell cols="2">14.64/0.4132 12.86/0.3788</cell></row><row><cell>CFSRCNN (Ours)</cell><cell cols="2">32.11/0.8988 32.03/0.9273</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>shows that, CFSRCNN outperforms MemNet by 0.23dB, 0.27dB, and 0.31dB in PSNR on Set14 for ×2 , ×3, and ×4 upscaling, respectively. Similarly, Table VI also shows that CFSRCNN outperforms several popular SR methods, such as CARN-M, TSCN and DRFN. As illustrated in TableVII, CFSRCNN achieves a significant PSNR gain over CARN-M by 1.24dB on U100 for ×2 upscaling. In TableVIII, CFSRCNN outperforms CARN for all the three scaling factors on 720p. Besides, these tables also show that CFSRCNN performs stably well.Figs. 4-6 compare the subjective visual quality of CF-SRCNN with that of four SR methods, including Bicubic, SRCNN, SelfEx, and CARN-M for ×2 upscaling on Set14, ×3 upscaling on B100 and ×4 upscaling U100, respectively. To facilitate subjective comparison visually, we enlarge selected regions in the SR images, showing that the images superresolved by CFSRCNN are clearer than those super-resolved by the other methods for ×2, ×3, and ×4 upscaling.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF AVERAGE PSNR/SSIM PERFORMANCES OF VARIOUS SR METHODS FOR ×2, ×3, AND ×4 UPSCALING ON SET5.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>×2 PSNR/SSIM</cell><cell>×3 PSNR/SSIM</cell><cell>×4 PSNR/SSIM</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="3">33.66/0.9299 30.39/0.8682 28.42/0.8104</cell></row><row><cell></cell><cell>A+ [9]</cell><cell cols="3">36.54/0.9544 32.58/0.9088 30.28/0.8603</cell></row><row><cell></cell><cell>RFL [7]</cell><cell cols="3">36.54/0.9537 32.43/0.9057 30.14/0.8548</cell></row><row><cell></cell><cell>SelfEx [56]</cell><cell cols="3">36.49/0.9537 32.58/0.9093 30.31/0.8619</cell></row><row><cell></cell><cell>CSCN [67]</cell><cell cols="3">36.93/0.9552 33.10/0.9144 30.86/0.8732</cell></row><row><cell></cell><cell>RED30 [21]</cell><cell cols="3">37.66/0.9599 33.82/0.9230 31.51/0.8869</cell></row><row><cell></cell><cell>DnCNN [58]</cell><cell cols="3">37.58/0.9590 33.75/0.9222 31.40/0.8845</cell></row><row><cell></cell><cell>TNRD [68]</cell><cell cols="3">36.86/0.9556 33.18/0.9152 30.85/0.8732</cell></row><row><cell>Set5</cell><cell>FDSR [69] SRCNN [17]</cell><cell cols="3">37.40/0.9513 33.68/0.9096 31.28/0.8658 36.66/0.9542 32.75/0.9090 30.48/0.8628</cell></row><row><cell></cell><cell>FSRCNN [23]</cell><cell cols="3">37.00/0.9558 30.71/0.8657</cell></row><row><cell></cell><cell>RCN [61]</cell><cell cols="3">37.17/0.9583 33.45/0.9175 31.11/0.8736</cell></row><row><cell></cell><cell>VDSR [18]</cell><cell cols="3">37.53/0.9587 33.66/0.9213 31.35/0.8838</cell></row><row><cell></cell><cell>DRCN [19]</cell><cell cols="3">37.63/0.9588 33.82/0.9226 31.53/0.8854</cell></row><row><cell></cell><cell>CNF [51]</cell><cell cols="3">37.66/0.9590 33.74/0.9226 31.55/0.8856</cell></row><row><cell></cell><cell>LapSRN [70]</cell><cell>37.52/0.9590</cell><cell>-</cell><cell>31.54/0.8850</cell></row><row><cell></cell><cell>IDN [41]</cell><cell cols="3">37.83/0.9600 34.11/0.9253 31.82/0.8903</cell></row><row><cell></cell><cell>DRRN [20]</cell><cell cols="3">37.74/0.9591 34.03/0.9244 31.68/0.8888</cell></row><row><cell></cell><cell>BTSRN [71]</cell><cell>37.75/-</cell><cell>34.03/-</cell><cell>31.85/-</cell></row><row><cell></cell><cell>MemNet [22]</cell><cell cols="3">37.78/0.9597 34.09/0.9248 31.74/0.8893</cell></row><row><cell></cell><cell>CARN-M [27]</cell><cell cols="3">37.53/0.9583 33.99/0.9255 31.92/0.8903</cell></row><row><cell></cell><cell>CARN [27]</cell><cell cols="3">37.76/0.9590 34.29/0.9255 32.13/0.8937</cell></row><row><cell></cell><cell>EEDS+ [72]</cell><cell cols="3">37.78/0.9609 33.81/0.9252 31.53/0.8869</cell></row><row><cell></cell><cell>TSCN [73]</cell><cell cols="3">37.88/0.9602 34.18/0.9256 31.82/0.8907</cell></row><row><cell></cell><cell>DRFN [74]</cell><cell cols="3">37.71/0.9595 34.01/0.9234 31.55/0.8861</cell></row><row><cell></cell><cell>RDN [38]</cell><cell cols="3">38.24/0.9614 34.71/0.9296 32.47/0.8990</cell></row><row><cell></cell><cell>CSFM [63]</cell><cell cols="3">38.26/0.9615 34.76/0.9301 32.61/0.9000</cell></row><row><cell></cell><cell>SRFBN [75]</cell><cell cols="3">38.11/0.9609 34.70/0.9292 32.47/0.8983</cell></row><row><cell></cell><cell cols="4">CFSRCNN (Ours) 37.79/0.9591 34.24/0.9256 32.06/0.8920</cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISON OF AVERAGE PSNR/SSIM PERFORMANCES OF VARIOUS</cell></row><row><cell cols="5">SR METHODS FOR ×2, ×3, AND ×4 UPSCALING ON SET14.</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>×2 PSNR/SSIM</cell><cell>×3 PSNR/SSIM</cell><cell>×4 PSNR/SSIM</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="3">30.24/0.8688 27.55/0.7742 26.00/0.7027</cell></row><row><cell></cell><cell>A+ [9]</cell><cell cols="3">32.28/0.9056 29.13/0.8188 27.32/0.7491</cell></row><row><cell></cell><cell>RFL [7]</cell><cell cols="3">32.26/0.9040 29.05/0.8164 27.24/0.7451</cell></row><row><cell></cell><cell>SelfEx [56]</cell><cell cols="3">32.22/0.9034 29.16/0.8196 27.40/0.7518</cell></row><row><cell></cell><cell>CSCN [67]</cell><cell cols="3">32.56/0.9074 29.41/0.8238 27.64/0.7578</cell></row><row><cell></cell><cell>RED30 [21]</cell><cell cols="3">32.94/0.9144 29.61/0.8341 27.86/0.7718</cell></row><row><cell></cell><cell>DnCNN [58]</cell><cell cols="3">33.03/0.9128 29.81/0.8321 28.04/0.7672</cell></row><row><cell></cell><cell>TNRD [68]</cell><cell cols="3">32.51/0.9069 29.43/0.8232 27.66/0.7563</cell></row><row><cell></cell><cell>FDSR [69]</cell><cell cols="3">33.00/0.9042 29.61/0.8179 27.86/0.7500</cell></row><row><cell>Set14</cell><cell>SRCNN [17] FSRCNN [23]</cell><cell cols="3">32.42/0.9063 29.28/0.8209 27.49/0.7503 32.63/0.9088 29.43/0.8242 27.59/0.7535</cell></row><row><cell></cell><cell>RCN [61]</cell><cell cols="3">32.77/0.9109 29.63/0.8269 27.79/0.7594</cell></row><row><cell></cell><cell>VDSR [18]</cell><cell cols="3">33.03/0.9124 29.77/0.8314 28.01/0.7674</cell></row><row><cell></cell><cell>DRCN [19]</cell><cell cols="3">33.04/0.9118 29.76/0.8311 28.02/0.7670</cell></row><row><cell></cell><cell>CNF [51]</cell><cell cols="3">33.38/0.9136 29.90/0.8322 28.15/0.7680</cell></row><row><cell></cell><cell>LapSRN [70]</cell><cell cols="3">33.08/0.9130 29.63/0.8269 28.19/0.7720</cell></row><row><cell></cell><cell>IDN [41]</cell><cell cols="3">33.30/0.9148 29.99/0.8354 28.25/0.7730</cell></row><row><cell></cell><cell>DRRN [20]</cell><cell cols="3">33.23/0.9136 29.96/0.8349 28.21/0.7720</cell></row><row><cell></cell><cell>BTSRN [71]</cell><cell>33.20/-</cell><cell>29.90/-</cell><cell>28.20/-</cell></row><row><cell></cell><cell>MemNet [22]</cell><cell cols="3">33.28/0.9142 30.00/0.8350 28.26/0.7723</cell></row><row><cell></cell><cell>CARN-M [27]</cell><cell cols="3">33.26/0.9141 30.08/0.8367 28.42/0.7762</cell></row><row><cell></cell><cell>CARN [27]</cell><cell cols="3">33.52/0.9166 30.29/0.8407 28.60/0.7806</cell></row><row><cell></cell><cell>EEDS+ [72]</cell><cell cols="3">33.21/0.9151 29.85/0.8339 28.13/0.7698</cell></row><row><cell></cell><cell>TSCN [73]</cell><cell cols="3">33.28/0.9147 29.99/0.8351 28.28/0.7734</cell></row><row><cell></cell><cell>DRFN [74]</cell><cell cols="3">33.29/0.9142 30.06/0.8366 28.30/0.7737</cell></row><row><cell></cell><cell>RDN [38]</cell><cell cols="3">34.01/0.9212 30.57/0.8468 28.81/0.7871</cell></row><row><cell></cell><cell>CSFM [63]</cell><cell cols="3">34.07/0.9213 30.63/0.8477 28.87/0.7886</cell></row><row><cell></cell><cell>SRFBN [75]</cell><cell cols="3">33.82/0.9196 30.51/0.8461 28.81/0.7868</cell></row><row><cell></cell><cell cols="4">CFSRCNN (Ours) 33.51/0.9165 30.27/0.8410 28.57/0.7800</cell></row></table><note><p>We then compare the run-time complexity of CFSRCNN with that of six methods, including VDSR, DRRN, MemNet,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF AVERAGE PSNR/SSIM PERFORMANCES OF VARIOUS SR METHODS FOR ×2, ×3, AND ×4 UPSCALING ON B100. SRFBN, and CARN-M, on HR images of sizes 256 × 256, 512 × 512 and 1024 × 1024 for ×2 upscaling. TableIXshows that CFSRCNN achieves the fastest processing speed. Besides run-time, we also evaluate the number of parameters and flops<ref type="bibr" target="#b75">[76]</ref> in TableXthat reflect the SR model complexity (i.e., computational cost and memory consumption) for SR</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>×2 PSNR/SSIM</cell><cell>×3 PSNR/SSIM</cell><cell>×4 PSNR/SSIM</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="3">29.56/0.8431 27.21/0.7385 25.96/0.6675</cell></row><row><cell></cell><cell>A+ [9]</cell><cell cols="3">31.21/0.8863 28.29/0.7835 26.82/0.7087</cell></row><row><cell></cell><cell>RFL [7]</cell><cell cols="3">31.16/0.8840 28.22/0.7806 26.75/0.7054</cell></row><row><cell></cell><cell>SelfEx [56]</cell><cell cols="3">31.18/0.8855 28.29/0.7840 26.84/0.7106</cell></row><row><cell></cell><cell>CSCN [67]</cell><cell cols="3">31.40/0.8884 28.50/0.7885 27.03/0.7161</cell></row><row><cell></cell><cell>RED30 [21]</cell><cell cols="3">31.98/0.8974 28.92/0.7993 27.39/0.7286</cell></row><row><cell></cell><cell>DnCNN [58]</cell><cell cols="3">31.90/0.8961 28.85/0.7981 27.29/0.7253</cell></row><row><cell></cell><cell>TNRD [68]</cell><cell cols="3">31.40/0.8878 28.50/0.7881 27.00/0.7140</cell></row><row><cell>B100</cell><cell>FDSR [69]</cell><cell cols="3">31.87/0.8847 28.82/0.7797 27.31/0.7031</cell></row><row><cell></cell><cell>SRCNN [17]</cell><cell cols="3">31.36/0.8879 28.41/0.7863 26.90/0.7101</cell></row><row><cell></cell><cell>FSRCNN [23]</cell><cell cols="3">31.53/0.8920 28.53/0.7910 26.98/0.7150</cell></row><row><cell></cell><cell>VDSR [18]</cell><cell cols="3">31.90/0.8960 28.82/0.7976 27.29/0.7251</cell></row><row><cell></cell><cell>DRCN [19]</cell><cell cols="3">31.85/0.8942 28.80/0.7963 27.23/0.7233</cell></row><row><cell></cell><cell>CNF [51]</cell><cell cols="3">31.91/0.8962 28.82/0.7980 27.32/0.7253</cell></row><row><cell></cell><cell>LapSRN [70]</cell><cell>31.80/0.8950</cell><cell>-</cell><cell>27.32/0.7280</cell></row><row><cell></cell><cell>IDN [41]</cell><cell cols="3">32.08/0.8985 28.95/0.8013 27.41/0.7297</cell></row><row><cell></cell><cell>DRRN [20]</cell><cell cols="3">32.05/0.8973 28.95/0.8004 27.38/0.7284</cell></row><row><cell></cell><cell>BTSRN [71]</cell><cell>32.05/-</cell><cell>28.97/-</cell><cell>27.47/-</cell></row><row><cell></cell><cell>MemNet [22]</cell><cell cols="3">32.08/0.8978 28.96/0.8001 27.40/0.7281</cell></row><row><cell></cell><cell>CARN-M [27]</cell><cell cols="3">31.92/0.8960 28.91/0.8000 27.44/0.7304</cell></row><row><cell></cell><cell>CARN [27]</cell><cell cols="3">32.09/0.8978 29.06/0.8034 27.58/0.7349</cell></row><row><cell></cell><cell>EEDS+ [72]</cell><cell cols="3">31.95/0.8963 28.88/0.8054 27.35/0.7263</cell></row><row><cell></cell><cell>TSCN [73]</cell><cell cols="3">32.09/0.8985 28.95/0.8012 27.42/0.7301</cell></row><row><cell></cell><cell>DRFN [74]</cell><cell cols="3">32.02/0.8979 28.93/0.8010 27.39/0.7293</cell></row><row><cell></cell><cell>RDN [38]</cell><cell cols="3">32.34/0.9017 29.26/0.8093 27.72/0.7419</cell></row><row><cell></cell><cell>CSFM [63]</cell><cell cols="3">32.37/0.9021 29.30/0.8105 27.76/0.7432</cell></row><row><cell></cell><cell>SRFBN [75]</cell><cell cols="3">32.29/0.9010 29.24/0.8084 27.72/0.7409</cell></row><row><cell></cell><cell cols="4">CFSRCNN (Ours) 32.11/0.8988 29.03/0.8035 27.53/0.7333</cell></row><row><cell></cell><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISON OF AVERAGE PSNR/SSIM PERFORMANCES OF VARIOUS</cell></row><row><cell cols="5">SR METHODS FOR ×2, ×3, AND ×4 UPSCALING ON U100.</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>×2 PSNR/SSIM</cell><cell>×3 PSNR/SSIM</cell><cell>×4 PSNR/SSIM</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="3">26.88/0.8403 24.46/0.7349 23.14/0.6577</cell></row><row><cell></cell><cell>A+ [9]</cell><cell cols="3">29.20/0.8938 26.03/0.7973 24.32/0.7183</cell></row><row><cell></cell><cell>RFL [7]</cell><cell cols="3">29.11/0.8904 25.86/0.7900 24.19/0.7096</cell></row><row><cell></cell><cell>SelfEx [56]</cell><cell cols="3">29.54/0.8967 26.44/0.8088 24.79/0.7374</cell></row><row><cell></cell><cell>RED30 [21]</cell><cell cols="3">30.91/0.9159 27.31/0.8303 25.35/0.7587</cell></row><row><cell></cell><cell>DnCNN [58]</cell><cell cols="3">30.74/0.9139 27.15/0.8276 25.20/0.7521</cell></row><row><cell></cell><cell>TNRD [68]</cell><cell cols="3">29.70/0.8994 26.42/0.8076 24.61/0.7291</cell></row><row><cell></cell><cell>FDSR [69]</cell><cell cols="3">30.91/0.9088 27.23/0.8190 25.27/0.7417</cell></row><row><cell>U100</cell><cell>SRCNN [17] FSRCNN [23]</cell><cell cols="3">29.50/0.8946 26.24/0.7989 24.52/0.7221 29.88/0.9020 26.43/0.8080 24.62/0.7280</cell></row><row><cell></cell><cell>VDSR [18]</cell><cell cols="3">30.76/0.9140 27.14/0.8279 25.18/0.7524</cell></row><row><cell></cell><cell>DRCN [19]</cell><cell cols="3">30.75/0.9133 27.15/0.8276 25.14/0.7510</cell></row><row><cell></cell><cell>LapSRN [70]</cell><cell>30.41/0.9100</cell><cell>-</cell><cell>25.21/0.7560</cell></row><row><cell></cell><cell>IDN [41]</cell><cell cols="3">31.27/0.9196 27.42/0.8359 25.41/0.7632</cell></row><row><cell></cell><cell>DRRN [20]</cell><cell cols="3">31.23/0.9188 27.53/0.8378 25.44/0.7638</cell></row><row><cell></cell><cell>BTSRN [71]</cell><cell>31.63/-</cell><cell>27.75/-</cell><cell>25.74/-</cell></row><row><cell></cell><cell>MemNet [22]</cell><cell cols="3">31.31/0.9195 27.56/0.8376 25.50/0.7630</cell></row><row><cell></cell><cell>CARN-M [27]</cell><cell cols="3">30.83/0.9233 26.86/0.8263 25.63/0.7688</cell></row><row><cell></cell><cell>CARN [27]</cell><cell cols="3">31.92/0.9256 28.06/0.8493 26.07/0.7837</cell></row><row><cell></cell><cell>TSCN [73]</cell><cell cols="3">31.29/0.9198 27.46/0.8362 25.44/0.7644</cell></row><row><cell></cell><cell>DRFN [74]</cell><cell cols="3">31.08/0.9179 27.43/0.8359 25.45/0.7629</cell></row><row><cell></cell><cell>RDN [38]</cell><cell cols="3">32.89/0.9353 28.80/0.8653 26.61/0.8028</cell></row><row><cell></cell><cell>CSFM [63]</cell><cell cols="3">33.12/0.9366 28.98/0.8681 26.78/0.8065</cell></row><row><cell></cell><cell>SRFBN [75]</cell><cell cols="3">32.62/0.9328 28.73/0.8641 26.60/0.8015</cell></row><row><cell></cell><cell cols="4">CFSRCNN (Ours) 32.07/0.9273 28.04/0.8496 26.03/0.7824</cell></row><row><cell>RDN,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>OF AVERAGE PSNR/SSIM PERFORMANCES OF VARIOUS SR METHODS FOR ×2, ×3, AND ×4 UPSCALING ON 720P. [27] 43.62/0.9791 39.87/0.9602 37.61/0.9389 CARN [27] 44.57/0.9809 40.66/0.9633 38.03/0.9429 CFSRCNN (Ours) 44.77/0.9811 40.93/0.9656 38.34/0.9482 TABLE IX COMPARISON OF RUN-TIME (SECONDS) OF VARIOUS SR METHODS ON HR IMAGES OF SIZES 256 × 256, 512 × 512 AND 1024 × 1024 FOR UPSCALING.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">×2 PSNR/SSIM</cell><cell cols="2">×3 PSNR/SSIM</cell><cell>×4 PSNR/SSIM</cell></row><row><cell>720p</cell><cell cols="4">CARN-M Single Image Super-Resolution</cell></row><row><cell></cell><cell>Size</cell><cell>256 × 256</cell><cell cols="2">512 × 512</cell><cell>1024 × 1024</cell></row><row><cell></cell><cell>VDSR [18]</cell><cell>0.0172</cell><cell></cell><cell>0.0575</cell><cell>0.2126</cell></row><row><cell cols="2">DRRN [20]</cell><cell>3.063</cell><cell></cell><cell>8.050</cell><cell>25.23</cell></row><row><cell cols="2">MemNet [22]</cell><cell>0.8774</cell><cell></cell><cell>3.605</cell><cell>14.69</cell></row><row><cell></cell><cell>RDN [38]</cell><cell>0.0553</cell><cell></cell><cell>0.2232</cell><cell>0.9124</cell></row><row><cell cols="2">SRFBN [75]</cell><cell>0.0761</cell><cell></cell><cell>0.2508</cell><cell>0.9787</cell></row><row><cell cols="2">CARN-M [27]</cell><cell>0.0159</cell><cell></cell><cell>0.0199</cell><cell>0.0320</cell></row><row><cell cols="2">CFSRCNN (Ours)</cell><cell>0.0153</cell><cell></cell><cell>0.0184</cell><cell>0.0298</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X COMPARISON</head><label>X</label><figDesc>OF MODEL COMPLEXITIES OF VARIOUS SR METHODS FOR ×2 UPSCALING.</figDesc><table><row><cell>Methods</cell><cell>Parameters</cell><cell>Flops</cell></row><row><cell>VDSR [18]</cell><cell>665K</cell><cell>15.82G</cell></row><row><cell>DnCNN [58]</cell><cell>556K</cell><cell>13.20G</cell></row><row><cell>DRCN [19]</cell><cell>1,774K</cell><cell>42.07G</cell></row><row><cell>MemNet [22]</cell><cell>677K</cell><cell>16.06G</cell></row><row><cell>CARN-M [27]</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 03:24:05 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="70" xml:id="foot_1"><p>papers in toptier academic journals and conferences. He has served as an Co-Editors-in-Chief of the International Journal of Image and Graphics, an Associate Editor of the CAAI Transactions on Intelligence Technology, an editor of the Pattern Recognition and Artificial Intelligence. More information please refer to http://www.yongxu.org/lunwen.html.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Nature Science Foundation of China Gant No. 61876051 and in part by the Shenzhen Key Laboratory of Visual Object Detection and Recognition under Grant No. ZDSYS20190902093015527.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast spatio-temporal residual network for video super-resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and cross-modality synthesis of 3d medical images using weakly-supervised joint convolutional sparse coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Computer Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6070" to="6079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-resolution dictionary learning for face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="283" to="292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel multicamera system for high-speed touchless palm recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. Syst</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A bayesian nonparametric approach to image super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="358" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jchao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Computer Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means steering kernel regression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alternatively constrained dictionary learning for image superresolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="377" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generalized accelerated proximal gradient approach for total-variation-based image restoration¡? query valign=&quot;-12pt&quot; copyedited file</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2748" to="2759" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image denoising using deep cnn with batch renormalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Net</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="461" to="473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast single image rain removal via a deep decomposition-composition network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02688</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive image deraining networks: a better and simpler baseline</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3937" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to deblur images with exemplars</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detail-preserving image super-resolution via recursively dilated residual network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Computer Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2802" to="2810" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vis</title>
		<meeting>European Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Serf: a simple, effective, robust, and fast image super-resolver from cascaded linear regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4091" to="4102" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vis</title>
		<meeting>European Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single image superresolution via cascaded multi-scale cross network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1802.08808</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vis</title>
		<meeting>European Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Computer Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised deep slow feature analysis for change detection in multi-temporal remote sensing images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9976" to="9992" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning on image denoising: An overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13171</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ptb-tir: A thermal infrared pedestrian tracking benchmark</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1576" to="1590" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention-fused deep matching network for natural language inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="4033" to="4040" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale dense network for singleimage super-resolution</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoustics, Speech Signal Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1742" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vis</title>
		<meeting>European Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simultaneous color-depth super-resolution with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="356" to="369" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Color-guided depth map super resolution using convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">672</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical features driven residual learning for depth map super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint sub-bands learning with clique structures for wavelet domain super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast and accurate single image superresolution via information distillation network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Lightweight and efficient image super-resolution with block state-based recursive network</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12546</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Carn: convolutional anchored regression network for fast and accurate single image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vis</title>
		<meeting>European Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single image superresolution via laplacian information distillation network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digital Home</title>
		<meeting>Int. Conf. Digital Home</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lightweight feature fusion network for single image super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="538" to="542" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Lightweight image super-resolution with adaptive weighted learning network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02358</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Iterative correction of intersymbol interference: Turbo-equalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jézéquel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Electronique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Picart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Didier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glavieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Trans. Telecom</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="507" to="511" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image super resolution based on fusing multiple convolution neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-level waveletcnn for image restoration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Computer Vis. Pattern Recog. Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Vancouver</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Real-world noisy image denoising: A new benchmark</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02603</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE int. conf. comput. vis</title>
		<imprint>
			<biblScope unit="page" from="244" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Structure-preserving image super-resolution via contextualized multitask learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2804" to="2815" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image super-resolution via progressive cascading residual network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="791" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Channel-wise and spatial feature modulation network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Hetconv: Heterogeneous kernel-based convolutions for deep cnns</title>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04120</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast single image super-resolution via dilated residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ya-Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi-Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiaojun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image superresolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">End-to-end image superresolution via deep and shallow convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">970</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Two-stage convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recog</title>
		<meeting>Int. Conf. Pattern Recog</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Drfn: Deep recurrent fusion network for single-image super-resolution with large factors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="337" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attention-guided cnn for image denoising</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">He has published over 20 papers in academic journals and conferences, including Neural Networks, Pattern Recognition Letters and ICASSP</title>
	</analytic>
	<monogr>
		<title level="m">He is a PC of the 18th IEEE International Conference on Dependable, Autonomic and Secure Computing (DASC 2020), a PC Assistant of IJCAI 2019, a reviewer of some journals and conferences, such as the IEEE Transactions on Industrial Informatics, the Computer Vision and Image Understanding, the Nerocomputing, the Visual Computer, the Journal of Modern Optics, the IEEE Access, the CAAI Transactions on Intelligence Technology</title>
		<title level="s">Shenzhen. He received the M.S. degree and B.S. degree at Harbin University of Science and Technology</title>
		<imprint>
			<date type="published" when="2014">2017 and 2014. 2019</date>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science and Technology at Harbin Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>International Conference on Artificial Intelligence and the Information Processing and Clod Computing (AIIPC 2019. Yong Xu (Senior Member, IEEE) received his B.S. degree, M.S. degree in 1994 and 1997, respectively. He received the Ph.D. degree in Pattern Recognition and Intelligence system at NUST (China) in 2005. Now he works at Harbin Institute of Technology, Shenzhen. His current interests include pattern recognition, deep learning, biometrics, machine learning and video analysis. He has published over</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
