<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invariance Principle Meets Out-of-Distribution Generalization on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-11">11 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
							<email>yqchen@cse.cuhk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
							<email>csygzhang@comp.hkbu.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
							<email>hyang@cse.cuhk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
							<email>klma@cse.cuhk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<email>tongliang.liu@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
							<email>jcheng@cse.cuhk.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Invariance Principle Meets Out-of-Distribution Generalization on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-11">11 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.05441v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent developments in using the invariance principle from causality to enable out-of-distribution (OOD) generalization on Euclidean data, e.g., images, studies on graph data are limited. Different from images, the complex nature of graphs poses unique challenges that thwart the adoption of the invariance principle for OOD generalization. In particular, distribution shifts on graphs can happen at both structure-level and attribute-level, which increases the difficulty of capturing the invariance. Moreover, domain or environment partitions, which are often required by OOD methods developed on Euclidean data, can be expensive to obtain for graphs. Aiming to bridge this gap, we characterize distribution shifts on graphs with causal models, and show that the OOD generalization on graphs with invariance principle is possible by identifying an invariant subgraph for making predictions. We propose a novel framework to explicitly model this process using a contrastive strategy. By contrasting the estimated invariant subgraphs, our framework can provably identify the underlying invariant subgraph under mild assumptions. Experiments across several synthetic and real-world datasets demonstrate the state-of-the-art OOD generalization ability of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph representation learning with graph neural networks (GNNs) has gained huge success in tasks involving relational information <ref type="bibr" target="#b35">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b26">Hamilton et al., 2017;</ref><ref type="bibr" target="#b76">Veličković et al., 2018;</ref><ref type="bibr" target="#b81">Xu et al., 2018</ref><ref type="bibr" target="#b82">Xu et al., , 2019))</ref>. However, it assumes that the training and test data are drawn from the same distribution, which does not always hold in reality <ref type="bibr" target="#b28">(Hu et al., 2020;</ref><ref type="bibr" target="#b37">Koh et al., 2021;</ref><ref type="bibr" target="#b93">Huang et al., 2021)</ref>. The mismatch between training and test distribution, i.e., distribution shifts, introduced by some underlying environmental factors related to data collection or processing, can seriously degrade the performance of deployed models <ref type="bibr" target="#b5">(Beery et al., 2018;</ref><ref type="bibr" target="#b18">DeGrave et al., 2021)</ref>. Such out-of-distribution (OOD) generalization failures become the major roadblock for practical applications of graph representation learning models.</p><p>In parallel, enabling OOD generalization on Euclidean data has received surging attention and several solutions were proposed <ref type="bibr" target="#b3">(Arjovsky et al., 2019;</ref><ref type="bibr" target="#b60">Sagawa* et al., 2020;</ref><ref type="bibr" target="#b8">Bengio et al., 2020;</ref><ref type="bibr" target="#b25">Gulrajani and Lopez-Paz, 2021;</ref><ref type="bibr" target="#b66">Schölkopf et al., 2021;</ref><ref type="bibr" target="#b39">Krueger et al., 2021;</ref><ref type="bibr" target="#b17">Creager et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>. In particular, the invariance principle from causality is at the heart of those works <ref type="bibr" target="#b55">(Peters et al., 2016;</ref><ref type="bibr">Pearl, 2009)</ref>. The principle generically assumes that there exists a subset of inputs that carry most of the information about the underlying causes of the label. Predictions that merely focus on this part are invariant to a large class of distribution shifts in the sense of Independent Causal Mechanism (ICM) assumption <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b58">Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b38">Koyama and Yamaguchi, 2020;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>.</p><p>Despite the success of the invariance principle on Euclidean data, the complex nature of graph data raises several unique challenges that prohibit direct applications of the principle. First, distribution shifts on graphs can happen at both structure-level and attribute-level, and can exhibit via various graph properties such as graph sizes <ref type="bibr" target="#b9">(Bevilacqua et al., 2021)</ref> and homophily <ref type="bibr" target="#b47">(McPherson et al., 2001)</ref>, which increases the difficulty of characterizing the sources of distribution shifts and capturing the invariance. Second, OOD algorithms developed and analyzed on Euclidean data often require additional environment (or domain) labels for distinguishing the sources of distribution shifts. However, collecting environment labels for graphs requires specific expert knowledge, which can be expensive due to the abstraction of graph data <ref type="bibr" target="#b28">(Hu et al., 2020)</ref>. Hence, the above problems pose the following problem:</p><p>How could we generalize the invariance principle to enable OOD generalization of GNNs?</p><p>In this paper, we provide answers to this question by studying the OOD generalization on graph classification. Specifically, we study 3 generic Structural Causal Models (SCM) <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref> for characterizing the graph generation process and distinguishing the sources of distribution shifts. Built upon these SCMs, we generalize the invariance principle to graphs. That is, when predicting the label for a specific graph G, we can identify an invariant and a critical subgraph G c that contains and only contains most the of information in G about the underlying cause, so that the prediction for G based on G c is stable to distribution shifts by the ICM assumption. Hence, the problem of achieving OOD generalization on graphs is rephrased into invariant subgraph identification and classification (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>To instantiate the invariance principle for graphs, we propose a novel framework, called Graph Out-Of-Distribution generalization (GOOD), to explicitly model the two sub-processes by decomposing a GNN model into the following two modules (cf., Fig. <ref type="figure" target="#fig_0">1</ref>): a) a featurizer for identifying the underlying G c from G; b) a classifier for making predictions based on G c . Specifically, the featurizer aims to distinguish the invariant subgraph G c from the other parts of G that can be easily influenced by environment (or domain) shifts. A soft mask is applied to G for disentangling these two parts that are entangled at both structure and attribute level during the generation. To ensure the identifiability of the underlying G c , we propose a contrastive strategy, inspired by the fact that the G c s from the same classes should have high mutual information that is invariant to distribution shifts. We show that this strategy can enable the featurizer to provably identify the underlying G c (Theorem 4.2). Therefore, the predictions that merely focus on the identified G c are invariant to distribution shifts. We verify the effectiveness of GOOD on several synthetic and real-world datasets with various distribution shifts. Our experiments demonstrate that GOOD significantly outperforms existing methods and achieves the state-of-the-art OOD generalization performance on graphs.</p><p>Our main contributions can be summarized as follows:</p><p>• Through the lens of causality, we establish general SCMs to characterize the distribution shifts on graphs, paving the way for generalizing the invariance principle to graphs (Sec. 3). • We instantiate the invariance principle for OOD generalization on graphs through a novel framework GOOD, where the prediction is decomposed into the subgraph identification and classification. We show that the provable identifiability of the underlying invariant subgraph can be achieved using a contrastive strategy. Hence, the model can generalize to OOD graphs (Sec. 4).</p><p>• Extensive experiments on 13 synthetic and real-world datasets demonstrate that GOOD outperforms previous methods and achieves the state-of-the-art OOD generalization performance (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Graph Neural Networks. Let G = (A, X) denote a graph with n nodes and m edges, where A ∈ {0, 1} n×n is the adjacency matrix, and X ∈ R n×d is the node feature matrix with a node feature dimension of d. In graph classification, we are given a set of N graphs {G i } N i=1 ⊆ G and their labels</p><formula xml:id="formula_0">{Y i } N i=1 ⊆ Y = R c from c classes.</formula><p>Then, we train a GNN h θ • ρ with an encoder h θ : G → R h that learns a meaningful representation r G for each graph G to help predict their labels y G = ρ(r G ) with a downstream classifier ρ : R h → Y. The representation r G is typically obtained by performing pooling with a READOUT function on the learned node representations:</p><formula xml:id="formula_1">r G = READOUT({r (K) u |u ∈ V }),<label>(1)</label></formula><p>where the READOUT is a permutation invariant function (e.g., SUM, MEAN) <ref type="bibr" target="#b88">(Ying et al., 2018;</ref><ref type="bibr" target="#b52">Murphy et al., 2019;</ref><ref type="bibr" target="#b82">Xu et al., 2019;</ref><ref type="bibr" target="#b14">Chen et al., 2020;</ref><ref type="bibr" target="#b50">Morris et al., 2021)</ref>, and r (K) u stands for the node representation of u ∈ V at K-th layer that is obtained by neighbor aggregation:</p><formula xml:id="formula_2">r (K) u = σ(W K • a({r (K−1) v }|v ∈ N (u) ∪ {u})),<label>(2)</label></formula><p>where N (u) is the set of neighbors of node u, σ(•) is an activation function, e.g., ReLU, and a(•) is an aggregation function over neighbors, e.g., MEAN.</p><p>Invariant Learning and OOD generalization on graphs. Invariant learning typically considers a supervised learning setting based on the data D = {D e } e collected from multiple environments E all , where D e = {G e i , y e i } n e i=1 is the dataset from environment e ∈ E all , n e is the number of instances in environment e, and G e i ∈ G and y e i ∈ Y correspond to the input graph and the label for the i-th instance from D e . (G e i , y e i ) from a single environment e are considered as drawn independently from identical distribution P e . The goal of OOD generalization is to train a GNN h θ • ρ : G → Y with data from training environments D tr = {D e } e∈Etr⊆E all , and generalize well to all (unseen) environments, i.e., to minimize: min</p><formula xml:id="formula_3">θ max e∈E all R e (h θ • ρ),<label>(3)</label></formula><p>where R e is the empirical risk under environment e <ref type="bibr" target="#b74">(Vapnik, 1991;</ref><ref type="bibr" target="#b55">Peters et al., 2016;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph OOD Generalization through the Lens of Causality</head><p>It is known that OOD generalization is impossible without assumptions on E all <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>. Thus, in this section, we first formulate the data generation process with structural causal model (SCM) and latent-variable model <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017;</ref><ref type="bibr" target="#b40">Kügelgen et al., 2021)</ref>, and characterize the sources of distribution shifts on graphs. Then, we investigate whether the previous methods can be applied to achieving OOD generalization on graphs.</p><formula xml:id="formula_4">S C E G G c G s G (a) G-Gen SCM E S Y G C (b) FIIF SCM E S Y G C (c) PIIF SCM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph generation process</head><p>We take a latent-variable model perspective on the graph generation and assume that the graph is generated through a mapping f : Z → G, where Z ⊆ R n is the latent space and G = ∪ ∞ N =1 {0, 1} N × R N ×d is the graph space. Following previous works <ref type="bibr" target="#b40">(Kügelgen et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>, we partition the latent variable into an invariant part C ∈ C = R nc and a varying part S ∈ S = R ns , s.t., n = n c + n s . Let E be a random variable that denotes the environments. Given the partition of Z, C keeps the invariant information that cannot be affected by E, while S keeps the varying information that can be affected by E 1 . C and S control the generation of the observed graphs (Assumption 3.1) and can have multiple types of interactions at the latent space (Assumption 3.2, 3.3).</p><p>Graph generation model. We formally elaborate the SCM for the graph generation process in Assumption 3.1 and Fig. <ref type="figure" target="#fig_1">2(a)</ref>, where noises in the structural equations are omitted for simplicity <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>.</p><formula xml:id="formula_5">Assumption 3.1 (Graph Generation SCM). (Z c A , Z c X ) := f (A,X) c gen (C), (Z s A , Z s X ) := f (A,X) s gen (S), G c := f Gc gen (Z c A , Z c X ), G s := f Gs gen (Z s A , Z s X , E G ), G := f G gen (G c , G s , E G ).</formula><p>In Assumption 3.1, C and S control the generation of the adjacency matrices and features of the invariant subgraph G c and spurious subgraph G s through two pairs of latent variables (Z c A , Z c X ) and (Z s A , Z s X ), respectively. Then, G c and G s are entangled into the observed graph G through f G gen . It can be a simply JOIN of a G c with one or multiple G s , or more complex generation processes controlled by the latent variables <ref type="bibr" target="#b67">(Snijders and Nowicki, 1997;</ref><ref type="bibr" target="#b43">Lovász and Szegedy, 2006;</ref><ref type="bibr" target="#b90">You et al., 2018;</ref><ref type="bibr" target="#b45">Luo et al., 2021)</ref>.</p><p>Moreover, a subset of environment latent variable E G ⊆ E will also affect the generation of G and G s . Thus, graphs collected from different environments (or domains) can have different structure-level properties (e.g., degrees, graph sizes) <ref type="bibr" target="#b9">(Bevilacqua et al., 2021)</ref> as well as feature-level properties (e.g., homophily) <ref type="bibr" target="#b13">(Chen et al., 2022)</ref>, while G c remains invariant. In fact, the generation of many real-world graphs can be abstracted as Assumption 3.1. For example, in drug discovery, the treatment effects of a drug to a specific disease is usually captured by a sub-molecule of the molecule graph, which is invariant across several species or domains <ref type="bibr" target="#b10">(Bohacek et al., 1996;</ref><ref type="bibr" target="#b70">Sterling and Irwin, 2015)</ref>.</p><p>Interactions at latent space. Depending on whether the latent invariant part C is fully informative about label Y , i.e., (S, E) ⊥ ⊥ Y |C, the latent interactions between C and S can be categorized into Fully Informative Invariant Features (FIIF, Fig. <ref type="figure" target="#fig_1">2(b)</ref>) and Partially Informative Invariant Features (PIIF, Fig. <ref type="figure" target="#fig_1">2(c</ref>))<ref type="foot" target="#foot_0">2</ref> . We follow <ref type="bibr" target="#b3">Arjovsky et al. (2019)</ref>; <ref type="bibr" target="#b1">Ahuja et al. (2021)</ref> to formulate the SCMs for FIIF and PIIF with noises omitted for brevity <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>.</p><formula xml:id="formula_6">Assumption 3.2 (FIIF Structural Causal Model). Y := f inv (C), S := f spu (C, E), G := f gen (C, S). Assumption 3.3 (PIIF Structural Causal Model). Y := f inv (C), S := f spu (Y, E), G := f gen (C, S).</formula><p>In the two SCMs above, f gen : Z = R nc+ns → G describes the graph generation process as Assumption 3.1. The generation of S is entailed by f spu that takes E as one of the inputs. Besides, f inv : Z = R nc → Y separates and assigns labels for different C, which indicates the labelling process. For classification task, the separation assumption is usually necessary <ref type="bibr" target="#b51">(Muller et al., 2001;</ref><ref type="bibr" target="#b12">Chen et al., 2005)</ref>. Hence, we also introduce the following assumption on the separability for completeness, which are generically compatible with many other variants <ref type="bibr" target="#b48">(Mika et al., 1999;</ref><ref type="bibr" target="#b65">Schölkopf, 2019)</ref>. Assumption 3.4 (Latent Separability). Given any pair of labels (y, y ) ∼ Y × Y, the corresponding (C, C ) ∼ J(y, y ) are bounded<ref type="foot" target="#foot_1">3</ref> and satisfy that:</p><formula xml:id="formula_7">E C,C ∼J (y,y) d(C, C ) ≤ E C,C ∼J (y,y ) d(C, C ),</formula><p>where J (y, y ) is the joint conditional distribution P C|Y =y,C |Y =y , d(•) is a distance measure at the latent space C, and the equality holds if and only if y = y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Failures of graph OOD generalization</head><p>Built upon the setup of the graph generation process, in this section, we formally derive the concept of an invariant GNN that can generalize to OOD graphs. Then, we investigate whether the existing methods can endow GNNs with OOD generalization ability, i.e., learning an invariant GNN.</p><p>Definition 3.5 (Invariant GNN). Given a set of graphs with their labels D = {G i , y i } N i=1 and environments E all that follow the same graph generation process in Sec. 3.1, considering a GNN h θ • ρ that has a permutation invariant graph encoder h θ : G → R h and a downstream classifier ρ : R h → Y, h θ • ρ is an invariant GNN if it minimizes the worst risk at all environments, i.e., max e∈E all R e (Eq. 3).</p><p>With the above definition, we can analyze whether an invariant GNN can be learned with previous methods such as optimization objectives or architecture choices. We provide both theoretical discussions<ref type="foot" target="#foot_2">4</ref> and empirical case studies based on the synthetic BAMotif <ref type="bibr" target="#b44">(Luo et al., 2020)</ref> graphs shown in Fig. <ref type="figure">3</ref>. Specific settings are given in Appendix C.</p><p>Can GNNs optimized with ERM generalize to OOD? Standard training of GNNs adopts the empirical risk minimization (ERM <ref type="bibr" target="#b74">(Vapnik, 1991)</ref>) through minimizing the supervised losses over all training samples. However, neural networks optimized with ERM are shown to leverage the shortcuts, i.e., the parts in G that is strongly correlated with Y through S in a certain environment E = e, to make predictions <ref type="bibr" target="#b24">(Geirhos et al., 2020)</ref>. Hence, the predictions P (Y |G, E = e) can be changed drastically when switching to another environment E = e such that P (Y |G, E = e) = P (Y |G , E = e ) even G and G have the same label. In other words, predictors that rely on any subset of G controlled by S, can hardly generalize to OOD graphs, i.e., other environments. Considering a more concrete graph classification problem where all graphs contain only a single node, with strictly linear separable invariant features (i.e., f inv is linear and f gen is invertible<ref type="foot" target="#foot_3">5</ref> ), there can be infinite Bayes optimal solutions (i.e., minimizers of the ERM objective), given limited coverage of all possible S at the training data. However, there only exists one invariant GNN predictor that merely focuses on the invariant features. Therefore, GNNs optimized with ERM will fail to generalize to OOD almost surely. Empirical results on more complex samples in Fig. <ref type="figure">3</ref>(b) verified our conclusion.</p><p>Can message passing improve OOD generalization of GNNs? Aggregating neighbor information with more layers to denoise the input signal, or enhancing the expressivity with more powerful readout functions, are two common choices in GNNs to improve the generalization ability <ref type="bibr" target="#b81">(Xu et al., 2018;</ref><ref type="bibr" target="#b41">Li et al., 2018;</ref><ref type="bibr" target="#b82">Xu et al., 2019;</ref><ref type="bibr" target="#b85">Yang et al., 2021)</ref>. However, in Fig. <ref type="figure">3</ref>(c), we empirically found that GCNs with more layers and more powerful sum readout are still sensitive to distribution shifts. In particular, stacking more layers can help denoise certain shifts, while the OOD performance will drop more sharply when the bias increases. Intuitively, if the spurious features from nodes cannot be eliminated by the denoising property of a deeper GNN, they can spread among the whole graph more widely, which in turn leads to stronger spurious correlations. Besides, the spurious correlations can be more difficult to be disentangled if there are distribution shifts at both structure-level and attributelevel. Since the node representations from hidden layers can also encode graph topology features <ref type="bibr" target="#b82">(Xu et al., 2019)</ref>, distribution shifts introduced through Z s A and Z s X will doubly mix at the learned features. In the worst case, the information about Z c A and Z c X can be partially covered by or even replaced by Z s A and Z s X , which will greatly improve the difficulty or even make the OOD generalization impossible for message passing GNNs trained with ERM.</p><p>Can OOD objectives improve OOD generalization of GNNs? Recently, tackling the OOD generalization problem on Euclidean data with the invariance principle has obtained huge success <ref type="bibr" target="#b55">(Peters et al., 2016;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019;</ref><ref type="bibr" target="#b60">Sagawa* et al., 2020;</ref><ref type="bibr" target="#b39">Krueger et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>, where many objectives are proposed for regularizing the model behaviors. In the discussion, we focus on a representative objective, invariant risk minimization (IRM) <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref>, while our discussion can also be extended for other similar objectives. Specifically, IRM formulates OOD generalization problem as:</p><formula xml:id="formula_8">min θ,fc 1 |E tr | e∈Etr R e (h θ • ρ) s.t. ρ ∈ arg min ρ R e (h θ • ρ), ∀e ∈ E tr .</formula><p>Such formulation has promising theoretical properties for OOD generalization at linear regime that is also empirically verified on images <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref>. However, there are two main issues that can degenerate the performance of IRM when applying to graphs. First, non-trivial environment partitions or labels are required for provably guarantees of optimality by IRM. However, collecting environment labels or meaning partitions of graphs requires expert knowledge and understandings about these abstract data structures, and thus they can be expensive to obtain and are usually not available in many benchmarks <ref type="bibr" target="#b49">(Morris et al., 2020;</ref><ref type="bibr" target="#b21">Dwivedi et al., 2020;</ref><ref type="bibr" target="#b28">Hu et al., 2020)</ref>. Alternative options for obtaining the partition such as random partitions, tend to give little help for graph OOD generalization <ref type="bibr" target="#b17">(Creager et al., 2021)</ref>, as it can be trivially deemed as mini-batching. Second, even with environment labels, IRM can hardly generalize to OOD or even fail catastrophically at non-linear regime if there is not sufficient support overlap with the test environments, i.e.,∪ e∈Ete supp(P e ) ⊆ ∪ e∈Etr supp(P e ) <ref type="bibr" target="#b59">(Rosenfeld et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>. In addition to IRM, the failure can also happen to other alternative objectives <ref type="bibr" target="#b39">(Krueger et al., 2021;</ref><ref type="bibr" target="#b7">Bellot and van der Schaar, 2020;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref> as stated in Theorem 5.1 in <ref type="bibr" target="#b59">Rosenfeld et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Challenges of graph OOD generalization</head><p>The aforementioned failures reveal that existing OOD methods developed for Euclidean data can hardly be directly applied to graphs and pose new challenges for graph OOD generalization: a) Distribution shifts on graphs are more complicated as implied by Assumption. 3.1; b) Environment labels are usually not available due to the abstract graph data structure. Therefore, a natural question is raised: Can we generalize and instantiate the invariance principle for OOD generalization on graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Invariance Principle for Graph OOD Generalization</head><p>We provide affirmative answers to the previous question. Built upon the SCMs established in Sec. 3.1, we generalize the invariance principle for graph OOD generalization, and propose a novel framework, Graph Out Of Distribution generalization (GOOD), to instantiate the principle with provable guarantees of OOD generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Invariant subgraph for graph OOD generalization</head><p>Towards generalizing the invariance principle to graphs, observing that, for both FIIF and PIIF (Assumption 3.2, 3.3), the causal relationship C → Y is independent to environments E by the ICM assumption <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>. Furthermore, G c carries the major information of C about Y in the observed graph G, hence the correlation between G c and Y is also invariant across different environments. In other words, GNNs that merely focus on G c to predict Y can generalize to OOD graphs.</p><p>Proposition 4.1. Let G c denote the subgraph space for G c , given a set of graphs with their labels</p><formula xml:id="formula_9">D = {G (i) , y (i) } N</formula><p>i=1 and E all that follow the graph generation process in Sec. 3.1, a GNN h • ρ : G c → Y solving the following objective can generalize to OOD graphs in the sense of Definition. 3.5:</p><formula xml:id="formula_10">min θ R Gc (h θ • ρ),</formula><p>where R Gc is the empirical risk over {G</p><formula xml:id="formula_11">(i) c , y (i) } N i=1 and G (i) c is the underlying invariant subgraph G c for G (i) .</formula><p>Naturally, Proposition 4.1 paves a way towards graph OOD generalization, where an invariant predictions can be made via the following two sub-processes: a) Identifying the underlying invariant subgraph G c ; b) Predicting the label for the original graph G based on G c . Inspired by this observation, we propose the following implementation of Proposition 4.1 to learn an invariant GNN 6 (Definition. 3.5):</p><formula xml:id="formula_12">min θ R Ĝc (h θ • ρ), s.t. Ĝc = arg max Gc⊆G I( Gc ; Y ), Y ⊥ ⊥ E| Gc .</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GOOD Framework</head><p>The most difference of Eq. 4 from standard ERM is that, an invariant subgraph G c needs to be isolated from G for making predictions, which is the key to achieve OOD generalization. However, identifying G c can be difficult due to the complex entanglement of G c and G s in G as Assumption 3.1. Directly optimizing for such a complicated problem can hardly be realized without a proper architecture <ref type="bibr" target="#b83">(Xu et al., 2020</ref><ref type="bibr" target="#b84">(Xu et al., , 2021))</ref>.</p><p>Inspired by the rationales of GNN reasoning uncovered by <ref type="bibr" target="#b83">Xu et al. (2020)</ref>, we propose the GOOD framework that explicitly aligns with the two sub-processes in Eq. 4, by decoupling the model into a featurizer and a classifier. Specifically, the featurizer g : G → G c aims to identify the underlying G c , and the classifier f c : G c → Y will predict the label Y based on the estimated G c . Each input G will be processed as follows (see also Algorithm 1 in Appendix).</p><p>Featurizer. Instead of directly isolating a subgraph from G, the featurizer g predicts a soft subgraph Ĝc by learning a soft mask M and applying it to obtain the Ĝc = M G, where stands for the hadamard product. The output Ĝc is a weighted graph for properly disentangling the complex mixing of the underlying G c and G s in Assumption 3.1.</p><p>Classifier. Given the estimated Ĝc from the featurizer, the classifier f c will predict the label ŷ = f c ( Ĝc ). Once the underlying G c is successfully disentangled, the predictions made by the classifier are invariant to distribution shifts introduced by E, in the sense of Proposition 4.1.</p><p>6. We denote the identification of Gc from G with a generalized ⊆ that takes subsets of edges and features from G or its projection into some feature space, i.e., h θ (G).</p><p>Optimization objective. To train the model end-to-end, we merge I( Ĝc , Y ) the empirical risk via the variational bound <ref type="bibr" target="#b2">(Alemi et al., 2017;</ref><ref type="bibr" target="#b93">Yu et al., 2021)</ref>:</p><formula xml:id="formula_13">I( Ĝc , Y ) ≥ p(y, Ĝc ) log p(y| Ĝc )dyd Ĝc ≈ −R( Ĝc , f c ),<label>(5)</label></formula><p>hence we can further re-write Eq. 4 as:</p><formula xml:id="formula_14">min g,fc R( Ĝc , f c ), s.t.Y ⊥ ⊥ E| Ĝc , Ĝc = g(G),<label>(6)</label></formula><p>where R( Ĝc , f c ) is the empirical loss of f c based on Ĝc .</p><p>Eq. 6 provides a general approach to learn an invariant GNN. However, it is challenging to implement the independence constraint Y ⊥ ⊥ E| Ĝc into a differentiable objective, especially when the environment information E is unavailable. To mitigate this issue, we resort to utilize the properties of G c implied by SCMs in Sec. 3.1. Specifically, given a set of graphs {G (i) , y (i) } N i=1 , the underlying G c s from the same class should have high mutual information across different environments. since causal relationship is stable to environment changes. In contrast, G c and G s do not have such properties as G s is also affected by environment E.</p><p>Intuitively, if the size of the underlying G c is known and fixed, i.e., |G c | = r c , we can get rid of potential G s from the estimated Ĝc by maximizing the mutual information between different Ĝc s drawn from the same class. Hence, we can rephrase Eq. 6 as the following objective (GOODv1): max g,fc</p><formula xml:id="formula_15">I( Ĝc ; Y ) + I( Ĝc ; Gc |Y ), s.t. |g(G)| ≤ r c , ∀G,<label>(7)</label></formula><p>where Ĝc = g(G), Gc = g( G) and G ∼ P (G|Y ), i.e., G and G have the same label. However, the size of G c is usually unknown or changes for different C. In this circumstance, maximizing Eq. 7 without constraints can lead to the presence of G s in Ĝc , causing predictions unstable to distribution shifts. For instance, Ĝc = G is a trivial solution to Eq. 7 when r c = ∞.</p><p>To circumvent this limitation, we further resort to the properties of G s . Intuitively, G s or the left part after isolating Ĝc from G, i.e., G − Ĝc , should also have high mutual information between Y , for both FIIF and PIIF. Hence, we can enforce G − Ĝc to "compete" with Ĝc when maximizing their mutual information between Y and them, respectively. Accordingly, we further derive GOODv2 as the following: max g,fc</p><formula xml:id="formula_16">I( Ĝc ; Y ) + I( Ĝc ; Gc |Y ) + I( Ĝs ; Y ), s.t. I( Ĝs ; Y ) ≤ I( Ĝc ; Y ), Ĝs = G − Ĝc ,<label>(8)</label></formula><p>where ( Ĝc , G) are the same as Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Theoretical Analysis and Practical Discussions</head><p>We derive theoretical guarantees to show how objectives Eq. 7 and Eq. 8 can help identify the underlying G c . Then, we introduce the practical implementations for Eq. 7 and Eq. 8, as well as further implications of our theory.</p><p>Theorem 4.2 (GOOD Induces Invariant GNN). Given a set of graphs with their labels</p><formula xml:id="formula_17">D = {G (i) , y (i) } N i=1</formula><p>and E all that follow the graph generation process in Sec. 3.1 with invertible f c gen and noises set U s.t. H(U ) → 0, assuming that the samples from each training environments with |E tr | ≥ 2 are equally distributed, considering a model g • f c induced by GOOD embedded with sufficient expressive permutation invariant encoders h : G c → R h that can differentiate every graph in the support of all environments ∪ G e ∈∪ e∈E all supp(P e ) P(G e ) where P(G e ) is the power set of G e : (i) if |G c | = r c , ∀G is known, and the size of the estimated subgraph is constrained Ĝc = g(G) ≤ r c , ∀G, then each solution g • f c to Eq. 7, elicits an invariant GNN (Definition. 3.5) in the sense of Proposition 4.1. (ii) each solution g • f c to Eq. 8, elicits an invariant GNN (Definition. 3.5) in the sense of Proposition 4.1.</p><p>We prove Theorem 4.2 (i) and (ii) in Appendix D.2, D.3, respectively. Intuitively, merely maximizing the mutual information between Y and Ĝc is essentially performing ERM that may introduce spurious subsets G p s ⊆ G s into Ĝc . In contrast, simultaneously maximizing I( Ĝc ; Gc |Y ) with the competition constraint can serve as a regularization that prunes those redundant G p s . Practical implementations of GOOD objectives. After showing the power of GOOD, we introduce the practical implementations of GOODv1 and GOODv2 objectives. The approximation of the first term I( Ĝc ; Y ) can be achieved via the variational bound as Eq. 5. While the exact estimation of the second term I( Ĝc ; Gc |Y ) can be expensive, contrastive learning provides a practical solution for its approximation <ref type="bibr" target="#b15">(Chopra et al., 2005;</ref><ref type="bibr" target="#b61">Salakhutdinov and Hinton, 2007;</ref><ref type="bibr" target="#b73">van den Oord et al., 2018;</ref><ref type="bibr" target="#b6">Belghazi et al., 2018)</ref>:</p><formula xml:id="formula_18">I( Ĝc ; Gc |Y ) ≈ E ( Ĝ, G)∼P( Ĝ, G|y) {G i } M i=1 ∼P(G|Y\y) σ( Ĝ, G, {G i } M i=1 ),<label>(9)</label></formula><p>where σ( Ĝ, G, {G i } M i=1 ) is defined as:</p><formula xml:id="formula_19">σ( Ĝ, G, {G i } M i=1 ) = log e φ(h Ĝc ,h Gc ) e φ(h Ĝc ,h Gc ) + M i e φ(h T Ĝc h G i c )</formula><p>, where φ represents a similarity metric, e.g. cosine similarity, and Ĝc = g( Ĝ), Gc = g( G), G i c = g(G i ). As |M | → ∞, Eq. 9 approximates I( Ĝc ; Gc |Y ) in the sense of non-parameteric resubstitution entropy estimator via von Mises-Fisher kernel density estimation <ref type="bibr" target="#b0">(Ahmad and Lin, 1976;</ref><ref type="bibr" target="#b31">Kandasamy et al., 2015;</ref><ref type="bibr" target="#b77">Wang and Isola, 2020)</ref>. Hence, plugging it into Eq. 7, 8 can relieve the issue when approximating I( Ĝc ; Gc |Y ) in practice.</p><p>As for the third term I( Ĝs ; Y ) and the constraint I( Ĝs ; Y ) ≤ I( Ĝc ; Y ), a practical implementation is to follow the idea of hinge loss:</p><formula xml:id="formula_20">I( Ĝs ; Y ) ∼ 1 N R Ĝs • I(R Ĝs ≤ R Ĝc ), (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>where N is the number of samples, I is a indicator function that outputs 1 when the interior condition is satisfied otherwise 0, and R Ĝs and R Ĝc are the empirical risk vector of the predictions for each sample based on G s and G c respectively. We leave more details about our implementation in the experiments in Appendix E.2. Significance and implications of GOOD. Although primarily serving for graph OOD generalization problem, our theory complements the identifiability study on graphs through contrastive learning, and aligns with the discoveries in the image domain that contrastive learning learns to isolate the content (C) and style (S) <ref type="bibr" target="#b96">(Zimmermann et al., 2021;</ref><ref type="bibr" target="#b40">Kügelgen et al., 2021)</ref>. Moreover, our theoretical analysis also partially explains the empirical success of graph contrastive learning <ref type="bibr" target="#b91">(You et al., 2020;</ref><ref type="bibr" target="#b46">Ma et al., 2021;</ref><ref type="bibr" target="#b92">You et al., 2021)</ref>, where the GNN may implicitly learn to identify the underlying invariant subgraph. Besides, the framework of GOOD can also be applied to other tasks involving relational information such as node classification. Thorough discussions and more details are given in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct extensive experiments with 13 datasets including both synthetic data and real-world data that have various distribution shifts, to comprehensively verify our theoretical findings and the effectiveness of GOOD. We leave more details about our experimental setup in Appendix E due to space constraint.</p><p>Datasets. According to graph generation process in Sec. 3.1, we use BAMotif <ref type="bibr" target="#b89">(Ying et al., 2019;</ref><ref type="bibr" target="#b44">Luo et al., 2020;</ref><ref type="bibr" target="#b79">Wu et al., 2022b)</ref> to construct 3-class SPMotif datasets with artificial FIIF distribution shifts at the structural level (SPMotif-Struc), and a more difficult version mixed with FIIF distribution shifts at attribute level (SPMotif-Mixed). We also use four of TU datasets <ref type="bibr" target="#b49">(Morris et al., 2020)</ref> to with distribution shifts at graph sizes <ref type="bibr" target="#b86">(Yehudai et al., 2021;</ref><ref type="bibr" target="#b9">Bevilacqua et al., 2021)</ref>. Meanwhile, we convert the colored mnist dataset in IRM <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref> using the same algorithm as <ref type="bibr" target="#b36">Knyazev et al. (2019)</ref>, and also split Graph-SST <ref type="bibr" target="#b94">(Yuan et al., 2020)</ref> to inject degree biases, to cover more types of distribution shifts that possibly appear in graphs. More details about the datasets can be found in Appendix E.1.</p><p>Baselines and our methods. In terms of optimization objectives, we compare GOOD to the state-of-the-art OOD objectives including IRM <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref>, v-Rex <ref type="bibr" target="#b39">(Krueger et al., 2021)</ref> and IB-IRM <ref type="bibr" target="#b1">(Ahuja et al., 2021)</ref>, where we generate random environment partitions for these methods. Besides, we also compare GOOD with the only OOD objective EIIL <ref type="bibr" target="#b17">(Creager et al., 2021)</ref> that does not require environment information. In terms of architectures, we compare our methods to the stateof-the-art interpretable GNN methods GIB <ref type="bibr" target="#b93">(Yu et al., 2021)</ref>, ASAP Pooling <ref type="bibr" target="#b57">(Ranjan et al., 2020)</ref>, and DIR <ref type="bibr" target="#b79">(Wu et al., 2022b</ref>) that enables OOD GNN explainability. We select the interpretability ratio (i.e., r c ) according to the validation performances for both GNN explainability baselines and GOOD. We use a 3-layer GNN encoder for all methods for fair comparison. More implementation details are given in Appendix E.4.</p><p>Evaluation. We report classification accuracy for all datasets by default, except for TU datasets where we use Matthews correlation coefficient following the previous work <ref type="bibr" target="#b9">(Bevilacqua et al., 2021)</ref> We repeat the evaluation multiple times, select model based on its performance at the validation set during each run, and report the mean and standard deviation of the corresponding metric.</p><p>OOD generalization performance on structure and mixed shifts. In Table <ref type="table" target="#tab_1">1</ref>, we report the training accuracy and testing accuracy of each method, whereas we omit GIB due to its consistent divergence. Different biases indicate different levels of distribution shifts. First, it can be found that most of the methods converge to more than 99% training accuracy, while the test accuracy decreases dramatically as the bias increases and as the distribution shifts are mixed, concluding our discussions in Sec. 3.2. Moreover, since G c and G s in the test data have appeared during training, OOD objectives or even interpretable baselines without specialized design for OOD generalization can improve the test accuracy in a certain level. However, GOODv1 outperforms most previous methods up to 10% and  GOODv2 further pushes the state-of-the-art further up to 12%, which verifies the effectiveness of GOOD.</p><p>OOD generalization performance on graph size shifts. In Table <ref type="table" target="#tab_2">2</ref>, we additionally compare GOOD to the specialized designed methods for OOD generalization in terms of graph sizes (Γ GNNs) <ref type="bibr" target="#b9">(Bevilacqua et al., 2021)</ref>, where we include the author reported results for both kernel methods and Γ GNNs. It can be found that GOOD consistently and significantly outperforms the previous state-of-the-art, which further demonstrates the generality of GOOD.</p><p>OOD generalization performance on other shifts. In Table <ref type="table" target="#tab_3">3</ref>, we experiment with more types of distribution shifts that can appear on graphs. Notably, since the underlying generation processes of these datasets are more complex, the ground truth ratio r c may not be fixed. Hence, the superiority of GOODv1 can be degraded when compared to other baselines, while GOODv2 keeps its outstanding OOD generalization ability that outperforms all of the other methods by a significant margin, serving for a strong evidence for our theoretical conclusion and its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We studied the OOD generalization problem in graph classification. Through the lens of causality, we established SCMs for characterizing the distribution shifts that can happen on graphs, generalized and instantiated the invariance principle to graphs via a novel framework GOOD that has promising theoretical and empirical OOD generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. More Related Works and Discussions</head><p>A.1 More related works GNN explainability. Works in GNN explanation also aim to find a subgraph of the input as the explanations for the prediction of a GNN model <ref type="bibr" target="#b89">(Ying et al., 2019;</ref><ref type="bibr" target="#b94">Yuan et al., 2020)</ref>. Though some may leverage causality in explanation generation <ref type="bibr" target="#b42">(Lin et al., 2021)</ref>, they mostly focus on understanding the predictions of GNNs instead of OOD generalization. Recently there are two works aiming to provide robust explanations against distribution shifts <ref type="bibr" target="#b11">(Chang et al., 2020;</ref><ref type="bibr" target="#b79">Wu et al., 2022b)</ref>. However, they can hardly be adapted to identify the invariant subgraph for OOD generalization nor provide theoretical guarantees in our circumstances.</p><p>GNN extrapolation. Recently there is a surge of attention in improving the extrapolation ability of GNNs and apply them to various applications, such as mathematical reasoning <ref type="bibr" target="#b63">(Santoro et al., 2018;</ref><ref type="bibr" target="#b64">Saxton et al., 2019)</ref>, physics <ref type="bibr" target="#b4">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b62">Sanchez-Gonzalez et al., 2018)</ref>, and graph algorithms <ref type="bibr" target="#b71">(Tang et al., 2020;</ref><ref type="bibr" target="#b75">Velickovic et al., 2020;</ref><ref type="bibr" target="#b83">Xu et al., 2020;</ref><ref type="bibr" target="#b80">Xhonneux et al., 2021)</ref>. <ref type="bibr" target="#b84">Xu et al. (2021)</ref> study the neural network extrapolation ability from a geometrical perspective. <ref type="bibr" target="#b36">Knyazev et al. (2019)</ref>; <ref type="bibr" target="#b86">Yehudai et al. (2021)</ref> focus on the extrapolation of GNNs in terms of graph sizes, while making additional assumptions on the knowledge about ground truth attentions and access to test inputs. <ref type="bibr" target="#b9">Bevilacqua et al. (2021)</ref> study the graph size extrapolation problem of GNNs through a causal lens, while the induced the invariance principle is built upon assumptions on the specific family of graphs. <ref type="bibr" target="#b85">Han et al. (2021)</ref> improve OOD drug discovery by mitigating the overconfident misprediction issue. Different from these works, we consider the GNN extrapolation as a causal problem, establish generic SCMs that are compatible with several graph generation models, induce the invariance principle upon SCMs and instantiate it with theoretical guarantees. 7  Causality and invariant learning. Causality comes to the stage for demystifying and improving the huge success of machine learning algorithms to further advances <ref type="bibr">(Pearl, 2019;</ref><ref type="bibr" target="#b65">Schölkopf, 2019;</ref><ref type="bibr" target="#b66">Schölkopf et al., 2021)</ref>. One of the most widely applied concept from causality is the Independent Causal Mechanism (ICM) that assumes conditional distribution of each variable given its causes (i.e., its mechanism) does not inform or influence the other conditional distributions <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>. The invariance principle is also induced from the ICM assumption. Once proper assumptions about the underlying data generation process via Structural Causal Models (SCM) are established, it is promising to apply the invariance principle to machine learning models for finding an invariant representation about the causal relationship between the underlying causes and the label. Consequently, models built upon the invariant representation can generalize to unseen environments or domains with guaranteed performance <ref type="bibr" target="#b55">(Peters et al., 2016;</ref><ref type="bibr" target="#b58">Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019;</ref><ref type="bibr" target="#b60">Sagawa* et al., 2020;</ref><ref type="bibr" target="#b8">Bengio et al., 2020;</ref><ref type="bibr" target="#b38">Koyama and Yamaguchi, 2020;</ref><ref type="bibr" target="#b25">Gulrajani and Lopez-Paz, 2021;</ref><ref type="bibr" target="#b39">Krueger et al., 2021;</ref><ref type="bibr" target="#b17">Creager et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021)</ref>. Different from domain adaption or transfer learning <ref type="bibr" target="#b58">(Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b15">Chuang et al., 2020)</ref> which aims to learn the invariant information shared across source domain and target domain and generically assumes the availability of target domain data, the out of distribution (OOD) generalization problem studied in this paper does not pose such assumptions hence the learned model has to get rid of all spurious correlations introduced via S to be min-max optimal (Eq. 3) and generalize to OOD data with provable guarantees. We refer interested readers to Pearl (2019); Schölkopf (2019); <ref type="bibr" target="#b66">Schölkopf et al. (2021)</ref> for in-depth understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More discussions on connections of GOOD with existing works</head><p>Although primarily serving for graph OOD generalization problem, our theory complements the identifiability study on graphs through contrastive learning, and aligns with the discoveries in the image 7. We provide additional details in Appendix C.3.</p><p>domain that contrastive learning learns to isolate the content (C) and style (S) <ref type="bibr" target="#b96">(Zimmermann et al., 2021;</ref><ref type="bibr" target="#b40">Kügelgen et al., 2021)</ref>. Moreover, our theoretical analysis also partially explains the empirical success of graph contrastive learning <ref type="bibr" target="#b91">(You et al., 2020;</ref><ref type="bibr" target="#b46">Ma et al., 2021;</ref><ref type="bibr" target="#b92">You et al., 2021)</ref>, where the GNN may implicitly learn to identify the underlying invariant subgraph. Besides, the framework of GOOD can also be applied to other tasks involving relational information such as node classification. While we sample positive samples according to the labels, the previous contrastive learning with augmentations can also be applied to Eq. 9 for reducing the sample complexity. supervised contrastive loss in image domain <ref type="bibr" target="#b32">(Khosla et al., 2020)</ref>.</p><p>On expressivity of GOOD. The expressivity of GOOD is essentially constrained by the encoders embedded for learning graph representations. During isolating G c from G, if the encoder can not differentiate two isomorphic graphs G c and G c ∪ G p s where G p s ⊆ G s , then the featurizer will fail to identify the underlying invariant subgraph. Moreover, the classifier will also fail if the encoder can not differentiate two non-isomorphic G c s from different classes. Thus, adopting more powerful graph representation encoders into GOOD can improve the OOD generalization.</p><p>On GOOD and graph information bottleneck. Under the FIIF assumption on latent interaction, the independence condition in Eq. 4 can also be rewritten as Y ⊥ ⊥ S| Ĝc , which further implies Y ⊥ ⊥ S| Ĝc . Hence it is natural to use Information Bottleneck (IB) objective <ref type="bibr" target="#b72">(Tishby et al., 1999)</ref> to solve for G c : min</p><formula xml:id="formula_22">θ R Gc (h θ • ρ), s.t. G c = arg max Ĝc⊆G I( Ĝc , Y ) − I( Ĝc , G),<label>(11)</label></formula><p>which explains the success of many existing works in finding predictive subgraph through IB <ref type="bibr" target="#b93">(Yu et al., 2021)</ref>. However, the estimation of I( Ĝc , G) is notoriously difficult due to the complexity of graph, which can lead to unstable convergence as observed in our experiments. In contrast, optimization with contrastive objective as Eq. 9 induces smooth and stable convergence. Moreover, <ref type="bibr" target="#b1">Ahuja et al. (2021)</ref> introduce IB objective into IRM <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref> to tackle OOD generalization in linear regime under FIIF assumption, However, it can not be directly applied to our case due to the assumption of Gaussian prior on G c for FIIF and the failure of IRM in the non-linear regime <ref type="bibr" target="#b59">(Rosenfeld et al., 2021)</ref> for PIIF.</p><p>On GOOD for node classification. As the task of node classification can be viewed as graph classification based on the ego-graphs of a node, our analysis and discoveries can generalize to node classification. More specifically, the invariance principle for node classification can be implemented by identifying an invariant subgraph from the K-hop neighbor graph of each node, and making predictions based on it, i.e., Y ⊥ ⊥ E|G ego c ⊆ G ego u for node u <ref type="bibr" target="#b78">(Wu et al., 2022a)</ref>. </p><formula xml:id="formula_23">S C E G Z c X Z c A Z s X Z s A G c G s G (a) Graph Generation Model E S Y G C (b) FIIF SCM E S Y G C (c) PIIF SCM E S 1 C S 2 Y G (d) MIIF SCM</formula><formula xml:id="formula_24">Y := f inv (C), S := f spu (Y, E), G := f gen (C, S). Assumption B.4 (MIIF SCM). Y := f inv (C), S 1 := f spu (C, E), S 2 := f spu (Y, E), G := f gen (C, S 1 , S 2 ).</formula><p>Supplementary to the graph generation process in Sec. 3.1, we provide full SCMs on the graph generation process in this section as shown in Specifically, the graph generation processes are controlled by f gen : Z → G. Shown as Fig. <ref type="figure" target="#fig_2">4</ref>(a), given the variable partitions C and S at the latent space Z, they control the generation of the adjacency matrix and features for the invariant subgraph G c and spurious subgraph G s through two pairs of latent variables (Z c A , Z c X ) and (Z s A , Z s X ), respectively. Then, G c and G s are entangled into the observed graph G through f G gen . It can be a simply JOIN of a G c with one or multiple G s , or more complex generation processes controlled by the latent variables <ref type="bibr" target="#b67">(Snijders and Nowicki, 1997;</ref><ref type="bibr" target="#b43">Lovász and Szegedy, 2006;</ref><ref type="bibr" target="#b90">You et al., 2018;</ref><ref type="bibr" target="#b45">Luo et al., 2021;</ref><ref type="bibr" target="#b86">Yehudai et al., 2021)</ref>.</p><p>Moreover, a subset of environment latent variable E G ⊆ E will affect the generation of G and G s . Thus, graphs collected from different environments can have different structure-level properties such as degrees, graph sizes, homophily, as well as feature-level properties <ref type="bibr" target="#b36">(Knyazev et al., 2019;</ref><ref type="bibr" target="#b86">Yehudai et al., 2021;</ref><ref type="bibr" target="#b9">Bevilacqua et al., 2021;</ref><ref type="bibr" target="#b13">Chen et al., 2022)</ref>.</p><p>As for the interaction between C and S at the latent space, we categorize the interaction modes into Fully Informative Invariant Features (FIIF, Fig. <ref type="figure" target="#fig_3">4(b)</ref>), and Partially Informative Invariant Features (PIIF, Fig. <ref type="figure" target="#fig_3">4(c</ref>)), depending on whether the latent invariant part C is fully informative about label Y , i.e., (S, E) ⊥ ⊥ Y |C. It is also possible that FIIF and PIIF are entangled into a Mixed Informative Invariant Features (MIIF,Fig. <ref type="figure" target="#fig_3">4(d)</ref>). We follow <ref type="bibr" target="#b3">Arjovsky et al. (2019)</ref>; <ref type="bibr" target="#b1">Ahuja et al. (2021)</ref> to formulate the SCMs for FIIF and PIIF, where we omit noises for simplicity <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>. Since MIIF is built upon FIIF and PIIF, we will focus on the axiom interaction modes (FIIF and PIIF) in this paper, while most of our discussions can be extended to MIIF or more complex interactions built upon FIIF and PIIF.</p><p>C.1 More empirical details about failure case study at Sec. 3.2</p><p>We conduct our case studies based on GCN, GIN with mean or sum readout <ref type="bibr" target="#b35">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b82">Xu et al., 2019)</ref>, and hidden dimension of 64, optimized with either ERM or IRM <ref type="bibr" target="#b74">(Vapnik, 1991;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019)</ref>. As the environment partitions are not available, we generate 2 environments with random partitions. During training, we use a batch size of 32, learning rate of 1e − 3 with Adam optimizer <ref type="bibr" target="#b33">(Kingma and Ba, 2015)</ref>, batch normalization between hidden layers <ref type="bibr" target="#b30">(Ioffe and Szegedy, 2015)</ref>, and early stop the training when the validation accuracy does not increase till 5 epoch after first 20 epochs. All of the experiments are repeated 5 times.</p><p>We construct 3-class synthetic datasets based on BAMotif <ref type="bibr" target="#b44">(Luo et al., 2020)</ref>, where the model needs to tell which motif the graph contains. We inject the distribution shifts in the training data while keep the testing data and validation data without the biases. For structure-level shifts, we introduce the artificial bias based on FIIF, where the motif and the base graph are spuriously correlated with a probability of various bias. For mixed shifts, we additionally introduced attribute-level shifts based on FIIF, where all of the node features are spuriously correlated with a probability of various bias. The number of training graphs is 600 for each class and the number of graphs in validation and testing set is 200 for each class. More construction details are given in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Theoretical examples for failure case study at Sec. 3.2</head><p>Firstly, we follow <ref type="bibr" target="#b1">Ahuja et al. (2021)</ref> to introduce the formal examples on the failures of GNNs optimized with ERM or IRM <ref type="bibr" target="#b74">(Vapnik, 1991;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019)</ref>  </p><formula xml:id="formula_25">Y := (w * inv • C) ⊕ N, N ∼ Ber(q), N ⊥ ⊥ (C, S), X ← S(C, S),</formula><p>where w * inv ∈ R nc with w * inv = 1 is the labeling hyperplane, C ∈ R nc , S ∈ R ns are the corresponding invariant and varying latent variables, N is Bernoulli binary noise with a parameter of q and identical across all environments, ⊕ is the XOR operator, S is invertible.</p><p>Given data generation process as Assumption B.1, and latent space interaction as Assumption B.2 or B.3, and strictly separable invariant features 3.4, consider a k-layer linearized GNN h θ • ρ using mean as READOUT for binary graph classification, if ∪ e∈Ete supp(P e ) ⊆ ∪ e∈Etr supp(P e ):</p><p>(i) For graphs features are generated as Definition C.1, h θ • ρ optimized with ERM or IRM will fail to generalize OOD (Eq. 3) almost surely; (ii) For graphs with more than two nodes, globally same node features generated as Definition C.1, and graph labels that are the same as global node labels, h θ • ρ optimized with ERM or IRM will fail to generalize OOD (Eq. 3) almost surely; For graph classification, if the number of nodes is fixed to one, it covers the linear classification as above. When ∪ e∈Ete supp(P e ) ⊆ ∪ e∈Etr supp(P e ), it implies the S from training environments E tr does not cover S from testing environments, while C can be covered. Moreover, the condition of strictly separable training data now can be formulated as min C∈∪ e∈E tr (C⊆G e ) sgn(w</p><formula xml:id="formula_26">* inv • C)(w * inv • C) &gt; 0.</formula><p>Recall that ERM trains the model by minimizing the empirical risk (e.g., 0-1 loss) over all training data, and IRM formulates OOD generalization as:</p><formula xml:id="formula_27">min θ,fc 1 |E tr | e∈Etr R e (h θ • ρ) s.t. ρ ∈ arg min ρ R e (h θ • ρ), ∀e ∈ E tr .<label>(12)</label></formula><p>However, both ERM and IRM can not enable OOD generalization, i.e., finding the ground truth w * inv , following the Theorem 3 from <ref type="bibr" target="#b1">Ahuja et al. (2021)</ref>:</p><p>Theorem C.2 (Insufficiency of ERM and IRM). Suppose each e ∈ E all follows Definition. C.1, C are strictly separable, bounded and satisfy the support overlap between E tr and E te , and S are bounded, if S does not support the overlap, then both ERM and IRM fail at solving the OOD generalization problem.</p><p>The reason is that, when C from all environments are strictly separable, there can be infinite many Bayes optimal solutions given training data {G e , y e } e∈Etr , while there is only one optimal solution that does not rely on S. Hence, the probability of generalization to OOD (finding the optimal solution) tends to be 0 in probability.</p><p>As for case (ii), when the GNN uses mean readout to classify more than one node graphs, assuming the graph label is determined by the node label and all of the nodes have the same label that are determined as Definition C.1, then GNN optimized with ERM and IRM will also fail because of the same reasons as case (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Discussions on the failures of previous OOD related solutions</head><p>First of all, for IRM or similar objectives <ref type="bibr" target="#b60">(Sagawa* et al., 2020;</ref><ref type="bibr" target="#b39">Krueger et al., 2021;</ref><ref type="bibr" target="#b1">Ahuja et al., 2021;</ref><ref type="bibr" target="#b7">Bellot and van der Schaar, 2020</ref>) that requires environment information or non-trivial data partitions, they can hardly be applied to graphs due to the lack of such information. The reason is that obtaining such information can be expensive due to the abstraction of graphs. Moreover, as proved in Theorem 5.1 of <ref type="bibr" target="#b59">Rosenfeld et al. (2021)</ref>, when there is not sufficient support overlap between training environments and testing environments, the IRM or similar objectives can fail catastrophically when being applied to non-linear regime. The only OOD objective EIIL <ref type="bibr" target="#b17">(Creager et al., 2021)</ref> that does not require environment labels, also rely on similar assumptions on the support overlap. We also empirically find the failure of it at our experiments.</p><p>Moreover, since part of explainability works also try to find a subset of the inputs for interpretable prediction robustly against distribution shifts. Here we also provide a discussion for these works. The first work following this line is InvRAT <ref type="bibr" target="#b11">(Chang et al., 2020)</ref>, which develops an information-theoretic objective: min</p><formula xml:id="formula_28">g,fc max fs R(g • f c , Y ) + λh(R(g • f c , Y ) − R e (g • f s , Y, E)).<label>(13)</label></formula><p>However, it also requires extra environment label for optimization that are often unavailable in graphs.</p><p>Besides, the corresponding assumption on the data generation for guaranteed performance is essentially PIIF if applied to our case, while it can not provide any theoretical guarantees on FIIF. We also notice a recent work, DIR <ref type="bibr" target="#b79">(Wu et al., 2022b</ref>) that proposes an alternative objective which does not require environment label:</p><formula xml:id="formula_29">min E s [R(h, Y |do(S = s))] + λVar s ({R(h, Y |do(S = s))}).<label>(14)</label></formula><p>However, the theoretical justification established for DIR (Theorem 1 to Corollary 1 in <ref type="bibr" target="#b79">Wu et al. (2022b)</ref>) essentially depends on quality of the generator g which can be prone to spurious correlations. Thus, DIR can hardly provide any theoretical guarantees when applied to our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Theories and Discussions</head><p>In this section, we provide proofs for propositions and theorems mentioned in the main paper.</p><p>The proof of Theorem D.2 is essentially to show the estimated Ĝc through Eq. 16 is the underlying G c , hence the minimizer of Eq. 16 elicits an invariant GNN predictor, according Proposition D.1. In the next, we are going to take a information-theoretic view of the first term I( Ĝc ; Y ) and the second term I( Ĝc ; Gc |Y ) to conclude the proof.</p><p>As for the first term, we are going to use the following lemmas:</p><p>Lemma D.3. If all of the environments e ∈ E tr are equally distributed, given |D tr | = N sufficient large, and R(g • f c ) → r where r is the minimal possible empirical risk, i.e., Bayes optimal, we have:</p><formula xml:id="formula_30">1 N (x i ,y i )∈Dtr R(x i , y i ) = R e = r, H(Φ) = H e (Φ), ∀e ∈ E tr .</formula><p>Given Lemma D.3 and the fact that the empirical risk of R(g • f c ) approximates the mutual information I( Ĝc ; Y ) as an lower bound (Eq. 5, or cf., <ref type="bibr" target="#b2">Alemi et al. (2017)</ref>; <ref type="bibr" target="#b93">Yu et al. (2021)</ref>), when the optimal empirical risk is achieved, it suffices to know the I( Ĝc ; Y ) is maximized.</p><p>Obviously, when Ĝc = G c is a maximizer of</p><formula xml:id="formula_31">I( Ĝc ; Y ) = I(C; Y ) = H(Y ), since f c gen : C → G c is invertible and C causes Y .</formula><p>However, there might be some subset G p s ⊆ G s from the underlying G s that entail the same information about label, i.e., To avoid the appearance of spuriously correlated G s in Ĝc , we will use the second term to eliminate it: max g,fc  Since conditioning will lower the entropy for both discrete and continuous variables <ref type="bibr" target="#b16">(Cover and Thomas, 2006;</ref><ref type="bibr" target="#b87">Yeung, 2008)</ref>, we have: ∆I( Ĝc , E = ê; Gc , E = ẽ|Y ) &lt; 0, (25) which implies the existence of Ĝp s in Ĝc will lower down the second term in Eq. 16 for the case (i). For (ii), we have:</p><formula xml:id="formula_32">I(G l c ∪ G p s ; Y ) = I(G c ; Y ) where Ĝc = G p c ∪ G p s and G l c = G c ∩ Ĝc . For FIIF (Assumption 4(b)), it can not happen otherwise, let G p c = G c − G l c , then we have: I(G l c ∪ G p s ; Y ) = I(G l c ∪ G p c ; Y ) I(G l c ; Y ) + I(G p s ; Y |G l c ) = I(G l c ; Y ) + I(G p c ; Y |G l c ) I(G p s ; Y |G l c ) = I(G p c ; Y |G l c ) H(Y |G l c ) − H(Y |G l c , G p s ) = H(Y |G l c ) − H(Y |G l c , G p c ) H(Y |G l c ) − H(Y |G l c , G p s ) = H(Y |G l c ), H(Y |G l c , G p s ) = 0,<label>(17)</label></formula><formula xml:id="formula_33">I( Ĝc ; Gc |Y ), = H( Ĝc |Y ) − H( Ĝc | Gc , Y ),<label>(18)</label></formula><formula xml:id="formula_34">H( Ĝc , E = ê| Gc , E = ẽ, Y ) = H( Gl c , Gp s , E = ê| Gl c , Gp s , E = ẽ, Y ) = H( Ĝp s | Gl c , Gp s , E = ẽ, Y, Ĝl c , E = ê) + H( Ĝl c , E = ê| Gl c , Gp s , E = ẽ, Y ),<label>(26)</label></formula><p>Given Lemma D.5, we know Ĝc at least contains some subset of the underlying G c , otherwise the constraint I( Ĝs ; Y ) ≤ I( Ĝc ; Y ) will be violated if G c ⊆ Ĝs . Then, consider different Ĝc that:</p><p>(i) contains a subset of the underlying G c ;</p><p>(ii) contain the underlying G c as well as part of the underlying G s ; If the two scenarios (i) and (ii) can not hold when optimizing Eq. 28, then we can conclude that Ĝc can only contain the underlying G c .</p><p>First of all, for case (i), considering maximizing I( Ĝc ; Gc |Y ), similar to the proof for Theorem D.2, is essentially maximizing I( Ĝc , E = ê; Gc , E = ẽ|Y ), ∀ê, ẽ ∈ E tr . Since G c ⊥ ⊥ E, let ĝ be a solution for case (i), if ∃G p c ⊆ G c while G p c ⊆ ĝ(G) for some G, adding the left G p c to ĝ(G) can always enlarge I( Ĝc ; Gc |Y ), while not affecting the optimization of I( Ĝs ; Y ) + I( Ĝc ; Y ). Thus, case (i) can not hold due to its sub-optimality.</p><p>As for case (ii), let Ĝ * c and G * c be the ground truth G c s for Ĝ and G, Ĝs and Gs be the ground truth G s s for Ĝ and G, and Ĝp s ⊆ Ĝs and Gp s ⊆ Gs be subsets from G s s included in estimated Ĝc and Gc , respectively.</p><p>Recall that maximizing I( Ĝc ; Gc |Y ) is essentially maximizing I( Ĝc , E = ê; Gc , E = ẽ|Y ), ∀ê, ẽ ∈ E tr , hence, we have: max g,fc</p><formula xml:id="formula_35">I( Ĝc ; Gc |Y ), = I( Ĝc , E = ê; Gc , E = ẽ|Y ) = H( Ĝc , E = ê|Y ) − H( Ĝc , E = ê| Gc , E = ẽ, Y ).<label>(29)</label></formula><p>We claim Eq. 29 can eliminate any potential subsets in the estimated Ĝc .</p><p>Note that now Ĝc = Ĝ * c ∪ Ĝp s and Gc = G * c ∪ Gp s . Then, we have:</p><formula xml:id="formula_36">H( Ĝc , E = ê|Y ) = H(E = ê| Ĝc , Y ) + H( Ĝc |E = ê, Y ) = H( Ĝ * c ∪ Ĝp s |E = ê, Y ) = H( Ĝ * c |E = ê, Y ) + H( Ĝp s | Ĝ * c , E = ê, Y ) = H( Ĝ * c |Y ) + H( Ĝp s | Ĝ * c , E = ê, Y )<label>(30)</label></formula><p>where the second equality is due to E = ê is deterministic. As for H( Ĝc , E = ê| Gc , E = ẽ, Y ), WLOG., we can divide all of the possible cases into two:</p><p>(a) Gc contains some Gp s ⊆ Gs ;</p><p>(b) Both Ĝc and Gc contain some Ĝp s ⊆ Ĝs and Gp s ⊆ Gs , respectively.</p><p>For (a), we have:</p><formula xml:id="formula_37">H( Ĝc , E = ê| Gc , E = ẽ, Y ) = H( G * c , Gp s , E = ê| Gc , E = ẽ, Y ) = H( Gp s | Ĝc , E = ẽ, Y, G * c , E = ê) + H( G * c , E = ê| Ĝc , E = ẽ, Y ),<label>(31)</label></formula><p>Similarly, including additional Ĝp s in Ĝc brings no benefit for minimizing H( Ĝc , E = ê| Gc , E = ẽ, Y ), as Ĝs is determined given E, Y in PIIF and E, C in FIIF, i.e., H( Gp</p><formula xml:id="formula_38">s | Ĝc , E = ẽ, Y, G * c , E = ê) = 0.</formula><p>For (b), we have: </p><formula xml:id="formula_39">H( Ĝc , E = ê| Gc , E = ẽ, Y ) = H( G * c , Gp s , E = ê| G * c , Gp s , E = ẽ, Y ) = H( Ĝp s | G * c , Gp s , E = ẽ, Y, Ĝ * c , E = ê) + H( Ĝ * c , E = ê| G * c , Gp s , E = ẽ, Y ), (<label>32</label></formula><formula xml:id="formula_40">H(Y ) = I(Y ; Ĝ * c ) ≥ I(Y ; Ĝc ) = I(Y ; Ĝ * c ∪ G p s ),</formula><p>since Y is determined when given the ground truth G c . Hence, only the underlying G c maximizes the objective (Eq. 28), which implies that solving for the objective (Eq. 28) can elicit an invariant GNN predictor according to Proposition D.1. SPMotif datasets. We construct 3-class synthetic datasets based on BAMotif <ref type="bibr" target="#b89">(Ying et al., 2019;</ref><ref type="bibr" target="#b44">Luo et al., 2020;</ref><ref type="bibr" target="#b79">Wu et al., 2022b)</ref>, where the model needs to tell which one of three motifs (House, Cycle, Crane) that the graph contains. For each dataset, we generate 3000 graphs for each class at the training set, 1000 graphs for each class at the validation set and testing set, respectively. During the construction, we merely inject the distribution shifts in the training data while keep the testing data and validation data without the biases. For structure-level shifts (SPMotif-Struc), we introduce the bias based on FIIF, where the motif and one of the three base graphs (Tree, Ladder, Wheel) are artificially (spuriously) correlated with a probability of various biases, and equally correlated with the other two. Specifically, given a predefined bias b, the probability of a specific motif (e.g., House) and a specific base graph (Tree) will co-occur is b while for the others is (1 − b)/2 (e.g., House-Ladder, House-Wheel). We use random node features for SPMotif-Struc, in order to study the influences of structure level shifts. Moreover, to simulate more realistic scenarios where both structure level and topology level have distribution shifts, we also construct SPMotif-Mixed for mixed distribution shifts. We additionally introduced FIIF attribute-level shifts based on SPMotif-Struc, where all of the node features are spuriously correlated with a probability of various biases by setting to the same number of corresponding labels. Specifically, given a predefined bias b, the probability that all of the node features of a graph has label y (e.g., y = 0) being set to y (e.g., X = 0) is b while for the others is (1 − b)/2 (e.g., P (X = 1) = P (X = 2) = (1 − b)/2). More complex distribution shift mixes can be studied following our construction approach, which we will leave for future works.</p><p>TU datasets. To study the effects of graph sizes shifts, we follow <ref type="bibr" target="#b86">Yehudai et al. (2021)</ref>; <ref type="bibr" target="#b9">Bevilacqua et al. (2021)</ref> to study the OOD generalization abilities of various methods on four of TU datasets <ref type="bibr" target="#b49">(Morris et al., 2020)</ref>, i.e., PROTEINS, DD, NCI1, NCI109. Specifically, we use the data splits generated by <ref type="bibr" target="#b86">Yehudai et al. (2021)</ref> and use the Matthews correlation coefficient as evaluation metric following <ref type="bibr" target="#b9">Bevilacqua et al. (2021)</ref> due to the class imbalance in the splits. The splits are generated as follows: Graphs with sizes smaller than the 50-th percentile are assigned to training, while graphs with sizes larger than the 90-th percentile are assigned to test. A validation set for hyperparameters tuning consists of 10% held out examples from training. We also provide a detailed statistics about these datasets in table 5.</p><p>Graph-SST datasets. Inspired by the data splits generation for studying distribution shifts on graph sizes, we split the data curated from sentiment graph data <ref type="bibr" target="#b94">(Yuan et al., 2020)</ref>, that converts sentiment sentence classification datasets SST5 and SST-Twitter <ref type="bibr" target="#b68">(Socher et al., 2013;</ref><ref type="bibr" target="#b20">Dong et al., 2014)</ref> into graphs, where node features are generated using BERT <ref type="bibr" target="#b19">(Devlin et al., 2019)</ref> and the edges are parsed by a Biaffine parser <ref type="bibr" target="#b23">(Gardner et al., 2018)</ref>. Our splits are created according to the averaged degrees of each graph. Specifically, we assign the graphs as follows: Those that have smaller or equal than 50-th percentile averaged degree are assigned into training, those that have averaged degree large than 50-th percentile while smaller than 80-th percentile are assigned to validation set, and the left are assigned to test set. For SST5 we follow the above process while for Twitter we conduct the above split in an inversed order to study the OOD generalization ability of GNNs trained on large degree graphs to small degree graphs.</p><p>CMNIST-sp. To study the effects of PIIF shifts, we select the ColoredMnist dataset created in IRM <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref>. We convert the ColoredMnist into graphs using super pixel algorithm introduced by <ref type="bibr" target="#b36">Knyazev et al. (2019)</ref>. Specifically, the original Mnist dataset are assigned to binary labels where images with digits 0 − 4 are assigned to y = 0 and those with digits 5 − 9 are assigned to y = 1. Then, y will be flipped with a probability of 0.25. Thirdly, green and red colors will be respectively assigned to images with labels 0 and 1 an averaged probability of 0.15 (since we do not have environment splits) for the training data. While for the validation and testing data the probability is flipped to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Implementations of GOOD introduced in Sec. 4</head><p>Note that GOOD can have multiple implementations. For the purpose of verifying our theory, we do not choose sophisticated architectures in our experiments. Experimental results in Sec. 5 also demonstrates the promising OOD generalization ability of GOOD is general with basic GNN architecture hence can be easily integrated into the existing works from the prosperous GNN literature.</p><p>We now introduce the details of the architectures used in our experiments. Recall that GOOD decomposes a model for graph classification into two modules, i.e., a featurizer: g : G → G c and a classifier f c : G c → Y. Specifically, for the implementation of Featurizer, we choose one of the common practices GAE <ref type="bibr" target="#b34">(Kipf and Welling, 2016)</ref> for calculating the sampled weights for each edge. More formally, the soft mask is predicted through the following equation:</p><formula xml:id="formula_41">Z = GNN(G) ∈ R n×h , M = σ(ZZ T ) ∈ R n×n .</formula><p>If a sampling ratio r c is predetermined, we sample r c of total edges with the largest predicted weights as a soft estimation of Ĝc . Then, the estimated Ĝc will be forwarded to the classifier f c for predicting the labels of the original graph. Though Theorem D.2 assumes r c is known, in real applications we do not know the specific r c . Hence, in experiments, we select r c according to the validation performance. To thoroughly study the effects of I( Ĝs ; Y ) comparing to GOODv1, we stick to using the same r c and sampling process for GOODv2, while GOODv2 essentially requires less specific knowledge about ground truth r c hence achieving better empirical performance.</p><p>For the implementation of the information theoretic objectives, we will elaborate the GOODv2 while the implementation of GOODv1 can be obtained via removing the third term from GOODv2.</p><p>Recall that GOODv2 has the following formulation: , where φ represents a similarity metric, e.g.. cosine similarity, and Ĝc = g( Ĝ), Gc = g( G), G i c = g(G i ). As |M | → ∞, E q. 34 approximates I( Ĝc ; Gc |Y ) in the sense of non-parameteric resubstitution entropy estimator via von Mises-Fisher kernel density estimation <ref type="bibr" target="#b0">(Ahmad and Lin, 1976;</ref><ref type="bibr" target="#b31">Kandasamy et al., 2015;</ref><ref type="bibr" target="#b77">Wang and Isola, 2020)</ref>.</p><p>While for the third term I( Ĝs ; Y ) and the constraint I( Ĝs ; Y ) ≤ I( Ĝc ; Y ), a straightforward implementation is to imitate the hinge loss:</p><formula xml:id="formula_42">I( Ĝs ; Y ) ∼ 1 N R Ĝs • I(R Ĝs ≤ R Ĝc ), (<label>35</label></formula><formula xml:id="formula_43">)</formula><p>where N is the number of samples, I is a indicator function that outputs 1 when the interior condition is satisfied otherwise 0, and R Ĝs and R Ĝc are the empirical risk vector of the predictions for each sample based on G s and G c respectively. One can also formulate Eq. 33 from game-theoretic perspective <ref type="bibr" target="#b11">(Chang et al., 2020)</ref>. Finally, we can derive the specific loss for the optimization of GOODv2 combining Eq. 34 and Eq. 35: R Ĝc + αE ( Ĝ, G)∼P( Ĝ, G|y)</p><formula xml:id="formula_44">{G i } M</formula><p>i=1 ∼P(G|Y\y) log e φ(h Ĝc ,h Gc ) e φ(h Ĝc ,h Gc ) + M i e φ(h</p><formula xml:id="formula_45">T Ĝc h G i c ) + β 1 N R Ĝs • I(R Ĝs ≤ R Ĝc ),<label>(36)</label></formula><p>where R Ĝc , R Ĝs are the empirical risk when using Ĝc , Ĝs to predict Y through the classifier, h Ĝc is the permutation invariant representation of Ĝc which can be induced from the GNN encoder either in the featurizer or in the classifier, α, β are the weights for I( Ĝc ; Gc |Y ) and I( Ĝs ; Y ), and φ is implemented as cosine similarity. The optimization loss for GOODv1 merely contains the first two terms in Eq. 36.</p><p>The detailed algorithm for GOOD is given in the following Algorithm, assuming the h Ĝc is obtained via the graph encoder in f c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Training and Optimization in Experiments</head><p>For fair comparison, GOOD uses the same GNN architecture for GNN encoders as the baseline methods, whereas α is taken from {0.5, 1, 2, 4, 8, 16} and β is taken from {0.5, 1, 2, 4} according to the validation performances. Moreover, we also have various options for obtaining the features in Ĝc , for obtaining h Ĝc , as well as for obtaining predictions based on Ĝs . Options for obtaining the features in Ĝc are: {from g, from the raw features}. Options for obtaining h Ĝc are: {from the GNN encoder of the classifier f c with the same pooling as the classifier, from the GNN encoder of the featurizer g with a SUM global pooling, }. Options for obtaining predictions based on Ĝs are:{from another classifier with shared GNN encoder of f c , from another classifier with shared GNN encoder of f c while without gradients backwards to the encoder, from a single GNN convolution and a same pooling as f c }. We {0.01, 0.1, 1, 10} according to the validation performances at the datasets, while we stick to the authors claimed hyperparameters for the datasets they also experimented with.</p><p>Besides, we refer the implementations in DomainBed <ref type="bibr" target="#b25">(Gulrajani and Lopez-Paz, 2021)</ref> for IRM <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref>, V-Rex <ref type="bibr" target="#b39">(Krueger et al., 2021)</ref> and IB-IRM <ref type="bibr" target="#b1">(Ahuja et al., 2021)</ref>. For EIIL <ref type="bibr" target="#b17">(Creager et al., 2021)</ref>, we use the author released implementations about assigning different samples the weights for being put in each environment and calculating the IRM loss. Since the environment information is not available, we perform random partitions on the training data to obtain two equally large environments for these objectives. Moreover, we select the weights for the corresponding regularization from {0.01, 0.1, 1, 10, 100} for these objectives according to the validation performances of IRM and stick to it for others, since we empirically observe that they perform similarly with respect to the regularization weight choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Evaluation and optimization details</head><p>During the experiments, we do not turn the hyperparameters exhaustively while following the common recipes for optimizing GNNs. By default, we use Adam optimizer <ref type="bibr" target="#b33">(Kingma and Ba, 2015)</ref> with a learning rate of 1e−3 and a batch size of 32 for all models at all datasets. All GNNs in our experiments have 3 layers and Batch Normalization <ref type="bibr" target="#b30">(Ioffe and Szegedy, 2015)</ref>, where the hidden dimensions are chose from <ref type="bibr">{32, 64, 128} and finalized to 32, 32, 64, 32, 128, 128</ref> for SPMotif, Proteins and DD and NCI1, NCI109, CMNIST, SST5, and Twitter. Experiments in Sec. 5 use the GCN with mean readout <ref type="bibr" target="#b35">(Kipf and Welling, 2017)</ref> by default for all datasets except Proteins where we empirically observe better validation performance with a GIN and max readout <ref type="bibr" target="#b82">(Xu et al., 2019)</ref>. We also employ an early stopping of 5 epochs according the validation performance, after 20, 20, 5, 10, 5 training epochs for SPMotif, TU datasets, CMnist, SST5, and Twitter, in order to prevent overfitting. We also use dropout <ref type="bibr" target="#b69">(Srivastava et al., 2014)</ref> with a dropout rate of 0.5 for CMNIST, SST5, Twitter, and 0.3 for TU datasets following the practice of <ref type="bibr" target="#b9">Bevilacqua et al. (2021)</ref>. For the choice of r c we search it from {0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} according to the validation performances and set to 0.25, 0.3, 0.6, 0.7, 0.8, 0.5 for SPMotif, Proteins and DD, NCI1, NCI109, CMnist, SST5 and Twitter, respectively. We run each experiment 10 on TU datasets and 5 times for others, and report the mean and standard deviation of the corresponding metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Software and hardware</head><p>We implement our methods with PyTorch <ref type="bibr" target="#b53">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type="bibr" target="#b22">(Fey and Lenssen, 2019)</ref>. We ran our experiments on Linux Servers with 40 cores Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 256 GB Memory, and Ubuntu 18.04 LTS installed. GPU environments are varied from 4 NVIDIA RTX 2080Ti graphics cards with CUDA 10.2, 2 NVIDIA RTX 2080Ti and 2 NVIDIA RTX 3090Ti graphics cards with CUDA 11.3, and NVIDIA TITAN series with CUDA 11.3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph Out-Of-Distribution generalization framework (GOOD): The featurizer g first identifies an (orange colored) invariant subgraph G c for each input. The training of g employs a contrastive strategy so that the estimated G c s will be mapped to a latent sphere with favorable properties for provable identifiability. With the identified invariant subgraph G c , the predictions made by classifier f c based on G c are invariant to distribution shifts.</figDesc><graphic url="image-1.png" coords="2,268.97,119.08,102.77,102.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SCMs on graph distribution shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Full SCMs on Graph Distribution Shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Formal descriptions are as Assumptions B.1, B.2, B.3, B.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>through a linear binary classification problem: Definition C.1 (Linear classification structural equation model (FIIF)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where the second last equality is due to C → Y and the invertibility of f c gen : C → G c in FIIF. Obviously, it can not hold since knowing G l c , G p s can not determine Y . While for PIIF, since S ⊥ ⊥ Y |C, it follows G s ⊥ ⊥ Y |G c , which means G s can entail some information about Y other than that entailed by G c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>where</head><label></label><figDesc>Ĝc = g(G), Gc = g( G) are two positive samples drawn from the same class (i.e., condition on the same Y ). Since the all of the training environments are equally distributed, maximizing I( Ĝc ; Gc |Y ) is essentially maximizes I( Ĝc , E = ê; Gc , E = ẽ|Y ), ∀ê, ẽ ∈ E tr , hence, we have: max g,fc I( Ĝc ; Gc |Y ), = I( Ĝc , E = ê; Gc , E = ẽ|Y ) = H( Ĝc , E = ê|Y ) − H( Ĝc , E = ê| Gc , E = ẽ, Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>claim Eq. 19 can eliminate any potential subsets in the estimated Ĝc . Otherwise, suppose there are some subsets Ĝp s ⊆ Ĝs and Gp s ⊆ Gs in estimated Ĝc , Gc , where Ĝs , Gs be the corresponding underlying G s s for Ĝc , Gc . Let Ĝ * c and G * c be the ground truth G c , Ĝp c = Ĝ * c − Ĝc and Gp c = G * c − Gc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>) Similarly, H( Ĝp s | G * c , Gp s , E = ẽ, Y, Ĝ * c , E = ê) = 0 as Ĝs is determined given E, Y in PIIF and E, C in FIIF. Moreover, for H( Ĝ * c , E = ê| G * c , Gp s , E = ẽ, Y ), in FIIF, when conditioning on G * c , Gp s can not bring no additional information about Ĝ * c , while in PIIF, when conditioning on Y , Gp s can not bring no additional information about Ĝ * c either. To summarize, when maximizing including additional I( Ĝc ; Gc |Y ), including any additional G p s ⊆ G s can not bring additional benefit while I(G − Ĝc − G p s ; Y ) ≤ I(G − Ĝc ; Y ). Moreover, considering the effects of including G p s in Ĝc on the optimization of I(Y ; Ĝc ), since G c ⊆ Ĝc and all of the training environments are balanced distributed, maximizing I(Y ; Ĝc ) essentially maximizes I(Y ; Ĝc |E = e), ∀e ∈ E tr . For FIIF, including any G p s ⊂ G s in Ĝc can not contribute to I(Y ; Ĝc |E = e), since Y is determined given G c , while it will decrease I(G − Ĝc ; Y ). More formally, ∀G p s ⊆ G s , we have I(G − Ĝc − G p s ; Y ) ≤ I(G − Ĝc ; Y ), ∀G p s ⊆ G s , while I(Y ; Ĝc |E = e) = I(Y ; Ĝc + G p s |E = e), ∀e ∈ E tr . For PIIF, we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>; Y ) + I( Ĝc ; Gc |Y ) + I( Ĝs ; Y ), s.t. I( Ĝs ; Y ) ≤ I( Ĝc ; Y ), Ĝs = G − Ĝc ,(33)where Ĝc = g(G), Gc = g( G) and G ∼ P (G|Y ), i.e., G and G have the same label. In Sec. 4.3, we introduce a contrastive approximation for I( Ĝc ; Gc |Y ):I( Ĝc ; Gc |Y ) ≈ E ( Ĝ, G)∼P( Ĝ, G|y) {G i } M i=1 ∼P(G|Y\y) σ( Ĝ, G, {G i } M i=1 ),(34)where σ( Ĝ, G, {G i } M i=1 ) is defined as:σ( Ĝ, G, {G i } M i=1 ) = log e φ(h Ĝc ,h Gc )e φ(h Ĝc ,h Gc )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 3: Failures of graph OOD generalization. (a) GNNs optimized with ERM (GNN-ERM) and IRM (GNN-IRM) are required to classify whether the graph contains a "house" or "cycle" motif, where the colors represent node features. However, distribution shifts in the training exists at both structure level (From left to right: "house" mostly co-occur with a hexagon), and at attribute level (From upper</figDesc><table><row><cell>Training Data</cell><cell>Testing Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell>Struc-ERM Mixed-ERM</cell><cell></cell><cell></cell><cell>65</cell><cell>1-Layer 3-Layer</cell></row><row><cell></cell><cell>"House"</cell><cell></cell><cell>55</cell><cell></cell><cell></cell><cell>Struc-IRM Mixed-IRM</cell><cell></cell><cell></cell><cell>60</cell><cell>5-Layer 7-Layer</cell></row><row><cell></cell><cell>"House"</cell><cell>Accuracy</cell><cell>45 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>45 50 55</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell></row><row><cell></cell><cell>"Cycle"</cell><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30 35</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell>0.5</cell><cell>0.6 Data Biases 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell></cell><cell>0.4</cell><cell>0.5</cell><cell>0.6 Data Biases 0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell cols="2">(a) Failure cases of GNN-ERM and GNN-IRM.</cell><cell cols="6">(b) OOD failures of objectives.</cell><cell cols="3">(c) OOD failures of deeper</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GNNs.</cell></row></table><note>to lower: graphs nodes are mostly green colored if they contain "house", or blued colored if they contain "cycle"). GNN-ERM tend to leverage these shortcuts and predict graphs that have a hexagon or have mostly green nodes as "house". GNN-IRM tend to fail at testing data that is not covered by the training distribution. (b) Both GCNs with mean readout optimized with ERM and IRM cannot generalize to OOD, neither for structure-level shifts (Struc-) nor mixed with feature shifts (Mixed-). (c) GCNs with sum readout and more message passing layers are still sensitive to distribution shifts.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>OOD generalization performance on structure and mixed shifts in synthetic graphs in terms of accuracy.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SPMotif-Struc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SPMotif-Mixed</cell><cell></cell></row><row><cell></cell><cell cols="2">bias=0.33</cell><cell cols="2">bias=0.60</cell><cell cols="2">bias=0.90</cell><cell cols="2">bias=0.33</cell><cell cols="2">bias=0.60</cell><cell cols="2">bias=0.90</cell></row><row><cell></cell><cell>Test (↑)</cell><cell>Train</cell><cell>Test (↑)</cell><cell>Train</cell><cell>Test (↑)</cell><cell>Train</cell><cell>Test (↑)</cell><cell>Train</cell><cell>Test (↑)</cell><cell>Train</cell><cell>Test (↑)</cell><cell>Train</cell></row><row><cell>ERM</cell><cell cols="12">59.49 (3.50) 99.67 (0.08) 55.48 (4.84) 99.54 (0.07) 49.64 (4.63) 99.05 (0.79) 58.18 (4.30) 99.24 (0.31) 49.23 (4.22) 98.60 (0.42) 41.36 (3.29) 99.23 (0.38)</cell></row><row><cell>ASAP</cell><cell cols="12">56.21 (3.72) 99.57 (0.20) 55.11 (4.82) 99.33 (0.53) 47.30 (6.08) 99.59 (0.19) 55.21 (3.99) 98.47 (1.63) 52.56 (3.24) 99.05 (0.57) 41.05 (1.48) 99.45 (0.13)</cell></row><row><cell>DIR</cell><cell cols="12">59.58 (5.62) 98.53 (1.02) 59.18 (6.52) 96.22 (1.79) 47.03 (13.6) 90.22 (15.2) 67.28 (4.06) 96.70 (1.44) 58.93 (14.3) 91.07 (12.2) 38.58 (5.88) 74.14 (22.7)</cell></row><row><cell>IRM</cell><cell cols="12">60.37 (8.93) 99.57 (0.20) 54.99 (4.89) 99.39 (0.33) 53.70 (6.50) 99.61 (0.11) 58.03 (3.54) 99.04 (0.48) 49.79 (4.84) 98.38 (0.92) 41.38 (2.35) 99.51 (0.20)</cell></row><row><cell>V-Rex</cell><cell cols="12">49.91 (3.33) 98.22 (0.98) 48.87 (1.68) 98.24 (1.17) 48.60 (4.93) 97.99 (0.81) 50.10 (1.45) 98.78 (0.42) 47.81 (2.90) 98.51 (0.72) 48.04 (2.31) 98.19 (0.77)</cell></row><row><cell>EIIL</cell><cell cols="12">57.11 (4.96) 99.58 (0.11) 54.01 (5.62) 98.98 (0.96) 49.79 (11.6) 99.52 (0.39) 54.29 (4.83) 98.57 (0.70) 49.02 (3.63) 98.80 (0.54) 41.30 (2.05) 99.27 (0.19)</cell></row><row><cell>IB-IRM</cell><cell cols="12">56.53 (3.30) 99.61 (0.19) 53.03 (5.35) 99.07 (0.38) 43.94 (2.02) 99.35 (0.29) 59.67 (4.49) 98.83 (1.16) 52.47 (6.98) 97.68 (1.94) 38.34 (2.00) 99.32 (0.40)</cell></row><row><cell cols="13">GOODv1 64.84 (9.21) 99.40 (1.17) 62.23 (9.13) 99.56 (0.27) 54.03 (6.92) 98.07 (2.55) 67.50 (4.99) 99.81 (0.16) 66.77 (11.6) 99.45 (0.38) 45.11 (9.58) 97.36 (2.83)</cell></row><row><cell cols="13">GOODv2 64.75 (9.40) 96.69 (7.12) 74.76 (13.1) 99.82 (0.17) 59.06 (10.7) 98.83 (1.48) 66.82 (13.9) 93.63 (11.4) 65.33 (9.84) 96.82 (5.48) 51.60 (7.30) 98.80 (0.70)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>OOD generalization performance on graph size shifts in real-world graphs in terms of Matthews correlation coefficient.</figDesc><table><row><cell>Datasets</cell><cell>NCI1</cell><cell>NCI109</cell><cell>PROTEINS</cell><cell>DD</cell><cell>Avg</cell></row><row><cell>ERM</cell><cell cols="2">0.15 (0.05) 0.16 (0.02)</cell><cell cols="3">0.22 (0.09) 0.27 (0.09) 0.20</cell></row><row><cell>ASAP</cell><cell cols="2">0.14 (0.10) 0.15 (0.09)</cell><cell cols="3">0.15 (0.19) 0.22 (0.13) 0.17</cell></row><row><cell>GIB</cell><cell cols="2">0.13 (0.10) 0.16 (0.02)</cell><cell cols="3">0.19 (0.08) 0.01 (0.18) 0.12</cell></row><row><cell>DIR</cell><cell cols="2">0.21 (0.06) 0.13 (0.05)</cell><cell cols="3">0.25 (0.14) 0.20 (0.10) 0.20</cell></row><row><cell>IRM</cell><cell cols="2">0.17 (0.02) 0.14 (0.01)</cell><cell cols="3">0.21 (0.09) 0.22 (0.08) 0.19</cell></row><row><cell>V-Rex</cell><cell cols="2">0.15 (0.04) 0.15 (0.04)</cell><cell cols="3">0.22 (0.06) 0.21 (0.07) 0.18</cell></row><row><cell>EIIL</cell><cell cols="2">0.14 (0.03) 0.16 (0.02)</cell><cell cols="3">0.20 (0.05) 0.23 (0.10) 0.19</cell></row><row><cell>IB-IRM</cell><cell cols="2">0.15 (0.06) 0.14 (0.04)</cell><cell cols="3">0.23 (0.05) 0.19 (0.13) 0.18</cell></row><row><cell cols="3">WL kernel 0.39 (0.00) 0.21 (0.00)</cell><cell cols="3">0.00 (0.00) 0.00 (0.00) 0.15</cell></row><row><cell>GC kernel</cell><cell cols="2">0.02 (0.00) 0.00 (0.00)</cell><cell cols="3">0.29 (0.00) 0.00 (0.00) 0.08</cell></row><row><cell>Γ1-hot</cell><cell cols="2">0.17 (0.08) 0.25 (0.06)</cell><cell cols="3">0.12 (0.09) 0.23 (0.08) 0.19</cell></row><row><cell>ΓGIN</cell><cell cols="2">0.24 (0.04) 0.18 (0.04)</cell><cell cols="3">0.29 (0.11) 0.28 (0.06) 0.25</cell></row><row><cell>ΓRPGIN</cell><cell cols="2">0.26 (0.05) 0.20 (0.04)</cell><cell cols="3">0.25 (0.12) 0.20 (0.05) 0.23</cell></row><row><cell>GOODv1</cell><cell cols="2">0.22 (0.07) 0.23 (0.09)</cell><cell cols="3">0.40 (0.06) 0.29 (0.08) 0.29</cell></row><row><cell>GOODv2</cell><cell cols="2">0.27 (0.07) 0.22 (0.05)</cell><cell cols="3">0.31 (0.12) 0.26 (0.08) 0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>OOD generalization performance on other distribution shifts in real-world graphs in terms of accuracy.</figDesc><table><row><cell>Datasets</cell><cell>CMnist-sp</cell><cell>Graph-SST5</cell><cell>Twitter</cell><cell>Avg</cell></row><row><cell>ERM</cell><cell>13.96 (5.48)</cell><cell cols="3">43.89 (1.73) 60.81 (2.05) 39.56</cell></row><row><cell>ASAP</cell><cell>10.79 (1.44)</cell><cell cols="3">44.67 (1.31) 62.36 (1.40) 39.27</cell></row><row><cell>GIB</cell><cell>15.40 (3.91)</cell><cell cols="3">38.64 (4.52) 48.08 (2.27) 34.04</cell></row><row><cell>DIR</cell><cell>15.50 (8.65)</cell><cell cols="3">41.12 (1.96) 59.85 (2.98) 38.82</cell></row><row><cell>IRM</cell><cell>10.80 (0.83)</cell><cell cols="3">43.69 (1.26) 63.50 (1.23) 39.33</cell></row><row><cell>V-Rex</cell><cell>11.20 (1.07)</cell><cell cols="3">43.28 (0.52) 63.21 (1.57) 39.23</cell></row><row><cell>EIIL</cell><cell>10.30 (0.37)</cell><cell cols="3">42.98 (1.03) 62.76 (1.72) 38.68</cell></row><row><cell>IB-IRM</cell><cell>14.43 (1.04)</cell><cell cols="3">43.11 (1.75) 62.20 (1.59) 39.91</cell></row><row><cell cols="2">GOODv1 19.77 (17.1)</cell><cell cols="3">44.71 (1.14) 63.66 (0.84) 42.71</cell></row><row><cell cols="2">GOODv2 36.42 (11.5)</cell><cell cols="3">45.25 (1.27) 64.45 (1.99) 48.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>For both PIIF and FIIF, including additional Ĝp s in Ĝc can not increase H( Ĝp s | Ĝ * c , E = ê, Y ) as Ĝs is determined given E, Y in PIIF and E, C in FIIF.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Detailed statistics of selected TU datasets. Table from Yehudai et al. (2021); Bevilacqua et al. (2021).</figDesc><table><row><cell></cell><cell></cell><cell>NCI1</cell><cell></cell><cell></cell><cell>NCI109</cell><cell></cell></row><row><cell></cell><cell cols="3">all Smallest 50% Largest 10%</cell><cell cols="3">all Smallest 50% Largest 10%</cell></row><row><cell>Class A</cell><cell>49.95%</cell><cell>62.30%</cell><cell cols="2">19.17% 49.62%</cell><cell>62.04%</cell><cell>21.37%</cell></row><row><cell>Class B</cell><cell>50.04%</cell><cell>37.69%</cell><cell cols="2">80.82% 50.37%</cell><cell>37.95%</cell><cell>78.62%</cell></row><row><cell>Num of graphs</cell><cell>4110</cell><cell>2157</cell><cell>412</cell><cell>4127</cell><cell>2079</cell><cell>421</cell></row><row><cell>Avg graph size</cell><cell>29</cell><cell>20</cell><cell>61</cell><cell>29</cell><cell>20</cell><cell>61</cell></row><row><cell></cell><cell></cell><cell>PROTEINS</cell><cell></cell><cell></cell><cell>DD</cell><cell></cell></row><row><cell></cell><cell cols="3">all Smallest 50% Largest 10%</cell><cell cols="3">all Smallest 50% Largest 10%</cell></row><row><cell>Class A</cell><cell>59.56%</cell><cell>41.97%</cell><cell cols="2">90.17% 58.65%</cell><cell>35.47%</cell><cell>79.66%</cell></row><row><cell>Class B</cell><cell>40.43%</cell><cell>58.02%</cell><cell cols="2">9.82% 41.34%</cell><cell>64.52%</cell><cell>20.33%</cell></row><row><cell>Num of graphs</cell><cell>1113</cell><cell>567</cell><cell>112</cell><cell>1178</cell><cell>592</cell><cell>118</cell></row><row><cell>Avg graph size</cell><cell>39</cell><cell>15</cell><cell>138</cell><cell>284</cell><cell>144</cell><cell>746</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">. Note that FIIF and PIIF can be mixed as Mixed Informative Invariant Features (Appendix 4(d)) in several ways, while our analysis will focus on the axiom ones for the purpose of generality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">. We say C is bounded if ∃0 ≤ M &lt; ∞ such that ∀C, C ∈ C, d(C, C ) ≤ M .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">. We provide formal failure cases in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">. More details are given in Appendix C.2.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Full Causal Structural Models on Data Generation Process</head><p>Assumption B.1 (Graph Generation SCM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption B.2 (FIIF SCM).</head><p>Y := f inv (C), S := f spu (C, E), G := f gen (C, S).</p><p>Appendix C. More Details about Failure Case Studies in Sec. 3.2</p><p>In this section, we provide details on failure case studies in Sec. 3.2. We firstly elaborate the empirical evaluation setting.</p><p>D.1 Proof for Proposition 4.1 Proposition D.1. Let G c denote the subgraph space for G c , given a set of graphs with their labels D = {G (i) , y (i) } N i=1 and E all that follow the graph generation process in Sec. 3.1 (or Sec. B), a GNN h•ρ : G c → Y solving the following objective can generalize to OOD graphs in the sense of Definition. 3.5:</p><p>where R Gc is the empirical risk over {G</p><p>c is the underlying invariant subgraph G c for G (i) .</p><p>Proof. We establish the proof with independent causal mechanism (ICM) assumption in SCM <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr" target="#b56">Peters et al., 2017)</ref>. In particular, given the data generation assumption, i.e., for both FIIF (Assumption 3.2) and PIIF (Assumption 3.3), we have: ∀e,</p><p>where we call ICM for the first three equalities. From Eq. 15, it suffices to know P (Y |G c ) is invariant across different environments. Hence, a GNN predictor h • ρ : G c → Y optimized with empirical risk given G c , essentially minimizing the empirical risk across all environments, i.e., min R Gc = min max R e . Thus, if h • ρ solves min R Gc , it also solves min max R e , hence it elicits a invariant GNN predictor according to Definition. 3.5. D.2 Proof for theorem 4.2 (i) Theorem D.2 (GOODv1 Induces Invariant GNN). Given a set of graphs with their labels D = {G (i) , y (i) } N i=1 and E all that follow the graph generation process in Sec. 3.1 with invertible f c gen and noises set U s.t. H(U ) → 0, assuming that the samples from each training environments with |E tr | ≥ 2 are equally distributed, considering a model g •f c induced by GOOD embedded with sufficient expressive permutation invariant encoders h : G c → R h that can differentiate every graph in the support of all environments ∪ G e ∈∪ e∈E all supp(P e ) P(G e ) where P(G e ) is the power set of G e : each solution g • f c to the following objective, i.e., the minimizer of Eq. 9, elicits an invariant GNN (Definition. 3.5) that is able to generalize to OOD (Eq. 3) through Proposition 4.1.</p><p>where Ĝc = g(G), Gc = g( G) and G ∼ P (G|Y ), i.e., G and G have the same label.</p><p>Proof. We re-write the objective as follows: max g,fc</p><p>where Ĝc = g(G), Gc = g( G) and G ∼ P (G|Y ), i.e., G and G have the same label.</p><p>Similarly, H( Ĝp</p><p>which will further reduce the I( Ĝc , E = ê; Gc , E = ẽ|Y ) hence can not enable ∆I( Ĝc , E = ê; Gc , E = ẽ|Y ) &gt; 0.</p><p>To summarize, only the underlying G c maximizes the objective (Eq. 16), hence solving for the objective (Eq. 16) can elicit an invariant GNN predictor according to Proposition D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Proof for theorem 4.2 (ii)</head><p>Theorem D.4 (GOODv2 Induces Invariant GNN). Given a set of graphs with their labels D = {G (i) , y (i) } N</p><p>i=1 and E all that follow the graph generation process in Sec. 3.1 with invertible f c gen and noises set U s.t. H(U ) → 0, assuming that the samples from each training environments with |E tr | ≥ 2 are equally distributed, considering a model g •f c induced by GOOD embedded with sufficient expressive permutation invariant encoders h : G c → R h that can differentiate every graph in the support of all environments ∪ G e ∈∪ e∈E all supp(P e ) P(G e ) where P(G e ) is the power set of G e : each solution g • f c to the following objective, i.e., the minimizer of Eq. 9, elicits an invariant GNN (Definition. 3.5) that is able to generalize to OOD (Eq. 3) through Proposition 4.1. </p><p>where Ĝc = g(G), Gc = g( G) and G ∼ P (G|Y ), i.e., G and G have the same label.</p><p>The proof for Theorem D.4 is essentially to show the estimated Ĝc through Eq. 28 is the underlying G c , hence the minimizer of Eq. 28 elicits an invariant GNN predictor, according Proposition D.1. In the next, we are going to take a information-theoretic view of Eq. 28 separably to conclude the proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Detailed Experimental Settings</head><p>In this section, we provide more details about our experimental settings in Sec. 5, including the dataset preparation, dataset statistics, baseline and our method implementations as well as evaluation protocols. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Details about the datasets</head><p>We provide more details about the motivation and construction method of the datasets that are used in our experiments. Statistics of the datasets are presented in Table <ref type="table">4</ref>. , and weighted as Eq. 35, as R s ; Updated parameters of g, f c with respect to R Ĝc + αR c + βR s ; end for select the corresponding options according to the validation performance with several runs of random α and β, and stick to one for each dataset. As a result, we empirically find using the raw node features for Ĝc , obtaining h Ĝc via a global ADD pooling with the featurizer outputs, and obtaining predictions based on Ĝs from another classifier with shared GNN encoder of f c while without gradients backwards to the encoder, has better validation performances. Except for TU datasets where we use the outputs of g as the features of Ĝc , and obtain predictions with one GNN layer for the prediction of Ĝs empirically has better validation performances.</p><p>Moreover, as a byproduct of this paper, we will keep updating more empirical results with more architectures, formulations of Eq. 33 as well as representation options as above, serving for benchmarking different GNN architectures and fully realization of the power of GOOD in terms of OOD generalization performances on graphs.</p><p>Full codes will be made publicly available upon acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Implementations of baselines</head><p>We elaborate the details for our implementations of baselines in this section. Specifically, for GIB <ref type="bibr" target="#b93">(Yu et al., 2021)</ref> and ASAP <ref type="bibr" target="#b57">(Ranjan et al., 2020)</ref> we also use the author released codes, and for ASAP <ref type="bibr" target="#b57">(Ranjan et al., 2020)</ref>. During the implementation, we use the same r c for both GIB and ASAP. However, we empirically observe that the optimization process in GIB can be unstable during its nested optimization for approximating of the mutual information of the predicted subgraph and the input graph. We use a larger batch size of 128 or reduce the nested optimization steps to be lower than 20 for stabilizing the performance. If the optimization failed due to the instability during training, we will select the results with best validation accuracy as the final outcomes. For ASAP, similar phenomenon is also observed during the experiment for NCI1 dataset due to the choice of r c , where we set the r c to 0.7 for ASAP specifically while for others it is 0.6. For DIR <ref type="bibr" target="#b79">(Wu et al., 2022b)</ref>, we use the codes provided by the authors 8 . We select the hyperparamter for the proposed DIR regularization from 8. https://anonymous.4open.science/r/DIR/</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A nonparametric estimation of the entropy for absolutely continuous distributions (corresp.)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pi-Erh</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="375" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Invariance principle meets information bottleneck for out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Gagnon-Audet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision European Conference, Part XVI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11220</biblScope>
			<biblScope unit="page" from="472" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generalization and invariances in the presence of unobserved confounding</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10653</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The art and practice of structure-based drug design: A molecular modeling perspective</title>
		<author>
			<persName><forename type="first">Regine</forename><forename type="middle">S</forename><surname>Bohacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Mcmartin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">C</forename><surname>Guida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medicinal Research Reviews</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tutorial on ν-support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Pai-Hsuen Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Stochastic Models in Business and Industry</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="136" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding and improving graph injection attack by promoting unnoticeability</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating generalization under distribution shifts via domain-invariant representations</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ching-Yao Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2005-06">2005. June 2005. 2005. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1984" to="1994" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Wiley Series in Telecommunications and Signal Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>ISBN 0471241954</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AI for radiographic COVID-19 detection selects shortcuts over signal</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">D</forename><surname>Janizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machince Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="610" to="619" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07640</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">In search of lost domain generalization</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reliable graph neural networks for drug discovery under distributional shift</title>
		<author>
			<persName><forename type="first">Kehang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><forename type="middle">Zhe</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yusuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Roohani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Datasets and Benchmarks Track</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonparametric von mises estimators for entropies, divergences and mutual informations</title>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4204" to="4214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berton</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization with maximal invariant predictor</title>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoichiro</forename><surname>Yamaguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01883</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised learning with data augmentations provably isolates content from style</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graphdf: A discrete flow model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="7192" to="7203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving graph representation learning by contrastive regularization</title>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barakeel</forename><surname>Fanseu Kamhoua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11525</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Mullers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks for signal processing IX: Proceedings of the 1999 IEEE signal processing society workshop (cat. no. 98th8468)</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go machine learning: The story so far</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09992</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName><forename type="first">K-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Judea Pearl. The seven tools of causal inference, with reflections on machine learning</title>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="2009-02">2009. feb 2019</date>
			<publisher>Judea Pearl. Causality. Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ASAP: adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The risks of invariant risk minimization</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4467" to="4476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Causality for machine learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10500</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="612" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Classification</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="75" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Zinc 15 -ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards scale-invariant graph-related problem solving by iterative homogeneous gnns</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Allerton Conference on Communication, Control and Computing</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Principles of risk minimization for learning theory</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Towards distribution shift of node-level prediction on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">How to transfer algorithmic reasoning knowledge to learn new algorithms?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Louis-Pascal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">What can neural networks reason</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Shaolei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Rethinking graph regularization for graph neural networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4573" to="4581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Information Theory and Network Coding. 01</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9240" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5694" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Graph information bottleneck for subgraph recognition</title>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<title level="m">Explainability in graph neural networks: A taxonomic survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06196</idno>
		<title level="m">Adversarial robustness through the lens of causality</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Contrastive learning inverts the data generating process</title>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12979" to="12990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
