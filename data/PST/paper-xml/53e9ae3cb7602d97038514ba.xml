<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognition of Space-Time Hand-Gestures using Hidden Markov Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanghee</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept, of Computer Science Korea Advanced Institute of Science and Technologies</orgName>
								<address>
									<addrLine>373-1, Kusong-dong, Yusong-ku Taejeon</addrLine>
									<postCode>+8242 869 3572</postCode>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kwangyun</forename><surname>Wohn</surname></persName>
							<email>wohn@vr.kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Dept, of Computer Science Korea Advanced Institute of Science and Technologies</orgName>
								<address>
									<addrLine>373-1, Kusong-dong, Yusong-ku Taejeon</addrLine>
									<postCode>+8242 869 3572</postCode>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognition of Space-Time Hand-Gestures using Hidden Markov Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E6B5BF944CA552F8FE7760FD58988C3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand gesture recognition</term>
					<term>hidden Markov model</term>
					<term>connected hand movement pattern</term>
					<term>command-like gesture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapidly growing interest in interactive threedimensional(3D) computer environm ents highly recom mend the hand gesture as one o f their interaction modalities. Among several factors constituting a hand gesture, hand movement pattern is spatiotemporally variable and inform a tive, but its automatic recognition is not trivial. In this paper, we describe a hidden Markov(HM M )-based method for recognizing the space-time hand movement pat tern. HMM models the spatial variance and the time-scale variance in the hand movement. As for the recognition of the continuous, connected hand movement patterns, HMMbased segmentation method is introduced. To deal with the dimensional complexity caused by the 3D problem space, the plane fitting method is employed and the 3D data is re duced into 2D. These 2D data are then encoded as the input to HMMs.</p><p>In addition to the hand movement, which is regarded as the primary attribute o f the hand gesture, we also consider the hand configuration(posture) and the palm orientation. These three major attributes are processed in parallel and rather in dependently, followed by the inter-attribute communication for finding the proper interpretation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A hand gesture is a m o v e m e n t th a t w e m a k e w ith o u r h a n d s to e x p r e s s e m o tio n o r in f o r m a tio n , e it h e r i n s t e a d o f s p e a k in g o r w h ile w e a r e s p e a k in g [1],</head><p>The use o f natural hand gestures for computer-human interaction can help people to com municate with computer in more intuitive way. Moreover, recent studies on threedimensional(3D) virtual environm ent and the developments o f various 3D input devices encourage to add this kind of 3D interaction modality to the user interface design. <ref type="bibr" target="#b1">[2]</ref>, Most o f the current researches on hand gesture recognition have targeted g e s t ic u l a t io n or the s ig n la n g u a g e <ref type="bibr" target="#b1">[2]</ref> [14] <ref type="bibr" target="#b12">[13]</ref>. In case o f dealing with g e s t ic u l a t io n that deals with the simple transparent gestures as a subsidiary means of communication, most work has concentrated on the integration o f gesture and the other primary modality like speech, while the recognition of static and discrete hand configurations(postures) has been mainly studied for s ig n la n g u a g e recognition. As for the hand movement pattern, which is one o f the most important attributes in the gesture, most existing work in both cases has considered only dis tinct linear hand movements like u p , d o w n , to and f r o or has limited their recognition target to the two dimensional movement path. Hand gestures in general, however, can reveal more complex movement patterns than these. This becomes more apparent when we consider the c o m m a n d -lik e g e s tu r e s that might provide an effective way o f interaction for virtual reality and other 3D appli cations like 3D CAD. C o m m a n d -lik e g e s tu r e s in those applications typically comprise two kinds of gesturesobject description gestures and action indication ones. The object description gesture corresponds to the hand movement pattern w hich roughly draw s the corresponding shape of the object. T he action indication gesture roughly draw s the target action trajectory in the 3D space. As these are draw ing som e shape(pictographic and kinetographic, respectively), they are all involved w ith m ore com plex hand m ovem ent patterns than the distinct linear gestures. This paper discusses a recognition m ethod for the spacetim e hand gestures conducted in the 3D space, particularly involved w ith various nonlinear hand m ovem ent patterns and their connected ones. We consider the object description and action indication gestures m entioned above as the basic vocabulary that can be used as a m ethod to m anipulate the synthetic object in the virtual environm ent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The broad range of free hand gesture includes g e s t ic u l a t io n , l a n g u a g e -lik e g e s tu r e s , p a n to m im e s , e m b le m s , and s ig n l a n g u a g e s</head><p>T his 3D hand gesture recognition problem has the follow ing characteristics:</p><p>• T e m p o r a l v a r i a n c e : Space-tim e gesture is generated w ith nonuniform scale in the speed. B oth inter-person variation and intra-person variation exist.</p><p>• S p a t i a l c o m p l e x i t y : B asically, this com plexity com es from the hum an variability in the 3D space. T his com plexity is due to the follow ing aspects.</p><p>-large variation o f the shape -rotational variance -translational variance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• N o s t a r t i n g / e n d i n g p o i n t :</head><p>T here is no explicit indica tions o f starting and ending o f the gesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• R e p e a t a b i l i t y a n d c o n n e c t i v i t y</head><p>: T he repeatability and connectivity o f gesture patterns add difficulties because the recognition process has to deal w ith segm entation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• M u l t i p l e a t t r i b u t e s :</head><p>T here are other attributes than the hand m ovem ent. G esture recognition process also has to sim ultaneously consider these other aspects, i.e . like hand postures and the region in w hich the gesture is car ried out, and the change o f orientation, etc.</p><p>We em ployed the hidden M arkov m odel(H M M ) to recog nize and segm ent the 3D hand m ovem ent pattern o f gesture. T his approach is inspired by the success o f the application o f H M M in the speech recognition <ref type="bibr" target="#b2">[3]</ref> and hand-w ritten character recognition <ref type="bibr" target="#b3">[4]</ref> [5] [6] [7] <ref type="bibr" target="#b7">[8]</ref>. T he hand gesture recognition problem is very sim ilar to the on-lin e hand w ritten character recognition problem , although there are differences in that the m otion in a gesture has m ore variations due to their 3D characteristics and does not give starting and ending points w hich are apparent as pen up / dow n in on-line handw riting.</p><p>H M M can deal w ith the tem poral variance and the shape variance w hile preserving the order in the hand m ovem ent.</p><p>T he rotational variance and the global translation in the 3D space still cause the dim ensional com plexity. To cut down the com plexity, the m ethod to reduce the 3D data into 2D is introduced and the relative directional encoding schem e is em ployed.</p><p>We also utilize other attributes o f the hand gesture and discuss the integration o f them w ith H M M s o f the hand m ovem ent patterns.</p><p>Section 2 briefly review s the related w ork on the gesture recognition. In section 3, we present a H M M -based fram e w ork fo r hand gesture recognition. Section 4 describes the initial experim ents on the gesture vocabulary regarding the 3D virtual object m anipulation application. Finally, section 5 gives the sum m ary o f this paper and further work to do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R E L A T E D W O R K</head><p>A ttem pts at the gesture recognition are gaining popularity recently. As for the recognition o f the hand m ovem ent pattern, m ost o f the attem pts deal w ith the 2D, linear or sm all restricted set o f m ovem ent patterns. Since the form of their recognition targets is m ostly the isolated gesture, there are few considerations on the m ethod fo r the recognition of the connected pattern. A lso, few w orks consider attributes other than the hand m ovem ent.</p><p>R ubine <ref type="bibr" target="#b8">[9]</ref> used the feature analysis technique to discrim i nate the single-stroke, 2D trajectory patterns o f m ouse ges tures. W exelblat <ref type="bibr" target="#b9">[10]</ref> discussed the path analysis by feature finding, and tem poral integration o f several hand gesture at tributes. Since these feature-based approaches extract som e low -level features from the raw data and the classification is done by analyzing them , finding the proper feature set is im portant. W ilso n [ll] em ployed the concepts o f fuzzy config uration states to handle the repeatability and tim e-scale vari ability o f gesture. Independent from our work, Stam er <ref type="bibr" target="#b13">[14]</ref> has recently proposed the H M M -based recognition method, w hich is sim ilar to ours. Since different attributes o f a hand gesture are all scram bled into one feature set, however, a slight change like adding a new feature needs retraining o f the w hole netw ork. M oreover, the com plexity o f the 3D problem space w hich are involved w ith the rotational variance and the global translational variance w as not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H A N D G E ST U R E R E C O G N IT IO N A ttributes and D efinitions</head><p>We describe the hand gesture in term s o f the follow ing three attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• h a n d c o n f i g u r a t i o n ( i.e . p o s t u r e ) :</head><p>T he hand posture can carry som e m eaning by itself o r by accom panying the hand m ovem ent. It can be regarded that the significant change o f the hand posture in the m idst o f gestu re's gross hand m ovem ent is rare <ref type="bibr" target="#b14">[ 15]</ref>, Different palm orientations may reflect different mean ings while the hand posture and the movement pattern are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• h a n d m o v e m e n t :</head><p>The path that the hand moves may be the major com po nent of human hand gesture. The path might illustrate the outline o f an object or its dynamic behavior and so</p><p>With the above description, a hand gesture can be defined as the across-time behavior o f the parallel observations o f those three attributes. We now define p r i m e g e s tu r e as the unit of the hand gesture in which no significant changes of the pos ture or the orientation are observed. And in particular, we re fer its hand movement pattern as the m o v e m e n t p r im e . Then, the hand gesture can be described in terms o f a sequence of  Figure <ref type="figure">2</ref> shows a sample vocabulary for these categories.</p><p>The movement primes sketched in the p i c t o g r a p h i c category can specify some object by the similarity of the hand-drawn shape. Each of the primes in figure <ref type="figure">2</ref> is labeled with a pos sible interpretation, but the label is chosen ju st as an exam ple and actually it can be anything because the high-level meaning will be attached in the application-specific domain later. The movement primes in the k in e to g r a p h i c category can specify some actions. With the sequential generation of the gestures in these two categories, we can make a 'ball' to 'rotate' in a virtual environment, for example.</p><p>In addition to the movement primes in figure <ref type="figure">2</ref>, the quantity description primes can be added. The number can be drawn by hand in the 3D space, for example. Figure <ref type="figure">3</ref> shows some patterns of possible gesture commands with this kind of vo cabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition o f the Hand M ovement Pattern</head><p>For the recognition of the hand movement pattern, we first re duce the 3D complexity to 2D at the encoding stage, and then HMM-based recognition and segmentation are conducted.</p><p>(a) JD ravt data (b) plane fitting We em ploy the chain encoding schem e for describing the hand m ovem ent path to elim inate the variance caused by the global translation. Since this m ethod uses only the relative inform ation o f the position changes, the global translation does not affect the encoding. T he straightforw ard use o f three-dim ensional chain, how ever, generates fully different code sequence w hen the gesture is conducted in slightly rotated global direction. This observation suggests that the rotation-invariant encoding schem e is needed.</p><p>B asic idea for achieving the rotational invariance is to reduce the three-dim ensional d ata to tw o-dim ensional ones. F or this reduction, w e first find the best fitting plane for the sequence o f 3D positions and then project the 3D position sequence to the 2D coordinates on that plane. F igure 4 show s this reduc tion process. T his can be regarded as extracting the 2D tra jectory w hich is the essence o f the gesture. T hose fitted data are then chain encoded so as to be fed to the H M M to find the corresponding m ovem ent prim e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H M M -based R ecognition</head><p>Form al definition o f an H M M consists o f the follow ing elem ents <ref type="bibr" target="#b2">[3]</ref>.</p><p>• states S = { s i , S2 , ..., sjv) state at tim e t : qt ( N : the num ber o f states in the m odel)</p><p>• sym bols V = { 17 , v 2, ..., v m } ( M : the num ber o f distinct observation sym bols per state)</p><p>• A = {a, j } : the state transition probability distribution </p><formula xml:id="formula_0">aij = P [q t+ 1 = S j\q t = Si], 1 &lt; t\ j &lt; N • B =</formula><formula xml:id="formula_1">ai(i) = TTibi(Oi), 1 &lt;i &lt;N recursion: N Qt+i(j) = [Y^&lt;*t(i)aij]bj(Ot + i). 1 = 1 w h e re l &lt; t &lt; T -1, l &lt; j &lt; N P (0\\) = Y J &lt;*T(i) i = l</formula><p>For the param eter estim ation that adjusts the param eters of th e m odel A = (A , B , w) such that P ( 0 |A ) is locally m axi m ized using an iterative procedure, the B aum -W elch m ethod is em ployed. First, let us define (i, j ) as the probability of being in state s, at tim e t and state s, at tim e 1 + 1. given the not produce any output and there is no transition penalty. The network search to find the best path is done by a kind of dynamic programming techniques called Viterbi algorithm which has already shown its effectiveness in speech and char acter recognition literature. Viterbi search begins at tim e 1 and proceeds forward until it reaches time T , com puting the score St{s), which is defined recursively as:</p><formula xml:id="formula_2">Si(i) = 7Tj6|(Oj), l&lt; i &lt;N, St(j) -m ax [£t _i(»')a,•_,•]&amp; ,• (O t ), 2 &lt;t &lt;T, IQ&lt;N 1 &lt;j &lt;N,</formula><p>where N is the number o f states in the HMM network, T is the length o f the observation sequence O -0\O i -Ot, aK 1 is the state transition probability from state i to state j, and bj(k) is the observation probability o f symbol Ok at state j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The HMM Network</head><p>Single hand gesture can be repeated and m ultiple hand ges tures can be conducted consecutively. We constructed HMM network to provide an automatic way to segment and extract the constituting gesture sequence.</p><p>With this structure, the recognition problem is to find the maximal probability path in the network for the given data.</p><p>As an outcome, optimal sequence and the associated move ment prime labels are obtained. The network search to find the best path is done by the Viterbi search explained in sec tion 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Attribute Comm unication</head><p>To find the fitting plane at the encoding stage, first a single sequence of hand movement data to be fitted into a common plane has to be found. Since single prime gesture is con ducted against a single plane in the space, the data points in it can be fitted into the common plane. The fitting planes can be different each other, however, when the different prime ges tures are connected. In this case, we must find the separate fitting planes for the constituting gestures. To decide when to fit data, the attributes other than the hand movement are utilized. Figure <ref type="figure">6</ref>: The HMM network for the recognition of con nected hand movement pattern Figure <ref type="figure">6</ref> shows this network structure. HMMs are con structed for the hand movement primes and for their juncture patterns. Null transition is connected from the final node of each movement prime HMM to each juncture HMM, and similarly, each juncture HMM is connected to the initial node of each movement prime HMM. Null transitions do inform ation from the other tw o attributes than the hand m ovem ent to decide the fitting point. Since several gestures can be conducted in a single plane w hile keeping the same posture and orientation, how ever, H M M netw ork is still responsible for finding the appropriate segm entation.</p><p>In addition to give the apparently transitional inform ation, all the attributes o f course contribute to decide the integrated meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E X P E R IM E N T S</head><p>We designed an experim ental system to validate our ap proach. T he overall structure is show n in figure <ref type="figure">8</ref>  To sm ooth the raw data that can have noise from the tracker or from the hand m ovem ent itself, the fixed-size w indow averaging is applied to the sam ple points.</p><p>W ith these preprocessed data, the best fitting plane is found by the least squares m ethod. T he 3D sam ple points are then projected onto the plane so that a 2D point sequence is o b tained. T he global size o f these 2D data are then norm alized.   Though the recognition accuracy for com plex connected pat terns is about 80% and some segmentation misses exist, they are expected to be improved by the context modeling or the high-level pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION AND FUTURE W ORK</head><p>We have proposed a recognition method for the threedimensional space-time hand gesture. Among the attributes o f gesture, the hand movement pattern is most variable and can be arbitrarily complex. In other words, the hand movement pattern is intrinsically dynamic. Some of the typical pattern recognition approaches like feature-based classification and the artificial neural network doesn't seem to fit in this problem since they are more appropriate for modeling and recognizing the static pattern. In contrast, HMM is more natural and effective in extracting and recog nizing the spatio-temporally dynamic pattern.</p><p>In this respect, we modeled the hand gestures as the se quence of the prime gestures and defined the movement primes as the unit of recognition for the hand movement pattern.</p><p>Interconnected HMM network is constructed for such movement primes and their juncture patterns. It segments and recognizes the complex movement patterns involving the connection or the repetition of some gestures. To reduce the dimensional complexity, we find the best fitting plane and 2D chain encoding scheme is employed. Although current experiments are still at its initial stage, the experimental result for the chosen movement primes shows the effectiveness o f the approach.</p><p>Further experiments would deal the gesture vocabulary in more specific 3D CAD domain, and the connected gestures as the CAD operation command would be explored and tested. Also, we expect the better juncture models would be gained by the context modeling in the specific domain. As for the more accurate recognition, we are considering the inclusion o f some temporal inform ation(like 'pause' between the con secutive gestures) in the HMM model explicitly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 3D hand gesture attributes</figDesc><graphic coords="3,64.76,35.01,227.76,84.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Examples o f movement primes Our gesture vocabulary is chosen by regarding c o m m a n d lik e g e s tu r e s which are recently gaining some interests with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 3D to 2D reduction by plane fitting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>model and the observation sequence St(i,j) = P {qt = Si,qt+1 = Sj |0 , A) and define the variable 71(») as the probability o f being in state S{ at tim e t, given the m odel A and the observation se quence O , as follow s; • 7T = {rr, } : the initial state distribution m = P[qx = s,], 1 &lt; i &lt; N 7»(») = ^6 ( * . j) -W ith the above param eters, an H M M is often described com pactly as A = (.4, B, tt). G iven this definition, w e create a discrete H M M for each m ovem ent prim e. S im ple left-to-right H M M show n in fig ure 5 is used as the structure for the m ovem ent prim e HM M s. Then, reestim ation form ula for rr, .4, B becom es: e x p e c te d fr e q u e n c y in s ta te s, a t tim e ( t = 1 ) = 71 E L Y 7,(0 '</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7 7 7 7</head><label>7</label><figDesc>^l t = l,3 t.Oi = Vk Ttij) b^k) = -------f -------------•£ e= i 7 t 0 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>i i | i ii i m m i i n m im m n i n m i iim n i n H * transition O r i e n t a t i o n i W I m i m m IIIH m u n m m m ill ! m u III n I h III n m l m h e c t o r s ( v e l o c i t i e s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Finding the movement prime</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>. The val ues o f the three attributes are sam pled by the separate sen sory channels. Each o f the attributes has its corresponding recognition m odule. E very sam ple o f the hand posture and the palm orientation is recognized by the posture recognition m odule and orientation quantizer, respectively. T he hand m ovem ent recognition m odule w hich consists o f the H M M s representing the m ovem ent prim es w aits until the posture transition o r the significant change o f the orientation is noti fied from the oth er attribute recognition m odules. If the p o s ture transition is detected o r the significant change o f orien tation occurs, the corresponding recognition m odule signals to fit the m ovem ent data into a plane. At present, initial experim ents are carried out fo r the recog nition o f the m ovem ent prim es listed in figure 2 in section 3.2 and som e o f their connected patterns. O ne-hand V PL D ataglove[12] is used as the input device to m easure the ten flex angles (tw o for each finger) and the attached Polhem us tracker senses the 3D absolute p osition (x , y, z) in the space. T hese sam pled data are acquired at the rate o f roughly 30 tim es per second. F or the training o f each m ovem ent prim e, som e specific postures w ere used to indicate the starting and the ending. P osture recognition is carried out by the fixed param eter approach w hich com pares the flex angles w ith the predeter m ined value ranges. W hen the intended posture transition is occurred, the posture recognition m odule notifies it so as to begin o r end the recording o f a new sequence o f the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9</head><label>9</label><figDesc>Figure9and figure10show the experim ent done on the con nected gesture. H M M netw ork autom atically segm ents the 3D hand m ovem ent data show n in figure9, and the recogni tion results are found as the m ovem ent prim e sequence in cluding jun ctu res as show n in figure10. At this tim e, ju n c ture m odels as many as the num ber o f chain directions are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: 3D raw data for a complex connected pattern hall juncture</figDesc><graphic coords="7,41.13,223.16,262.80,139.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>R ecognition accuracy for the m ovem ent primes m ovem ent sam ples.A fter collecting m ore than tw o hundred training samples per each m ovem ent prim e, a preprocessing is perform ed to filter out very close points w hich indicate the hand trem bling w hile the hand is trying to be fixed at a position in the space.</figDesc><table><row><cell>Id Ilex angles</cell></row><row><cell>3D p iis iiin n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>show s the recognition result for each m ovem ent</cell></row><row><cell>prim e specified in figure 2. M ore than 300 exam ples were</cell></row><row><cell>collected for each m ovem ent prim e and about 200 o f them</cell></row><row><cell>w ere used as the training data and the rem aining ones are</cell></row><row><cell>used for the recognition test. The data set for the recognition</cell></row><row><cell>test was not used for any portion o f the training. The</cell></row><row><cell>chain-coded data were applied to each H M M that represents</cell></row><row><cell>a m ovem ent prim e and the probability for each model was</cell></row><row><cell>com puted. T he num ber o f states in the H M M topologies</cell></row><row><cell>w ere fine-tuned for each m ovem ent prim e and determ ined</cell></row><row><cell>em pirically. T hough som e m isses are observed, the result</cell></row><row><cell>show s the discrim inating pow er o f the H M M when it is</cell></row><row><cell>applied to the hand m ovem ent pattern recognition problem.</cell></row><row><cell>T he recognition result o f the hand m ovem ent pattern was</cell></row><row><cell>finally com bined w ith the inform ation o f other attributes</cell></row><row><cell>(posture) and determ ines w hat the conducted hand gesture</cell></row><row><cell>was.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>VRST'96 July 1-4,1996</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>July 1-4,1996VRST'96</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>July 1-4,1996</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOW LEDGM ENTS</head><p>This research was carried as a part o f U biquitous Com put ing Project supported by CAIR(Center for Artificial Intelli gence Research) and Research Project on M ultim edia Cre ation Technology supported by Samsung and KAIST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<title level="m">Collins Cobuild Dictionary, HarperCollins pub lishers</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Coverbal Iconic Gesture in Human-Computer Interaction, M aster&apos;s Thesis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sparrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Tutorial on Hidden Markov M od els and Selected Applications in Speech Recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Uncon strained Handwritten Word Recognition with Intercon nected Hidden Markov M odels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kwon</surname></persName>
		</author>
		<idno>IWFHR-3</idno>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="455" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Probabilistic Framework for On-line Hand writing Recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bellegarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Nathan</surname></persName>
		</author>
		<idno>IWFHR-3</idno>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Ujisaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Eigi</surname></persName>
		</author>
		<title level="m">O n line U nconstrained H andw riting R ecognition by a P rob abilistic M ethod</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="235" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">O ff-line H andw ritten W ord R ecognition with H idden M arkov M odels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>KAIST</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">C onnected and degraded text recognition using H idden M arkov M odel</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><forename type="middle">K</forename><surname>Uo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interna tional Journal o f Pattern Recognition and Artificial In telligence</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1345" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ubine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S pecifying G estures by E xam ple</title>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="329" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An A pproach to N atural G esture in V irtual E nvironm ents</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Exelblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM journal o f Transactions on Computer-Human Interface</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>submitted to</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U sing C onfiguration States fo r the R epresentation and R ecognition o f G esture</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Obick</surname></persName>
		</author>
		<idno>No. 308</idno>
	</analytic>
	<monogr>
		<title level="m">M IT M edia Laboratory</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Data-GLove Model 2 Operation Manual</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>V PL Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">M odeling the Interaction betw een Speech and G esture</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Achorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedm An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Pennsylvania, M ay</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Techni cal Rep. No. MS-CIS-94-23</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<title level="m">Visual R ecognition of A m erican Sign L anguage using H idden M arkov M od els</title>
		<meeting><address><addrLine>Zurich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
	<note>International Workshop on Automatic Face-and Gesture-Recognition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tow ard a V ision-based H and G esture In terface</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K H</forename><surname>Uek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ofVRST &apos;94</title>
		<meeting>ofVRST &apos;94<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
