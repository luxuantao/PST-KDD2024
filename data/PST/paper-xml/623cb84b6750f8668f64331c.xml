<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLASSIFIER-FREE DIFFUSION GUIDANCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-26">26 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
							<email>jonathanho@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
						</author>
						<title level="a" type="main">CLASSIFIER-FREE DIFFUSION GUIDANCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-26">26 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.12598v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Diffusion models have recently emerged as an expressive and flexible family of generative models, delivering competitive sample quality and likelihood scores on image and audio synthesis tasks <ref type="bibr" target="#b17">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b18">Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b6">Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021b;</ref><ref type="bibr">Kingma et al., 2021;</ref><ref type="bibr">Song et al., 2021a)</ref>. These models have delivered audio synthesis performance rivaling the quality of autoregressive models with substantially fewer inference steps <ref type="bibr" target="#b1">(Chen et al., 2021;</ref><ref type="bibr" target="#b11">Kong et al., 2021)</ref>, and they have delivered ImageNet generation results outperforming BigGAN-deep <ref type="bibr" target="#b0">(Brock et al., 2019)</ref> and VQ-VAE-2 <ref type="bibr" target="#b13">(Razavi et al., 2019)</ref> in terms of FID score and classification accuracy score <ref type="bibr" target="#b7">(Ho et al., 2021;</ref><ref type="bibr">Dhariwal &amp; Nichol, 2021)</ref>. <ref type="bibr">Dhariwal &amp; Nichol (2021)</ref> proposed classifier guidance, a technique to boost the sample quality of a diffusion model using an extra trained classifier. Prior to classifier guidance, it was not known how to generate "low temperature" samples from a diffusion model similar to those produced by truncated BigGAN <ref type="bibr" target="#b0">(Brock et al., 2019)</ref> or low temperature Glow <ref type="bibr" target="#b9">(Kingma &amp; Dhariwal, 2018)</ref>: naive attempts, such as scaling the model score vectors or decreasing the amount of Gaussian noise added during diffusion sampling, are ineffective <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>. Classifier guidance instead mixes a diffusion model's score estimate with the input gradient of the log probability of a   <ref type="bibr" target="#b16">(Salimans et al., 2016)</ref> and FID score <ref type="bibr" target="#b5">(Heusel et al., 2017)</ref> (or precision and recall) in a manner similar to varying the truncation parameter of BigGAN.</p><p>We are interested in whether classifier guidance can be performed without a classifier. Classifier guidance complicates the diffusion model training pipeline because it requires training an extra classifier, and this classifier must be trained on noisy data so it is generally not possible to plug in a pre-trained classifier. Furthermore, because classifier guidance mixes a score estimate with a classifier gradient during sampling, classifier-guided diffusion sampling can be interpreted as attempting to confuse an image classifier with a gradient-based adversarial attack. This raises the question of whether classifier guidance is successful at boosting classifier-based metrics such as FID and Inception score (IS) simply because it is adversarial against such classifiers. Stepping in direction of classifier gradients also bears some resemblance to GAN training, particularly with nonparameteric generators; this also raises the question of whether classifier-guided diffusion models perform well on classifier-based metrics because they are beginning to resemble GANs, which are already known to perform well on such metrics.</p><p>To resolve these questions, we present classifier-free guidance, our guidance method which avoids any classifier entirely. Rather than sampling in the direction of the gradient of an image classifier, classifier-free guidance instead mixes the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model. By sweeping over the mixing weight, we attain a FID/IS tradeoff similar to that attained by classifier guidance. Our classifier-free guidance results demonstrate that pure generative diffusion models are capable of synthesizing extremely high fidelity samples possible with other types of generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We train diffusion models in continuous time <ref type="bibr">(Song et al., 2021b;</ref><ref type="bibr" target="#b1">Chen et al., 2021;</ref><ref type="bibr">Kingma et al., 2021)</ref>: letting x ? p(x) and z = {z ? | ? ? [? min , ? max ]} for hyperparameters ? min &lt; ? max ? R, the forward process q(z|x) is the variance-preserving Markov process <ref type="bibr" target="#b17">(Sohl-Dickstein et al., 2015)</ref>:</p><formula xml:id="formula_0">q(z ? |x) = N (? ? x, ? 2 ? I), where ? 2 ? = 1/(1 + e -? ), ? 2 ? = 1 -? 2 ? (1) q(z ? |z ? ) = N ((? ? /? ? )z ? , ? 2 ?|? I), where ? &lt; ? , ? 2 ?|? = (1 -e ?-? )? 2 ? (2)</formula><p>We will use the notation p(z) (or p(z ? )) to denote the marginal of z (or z ? ) when x ? p(x) and z ? q(z|x). Note that ? = log ? 2 ? /? 2 ? , so ? can be interpreted as the log signal-to-noise ratio of z ? , and the forward process runs in the direction of decreasing ?.</p><p>Conditioned on x, the forward process can be described in reverse by the transitions q(z ? |z ? , x) = N ( ?? |? (z ? , x), ?2 ? |? I), where</p><formula xml:id="formula_1">?? |? (z ? , x) = e ?-? (? ? /? ? )z ? + (1 -e ?-? )? ? x, ?2 ? |? = (1 -e ?-? )? 2 ? (3)</formula><p>The reverse process generative model starts from p ? (z ?min ) = N (0, I). We specify the transitions:</p><formula xml:id="formula_2">p ? (z ? |z ? ) = N ( ?? |? (z ? , x ? (z ? )), (? 2 ? |? ) 1-v (? 2 ?|? ) v )<label>(4)</label></formula><p>During sampling, we apply this transition along an increasing sequence ? min = ? 1 &lt; ? ? ? &lt; ? T = ? max for T timesteps; in other words, we follow the discrete time ancestral sampler of <ref type="bibr" target="#b17">Sohl-Dickstein et al. (2015)</ref>; <ref type="bibr" target="#b6">Ho et al. (2020)</ref>. If the model x ? is correct, then as T ? ?, we obtain samples from an SDE whose sample paths are distributed as p(z) <ref type="bibr">(Song et al., 2021b)</ref>, and we use p ? (z) to denote the continuous time model distribution. The variance is a log-space interpolation of ?2 ? |? and ? 2 ?|? as suggested by <ref type="bibr">Nichol &amp; Dhariwal (2021)</ref>; we found it effective to use a constant hyperparameter v rather than learned z ? -dependent v. Note that the variances simplify to ?2 ? |? as ? ? ?, so v has an effect only when sampling with non-infinitesimal timesteps as done in practice.</p><p>The reverse process mean comes from an estimate x ? (z ? ) ? x plugged into q(z ? |z ? , x) <ref type="bibr" target="#b6">(Ho et al., 2020;</ref><ref type="bibr">Kingma et al., 2021)</ref> (x ? also receives ? as input, but we suppress this to keep our notation clean). We parameterize x ? in terms of -prediction <ref type="bibr" target="#b6">(Ho et al., 2020)</ref>: x ? (z ? ) = (z ? -? ? ? (z ? ))/? ? , and we train on the objective</p><formula xml:id="formula_3">E ,? ? (z ? ) -2 2 (5)</formula><p>where ? N (0, I), z ? = ? ? x + ? ? , and ? is drawn from a distribution p(?) over [? min , ? max ]. This objective is denoising score matching <ref type="bibr" target="#b21">(Vincent, 2011;</ref><ref type="bibr" target="#b8">Hyv?rinen &amp; Dayan, 2005)</ref> over multiple noise scales <ref type="bibr" target="#b18">(Song &amp; Ermon, 2019)</ref>, and when p(?) is uniform, the objective is proportional to the variational lower bound on the marginal log likelihood of the latent variable model p ? (x|z)p ? (z)dz, ignoring the term for the unspecified decoder p ? (x|z) and for the prior at z ?min <ref type="bibr">(Kingma et al., 2021)</ref>.</p><p>If p(?) is not uniform, the objective can be interpreted as weighted variational lower bound whose weighting can be tuned for sample quality <ref type="bibr" target="#b6">(Ho et al., 2020;</ref><ref type="bibr">Kingma et al., 2021)</ref>. We use a p(?) inspired by the discrete time cosine noise schedule of <ref type="bibr">Nichol &amp; Dhariwal (2021)</ref>: we sample ? via ? = -2 log tan(au + b) for uniformly distributed u ? [0, 1], where b = arctan(e -?max/2 ) and a = arctan(e -?min/2 ) -b. This represents a hyperbolic secant distribution modified to be supported on a bounded interval. For finite timestep generation, we use ? values corresponding to uniformly spaced u ? [0, 1], and the final generated sample is x ? (z ?max ).</p><p>Because the loss for ? (z ? ) is denoising score matching for all ?, the score ? (z ? ) learned by our model estimates the gradient of the log-density of the distribution of our noisy data z ? , that is ? (z ? ) ? -? ? ? z ? log p(z ? ); note, however, that because we use unconstrained neural networks to define ? , there need not exist any scalar potential whose gradient is ? . Sampling from the learned diffusion model resembles using Langevin diffusion to sample from a sequence of distributions p(z ? ) that converges to the conditional distribution p(x) of the original data x.</p><p>In the case of conditional generative modeling, the data x is drawn jointly with conditioning information c, i.e. a class label for class-conditional image generation. The only modification to the model is that the reverse process function approximator receives c as input, as in ? (z ? , c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GUIDANCE</head><p>An interesting property of certain generative models, such as GANs and flow-based models, is the ability to perform truncated or low temperature sampling by decreasing the variance or range of noise inputs to the generative model at sampling time. The intended effect is to decrease the diversity of the samples while increasing the quality of each individual sample. Truncation in BigGAN <ref type="bibr" target="#b0">(Brock et al., 2019)</ref>, for example, yields a tradeoff curve between FID score and Inception score for low and high amounts of truncation, respectively. Low temperature sampling in Glow (Kingma &amp; Dhariwal, 2018) has a similar effect.</p><p>Unfortunately, straightforward attempts of implementing truncation or low temperature sampling in diffusion models are ineffective. For example, scaling model scores or decreasing the variance of Gaussian noise in the reverse process cause the diffusion model to generate blurry, low quality samples <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CLASSIFIER GUIDANCE</head><p>To obtain a truncation-like effect in diffusion models, <ref type="bibr">Dhariwal &amp; Nichol (2021)</ref>  </p><formula xml:id="formula_4">? ? (z ? , c) = ? (z ? , c) -w? ? ? z ? log p ? (c|z ? ) ? -? ? ? z ? [log p(z ? |c) + w log p ? (c|z ? )],</formula><p>where w is a parameter that controls the strength of the classifier guidance. This modified score ? ? (z ? , c) is then used in place of ? (z ? , c) when sampling from the diffusion model, resulting in approximate samples from the distribution</p><formula xml:id="formula_5">p? (z ? |c) ? p ? (z ? |c)p ? (c|z ? ) w .</formula><p>The effect is that of up-weighting the probability of data for which the classifier p ? (c|z ? ) assigns high likelihood to the correct label: data that can be classified well scores high on the Inception score of perceptual quality <ref type="bibr" target="#b16">(Salimans et al., 2016)</ref>, which rewards generative models for this by design. Dhariwal &amp; Nichol therefore find that by setting w &gt; 0 they can improve the Inception score of their diffusion model, at the expense of decreased diversity in their samples.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the effect of numerically solved guidance p? (z ? |c) ? p ? (z ? |c)p ? (c|z ? ) w on a toy 2D example of three classes, in which the conditional distribution for each class is an isotropic Gaussian. The form of each conditional upon applying guidance is markedly non-Gaussian. As guidance strength is increased, each conditional places probability mass farther away from other classes and towards directions of high confidence given by logistic regression, and most of the mass becomes concentrated in smaller regions. This behavior can be seen as a simplistic manifestation of the Inception score boost and sample diversity decrease that occur when classifier guidance strength is increased in an ImageNet model.</p><p>Applying classifier guidance with weight w + 1 to an unconditional model would theoretically lead to the same result as applying classifier guidance with weight w to a conditional model, because p ? (z ? |c)p ? (c|z ? ) w ? p ? (z ? )p ? (c|z ? ) w+1 ; or in terms of scores,</p><formula xml:id="formula_6">? (z ? ) -(w + 1)? ? ? z ? log p ? (c|z ? ) ? -? ? ? z ? [log p(z ? ) + (w + 1) log p ? (c|z ? )] = -? ? ? z ? [log p(z ? |c) + w log p ? (c|z ? )],</formula><p>but interestingly, Dhariwal &amp; Nichol obtain their best results when applying classifier guidance to an already class-conditional model, as opposed to applying guidance to an unconditional model. For this reason, we will stay in the setup of guiding an already conditional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CLASSIFIER-FREE GUIDANCE</head><p>While classifier guidance successfully trades off IS and FID as expected from truncation or low temperature sampling, it is nonetheless reliant on gradients from an image classifier and we seek to eliminate the classifier for the reasons stated in Section 1. Here, we describe classifier-free guidance, which achieves the same effect without such gradients. Classifier-free guidance is an alternative method of modifying ? (z ? , c) to have the same effect as classifier guidance, but without a classifier. Algorithms 1 and 2 describe training and sampling with classifier-free guidance in detail.</p><p>Instead of training a separate classifier model, we choose to train an unconditional denoising diffusion model p ? (z) parameterized through a score estimator ? (z ? ) together with the conditional model p ? (z|c) parameterized through ? (z ? , c). We use a single neural network to parameterize both models, where for the unconditional model we can simply input a null token ? for the class identifier c when predicting the score, i.e. ? (z ? ) = ? (z ? , c = ?). We jointly train the unconditional and Algorithm 2 Conditional sampling with classifier-free guidance Require: w: guidance strength Require: c: conditioning information for conditional sampling Require: ? 1 , . . . , ? T : increasing log SNR sequence with ? 1 = ? min , ? T = ? max 1: z 1 ? N (0, I) 2: for t = 1, . . . , T do Form the classifier-free guided score at log SNR ? t 3:</p><formula xml:id="formula_7">? t = (1 + w) ? (z t , c) -w ? (z t )</formula><p>Sampling step (could be replaced by another sampler, e.g. DDIM)</p><p>4:</p><p>xt = (z t -? ?t ? t )/? ?t 5:</p><formula xml:id="formula_8">z t+1 ? N ( ??t+1|?t (z t , xt ), (? 2 ?t+1|?t ) 1-v (? 2 ?t|?t+1 ) v ) if t &lt; T else z t+1 = xt 6: end for 7: return z T +1</formula><p>conditional models simply by randomly setting c to the unconditional class identifier ? with some probability p uncond , set as a hyperparameter. (It would certainly be possible to train separate models instead of jointly training them together, but we choose joint training because it is extremely simple to implement, does not complicate the training pipeline, and does not increase the total number of parameters.) We then perform sampling using the following linear combination of the conditional and unconditional score estimates:</p><formula xml:id="formula_9">? ? (z ? , c) = (1 + w) ? (z ? , c) -w ? (z ? )<label>(6)</label></formula><p>Eq. ( <ref type="formula" target="#formula_9">6</ref>) has no classifier gradient present, so taking a step in the ? ? direction cannot be interpreted as a gradient-based adversarial attack on an image classifier. Furthermore, ? ? is constructed from score estimates that are non-conservative vector fields due to the use of unconstrained neural networks, so there in general cannot exist a scalar potential such as a classifier log likelihood for which ? ? is the classifier-guided score.</p><p>Despite the fact that there in general may not exist a classifier for which Eq. ( <ref type="formula" target="#formula_9">6</ref>) is the classifierguided score, it is in fact inspired by the gradient of an implicit classifier p i (c|z ? ) ? p(z ? |c)/p(z ? ).</p><p>If we had access to exact scores * (z ? , c) and * (z ? ) (of p(z ? |c) and p(z ? ), respectively), then the gradient of this implicit classifier would be</p><formula xml:id="formula_10">? z ? log p i (c|z ? ) = -1 ? ? [ * (z ? , c) - * (z ? )]</formula><p>, and classifier guidance with this implicit classifier would modify the score estimate into ? * (z ? , c) = (1 + w) * (z ? , c) -w * (z ? ). Note the resemblance to Eq. ( <ref type="formula" target="#formula_9">6</ref>), but also note that ? * (z ? , c) differs fundamentally from ? ? (z ? , c). The former is constructed from the scaled classifier gradient * (z ? , c) - * (z ? ); the latter is constructed from the estimate ? (z ? , c)? (z ? ), and this expression is not in general the (scaled) gradient of any classifier, again because the score estimates are the outputs of unconstrained neural networks.</p><p>It is not obvious a priori that inverting a generative model using Bayes' rule yields a good classifier that provides a useful guidance signal. For example, <ref type="bibr" target="#b3">Grandvalet &amp; Bengio (2004)</ref> find that discriminative models generally outperform implicit classifiers derived from generative models, even in artificial cases where the specification of those generative models exactly matches the data distribution. In cases such as ours, where we expect the model to be misspecified, classifiers derived by Bayes' rule can be inconsistent <ref type="bibr" target="#b4">(Gr?nwald &amp; Langford, 2007)</ref> and we lose all guarantees on their performance. Nevertheless, in Section 4, we show empirically that classifier-free guidance is able to trade off FID and IS in the same way as classifier guidance. In Section 5 we discuss the implications of classifier-free guidance in relation to classifier guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We train diffusion models with classifier-free guidance on area-downsampled class-conditional ImageNet <ref type="bibr" target="#b14">(Russakovsky et al., 2015)</ref>, the standard setting for studying tradeoffs between FID and Inception scores starting from the BigGAN paper <ref type="bibr" target="#b0">(Brock et al., 2019)</ref>.</p><p>The purpose of our experiments is to serve as a proof of concept to demonstrate that classifier-free guidance is able to attain a FID/IS tradeoff similar to classifier guidance and to understand the behavior of classifier-free guidance, not necessarily to push sample quality metrics to state of the art Figure <ref type="figure">3</ref>: Classifier-free guidance on 128x128 ImageNet. Left: non-guided samples, right: classifierfree guided samples with w = 3.0. Interestingly, strongly guided samples such as these display saturated colors. See Fig. <ref type="figure" target="#fig_5">8</ref> for more. on these benchmarks. For this purpose, we use the same model architectures and hyperparameters as the guided diffusion models of Dhariwal &amp; Nichol (2021) (apart from continuous time training as specified in Section 2); those hyperparameter settings were tuned for classifier guidance and hence may be suboptimal for classifier-free guidance. Furthermore, since we amortize the conditional and unconditional models into the same architecture without an extra classifier, we in fact are using less model capacity than previous work. Nevertheless, our classifier-free guided models still produce competitive sample quality metrics and sometimes outperform prior work, as can be seen in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VARYING THE CLASSIFIER-FREE GUIDANCE STRENGTH</head><p>Here we experimentally verify the main claim of this paper: that classifier-free guidance is able to trade off IS and FID in a manner like classifier guidance or GAN truncation. We apply our proposed classifier-free guidance to 64 ? 64 and 128 ? 128 class-conditional ImageNet generation. In Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure">4</ref>, we show sample quality effects of sweeping over the guidance strength w on our 64 ? 64 ImageNet models; Table <ref type="table">2</ref> and Fig. <ref type="figure" target="#fig_3">5</ref> show the same for our 128 ? 128 models. We consider w ? {0, 0.1, 0.2, . . . , 4} and calculate FID and Inception Scores with 50000 samples for each value following the procedures of <ref type="bibr" target="#b5">Heusel et al. (2017)</ref> and <ref type="bibr" target="#b16">Salimans et al. (2016)</ref>. All models used log SNR endpoints ? min = -20 and ? max = 20. The 64 ? 64 models used sampler noise interpolation coefficient v = 0.3 and were trained for 400 thousand steps; the 128 ? 128 models used v = 0.2 and were trained for 2.7 million steps.</p><p>We obtain the best FID results with a small amount of guidance (w = 0.1 or w = 0.3, depending on the dataset) and the best IS result with strong guidance (w ? 4). Between these two extremes we see a clear trade-off between these two metrics of perceptual quality, with FID monotonically decreasing and IS monotonically increasing with w. Our results compare favorably to <ref type="bibr">Dhariwal &amp; Nichol (2021)</ref> and <ref type="bibr" target="#b7">Ho et al. (2021)</ref>   <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>The most practical advantage of our classifier-free guidance method is its extreme simplicity: it is only a one-line change of code drop out the conditioning-and during sampling-to mix the conditional and unconditional score estimates. Classifier guidance, by contrast, complicates the training pipeline since it requires training an extra classifier. This classifier must be trained on noisy z ? , so it is not possible to plug in a standard pre-trained classifier.</p><p>Since classifier-free guidance is able to trade off IS and FID like classifier guidance without needing an extra trained classifier, we have demonstrated that guidance can be performed with a pure generative model. Furthermore, our diffusion models are parameterized by unconstrained neural networks and therefore their score estimates do not necessarily form conservative vector fields, unlike classifier gradients <ref type="bibr">(Salimans &amp; Ho, 2021)</ref>. Therefore, our classifier-free guided sampler follows step directions that do not resemble classifier gradients at all and thus cannot be interpreted as a gradient-based adversarial attack on a classifier, and hence our results show that boosting the classifier-based IS and FID metrics can be accomplished with pure generative models with a sampling procedure that is not adversarial against image classifiers using classifier gradients.</p><p>We also have arrived at an intuitive explanation for how guidance works: it decreases the unconditional likelihood of the sample while increasing the conditional likelihood. Classifier-free guidance accomplishes this by decreasing the unconditional likelihood with a negative score term, which to our knowledge has not yet been explored and may find uses in other applications.</p><p>Classifier-free guidance as presented here relies on training an unconditional model, but in some cases this can be avoided. If the class distribution is known and there are only a few classes, we can use the fact that c p(x|c)p(c) = p(x) to obtain an unconditional score from conditional scores without explicitly training for the unconditional score. Of course, this would require as many forward passes as there are possible values of c and would be inefficient for high dimensional conditioning.</p><p>A potential disadvantage of classifier-free guidance is sampling speed. Generally, classifiers can be smaller and faster than generative models, so classifier guided sampling may be faster than classifierfree guidance because the latter needs to run two forward passes of the diffusion model, one for conditional score and another for the unconditional score. The necessity to run multiple passes of the diffusion model might be mitigated by changing the architecture to inject conditioning late in the network, but we leave this exploration for future work.</p><p>Finally, any guidance method that increases sample fidelity at the expense of diversity must face the question of whether decreased diversity is acceptable. There may be negative impacts in deployed models, since sample diversity is important to maintain in applications where certain parts of the data are underrepresented in the context of the rest of the data. It would be an interesting avenue of future work to try to boost sample quality while maintaining sample diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented classifier-free guidance, a method to increase sample quality while decreasing sample diversity in diffusion models. Classifier-free guidance can be thought of as classifier guidance without a classifier, and our results showing the effectiveness of classifier-free guidance confirm that pure generative diffusion models are capable of maximizing classifier-based sample quality metrics while entirely avoiding classifier gradients. We look forward to further explorations of classifier-free guidance in a wider variety of settings and data modalities.</p><p>A SAMPLES  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classifier-free guidance on the malamute class for a 64x64 ImageNet diffusion model. Left to right: increasing amounts of classifier-free guidance, starting from non-guided samples on the left. A short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications: https://openreview.net/pdf?id=qw8AKxfYbI</figDesc><graphic url="image-1.png" coords="1,-61.78,571.54,340.57,168.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of guidance on a mixture of three Gaussians, each mixture component representing data conditioned on a class. The leftmost plot is the non-guided marginal density. Left to right are densities of mixtures of normalized guided conditionals with increasing guidance strength.</figDesc><graphic url="image-5.png" coords="2,108.00,81.86,395.99,98.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>introduce classifier guidance, where the diffusion score ? (z ? , c) ? -? ? ? z ? log p(z ? |c) is modified to include the Algorithm 1 Joint training a diffusion model with classifier-free guidance Require: p uncond : probability of unconditional training 1: repeat 2: (x, c) ? p(x, c) Sample data with conditioning from the dataset 3: c ? ? with probability p uncond Randomly discard conditioning to train unconditionally ? = ? ? x + ? ? Corrupt data to the sampled log SNR value 7: Take gradient step on ? ? ? (z ? , c) -2 Optimization of denoising model 8: until converged gradient of the log likelihood of an auxiliary classifier model p ? (c|z ? ) as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: IS/FID curves over guidance strengths for ImageNet 128x128 models. Each curve represents sampling with a different number of timesteps T . Accompanies Table2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 6: Classifier-free guidance on ImageNet 64x64. Left: random classes. Right: single class (malamute). The same random seed was used for sampling in each subfigure.</figDesc><graphic url="image-9.png" coords="12,127.80,505.76,356.39,176.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: More examples of classifier-free guidance on 128x128 ImageNet. Left: non-guided samples, right: classifier-free guided samples with w = 3.0.</figDesc><graphic url="image-13.png" coords="14,108.00,107.79,395.99,564.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="13,108.00,71.90,395.98,196.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-11.png" coords="13,108.00,285.32,395.98,196.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-12.png" coords="13,108.00,498.73,395.98,196.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, and in fact our 128 ? 128 results are the state of the art in the literature. At w = 0.3, our model's FID score on 128 ? 128 ImageNet outperforms the classifier-guided ADM-G, and at w = 4.0, our model outperforms BigGAN-deep at both FID and IS when BigGAN-deep is evaluated its best-IS truncation level. Figures 1, 3 and 6 to 8 show randomly generated samples from our model for different levels of guidance: here we clearly see that increasing classifier-free guidance strength has the expected effect of decreasing sample variety and increasing individual sample fidelity. ImageNet 64x64 results (w = 0.0 refers to non-guided models).Figure 4: IS/FID curves over guidance strengths for ImageNet 64x64 models. Each curve represents a model with unconditional training probability p uncond. Accompanies Table 1.</figDesc><table><row><cell>Model</cell><cell></cell><cell>FID (?)</cell><cell></cell><cell>IS (?)</cell></row><row><cell cols="2">ADM (Dhariwal &amp; Nichol, 2021)</cell><cell>2.07</cell><cell></cell><cell>-</cell></row><row><cell cols="2">CDM (Ho et al., 2021)</cell><cell>1.48</cell><cell></cell><cell>67.95</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">p uncond = 0.1/0.2/0.5</cell></row><row><cell cols="2">w = 0.0</cell><cell>1.8 / 1.8 / 2.21</cell><cell></cell><cell>53.71 / 52.9 / 47.61</cell></row><row><cell cols="2">w = 0.1</cell><cell cols="2">1.55 / 1.62 / 1.91</cell><cell>66.11 / 64.58 / 56.1</cell></row><row><cell cols="2">w = 0.2</cell><cell>2.04 / 2.1 / 2.08</cell><cell></cell><cell>78.91 / 76.99 / 65.6</cell></row><row><cell cols="2">w = 0.3</cell><cell cols="2">3.03 / 2.93 / 2.65</cell><cell>92.8 / 88.64 / 74.92</cell></row><row><cell cols="2">w = 0.4</cell><cell>4.3 / 4 / 3.44</cell><cell></cell><cell>106.2 / 101.11 / 84.27</cell></row><row><cell cols="2">w = 0.5</cell><cell cols="2">5.74 / 5.19 / 4.34</cell><cell>119.3 / 112.15 / 92.95</cell></row><row><cell cols="2">w = 0.6</cell><cell cols="2">7.19 / 6.48 / 5.27</cell><cell>131.1 / 122.13 / 102</cell></row><row><cell cols="2">w = 0.7</cell><cell cols="2">8.62 / 7.73 / 6.23</cell><cell>141.8 / 131.6 / 109.8</cell></row><row><cell cols="2">w = 0.8</cell><cell cols="2">10.08 / 8.9 / 7.25</cell><cell>151.6 / 140.82 / 116.9</cell></row><row><cell cols="2">w = 0.9</cell><cell cols="2">11.41 / 10.09 / 8.21</cell><cell>161 / 150.26 / 124.6</cell></row><row><cell cols="2">w = 1.0</cell><cell cols="2">12.6 / 11.21 / 9.13</cell><cell>170.1 / 158.29 / 131.1</cell></row><row><cell cols="2">w = 2.0</cell><cell cols="2">21.03 / 18.79 / 16.16</cell><cell>225.5 / 212.98 / 183</cell></row><row><cell cols="2">w = 3.0</cell><cell cols="3">24.83 / 22.36 / 19.75 250.4 / 237.65 / 208.9</cell></row><row><cell cols="2">w = 4.0</cell><cell cols="3">26.22 / 23.84 / 21.48 260.2 / 248.97 / 225.1</cell></row><row><cell></cell><cell>p uncond = 0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>p uncond = 0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell>p uncond = 0.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FID</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell>IS</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Ben Poole</rs> and <rs type="person">Mohammad Norouzi</rs> for discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VARYING THE UNCONDITIONAL TRAINING PROBABILITY</head><p>The main hyperparameter of classifier-free guidance at training time is p uncond , the probability of training on unconditional generation during joint training of the conditional and unconditional diffusion models. Here, we study the effect of training models on varying p uncond on 64 ? 64 ImageNet.</p><p>Table <ref type="table">1</ref> and Fig. <ref type="figure">4</ref> show the effects of p uncond on sample quality. We trained models with p uncond ? {0.1, 0.2, 0.5}, all for 400 thousand training steps, and evaluated sample quality across various guidance strengths. We find p uncond = 0.5 consistently performs worse than p uncond ? {0.1, 0.2} across the entire IS/FID frontier; p uncond ? {0.1, 0.2} perform about equally as well as each other.</p><p>Based on these findings, we relatively small portion of the model capacity of the diffusion model needs to be dedicated to the unconditional generation task in order to produce classifier-free guided scores effective for sample quality. Interestingly, for classifier guidance, Dhariwal &amp; Nichol report that relatively small classifiers with little capacity are sufficient for effective classifier guided sampling, mirroring this phenomenon that we found with classifier-free guided models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VARYING THE NUMBER OF SAMPLING STEPS</head><p>Since the number of sampling steps T is known to have a major impact on the sample quality of a diffusion model, here we study the effect of varying T on our 128 ? 128 ImageNet model. Table <ref type="table">2</ref> and Fig. <ref type="figure">5</ref> show the effect of varying T ? {128, 256, 1024} over a range of guidance strengths. As expected, sample quality improves when T is increased, and for this model T = 256 attains a good balance between sample quality and sampling speed.</p><p>Note that T = 256 is approximately the same number of sampling steps used by ADM-G <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>, which is outperformed by our model. However, it is important to note that each sampling step for our method requires evaluating the denoising model twice, once for the conditional ? (z ? , c) and once for the unconditional ? (z ? ). Because we used the same model architecture as ADM-G, the fair comparison in terms of sampling speed would be our T = 128 setting, which underperforms compared to ADM-G in terms of FID score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model FID (?) IS (?)</head><p>BigGAN-deep, max IS <ref type="bibr" target="#b0">(Brock et al., 2019)</ref> 25 253 BigGAN-deep <ref type="bibr" target="#b0">(Brock et al., 2019)</ref> 5.7 124.5 CDM <ref type="bibr" target="#b7">(Ho et al., 2021)</ref> 3.52 128.8 LOGAN <ref type="bibr" target="#b22">(Wu et al., 2019)</ref> 3.36 148.2 ADM-G <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref> 2.97 -   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wave-Grad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat GANs on image synthesis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems</title>
		<meeting>the 17th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Suboptimal behavior of bayes and mdl in classification under misspecification</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gr?nwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="119" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15282</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational diffusion models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<title level="m">DiffWave: A Versatile Diffusion Model for Audio Synthesis. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
	<note>Aaron van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Should EBMs model the energy or the score?</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Based Models Workshop-ICLR 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2101</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<title level="m">Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Latent optimisation for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><surname>Logan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
