<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Pagliari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">PEE/COPPE/DEL</orgName>
								<orgName type="institution">Universidade Federal do Rio de Janeiro</orgName>
								<address>
									<postCode>21941-972</postCode>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Insti-tuto Militar de Engenharia</orgName>
								<address>
									<postCode>22290-270</postCode>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A8ED65A559651DA0756C5B0863C93B0</idno>
					<idno type="DOI">10.1109/TIP.2012.2226045</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Multiscale transforms are among the most popular techniques in the field of pixel-level image fusion. However, the fusion performance of these methods often deteriorates for images derived from different sensor modalities. In this paper, we demonstrate that for such images, results can be improved using a novel undecimated wavelet transform (UWT)based fusion scheme, which splits the image decomposition process into two successive filtering operations using spectral factorization of the analysis filters. The actual fusion takes place after convolution with the first filter pair. Its significantly smaller support size leads to the minimization of the unwanted spreading of coefficient values around overlapping image singularities. This usually complicates the feature selection process and may lead to the introduction of reconstruction errors in the fused image. Moreover, we will show that the nonsubsampled nature of the UWT allows the design of nonorthogonal filter banks, which are more robust to artifacts introduced during fusion, additionally improving the obtained results. The combination of these techniques leads to a fusion framework, which provides clear advantages over traditional multiscale fusion approaches, independent of the underlying fusion rule, and reduces unwanted side effects such as ringing artifacts in the fused reconstruction.</p><p>Index Terms-Image fusion, nonorthogonal filter banks, spectral factorization, undecimated wavelet transform (UWT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITHIN the last decades substantial progress was achieved in the imagery sensor field. These advances led to the availability of a vast amount of data, coming from multiple sensors. Often it is convenient to merge such multisensor data into one composite representation for interpretation purposes. In image-based applications this plethora of combination techniques became generally known as image fusion and is nowadays a promising research area.</p><p>The process of image fusion can be performed at pixel-, feature-or decision-level <ref type="bibr" target="#b0">[1]</ref>. Image fusion at pixel-level repre-sents the combination of information at the lowest level, since each pixel in the fused image is determined by a set of pixels in the source images. Generally, pixel-level techniques can be divided into spatial and transform domain techniques <ref type="bibr" target="#b1">[2]</ref>. Among the transform domain techniques, the most frequently used methods are based on multiscale transforms where fusion is performed on a number of different scales and orientations, independently. The multiscale transforms usually employed are Pyramid Transforms <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, the Discrete Wavelet Transform (DWT) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>, the Undecimated Wavelet Transform (UWT) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>, the Dual-Tree Complex Wavelet Transform (DTCWT) <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, the Curvelet Transform (CVT) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the Contourlet Transform (CT) <ref type="bibr" target="#b18">[19]</ref> and the Nonsubsampled Contourlet Transform (NSCT) <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>.</p><p>Please note that only multiscale pixel-level image fusion will be addressed in the course of this work. In addition, all input images are assumed to be adequately aligned and registered prior to the fusion process.</p><p>In multiscale pixel-level image fusion, a transform coefficient of an image is associated with a feature if its value is influenced by the feature's pixel. In order to simplify the discussion, we will refer to a given decomposition level j , orientation band p and position m, n of a coefficient as its localization. A given feature from one of the source images is only conserved correctly in the fused image if all associated coefficients are employed to generate the fused multiscale representation. However, in many situations this is not practical since, given a localization l, the coefficient y A (l) from image I A may be associated to a feature f A and the coefficient y B (l) from image I B may be associated to a feature f B . In this case, choosing one coefficient instead of the other may result in the loss of an important salient feature from one of the source images. For example, in the case of a camouflaged person hiding behind a bush the person may appear only in the infrared image and the bush only in the visible image. If the bush has high textural content, this may result in large coefficient values at coincident localizations in both decompositions of an infrared-visible image pair. However, in order to conserve as much as possible of the information from the scene, most coefficients belonging to the person (infrared image) and the bush (visible image) would have to be transferred to the fused decomposition. If there are many such coefficients at coincident localizations, a fusion rule that chooses just one of the coefficients for each localization It is important to note that the above mentioned problem is aggravated with the increase of the support of the filters used during the decomposition process. This results in an undesirable spreading of coefficient values over the neighborhood of salient features, introducing additional areas that exhibit coefficients in the source images with coincident localizations. In a previous work, Petrović and Xydeas dealt with this problem by employing image gradients <ref type="bibr" target="#b8">[9]</ref>. In this paper, we propose a novel UWT-based pixel-level image fusion approach, which attempts to circumvent the coefficient spreading problem by splitting the image decomposition procedure into two successive filter operations using spectral factorization of the analysis filters. A schematic flow-chart of the suggested image fusion framework is given in Fig. <ref type="figure" target="#fig_0">1</ref>. The co-registered source images are first transformed to the UWT domain by using a very short filter pair, derived from the first spectral factor of the overall analysis filter bank. After the fusion of the high-pass coefficients, the second filter pair, consisting of all remaining spectral factors, is applied to the approximation and fused, detail images. This yields the first decomposition level of the proposed fusion approach. Next, the process is recursively applied to the approximation images until the desired decomposition depth is reached. After merging the approximation images at the coarsest scale the inverse transform is applied to the composite UWT representation, resulting in the final fused image. Notice that this methodology is in contrast to conventional multiscale image fusion approaches, where the detail image fusion is not performed until the input image signals are fully decomposed using an analysis filter bank without spectral factorization. In addition, the implemented filter banks were especially designed for the use with the UWT and exhibit useful properties such as being robust to the ringing artifact problem. In the course of this work, we will show that our framework significantly improves fusion results for a large group of input images.</p><p>The remaining sections of this paper are organized as follows. Section II reviews multiscale techniques used in the context of pixel-level image fusion. In Section III the proposed image fusion framework is introduced in detail, whilst Section IV outlines the implemented filter banks. Finally, the obtained results are presented and compared with other state-of-the-art fusion frameworks in Section V, before we state our main conclusions in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MULTISCALE IMAGE FUSION</head><p>In general, pixel-level techniques can be divided into spatial and transform domain techniques. As for spatial domain techniques, the fusion is performed by combining all input images in a linear or non-linear fashion using weighted average, variance or total-variation based algorithms <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Transform domain techniques map (transform) each source image into the transform domain (e.g. wavelet domain), where the actual fusion process takes place. The final fused image is obtained by taking the inverse transform of the composite representation. The main motivation behind moving to the transform domain is to work within a framework, where the image's salient features are more clearly depicted than in the spatial domain.</p><p>While many different transforms have been proposed for image fusion purposes, most of the transform domain techniques use multiscale transforms. This is motivated by the fact that images tend to present features in many different scales. In addition, the human visual system seems to exhibit high similarities with the properties of multiscale transforms. More precisely, strong evidence exists that the entire human visual field is covered by neurons that are selective to a limited range of orientations and spatial frequencies, and can detect local features like edges and lines. This makes them very similar to the basis functions of multiscale transforms <ref type="bibr" target="#b24">[25]</ref>.</p><p>The usage of multiscale image transforms is not a recent approach in image fusion applications. The first multiscale image fusion approach was proposed by Burt <ref type="bibr" target="#b2">[3]</ref> in 1985 and is based on the Laplacian Pyramid in combination with a pixel-based maximum selection rule. The use of the DWT in image fusion was first proposed by Li et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>In their implementation the maximum absolute value within a window is chosen as an appropriate activity measure. In 2004 Pajares et al. <ref type="bibr" target="#b9">[10]</ref> published a DWT-based image fusion tutorial including an exhaustive study on coefficient merging techniques. About the same time Petrović and Xydeas <ref type="bibr" target="#b8">[9]</ref> presented another DWT-based approach which used a gradient image representation in combination with so-called gradient filters. The actual fusion was performed on the gradient images, prompting the authors to refer to their contribution as a "fuse-then-decompose" approach.</p><p>Despite the success of classical wavelet methods, some limitations reduce their effectiveness in certain situations. For example, wavelets rely on a dictionary of roughly isotropic elements and their basis functions are oriented only on a small number of directions, due to the standard tensor product construction in two dimensions <ref type="bibr">(2-D)</ref>. This led to the introduction of several new multiscale transforms in recent years, that are able to circumvent these shortcomings and capture the intrinsic properties of natural images better than classical multiscale transforms. Among them, the DTCWT <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and the NSCT <ref type="bibr" target="#b27">[28]</ref>, are extensively used in image fusion applications (see <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>). More recently, Li et al. <ref type="bibr" target="#b28">[29]</ref> conducted a performance study on different multiscale transforms for image fusion and stated that the best results for medical, multifocus and multisensor image fusion can be achieved using the NSCT, followed by the DTCWT and the UWT.</p><p>We will use the remainder of this section to briefly review the theory behind the UWT which will be needed later in this paper. Our proposed fusion framework will be introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Undecimated Wavelet Transform</head><p>While the decimated (bi)orthogonal wavelet transform is highly used in image compression algorithms such as JPEG-2000 <ref type="bibr" target="#b29">[30]</ref>, results are far from optimal for other applications such as image fusion. This is mainly due to the downsampling in each decomposition step of the DWT which may cause a large number of artifacts when reconstructing an image after modification of its wavelet coefficients <ref type="bibr" target="#b30">[31]</ref>. Thus, for applications such as image fusion, where redundancy is not a crucial factor, performance can be improved significantly by removing the decimation step in the DWT, leading to the non-orthogonal, translation-invariant UWT.</p><p>Like the DWT, the UWT is implemented using a filter bank which decomposes an one-dimensional (1-D) signal c 0 into a set W = {w 1 , . . . , w J , c J }, in which w j represents the highpass or wavelet coefficients at scale j and c J are the lowpass or approximation coefficients at the lowest scale J . The passage from one resolution to the next one is obtained using the "à trous" algorithm <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, where the analysis lowpass and analysis high-pass filter h and g are upsampled by 2 j when processing the j th scale, where j = 0, . . . , J -1. Thus, the UWT decomposition is defined as</p><formula xml:id="formula_0">c j +1 [n] = ( h( j ) * c j )[n] = m h[m]c j [n + 2 j m] w j +1 [n] = ( ḡ( j ) * c j )[n] = m g[m]c j [n + 2 j m] (1)</formula><p>where h[n] = h[-n] and h ( j ) [n] = h[ n 2 j ] if n 2 j is an integer and 0, otherwise. The reconstruction at scale j is obtained by</p><formula xml:id="formula_1">c j [n] = 1 2 ( h( j ) * c j +1 )[n] + ( g( j ) * w j +1 )[n]<label>(2)</label></formula><p>where h and g are the upsampled low-pass and high-pass synthesis filters, respectively. Perfect reconstruction holds if the used analysis and synthesis filters satisfy the condition</p><formula xml:id="formula_2">H (z -1 ) H (z) + G(z -1 ) G(z) = 1 ( 3 )</formula><p>in the z-transform domain, which provides additional freedom during the filter selection process compared to the DWT where, in addition to the perfect reconstruction condition, an anti-aliasing condition has to be satisfied as well.</p><p>The UWT can be extended to 2-D by</p><formula xml:id="formula_3">c j +1 [m, n] = h( j ) h( j ) * c j [m, n] w 1 j +1 [m, n] = h( j ) ḡ( j ) * c j [m, n] w 2 j +1 [m, n] = ḡ( j ) h( j ) * c j [m, n] w 3 j +1 [m, n] = ḡ( j ) ḡ( j ) * c j [m, n] (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where the rows and columns are filtered separately by h and g, leading to three high-pass or detail images w 1 , w 2 , w 3 per stage, corresponding to the horizontal, vertical and diagonal directions. The redundancy factor of an UWT J -level decomposition is 3 J + 1, since each high-pass image has the same size than the original image. Since the filters do not need to be (bi)orthogonal, an alternative approach in multispectral image fusion (e.g. fusion of high-resolution panchromatic images with low-resolution multispectral images) is to define g</p><formula xml:id="formula_5">[n] = δ[n] -h[n],</formula><p>where δ[n] represents an impulse at n = 0 <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In 2-D this yields g[m, n] = δ[m, n]h[m, n], which suggests that the detail images can be obtained by taking the difference between two successive approximation images</p><formula xml:id="formula_6">w j +1 [m, n] = c j [m, n] -c j +1 [m, n]. (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>Please note that, in this case, for each scale we only obtain one detail image and not three as in the general case [see eq. ( <ref type="formula" target="#formula_3">4</ref>)]. The reconstruction is obtained by co-addition of all detail images to the approximation image, that is</p><formula xml:id="formula_8">c 0 [m, n] = c J [m, n] + J j =1 w j [m, n]<label>(6)</label></formula><p>which implies that the synthesis filters are all-pass filters with h</p><formula xml:id="formula_9">[m, n] = g[m, n] = δ[m, n] [31]</formula><p>. A common choice for the analysis, low-pass filter h is a B-spline filter. In the literature this implementation of the UWT is known as Isotropic Undecimated Wavelet Transform <ref type="bibr" target="#b30">[31]</ref> or Additive Wavelet Transform <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. UWT-BASED FUSION SCHEME WITH SPECTRAL FACTORIZATION</head><p>As we have seen in the previous section, an input image can be represented in the transform domain by a sequence of detail images at different scales and orientations along with an approximation image at the coarsest scale. Hence, the multiscale decomposition of an input image I k can be represented as</p><formula xml:id="formula_10">y k = {y 1 k , y 2 k , . . . , y J k , x J k }<label>(7)</label></formula><p>where x J k represents the approximation image at the lowest scale J and y j k , j = 1, . . . , J represent the detail images at level j . These are comprised of various orientation bands</p><formula xml:id="formula_11">y j k = {y j k [• , 1], y j k [• , 2], . . . , y j k [• , P]}, p = 1, . . . , P.</formula><p>For convenience we will henceforth use the vector coordinate n = [m, n] to index the location of the coefficients. Thus, y j k [n, p] represents the detail coefficient of input image k, at location n, within decomposition level j and orientation band p. In order to simplify the discussion, we assume, without loss of generality, that the fused image will be generated from two source images I A and I B which are assumed to be registered prior to the fusion process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spectral Factorization</head><p>Plenty of transforms are at our disposal to perform image fusion tasks, among them the DWT, CVT and CT, as well as the UWT, DTCWT and NSCT. A first classification can be made based on the underlying redundancy and shift-variance of these transforms. Whereas the highly redundant UWT, DTCWT and NSCT are invariant to shifts occurring in the input images, the DWT, CVT and CT represent shift-variant transforms with no or limited redundancy. As stated in various studies (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b28">[29]</ref>), redundancy and shift-invariance are desirable properties in image fusion applications since they allow for a higher robustness to rapid changes in coefficient values, thus, reducing the amount of reconstruction errors in the fused image. Motivated by these observations, we will discard the DWT, CVT and CT and focus solely on redundant transforms in our ongoing discussion.</p><p>Another crucial point in multiscale pixel-level image fusion frameworks is the choice of an appropriate filter bank. Fig. <ref type="figure" target="#fig_1">2</ref> attempts to illustrate the impact of the length of the chosen filter bank on the fusion performance. In this example the highpass portions of two 1-D step functions are fused using one stage of the 2-tap Haar and 6-tap "db3" filters, <ref type="foot" target="#foot_0">1</ref> respectively. The applied fusion rule is a very simple "choose max" rule as expressed in eq. ( <ref type="formula" target="#formula_17">10</ref>). The high-pass subbands, obtained by applying the Haar filter, can be seen in Fig. <ref type="figure" target="#fig_1">2(b</ref>) and (f), whereas the result using the 6-tap "db3" filter is illustrated in Fig. <ref type="figure" target="#fig_1">2(c</ref>) and (g). It can be observed that the "db3" filter needs five coefficients to represent the step change. Thus, although most energy is concentrated in the central coefficient, the remaining four coefficients correspond to regions where no change in the signal value occurred. When attempting to fuse the two "db3" filtered high-pass subbands we are confronted with a problem, namely, to combine the two signals without losing information. This can be observed in Fig. <ref type="figure" target="#fig_1">2(h)</ref>, where not all non-zero coefficients from Fig. <ref type="figure" target="#fig_1">2(c</ref>) and (g) could be incorporated. On the other hand, the Haar filtered signal contains only one non-zero coefficient corresponding exactly to the position of the signal transition. Thus, as illustrated in Fig. <ref type="figure" target="#fig_1">2(d)</ref>, both non-zero coefficients are transferred to the fused image without any loss of information. Therefore, it can be concluded that filters with large support size may result in an undesirable spreading of coefficient values which, in case of salient features located very close to each other in both input images, may lead to coefficients with coincident localizations in the transform domain. Since it is difficult to resolve such overlaps, distortions may be introduced during the fusion process, such as ringing artifacts or even loss of information.</p><p>Although the situation depicted in Fig. <ref type="figure" target="#fig_1">2</ref> may seem at first somewhat artificial, we will see in the next sections that multisensor images and, among them, especially medical image pairs often exhibit similar properties. Hence, for these images the fusion performance considerably degrades with an increase of the filter size. We can therefore reduce the problem of choosing a proper redundant multiscale transform to its ability to incorporate a filter bank with a sufficiently small support size, thus, minimizing the coefficient spreading problem. From this point of view, the UWT appears to be an attractive choice, since, due to the standard tensor product construction in 2-D, the UWT offers directionality without increasing the overall length of the implemented filter banka property not shared by the NSCT and DTCWT. As for the NSCT, the increased filter lengths are mainly due to the iterated nature of the nonsubsampled directional filter bank involved (see <ref type="bibr" target="#b32">[33]</ref> for a thorough discussion on the construction of directional filter banks). In the case of the DTCWT, as reported in <ref type="bibr" target="#b26">[27]</ref>, the increased filter length is due to the half-sample delay condition imposed on the filter banks involved, which results in longer filters than in the real wavelet transform case.</p><p>Following the remarks stated so far, we are tempted to arrive at the conclusion that the best fusion results for source images derived from different sensor modalities, are obtained by simply applying the UWT in combination with the very short 2-tap Haar filter bank. Indeed, surprisingly good results are achieved using this simple fusion strategy for infraredvisible and medical image fusion. However, the Haar filter bank presents some well-known deficiencies, like the introduction of blocking artifacts when reconstructing an image after manipulation of its wavelet coefficients, which might deteriorate the fusion performance in certain situations. This is mainly due to the lack of regularity exhibited by the Haar wavelet <ref type="bibr" target="#b30">[31]</ref>. Roughly speaking, the regularity of a wavelet or scaling function [ψ(t) and φ(t), respectively] relates to the number of continuous derivatives that a wavelet has. In case of the Haar wavelet, the low-pass analysis filter, H (z), has only one zero at z = -1, leading to the well-known, nonsmooth Haar scaling function. In order to construct smoother scaling functions, more zeros have to be introduced at z = -1, inevitably leading to filters with longer support <ref type="bibr" target="#b33">[34]</ref>.</p><p>Based on these observations we arrive at the following question: How can we combine the advantages of filters with small support size with the ones of filter banks exhibiting a high degree of regularity in the context of image fusion? In conventional multiscale fusion approaches this dilemma usually results in a trade-off between short-length filters and filters with better regularity and frequency domain behavior, usually with a small bias towards filter banks with short support sizes. In this paper, we propose a novel UWT-based fusion approach that splits the filtering process into two successive filtering operations and performs the actual fusion after convolving the</p><formula xml:id="formula_12">H(z) x A x B x 2 A x 2 B F 1-z -1 1-z -2 H(z) 1-z -1 Q(z) y 1 F F Q(z 2 ) y 2 F H(z 2 ) H(z 2 ) 1-z -2</formula><p>Fig. <ref type="figure">3</ref>. Implementation of the UWT-based fusion scheme with spectral factorization for two decomposition levels in 1-D.</p><p>input signal with the first filter pair, exhibiting a significantly smaller support size than the original filter. The proposed method is based on the fact that the low-pass analysis filter H (z) and the corresponding high-pass analysis filter G(z) can always be expressed in the form</p><formula xml:id="formula_13">H (z) = 1 + z -1 P(z) G(z) = 1 -z -1 Q(z) (8)</formula><p>by spectral factorization in the z-transform domain. Thus, in our framework the input images are first decomposed by applying a Haar filter pair, represented by the first spectral factors (1 + z -1 ) and (1z -1 ), respectively. The resulting horizontal, vertical and diagonal detail images can afterwards be fused according to an arbitrary fusion rule. Next, the filter pair represented by the second spectral factor (P(z) and Q(z) in eq. ( <ref type="formula">8</ref>)), is applied to the approximation and fused detail images, yielding the first decomposition level of the proposed fusion scheme. For each subsequent level, the analysis filters are upsampled according to the "à trous" algorithm, leading to the following, generalized analysis filter bank</p><formula xml:id="formula_14">H z 2 j -1 = 1 + z -2 j -1 P z 2 j -1 G z 2 j -1 = 1 -z -2 j -1 Q z 2 j -1<label>(9)</label></formula><p>and the aforementioned procedure is recursively applied to the approximation images, until the desired number of decomposition levels is reached. After merging the low-pass approximation images, the final fused image is obtained by applying the inverse transform, using the corresponding synthesis filter bank without spectral factorization. The implementation of the proposed algorithm for two 1-D signals x A and x B , and two decomposition levels is depicted in Fig. <ref type="figure">3</ref>, where F symbolizes the fusion of the high-pass coefficients. It is important to stress that spectral factorization is not applied to the low-pass filter H (z) since it is assumed that all salient features of the input signals are embodied in the high-frequency coefficients. Although this assumption remains also true for images, when using separable filters the horizontal and vertical detail bands are obtained by applying both lowpass and high-pass filters to the columns and rows of the input images. Thus, it is necessary to apply spectral factorization also to the low-pass channel. Only in case of the low-low rows columns</p><formula xml:id="formula_15">I A I B y 1 F [•,1] y 1 F [•,2] x 1 A x 1 B 1+z -1 F P (z) y 1 F [•,3]</formula><p>rows columns channel (successive application of H (z) to the columns and rows of the input image) spectral factorization will not be employed. The implementation of the first stage of our image fusion framework is depicted in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><formula xml:id="formula_16">1-z -1 1+z -1 1-z -1 1+z -1 1-z -1 P (z) F Q(z) F Q(z) P (z) Q(z) P (z) Q(z)</formula><p>The novelty of the proposed fusion framework lies in its ability to combine the properties of filters with short support size with filters with large support size and therefore higher regularity. In more detail, due to the compact support of the used (1 ± z -2 j -1 ) factors the undesirable spreading of coefficient values in the neighborhood of salient features during the convolution process is largely reduced. This allows for a more reliable feature selection and reduces both the introduction of distortions and the loss of contrast information during the fusion process, conditions commonly observed in traditional multiscale fusion frameworks. The subsequent filtering with the second spectral factor accounts for the freedom of implementing an arbitrary filter bank (satisfying the perfect reconstruction condition), hence combining the advantages of a very short filter with the benefits of filters with higher orders. In other words, we avoid the introduction of blocking artifacts during reconstruction, as well as the coefficient spreading problem. Please note that the spectral factorization scheme, as presented in this subsection, cannot be straightforwardly adapted to the NSCT and the DTCWT. This is mainly due to the filter design restrictions imposed by these transforms, preventing the meaningful application of such a factorization scheme. As we are going to show later, the presented approach is particularly well suited for the fusion of infrared-visible and medical images, which tend to exhibit a high degree of information at coincident localizations. For these image groups the presented framework outperforms traditional fusion frameworks based on the DTCWT and NSCT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Rule</head><p>As for the combination of the input image pairs, a wide range of fusion rules can be found in the literature. In general, these rules vary greatly in terms of their complexity and effectiveness. The spectral factorization method proposed here can be employed together with any fusion rule. Therefore, in order to assess the effectiveness of the proposed method, we applied four different fusion rules.</p><p>The first investigated combination scheme is the simple "choose max" (CM) or maximum selection fusion rule. By this rule the coefficient yielding the highest energy is directly transferred to the fused decomposed representation. Hence, for each decomposition level j , orientation band p and location n, the fused, detail images y j F are defined as</p><formula xml:id="formula_17">y j F [n, p] = y j A [n, p] if y j A [n, p] &gt; y j B [n, p] y j B [n, p] otherwise . (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>This choice is motivated by the fact that salient features, such as edges, lines or other discontinuities, result in large magnitude coefficients, and thus can be captured using this combination scheme. However, the simple CM fusion rule does not take into account that, by construction, each coefficient within a multiscale decomposition is related to a set of coefficients in other orientation bands and decomposition levels. Hence, in order to conserve a given feature from one of the source images, all the coefficients corresponding to it have to be transferred to the composite multiscale representation as well. One way to improve the fusion results is therefore the use of intrascale grouping in combination with the CM fusion scheme of eq. ( <ref type="formula" target="#formula_17">10</ref>) (CM-IS)</p><formula xml:id="formula_19">y j F [n, p] = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ y j A [n, p] if Q q=1 y j A [n, q] &gt; Q q=1 y j B [n, q] y j B [n, p] otherwise<label>(11)</label></formula><p>where the fusion decision at each decomposition level is taken jointly for all orientation bands. Since the combination schemes of eqs. ( <ref type="formula" target="#formula_17">10</ref>) and ( <ref type="formula" target="#formula_19">11</ref>) suffer from a relative low tolerance against noise which may lead to a "salt and pepper" appearance of the selection maps, robustness can be added to the fusion process using an areabased selection criteria <ref type="bibr" target="#b34">[35]</ref>. For this purpose we expand the CM-IS combination scheme of eq. ( <ref type="formula" target="#formula_19">11</ref>) by defining the following fusion rule (CM-A): Calculate the activity a j k of each coefficient as the energy within a 3 × 3 window W centered at the current coefficient position n</p><formula xml:id="formula_20">a j k [n, p] = n∈W y j k [n + n, p] 2 (12) w j A [n, p] = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1 i f m j AB [n, p] ≤ T and Q q=1 a j A [n, q] &gt; Q q=1 a j B [n, q] 0 i f m j AB [n, p] ≤ T and Q q=1 a j A [n, q] ≤ Q q=1 a j B [n, q] 1 2 + 1 2 1-m j AB [n, p] 1-T if m j AB [n, p] &gt; T and Q q=1 a j A [n, q] &gt; Q q=1 a j B [n, q] 1 2 -1 2 1-m j AB [n, p] 1-T if m j AB [n, p] &gt; T and Q q=1 a j A [n, q] ≤ Q q=1 a j B [n, q] (16a) w j B [n, p] = 1 -w j A [n, p] (16b)</formula><p>and select the coefficient which yields the highest activity, again, by considering the intra-scale dependencies between coefficients from different orientation bands</p><formula xml:id="formula_21">y j F [n, p] = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ y j A [n, p] if Q q=1 a j A [n, q] &gt; Q q=1 a j B [n, q] y j B [n, p] otherwise . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>The fusion rules discussed so far work well under the condition that only one of the source images provides the most useful information. However, this assumption is not always valid and a fusion rule which uses a weighted combination of the transform coefficients may give better results. Following this reasoning we implement as the fourth fusion rule, a modified version of the one given by Burt and Kolczynski in <ref type="bibr" target="#b35">[36]</ref> (CM-AM). In their approach a match measure m j AB is calculated which is used to determine the similarity between the transformed source images</p><formula xml:id="formula_23">m j AB [n, p] = 2 n∈W y j A [n + n, p]y j B [n + n, p] a j A [n, p] + a j B [n, p] (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where W is a 3×3 window centered at n and a j k is the activity measure of eq. ( <ref type="formula">12</ref>). The fused coefficients y j F are given by the weighted average</p><formula xml:id="formula_25">y j F [n, p] = w j A [n, p]y j A [n, p] + w j B [n, p]y j B [n, p] (15)</formula><p>where the weights w j A and w j B are determined by eqs. (16a) and (16b) below, for some threshold T .</p><p>The low-pass approximation images will be treated differently by our combination schemes. Unlike the case of the detail images, high magnitudes in the approximation images do not necessarily correspond to important features within the source images. Thus, for all previously introduced combination schemes, the fused approximation coefficients x J F are obtained by a simple averaging operation</p><formula xml:id="formula_26">x J F [n] = x J A [n] + x J B [n] 2 . (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>In the literature more sophisticated and effective approximation image fusion rules can be found. However, as stated in <ref type="bibr" target="#b8">[9]</ref>, these rules have little influence on the overall fusion performance. Additionally, since our proposed fusion framework does not suggest any improvements regarding the fusion of the approximation images, eq. ( <ref type="formula" target="#formula_26">17</ref>) will suffice for the assessment of our method.</p><p>In the next section a new class of filters, which, as far as we know, has not been used in the context of image fusion previously, will be introduced. In more detail, we will place our emphasis on non-orthogonal filter banks which do not satisfy the anti-aliasing condition of the DWT and can therefore only be used in the nonsubsampled case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FILTER BANK DESIGN</head><p>Due to the nonsubsampled nature of the UWT, many ways exist to construct the fused image from its wavelet coefficients. For a given analysis filter bank (h, g), any synthesis filter bank ( h, g) satisfying the perfect reconstruction condition of eq. ( <ref type="formula">3</ref>) can be used for reconstruction. This is considerably simpler and offers more design freedom than in the decimated case, where an additional anti-aliasing condition has to be obeyed. As a consequence, filter banks can be used such that ( h, g) are positive, making the reconstruction more robust to ringing artifacts. In the remainder of this section such filters, which will later be used in our experiments, are explained in more detail. A more thorough discussion on filter bank design for undecimated wavelet decompositions can be found in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b36">[37]</ref>. Note that none of these filters obey the anti-aliasing condition and can therefore only be used in the undecimated case.</p><p>We will start our discussion with the Isotropic Undecimated Wavelet Transform from Section II-A, which is frequently used in multispectral image fusion. In this approach, only one detail image for each scale is obtained and not three as in the general case. It is implemented using the non-orthogonal, 1-D filter bank</p><formula xml:id="formula_28">h[n] = [1, 4, 6, 4, 1]</formula><p>16</p><formula xml:id="formula_29">g[n] = δ[n] -h[n] = [-1, -4, 10, -4, -1] 16 h[n] = g[n] =[ 0, 0, 1, 0, 0] (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>where h is derived from the B 3 -spline function. The standard three-directional UWT can be obtained by expanding the filter bank to 2-D as described by eq. ( <ref type="formula" target="#formula_3">4</ref>). This approach has some interesting characteristics. For example, due to the lack of convolutions during reconstruction ( h = g = δ), no additional distortions are introduced when constructing the fused image. Furthermore, since the fused image can be obtained by a simple co-addition of all detail images and the approximation image, a very fast reconstruction is possible. On the other hand, distortions introduced during the fusion process remain unfiltered in the reconstructed image.</p><p>Alternatively, if we choose h and g as before but define the synthesis low-pass filter h as h, we obtain a filter g given by g = δ + h. This yields filters with the following coefficients:</p><formula xml:id="formula_31">h[n] = h[n] = [1, 4, 6, 4, 1]</formula><p>16</p><formula xml:id="formula_32">g[n] = δ[n] -h[n] = [-1, -4, 10, -4, -1] 16 g[n] = δ[n] + h[n] = [1, 4, 22, 4, 1] 16 . (<label>19</label></formula><formula xml:id="formula_33">)</formula><p>In this scenario g consists entirely of positive coefficients, being thus no longer related to a wavelet function. On the other hand, such a lack of oscillations provides a reconstruction less vulnerable to ringing artifacts. Additionally, distortions introduced during the fusion stage are not transferred unprocessed to the reconstructed image as in the standard case where only summations are involved during reconstruction.</p><p>A slight variation of the previous example is obtained by defining g = δh * h, resulting in the filter bank</p><formula xml:id="formula_34">h[n] = h[n] = [1, 2, 1] 4 g[n] = δ[n] -h[n] * h[n] = [-1, -4, 10, -4, -1] 16 g[n] = δ[n] = [ 0, 0, 1, 0, 0] (<label>20</label></formula><formula xml:id="formula_35">)</formula><p>where h is derived from the B 1 -spline function. Finally, we would like to point out that plenty of other alternatives exist. For example the filter bank</p><formula xml:id="formula_36">h[n] = [1, 1] 2 g[n] = [-1, 2, -1] 4 h[n] = [1, 3, 3, 1] 8 g[n] = [1, 6<label>, 1] 4 (21)</label></formula><p>also leads to a solution where both synthesis filters are positive. We will see in the next section that these filters, in combination with spectral factorization, yield superior fusion results compared to traditional techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section the performance of the proposed fusion framework will be investigated, using three different sets of image-pairs. The first set consists solely of infrared-visible image pairs, whereas the second and third group comprise medical and multifocus images, respectively. The corresponding thumbnails of all used source images, divided into their corresponding groups, are illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>The performance of the proposed UWT fusion scheme with spectral factorization is compared to the results obtained by applying the NSCT, the DTCWT and the UWT without spectral factorization. As for the NSCT and DTCWT, we followed the recommendations published in <ref type="bibr" target="#b28">[29]</ref> regarding the filter choices and (in case of the NSCT) number of directions. Table <ref type="table" target="#tab_1">I</ref> lists the used settings for the NSCT and DTCWT for each image group. Further information on the used DTCWT and NSCT filters can be found in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>, respectively.</p><p>In case of the UWT-based image fusion, we will mainly concentrate on the filters from Section IV. Hence, in our experiments the non-orthogonal filter banks from eqs. ( <ref type="formula" target="#formula_29">18</ref>)-( <ref type="formula">21</ref>) will be used. Additionally, we will also consider some biorthogonal filters, which are frequently used in image processing applications such as the LeGall 5/3, CDF 9/7 and Rod 6/6 filter bank <ref type="bibr" target="#b37">[38]</ref>. In order to avoid referring to filter banks by their respective equation numbers, we will associate the following names to them. Henceforth, the filter banks presented in eqs. ( <ref type="formula" target="#formula_29">18</ref>)-( <ref type="formula">21</ref>) will be referred to as "Spline_1", "Spline_2", "Spline_3" and "Haar_1" filter banks, respectively. Please note that, in case of the NSCT and DTCWT, different filter banks have been used for each of the three classes of input images, according to Table <ref type="table" target="#tab_1">I</ref>. In contrast, for the UWT-based approaches, the same filter banks will be used for all three image classes. For all transforms four decomposition levels are chosen.</p><p>As for the objective assessment of multiscale image fusion, a considerable number of evaluation metrics can be found in the literature (see <ref type="bibr" target="#b38">[39]</ref> for an overview). Among them, nonreference fusion scores which evaluate fusion for a large set of different source images without presuming knowledge of a ground truth are of particular interest. These metrics consider only the input images and the fused image to produce a single numerical score that indicates the success of the fusion process <ref type="bibr" target="#b39">[40]</ref>. In this work we used three such non-reference fusion metrics to evaluate the achieved results. These are the performance measure proposed by Xydeas and Petrović Q AB/F <ref type="bibr" target="#b40">[41]</ref>, the third fusion metric proposed by Piella Q P in <ref type="bibr" target="#b41">[42]</ref>, as well as the Mutual Information (MI), first introduced by Qu et al. <ref type="bibr" target="#b42">[43]</ref> in the context of image fusion. All three fusion scores express the overall fusion performance as a normalized score where values closer to 1 indicate a higher quality of the composite image. These metrics belong to the most frequently used fusion scores and, as was shown in <ref type="bibr" target="#b39">[40]</ref>, correspond well with subjective evaluation. However, it is important to point out that they only provide a relative assessment on how the input images were fused rather than on the overall quality of the fused image <ref type="bibr" target="#b38">[39]</ref>. Consequently, visual inspection is still necessary to confirm the obtained results.</p><p>Tables II-IV list the average results as well as the corresponding standard deviations (σ ) for all infrared-visible, medical and multifocus image pairs, respectively, obtained by applying the DTCWT, NSCT and UWT with and without spectral factorization. In all of these simulations the low-pass approximation images are fused using the averaging operation given in eq. ( <ref type="formula" target="#formula_26">17</ref>), whereas the fused detail images are obtained by applying the "choose max" (CM) fusion rule of eq. ( <ref type="formula" target="#formula_17">10</ref>). It can be noted that the proposed spectral factorization method works well for infrared-visible and medical image pairs, but does not yield any improvements for multifocus image pairs. In a nutshell, this is due to the fact that multifocus image pairs only differ in their high frequency content but are identical otherwise. Thus, the source images tend not to contain salient features at coincident localizations. Therefore, a situation as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, where the effect of the coefficient spreading problem for two 1-D step functions is shown, is unlikely to occur. Consequently, for multifocus images, the application of filters with small support size yields no benefits, and, as can be seen in Table <ref type="table" target="#tab_6">IV</ref>, best results are achieved using the NSCT and DTCWT. For infrared-visible and medical image pairs the situation is substantially different. Since these image types come from different sensors, they exhibit a high degree of dissimilarity between different spectral bands. Hence, the application of filters with small support prior to the fusion process considerably improves the fusion result. We will start our discussion with Table <ref type="table" target="#tab_2">II</ref>, which lists the results for infrared-visible image fusion. By looking at the second column, exhibiting the average results for the Q AB/F fusion metric, it can be noted that the proposed method yields significantly better results for all filter banks under test, compared to the results for the DTCWT, NSCT and UWT without spectral factorization, suggesting that the edges are better preserved using the UWT with spectral factorization. This is a particularly important result since the preservation of salient information is one of the main motivations of this work. In the case of the MI fusion metric, improvements are achieved for all non-orthogonal filter banks. On the other hand, for the Q P fusion metric the proposed method yields no gains. Furthermore, it can be seen that the best scores are obtained for the non-orthogonal filter banks introduced in Section IV. Thus, this indicates that the increased filter design freedom of the UWT leads to filter banks which perform well in the context of infrared-visible image fusion. Finally, we would like to point out that the proposed spectral factorization framework significantly outperforms the fusion results obtained by state-of-the-art transforms such as the DTCWT and NSCT for all three fusion metrics.</p><p>The results of the fusion of an infrared-visible image pair using the DTCWT, NSCT and UWT with and without spectral factorization are shown in Fig. <ref type="figure" target="#fig_4">6</ref>. The "Haar_1" filter bank was employed in the UWT approaches. Examining the results on the zoomed images, illustrated in Fig. <ref type="figure" target="#fig_4">6</ref>(e)-(h), the contours of the UWT-based fusion approaches seem to be slightly more accentuated. This is particularly visible when observing the persons lower body part, displayed in the center of the image.</p><p>When examining the results shown in Table <ref type="table" target="#tab_2">III</ref>, the same conclusions can be drawn for the set of medical images. However, since medical image pairs present, in general, an elevated number of regions, exhibiting information at coincident localizations, our approach yields even better results for these images than for the set of infrared-visible images. This gain in fusion performance is most apparent when looking at the Q AB/F fusion score of the two image groups. Whereas for both image classes a considerable improvement is achieved for all filter banks, the gain is more than twice as high for medical image pairs. A similar tendency can be observed for the MI fusion metric, where the UWT fusion with spectral factorization produces a higher score for all tested filter banks, again suggesting the superiority of the proposed approach. In contrast, a moderate drop in fusion performance occurs for the Q P metric. However, it should be pointed out that this does not agree with subjective perception, as shown in the medical images fusion example (Fig. <ref type="figure" target="#fig_5">7</ref>). As before, best results are obtained when using the non-orthogonal filter banks of Section IV. Furthermore, the proposed method yields superior results for all three objective metrics when compared to conventional methods based on the NSCT and DTCWT. Fig. <ref type="figure" target="#fig_5">7</ref> shows the results for the fusion of a medical image pair, obtained by applying the DTCWT-and NSCT-based fusion scheme, as well as the UWT-based fusion scheme with and without spectral factorization in combination with the "Haar_1" filter bank. Looking at the results obtained for the DTCWT and NSCT, it can be observed that both schemes suffer from a significant loss of edge information, particularly noticeable at the outermost borders of the zoomed images [Fig. <ref type="figure" target="#fig_5">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e)-(h)]</head><p>. There, information belonging to the skull bone (white stripe enclosed within the gray, tube-like structure) partially disappeared. This is due to the superposition of the skull bones, originating from the medical source image pair, resulting in coefficient overlaps in the DTCWT and NSCT transform domain, which cannot be resolved by the fusion algorithm. As for the fusion results obtained with the UWT, this effect is reduced to a minimum and the edge information is preserved to a much higher degree. Moreover, in case of the UWT with spectral factorization, the edges appear to be slightly more accentuated than in the fusion scenario without spectral factorization, thus indicating the perceptual superiority of the proposed spectral factorization approach.</p><p>To demonstrate the independence of the achieved results with respect to the underlying fusion rule, Figs. <ref type="figure">8</ref> and<ref type="figure">9</ref> show the average fusion results for all infrared-visible and medical image pairs, respectively, employing several different combination schemes. In more detail, we utilized the four fusion schemes discussed in Section III-B in combination with the DTCWT and NSCT, as well as with the UWT with and without our proposed spectral factorization approach   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CM-IS CM with intra-scale grouping (11)</head><p>CM-A CM-IS with window-based activity measure <ref type="bibr" target="#b11">(12)</ref> and ( <ref type="formula" target="#formula_21">13</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CM-AM</head><p>Fusion rule by Burt and Kolczynski <ref type="bibr" target="#b35">[36]</ref> (12), ( <ref type="formula" target="#formula_23">14</ref>)- <ref type="bibr" target="#b15">(16)</ref> (in Figs. 8 and 9 referred to as UWT and UWT-SF, respectively) and grouped the results in accordance with the used fusion metric. Table <ref type="table" target="#tab_6">V</ref> gives an overview on the used fusion rules for all detail images. The coefficients from the approximation image were fused using the averaging operation of eq. ( <ref type="formula" target="#formula_26">17</ref>). As for the UWT-based approaches, the "Haar_1" filter bank was employed for all infrared-visible image pairs whereas the "Spline_2" filter bank was used for the set of medical image pairs. By observing the results it can be noted that for all investigated fusion schemes the best results are achieved using the proposed spectral factorization method. In fact for infrared-visible image pairs it only ranks second for the Q P fusion metric together with the CM fusion rule, whereas for medical image pairs it gains first place for the Q AB/F and MI fusion metric and only ranks second for the Q P score. Note that this is in accordance with the results presented in Tables <ref type="table" target="#tab_2">II</ref> and<ref type="table" target="#tab_2">III</ref>. Two important conclusions can be drawn from this observation: a) the introduced fusion framework with spectral factorization indeed tends to generate the best multiscale fusion results independent of the employed fusion rule and b) no tested combination scheme was able to resolve the problems originating from the superposition of coefficient values within the same spectral band. Consequently, since the probability of coefficients with coincident localizations can be directly associated with the support length of the applied filter bank, our proposed framework with spectral factorization can in fact be considered as a good alternative to alleviate the original problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>A novel UWT-based pixel-level image fusion approach is presented in this paper. It successfully improves fusion results for images exhibiting features at nearby located or coincident pixel locations -conditions commonly but not exclusively found in multisensor imagery. Our method spectrally divides the analysis filter pair into two factors which are then separately applied to the input image pair, splitting the image decomposition procedure into two successive filter operations. The actual fusion step takes place after convolution with the first filter pair. It is equivalent, as far as the coefficient spread is concerned, to a filter with significantly smaller support size than the original filter pair. Thus, the effect of the coefficient spreading problem, which tends to considerably complicate the feature selection process, is successfully reduced. This leads to a better conservation of features which are located close to each other in the input images. In addition, this solution leaves room for further improvements by taking advantage of the nonsubsampled nature of the UWT, which permits the design of non-orthogonal filter banks where both synthesis filters exhibit only positive coefficients. Such filters provide a reconstructed, fused image less vulnerable to ringing artifacts.</p><p>The obtained experimental results have been analyzed in terms of the three objective metrics Q AB/F , MI and Q P . They showed that for multisensor images, such as infrared-visible and medical image pairs, the proposed spectral factorization framework significantly outperforms fusion schemes based on state-of-the-art transforms such as the DTCWT and NSCT, independent of the underlying fusion rule. Additionally, the perceptual superiority of the proposed framework was suggested by informal visual inspection of a fused infrared-visible as well as a fused medical image pair.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Coefficient spreading effect. (a) and (e) Input signals. (b) and (f) Haar filtered input signals. (c) and (g) "db3" filtered input signals. (d) Fusion of the Haar filtered signals. (h) Fusion of the "db3" filtered signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Implementation of the first stage of the UWT-based fusion scheme with spectral factorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Thumbnails of all image pairs used for evaluation purposes. (a) Infrared-visible images (ten pairs). Top row: infrared images. Bottom row: visible images. (b) Medical images (five pairs). (c) Multifocus images (five pairs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Fusion results for an infrared-visible image pair. (a) DTCWT fused. (b) NSCT fused. (c) UWT fused without spectral factorization. (d) UWT fused with spectral factorization. (e)-(h) Zoomed-in versions of (a)-(d).</figDesc><graphic coords="10,326.51,429.89,107.30,108.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Fusion results for a medical image pair. (a) DTCWT fused. (b) NSCT fused. (c) UWT fused without spectral factorization. (d) UWT fused with spectral factorization. (e)-(h) Zoomed-in versions of (a)-(d).</figDesc><graphic coords="11,63.47,421.73,107.30,107.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Comparison of different fusion rules for infrared-visible image pairs using (a) Q AB/F , (b) MI, and (c) Q P fusion metrics.</figDesc><graphic coords="11,177.59,176.45,107.30,107.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Multiscale Image Fusion Using the Undecimated Wavelet Transform With Spectral Factorization and Nonorthogonal Filter Banks Andreas Ellmauthaler, Student Member, IEEE, Carla L. Pagliari, Senior Member, IEEE, and Eduardo A. B. da Silva, Senior Member, IEEE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I TRANSFORM</head><label>I</label><figDesc>SETTINGS FOR THE NSCT AND DTCWT (ACCORDING TO<ref type="bibr" target="#b28">[29]</ref>). THE NSCT FILTER BANKS TO THE LEFT (THIRD COLUMN) ARE APPLIED DURING THE NONSUBSAMPLED PYRAMIDAL DECOMPOSITION STAGE WHEREAS THE FILTER BANKS ON THE RIGHT SIDE (FOURTH COLUMN) ARE USED WITHIN THE NONSUBSAMPLED DIRECTIONAL DECOMPOSITION. THE NUMBER OF DIRECTIONAL DECOMPOSITIONS, IN INCREASING ORDER FROM THE FIRST TO THE FOURTH STAGE, IS GIVEN IN THE LAST COLUMN. AS FOR THE DTCWT, THE FILTER BANKS TO THE LEFT ARE EMPLOYED IN THE FIRST DECOMPOSITION STAGE WHEREAS THE FILTER BANKS ON THE RIGHT HAND SIDE</figDesc><table><row><cell></cell><cell cols="3">ARE APPLIED IN ALL REMAINING STAGES</cell><cell></cell></row><row><cell>Image Class</cell><cell>Transform</cell><cell>Filters</cell><cell></cell><cell>Directions</cell></row><row><cell>Infrared-</cell><cell>NSCT</cell><cell>pyrexc</cell><cell>7-9</cell><cell>[4, 8, 8, 16]</cell></row><row><cell>visible</cell><cell>DTCWT</cell><cell>LeGall 5/3</cell><cell>Q-Shift 06</cell><cell></cell></row><row><cell>Medical</cell><cell>NSCT DTCWT</cell><cell>pyrexc LeGall 5/3</cell><cell>vk Q-Shift 06</cell><cell>[4, 8, 8, 16]</cell></row><row><cell>Multifocus</cell><cell cols="3">NSCT DTCWT Near Symmetric 5/7 Q-Shift 06 CDF 9/7 7-9</cell><cell>[4, 8, 8, 16]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II FUSION</head><label>II</label><figDesc>RESULTS FOR INFRARED-VISIBLE IMAGE PAIRS. (a) DTCWT</figDesc><table><row><cell cols="7">AND NSCT. (b) UWT WITHOUT SPECTRAL FACTORIZATION.</cell></row><row><cell></cell><cell cols="5">(c) UWT WITH SPECTRAL FACTORIZATION</cell></row><row><cell>Transform</cell><cell>Q AB/F</cell><cell></cell><cell>MI</cell><cell></cell><cell>Q P</cell></row><row><cell></cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell></row><row><cell cols="7">DTCWT 0.566430 0.070921 0.153833 0.050997 0.770684 0.046468</cell></row><row><cell>NSCT</cell><cell cols="6">0.578594 0.073724 0.156266 0.052491 0.771908 0.048908</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell></row><row><cell>Filter Bank</cell><cell>Q AB/F</cell><cell></cell><cell>MI</cell><cell></cell><cell>Q P</cell></row><row><cell></cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell></row><row><cell>Haar_1</cell><cell cols="6">0.578324 0.076288 0.155425 0.048781 0.775986 0.045510</cell></row><row><cell>Spline_1</cell><cell cols="6">0.561764 0.074939 0.154563 0.049088 0.748338 0.058984</cell></row><row><cell>Spline_2</cell><cell cols="6">0.579896 0.078303 0.154644 0.047471 0.774538 0.046703</cell></row><row><cell>Spline_3</cell><cell cols="6">0.585656 0.075411 0.156754 0.049897 0.776707 0.046627</cell></row><row><cell cols="7">LeGall 5/3 0.576931 0.072587 0.156904 0.045766 0.774282 0.045766</cell></row><row><cell>CDF 9/7</cell><cell cols="6">0.570694 0.072329 0.154570 0.052142 0.770903 0.046824</cell></row><row><cell>Rod 6/6</cell><cell cols="6">0.577536 0.072789 0.157002 0.053023 0.774115 0.046316</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>Filter Bank</cell><cell>Q AB/F</cell><cell></cell><cell>MI</cell><cell></cell><cell>Q P</cell></row><row><cell></cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell><cell>Mean</cell><cell>σ</cell></row><row><cell>Haar_1</cell><cell cols="6">0.594900 0.073936 0.157380 0.051220 0.775059 0.046774</cell></row><row><cell>Spline_1</cell><cell cols="6">0.581765 0.073912 0.155516 0.050239 0.761057 0.053034</cell></row><row><cell>Spline_2</cell><cell cols="6">0.593351 0.076468 0.156556 0.049104 0.772665 0.047424</cell></row><row><cell>Spline_3</cell><cell cols="6">0.595297 0.073612 0.157180 0.050948 0.773945 0.047559</cell></row><row><cell cols="7">LeGall 5/3 0.588013 0.069352 0.156399 0.052805 0.773715 0.046523</cell></row><row><cell>CDF 9/7</cell><cell cols="6">0.578808 0.068751 0.153303 0.051315 0.767235 0.048988</cell></row><row><cell>Rod 6/6</cell><cell cols="6">0.584832 0.070085 0.156256 0.052504 0.771131 0.047787</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V OVERVIEW</head><label>V</label><figDesc>ON THE USED FUSION RULES</figDesc><table><row><cell>Abbreviation Description</cell><cell>Equation(s)</cell></row></table><note><p><p><p>CM</p>"Choose Max" fusion rule</p><ref type="bibr" target="#b9">(10)</ref> </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the course of this work filters are referred to by their respective names within the Matlab Wavelet Toolbox. More information can be found at http://www.mathworks.com/products/wavelet/.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Brazilian Funding Agency CAPES (Projeto Pro-Defesa). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Xiaolin Wu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Andreas Ellmauthaler (S <ref type="bibr">'12)</ref> was born in Schwarzach im Pongau, Austria. He received the Dipl.Ing. (FH) degree telecommunications engineering from the University of Applied Sciences Salzburg, Salzburg, Austria, and the M.Sc. degree in computer sciences from Halmstad University, Halmstad, Sweden, both in 2007. He is currently pursuing the Ph.D. degree in electrical engineering with the Federal University of Rio de Janeiro (COPPE/UFRJ), Rio de Janeiro, Brazil.</p><p>He was a Research Assistant with the University of Applied Sciences Salzburg from 2007 to 2009, where he was involved in research with several industrial projects. His current research interests include digital signal and image processing, especially multiscale transforms and its applications to image fusion, as well as multiple-view systems. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A categorization of multiscaledecomposition-based image fusion schemes with a performance study for a digital camera application</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1999-08">Aug. 1999</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1315" to="1326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pixel-based and region-based image fusion schemes using ICA bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pyramid as a structure for efficient computation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiresolution Image Processing and Analysis</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="6" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image fusion by a ratio of low-pass pyramid</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image fusion by using steerable pyramid</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="929" to="939" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive wavelets and their applications to image fusion and compression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Amsterdam</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region-based image fusion scheme for concealed weapon detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annu. Conf</title>
		<meeting>31st Annu. Conf</meeting>
		<imprint>
			<date type="published" when="1997-04">Apr. 1997</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multisensor image fusion using the wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Models Image Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based multiresolution image fusion</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Xydeas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="237" />
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A wavelet-based image fusion tutorial</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pajares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cruz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1855" to="1872" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image sequence fusion using a shift-invariant wavelet transform</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rockinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="288" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiresolution-based image fusion with additive wavelet decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arbiol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1204" to="1211" />
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-driven fusion of high spatial and spectral resolution images based on oversampled multiresolution analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2300" to="2312" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Redundant versus orthogonal wavelet decomposition for multisensor image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chibani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Houacine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="887" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pixel-and region-based image fusion with complex wavelets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>O'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual tree discrete wavelet transform with application to image fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Adhami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Southeastern Symp. Syst. Theory</title>
		<meeting>38th Southeastern Symp. Syst. Theory</meeting>
		<imprint>
			<date type="published" when="2006-03">Mar. 2006</date>
			<biblScope unit="page" from="430" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fusion of multispectral and panchromatic satellite images using the curvelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="140" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Remote sensing image fusion using the curvelet transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image fusion based on a new contourlet packet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image fusion using nonsubsampled contourlet transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Image Graph</title>
		<meeting>4th Int. Conf. Image Graph</meeting>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
			<biblScope unit="page" from="719" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multifocus image fusion using the nonsubsampled contourlet transform</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="1334" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hybrid multiresolution method for multisensor multimodal image fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1519" to="1526" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A real time pixel-level based image fusion via adaptive weight averaging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farooq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Inf. Fusion</title>
		<meeting>3rd Int. Conf. Inf. Fusion</meeting>
		<imprint>
			<date type="published" when="2000-07">Jul. 2000</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>WeC3/3-WeC3/13</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A total variation-based algorithm for pixellevel image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2137" to="2143" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scale-invariance and self-similar &apos;wavelet&apos; transforms: An analysis of natural scenes and mammalian visual systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets, Fractals and Fourier Transforms: New Developments and New Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="151" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex wavelets for shift invariant analysis and filtering of signals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmonic Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="234" to="253" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The dual-tree complex wavelet transform</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Selesnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="123" to="151" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The nonsubsampled contourlet transform: Theory, design, and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Da Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3089" to="3101" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance comparison of different multiresolution transforms for image fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m">Information Technology -JPEG-2000 Image Coding System: Core Coding System, ISO Standard 15444-1:2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The undecimated wavelet decomposition and its reconstruction</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="309" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: Wedding the a trous and Mallat algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992-10">Oct. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Directional multiresolution image representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Electr. Eng., Swiss Federal Inst. Technology Lausanne</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Lausanne, Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<title level="m">A Wavelet Tour of Signal Processing: The Sparse Way</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multisensor pixel-level image fusion</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Petrović</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Manchester, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Stat., Univ. Manchester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhanced image capture through fusion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kolczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Comput. Vis</title>
		<meeting>4th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Oversampled filter banks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cvetkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1245" to="1255" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient decompositions for signal coding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A M</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COPPE</title>
		<imprint>
			<date type="published" when="1999-03">Mar. 1999</date>
			<pubPlace>Rio de Janeiro, Brazil</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>UFRJ</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision: A comparative study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="109" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Subjective tests for image fusion evaluation and objective metric validation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="216" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Objective image fusion performance measure</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Xydeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="309" />
			<date type="published" when="2000-02">Feb. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new quality metric for image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heijmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information measure for performance of image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="313" to="315" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
