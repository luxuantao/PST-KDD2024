<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Self-Organizing Maps: Background, Theories, Extensions and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hujun</forename><surname>Yin</surname></persName>
							<email>hujun.yin@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<postCode>M60 1QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Self-Organizing Maps: Background, Theories, Extensions and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5114E3CE1D0C647E0FA9391B3D172A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For many years, artificial neural networks (ANNs) have been studied and used to model information processing systems based on or inspired by biological neural structures. They not only can provide solutions with improved performance when compared with traditional problem-solving methods, but also give a deeper understanding of human cognitive abilities. Among various existing neural network architectures and learning algorithms, Kohonen's selforganizing map (SOM) <ref type="bibr" target="#b45">[46]</ref> is one of the most popular neural network models. Developed for an associative memory model, it is an unsupervised learning algorithm with a simple structure and computational form, and is motivated by the retina-cortex mapping. Self-organization in general is a fundamental pattern recognition process, in which intrinsic inter-and intra-pattern relationships among the stimuli and responses are learnt without the presence of a potentially biased or subjective external influence. The SOM can provide topologically preserved mapping from input to output spaces. Although the computational form of the SOM is very simple, numerous researchers have already examined the algorithm and many of its problems, nevertheless research in this area goes deeper and deeper -there are still many aspects to be exploited.</p><p>In this Chapter, we review the background, theories and statistical properties of this important learning model and present recent advances from various pattern recognition aspects through a number of case studies and applications. The SOM is optimal for vector quantization. Its topographical ordering provides the mapping with enhanced fault-and noise-tolerant abilities. It is also applicable to many other applications, such as dimensionality reduction, data visualization, clustering and classification. Various extensions of the SOM have been devised since its introduction to extend the mapping as effective solutions for a wide range of applications. Its connections with other learning paradigms and application aspects are also exploited. The Chapter is intended to serve as an updated, extended tutorial, a review, as well as a reference for advanced topics in the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Kohonen's self-organizing map (SOM) is an abstract mathematical model of topographic mapping from the (visual) sensors to the cerebral cortex. Modeling and analyzing the mapping are important to understanding how the brain perceives, encodes, recognizes and processes the patterns it receives and thus, if somewhat indirectly, are beneficial to machine-based pattern recognition. This Section looks into the relevant biological models, from two fundamental phenomena involved -lateral inhibition and Hebbian learning -to Willshaw and von der Malsburg's self-organization retinotopic model, and then subsequently to Kohonen's simplified and abstracted SOM model. Basic operations and the SOM algorithm, as well as methods for choosing model parameters, are also given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Biological Background: Lateral Inhibition and Hebbian Learning</head><p>Human visual perception and the brain make up the most complex cognition system and the most complex of all biological organs. Visual inputs contribute to over 90% of the total information (from all sensors) entering the brain. Nature and our living environment are constantly shaping our perception and cognition systems. Physiologically, human and indeed other animal visual (and other perception) systems have been evolved to so many different types of eyes and mammalian visual pathways for different purposes and conditions. For example, many insects have compound eyes, which have good temporal resolution and are more directionally sensitive and at the same time make them smaller and group them into a single structure -giving insects a better ability to detect spatial patterns and movement in order to escape from predators. Compound eyes have good time resolving power. Human eyes need 0.05 second to identify objects, while compound eyes need only 0.01 second. That is, they are good at recognizing (fast) moving objects. Eyes of large animals including humans have evolved to single-chambered 'camera lens' eyes, which have excellent angle resolution and are capable of seeing distant objects. Camera eyes have great space resolving power: high spatial resolution for good details of objects and patterns, and long depth resolution for both very near and very far objects. They also have brilliant sensitivities for light intensity -over 20 billion times (that is, 206 dB) range (the brightest to the darkest) -compared with most digital cameras, which are below 16-bit resolution <ref type="bibr">(30 dB)</ref>.</p><p>What information do eyes extract from the retina or sensory cells? Visual information is processed in both the retina and brain, but it is widely believed and verified that most processing is done in the retina, such as extracting lines, angles, curves, contrasts, colours, and motion. The retina then encodes the information and sends through optic nerves and optic chiasma, where some left and right nerves are crossed, to the brain cortex in the left and/or right hemispheres. The retina is a complex neural network. Figure <ref type="figure" target="#fig_0">1</ref> shows a drawing of the cross section of the retina. The human retina has over 100 million photosensitive cells (combining rods and cones), processing in parallel the raw images, codes and renders to just over one million optic nerves, to be transmitted in turn to the brain cortex.</p><p>The Perceptron models some cells in the retina, especially the bipolar and ganglion cells. These cells take inputs from the outputs of cells in the previous layer. To put many units together and connect them into layers, one may hope the resulting network -the Multi-Layer Perceptron -will have some functionality similar to the retina (despite neglecting some horizontal interconnections). Indeed, such a structure has been demonstrated as being capable of certain cognitive and information processing tasks.</p><p>Cells in neural networks (either in the retina or brain) also connect and interact horizontally. The experiment on limulus, or the horseshoe crab, by Haldan K. Hartline (1967 Nobel Prize Laureate) and his colleagues in the 1960s, has confirmed such processing on the limulus retina (the surface of the compound eye is shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>). They revealed the so-called 'lateral inhibition' activity among the retina cells. In other words, there exist both short-range excitatory interaction between nearby cells, as well as long-range inhibitory interaction between distant neighbours. This consequently explains the so-called 'Mach band' phenomenon on the edges or sharp changes of light intensity <ref type="bibr" target="#b86">[87]</ref> -see Fig. <ref type="figure" target="#fig_1">2(b)</ref>. </p><formula xml:id="formula_0">)</formula><p>recordings of spike rate in the ommatidium axon (the upper curve is the response to the small spot light at high and low intensities corresponding to those of the test pattern in the insert; the lower curve is the response to the rectangular lighter/darker test pattern (from <ref type="bibr" target="#b86">[87]</ref>; also see <ref type="bibr" target="#b98">[99]</ref>) Fig. <ref type="figure">3</ref>. Artistic drawing of a woman (left), and applying Pearson and Ronbinson's edge detector -an improved Marr and Hildreth mask -on the photo of the same woman (right) ( <ref type="bibr" target="#b85">[86]</ref>, reprinted by permission of Cambridge University Press; also cited in <ref type="bibr" target="#b8">[9]</ref>)</p><p>Lateral inhibition tells us that neurons in the retina do not just feed the information to upper levels, but also perform an important visual processing task: edge detection and enhancement. Figure <ref type="figure">3</ref> demonstrates a psychological experiment that also confirms such fundamental processing in visual perception.</p><p>Neural networks present completely different approaches to computing and machine intelligence from traditional symbolic AI. The goal is to emulate the way that natural systems, especially brains, perform on various cognitive or recognition tasks. When a network of simple processing units interconnect with each other, there are potentially a massive number of synaptic weights available to be configured and modified such that the network will suit a particular task. This configuration and modification process is carried out by a learning procedure, that is, learning or training algorithm. The way these simple units connect together is called the neural architecture. There are two basic types: feed-forward, in which layers of neurons are concatenated, and recurrent, in which neurons have feedback from themselves and others. Examples of these two architectures are the Multi-Layer Perceptron (MLP), and the Hopfield network, respectively.</p><p>The Oxford English Dictionary defines learning as "the process which leads to the modification of behavior". <ref type="foot" target="#foot_0">1</ref> Learning in general intelligent systems is often referred to as a process of the systems' adaptation to their experience or environment -a key feature of intelligence. According to Hebb, learning occurs when "some growth process or metabolic change takes place" <ref type="bibr" target="#b29">[30]</ref>. Learning in the context of neural networks can be defined as <ref type="bibr" target="#b28">[29]</ref>:</p><p>"Learning is a process by which the free parameters of neural networks are adapted through a process of simulation by the environment in which the network is embedded. The type of learning is determined by the manner in which the parameter changes take place."</p><p>Neural networks also differ from traditional pattern recognition approaches, which usually require solving some well-defined functions or models, such as feature extraction, transformation, and discriminant analysis by a series of processing steps. Neural networks can simply learn from examples. Presented repeatedly with known examples of raw patterns and with an appropriate learning or training algorithm, they are able to extract the most intrinsic nature of the patterns and perform recognition tasks. They will also have the ability to carry out similar recognition tasks, not only on trained examples but also on unseen patterns. Learning methods and algorithms undoubtedly play an important role in building successful neural networks.</p><p>Although many learning methods have been proposed, there are two fundamental kinds of learning paradigms: supervised learning and unsupervised learning. The former is commonly used in most feed-forward neural networks, in which the input-output (or input-target) functions or relationships are built from a set of examples, while the latter resembles a self-organization process in the cortex and seeks inter-relationships and associations among the input.</p><p>The most representative supervised learning rule is error-correction learning. When presented with an input-output pair, learning takes place when an error exists between a desired response or target output and the actual output of the network. This learning rule applies an adjustment, proportional to this error, to the weights of the neuron concerned. Derivation of such a rule can be often traced backed to minimizing the mean-square error function -more details can be found in <ref type="bibr" target="#b28">[29]</ref>. A derivative of supervised learning is so-called reinforcement learning, based on trail-and-error (and reward). The following definition has been given by <ref type="bibr" target="#b100">[101]</ref>:</p><p>"If an action taken by a learning system is followed by a satisfactory state of affairs, then the tendency of the system to produce that particular action is strengthened or reinforced. Otherwise, the tendency of the system to produce that action is weakened".</p><p>In contrast to supervised learning, there is no direct teacher to provide how much output error a particular action has produced. Instead, the output has been quantified into either 'positive' or 'negative' corresponding to closer to or further from the goal. Reinforcement learning has popular backing from psychology.</p><p>Self-organization often involves both competition and correlative learning. When presented with a stimulus, neurons compete among themselves for possession or ownership of this input. The winners then strengthen their weights or their relationships with this input. Hebbian learning is the most common rule for unsupervised or self-organized learning. As stated in <ref type="bibr" target="#b29">[30]</ref>:</p><p>"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic changes take place in one or both cells such that As efficiency as one of the cells firing B, is increased." Mathematically, the Hebbian learning rule can be directly interpreted as,</p><formula xml:id="formula_1">∂w ij (t) ∂t = αx i (t)y i (t) (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where α is a positive learning rate (0 &lt; α &lt; 1), and x and y are the input and output of the neural system, respectively (or can also be regarded as the outputs of the two neurons). That is, the change of the synaptic weight is proportional to the correlation between an input and its associated output. If the input and output are coherent, the weight connecting them is strengthened (xy is positive), otherwise, weakened (xy is either negative or zero).</p><p>Hebbian learning requires some modification before it can be used in practice, otherwise the weight will easily become saturated or unlimited (positive or negative). One solution is to add a 'forgetting term' to prevent weights from increasing/decreasing monotonically as in the SOM (see the next Section). Alternatively, we can normalize the weights. For instance, Oja proposed a weight normalization scheme on all weights. This naturally introduces a forgetting term to the Hebbian rule <ref type="bibr" target="#b80">[81]</ref>,</p><formula xml:id="formula_3">w i (t + 1) = w i (t) + αx i (t)y(t) { n j=1 [w j (t) + αx j (t)y(t)] 2 } 1/2 (2) ≈ w i (t) + α(t)[x i (t) -y(t)w i (t)] + O(α 2 )</formula><p>where O(α 2 ) represents second-and high-order terms in α, and can be ignored when a small learning rate is used. The resulting Oja learning algorithm is a so-called principal component network, which learns to extract the most variant directions among the data set. Other variants of Hebbian learning include many algorithms used for Independent Component Analysis ( <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b81">82]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From Von Marsburg and Willshaw's Self-Organization Model to Kohonen's SOM</head><p>External stimuli are received by various sensors or receptive fields (for example, visual-, auditory-, motor-, or somato-sensory), coded or abstracted by the living neural networks, and projected through axons onto the cerebral cortex, often to distinct parts of the cortex. In other words, the different areas of the cortex (cortical maps) often correspond to different sensory inputs, though some brain functions require collective responses. Topographically ordered maps are widely observed in the cortex. The main structures (primary sensory areas) of the cortical maps are established before birth ( <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b114">[115]</ref>, and similar) in a predetermined topographically ordered fashion. Other more detailed areas (associative areas), however, are developed through self-organization gradually during life and in a topographically meaningful order. Therefore studying such topographically ordered projections, which had been ignored during the early period of neural information processing research <ref type="bibr" target="#b47">[48]</ref>, is undoubtedly important for understanding and constructing dimension-reduction mapping and for the effective representation of sensory information and feature extraction.</p><p>The self-organized learning behavior of brains has been studied for a long time by many people. Pioneering works include for example, Hebb's learning law (1949) <ref type="bibr" target="#b29">[30]</ref>, Marr's theory of the cerebellar cortex (1969) <ref type="bibr" target="#b71">[72]</ref>, Willshaw, Buneman and Longnet-Higgins's non-holographic associative memory (1969) <ref type="bibr" target="#b113">[114]</ref>, Gaze's studies on nerve connections (1970), von der Malsburg and Willshaw's self-organizing model of retina-cortex mapping ( <ref type="bibr" target="#b110">[111]</ref>, <ref type="bibr" target="#b114">[115]</ref>), Amari's mathematical analysis of self-organization in the cortex (1980), Kohonen's self-organizing map (1982), and Cottrell and Fort's self-organizing model of retinotopy <ref type="bibr">(1986)</ref>. Many still have immense influence on today's research. Von der Malsburg (1973) and <ref type="bibr" target="#b114">Willshaw (1976)</ref> first developed, in mathematical form, the self-organizing topographical mapping, mainly from two-dimensional presynaptic sheets to two-dimensional postsynaptic sheets, based on retinatopic mapping: the ordered projection of visual retina to the visual cortex (see Fig. <ref type="figure" target="#fig_2">4</ref>). Their basic idea was: "......the geometrical proximity of presynaptic cells is coded in the form of correlations in their electrical activity. These correlations can be used in the postsynaptic sheet to recognize axons of neighbouring presynaptic cells and to connect them to neighbouring postsynaptic cells, hence producing a continuous mapping......" The model uses short-range excitatory connections between cells so that activity in neighbouring cells becomes mutually reinforced, and uses longrange inhibitory interconnections to prevent activity from spreading too far. The postsynaptic activities {y j (t), j = 1, 2, ... N y }, at time t, are expressed by</p><formula xml:id="formula_4">∂y i (t) ∂t + cy i (t) = j w ij (t)x i (t) + k e ik y * k (t) - k b ik y * k (t) (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where c is the membrane constant, w ij (t) is the synaptic strength between cell i and cell j in pre-and post-synaptic sheets respectively, {x i (t), i = 1, 2, ... N x }, the state of the presynaptic cells, equal to 1 if cell i is active or 0 otherwise, e kj and b kj are short-range excitation and long-range inhibition constants respectively, and y * j (t) is an active cell in postsynaptic sheet at time t. The postsynaptic cells fire if their activity is above a threshold, say,</p><formula xml:id="formula_6">y * i (t) = y * j (t) -θ, if y * j (t) &gt; θ 0 o t h e r w i s e<label>(4)</label></formula><p>The modifiable synaptic weights between pre-and post-synaptic sheets are then facilitated in proportion to the product of activities in the appropriate pre-and postsynaptic cells (Hebbian learning):</p><formula xml:id="formula_7">∂w ij (t) ∂t = αx i (t)y * j (t), subject to 1 N x i w ij = constant (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where α is a small constant representing the learning rate. To prevent the synaptic strengths becoming unstable, the total strength associated with each postsynaptic cell is limited by normalization to a constant value after each iteration. <ref type="bibr" target="#b45">Kohonen (1982)</ref> abstracted the above self-organizing learning principle and function and proposed a much simplified learning mechanism which cleverly incorporates the Hebbian learning rule and lateral interconnection rules and can emulate the self-organizing learning effect. Although the resulting SOM algorithm was more or less proposed in a heuristic manner ( <ref type="bibr" target="#b53">[54]</ref>), it is a simplified and generalized model of the above self-organization process. As commented in <ref type="bibr" target="#b90">[91]</ref>: "Kohonen's model of self-organizing maps represented an important abstraction of the earlier model of von der Malsburg and Willshaw; the model combines biological plausibility with proven applicability in a broad range of difficult data processing and optimization problems..."</p><p>In Kohonen's model, the postsynaptic activities are similar to Eqn. (3). To find the solutions of this equation and ensure they are non-negative properties, a sigmoid type of nonlinear function is applied to each postsynaptic activity:</p><formula xml:id="formula_9">y j (t + 1) = ϕ w T j x(t) + i h ij y i (t)<label>(6)</label></formula><p>where h kj is similar to e kj and b kj , and the input is described as a vector as the map can be extended to any dimensional input. A typical structure is shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>A spatially-bounded cluster or bubble will then be formed among the postsynaptic activities and will stabilize at a maximum (without loss of generality which is assumed to be unity) when within the bubble, or a minimum (that is, zero) otherwise,</p><formula xml:id="formula_10">y j (t + 1) = 1, if neuron j is inside the bubble 0, otherwise<label>(7)</label></formula><p>The bubble is centred on a postsynaptic cell whose synaptic connection with the presynaptic cells is mostly matched with the input or presynaptic state, that is the first term in the function in Eqn. ( <ref type="formula" target="#formula_9">6</ref>) is the highest. The range or size, denoted by η(t), of the bubble depends on the ratio of the lateral excitation and inhibition. To modify the Hebbian learning rule, in other words Eqn. (5), instead of using normalization, a forgetting term, βy j (t)w ij (t), is added. Let α = β, and apply Eqn. <ref type="bibr" target="#b6">(7)</ref>, the synaptic learning rule can then be formulated as, At each time step the best matching postsynaptic cell is chosen according to the first term of the function in Eqn. <ref type="bibr" target="#b5">(6)</ref>, which is the inner product, or correlation, of the presynaptic input and synaptic weight vectors. When normalization is applied to the postsynaptic vectors, as it usually is, this matching criterion is similar to the Euclidean distance measure between the weight and input vectors. Therefore the model provides a very simple computational form. The lateral interconnection between neighbouring neurons and the 'Mexican-hat' excitatory or inhibitory rules are simulated (mathematically) by a simple local neighbourhood excitation centred on the winner. Thus the neuron's lateral interconnections (both excitatory and inhibitory) have been replaced by neighbourhood function adjustment. The neighbourhood function's width can emulate the control of the exciting and inhibiting scalars. The constrained (with a decaying or forgetting term) Hebbian learning rule has been simplified and becomes a competitive learning model. <ref type="bibr" target="#b47">[48]</ref>, and so on). In his studies, he has found that the spatially ordered representation of sensory information in the brain is highly related to the memory mechanism, and that the inter-representation and information storage can be implemented simultaneously by an adaptive, massively parallel, and self-organizing network <ref type="bibr" target="#b47">[48]</ref>. This simulated cortex map, on the one hand can perform a self-organized search for important features among the inputs, and on the other hand can arrange these features in a topographically meaningful order. This is why the map is also sometimes termed the 'self-organizing feature map', or SOFM. In this Chapter, however, it will be referred to by its commonly used term, the 'self-organizing map' (SOM), which comes from Kohonen's original definition and purpose, namely associative memory.</p><formula xml:id="formula_11">∂w ij (t) ∂t = αy j (t)x i (t) -βy j (t)w ij (t) = α[x i (t) -w ij (t)]y j (t) = α[x i (t) -w ij (t)], if j ∈ η(t) 0 i f j / ∈ η(t)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most of Kohonen's work has been in associative memories ([43]-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The SOM Algorithm</head><p>The SOM uses a set of neurons, often arranged in a 2-D rectangular or hexagonal grid, to form a discrete topological mapping of an input space, X ∈ R n . At the start of the learning, all the weights {w 1 , w 2 , ..., w M } are initialized to small random numbers. w i is the weight vector associated to neuron i and is a vector of the same dimension -n -of the input, M is the total number of neurons, and let r i be the location vector of neuron i on the grid. Then the algorithm repeats the steps shown in Algorithm 1, where η(ν, k, t) is the neighbourhood function, and Ω is the set of neuron indexes. Although one can use the original stepped or top-hat type of neighbourhood function (one when the neuron is within the neighbourhood; zero otherwise), a Gaussian form is often used in practice -more specifically</p><formula xml:id="formula_12">η(ν, k, t) = exp[-rν -r k 2 2σ(t) 2 ]</formula><p>, with σ representing the effective range of the neighbourhood, and is often decreasing with time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Self-Organizing Map algorithm repeat</head><p>1. At each time t, present an input x(t), and select the winner,</p><formula xml:id="formula_13">ν(t) = arg min k∈Ω x(t) -w k (t)<label>(9)</label></formula><p>2. Update the weights of the winner and its neighbours,</p><formula xml:id="formula_14">∆w k (t) = α(t)η(ν, k, t)[x(t) -wν(t)]<label>(10)</label></formula><p>until the map converges</p><p>The coefficients {α(t), t ≥ 0}, termed the 'adaptation gain', or 'learning rate', are scalar-valued, decrease monotonically, and satisfy <ref type="bibr" target="#b46">[47]</ref>: <ref type="bibr" target="#b10">(11)</ref> They are the same as to those used in stochastic approximation ( <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b93">94]</ref>). The third condition in Eqn. <ref type="bibr" target="#b10">(11)</ref> has been relaxed by Ritter and Schulten to a less restrictive one, namely, lim t→∞ α(t) → 0 <ref type="bibr" target="#b89">[90]</ref>.</p><formula xml:id="formula_15">(i) 0 &lt; α(t) &lt; 1; (ii) lim t→∞ α(t) → ∞; (iii) lim t→∞ α 2 (t) &lt; ∞;</formula><p>If the inner product similarity measure is adopted as the best matching rule,</p><formula xml:id="formula_16">ν(t) = arg min k∈Ω [w T k x(t)]<label>(12)</label></formula><p>then the corresponding weight updating will become <ref type="bibr" target="#b52">[53]</ref>:</p><formula xml:id="formula_17">w k (t + 1) = w k (t)+α(t)x(t) w k (t)+α(t)x(t) k ∈ η ν w k (t) k / ∈ η ν (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Such a form is often used in text/document mining applications (for example, <ref type="bibr" target="#b22">[23]</ref>).</p><p>The SOM algorithm vector-quantizes or clusters the input space and produces a map which preserves topology. It can also be and has been used for classification. In this case, the map is trained on examples of known categories. The nodes are then classified or labelled so that the map can be used to classify unseen samples. Various methods for labelling nodes can be found in <ref type="bibr" target="#b58">[59]</ref>. The classification performance can be further improved by the Learning Vector Quantization (LVQ) algorithm <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convergence and Cost Functions</head><p>Although the SOM algorithm has a simple computational form, a formal analysis of it and the associated learning processes and mathematical properties is not easily realized. Some important issues still remain unanswered. Selforganization, or more specifically the ordering process, has been studied in some depth; however a universal conclusion has been difficult, if not impossible, to obtain. This Section reviews and clarifies the statistical and convergence properties of the SOM and associated cost functions, the issue that still causes confusions to many even today. Various topology preservation measures will be analyzed and explained.</p><p>The SOM was proposed to model the sensory-to-cortex mapping thus the unsupervised associative memory mechanism. Such a mechanism is also related to vector quantization (VQ) <ref type="bibr" target="#b62">[63]</ref> in coding terms. The SOM has been shown to be an asymptotically optimal VQ <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b125">126]</ref>. More importantly, with its neighbourhood learning, the SOM is both an error tolerant VQ and Bayesian VQ <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>Convergence and ordering has only been formally proven in the trivial one-dimensional case. A full proof of both convergence and ordering in multidimensional systems is still outstanding, although there have been several attempts (for instance, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b125">126]</ref>). <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref> especially showed that there was no cost function that the SOM will follow exactly. Such an issue is also linked to the claimed lack of an exact cost function that the algorithm follows. Recent work by various researchers has already shed light on this intriguing issue surrounding the SOM. Yin and Allison extended the Central Limit Theorem and used it to show that when the neighbourhood reduces to just the winner, the weight vectors (code references) are asymptotically Gaussian distributed and will converge in a mean squares sense to the means of the Voronoi cells -in other words, an optimal VQ (with the SOM's nearest distance winning rule) <ref type="bibr" target="#b125">[126]</ref>,</p><formula xml:id="formula_19">w k → 1 P (X k ) V k xp(x)dx<label>(14)</label></formula><p>where V k is the Voronoi cell (the data region) for which the weight vector w k is responsible, and p(x) is the probability density function of the data. In general cases with the effect of the neighbourhood function, the weight vector is a kernel smoothed mean <ref type="bibr" target="#b118">[119]</ref>,</p><formula xml:id="formula_20">w k → T t=1 η(ν, k, t)x(t) T t=1 η(ν, k, t)<label>(15)</label></formula><p>Yin and Allison have also proved that the initial state has a diminishing effect on the final weights when the learning parameters follow the convergence conditions <ref type="bibr" target="#b125">[126]</ref>. Such an effect has been recently verified by <ref type="bibr" target="#b14">[15]</ref> using Monte-Carlo bootstrap cross validation; the ordering was not considered. In practice, as only limited data samples are used and training is performed in finite time, good initialization can help guide to a faster or even better convergence. For example, initializing the map to a principal linear sub-manifold can reduce the ordering time, if the ordering process is not a key requirement.</p><p>Luttrell first related hierarchical noise tolerant coding theory to the SOM. When the transmission channel noise is considered, a two-stage optimization has to be done, not only to minimize the representation distortion (as in VQ) but also to minimize the distortion caused by the channel noise. He revealed that the SOM can be interpreted as such a coding algorithm. The neighbourhood function acts as the model for the channel noise distribution and should not go to zero as in the original SOM. Such a noise tolerant VQ has the following objective function ( <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>),</p><formula xml:id="formula_21">D 2 = dxp(x) dnπ x -w k 2 (16)</formula><p>where n is the noise variable and π(n) is the noise distribution. <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b77">[78]</ref> have also linked the SOM and this noise tolerant VQ with minimal wiring of cortex-like maps.</p><p>When the code book (the map) is finite, the noise can be considered as discrete, then the cost function can be re-expressed as,</p><formula xml:id="formula_22">D 2 = i Vi k π(i, k) x -w k 2 p(x)dx (17)</formula><p>where V i is the Voronoi region of cell i. When the channel noise distribution is replaced by a neighbourhood function (analogous to inter-symbol dispersion), it becomes the cost function of the SOM algorithm. The neighbourhood function can be interpreted as a channel noise model. Such a cost function has been discussed in the SOM community (for example, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b116">[117]</ref>). The cost function is therefore,</p><formula xml:id="formula_23">E(w 1 , • • • , w N ) = i Vi k η(i, k) x -w k 2 p(x)dx (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>This leads naturally to the SOM update algorithm using the sample or stochastic gradient descent method <ref type="bibr" target="#b91">[92]</ref> -that is, for each Voronoi region, the sample cost function is,</p><formula xml:id="formula_25">Êi (w 1 , • • • , w N ) = Vi k η(i, k) x -w k 2 p(x)dx (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>The optimization for all weights {w 1 , w 2 , • • • , w N } can be sought using the sample gradients. The sample gradient for w j is,</p><formula xml:id="formula_27">∂ Êi (w 1 , • • • , w N ) ∂w j = ∂ k η(i, k) x -w k 2 ∂w j = 2η(i, k) x -w j (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>which leads to the SOM update rule -Eqn. <ref type="bibr" target="#b9">(10)</ref>. Note that although the neighbourhood function η i,k is only implicitly related to w j , it does not contribute to the weight optimization, nor does the weight optimization lead to its adaptation (neighbourhood adaptation is often controlled by a pre-specified scheme, unrelated to the weight adaptation); thus the neighbourhood can be omitted from the partial differentiation. This point has caused problems in interpreting the SOM cost function in the past.</p><p>It has however been argued that this energy function is violated at boundaries of Voronoi cells where input has exactly the same smallest distance to two neighbouring neurons. Thus this energy function holds mainly for the discrete case where the probability of such boundary input points is close to zero or the local (sample) cost function Êi should be used in deciding the winner <ref type="bibr" target="#b31">[32]</ref>. When a spatial-invariant neighbourhood function is used (as is often the case), assigning the boundary input to either cell will lead to the same local sample cost (or error), therefore any input data on the boundary can be assigned to either Voronoi cells that have the same smallest distance to it, just as in the ordinary manner (on a first-come-first-served fashion, for example). Only when the neurons lie on the map borders does such violation occur, due to unbalanced neighbourhood neurons. The result is slightly more contraction towards to the centre (inside) of the map for the border neurons, compared to the common SOM algorithm, as shown in <ref type="bibr" target="#b49">[50]</ref>. Using either the simple distance or local distortion measure as the winning rule will result in border neurons being contracted towards the centre of the map, especially when the map is not fully converged or when the effective range of the neighbourhood function is large. With the local distortion rule, this boundary effect is heavier as greater local error is incurred at the border neurons due to their few neighbouring neurons compared with any inside neurons.</p><p>To follow the cost function exactly, the winning rule should be modified to follow the local sample cost function Êi (or the local distortion measure) instead of the simplest nearest distance, that is,</p><formula xml:id="formula_29">ν = arg min i k η(i, k) x -w k 2 (21)</formula><p>When the neighbourhood function is symmetric (as is often the case), and when the data density function is smooth, this local distortion winning rule is the same as the simplest nearest distance rule for most non-boundary nodes, especially if the number of nodes is large. On the map borders, however, differences exist due to the imbalance of nodes present in the neighbourhoods. Such differences become negligible to the majority of the neurons, especially when a large map is used, and when the neighbourhood function shrinks to its minimum scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topological Ordering</head><p>The ordering to a large extent is still an outstanding and subtle issue, largely due to the fact that there is no clear (or agreed) definition of 'order' <ref type="bibr" target="#b24">[25]</ref>. This is the very reason why a full self-organization convergence theorem including both statistical convergence, ordering, and the exact cost function, is still subject to debate -which has prompted many alternatives, such as <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and <ref type="bibr" target="#b99">[100]</ref>. The ordering and ordered map are clearly defined only in the 1-D trivial case. Extension to higher dimensions proves difficult, if not impossible. <ref type="bibr" target="#b6">[7]</ref> have proposed a measure called topology product to measure the topological ordering of the map,</p><formula xml:id="formula_30">P = 1 N 2 -N i j log j l=1 d D (w i , w η o (l,i) )d O (i, η o (l, i)) d D (w i , w η D (l,i) )d O (i, η D (l, i)) 1 2k<label>(22)</label></formula><p>where d D and d O represent the distance measures in the input (or data) space, and output (or map) space, respectively; η(l, i) represents the lth neighbour of node i in either data (D) or map (O) space.</p><p>The first ratio in the product measures the ratio or match of weight distance sequences of a neighbourhood (up to j) on the map and in the data space. The second ratio is the index distance sequences of the neighbourhood on the map and in the data space. The topographic product measures the product of the two ratios of all possible neighbourhoods.</p><p>[109] proposed a topographic function to measure the 'neighbourhoodness' of weight vectors in data space as well as on the lattice. The neighbourhood-ness of the weight vectors is defined by the adjacent Voronoi cells of the weights. The function measures the degree to which the weight vectors are ordered in the data space as to their indexes on the lattice, as well as how well the indexes are preserved when their weight vectors are neighbours.</p><p>Defining a fully ordered map can be straightforward using the distance relations <ref type="bibr" target="#b116">[117]</ref>. For example, if all the nearest neighbour nodes (on the lattice) have their nearest neighbour nodes' weights in their nearest neighbourhood in the data space, we can call the map a 1st-order (ordered) map <ref type="bibr" target="#b116">[117]</ref>, that is,</p><formula xml:id="formula_31">d(w i , w j ) ≤ d(w i , w k ), ∀i ∈ Ω; j ∈ η 1 i ; k / ∈ η 1 i (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>where Ω is the map, and η 1 i denotes the 1st-order neighbourhood of node i. Similarly if the map is a 1st-order ordered map, and all the 2nd nearest neighbouring nodes (on the lattice) have their 2nd nearest neighbouring nodes' weights in their 2nd nearest neighbourhood, we can call the map is a 2nd-order (ordered) map. For the 2nd ordered map, the distance relations to be satisfied are,</p><formula xml:id="formula_33">d(w i , w j ) ≤ d(w i , w k ), ∀i ∈ Ω; j ∈ η 1 i ; k / ∈ η 1 i &amp;k ∈ η 2 i ; l / ∈ η 2 i (<label>24</label></formula><formula xml:id="formula_34">)</formula><p>and so forth to define higher ordered maps with interneuron distance hierarchies <ref type="bibr" target="#b116">[117]</ref>.</p><p>An mth order map is optimal for tolerating channel noise spreading up to the mth neighbouring node. Such fully ordered maps however may not be always achievable, especially when the mapping is a dimensional reduction one. Then the degree (percentage) of nodes with their weights being ordered can be measured, together with the probabilities of the nodes being utilized, can be used to determine the topology preservation and that to what degree and to what order the map can tolerate the channel noise. <ref type="bibr" target="#b24">[25]</ref> proposed the C measure -a correlation between the similarity of stimuli in the data space and the similarity of their prototypes in the map space -to quantify the topological preservation,</p><formula xml:id="formula_35">C = i j F (i, j)G[M (i), M(j)] (<label>25</label></formula><formula xml:id="formula_36">)</formula><p>where F and G are symmetric similarity measures in the input and map spaces respectively, and can be problem specific, and M (i) and M (j) are the mapped points or weight vectors of node i and j, respectively.</p><p>The C measure directly evaluates the correlation between distance relations between two spaces. Various other topographic mapping objectives can be unified under the C measure, such as multidimensional scaling, minimal wiring, the travelling salesperson problem (TSP), and noise tolerant VQ. It has also been shown that if a mapping that preserves ordering exists then maximizing C will find it. Thus the C measure is also the objective function of the mapping, an important property different from other topology preservation measures and definitions.</p><p>One can always use the underlying cost function Eqn. <ref type="bibr" target="#b17">(18)</ref> to measure the goodness of the resulting map including the topology preservation, at least one can use a temporal window to take a sample of it as suggested in <ref type="bibr" target="#b49">[50]</ref>. The (final) neighbourhood function specifies the level of topology (ordering) the mapping is likely to achieve or is required. To draw an analogy to the above C measure, the neighbourhood function can be interpreted as the G measure used in Eqn. ( <ref type="formula" target="#formula_35">25</ref>) and the x-w k 2 term represents the F measure. Indeed, the input x and weight w j are mapped on the map as node index i and j, and their G measure is the neighbourhood function (for example, exponentials). Such an analogy also sheds light on the scaling effect of the SOM. Multidimensional scaling also aims to preserve local similarities on a mapped space (see the next Section for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extensions and Links with Other Learning Paradigms</head><p>The SOM has been a popular model for unsupervised learning as well as pattern organization and association. Since its introduction, various extensions have been reported to enhance its performance and scope. For instance, 'Neural Gas' was developed to map data onto arbitrary or hierarchical map structures rather than confined to rectangular grids for improved VQ performance <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref>. The adaptive subspace SOM (ASSOM) has been proposed to combine principal component learning and the SOM to map data with reduced feature space, in order to form translation-, rotation-and scale-invariant filters <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. The parameterized SOM (PSOM) has been proposed to extend SOM for continuous mapping using basis functions on the grid to interpolate the map <ref type="bibr" target="#b111">[112]</ref>. The stochastic SOM <ref type="bibr" target="#b25">[26]</ref> defines a topographic mapping from a Bayesian framework and a Markov chain encoder, and further explains the stochastic annealing effect in SOM. The Dislex <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> applies hierarchical topographical maps to associate cross-modal patterns such as images with audio or symbols. The U-matrix was proposed to imprint the distance information on the map for visualization <ref type="bibr" target="#b104">[105]</ref>. The visualization induce SOM (ViSOM) has been proposed to directly preserve distance information on the map so as to visualize data structures and distributions <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>.</p><p>The Temporal Kohonen map <ref type="bibr" target="#b10">[11]</ref> and its improved version, the Recurrent SOM <ref type="bibr" target="#b107">[108]</ref>, as well as the Recursive SOM <ref type="bibr" target="#b109">[110]</ref> have extended the SOM for mapping temporal data such as time series, or sequential data such as protein sequences. Extensions along this direction continue to be a focus of research. Extension on probabilistic approaches which enhances the scope and capability of SOM include the Self-Organizing Mixture Network (SOMN) <ref type="bibr" target="#b127">[128]</ref>, Kernel-based topographic maps <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b106">107]</ref>; and the generic topographic mapping (GTM) <ref type="bibr" target="#b7">[8]</ref>. There are many extensions developed in recent years -too many to completely list here. Recent extensions are also proposed for handling non-vectorial <ref type="bibr" target="#b54">[55]</ref> and qualitative data <ref type="bibr" target="#b34">[35]</ref>. For more comprehensive lists and recent developments, please refer to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b82">83]</ref>.</p><p>The remainder of this Section covers extensions of SOM and their associations with data visualization, manifold mapping, density modeling and kernel methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SOM, Multidimensional Scaling and Principal Manifolds</head><p>The SOM is often associated with VQ and clustering. However it is also associated with data visualization, dimensionality reduction, nonlinear data projection, and manifold mapping. A brief review on various data projection methods and their relationships has been given before <ref type="bibr" target="#b120">[121]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multidimensional Scaling</head><p>Multidimensional scaling (MDS) is a traditional study related to dimensionality reduction and data projection. MDS tries to project data points onto an (often two-dimensional) sheet by preserving as closely as possible the interpoint metrics <ref type="bibr" target="#b13">[14]</ref>. The projection is generally nonlinear and can reveal the overall structure of the data. A general fitness function or the so-called stress function is defined as,</p><formula xml:id="formula_37">S = i,j (d ij -D ij ) 2 i,j D 2 ij (<label>26</label></formula><formula xml:id="formula_38">)</formula><p>where d ij represents the proximity of data points i and j in the original data space, and D ij represents the dissimilarity or distance (usually Euclidean) between mapped points i and j in the projected space. Note, that global Euclidean distance is usually used to calculate the inter-point distances.</p><p>Recently, Isomap was proposed to use geodesic (curvature) distance instead for better nonlinear scaling <ref type="bibr" target="#b101">[102]</ref>.</p><p>MDS relies on an optimization algorithm to search for a configuration that gives as low a stress as possible. A gradient method is commonly used for this purpose. Inevitably, various computational problems -such as local minima and divergence -may occur due to the optimization process itself. The methods are also often computationally intensive. The final solution depends on the starting configuration and parameters used in the algorithm.</p><p>Sammon mapping is a widely-known example of MDS <ref type="bibr" target="#b94">[95]</ref>. The objective is to minimize the differences between inter-point (Euclidean) distances in the original space and those in the projected plane. In Sammon mapping intermediate normalization (of the original space) is used to preserve good local distributions and at the same time maintain a global structure. The Sammon stress is expressed as,</p><formula xml:id="formula_39">S Sammon = 1 i&lt;j d ij i&lt;j (d ij -D ij ) 2 d ij<label>(27)</label></formula><p>A second order Newton optimization method is used to recursively solve the optimal configuration. It converges faster than the simple gradient method, but the computational complexity is even higher. It still has local minima and inconsistency problems. Sammon mapping has been shown to be useful for data structure analysis. However, like other MDS methods, the Sammon algorithm is a point-to-point mapping, which does not provide an explicit mapping function and cannot naturally accommodate new data points. It also requires the computation and storage of all the inter-point distances. This proves difficult or even impossible for many practical applications where data arrives sequentially, the quantity of data is large, and/or memory space for the data is limited.</p><p>In addition to being computationally expensive, especially for large data sets, and not being adaptive, another major drawback of MDS is lack of an explicit projection function. Thus for any new input data, the mapping has to be recalculated based on all available data. Although some methods have been proposed to accommodate the new arrivals using triangulation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref>, the methods are generally not adaptive. However, such drawbacks can be overcome by implementing or parameterizing MDS using neural networksfor example, <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b70">71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Principal Component Analysis (PCA)</head><p>Principal Component Analysis (PCA) is a classic linear projection method aiming at finding orthogonal principal directions from a set of data, along which the data exhibiting the largest variances. By discarding the minor components, PCA can effectively reduce data variables and display the dominant ones in a linear, low dimensional subspace. It is an optimal linear projection in the sense of the mean-square error between original points and projected ones, in other words,</p><formula xml:id="formula_40">min x ⎡ ⎣ x - m j=1 (q T j x)q j ⎤ ⎦ 2<label>(28)</label></formula><p>where {q, j = 1, 2, • • • , m, m ≤ n} are orthogonal eigenvectors representing principal directions. They are the first m principal eigenvectors of the covariance matrix of the input. The second term in the above bracket is the reconstruction or projection of x onto these eigenvectors. The term q T j x represents the projection of x onto the jth principal dimension.</p><p>Traditional methods for solving the eigenvector problem involve numerical methods. Though fairly efficient and robust, they are not usually adaptive and often require presentation of the entire data set. Several Hebbian-based learning algorithms and neural networks have been proposed for performing PCA, such as the subspace network <ref type="bibr" target="#b80">[81]</ref> and the generalized Hebbian algorithm <ref type="bibr" target="#b95">[96]</ref>. The limitation of linear PCA is obvious, as it cannot capture nonlinear relationships defined by higher than second-order statistics. If the input dimension is much higher than two, the projection onto the linear principal plane will provide limited visualization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nonlinear PCA and Principal Manifolds</head><p>Extension to nonlinear PCA (NLPCA) is not unique, due to the lack of a unified mathematical structure and an efficient and reliable algorithm, and in some cases due to excessive freedom in selection of representative basis functions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b69">70]</ref>. Several methods have been proposed for nonlinear PCA, such as the five-layer feedforward associative network <ref type="bibr" target="#b55">[56]</ref> and the kernel PCA <ref type="bibr" target="#b96">[97]</ref>. The first three layers of the associative network project the original data onto a curve or surface, providing an activation value for the bottleneck node. The last three layers define the curve and surface. The weights of the associative NLPCA network are determined by minimizing the following objective function, min</p><formula xml:id="formula_41">x x -f {s f (x)} 2<label>(29)</label></formula><p>where f : R 1 → R n (or R 2 → R n ). The function modelled by the last three layers defines a curve (or a surface), s f : R n → R 1 (or R n → R 2 ); the function modelled by the first three layers defines the projection index.</p><p>The kernel-based PCA uses nonlinear mapping and kernel functions to generalize PCA to NLPCA and has been used for various pattern recognition tasks. The nonlinear function Φ(x) maps data onto high-dimensional feature space, where the standard linear PCA can be performed via kernel functions: k(x, y) = (Φ(x) • Φ(y)). The projected covariance matrix is then,</p><formula xml:id="formula_42">Cov = 1 N N i=1 Φ(x i )Φ(x i ) T (<label>30</label></formula><formula xml:id="formula_43">)</formula><p>The standard linear eigenvalue problem can now be written as λV = KV, where the columns of V are the eigenvectors, and K is a N × N matrix with elements as kernels</p><formula xml:id="formula_44">k ij := k(x i , x j ) = (Φ(x i ) • Φ(x i )).</formula><p>The principal curves and principal surfaces <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref> are the principal nonlinear extensions of PCA. The principal curve is defined as a smooth and self-consistency curve, which does not intersect itself. Denote x as a random vector in R n with density p and finite second moment. Let f (•) be a smooth unit-speed curve in R n , parameterized by the arc length (from one end of the curve) over Λ ∈ R, a closed interval.</p><p>For a data point x its projection index on f is defined as</p><formula xml:id="formula_45">ρ f (x) = sup ρ∈Λ {ρ : x -f (ρ) = inf θ x -f (θ) } (<label>31</label></formula><formula xml:id="formula_46">)</formula><p>The curve is called self-consistent or a principal curve of ρ if</p><formula xml:id="formula_47">f (ρ) = E[X | ρ f (X) = ρ] (<label>32</label></formula><formula xml:id="formula_48">)</formula><p>The principal component is a special case of the principal curves if the distribution is ellipsoidal. Although principal curves have been mainly studied, extension to higher dimensions -for example principal surfaces or manifoldsis feasible in principle. However, in practice, a good implementation of principal curves/surfaces relies on an effective and efficient algorithm. The principal curves/surfaces are more of a concept that invites practical implementations. The HS algorithm is a nonparametric method <ref type="bibr" target="#b27">[28]</ref>, which directly iterates the two steps of the above definition. It is similar to the standard LGB VQ algorithm <ref type="bibr" target="#b62">[63]</ref>, combined with some smoothing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 The Hastie and Stuetzle (HS) algorithm</head><p>Initialization: choose the first linear principal component as the initial curve, f (0) (x). repeat Projection: project the data points onto the current curve and calculate the projections index -that is ρ (t) (x) = ρ f (t) (x).</p><p>Expectation: for each index, take the mean of the data points projected onto it as the new curve point -in other words,</p><formula xml:id="formula_49">f t+1 (ρ) = E[X | ρ f (t) X = ρ]</formula><p>. until a convergence criterion is met (for example, when the change of the curve between iterations falls below a threshold).</p><p>For a finite data set, the density p is often unknown, and the above expectation is replaced by a smoothing method such as the locally weighted runningline smoother or smoothing splines. For kernel regression, the smoother is,</p><formula xml:id="formula_50">f (ρ) = N i=1 x i K(ρ, ρ i ) N i=1 K(ρ, ρ i ) (33)</formula><p>The arc length is simply computed from the line segments. There are no proofs of convergence for the algorithm, but no convergence problems have been reported, although the algorithm is biased in some cases <ref type="bibr" target="#b27">[28]</ref>. Banfield and Reftery have modified the HS algorithm by taking the expectation of the residual of the projections in order to reduce the bias <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b41">[42]</ref> have proposed an incremental -for example, segment-by-segment -and arc length constrained method for practical construction of principal curves.</p><p>Tibshirani has introduced a semi-parametric model for the principal curve <ref type="bibr" target="#b102">[103]</ref>. A mixture model was used to estimate the noise along the curve; and the expectation-maximization (EM) method was employed to estimate the parameters. Other options for finding the nonlinear manifold include the Generic Topographic Map <ref type="bibr" target="#b7">[8]</ref> and probabilistic principal surfaces <ref type="bibr" target="#b9">[10]</ref>. These methods model the data by a means of a latent space. They belong to the semiparameterized mixture model, although types and orientations of the local distributions vary from method to method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization induced SOM (ViSOM)</head><p>For scaling and data visualization, a direct and faithful display of data structure and distribution is highly desirable. ViSOM has been proposed to extend the SOM for direct distance preservation on the map <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>, instead of using a colouring scheme such as U-matrix <ref type="bibr" target="#b104">[105]</ref>, which imprints qualitatively the inter-neuron distances as colours or grey levels on the map. For the map to capture the data structure naturally and directly, (local) distance quantities must be preserved on the map, along with the topology. The map can be seen as a smooth and graded mesh, or manifold embedded into the data space onto which the data points are mapped and the inter-point distances are approximately preserved.</p><p>In order to achieve that, the updating force, x(t)-w k (t), of the SOM algorithm is decomposed into two elements [x(t)w ν (t)] + [w ν (t)w k (t)]. The first term represents the updating force from the winner ν to the input x(t), and is the same to the updating force used by the winner. The second force is a lateral contraction force bringing neighbouring neuron k to the winner ν. In the ViSOM, this lateral contraction force is constrained or regulated in order to help maintain unified local inter-neuron distances w ν (t)w k (t) on the map.</p><formula xml:id="formula_51">w k (t + 1) = w k (t) + α(t)η(v, k, t)[x(t) -w ν (t)] + β[w ν (t) -w k (t)] (34)</formula><p>where the simplest constraint can be β = d νk (D νk λ)-1 , with d νk being the distance of neuron weights in the input space, D νk the distance of neuron indexes on the map, and λ a (required) resolution constant.</p><p>ViSOM regularizes the contraction force so that the distances between nodes on the map are analogous to the distances of their weights in the data space. The aim is to adjust inter-neuron distances on the map in proportion to those in the data space, in other words D vk ∝ d vk . When the data points are eventually projected onto a trained map, the distance between point i and j on the map is proportional to that of the original space, subject to the quantization error (the distance between a data point and its neural representative). This has a similar effect to MDS, which also aims at achieving this proportionality,</p><formula xml:id="formula_52">D ij ∝ d ij .</formula><p>The SOM is shown to be a qualitative scaling, while the ViSOM is a metric scaling <ref type="bibr" target="#b123">[124]</ref>. The key feature of ViSOM is that the distances between the neurons (which data are mapped to) on the map (in a neighbourhood) reflect the corresponding distances in the data space. When the map is trained and data points mapped, the distances between mapped data points will resemble approximately those in the original space (subject to the resolution of the map). This makes visualization more direct, quantitatively measurable, and visually appealing. The map resolution can be enhanced (and the computational cost reduced) by interpolating a trained map or incorporating local linear projections <ref type="bibr" target="#b121">[122]</ref>. The size or covering range of the neighbourhood function can also be decreased from an initially large value to a final smaller one. The final neighbourhood, however, should not contain just the winner. The rigidity or curvature of the map is controlled by the ultimate size of the neighbourhood. The larger this size, the flatter the final map is in the data space. Guidelines for setting these parameters have been given in <ref type="bibr" target="#b119">[120]</ref>. An example on data visualization will be shown in the next Section.</p><p>Several authors have since introduced improvements and extensions to ViSOM. For example, in <ref type="bibr" target="#b115">[116]</ref>, a probabilistic data assignment <ref type="bibr" target="#b25">[26]</ref> is used in both the input assignment and the neighbourhood function; also an improved second order constraint is adopted. The resulting SOM has a clearer connection to an MDS cost function. Estévez and Figueora extend the ViSOM to an arbitrary, neural gas type of map structure <ref type="bibr" target="#b20">[21]</ref>. Various other variants of SOM, such as hierarchical, growing, and hierarchical and growing structures are readily extendable to the ViSOM for various application needs.</p><p>The SOM has been related to the discrete principal curve/surface algorithm <ref type="bibr" target="#b90">[91]</ref>. However differences remain in both the projection and smoothing processes. In the SOM the data are projected onto the nodes rather than onto the curve. The principal curves perform the smoothing entirely in the data space -see Eqn. <ref type="bibr" target="#b32">(33)</ref>. The smoothing process in SOM and ViSOM, as a convergence criterion, is <ref type="bibr" target="#b119">[120]</ref>,</p><formula xml:id="formula_53">w k = L i=1 x i η(ν, k, i) L i=1 η(ν, k, i) (35)</formula><p>Smoothing is governed by the indexes of the neurons in the map space. The kernel regression uses the arc length parameters (ρ, ρ i ) or ρρ i exactly, while the neighbourhood function uses the node indexes (ν, k) or r νr k . Arc lengths reflect the curve distances between the data points. However, node indexes are integer numbers denoting the nodes or positions on the map grid, not the positions in the input space. So r νr k does not resemble w νw k in the common SOM. In the ViSOM, however, as the local inter-neuron distances on the map represent those in the data space (subject to the map resolution), the distances of nodes on the map are in proportion to the difference of their positions in the data space, that is r νr k ∼ w νw k . The smoothing process in the ViSOM resembles that of the principal curves as shown below,</p><formula xml:id="formula_54">w k = L i=1 x i η(ν, k, i) L i=1 η(ν, k, i) ≈ L i=1 x i η(w ν , w k , i) L i=1 η(w ν , w k , i) (36)</formula><p>This shows that ViSOM is a better approximation to the principal curves/ surfaces than the SOM. SOM and ViSOM are similar only when the data are uniformly distributed, or when the number of nodes becomes very large, in which case both SOM and ViSOM will closely approximate the principal curves/surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SOM and Mixture Models</head><p>The SOM has been linked with density matching models and the point density that the SOM produces is related to the density of the data. However the SOM does not exactly follow the data density. Such properties have been studied and treated under the VQ framework <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b116">117]</ref>.</p><p>The self-organizing mixture network (SOMN) <ref type="bibr" target="#b127">[128]</ref> extends and adapts the SOM to a mixture density model, in which each node characterizes a conditional probability distribution. The joint probability density of the data or the network output is described by a mixture distribution,</p><formula xml:id="formula_55">p(x | Θ) = K i=1 p i (x | θ i )P i (<label>37</label></formula><formula xml:id="formula_56">)</formula><p>where p i (x | θ i ) is the ith component-conditional density, and θ i is the parameter for the ith conditional density</p><formula xml:id="formula_57">, i = 1, 2, • • • , K; Θ = (θ 1 , θ 2 , • • • , θ K ) T</formula><p>, and P i is the prior probability of the ith component or node and is also called the mixing weights. For example, a Gaussian mixture has the following the conditional densities respectively,</p><formula xml:id="formula_58">p i (x | θ i ) = 1 (2π) d/2 | i | 1/2 exp - 1 2 (x -m i ) T i -1 (x -m i ) (<label>38</label></formula><formula xml:id="formula_59">)</formula><p>where θ i = {m i , i } are the mean vector and covariance matrix, respectively.</p><p>Suppose that the true environmental data density function and the estimated one are p(x) and p(x), respectively. The Kullback-Leibler information distance measures the divergence between these two, and is defined as,</p><formula xml:id="formula_60">I = -log p(x) p(x) p(x)dx (39)</formula><p>It is always positive and is equal to zero only when two densities are identical.</p><p>When the estimated density is modelled as a mixture distribution, one can seek the optimal estimate of the parameters by minimizing the Kullback-Leibler divergence via its partial differentials in respect to model parameters, more specifically,</p><formula xml:id="formula_61">∂I ∂θ i = - 1 p(x | Θ) ∂ p(x | Θ) ∂θ i p(x)dx, i= 1, 2, • • • , K<label>(40)</label></formula><p>As the true data density is not known, the stochastic gradient is used for solving these non-directly solvable equations. This results in the following adaptive update rules for the parameters and priors <ref type="bibr" target="#b128">[129]</ref>,</p><formula xml:id="formula_62">2 θi (t + 1) = θi (t) + α(t)η(ν(x), i) 1 p(x | Θ) ∂ p(x | Θ) ∂θ i = θi (t) + α(t)η(ν(x), i) Pi (t) j Pi (t)p j (x | θ j ) ∂ pi (x | Θi ) ∂θ i<label>(41)</label></formula><p>and</p><formula xml:id="formula_63">Pi (t + 1) = Pi (t) + α(t) pi (x | θi ) Pi (t) p(x | Θ) -Pi (t) = Pi (t) -α(t)η(ν(x), i) P (i | x) -Pi (t)<label>(42)</label></formula><p>where α(t) is the learning coefficient or rate at time step t (0 &lt; α(t) &lt; 1), and decreases monotonically. The winner is found via the maximum posterior probability of the node,</p><formula xml:id="formula_64">P (i | x) = Pi pi (x | θi ) j p(x | Θ)<label>(43)</label></formula><p>When the SOMN is limited to the homoscedastic case -namely equal variances and equal priors (non-informative priors) for all components -only the means are the learning variables. The above winner rule becomes,</p><formula xml:id="formula_65">v = arg max i pi (x | θi ) j pj (x | θj ) (44)</formula><p>When the conditional density function is isotropic or symmetric or is a function of xm , the above winning rule is a function of commonly used Euclidean norm xm . The corresponding weight updating rule is,</p><formula xml:id="formula_66">m i (t + 1) = m i (t) + α(t)η(ν(x), i) 1 j p j (x | θ j ) ∂p i (x | θ i ) ∂m i (45)</formula><p>For example, for a Gaussian mixture with equal variance and prior for all nodes, it is easy to show that the winning and mean update rules become,</p><formula xml:id="formula_67">v = arg max i exp - x -m i 2 2σ 2<label>(46)</label></formula><p>and</p><formula xml:id="formula_68">m i (t + 1) = m i (t) + α(t)η(ν(x), i) 1 2σ 2 1 j p j (x | θ j ) exp - x -m i 2 2σ 2 (x -m i )<label>(47)</label></formula><p>The winning rule becomes equivalent to the simple distance measure. The update formula bear a similarity to that of the original SOM. The term exp -x-mi 2 2σ 2 is playing a similar role as the neighbourhood function, defined by the distances between weights and input instead of node indexes. The SOM approximates it by quantizing it using node indexes. The SOMN is also termed a 'Bayesian SOM', as it applies the Bayesian learning principle to the SOM learning rule <ref type="bibr" target="#b128">[129]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SOM and Kernel Method</head><p>A kernel is a function K : X × X → R, where X is the input space. This function is a dot product of the mapping function φ(x) -in other words K(x; y) = [φ(x), φ(y)], where φ : X → F, F being a high dimensional inner product feature space. The mapping function φ(x) is often nonlinear and not known. All the operations are defined in terms of the kernel function instead of the mapping function. The kernel methodology has become increasingly popular within supervised learning paradigms, with the Support Vector Machine (SVM) being a widely known example. When nonlinearly mapping data or patterns to high dimensional space, the patterns often become linearly separable.</p><p>The kernel method has also been applied to the SOM. Following the kernel PCA <ref type="bibr" target="#b96">[97]</ref>, a k-means based kernel SOM has been proposed <ref type="bibr" target="#b68">[69]</ref>. Each data point x is mapped to the feature space via a (unknown or imaginary) nonlinear function φ(x). In principle each mean can be described as a weighted sum of the observations in the feature space m i = n γ i,n φ(x n ), where {γ i,n } are the constructing coefficients. The algorithm then selects a mean or assigns a data point with the minimum distance between the mapped point and the mean,</p><formula xml:id="formula_69">φ(x -m i 2 = φ(x - n γ i,n φ(x n ) 2 = K(x, x) -2 n γ i,n K(x, x n ) + n,m γ m,n K(x n , x m ) (48)</formula><p>The update of the mean is based on a soft learning algorithm,</p><formula xml:id="formula_70">m i (t + 1) = m i (t) + Λ[φ(x -m i (t)] (<label>49</label></formula><formula xml:id="formula_71">)</formula><p>where Λ is the normalized winning frequency of the ith mean and is defined as,</p><formula xml:id="formula_72">Λ = ζ i(x),j t+1 n=1 ζ i,n<label>(50)</label></formula><p>where ζ is the winning counter and is often defined as a Gaussian function between the indexes of the two neurons.</p><p>As the mapping function φ(x) is not known, the update rule (Eqn. ( <ref type="formula" target="#formula_70">49</ref>)) is further elaborated and leads to the following updating rules for the constructing coefficients of the means <ref type="bibr" target="#b68">[69]</ref>,</p><formula xml:id="formula_73">γ i,n (t + 1) = γ i,n (t)(1 -ζ), for n = t + 1 ζ, for n = t + 1<label>(51)</label></formula><p>Note that these constructing coefficients, {γ i,n }, together with the kernel function, effectively define the kernel SOM in feature space. The winner selection -that is, Eqn. (48) -operates on these coefficients and the kernel function. No explicit mapping function φ(x) is required. The exact means or neurons' weights -{m i } -are not required.</p><p>There is another, direct way to kernelize the SOM by mapping the data points and neuron weights, both defined in the input space, to a feature space, then applying the SOM in the mapped dot product space. The winning rules of this second type of kernel SOM have been proposed as follows, either in the input space <ref type="bibr" target="#b84">[85]</ref></p><formula xml:id="formula_74">, v = arg min i x -m i (<label>52</label></formula><formula xml:id="formula_75">)</formula><p>or in the feature space <ref type="bibr">[4]</ref>,</p><formula xml:id="formula_76">v = arg min i φ(x) -φ(m i )<label>(53)</label></formula><p>It will soon become clear that these two rules are equivalent for certain kernels, such as the Gaussian. The weight update rule proposed by <ref type="bibr">[4]</ref> is,</p><formula xml:id="formula_77">m i (t + 1) = m i (t) + α(t)η(v(x), i)∆J(x, m i )<label>(54)</label></formula><p>where J(x, m i ) = φ(x)φ(m i ) 2 is the distance function in the feature space or the proposed instantaneous or sample objective function. and are the learning rate and neighbourhood function, respectively.</p><p>Note that,</p><formula xml:id="formula_78">J(x, m i ) = φ(x) -φ(m i ) 2 = K(x, x) + K(m i , m i ) -2K(x, m i ) (55)</formula><p>and,</p><formula xml:id="formula_79">∇J(x, m i ) = ∂K(m i , m i ) ∂m i -2 ∂K(x, m i ) ∂m i (<label>56</label></formula><formula xml:id="formula_80">)</formula><p>Therefore this kernel SOM can also be operated entirely in the feature space with the kernel function. As the weights of the neurons are defined in the input space, they can be explicitly resolved.</p><p>These two kernel SOMs have been proved equivalent ( <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b122">[123]</ref>); they can all be derived from applying the energy function (Eqn. ( <ref type="formula" target="#formula_23">18</ref>)) on the mapped feature space,</p><formula xml:id="formula_81">E F = i vi j η(i, j) φ(x) -φ(m j ) 2 p(x)dx<label>(57)</label></formula><p>The kernel SOM can be seen as a result of directly minimizing this transformed energy stochastically, in other words, by using the sample gradient on</p><formula xml:id="formula_82">j η(v(x), j) φ(x) -φ(m j ) 2 , ∂ ÊF m i = ∂ m i j η(ν(x), j) φ(x) -φ(m j ) 2 = -2η(ν(x), i)∇J(x, m j ) (58)</formula><p>This leads to the same weight update rule of the kernel SOM as Eqn. <ref type="bibr" target="#b53">(54)</ref>.</p><p>Various kernel functions such as Gaussian (or radial basis function), Cauchy and polynomial, are readily applicable to the kernel SOM <ref type="bibr" target="#b58">[59]</ref>. For example, for Gaussian kernel, the winning and weight update rules are,</p><formula xml:id="formula_83">v = arg min i J(x, m i ) = arg min i [-2K(x -m i )]<label>(59)</label></formula><p>= arg min</p><formula xml:id="formula_84">i -exp - x -m i 2 2σ 2</formula><p>and,</p><formula xml:id="formula_85">m i (t + 1) = m i (t) + α(t)η(ν(x), i) 1 2σ 2 exp - x m i 2 2σ 2 (x -m i )<label>(60)</label></formula><p>respectively. Please note for Gaussian kernel functions, although the winning rule (Eqn. ( <ref type="formula" target="#formula_83">59</ref>)) is derived from the feature space, it is equivalent to that of the original SOM and is conducted in the input space.</p><p>Comparing the kernel SOM algorithm (Eqns. ( <ref type="formula" target="#formula_83">59</ref>) and ( <ref type="formula" target="#formula_85">60</ref>)) with those of the SOMN (Eqns. ( <ref type="formula" target="#formula_67">46</ref>) and ( <ref type="formula" target="#formula_68">47</ref>)), it can be easily seen that the two methods are the same <ref type="bibr" target="#b122">[123]</ref>. That is, the kernel SOM (with Gaussian kernels) is implicitly applying a Gaussian mixture to model the data. In other words, the SOMN is a kind of kernel method. As the SOM is seen as a special case of the SOMN, the original SOM has a certain effect of the kernel method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications and Case Studies</head><p>Thousands of applications of the SOM and its variants have been reported since its introduction <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b82">83]</ref> -too many to list here. There is a dedicated international Workshop on SOMs (WSOM), as well as focused sessions in many neural network conferences. There have also been several special journal issues dedicated to advances in SOM and related topics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38]</ref>. Moreover, many new applications are being reported in many relevant journals today. SOMs will remain an active topic in their continued extension, combination and applications in the years to come.</p><p>In this Section, several typical applications are provided as case studies. They include image and video processing; density or spectrum profile modeling; text/document mining and management systems; gene expression data analysis and discovery; and high dimensional data visualizations. Other typical applications not discussed here include image/video retrieval systems -for instance, PicSOM <ref type="bibr" target="#b56">[57]</ref>; nonlinear ICA (Nonlinear PCA and ICA) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b83">84]</ref>; classification (LVQ) <ref type="bibr" target="#b52">[53]</ref>; cross-modal information processing and associations <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref>; novelty detection <ref type="bibr" target="#b72">[73]</ref>; robotics <ref type="bibr" target="#b5">[6]</ref>; hardware implementation <ref type="bibr" target="#b97">[98]</ref>; and computer animation <ref type="bibr" target="#b112">[113]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Vector Quantization and Image Compression</head><p>The SOM is an optimal VQ when the neighbourhood eventually shrinks to just the winner, as it will satisfy the two necessary conditions for VQ (Voronoi partition and centroid condition). The use of the neighbourhood function makes the SOM superior to common VQs in two main respects. Firstly, the SOM is better at overcoming the under-or over-utilization and local minima problem. The second is that the SOM will produce a map (codebook) with some ordering (even when the neighbourhood eventually vanishes) among the code vectors, and this gives the map an ability to tolerate noise in the input or retrieval patterns. An example is provided in Fig. <ref type="figure" target="#fig_5">6</ref>  It has been found that SOMs generally perform better than other VQs especially in situations where local optima are present <ref type="bibr" target="#b116">[117]</ref>. The robustness of SOM has been further improved by introducing a constraint on the learning extent of a neuron based on the input space variance it covers. The algorithm is aiming to achieve global optimal VQ by limiting and unifying the distortions from all nodes to approximately equal amounts -the asymptotic property of the global optimal VQ (in other words, for a smooth underlying probability density and large number of code vectors as all regions in an optimal Voronoi partition have the same within region variance). The constraint is applied to the scope of the neighbourhood function so that the node covering a large region (thus having a large variance) has a large neighbourhood. The results show that the resulting quantization error is smaller. Such a SOM-based VQ has also been applied to video compression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> for improved performance at low bit rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Segmentation</head><p>The SOM has been used in a hierarchical structure, together with the Markov random field (MRF) model, for the unsupervised segmentation of textured images <ref type="bibr" target="#b124">[125]</ref>. The MRF is used as a measure of homogeneous texture features from a randomly placed local region window on the image. Such features are noisy and poorly known. They are input to a first SOM layer, which learns to classify and filter them. The second local-voting layer -a simplified SOM -produces an estimate of the texture type or label for the region. The hierarchical network learns to progressively estimate the texture model, and classify the various textured regions of similar type. Randomly positioning the local window at each iteration ensures that the consecutive inputs to the SOMs are uncorrelated. The size of the window is large at the beginning to capture patch-like texture homogeneities and shrinks with time to reduce the estimation parameter noise at texture boundaries. The weights of the neurons in the first layer will converge to the MRF model parameters of various texture types, whilst the weights of the second layer will be the prototypes of these types -that is, the segmented image. The computational form of the entire algorithm is simple and efficient. The theoretical analysis of the algorithm shows that it will converge to the maximum likelihood segmentation. Figure <ref type="figure" target="#fig_6">7</ref> shows a typical example of such applications. The number of texture types was subjectively assumed as four. Interestingly, the algorithm has segmented the image into four meaningful categories: 'trees', 'grass', 'buildings', and 'roads'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Density Modeling</head><p>Some function profiles (such as spectra) can be considered as density histograms. If a spectrum consists of many components, then the SOMN described in Sect. 4.2 can be used to estimate the component profiles of the spectrum <ref type="bibr" target="#b127">[128,</ref><ref type="bibr" target="#b128">129]</ref>. Re-sampling the observed spectrum will provide distribution data for training. The x-ray diffraction patterns of crystalline complex organic molecules (such as proteins) consist of a large number of Bragg diffraction spots. These patterns represent the intensity Fourier transform of the molecular structure (actually the electron density maps); the crystallographers need to determine the precise position of each spot together with its magnitude (namely, integrated spot intensity). The patterns exhibit relatively high background noise together with spot spreading (due to shortcomings in the experiments or limitations in the detection processes), which results in overlapping spots. Automatic analysis of these patterns is a non-trivial task.</p><p>An example of such a pattern image is shown in Fig. <ref type="figure" target="#fig_7">8</ref>(a), which is an 8-bit greyscale and of size 88×71 pixels. In order for the SOMN to learn the profiles of these diffraction spots, the image (diffraction intensity function) has to be re-sampled to provide distribution data. A set of training data (10,647 points in total) was obtained by double sampling this image. A 400-neuron SOMN, arranged in a 20 × 20 grid, was used to learn this density. In this case, the number of spots (peaks or components) in a pattern (a mixture) will not generally be known a priori. The initial positions of the neurons were regularly placed inside the data space -in other words, a <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b87">88]</ref> × <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b70">71]</ref> rectangular grid. The initial variances were assigned equally to a diagonal matrix with the diagonal values equal to a fraction of the grid size, and the initial mixing priors were assigned equally to 1/400. The grid was pre-ordered to save unnecessary computational cost as this is a mapping with the same dimension.</p><p>After a few learning cycles, the SOMN allocated the spots to the Gaussian kernels and decomposed the overlapping ones. Individual neurons and their parameters provide centre (mean vectors) and width (covariance matrices) information for relevant spots. The total intensity of each peak is readily obtainable and is simply related to its mixing weight. The result of the estimation after five epochs is shown in Fig. <ref type="figure" target="#fig_7">8(b</ref>). The number of active nodes (that is, surviving ones) is much less than the initial guess of 400. The SOMN has dynamically fitted to the correct mixture number and suppressed others.</p><p>As the updating at each input was limited to a small area (3 × 3 -the winner and its first order neighbourhood, in this example), the SOMN required a much lighter computational effort than updating the entire network at each input (as the EM algorithm would). This becomes particularly advantageous when the number of the nodes is large. In this example, The EM algorithm of the same size would require approximately 400/(3 × 3) ≈ 44 times more computing effort than the SOMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Gene Expression Analysis</head><p>The SOM has been applied as a valid and useful tool for clustering gene expressions <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b103">104]</ref>. Several attempts have been made to deal with ordered sequence or temporal sequences using SOM. A common approach is to use the trajectories of consecutive winning nodes on the SOM. Other methods are based on modification of the learning topology by introducing recurrent connections, for example the Temporal Kohonen Maps (TKM) or Recurrent SOM (RSOM) mentioned in Sect. 4. In TKM the participation of earlier input vectors in each unit is represented by using a recursive difference equation which defines the current unit activity as a function of the previous activations and the current input vector. In the RSOM, which is a modification of the TKM, the scalar node activities of the TKM are replaced by difference vectors defined as a recursive difference equation of the new input, the previous difference vectors, and the weight vectors of the units. One potential problem with recurrent models is stability. In the case of temporal gene expression clustering, the data items presented to the map are not a spatial vector, but a sequence with time order in itself. They are time-series corresponding to the expression levels over time of a particular gene. Therefore, if a common 2-D SOM is used, the trained map can then be used to mark the trajectories of the expressions of the genes for comparison purposes.</p><p>We approach the temporal extension of SOM from another perspective, this being the similarity metric. If the similarity metric takes into account temporal properties, then the neurons in the resultant map will exhibit temporal relationships. As time is one dimensional, a 1-D SOM is more appropriate.</p><p>In addition, a circular (that is, a closed 1-D SOM), can further detect cyclic temporal characteristics <ref type="bibr" target="#b79">[80]</ref>. A novel temporal metric termed the co-expression coefficient has been defined as <ref type="bibr" target="#b78">[79]</ref>, ce(x, y) =</p><p>x y dt</p><formula xml:id="formula_86">x 2 dt y 2 dt (61)</formula><p>where x and y are two (often modelled, thus smoothed) gene expression profiles; x and y are their derivatives. It can be seen that the co-expression coefficient is the correlation coefficient of the derivatives of the profiles. Comparing the derivatives of the two better profiles rather than directly on the two profiles captures their temporal properties.</p><p>Two yeast cell cycle datasets (208 and 511 genes) were modelled using RBFs and then the modelled profiles were differentiated <ref type="bibr" target="#b79">[80]</ref>. The Bayesian information criterion was used to validate the number of clusters obtained by the circular SOM. Each node presents the smaller distance only to its two neighbouring nodes in a chain-ordered fashion, this implies that characteristic traits are split or merged with larger or fewer number of clusters without changing the order or relation between them. Figure <ref type="figure" target="#fig_8">9</ref> presents the resulting SOMs and prototype profiles of the clusters. It can be easily seen that topology exists among the profiles. The topological order here refers to the time shift. This demonstrates that the proposed method is able to group profiles based on their temporal characteristics and can automatically order the groups based on their periodical properties.</p><p>Genes identified as cell-cycle-regulated by traditional biological methods have been used to evaluate the performance of the proposed technique. The result shows that the obtained clusters have high relevance to the distribution of these genes among the cell cycle phases identified by biological methods, compared with other clustering methods <ref type="bibr" target="#b79">[80]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Data Visualization</head><p>Data projection and visualization has become a major application area for neural networks, in particular for the SOMs <ref type="bibr" target="#b52">[53]</ref>, as its topology preserving property is unique among other neural models. Good projection and visualization methods help to identify clustering tendency, to reveal the underlying functions and patterns, and to facilitate decision support. A great deal of research has been devoted to this subject, and a number of methods have been proposed. A recent review on this subject can be found in <ref type="bibr" target="#b120">[121]</ref>.</p><p>The SOM has been widely used as a visualization tool for dimensionality reduction (for instance, <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b104">105]</ref>). The SOM's unique topology preserving property can be used to visualize the relative mutual relationships among the data. However, the SOM does not directly apply to scaling, which aims to reproduce proximity in (Euclidean) distance on a low visualization space, as it has to rely on a colouring scheme (for example, the U-matrix method <ref type="bibr" target="#b104">[105]</ref>) to imprint the distances crudely on the map. Often the distributions of the data points are distorted on the map. The recently proposed ViSOM <ref type="bibr" target="#b117">[118]</ref><ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref>, described in Sect. 4.1, constrains the lateral contraction force between the neurons in the SOM and hence regularizes the inter-neuron distances with respect to a scalable parameter that defines and controls the resolution of the map. It preserves the data structure as well as the topology as faithfully as possible. ViSOM provides a direct visualization of both the structure and distribution of the data. An example is shown in Fig. <ref type="figure" target="#fig_9">10</ref>, where a 100 × 100  Usually for a fine mapping, the resolution parameter needs to be set to a small value. Moreover, a large number of nodes, that is a large map, is required, as for all discrete mappings. However such a computational burden can be greatly reduced by interpolating a trained map <ref type="bibr" target="#b126">[127]</ref>, or by incorporating a local linear projection on the trained low resolution map <ref type="bibr" target="#b121">[122]</ref>.</p><p>A comparison with other mapping methods, such as PCA, Sammon mapping, Isomap and Local Linear Embedding (LLE) <ref type="bibr" target="#b92">[93]</ref> on a highly nonlinear 'S' shape manifold is also shown in Fig. <ref type="figure" target="#fig_10">11</ref>. In this example, the resolution of the ViSOM is enhanced <ref type="bibr" target="#b121">[122]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Text Mining and Information Management</head><p>With drastically increasing amounts of unstructured content available electronically within an enterprise or on the web, it is becoming inefficient if not impossible to rely on human operators to manually annotate electronic documents. (Web) content management systems have become an important area of research for many applications, such as e-libraries, enterprise portals, e-commerce, software content management, document management, and knowledge discovery. The documents, generated in an enterprise either centrally or locally by employees, are often unstructured or arranged in ad hoc manner (for example, emails, reports, web pages, presentations). Document management addresses many issues, such as storage, indexing, security, revision control, retrieval and organization of documents. Many existing full-text search engines return a large ranked list of documents, many of which are irrelevant. This is especially true when queries are short and very general words are used. Hence document organization has become important in information retrieval and content management.</p><p>The SOM has been applied to organize and visualize vast amounts of textual information. Typical examples include the Welfaremap <ref type="bibr" target="#b40">[41]</ref> and WEBSOM <ref type="bibr" target="#b33">[34]</ref>. Many SOM variants have been proposed and applied to document organization, for instance, TreeGCS <ref type="bibr" target="#b32">[33]</ref> and the growing hierarchical-SOM (GH-SOM) <ref type="bibr" target="#b87">[88]</ref>. The main advantage of SOM is the topology preservation of input space, which makes similar topics appear closely on the map. Most of these applications however are based on 2-D maps and grids, which are intuitive for the concept of a digital library. However such a presentation of information (mainly document files) is counter to all existing computer file organizers and explorers, such as MS Windows Explorer.</p><p>We present a new way of utilizing the SOM as a topology-preserving manifold tree-structure for content management and knowledge discovery <ref type="bibr" target="#b22">[23]</ref>. The method can generate a taxonomy of topics from a set of unannotated, unstructured documents. It consists of a hierarchy of self-organizing growing chains, each of which can develop independently in terms of size and topics. The dynamic development process is validated continuously using a proposed entropy-based Bayesian information criterion. Each chain meeting the criterion spawns child chains, with reduced vocabularies and increased specializations. This results in a topological tree hierarchy, which can be browsed like a table of contents directory or web portal. A typical tree is shown in Fig. <ref type="figure" target="#fig_1">12</ref>. The approach has been tested and compared with several existing methods on real world web page datasets. The results have clearly demonstrated the advantages and efficiency in content organization of the proposed method in terms of computational cost and representation. The preserved topology provides a unique, additional feature for retrieving related topics and confining the search space. An application prototype developed based this method is shown in Fig. <ref type="figure" target="#fig_0">13</ref>. The left panel displays the generated content tree with various levels and preserved topology on these levels. The right panel shows the details of a selected level or branch or a particular document. The method bears a similar interface to many computer file managers, especially the most popular MS Windows Explorer style. This Chapter provides an overview and review on the self-organizing map (SOM). First, it reviewed the biological background of SOM and showed that it is a simplified and abstract mathematical model of the retina-cortex mapping based on Hebbian learning and the lateral inhibition phenomena. Then from the mathematics of the algorithm, we discussed and explained its underlying cost function and various measures for mapping quality. Then its variant, the visualization induced SOM (ViSOM), was proposed for preserving local metrics on the map, and reviewed for use in data visualization and nonlinear manifold mapping. The relationships between SOM, ViSOM, multidimensional scaling, principal curve/surface, kernel PCA and several other nonlinear projection methods were analyzed and discussed. Both the SOM and ViSOM are multidimensional scaling methods and produce nonlinear dimensionreduction mapping or manifold of the input space. The SOM was shown to be a qualitative scaling method, while the ViSOM is a metric scaling method and approximates a discrete principal curve/surface. The SOM has also been extended to a probabilistic model and the resulting self-organizing mixture model also reveals that self-organization is an entropy-related optimization process. Furthermore, such a self-organizing model naturally approximates the kernel method. Examples and typical applications of SOM and ViSOM in the context of pattern recognition, clustering, classification, data visualization and mining, and nonlinear manifolds were presented.</p><p>Future challenges lie in several areas. First, although the SOM-related methods are finding wide application in more and more fields, to make the methods more efficient, robust and consistent is a key challenge, especially for large-scale, real-world applications. To adapt the methods for various input formats and conditions, such as temporal sequences and qualitative inputs, is also an on-going research focus. For general pattern recognition, the SOM may have more potential than implied by current practice, which often limits the SOM to a 2-D map and empirically chosen model parameters. Ways of applying and extending SOM for optimal clustering and classification also need to be investigated further. Last but not the least, to make this biologically inspired model more biologically relevant is also a key challenge. The model may have to be further extended in order to deal with complex biological signals and networks, for example in handling spikes and more importantly multiple, perhaps inhomogeneous and population spike trains. A synergy with other biologically relevant models seems necessary for modeling large-scale complex biological systems, especially the brain. Neural coding is widely studied under information theory. Probabilistic extensions of the SOM may provide useful tools in deciphering and interpreting the information content and relationships conveyed among stimuli and responses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A cross-sectional diagram of the retina, drawn by Santiago Ramón y Cajal (1853-1934)</figDesc><graphic coords="3,103.16,54.21,234.03,170.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Surface of the Limulus eye and simuli: small spot light and rectangular lighter/darker pattern; (b) recordings of spike rate in the ommatidium axon (the upper curve is the response to the small spot light at high and low intensities corresponding to those of the test pattern in the insert; the lower curve is the response to the rectangular lighter/darker test pattern (from<ref type="bibr" target="#b86">[87]</ref>; also see<ref type="bibr" target="#b98">[99]</ref>)</figDesc><graphic coords="4,62.60,54.18,315.25,111.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. von der Malsburg's self-organizing map model: local clusters in a presynaptic sheet are connected to local clusters in a postsynaptic sheet; there are lateral interconnections within the postsynaptic sheet (solid lines are used to indicate such connections)</figDesc><graphic coords="8,72.08,54.32,296.16,198.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Kohonen's self-organizing map model. The input is connected to every cell in the postsynaptic sheet (the map). The learning makes the map localized, in other words different local fields will respond to different ranges of inputs. The lateral excitation and inhibition connections are emulated by a mathematical modification, namely local sharing, to the learning mechanism (so there are no actual connections between cells -grey lines are used to indicate these virtual connections)</figDesc><graphic coords="10,84.08,53.70,272.03,184.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, in which (b) shows the 16 × 16 codebook trained on the Lena test image of 512 × 512 pixels by SOM with distinctive ordering found among the code vectors; and (a) shows the quantized Lena image by the trained codebook. The code vectors are of 4 × 4 pixel blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Quantized Lena image; (b) the SOM codebook (map)</figDesc><graphic coords="30,58.16,54.72,153.64,153.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) An aerial image; (b) segmented image using the SOM and Markov random field</figDesc><graphic coords="31,75.56,53.83,143.08,143.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) X-ray diffraction pattern (part); (b) modelled profiles by a 20 × 20 SOMN (from [129])</figDesc><graphic coords="32,70.00,55.00,298.00,401.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Circular SOMs for clustering temporal gene expressions (yeast cell cycle dataset): (a) 208-gene dataset; (b) 511-gene dataset (from [80])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Mapping and visualization of the iris data set: (top left) PCA, (top right) Sammon mapping; (bottom left) SOM with U matrix Colouring, (bottom right) ViSOM</figDesc><graphic coords="35,79.64,461.20,134.40,104.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Manifold mapping by various methods: (top left) original S-shape data and ViSOM embedding, (top right) Isomap projection; (bottom left) LLE projection, (bottom right) ViSOM projection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. A typical result of using a topological tree structure for organizing documents</figDesc><graphic coords="38,53.84,53.81,332.81,453.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="39,60.08,53.68,320.16,288.23" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Simpson JA, Weiner ESC (eds.) (1988) Oxford English Dictionary (2nd ed). Clarendon Press, Oxford, UK.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A common neighbourhood function, η(ν(x), i), can be added as in the SOM, but is optional.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks (Special Issue on New Developments in Self-Organising Maps)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="937" to="1155" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-organising maps for pattern recognition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kohonen Maps</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topographic organisation of nerve fields</title>
		<author>
			<persName><forename type="first">S-I</forename><surname>Ameri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin Mathematical Biology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="339" to="364" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel-Kohonen networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Neural Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="117" to="135" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ice floe identification in satellite images using mathematical morphology and clustering about principal curves</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Banfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="7" to="16" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A distributed robotic control system based on a temporal self-organizing neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afr</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ducker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man and Cybernetics -C</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="347" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantifying the neighborhood preservation of self-organizing feature maps</title>
		<author>
			<persName><forename type="first">H-U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Pawelzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="570" to="579" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GTM: the generative topographic mapping</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Svensén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="215" to="235" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual Perception: Physiology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lawerence Erlbraum Associates</title>
		<meeting><address><addrLine>East Essex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified model for probabilistic principal surfaces</title>
		<author>
			<persName><forename type="first">K-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The temporal Kohonen map</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="441" to="445" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A stochastic model of retinotopy: a self-organising process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="405" to="411" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks (Special Issue on Advances in Self-Organizing Maps)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="721" to="976" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maa</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical tools to assess the reliability of self-organising maps</title>
		<author>
			<persName><forename type="first">E</forename><surname>De Bolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="967" to="978" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sammon mapping using neural networks: a comparison</title>
		<author>
			<persName><forename type="first">D</forename><surname>De Ridder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rpw</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1307" to="1316" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asymptotic level density in topological feature maps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Dersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="230" to="236" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dimension reduction framework for understanding cortical maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="644" to="647" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-organising maps: ordering, convergence properties and energy functions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-organising maps: stationary states, metastability and convergence rate</title>
		<author>
			<persName><forename type="first">E</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online data visualization using the neural gas network</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Estévez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Figueroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="923" to="934" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient video compression codebooks using SOM-based vector quantisation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE -Vision, Image and Signal Processing</title>
		<meeting>IEE -Vision, Image and Signal essing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive topological tree structure (ATTS) for document organisation and visualisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1255" to="1271" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gaze</surname></persName>
		</author>
		<title level="m">The Information of Nerve Connections</title>
		<meeting><address><addrLine>London, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unifying objective function for topographic mappings</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Goodhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1291" to="1303" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Phase transitions in stochastic self-organizing maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reviews E</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="3876" to="3890" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image denoising using selforganising map-based nonlinear independent component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haritopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1085" to="1098" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Principal curves</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Organisation of behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perspectives and limitations of self-organising maps in blind separation of source signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Neural Information Processing (ICONIP&apos;96)</title>
		<editor>
			<persName><forename type="first">S-I</forename><surname>Amari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L-W</forename><surname>Chan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K-S</forename><surname>Leung</surname></persName>
		</editor>
		<meeting>Intl. Conf. Neural Information essing (ICONIP&apos;96)<address><addrLine>Hong Kong; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996-09">1996. September</date>
			<biblScope unit="page" from="1211" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Energy functions for self-organizing maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kohonen Maps</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="303" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical growing cell structures: TreeGCS</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">WEBSOM-self-organizing maps of document collections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lagus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Self-Organizing Maps (WSOM&apos;97)</title>
		<meeting>Workshop on Self-Organizing Maps (WSOM&apos;97)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">1997. June</date>
			<biblScope unit="page" from="310" to="315" />
		</imprint>
		<respStmt>
			<orgName>Helsinkis University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalising self-organising map for categorical data</title>
		<author>
			<persName><forename type="first">C-C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks (Special Issue on New Developments in Self-Organizing Systems)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1037" to="1389" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalisation of principal component analysis, optimisation problems, and neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joutsensalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="549" to="562" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bibliography of self-organizing map (SOM) papers: 1981-1997</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="176" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploratory data analysis by the self-organizing map: Structures of welfare and poverty in the world</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks in Financial Engineering</title>
		<editor>
			<persName><forename type="first">A-Pn</forename><surname>Refenes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Abu-Mostafa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Weigend</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A polygonal line algorithm for constructing principal curves</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS&apos;98)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="501" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Correlation matrix memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="353" to="359" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new model for randomly organised associative memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An adaptive associative memory principle</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="444" to="445" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-organised formation of topologically correct feature map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="56" to="69" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-organization and Associative Memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation of sensory information in self-organising feature maps, and relation of these maps to distributed memory networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">634</biblScope>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive, associative, and self-organizing functions in neural computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4910" to="4918" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-organizing maps: optimization approaches</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Makisara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</editor>
		<meeting><address><addrLine>North-Holland, Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="981" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The adaptive-subspace SOM (ASSOM) and its use for the implementation of invariant feature detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Artificial Neural Systems (ICANN&apos;95), 9-13 October</title>
		<editor>
			<persName><surname>Fogelman-Soulié</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</editor>
		<meeting>Intl. Conf. Artificial Neural Systems (ICANN&apos;95), 9-13 October<address><addrLine>Paris, France. EC2 Nanterre, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Emergence of invariant-feature detectors in the adaptivesubspace self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<title level="m">Self-Organising Maps</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Comparison of SOM point densities based on different criteria</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2081" to="2095" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How to make a large self-organising maps for nonvectorial data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Somervuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Institute Chemical Engineers J</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PicSOM -content-based image retrieval with self-organizing maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laakso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1199" to="1207" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clustering properties of hierarchical self-organizing maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Kernel self-organising maps for classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2033" to="2040" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive principal surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A triangulation method for the sequential mapping of points from n-space to two-space</title>
		<author>
			<persName><forename type="first">Rct</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Slagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="288" to="292" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weight-value convergence of the SOM algorithm for discrete input</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the rate of convergence in topology preserving neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bavarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="55" to="63" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Feed-forward neural networks and topographic mappings for exploratory data analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="95" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Derivation of a class of training algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Luttrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="229" to="232" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Code vector density in topographic mappings: Scalar case</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Luttrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="436" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of self-organising maps</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Luttrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="767" to="794" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The kernel self organising map</title>
		<author>
			<persName><forename type="first">D</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Intl. Conf. Knowledge-based Intelligence Engineering Systems and Applied Technologies</title>
		<meeting>4th Intl. Conf. Knowledge-based Intelligence Engineering Systems and Applied Technologies<address><addrLine>Brighton, UK; Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2000-08-30">2000. 30 August -1 September</date>
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Limitations of nonlinear PCA as performed with generic neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Malthouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="165" to="173" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Artificial Neural Networks for Feature Extraction and Multivariate Data Projection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="296" to="317" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A theory of cerebellar cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiology</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="437" to="470" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A self-organising network that grows when required</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marsland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Nehmzow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1041" to="1058" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A &quot;neuralgas&quot; network learns topologies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mäkisara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</editor>
		<meeting><address><addrLine>NorthHolland, Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="397" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Topology representing networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="507" to="522" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Script recognition with hierarchical feature maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dyslexic and category-specific aphasic impairments in a self-organizing feature map model of the lexicon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="334" to="366" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A type of duality between self-organising maps and minimal wiring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Modeling and analysis of gene expression time-series based on co-expression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Möller-Levet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Neural Systems, (Special Issue on Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="311" to="322" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Circular SOM for temporal characterisation of modelled gene expressions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Möller-Levet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Intelligent Engineering Data Engineering and Automated Learning Conf. (IDEAL&apos;05)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Gallagher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maire</forename><forename type="middle">F</forename></persName>
		</editor>
		<meeting>Intl. Conf. Intelligent Engineering Data Engineering and Automated Learning Conf. (IDEAL&apos;05)<address><addrLine>Brisbane, Australia; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005-06-08">2005. 6-8 July</date>
			<biblScope unit="volume">3578</biblScope>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Neural networks, principal components, and subspaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Neural Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">PCA, ICA, and nonlinear Hebbian learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Artificial Neural Networks (ICANN&apos;95), 9-13 October</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Fogelman-Soulié</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</editor>
		<meeting>Intl. Conf. Artificial Neural Networks (ICANN&apos;95), 9-13 October<address><addrLine>Paris, France. EC2, Nanterre, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Bibliography of self-organizing map (SOM) papers: 1998-2001 addendum</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="156" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Nonlinear blind source separation by self-organising maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Neural Information Processing (ICONIP&apos;96)</title>
		<editor>
			<persName><forename type="first">S-I</forename><surname>Amari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L-W</forename><surname>Chan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K-S</forename><surname>Leung</surname></persName>
		</editor>
		<meeting>Intl. Conf. Neural Information essing (ICONIP&apos;96)<address><addrLine>Hong Kong; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996-09">1996. September</date>
			<biblScope unit="page" from="1207" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A kernel-base SOM classifier in input space</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="227" to="231" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>in Chinese</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Computer-generated cartoons</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Images and Understandings</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Barlow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Blakemore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weston</forename><forename type="middle">-</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Ratcliff</surname></persName>
		</author>
		<title level="m">Mach Bands: Quantitative Studies on Neural Networks in the Retina</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Holden-Day, Inc</publisher>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The growing hierarchical selforganizing map: exploratory analysis of high-dimensional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merkl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dittenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1331" to="1341" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Asymptotical level density for class of vector quantisation processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Convergence properties of Kohonen&apos;s topology conserving maps: fluctuations, stability, and dimension selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Neural Computation and Self-organising Maps: An Introduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Stochastic approximation: A recursive method for solving regression problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Communication Systems: Theory and Applications</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A nonlinear mapping for data structure analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sammon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="409" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Optimal unsupervised learning in a single-layer linear feedforward network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="459" to="473" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1131" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Multi-dimensional self-organizing maps on massively parallel hardware</title>
		<author>
			<persName><forename type="first">U</forename><surname>Seiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michaelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Self-Organising Maps</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Allinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Slack</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="160" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1988">1988</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK.</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Yet another algorithm which can generate topography map</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1204" to="1207" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Reinforcement learning is direct adaptive optimal control</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. American Control Conf., 26-28 June</title>
		<meeting>American Control Conf., 26-28 June<address><addrLine>Boston, MA; Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="2143" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Principal curves revisited</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Analysis of gene expression data using self-organising maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Törönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kolehmainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castrén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Federation European Biochemical Socities Letters</title>
		<imprint>
			<biblScope unit="volume">451</biblScope>
			<biblScope unit="page" from="142" to="146" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Self-organising neural networks for visualisation and classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ultsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information and Classification</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Opitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Lausen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Klar</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="864" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Kernel-based equiprobabilitic topographic map formation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1847" to="1871" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Kernel-based equiprobabilitic topographic map formation achieved with an information-theoretic approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1029" to="1040" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A recurrent selforganizing map for temporal sequence processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varsta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ruiz Millän</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICANN&apos;97</title>
		<meeting>ICANN&apos;97<address><addrLine>Lausanne, Switzerland; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Topology preservation in self-organizing feature maps: exactdefinition and measurement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Martinetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="256" to="266" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Recursive self-organizing maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Voegtlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="979" to="992" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Self-organization of orientation sensitive cells in the striate cortex</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetik</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Rapid learning with parametrized self-organizing maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="131" to="153" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Real-time synthesis of 3D animations by learning parametric gaussians using self-organizing mixture networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z-Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Neural Information Processing (ICONIP&apos;06)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</editor>
		<meeting>Intl. Conf. Neural Information essing (ICONIP&apos;06)<address><addrLine>Hong Kong; Berlin, II</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-02-06">2006. 2-6 October</date>
			<biblScope unit="volume">4233</biblScope>
			<biblScope unit="page" from="671" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Non-holographic associative memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Willshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longnet-Higgins</forename><surname>Hc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="960" to="962" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">How patterned neural connections can be set up by self-organization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Willshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Royal Society of London -Series B</title>
		<meeting>Royal Society of London -Series B</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">PRSOM: A new visualization method by hybridizing multidimensional scaling and self-organizing map</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tws</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1362" to="1380" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Self-Organising Maps: Statistical Analysis, Treatment and Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electronics, University of York, UK.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<title level="m">Visualisation induced SOM (ViSOM)</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Allinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Slack</surname></persName>
		</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
	<note>Advances in Self-Organising Maps</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">ViSOM-A novel method for multivariate data projection and structure visualisation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="237" to="243" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Data visualisation and manifold mapping using the ViSOM</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Nonlinear multidimensional data projection and visualisation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Intelligent Data Engineering and Automated Learning (IDEAL&apos;03)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Cheung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<meeting>Intl. Conf. Intelligent Data Engineering and Automated Learning (IDEAL&apos;03)<address><addrLine>Hong Kong; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003-03">2003. March</date>
			<biblScope unit="volume">2690</biblScope>
			<biblScope unit="page" from="377" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Resolution enhancement for the ViSOM</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Self-Organizing Maps, 11-14 September</title>
		<meeting>Workshop on Self-Organizing Maps, 11-14 September<address><addrLine>Kitakyushu, Japan. Kyushu Institute of</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="208" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">On the equivalence between kernel self-organising maps and self-organising mixture density networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="784" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Connection between self-organising maps and metric multidimensional scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Joint Conf. Neural Networks (IJCNN2007</title>
		<meeting>Intl. Joint Conf. Neural Networks (IJCNN2007<address><addrLine>Orlando, FL; Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2007-08">2007. August</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of textured images using a hierarchical neural structure</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1842" to="1843" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">On the distribution and convergence of the feature space in self-organising maps</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1178" to="1187" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Interpolating self-organising map (iSOM)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1649" to="1650" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Self-organising mixture networks for probability density estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="405" to="411" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Bayesian self-organising map for Gaussian mixtures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE -Vision, Image and Signal Processing</title>
		<meeting>IEE -Vision, Image and Signal essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps (1st, 2nd and 3rd editions)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995, 1997. 2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Neural Networks: A Comprehensive Foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Macmillian</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Kohonen Maps</title>
		<editor>Oja E, Kaski S</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Advances in Self-Organising Maps</title>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Neural Computation and Selforganising Maps: An Introduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Faithful Representations and Topographic Maps: From Distortion to Information Based Self-Organization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Special Issue on New Developments in Self-Organising Maps</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="937" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Special Issue on New Developments in Self-Organizing Systems, Neural Networks</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ishikawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="1037" to="1389" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Special Issue on Advances in Self-Organizing Maps</title>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Cottrell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="721" to="976" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Workshop on Self-Organizing Maps (biennial, since 1997)</title>
		<imprint>
			<publisher>Key International Conferences/Workshops WSOM -Intl</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><surname>Ijcnn -Intl</surname></persName>
		</author>
		<title level="m">Joint Conference on Neural Networks (annual, since 1987)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<author>
			<persName><surname>Icann -Intl</surname></persName>
		</author>
		<title level="m">Conference on Artificial Neural Networks (annual, since 1991)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Artificial Neural Networks (annual, since</title>
		<author>
			<persName><surname>Esann -European</surname></persName>
		</author>
		<author>
			<persName><surname>Symp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<author>
			<persName><surname>Iconip -Intl</surname></persName>
		</author>
		<title level="m">IDEAL -International Conference on Intelligent Data Engineering and Automated Learning</title>
		<imprint>
			<date type="published" when="1994">1994. 1998</date>
		</imprint>
	</monogr>
	<note>Neural Information Processing</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Symposium on Artificial Neural Networks (annual, since</title>
		<author>
			<persName><surname>Isnn -Intl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<ptr target="http://www.cis.hut.fi/projects/somtoolbox/" />
		<title level="m">Open Source) Software SOM Toolbox</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
