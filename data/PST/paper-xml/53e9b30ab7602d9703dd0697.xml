<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Coresets for k-Means and k-Median Clustering *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sariel</forename><surname>Har-Peled</surname></persName>
							<email>sariel@uiuc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Soham</forename><surname>Mazumdar</surname></persName>
							<email>smazumda@uiuc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Coresets for k-Means and k-Median Clustering *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D6EA0EA5536CF6E389297B94225E5C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>F.2.2 [Theory of Computation]: Analysis of Algorithms and Problem Complexity-Non-numerical Algorithms and Problems Coreset</term>
					<term>k-median</term>
					<term>k-means</term>
					<term>Streaming</term>
					<term>Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we show the existence of small coresets for the problems of computing k-median and k-means clustering for points in low dimension. In other words, we show that given a point set P in R d , one can compute a weighted set S ⊆ P , of size O(kε -d log n), such that one can compute the k-median/means clustering on S instead of on P , and get an (1 + ε)-approximation.</p><p>As a result, we improve the fastest known algorithms for (1 + ε)-approximate k-means and k-median. Our algorithms have linear running time for a fixed k and ε. In addition, we can maintain the (1 + ε)-approximate k-median or k-means clustering of a stream when points are being only inserted, using polylogarithmic space and update time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Clustering is a widely used technique in Computer Science with applications to unsupervised learning, classification, data mining and other fields. We study two variants of the clustering problem in the geometric setting. The geometric k-median clustering problem is the following: Given a set P of points in R d , compute a set of k points in R d such that the sum of the distances of the points in P to their respective nearest median is minimized. The k-means problem differs from the above in that instead of the sum of distances, we minimize the sum of squares of distances. Interestingly the 1-mean is the center of mass of the points, while the 1-median problem, also known as the Fermat-Weber problem, has no such closed form. As such the problems have usually been studied separately from each other even in the approximate setting. We propose techniques which can be used for finding approximate k centers in both variants.</p><p>In the data stream model of computation, the points are read in a sequence and we desire to compute a function, clustering in our case, on the set of points seen so far. In typical applications, the total volume of data is very large and can not be stored in its entirety. Thus we usually require a data-structure to maintain an aggregate of the points seen so far so as to facilitate computation of the objective function. Thus the standard complexity measures in the data stream model are the storage cost, the update cost on seeing a new point and the time to compute the function from the aggregated data structure. k-median clustering. The k-median problem turned out to be nontrivial even in low dimensions and achieving a good approximation proved to be a challenge. Motivated by the work of Arora <ref type="bibr" target="#b4">[4]</ref>, which proposed a new technique for geometric approximation algorithms, Arora, Raghavan and Rao <ref type="bibr" target="#b5">[5]</ref> presented a O n O(1/ε)+1 time (1 + ε)-approximation algorithm for points in the plane. This was significantly improved by Kolliopoulos and Rao <ref type="bibr" target="#b26">[26]</ref> who proposed an algorithm with a running time of O( n log n log k) for the discrete version of the problem, where the medians must belong to the input set and = exp [O ((1 + log 1/ε)/ε) d -1 ]. The kmedian problem has been studied extensively for arbitrary metric spaces and is closely related to the un-capacitated facility location problem. Charikar et al. <ref type="bibr" target="#b12">[12]</ref> proposed the first constant factor approximation to the problem for an arbitrary metric space using a natural linear programming relaxation of the problem followed by rounding the fractional solution. The fastest known algorithm is due to Mettu and Plaxton <ref type="bibr" target="#b29">[29]</ref> who give an algorithm which runs in O(n(k + log n)) time for small enough k given the distances are bound by 2 O(n/ log(n/k)) . It was observed that if the constraint of having exactly k-medians is relaxed, the problem becomes considerably easier <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b24">24]</ref>. In particular, Indyk <ref type="bibr" target="#b22">[22]</ref> proposed a constant factor approximation algo-Problem Prev. Results</p><p>Our Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-median</head><p>O( n(log n) log k) <ref type="bibr" target="#b26">[26]</ref> (*) = exp [O ((1 + log 1/ε)/ε) d -1 ] O n + k O (1) log O (1) n [Theorem 5.5] discrete k-median O( n log n log k) <ref type="bibr" target="#b26">[26]</ref> O n + k O (1) log O (1) n</p><formula xml:id="formula_0">[Theorem 5.7] k-means O k (n(log n) k ε -2k 2 d ) [28] (**) O(n + k k+2 ε -(2d+1)k log k+1 n log k 1 ε ) [Theorem 6.1]</formula><p>Streaming k-median Const factor; Any metric space O(kpolylog) space <ref type="bibr" target="#b27">[27]</ref> k-means and k-median rithm which produces O(k) medians in O (nk) time. In the streaming context, Guha et al. <ref type="bibr" target="#b17">[17]</ref> propose an algorithm which uses O(n ε ) memory to compute 2 1/ε approximate kmedians. Charikar et al. <ref type="bibr" target="#b27">[27]</ref> improve the algorithm by reducing the space requirement to O(k • polylog(n)).</p><formula xml:id="formula_1">(1 + ε)-approx; Points in R d . O(kε -d log 2d+2 n) space [Theorem 7.2]</formula><p>k-means clustering. Inaba et al. <ref type="bibr" target="#b21">[21]</ref> observe that the number of Voronoi partitions of k points in R d is n kd and can be done exactly in time O(n kd+1 ). They also propose approximation algorithms for the 2-means clustering problem with time complexity O(n O(d) ). de la Vega et al. <ref type="bibr" target="#b13">[13]</ref> proposes a (1 + ε)-approximation algorithm, for high dimensions, with running time O(n k/ε 2 ). Matoušek <ref type="bibr" target="#b28">[28]</ref> proposed a (1+ε)-approximation algorithm for the geometric k-means problem with running time O nε -2k 2 d log k n .</p><p>Our Results.. We propose fast algorithms for the approximate k-means and k-medians problems in Euclidean metrics. The central idea behind our algorithms is computing a weighted point set which we call a (k, ε)-coreset. For an optimization problem, a coreset is a subset of input, such that we can get a good approximation to the original input by solving the optimization problem directly on the coreset. As such, to get good approximation, one needs to compute a coreset, as small as possible from the input, and then solve the problem on the coreset using known techniques. Coresets have been used for geometric approximation mainly in low-dimension <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b2">2]</ref>, although a similar but weaker concept was also used in high dimensions <ref type="bibr">[9,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b20">20]</ref>. In low dimensions coresets yield approximation algorithm with linear or near linear running time with an additional term that depends only on the size of the coreset.</p><p>In the present case, the property we desire of the (k, ε)coreset is that the clustering cost of the coreset for any arbitrary set of k centers is within (1 ± ε) of the cost of the clustering for the original input. To facilitate the computation of the coreset, we first show a linear time algorithm (for k = O(n 1/4 )), that constructs a O(kpolylog) sized set of centers such that the induced clustering gives a constant factor approximation to both the optimal k-means and the optimal k-medians clustering. We believe that the technique used for this fast algorithm is of independent interest. Note that it is faster than previous published fast algorithms for this problem (see <ref type="bibr" target="#b29">[29]</ref> and references therein), since we are willing to use more centers. Next, we show how to con-struct a suitably small coreset from the set of approximate centers. We compute the k clusterings for the coresets using weighted variants of known clustering algorithms. Our results are summarized in Table <ref type="table">1</ref>.</p><p>One of the benefits of our new algorithms is that in the resulting bounds, on the running time, the term containing 'n' is decoupled from the "nasty" exponential constants that depend on k and 1/ε. Those exponential constants seems to be inherent to the clustering techniques currently known for those problems.</p><p>Our techniques extend very naturally to the streaming model of computation. The aggregate data-structure is just a (k, ε)-coreset of the stream seen so far. The size of the maintained coreset is O(kε -d log n), and the overall space used is O((log 2d+2 n)/ε d ). The amortized time to update the data-structure on seeing a new point is O(k 5 + log 2 (k/ε)).</p><p>As a side note, our ability to get linear time algorithms for fixed k and ε, relies on the fact that our algorithms need to solve a batched version of the nearest neighbor problem. In our algorithms, the number of queries is considerably larger than the number of sites, and the distances of interest arise from clustering. Thus, a small additive error which is related to the total cost of the clustering is acceptable. In particular, one can build a data-structure that answers nearest neighbor queries in O(1) time per query, see Appendix B. Although this is a very restricted case, this result may nevertheless be of independent interest, as this is the first data-structure to offer nearest neighbor queries in constant time, in a nontrivial settings.</p><p>The paper is organized as follows. In Section 3, we describe a fast constant factor approximation algorithm which generates more than k means/medians. In Section 4, we prove the existence of coresets for k-median/means clustering. In Section 5 and Section 6, we combine the results of the two preceding sections, and present an (1+ε)-approximation algorithm for k-means and k-median respectively. In Section 7, we show how to use coresets for space efficient streaming. We conclude in Section 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FAST CONSTANT FACTOR APPROXI-MATION WITH MORE CENTERS</head><p>Let P be the given point set in R d . We want to quickly compute a constant factor approximation to the k-means clustering of P , while using more than k centers. The number of centers output by our algorithm is O k log 3 n . Surprisingly, the set of centers computed by the following algorithm is a good approximation for both k-median and kmeans. To be consistent, throughout this section, we refer to k-means, although everything holds nearly verbatim for k-median as well. Definition 3.1 (bad points) For a point set X, define a point p ∈ P as bad with respect to X, if the cost it pays in using a center from X is prohibitively larger than the cost Copt pays for it; more precisely d(p, X) ≥ 2d(p, Copt). A point p ∈ P which is not bad, is by necessity, if not by choice, good. Here Copt = Copt(P, k) is a set of optimal k-means centers realizing µopt(P, k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Construction of the Set X of Centers</head><p>For k = O(n 1/4 ), we can compute a 2-approximate kcenter clustering of P in linear time <ref type="bibr" target="#b18">[18]</ref>, or alternatively, for k = Ω(n 1/4 ), in O(n log k) time, using the algorithm of Feder and Greene <ref type="bibr" target="#b14">[14]</ref>. This is the min-max clustering where we cover P by a set of k balls such the radius of the largest ball is minimized. Let V be the set of k centers computed, together with the furthest point in P from those k centers.</p><p>Let L be the radius of this 2-approximate clustering. Since both those algorithms are simulating the (slower) algorithm of Gonzalez <ref type="bibr" target="#b16">[16]</ref>, we have the property that the minimal distance between any points of V is at least L. Thus, any kmeans clustering of P , must have cost at least (L/2) 2 , and is at most nL 2 , and as such L is a rough estimate of µopt(P, k). In fact, this holds even if we restrict out attention only to</p><formula xml:id="formula_2">V ; explicitly (L/2) 2 ≤ µopt(V, k) ≤ µopt(P, k) ≤ nL 2 .</formula><p>Next, we pick a random sample Y from P of size ρ = γk log 2 n, where γ is a large enough constant whose value would follow from our analysis. Let X = Y ∪ V be the required set of cluster centers. In the extreme case where ρ &gt; n, we just set X to be P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Large Good Subset for X</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Bad points are few</head><p>Consider the set Copt of k optimal centers for the k-means, and place a ball bi around each point of ci ∈ Copt, such that bi contain η = n/(20k log n) points of P . If γ is large enough, it is easy to see that with high probability, there is at least one point of X inside every ball bi. Namely, X ∩ bi = ∅, for i = 1, . . . , k.</p><p>Let P bad be the set of all bad points of P . Assume, that there is a point xi ∈ X inside bi, for i = 1, . . . , k. Observe, that for any p ∈ P \ bi, we have pxi ≤ 2 pci . In particular, if ci is the closest center in Copt to p, we have that p is good. Thus, with high probability, the only bad points in P are the one that lie inside the balls b1, . . . , b k . But every one of those balls, contain at most η points of P . It follows, that with high probability, the number of bad points in P with respect to X is at most</p><formula xml:id="formula_3">β = k • η = n/(20 log n).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Keeping Away from Bad Points</head><p>Although the number of bad points is small, there is no easy way to determine the set of bad points. We instead construct a set P ensuring that the clustering cost of the bad points in P does not dominate the total cost. For every point in P , we compute its approximate nearest neighbor in X. This can be easily done in O(n log |X| + |X| log |X|) time using appropriate data structures <ref type="bibr" target="#b6">[6]</ref>, or in</p><formula xml:id="formula_4">O(n + n |X| 1/4 log n) time using Corollary B.4 (with D = nL). This stage takes O(n) time, if k = O(n 1/4 ), else it takes O(n log |X| + |X| log |X|) = O(n log(k log n)) time, as |X| ≤ n.</formula><p>In the following, to simplify the exposition, we assume that we compute exactly the distance r(p) = d(p, X), for p ∈ P .</p><p>Next, we partition P into classes in the following way. Let <ref type="figure">,</ref><ref type="figure">M</ref>, where M = 2 lg n + 3. This partition of P can be done in linear time using the log and floor function.</p><formula xml:id="formula_5">P [a, b] = p ∈ P a ≤ r(p) &lt; b . Let P0 = P [0, L/(4n)], P∞ = P [2Ln, ∞] and Pi = P 2 i-1 L/n, 2 i L/n , for i = 1, . . .</formula><p>Let Pα be the last class in this sequence that contains more than 2β = 2(n/(20 log n)) points. Let P = V ∪ i≤α Pi. We claim that P is the required set. Namely, |P | ≥ n/2 and µX (P ) = O(µC opt (P )), where Copt = Copt(P, k) is the optimal set of centers for P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Proof of Correctness</head><formula xml:id="formula_6">Clearly, P contains at least (n -|P∞| -M • (2n/20 log n)) points. Since P∞ ⊆ P bad and |P bad | ≤ β, hence |P | &gt; n/2.</formula><p>If α &gt; 0, we have |Pα| ≥ 2β = 2(n/(20 log n)). Since P is the union of all the classes with distances smaller than the distances in Pα, it follows that the worst case scenario is when all the bad points are in Pα. But with high probability the number of bad points is at most β, and since the cost of all the points in Pα is roughly the same, it follows that we can charge the cost of the bad points in P to the good points in Pα.</p><p>Formally, let Q = Pα \ P bad . For any point p ∈ P ∩ P bad and q ∈ Q , we have d(p, X) ≤ 2d(q, X). Further |Q | &gt; |P bad |. Thus, µX (P ∩ P bad ) ≤ 4µX (Q ) ≤ 16µC opt (Q ) ≤ 16µC opt (P ). Thus, µX (P ) = µX (P ∩ P bad ) + µX (P \ P bad ) ≤ 16µC opt (P ) + 4µC opt (P ) = 20µC opt (P ).</p><p>If α = 0 then for any point p ∈ P , we have (d(p, X)) 2 ≤ n(L/4n) 2 ≤ L 2 /(4n) and thus µX (P ) ≤ L 2 /4 ≤ µC opt (V ) ≤ µC opt (P ), since V ⊆ P .</p><p>In the above analysis we assumed that the nearest neighbor data structure returns the exact nearest neighbor. If we were to use an approximate nearest neighbor instead, the constants would slightly deteriorate. </p><formula xml:id="formula_7">(n) if k = O(n 1/4 ), and O(n log (k log n)) otherwise.</formula><p>Now, finding a constant factor k-median clustering is easy. Apply Lemma 3.2 to P , remove the subset found, and repeat on the remaining points. Clearly, this would require O(log n) iterations. We can extend this algorithm to the weighted case, by sampling O(k log 2 W ) points at every stage, where W is the total weight of the points. Note however, that the number of points no longer shrink by a factor of two at every step, as such the running time of the algorithm is slightly worse. Furthermore, the set X is a good set of centers for kmedian. Namely, we have that νX (P ) ≤ 32νopt(P, k).</p><p>If the point set P is weighted, with total weight W , then the size of X becomes O(k log 3 W ), and the running time becomes O(n log 2 W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CORESET FROM APPROXIMATE CLUSTERING</head><formula xml:id="formula_8">Definition 4.1 (Coreset) For a weighted point set P ⊆ R d , a weighted set S ⊆ R d , is a (k, ε)-coreset of P for the k-median clustering, if for any set C of k points in R d , we have (1 -ε)νC (P ) ≤ νC (S) ≤ (1 + ε)νC (P ).</formula><p>Similarly, S is a (k, ε)-coreset of P for the k-means clustering, if for any set C of k points in R d , we have (1-ε)µC (P ) ≤ µC (S) ≤ (1 + ε)µC (P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Coreset for k-Median</head><p>Let P be a set of n points in R d , and A = {x1, . . . , xm} be a point set, such that νA(P ) ≤ cνopt(P, k), where c is a constant. We give a construction for a (k, ε)-coreset using A. Note that we do not have any restriction on the size of A, which in subsequent uses will be taken to be O(kpolylog).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The construction</head><p>Let Pi be the points of P having xi as their nearest neighbor in A, for i = 1, . . . , m. Let R = νA(P )/ (cn) be a lower bound estimate of the average radius R ν opt (P, k) = νopt(P, k)/n. For any p ∈ Pi, we have pxi ≤ cnR, since pxi ≤ νA(P ), for i = 1, . . . , m.</p><p>Next, we construct an appropriate exponential grid around each xi, and snap the points of P to those grids. Let Qi,j be an axis-parallel square with side length R2 j centered at xi, for j = 0, . . . , M, where M = 2 lg(cn) . Next, let Vi,0 = Qi,0, and let Vi,j = Qi,j \ Qi,j-1, for j = 1, . . . , M. Partition Vi,j into into a grid with side length rj = εR2 j /(10cd), and let Gi denote the resulting exponential grid for Vi,0, . . . , Vi,M . Next, compute for every point of Pi, the grid cell in Gi that contains it. For every non empty grid cell, pick an arbitrary point of Pi inside it as a representative point for the coreset, and assign it a weight equal to the number of points of Pi in this grid cell. Let Si denote the resulting weighted set, for i = 1, . . . , m, and let S = ∪iSi.</p><p>Note that |S| = O (|A| log n) /ε d . As for computing S efficiently. Observe that all we need is a constant factor approximation to νA(P ) (i.e., we can assign a p ∈ P to Pi if p, xi ≤ 2d(p, A)). This can be done in a naive way in O(nm) time, which might be quite sufficient in practice. Alternatively, one can use a data-structure that answers constant approximate nearest-neighbor queries in O(log m) when used on A after O(m log m) preprocessing <ref type="bibr" target="#b6">[6]</ref>. Another option for computing those distances between the points of P and the set A is using Theorem B.3 that works in O(n+mn 1/4 log n) time. Thus, for i = 1, . . . , m, we compute a set P i which consists of the points of P that xi (approximately) serves. Next, we compute the exponential grids, and compute for each point of P i its grid cell. This takes O(1) time per point, with a careful implementation, using hashing, the floor function and the log function. Thus, if It is easy to see that the above algorithm can be easily extended for weighted point sets.</p><formula xml:id="formula_9">m = O( √ n) the overall running time is O(n+mn 1/4 log n) = O(n) and O(m log m + n log m + n) = O(n log m) otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Proof of Correctness</head><p>Theorem 4.3 Given a point set P with n points, and a point set A with m points, such that νA(P ) ≤ cνopt(P, k), where c is a constant. Then, one can compute a weighted set S which is a (k, ε)-coreset for P , and</p><formula xml:id="formula_10">|S| = O (|A| log n)/ε d . The running time is O(n) if m = O( √ n) and O(n log m) otherwise.</formula><p>For a weighted point set P , with total weight W , the size of the coreset is</p><formula xml:id="formula_11">|S| = O (|A| log W )/ε d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Coreset for k-Means from Approximate Clustering</head><p>The construction of the k-means coreset follows the kmedian with a few minor modifications. Let P be a set of n points in R d , and a A be a point set A = {x1, . . . , xm}, such that µA(P ) ≤ cµopt(P, k). Let R = (µA(P )/(cn)) be a lower bound estimate of the average mean radius R µ opt (P, k) = µopt(P, k)/n. For any p ∈ Pi, we have pxi ≤ √ cnR, since pxi 2 ≤ µA(P ), for i = 1, . . . , m.</p><p>Next, we construct an exponential grid around each point of A, as in the k-median case, and snap the points of P to this grid, and we pick a representative point for such grid cell. See Section 4.1.1 for details. We claim that the resulting set of representatives S is the required coreset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.4 Given a set P with n points, and a point set</head><p>A with m points, such that µA(P ) ≤ cµopt(P, k), where c is a constant. Then, can compute a weighted set S which is a (k, ε)-coreset for P , and</p><formula xml:id="formula_12">|S| = O (m log n)/(cε) d . The running time is O(n) if m = O(n 1/4 ) and O(n log m) other- wise.</formula><p>If P is a weighted set with total weight W , then the size of the coreset is O (m log W )/ε d .</p><p>We omit the proof of this theorem. It goes along the same lines as the proof for the medians case but is much more tedious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PUTTING THINGS TOGETHER: A (1+ε)-APPROXIMATION FOR K-MEDIANS</head><p>We now present the approximation algorithm using exactly k centers. Assume that the input is a set of n points. We use the set of centers computed in Theorem 3.3 to compute a constant factor coreset using the algorithm of Theorem 4.3. The resulting coreset S, has size O(k log 4 n). Next we compute a O(n) approximation to the k-median for the coreset using the k-center (min-max) algorithm <ref type="bibr" target="#b16">[16]</ref>. Let C0 ⊆ S be the resulting set of centers. Next we apply the local search algorithm, due to Arya et al. <ref type="bibr" target="#b7">[7]</ref>, to C0 and S, where the set of candidate points is S. This local search algorithm, at every stage, picks a center c from the current set of centers Ccurr, and a candidate center s ∈ S, and swaps c out of the set of centers and s into the set of centers. Next, if the new set of centers C curr = Ccurr \ {c} ∪ {s} provides a considerable improvement over the previous solution (i.e., νC curr (S) ≤ (1ε/k)ν C curr (S) where ε here is an arbitrary small constant), then we set Ccurr to be C curr . Arya et al. <ref type="bibr" target="#b7">[7]</ref> showed that the algorithm terminates, and it provides a constant factor approximation to ν D opt (S, k), and as hence to νopt(P, k). It is easy to verify that it stops after O(k log n) such swaps. Every swap, in the worst case, requires considering |S| k sets. Computing the cost of clustering for every such candidate set of centers takes O (|S| k) time. Thus, the running time of this algorithm is O |S| 2 k 3 log n = O k 5 log 9 n . Finally, we use the new set of centers with Theorem 4.3, and get a (k, ε)coreset for P . It is easy to see that the algorithm works for weighted point-sets as well. Putting in the right bounds from Theorem 3.3 and Theorem 4.3 for weighted sets, we get the following. If P is a weighted set, with total weight W , the running time of the algorithm is O(n log 2 W + k 5 log 9 W ).</p><p>We would like to apply the algorithm of Kolliopoulos and Rao <ref type="bibr" target="#b26">[26]</ref> to the coreset, but unfortunately, their algorithm only works for the discrete case, when the medians are part of the input points. Thus, the next step is to generate from the coreset, a small set of candidate points in which we can assume all the medians lie, and use the (slightly modified) algorithm of <ref type="bibr" target="#b26">[26]</ref> on this set. Definition 5.2 (Weber-Centroid Set) Given a set P of n points in R d , a set D ⊆ R d is an (k, ε)-approximate webercentroid set for P , if there exists a subset C ⊆ D of size k, such that νC (P ) ≤ (1 + ε)νopt(P, k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5.3 Given a set</head><formula xml:id="formula_13">P of n points in R d , one can compute an (k, ε)-approximate weber-centroid set D of size O(k 2 ε -2d log 2 n) in time O n + k 5 log 9 n + k 2 ε -2d log 2 n .</formula><p>For a weighted point set P , with total weight W , the running time is O n log 2 W + k 5 log 9 W + k 2 ε -2d log 2 W , and the weber-centroid set is of size O(k 2 ε -2d log 2 W ).</p><p>Proof. Compute a (k, ε/12)-coreset S using Lemma 5.1. Retain the set B of k centers, for which νB(P ) = O(νopt(P, k)), which is computed during the construction of S. Further let R = νB(P )/n.</p><p>Next, compute around each point of S, an exponential grid using R, as was done in Section 4.1.1. This results in a point set D of size of O(k 2 ε -2d log 2 n). We claim that D is the required weber-centroid set. The proof proceeds on similar lines as the proof of Theorem 4.3 and is given in Appendix A.</p><p>We are now in the position to get a fast approximation algorithm. We generate the centroid set, and then we modify the algorithm of Kolliopoulos and Rao so that it considers centers only from the centroid set in its dynamic programming stage. For the weighted case, the depth of the tree constructed in <ref type="bibr" target="#b26">[26]</ref> is O(log W ) instead of O(log n). Further since their algorithm works in expectation, we run it independently O(log(1/δ)/ε) times to get a guarantee of (1δ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.5 ((1 + ε)-approx k-median) Given a set P of n points in R d , and parameter k, one can compute a (1 + ε)-approximate k-median clustering of P (in the continuous sense) in</head><formula xml:id="formula_14">O n + k 5 log 9 n + k 2 log 5 n time, where = exp [O ((1 + log 1/ε)/ε) d-1</formula><p>] and c is a constant. The algorithm outputs a set X of k points, such that νX (P ) ≤ (1 + ε)νopt(P, k). If P is a weighted set, with total weight W , the running time of the algorithm is O(n log 2 W + k 5 log 9 W + k 2 log 5 W ).</p><p>We can extend our techniques to handle the discrete median case efficiently as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5.6 Given a set</head><formula xml:id="formula_15">P of n points in R d , one can com- pute a discrete (k, ε)-approximate weber-centroid set D ⊆ P of size O(k 2 ε -2d log 2 n). The running time of this algo- rithm is O n + k 5 log 9 n + k 2 ε -2d log 2 n if k ≤ ε d n 1/4 and O n log n + k 5 log 9 n + k 2 ε -2d log 2 n otherwise.</formula><p>Proof. Omitted. Will appear in the full-version.</p><p>Combining Lemma 5.6 and Theorem 5.5, we get the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.7 (Discrete k-medians) One can compute an</head><formula xml:id="formula_16">(1 + ε)-approximate discrete k-median of a set of n points in time O n + k 5 log 9 n + k 2 log 5 n</formula><p>, where is the constant from Theorem 5.4.</p><p>Proof. The proof follows from the above discussion. As for the running time bound, it follows by considering separately the case when 1/ε 2d ≤ 1/n 1/10 , and the case when 1/ε 2d ≥ 1/n 1/10 , and simplifying the resulting expressions. We omit the easy but tedious computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">A (1+ε)-APPROXIMATION ALGORITHM FOR K-MEANS</head><p>We only state the result, the details are in Appendix C. Note that despite of the messy looking running time, our algorithm performs significantly better than Matoušek's algorithm <ref type="bibr" target="#b28">[28]</ref>, which runs in O(n(log n) k ε -2k 2 d ) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 6.1 (k-means) Given a n point set</head><formula xml:id="formula_17">P in R d , one can compute (1 + ε)-approximate k-means clustering of P in time O(n+k 5 log 9 n+k k+2 ε -(2d+1)k log k+1 n log k (1/ε)).</formula><p>For a weighted set with total weight W , the running time is</p><formula xml:id="formula_18">O(n log 2 W +k 5 log 9 W +k k+2 ε -(2d+1)k log k+1 W log k (1/ε)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">STREAMING</head><p>A consequence of our ability to compute quickly a (k, ε)coreset for a point set, is that we can maintain the coreset under insertions quickly. Observation 7.1 (i) If C1 and C2 are the (k, ε)-coresets for disjoint sets P1 and P2 respectively, then</p><formula xml:id="formula_19">C1 ∪ C2 is a (k, ε)-coreset for P1 ∪ P2. (ii) If C1 is (k, ε)-coreset for C2, and C2 is a (k, δ)-coreset for C3, then C1 is a (k, ε + δ)-coreset for C3.</formula><p>The above observation allows us to use Bentley and Saxe's technique <ref type="bibr" target="#b10">[10]</ref> as follows. Let P = (p 1, p2, . . . , pn) be the sequence of points seen so far. We partition P into sets P0, P1, P2, . . . , Pt such that each either Pi empty or |Pi| = 2 i M , for i &gt; 0 and M = O(k/ε d ). We refer to i as the rank of i.</p><p>Define ρj = ε/ c(j + 1) 2 where c is a large enough constant, and 1 + δj = j l=0 (1 + ρ l ), for j = 1, . . . , lg n . We store a (k, δj)-coreset Qj for each Pj. It is easy to verify that 1 + δj ≤ 1 + ε/2 for j = 1, . . . , lg n and sufficiently large c. Thus the union of the Qis is a (k, ε/2)-coreset for P .</p><p>On encountering a new point pu, the update is done in the following way: We add pu to P0. If P0 has less than M elements, then we are done. Note that for P0 its corresponding coreset Q0 is just itself. Otherwise, we set Q 1 = P0, and we empty Q0. If Q1 is present, we compute a (k, ρ2) coreset to Q1 ∪ Q 1 and call it Q 2 , and remove the sets Q1 and Q 1 . We continue the process until we reach a stage r where Qr did not exist. We set Q r to be Qr. Namely, we repeatedly merge sets of the same rank, reduce their size using the coreset computation, and promote the resulting set to the next rank. The construction ensures that Qr is a (k, δr) coreset for a corresponding subset of P of size 2 r M . It is now easy to verify, that Qr is a (k, j l=0 (1 + ρ l ) -1)-coreset for the corresponding points of P .</p><p>We modify the above construction, by computing a (k, ε/6)coreset Ri for Qi, whenever we compute Qi. The time to do this is dominated by the time to compute Qi. Clearly, ∪Ri is a (k, ε)-coreset for P at any point in time, and</p><formula xml:id="formula_20">|∪Ri| = O(kε -d log 2 n).</formula><p>Streaming k-means. In this case, the Qis are coresets for k-means clustering. Since Qi has a total weight equal to 2 i M (if it is not empty) and is generated as a (1+ρi) approximation, by Theorem C.2, |Qi| = O kε -d (i + 1) 2d (i + log M ) .</p><p>Thus the total storage requirement is O k log 2d+2 n /ε d . Specifically, a (k, ρj) approximation of a subset Pj of rank j is constructed after every 2 j M insertions, therefore using Theorem C.2 the amortized time spent for an update is</p><formula xml:id="formula_21">log (n/M ) i=0 1 2 i M O |Qi| log 2 |Pi| + k 5 log 9 |Pi| = log n M i=0 1 2 i M O k ε d i 2d (i + log M ) 2 + k 5 (i + log M ) 9 = O log 2 (k/ε) + k 5 .</formula><p>Further, we can generate an approximate k-means clustering from the (k, ε)-coresets, by using the algorithm of Theorem 6.1 on ∪iRi, with W = n. The resulting running time is O(k</p><formula xml:id="formula_22">5 log 9 n + k k+2 ε -(2d+1)k log k+1 n log k (1/ε)).</formula><p>Streaming k-medians. We use the algorithm of Lemma 5.1</p><p>for the coreset construction. Further we use Theorem 5.5 to compute an (1 + ε)-approximation to the k-median from the current coreset. The above discussion can be summarized as follows.</p><p>Theorem 7.2 Given a stream P of n points in R d and ε &gt; 0, one can maintain a (k, ε)-coresets for k-median and k-means efficiently and use the coresets to compute a (1 + ε)-approximate k-means/median for the stream seen so far.</p><p>The relevant complexities are:</p><p>• Space to store the information: O kε -d log 2d+2 n .</p><p>• Size and time to extract coreset of the current set:</p><formula xml:id="formula_23">O(kε -d log 2 n). • Amortized update time: O log 2 (k/ε) + k 5 . • Time to extract (1 + ε)-approximate k-means cluster- ing: O k 5 log 9 n + k k+2 ε -(2d+1)k log k+1 n log k (1/ε) . • Time to extract (1 + ε)-approximate k-median cluster- ing: O k log 7 n , where = exp [O ((1 + log 1/ε)/ε) d-1 ].</formula><p>Interestingly, once an optimization problem has a coreset, the coreset can be maintained under both insertions and deletions, using linear space. The following result follows in a plug and play fashion from [1, Theorem 5.1], and we omit the details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>In this paper, we showed the existence of small coresets for the k-means and k-median clustering. At this point, there are numerous problems for further research. In particular: 1. Can the running time of approximate k-means clustering be improved to be similar to the k-median bounds? Can one do FPTAS for k-median and k-means (in both k and 1/ε)? Currently, we can only compute the (k, ε)coreset in fully polynomial time, but not extracting the approximation itself from it. 2. Can the log n in the bound on the size of the coreset be removed? 3. Does a coreset exist for the problem of k-median and k-means in high dimensions? There are some partial relevant results <ref type="bibr">[9]</ref>. 4. Can one do efficiently (1 + ε)-approximate streaming for the discrete k-median case? 5. Recently, Piotr Indyk <ref type="bibr" target="#b23">[23]</ref> showed how to maintain a (1+ε)-approximation to k-median under insertion and deletions (the number of centers he is using is roughly </p><formula xml:id="formula_24">O(k log 2 ∆)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. WEBER-CENTROID SET CONSTRUCTION</head><p>Proof of <ref type="bibr">Lemma 5.3</ref> Let Copt be the optimal set of k medians. We snap each point of Copt to its nearest neighbor in D, and let X be the resulting set. Arguing as in the proof of Theorem 4.3, we have that νX (S) -νC opt (S) ≤ (ε/12)νC opt (S). On the other hand, from the definition of a coreset, we have that ν C opt (P ) -νC opt (S) ≤ (ε/12) νC opt (P ) and |νX (P ) -νX (S)| ≤ (ε/12) νX (P ). As such, νC opt (S) ≤ (1 + ε/12)νC opt (P ) and it follows νX (S) -νC opt (S) ≤ (ε/12) <ref type="bibr">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FUZZY NEAREST-NEIGHBOR SEARCH IN CONSTANT TIME</head><p>Let X be a set of m points in R d , such that we want to answer ε-approximate nearest neighbor queries on X. However, if the distance of the query point q to its nearest neighbor in X is smaller than δ, then it is legal to return any point of X in distance smaller than δ from q. Similarly, if a point is in distance larger than ∆ from any point of X, we can return any point of X. Namely, we want to do nearest neighbor search on X, when we care only for an accurate answer if the distance is in the range [δ, ∆]. Definition B.1 Given a point set X and parameters δ, ∆ and ε, a data structure D answers (δ, ∆, ε)-fuzzy nearest neighbor queries, if for an arbitrary query q, it returns a point x ∈ X such that 1. If d(q, X) &gt; ∆ then x is an arbitrary point of X.</p><p>2. If d(q, X) &lt; δ then x is an arbitrary point of X in distance smaller than δ from q.</p><p>3. Otherwise, qx ≤ (1 + ε)d(q, X).</p><p>In the following, let ρ = ∆/δ and assume that 1/ε = O(ρ). First, we construct a grid G∆ of size length ∆, using hashing and the floor function, we throw the points of X into their relevant cells in G∆. We construct a NN data structure for every non-empty cell in G∆. Given a query point q, we will compute its cell c in the grid G∆, and perform NN queries in the data-structure associated with c, and the data-structures associated with all its neighboring cells, returning the best candidate generated. This would imply O(3 d ) queries into the cell-level NN data-structure.</p><p>Consider Y to be the points of X stored in a cell c of G∆. We first filter Y so that there are no points in Y that are too close to each other. Namely, let G be the grid of side length δε/(10d). Again, map the points of Y into this grid G, in linear time. Next, scan over the nonempty cells of G, pick a representative point of Y from such a cell, and add it to the output point set Z. However, we do not add a representative point x to Z, if there is a neighboring cell to cx, which already has a representative point in Z, where cx is the cell in G containing x. Clearly, the resulting set Z ⊆ Y is well spaced, in the sense that there is no pair of points of Z that are in distance smaller than δε/(10d) from each other. As such, the result of a (δ, ∆, ε)-fuzzy NN query on Z is a valid answer for a equivalent fuzzy NN query done on Y , as can be easily verified. This filtering process can be implemented in linear time.</p><p>The point set Z has a bounded stretch; namely, the ratio between the diameter of Z and the distance of the closet pair is bounded by ∆/(δε/(10d)) = O(ρ 2 ). As such, we can use a data structure on Z for nearest neighbors on point set with bounded stretch <ref type="bibr" target="#b19">[19,</ref><ref type="bibr">Section 4.1]</ref>. This results in a quadtree T of depth O(log(ρ)) ≤ c log ρ, where c is constant. Answering NN queries, is now done by doing a point-location query in T , and finding the leaf of T that contains the query point q, as every leaf v in T store a point of Z which is a (1 + ε)-approximate nearest neighbor for all the points in cv, where cv is the region associated with v. The construction time of T is O(|Z| ε -d log ρ), and this also bound the size of T .</p><p>Doing the point-location query in T in the naive way, takes O(depth(T )) = O(log ρ) time. However, there is a standard technique to speed up the nearest neighbor query in this case to O(log depth(T )) <ref type="bibr" target="#b3">[3]</ref>. Indeed, observe that one can compute for every node in T a unique label, and furthermore given a query point q = (x, y) (we use a 2d example to simplify the exposition) and a depth i, we can compute in constant time the label of the node of the quadtree T of depth i that the point-location query for q would go through. To see that, consider the quadtree as being constructed on the unit square [0, 1] 2 , and observe that if we take the first i bits in the binary representation of x and y, denoted by xi and yi respectively, then the tuple (xi, yi, i) uniquely define the required node, and the tuple can be computed in constant time using bit manipulation operators.</p><p>As such, we hash all the nodes in T with their unique tuple id into a hash table. Given a query point q, we can now perform a binary search along the path of q in T , to find the node where this path "falls of" T . This takes O(log depth(T )) time.</p><p>One can do even better. Indeed, we remind the reader that the depth of T is c log ρ, where c is a constant. Let α = (log ρ)/(20dr) ≤ (log ρ)/(10dr), where r is an arbitrary integer parameter. If a leaf v in T is of depth u, we continue to split and refine it till all the resulting leaves of v lie in level α u/α in T . This would blow up the size of the quadtree by a factor of O((2 d ) α ) = O(ρ 1/r ). Furthermore, by the end of this process, the resulting quadtree has leaves only on levels with depth which is an integer multiple of α. In particular, there are only O(r) levels in the resulting quadtree T which contain leaves.</p><p>As such, one can apply the same hashing technique described above to T , but only for the levels that contains leaves. Now, since we do a binary search over O(r) possibilities, and every probe into the hash table takes constant time, it follows that a NN query takes O(log r) time.</p><p>We summarize the result in the following theorem.</p><p>Theorem B.2 Given a point set X with m points, and parameters δ, ∆ and ε &gt; 0, then one can preprocess X in O(mρ Proof. The idea is to quickly estimate τ , and then use Theorem B.2. To estimate τ , we use a similar algorithm to the closet-pair algorithm of Golin et al. <ref type="bibr" target="#b15">[15]</ref>. Indeed, randomly permute the points of P , let p1, . . . , pn be the points in permuted order, and let li be the current estimate of ri, where ri = max i j=1 d(pi, X) is the maximum distance between p1, . . . , pi and X. Let Gi be a grid of side length li, where all the cells contains points of X, or their neighbors are marked. For pi+1 we check if it contained inside one of the marked cells. If so, we do not update the current estimate, and set li+1 = li and Gi+1 = Gi. Otherwise, we scan the points of X, and we set li+1 = 2 √ dd(pi+1, X), and we recompute the grid Gi+1. It is easy to verify that ri+1 ≤ li+1 in such a case, and ri+1 ≤ 2 √ dli+1 if we do not rebuild the grid.</p><p>Thus, by the end of this process, we get ln, for which ln/(2 √ d) ≤ τ ≤ 2 √ dln, as required. As for the expected running time, note that if we rebuild the grid and compute d(pi+1, X) explicitly, this takes O(k) time. Clearly, if we rebuild the grid at stage i, and the next time at stage j &gt; i, it must be that ri ≤ li &lt; rj ≤ lj. However, in expectation, the number of different values in the series r1, r2, . . . , rn is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A (1+ε)-APPROXIMATION ALGORITHM FOR K-MEANS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Constant Factor Approximation</head><p>In this section we reduce the number of centers to be exactly k. We use the set of centers computed by Theorem 3.3 to compute a constant factor coreset using the algorithm of Theorem 4.4. The resulting coreset S, has size O(k log 4 n). Next we compute a O(n) approximation to the k-means for the coreset using the k-center (min-max) algorithm <ref type="bibr" target="#b16">[16]</ref>. Let C0 ⊆ S be the resulting set of centers. Next we apply the local search algorithm, due to Kanungo et al. <ref type="bibr" target="#b25">[25]</ref>, to C0 and S, where the set of candidate points is S. This local search algorithm, at every stage, picks a center c from the current set of centers Ccurr, and a candidate center s ∈ S, and swaps c out of the set of centers and c into the set of centers. Next, if the new set of centers C curr = Ccurr \{c}∪{s}</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Table 1 :</head><label>1</label><figDesc>For our results, all the running time bounds are in expectation and the algorithms succeed with high probability. (*) Getting this running time requires non-trivial modifications of the algorithm of Kolliopoulos and Rao. (**) The O k notation hides constants that depends solely on k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 3 . 3 (</head><label>33</label><figDesc>Clustering with more centers) Given a set P of n points in R d , and parameter k, one can compute a set X, of size O(k log 3 n), such that µX (P ) ≤ 32µopt(P, k). The running time of the algorithm is O(n) if k = O(n 1/4 ), and O(n log (k log n)) otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 4 . 2</head><label>42</label><figDesc>The weighted set S is a (k, ε)-coreset for P and |S| = O |A|ε -d log n . Proof. Let Y be an arbitrary set of k points in R d . For any p ∈ P , let p denote the image of p in S. The error is E = |νY (P ) -νY (S)| ≤ p∈P |d(p, Y )d(p , Y )|. Observe that d(p, Y ) ≤ pp + d(p , Y ) and d(p , Y ) ≤ pp + d(p, Y ) by the triangle inequality. Implying that |d(p, Y )d(p , Y )| ≤ pp . It follows that ) ≤ ενopt (P, k) , since pp ≤ ε 10c d(p, A) if d(p, A) ≥ R, and pp ≤ ε 10c R, if d(p, A) ≤ R, by the construction of the grid. This implies |νY (P ) -νY (S)| ≤ ενY (P ), since νopt(P, k) ≤ νY (P ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 5 . 1 (</head><label>51</label><figDesc>coreset) Given a set P of n points in R d , one can compute a k-median (k, ε)-coreset S of P , of size O (k/ε d ) log n , in time O n + k 5 log 9 n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 5 . 4 (</head><label>54</label><figDesc>[26]) Given a weighted point set P with n points in R d , with total weight W , a centroid set D of size at most n, and a parameter δ &gt; 0, one can compute (1 + ε)approximate k-median clustering of P using only centers from D. The running time is O ( n(log k)(log W ) log(1/δ)), where = exp [O ((1 + log 1/ε)/ε) d-1 ]. The algorithm succeeds with probability ≥ 1δ. The final algorithm is the following: Using the algorithms of Lemma 5.1 and Lemma 5.3 we generate a (k, ε)-coreset S and an ε-centroid set D of P , where |S| = O(kε -d log n) and |D| = O(k 2 ε -2d log 2 n). Next, we apply the algorithm of Theorem 5.4 on S and D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 7 . 3</head><label>73</label><figDesc>Given a point set P in R d , one can maintain a (k, ε)-coreset of P for k-median/means, using linear space, and in time O(kε -d log d+2 n log k log n ε + k 5 log 10 n) per insertion/deletions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>n i=1 1 Corollary B. 4</head><label>14</label><figDesc>/i = O(log n). Thus, the expected running time of this algorithm is O(n + k log n), as checking whether a point is in a marked cell, takes O(1) time by using hashing.We know that ln/(2√ d) ≤ τ ≤ 2 √ dln. Set δ = ln/(4d 2 n 5 ), ∆ = 2√ dln and build the (δ, ∆, ε)-fuzzy nearest neighbor data-structure of Theorem B.2 for X. We can now answer the nearest neighbor queries for the points of P in O(1) per query. Given a point set X of size m, a point set P of size n both in R d , and a parameter D, one can compute in O(n + mn 1/10 ε -d log(n/ε)) time, for every point p ∈ P a a point xp ∈ P , such that: • If d(p, X) &gt; D then xp is an arbitrary point in X. • If d(p, X) ≤ D then pxp ≤ (1 + ε)d(p, X) + D/n 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Definition 2.1</head><label></label><figDesc>For a point set X, and a point p, both in R d , let d(p, X) = minx∈X xp denote the distance of p from X.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Definition 2.2 (Clustering) For</head><label></label><figDesc>a weighted point set P with points from R d , with an associated weight function w : P → Z + and any point set C, we define νC (P ) = p∈P w(p)d(p, C) as the cost of the k-median clustering provided by C. Further let νopt(P, k) =min C⊆R d ,|C|=k νC (P ) denote the cost of the optimal k-median clustering for P .</figDesc><table><row><cell>Similarly, let µC (P ) =</cell><cell>p∈P w(p) (d(p, C)) 2 denote the</cell></row><row><cell cols="2">cost of the k-means clustering of P as provided by the set</cell></row><row><cell cols="2">of centers C. Let µopt(P, k) = min C⊆R d ,|C|=k µC (P ) denote</cell></row><row><cell cols="2">the cost of the optimal k-means clustering of P .</cell></row><row><cell cols="2">We refer to R ν opt (P, k) = νopt(P, k)/|P | as the average</cell></row><row><cell cols="2">radius of P . And to R µ opt (P, k) = µopt(P, k)/ |P | as the</cell></row><row><cell>average means radius of P .</cell><cell></cell></row><row><cell cols="2">Remark 2.3 We only consider positive integer weights. A</cell></row><row><cell cols="2">regular point set P may be considered as a weighted set with</cell></row><row><cell cols="2">weight 1 for each point, and total weight |P |.</cell></row><row><cell cols="2">Definition 2.4 (Discrete Clustering) In several cases, it</cell></row><row><cell cols="2">is convenient to consider the centers to be restricted to lie in</cell></row><row><cell cols="2">the original point set. Let ν D opt (P, k) = min C⊆P,|C|=k νC (P )</cell></row><row><cell cols="2">denote the cost of the optimal discrete k-median clustering</cell></row><row><cell cols="2">for P and let µ D opt (P, k) = min C⊆P,|C|=k µC (P ) denote the</cell></row><row><cell cols="2">cost of the optimal discrete k-means clustering of P .</cell></row><row><cell cols="2">Observation 2.5 For any point set P , the following hold</cell></row><row><cell cols="2">(i) µopt(P, k) ≤ µ D opt (P, k) ≤ 4µopt(P, k) and (ii) νopt(P, k) ≤</cell></row><row><cell>ν D opt (P, k) ≤ 2νopt(P, k).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Lemma 3.2 Given a set P of n points in R d , and parameter k, one can compute sets P and X ⊆ P such that, with high probability, |P | ≥ n/2, |X| = O(k log 2 n), and µC opt (P ) ≥ µX (P )/32, where Copt is the optimal set of k-means centers for P . The running time of the algorithm is O</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>where ∆ is the spread of the point set).</figDesc><table><row><cell>It would be interesting to see if one can extend our</cell></row><row><cell>techniques to maintain coresets also under deletions.</cell></row><row><cell>It is clear that there is a linear lower bound on the</cell></row><row><cell>amount of space needed, if one assume nothing. As</cell></row><row><cell>such, it would be interesting to figure out what are</cell></row><row><cell>the minimal assumptions for which one can maintain</cell></row><row><cell>(k, ε)-coreset under insertions and deletions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1 + ε/12)νC opt (P ) ≤ (ε/6)νC opt (P ).</figDesc><table><row><cell>As such,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ν X (P ) ≤</cell><cell cols="2">1 1 -ε/12</cell><cell cols="5">νX (S) ≤ 2νX (S)</cell></row><row><cell cols="5">≤ 2 νC opt (S) +</cell><cell>ε 6</cell><cell cols="2">νC opt (P )</cell></row><row><cell cols="2">≤ 2 1 +</cell><cell cols="2">ε 12</cell><cell cols="4">νC opt (P ) +</cell><cell>ε 6</cell><cell>νC opt (P )</cell></row><row><cell cols="4">≤ 3νC opt (P ),</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">for ε &lt; 1. Thus we have |νX (P ) -νX (S)| ≤ (ε/12)νX (P ) ≤</cell></row><row><cell cols="8">(ε/3)νC opt (P ). Putting things together, we have</cell></row><row><cell cols="8">νX (P ) -νC opt (P ) ≤ |νX (P ) -νX (S)|</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">+ ν X (S) -νC opt (S)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">+ νC opt (S) -νC opt (P )</cell></row><row><cell></cell><cell></cell><cell></cell><cell>≤</cell><cell></cell><cell>ε 3</cell><cell>+</cell><cell>ε 6</cell><cell>+</cell><cell>ε 12</cell><cell>νC opt (P )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">≤ ενC opt (P ).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Given a point set X of size m, and a point set P of size n both in R d , one can compute in O(n + mn 1/4 ε -d log(n/ε)) time, for every point p ∈ P , a point xp ∈ X, such that pxp ≤ (1 + ε)d(p, X) + τ /n 3 , where τ = maxp∈P d(p, X).</figDesc><table /><note><p><p><p>1/r ε -d log(ρ/ε)) time, so that one can answer (δ, ∆, ε)fuzzy nearest neighbor queries on X in O(log r) time. Here ρ = ∆/δ and r is an arbitrary integer number fixed in advance.</p>Theorem B.</p><ref type="bibr" target="#b3">3</ref> </p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Piotr Indyk and Satish Rao for useful discussions of problems studied in this paper and related problems.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Work on this paper was partially supported by a NSF CA-REER award CCR-0132901.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>provides a considerable improvement over the previous solution (i.e., µC curr (S) ≤ (1ε/k)µ C curr (S) where ε here is an arbitrary small constant), then we set Ccurr to be C curr . Extending the analysis of Arya et al. <ref type="bibr" target="#b7">[7]</ref>, for the k-means algorithm, Kanungo et al. <ref type="bibr" target="#b25">[25]</ref> showed that the algorithm terminates, and it provides a constant factor approximation to µ D opt (S, k), and as hence to µopt(P, k </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 The (1 + ε)-Approximation</head><p>Combining Theorem C.1 and Theorem 4.4, we get the following result for coresets.</p><p>Proof. We first compute a set A which provides a constant factor approximation to the optimal k-means clustering of P , using Theorem C.1. Next, we feed A into the algorithm Theorem 4.4, and get a (1 + ε)-coreset for P , of size O((k/ε d ) log W ).</p><p>We now use techniques from Matoušek <ref type="bibr" target="#b28">[28]</ref> to compute the (1 + ε)-approximate k-means clustering on the coreset.</p><p>Definition C.3 (Centroid Set) Given a set P of n points in R d , a set T ⊆ R d is an ε-approximate centroid set for P , if there exists a subset C ⊆ T of size k, such that µC (P ) ≤ (1 + ε)µopt(P, k).</p><p>Matoušek showed that there exists an ε-approximate centroid set of size O(nε -d log(1/ε)). Interestingly enough, his construction is weight insensitive. In particular, using an (k, ε/2)-coreset S in his construction, results in a ε-approximate centroid set of size O |S| ε -d log(1/ε) . ε time a εapproximate centroid set U for S, using the algorithm from <ref type="bibr" target="#b28">[28]</ref>. We have |U | = O(kε -2d log W log (1/ε)). Next we enumerate all k-tuples in U , and compute the k-means clustering cost of each candidate center set (using S). This takes</p><p>And clearly, the best tuple provides the required approximation.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating extent measures of points</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Comput. Mach</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximation algorithms for k-line center</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu. European Sympos. Algorithms</title>
		<meeting>10th Annu. European Sympos. Algorithms</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient algorithms and regular data structures for dilation, location and proximity problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annu</title>
		<meeting>40th Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Polynomial time approximation schemes for euclidean tsp and other geometric problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="753" to="782" />
			<date type="published" when="1998-09">Sep 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation schemes for Euclidean k-median and related problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Annu</title>
		<meeting>30th Annu</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="106" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching fixed dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local search heuristic for k-median and facility location problems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Munagala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Annu</title>
		<meeting>33rd Annu</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal core-sets for balls</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bȃdoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM-SIAM Sympos. Discrete Algorithms</title>
		<meeting>14th ACM-SIAM Sympos. Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="801" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximate clustering via core-sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bȃdoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Annu</title>
		<meeting>34th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decomposable searching problems i: Static-to-dynamic transformation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="358" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved combinatorial algorithms for the facility location and k-median problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annu</title>
		<meeting>40th Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A constant-factor approximation algorithm for the k-median problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annu</title>
		<meeting>31st Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximation schemes for clustering problems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>De La Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Annu</title>
		<meeting>35th Annu</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal algorithms for approximate clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Annu</title>
		<meeting>20th Annu</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple randomized algorithms for closest pair problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Golin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nordic J. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustering to minimize the maximum intercluster distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoret. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41th Annu</title>
		<meeting>41th Annu</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustering motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Annu</title>
		<meeting>42nd Annu</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A replacement for Voronoi diagrams of near linear size</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Annu</title>
		<meeting>42nd Annu</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Projective clustering in high dimensions using core-sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Annu</title>
		<meeting>18th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="312" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applications of weighted voronoi diagrams and randomization to variance-based k-clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu</title>
		<meeting>10th Annu</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sublinear time algorithms for metric space problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Annu</title>
		<meeting>31st Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="154" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">+ ε)-approximate bi-criterion algorithm for k-median on a stream</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Annu</title>
		<meeting>36th Annu</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Primal-dual approximation algorithms for metric facility location and k-median problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annu</title>
		<meeting>40th Annu</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A local search approximation algorithm for k-means clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Annu</title>
		<meeting>18th Annu</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A nearly linear-time approximation scheme for the euclidean κ-median problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kolliopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Annu. European Sympos. Algorithms</title>
		<meeting>7th Annu. European Sympos. Algorithms</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="378" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Better streaming algorithms for clustering problems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Annu</title>
		<meeting>35th Annu</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On approximate geometric k-clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matoušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="61" to="84" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal time bounds for approximate clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Uncertainty in Artifical Intelligence</title>
		<meeting>the 18th Conference on Uncertainty in Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="344" to="351" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
