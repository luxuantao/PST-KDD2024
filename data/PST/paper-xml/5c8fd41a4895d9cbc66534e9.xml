<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Objects Segmentation From High-Resolution Aerial Images Using U-Net With Pyramid Pooling Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><forename type="middle">Hee</forename><surname>Kim</surname></persName>
							<idno type="ORCID">0000-0001-7141-9288</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Communication Engineering</orgName>
								<orgName type="institution">Daegu Gyeongbuk Institute of Science and Technology</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><forename type="middle">Youn</forename><surname>Hwang</surname></persName>
							<email>jyhwang@dgist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Dabeeo Inc</orgName>
								<address>
									<postCode>04045</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jihwan</forename><forename type="middle">P</forename><surname>Choi</surname></persName>
							<email>jhchoi@dgist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Dabeeo Inc</orgName>
								<address>
									<postCode>04045</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Objects Segmentation From High-Resolution Aerial Images Using U-Net With Pyramid Pooling Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LGRS.2018.2868880</idno>
					<note type="submission">received June 24, 2018; revised July 25, 2018; accepted August 28, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial images</term>
					<term>convolutional neural networks (CNNs)</term>
					<term>object segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting manufactured features such as buildings, roads, and water from aerial images is critical for urban planning, traffic management, and industrial development. Recently, convolutional neural networks (CNNs) have become a popular strategy to capture contextual features automatically. In order to train CNNs, a large training data are required, but it is not straightforward to use free-accessible data sets due to imperfect labeling. To address this issue, we make a large scale of data sets using RGB aerial images and convert them to digital maps with location information such as roads, buildings, and water from the metropolitan area of Seoul in South Korea. The numbers of training and test data are 72 400 and 9600, respectively. Based on our self-made data sets, we design a multiobject segmentation system and propose an algorithm that utilizes pyramid pooling layers (PPLs) to improve U-Net. Test results indicate that U-Net with PPLs, called UNetPPL, learn finegrained classification maps and outperforms other algorithms of fully convolutional network and U-Net, achieving the mean intersection of union (mIOU) of 79.52 and the pixel accuracy of 87.61% for four types of objects (i.e., building, road, water, and background).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A ERIAL images can provide valuable information on areas that are difficult for people to access or access nonintrusively <ref type="bibr" target="#b0">[1]</ref>. The information obtained using aerial images is used in a variety of industries including land inventory, vegetation monitoring, and environmental assessment <ref type="bibr" target="#b1">[2]</ref>. In particular, extraction of manufactured features such as buildings, roads, railway lines, etc., or natural ones from aerial images is important in many applications that depend on geographic information systems (GISs). GISs are used to carry out tasks such as urban planning, traffic management, industrial development, and cartography as well as for emergency planning systems for evacuation and fire response <ref type="bibr" target="#b2">[3]</ref>.</p><p>The feature extraction method for creating or modifying a GIS is mainly to perform a visual analysis and manually digitize aerial images. It is still the main method to generate geospatial data but takes much labor and time to extract or identify features manually <ref type="bibr" target="#b3">[4]</ref>. With the development of the optical sensor technology, it is possible to obtain higher resolution images, and more accurate information can be therefore obtained from their images. Nevertheless, it is difficult yet to generate all the geospatial data by the abovementioned method.</p><p>There are two main types of research for detecting objects from aerial images: semiautomatic and fully automatic <ref type="bibr" target="#b4">[5]</ref>. However, because the semiautomatic methods require prior knowledge of the extraction process, such as identifying areas of the road and buildings through human intervention, the automatic methods, which are modeled on the analysis and interpretation of a human operator, to segment objects from aerial images is widely used in segmenting roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b6">[7]</ref>. Also, traditional machine learning techniques such as support vector machine and artificial neural networks have been studied in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. The automatic methods are objective without human intervention, but their results have generally been discouraging due to various adverse factors (image noise, shadows, etc.) <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recently, as the deep learning technology has evolved, it has been applied to many problems related to computer vision fields such as image classification, object segmentation, and semantic segmentation. In addition, deep learning techniques such as convolutional neural networks (CNNs) have attracted much attention to segment objects in satellite images <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To segment roads and buildings from satellite images, several semantic segmentation models based on deep learning techniques such as fully convolutional networks (FCNs) <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref>, and U-Net <ref type="bibr" target="#b17">[18]</ref> algorithms have been used.</p><p>Although it is possible for deep learning algorithms to learn contextual features automatically, there are several issues. First, it requires many data to train them. Existing open data sets are imperfect and not enough to train such deep learning algorithms, though there are the large-scale data sets for classification <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Second, due to the lack of multiclass data sets, binary data have been used in previous 1545-598X Â© 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>research <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To address these problems, we make a large scale of the data set (i.e., the numbers of training and test data are 72 400 and 9600, respectively), including multiple objects (e.g., building, road, water, and background). We also design a new semantic segmentation model, exploiting multiscale features by using the pyramid pooling layer (PPL) <ref type="bibr" target="#b20">[21]</ref> to extract information from various classes. The proposed model with our self-made data set outperforms other algorithms, as shown in the test results.</p><p>The main contributions of this paper are summarized as follows.</p><p>1) We integrated U-Net architecture with an eight-level PPL module to solve a problem that U-Net is not able to clearly distinguish the gap between buildings as well as to achieve the better shape prediction of the buildings mainly because the PPL provides a capability of global context aggregation. 2) We developed object segmentation systems by improving architecture and compared performances with other algorithms. The proposed systems show the performance of the pixel accuracy of 87.61% and the mean intersection of the union (mIOU) of 79.52 for the 9600 test images. 3) We made a large scale of data sets for multiple object segmentation from aerial images, including many complex obstacles in the metropolitan area of Seoul, South Korea. The rest of this paper is organized as follows. In Section II, the related work for deep learning-based objects segmentation is discussed. In Section III, the proposed object segmentation systems are discussed. In Sections IV and V, our data set and test results are described. Finally, we conclude our contributions in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP LEARNING-BASED OBJECT SEGMENTATION SYSTEMS</head><p>There are several algorithms that are applied to object segmentation based on CNNs <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Because the performance of deep learning algorithms depends on their structures, it should be optimized to improve the performance by adjusting and fine-tuning. In this section, CNN-based segmentation algorithms are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fully Convolutional Networks</head><p>An FCN is a modified CNN to semantically segment images. The FCN predicts every pixel of images, and this can be trained end-to-end and pixel-to-pixel. A key idea of FCN is changing of the CNN model from classification to dense prediction by reinterpretation of fully connected layers of the classifier as a fully convolution layer <ref type="bibr" target="#b24">[25]</ref>. The FCN consists of an encoder of input images and a decoder that upsamples encoded images by their original image size. The encoder part of FCN consists of visual geometry group network (VGGNet) <ref type="bibr" target="#b25">[26]</ref> that is a famous CNN classification model and the decoder part consists of a deconvolution layer for upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. U-Net</head><p>U-Net <ref type="bibr" target="#b17">[18]</ref> is a modified FCN for yielding more precise segmentation. U-Net has two different architectures compared to FCN. First, U-Net uses high-resolution features to increase localization accuracy by combining the decoding layer and the encoding layer. Second, while FCN uses 1 Ã 1 convolution layer at the last layer of the encoder to use the pretrained model, U-Net does not use 1 Ã 1 convolution at the encoding layer. Thus, U-Net shows an outstanding performance not only for processing biomedical images but also for segmenting objects from satellite imagery <ref type="bibr" target="#b26">[27]</ref>. In this paper, we also exploit the U-Net architecture for multiclass object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. U-NET WITH PYRAMID POOLING LAYERS</head><p>Feature maps in different subregions generated by PPLs significantly enhanced the segmentation of various classes. Thus, we redesigned the U-Net architecture by exploiting PPL. In the PPL, the output of each pyramid level is combined and upsampled to the same resolution as the input via transposed convolution. Finally, the per-pixel prediction is presented.</p><p>The proposed algorithm consists of the encoder-decoder structure. In the encoder part, spatial dimension is reduced up to 14 Ã 14 by convolution filters (3 Ã 3, stride 1) and the max pooling layer. The numbers of the filter channels are increased. In this process, we utilize batch normalization <ref type="bibr" target="#b27">[28]</ref> to prevent the overfitting problem as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. At the end of the encoder, we use PPL, consisting of eight layers. Because the pooling size is different <ref type="bibr">(1, 3, 8, 9, 11, 12, 13, and 14)</ref>, it is possible to extract features from several ranges. In the decoder part, to recover spatial dimension, upconvolution (or the transposed convolution) is used.</p><p>Although using consecutive pooling layers allows reducing the parameters and extract long-range information, it can lose local information such as boundaries of objects. To overcome this problem, skip connection methods are used <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The features extracted from PPL are upsampled and concatenated with the output of the 14th rectified linear unit (ReLU). By transposed convolution, the spatial dimension of combined features is recovered from 14 Ã 14 to 28 Ã 28. As it combines the output of the 11th, 8th, 5th, and 2nd ReLU and the transposed convolution is applied, the spatial dimension is recovered gradually, and the number of filters is regulated by convolution layers. Finally, the pixelwise prediction is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATA SET AND EXPERIMENTS</head><p>Most works to segment objects such as roads and buildings have been carried out using aerial images in rural areas <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. In addition, they have used many data from the OpenStreetMap (OSM) <ref type="bibr" target="#b28">[29]</ref>. However, the OSM has several defects (e.g., buildings match in a different direction or some regions are not mapped) since the OSM is a free editable mapping platform, which might be easily contaminated by misinformation added by anonymous editors. The defects of OSM data might disturb the training for CNN <ref type="bibr" target="#b29">[30]</ref>. To overcome the aforementioned disadvantages, a fine-tuning scheme is proposed in <ref type="bibr" target="#b12">[13]</ref>, showing that the fine-tuned network outperforms the untuned one. After they trained CNN by using a large scale of raw OSM data for binary classification, a tiny piece of the manually labeled data were used to tune convolutional filters. On the other hand, we created a large scale of accurately labeled data sets for multiclass object segmentation and performed experiments on the self-made data sets that were based on RGB aerial images over the area of Seoul, Suwon, Anyang, Gwacheon, and Goyang, which are among the most complex cities in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set</head><p>We perform our experiments with images of all the areas of the five cities mentioned above. The ground-truth data were obtained by changing vector data provided by the government agency of National Geographic Information Institute to images using Quantum GIS, a free and open-source GIS application. These data are more accurate than the ones from the OSM because they have been made by experts for many years. As changing a data format of the ground-truth data, the data are labeled into four classes: background, building, road, and water. Our data set, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, consists of pairs of RGB images with 0.51-m spatial resolution and ground-truth images. The data set covers an area of 551km 2 and is randomly divided into an area of 486.5km 2 for training and 64.5km 2 for testing. All the data were divided into multiple images with the pixel size of 224 Ã 224, of which 72 400 images were assigned to the training set and 9600 to the test set.</p><p>Our data sets have two advantages: first, the data sets consisted of images of the most complex cities. For applications, here, we highlight that the proposed method performs well in complex urban areas as well as in rural areas, for which our data sets are appropriate. Second, our data sets were labeled as multiple classes. Many problems have been investigated in binary classification, mainly road classification, and building segmentation due to the lack of multiclass data sets <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Moreover, classifying several classes at once can solve the problem of two or more classes being overlapped at one point when superposing several binary classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Setup</head><p>The average pyramid levels were applied to the end of feature extract and the ReLU are used as the activation functions for the hidden layers. We here used the crossentropy loss between prediction results and the ground truth for training the proposed model. The Adam optimizer <ref type="bibr" target="#b30">[31]</ref> with the learning rate Î± = 10 â4 , the exponential decay rates for the moment of estimates Î² 1 = 0.9 and Î² 2 = 0.999 and the constant value Îµ = 10 â8 were used to train our deep neural network. The FCN with PPL were trained and evaluated by the self-made data sets containing four objects classes (i.e., building, water, road, and background). The numbers of data were 72 400 and 9600 for training and testing, respectively. The mIOU and pixel accuracy were utilized as performance indices. The experiments were implemented using the public platform Tensorflow <ref type="bibr" target="#b31">[32]</ref> and run on an Intel core 6 i7-7820X CPU at 3.6 GHz with 2 GPUs of Nvidia Geforce Titan XP (12 GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head><p>In this section, we analyze the results of training for U-Net with PPL (UNetPPL) in detail. Fig. <ref type="figure" target="#fig_2">3</ref>, including input images and the ground truth, illustrates the results of training for each model. Note that each object is colored differently (e.g., green pixels designate buildings, blue designates background, orange designates roads, and brown designates water). To compare with each of deep learning architectures quantitatively, we compute mIOU, IOU per object and the average pixel accuracy of all objects as the results are summarized in TABLE I. Although the IOU of FCN-PPL is lower than that of FCN in a building class, the number of false positive pixels in FCN-PPL (â20 M pixels) is much lower than FCN (â23 M pixels). Also, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>, FCN-PPL distinguishes the buildings better and, moreover, represent the better-shaped building than FCN. Similarly, in FCN-PPL, the number of false positive pixels in UNetPPL (â19 M pixels) is much lower than that in U-Net (â23 M pixels). The main reason would be that the use of an eight-level pyramid pooling module allows to obtaining the effective global context information <ref type="bibr" target="#b20">[21]</ref> and, therefore, produces a high-quality result on object segmentation.</p><p>In addition to FCN, FCN-PPL, U-Net, and UNetPPL, we also conducted experiments on pyramid scene parsing network (PSPNet), which has proposed the pyramid pooling module. For most evaluation criteria, the proposed method provides higher scores than those of the others. It can achieve mIOU to 79.52, which is 0.50 higher than mIOU of U-Net. It is important to note that the compared U-Net is not the original architecture proposed in <ref type="bibr" target="#b17">[18]</ref> but a highly calibrated model for the enhanced capability of object segmentation from satellite images <ref type="bibr" target="#b26">[27]</ref>. Therefore, even a small gain of mIOU with UNetPPL is remarkable. In particular, UNetPPL outperforms other methods of segmenting roads and buildings, which are the most typical human-made objects as illustrated last example in Fig. <ref type="figure" target="#fig_2">3</ref>. Compared with the worst performance of FCN, UNetPPL shows the IOU improvement of 2.92 and 1.97 for building and roads, respectively.</p><p>We additionally demonstrated through these experiments that our model shows better performance in aerial photographs than PSPNet, which had suggested the pyramid pooling module. General convolution is superior to dilated convolution used in PSPNet for feature extraction in aerial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we modeled the enhanced semantic segmentation model capable of better segmenting multiclass objects from aerial images by exploiting a state-of-the-art CNN-based algorithm. Different from previous research that has utilized a lot of raw OSM data and a tiny piece of the manually labeled data set, we made a large scale of an accurately labeled data set, including major cities in South Korea. Our data set, based on the city centers, includes many complex obstacles, and thus, may be more practical than the binary labeled data in previous work.</p><p>Our proposed UNetPPL utilized PPLs and skip connections widely to extract multiscale features of objects. It showed better performance than other deep learning models in the segmentation of multiclass objects from aerial images. Test results showed that UNetPPL outperforms FCN, FCNPPL, U-Net, and PSPNet regarding pixel accuracy and mIOU for four classes (building, road, water, and background). Moreover, UNetPPL has a capability of classifying objects in detail (e.g., the interval of buildings), which is significantly advantageous in the complex environments.</p><p>Our CNN-based multiobject segmentation with a large scale of data set made to train algorithms only focused on the urban areas of South Korea. Data attributes of objects can be different from one area to another. Therefore, to enhance generalization of UNetPPL, supplementing other country data and the corresponding training should be required, which will be as addressed in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. U-Net-based object segmentation system. Note that Conv is the convolution layer, Upconv is the transposed convolution, and BN is the batch normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of our data set. (a) Image. Ground truth. In (b), green pixels designate buildings, blue designates background, orange designates roads, and brown designates water. South Korea. Our data sets are divided into training and test data sets, each of which is used for training and evaluating the proposed model in this paper, respectively. Note that contrasted from the binary labeled data in previous work, ours are labeled for multiple object classes.</figDesc><graphic url="image-3.png" coords="3,345.95,153.17,85.70,85.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results on the test set for each model. (a) Image. (b) Ground Truth. (c) FCN. (d) U-Net. (e) PSPNet. (f) FCNPPL. (g) UNetPPL.</figDesc><graphic url="image-13.png" coords="4,211.31,262.01,60.38,60.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>RESULTS FOR 9600 DATA</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the ICT Research and Development Program of MSIT/IITP through the Geo-Data Generation and Applicable Service Development Based on Satellite Imagery Data Conversion Platform under Grant 2018-0-01573, in part by the Samsung Research Funding Center of Samsung Electronics under Project SRFC-IT1502-10, and in part by the Ministry of Trade Industry and Energy, Ministry of Science and ICT, and Ministry of Health and Welfare Technology through the Development Program for AI-Bio-Robot-Medicine Convergence under Grant 20001234. (Jun Hee Kim and Haeyun Lee contributed equally to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic road extraction from high resolution satellite image using adaptive global thresholding and morphological operations</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Indian Soc. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="640" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring phenological variability from satellite imagery</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vanderzee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Ohlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vegetation Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="703" to="714" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic building detection from high-resolution satellite images based on morphology and internal gray variance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Kushwaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1767" to="1779" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object recognition and feature extraction from imagery: The feature analyst approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Opitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogram., Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">C42</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated extraction of road network from medium-and high-resolution images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P D</forename><surname>Poz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Zanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M D</forename><surname>Vale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate centerline detection and line width estimation of thick lines using the radon transform</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Couloigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="316" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic building detection in aerial images using a hierarchical feature based image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Conf. Pattern Recognit</title>
				<meeting>IEEE 20th Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="472" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Road centreline extraction from highresolution imagery based on multiscale structural features and support vector machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Road detection from highresolution satellite images using artificial neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mokhtarzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J V</forename><surname>Zoej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Observ. Geoinf</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object-space road extraction in rural areas using stereoscopic aerial images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dal Poz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A B</forename><surname>Gallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F C</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã</forename><forename type="middle">F O</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="654" to="658" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>LefÃ¨vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for large-scale remote-sensing image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Road structure refined CNN for road extraction in aerial image</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="709" to="713" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural networks to correct satellite image classification maps</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4962" to="4971" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building detection in very high resolution multispectral data with deep learning features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IGARSS</title>
				<meeting>IEEE IGARSS</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="1873" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int</title>
				<meeting>Int</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="3794" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogram. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.01.021</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Int. Soc. Photogramm. Remote Sens</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
				<meeting>Int. Conf. Learn. Represent. (ICLR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep Learning for Satellite Imagery Via Image Segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nowaczynski</surname></persName>
		</author>
		<ptr target="https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OpenStreetMap: User-generated street maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haklay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12" to="18" />
			<date type="published" when="2008-12">Oct./Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of site features, user attributes, and information verification behaviors on the perceived credibility of Web-based information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Flanagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Media Soc</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="342" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
				<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
