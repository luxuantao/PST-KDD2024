<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convergence of Adversarial Training in Overparametrized Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haochuan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Perception</orgName>
								<orgName type="department" key="dep2">MOE</orgName>
								<orgName type="department" key="dep3">School of EECS</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convergence of Adversarial Training in Overparametrized Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">410292A10CF31A249755BE18D673A3F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network.</p><p>Adversarial training <ref type="bibr" target="#b30">[31]</ref>, a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the ∞ -norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent studies have demonstrated that neural network models, despite achieving human-level performance on many important tasks, are not robust to adversarial examples-a small and human imperceptible input perturbation can easily change the prediction label <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>. This phenomenon brings out security concerns when deploying neural network models to real world systems <ref type="bibr" target="#b19">[20]</ref>. In the past few years, many defense algorithms have been developed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> to improve the network's robustness, but most of them are still vulnerable under stronger attacks, as reported in <ref type="bibr" target="#b2">[3]</ref>. Among current defense methods, adversarial training <ref type="bibr" target="#b30">[31]</ref> has become one of the most successful methods to train robust neural networks.</p><p>To obtain a robust network, we need to consider the "robust loss" instead of a regular loss. The robust loss is defined as the maximal loss within a neighborhood around the input of each sample, and minimizing the robust loss under empirical distribution leads to a min-max optimization problem.</p><p>Adversarial training <ref type="bibr" target="#b30">[31]</ref> is a way to minimize the robust loss. At each iteration, it (approximately) solves the inner maximization problem by an attack algorithm A to get an adversarial sample, and then runs a (stochastic) gradient-descent update to minimize the loss on the adversarial samples. Although adversarial training has been widely used in practice and hugely improves the robustness of neural networks in many applications, its convergence properties are still unknown. It is unclear whether a network with small robust error exists and whether adversarial training is able to converge to a solution with minimal adversarial train loss.</p><p>In this paper, we study the convergence of adversarial training algorithms and try to answer the above questions on over-parameterized neural networks. We consider width-m neural networks both for the setting of deep networks with H layers, and two-layer networks for some additional analysis. Our contributions are summarized below.</p><p>• For an H-layer deep network with ReLU activations, and an arbitrary attack algorithm, when the width m is large enough, we show that projected gradient descent converges to a network where the surrogate loss with respect to the attack A is within of the optimal robust loss (Theorem 4.1). The required width is polynomial in the depth and the input dimension.</p><p>• For a two-layer network with smooth activations, we provide a proof of convergence, where the projection step is not required in the algorithm (Theorem 5.1).</p><p>• We then consider the expressivity of neural networks w.r.t. robust loss (or robust interpolation). We show when the width m is sufficiently large, the neural network can achieve optimal robust loss ; see Theorems 5.2 and C.1 for the precise statement. By combining the expressivity result and the previous bound of the loss over the optimal robust loss, we show that adversarial training finds networks of small robust training loss (Corollary 5.1 and Corollary C.1).</p><p>• We show that the VC-Dimension of the model class which can robustly interpolate any n samples is lower bounded by Ω(nd) where d is the dimension. In contrast, there are neural net architectures that can interpolate n samples with only O(n) parameters and VC-Dimension at most O(n log n). Therefore, the capacity required for robust learning is higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Attack and Defense Adversarial examples are inputs that are slightly perturbed from a natural sample and yet incorrectly classified by the model. An adversarial example can be generated by maximizing the loss function within an -ball around a natural sample. Thus, generating adversarial examples can be viewed as solving a constrained optimization problem and can be (approximately) solved by a projected gradient descent (PGD) method <ref type="bibr" target="#b30">[31]</ref>. Some other techniques have also been proposed in the literature including L-BFGS <ref type="bibr" target="#b43">[44]</ref>, FGSM <ref type="bibr" target="#b21">[22]</ref>, iterative FGSM <ref type="bibr" target="#b25">[26]</ref> and C&amp;W attack <ref type="bibr" target="#b11">[12]</ref>, where they differ from each other by the distance measurements, loss function or optimization algorithms. There are also studies on adversarial attacks with limited information about the target model. For instance, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref> considered the black-box setting where the model is hidden but the attacker can make queries and get the corresponding outputs of the model.</p><p>Improving the robustness of neural networks against adversarial attacks, also known as defense, has been recognized as an important and unsolved problem in machine learning. Various kinds of defense methods have been proposed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>, but many of them are based on obfuscated gradients which does not really improve robustness under stronger attacks <ref type="bibr" target="#b2">[3]</ref>. As an exception, <ref type="bibr" target="#b2">[3]</ref> reported that the adversarial training method developed in <ref type="bibr" target="#b30">[31]</ref> is the only defense that works even under carefully designed attacks.</p><p>Adversarial Training Adversarial training is one of the first defense ideas proposed in earlier papers <ref type="bibr" target="#b21">[22]</ref>. The main idea is to add adversarial examples into the training set to improve the robustness. However, earlier work usually only adds adversarial example once or only few times during the training phase. Recently, <ref type="bibr" target="#b30">[31]</ref> showed that adversarial training can be viewed as solving a min-max optimization problem where the training algorithm aims to minimize the robust loss, defined as the maximal loss within a certain -ball around each training sample. Based on this formulation, a clean adversarial training procedure based on PGD-attack has been developed and achieved state-of-the-art results even under strong attacks. This also motivates some recent research on gaining theoretical understanding of robust error <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. Also, adversarial training suffers from slow training time since it runs several steps of attacks within one update, and several recent works are trying to resolve this issue <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53]</ref>. From the theoretical perspective, a recent work <ref type="bibr" target="#b45">[46]</ref> considers to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization and therefore ensure robustness. <ref type="bibr" target="#b50">[51]</ref> consider generalization upper and lower bounds for robust generalization. <ref type="bibr" target="#b28">[29]</ref> improves the robust generalization by data augmentation with GAN. <ref type="bibr" target="#b20">[21]</ref> considers to reduce the optimization of min-max problem to online learning setting and use their results to analyze the convergence of GAN. In this paper, our analysis for adversarial is quite general and is not restricted to any specific kind of attack algorithm.</p><p>Global convergence of Gradient Descent Recent works on the over-parametrization of neural networks prove that when the width greatly exceeds the sample size, gradient descent converges to a global minimizer from random initialization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55]</ref>. The key idea in the earlier literature is to show that the Jacobian w.r.t. parameters has minimum singular value lower bounded, and thus there is a global minimum near every random initialization, with high probability. However for the robust loss, the maximization cannot be evaluated and the Jacobian is not necessarily full rank.</p><p>For the surrogate loss, the heuristic attack algorithm may not even be continuous and so the same arguments cannot be utilized.</p><p>Certified Defense and Robustness Verification In contrast to attack algorithms, neural network verification methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> tries to find upper bounds of the robust loss and provide certified robustness measurements. Equipped with these verification methods for computing upper bounds of robust error, one can then apply adversarial training to get a network with certified robustness. Our analysis in Section 4 can also be extended to certified adversarial training.</p><p>3 Preliminaries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Let [n] = {1, 2, . . . , n}. We use N (0, I) to denote the standard Gaussian distribution. For a vector v, we use v 2 to denote the Euclidean norm. For a matrix A we use A F to denote the Frobenius norm and A 2 to denote the spectral norm. We use •, • to denote the standard Euclidean inner product between two vectors, matrices, or tensors. We let O(•), Θ(•) and Ω (•) denote standard Big-O, Big-Theta and Big-Omega notations that suppress multiplicative constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Neural Networks</head><p>Here we give the definition of our deep fully-connected neural networks. For the convenience of proof, we use the same architecture as defined in <ref type="bibr" target="#b0">[1]</ref>. <ref type="foot" target="#foot_0">2</ref> Formally, we consider a neural network of the following form.</p><p>Let x ∈ R d be the input, the fully-connected neural network is defined as follows: A ∈ R m×d is the first weight matrix, W (h) ∈ R m×m is the weight matrix at the h-th layer for h ∈ [H], a ∈ R m×1 is the output layer, and σ(•) is the ReLU activation function. <ref type="foot" target="#foot_1">3</ref> The parameters are W = (vec{A} , vec{W (1) } , • • • , vec{W (H) } , a ) . However, without loss of generality, during training we will fix A and a once initialized, so later we will refer to W as</p><formula xml:id="formula_0">W = (vec{W (1) } , • • • , vec{W (H) } ) .</formula><p>The prediction function is defined recursively:</p><formula xml:id="formula_1">x (0) = Ax x (h) = W (h) x (h-1) , h ∈ [H]<label>(1)</label></formula><p>x</p><formula xml:id="formula_2">(h) = σ x (h) , h ∈ [H] f (W, x) = a x (H) ,</formula><p>where x (h) and x (h) are the feature vectors before and after the activation function, respectively. Sometimes we also denote x (0) = x (0) .</p><p>We use the following initialization scheme: Each entry in A and W (h) for h ∈ [H] follows the i.i.d. Gaussian distribution N (0, 2 m ), and each entry in a follows the i.i.d. Gaussian distribution N (0, 1). As we mentioned, we only train on W (h) for h ∈ [H] and fix a and A. For a training set {x i , y i } n i=1 , the loss function is denoted : (R, R) → R, and the (non-robust) training loss is</p><formula xml:id="formula_3">L(W) = 1 n n i=1 (f (W, x i ), y i ).</formula><p>We make the following assumption on the loss function: Assumption 3.1 (Assumption on the Loss Function). The loss (f (W, x), y) is Lipschitz, smooth, convex in f (W, x) and satisfies (y, y) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Perturbation and the Surrogate Loss Function</head><p>The goal of adversarial training is to make the model robust in a neighbor of each datum. We first introduce the definition of the perturbation set function to determine the perturbation at each point. Definition 3.1 (Perturbation Set). Let the input space be X ⊂ R d . The perturbation set function is B : X → P(X ), where P(X ) is the power set of X . At each data point x, B(x) gives the perturbation set on which we would like to guarantee robustness. For example, a commonly used perturbation set is B(x) = {x : xx 2 ≤ δ}. Given a dataset {x i , y i } n i=1 , we say that the perturbation set is compatible with the dataset if B(x i ) ∩ B(x j ) = φ implies y i = y j . In the rest of the paper, we will always assume that B is compatible with the given data.</p><p>Given a perturbation set, we are now ready to define the perturbation function that maps a data point to another point inside its perturbation set. We note that the perturbation function can be quite general including the identity function and any adversarial attack <ref type="foot" target="#foot_2">4</ref> . Formally, we give the following definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.2 (Perturbation Function). A perturbation function is defined as a function</head><formula xml:id="formula_4">A : W × R d → R d ,</formula><p>where W is the parameter space. Given the parameter W of the neural network (1), A(W, x) maps x ∈ R d to some x ∈ B(x) where B(x) refers to the perturbation set defined in Definition 3.1.</p><p>Without loss of generality, throughout Section 4 and 5, we will restrict our input x as well as the perturbation set B(x) within the surface of the unit ball</p><formula xml:id="formula_5">S = {x ∈ R d : x 2 = 1}.</formula><p>With the definition of perturbation function, we can now define a large family of loss functions on the training set {x i , y i } n i=1 . We will show this definition covers the standard loss used in empirical risk minimization and the robust loss used in adversarial training. Definition 3.3 (Surrogate Loss Function). Given a perturbation function A defined in Definition 3.2, the current parameter W of a neural network f , and a training set {x i , y i } n i=1 , we define the surrogate loss L A (W) on the training set as</p><formula xml:id="formula_6">L A (W) = 1 n n i=1 (f (W, A(W, x i )), y i ).</formula><p>It can be easily observed that the standard training loss L(W) is a special case of surrogate loss function when A is the identity. The goal of adversarial training is to minimize the robust loss, i.e. the surrogate loss when A is the strongest possible attack. The formal definition is as follows: Definition 3.4 (Robust Loss Function). The robust loss function is defined as</p><formula xml:id="formula_7">L * (W) := L A * (W)</formula><p>where</p><formula xml:id="formula_8">A * (W, x i ) = argmax x i ∈B(xi) (f (W, x i ), y i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Convergence Results of Adversarial Training</head><p>We consider optimizing the surrogate loss L A with the perturbation function A(W, x) defined in Definition 3.2, which is what adversarial training does given any attack algorithm A. In this section, we will prove that for a neural network with sufficient width, starting from the initialization W 0 , after certain steps of projected gradient descent within a convex set B(R), the loss L A is provably upper-bounded by the best minimax robust loss in this set min</p><formula xml:id="formula_9">W∈B(R) L * (W),</formula><p>where</p><formula xml:id="formula_10">B(R) = W : W (h) -W (h) 0 F ≤ R √ m , h ∈ [H] .<label>(2)</label></formula><p>Denote P B(R) as the Euclidean projection to the convex set B(R). Denote the parameter W after the t-th iteration as W t , and similarly W (h) t . For each step in adversarial training, projected gradient descent takes an update</p><formula xml:id="formula_11">V t+1 = W t -α∇ W L A (W t ), W t+1 = P B(R) (V t+1 )</formula><p>, where</p><formula xml:id="formula_12">∇ W L A (W) = 1 n n i=1 l (f (W, A(W, x i )), y i ) ∇ W f (W, A(W, x i )),</formula><p>and the derivative stands for ∂ ∂f , the gradient ∇ W f is with respect to the first argument W. Specifically, we have the following theorem. </p><formula xml:id="formula_13">α = O mH 2 for T = Θ R 2 m α = Ω R 2 H 2 2</formula><p>steps, then with high probability we have</p><formula xml:id="formula_14">min t=1,••• ,T L A (W t ) -L * (W * ) ≤ ,<label>(3)</label></formula><p>where W * = arg min W∈B(R) L * (W).</p><p>Remark. Recall that L A (W) is the loss suffered with respect to the perturbation function A. This means, for example, if the adversary uses the projected gradient ascent algorithm, then the theorem guarantees that projected gradient ascent cannot successfully attack the learned network. The stronger the attack algorithm is during training, the stronger the guaranteed surrogate loss becomes.</p><p>Remark. The value of R depends on the approximation capability of the network, i.e. the greater R is, the less L * (W * ) will be, thus affecting the overall bound on min t L A (W t ). We will elaborate on this in the next section, where we show that for R independent of m there exists a network of small adversarial training error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proof Sketch</head><p>Our proof idea utilizes the same high-level intuition as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> that near the initialization the network is linear. However, unlike these earlier works, the surrogate loss neither smooth, nor semi-smooth so there is no Polyak gradient domination phenomenon to allow for the global geometric contraction of gradient descent. In fact due to the the generality of perturbation function A allowed, the surrogate loss is not differentiable or even continuous in W, and so the standard analysis cannot be applied. Our analysis utilizes two key observations. First the network f (W, A(W, x)) is still smooth w.r.t. the first argument <ref type="foot" target="#foot_4">6</ref> , and is close to linear in the first argument near initialization, which is shown by directly bounding the Hessian w.r.t. W. Second, the perturbation function A can be treated as an adversary providing a worst-case loss function A (f, y) as done in online learning. However, online learning typically assumes the sequence of losses is convex, which is not the case here. We make a careful decoupling of the contribution to non-convexity from the first argument and the worst-case contribution from the perturbation function, and then we can prove that gradient descent succeeds in minimizing the surrogate loss. The full proof is in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adversarial Training Finds Robust Classifier</head><p>Motivated by the optimization result in Theorem 4.1, we hope to show that there is indeed a robust classifier in B(R). To show this, we utilize the connection between neural networks and their induced Reproducing Kernel Hilbert Space (RKHS) via viewing networks near initialization as a random feature scheme <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. Since we only need to show the existence of a network architecture that robustly fits the training data in B(R) and neural networks are at least as expressive as their induced kernels, we may prove this via the RKHS connection. The strategy is to first show the existence of a robust classifier in the RKHS, and then show that a sufficiently wide network can approximate the kernel via random feature analysis. The approximation results of this section will be, in general, exponential in dimension dependence due to the known issue of d-dimensional functions having exponentially large RKHS norm <ref type="bibr" target="#b3">[4]</ref>, so only offer qualitative guidance on existence of robust classifiers.</p><p>Since deep networks contain two-layer networks as a sub-network, and we are concerned with expressivity, we focus on the local expressivity of two-layer networks. We write the standard two-layer network in the suggestive way <ref type="foot" target="#foot_5">7</ref> (where the width m is an even number)</p><formula xml:id="formula_15">f (W, x) = 1 √ m   m/2 r=1 a r σ(w r x) + m/2 r=1 a r σ( w r x)   ,<label>(4)</label></formula><p>and initialize as w r ∼ N (0,</p><formula xml:id="formula_16">I d ) i.i.d. for r = 1, • • • , m<label>2</label></formula><p>, and wr is set to be equal to w r , a r is randomly drawn from {1, -1} and a r = -a r . Similarly, we define the set</p><formula xml:id="formula_17">B(R) = {W : W -W 0 F ≤ R} 8 for W = (w 1 , • • • , w m/2 , w1 , • • • , wm/2</formula><p>), W 0 being the initialization of W, and fix all a r after initialization.</p><p>To make things cleaner, we will use a smooth activation function σ(•) throughout this section <ref type="foot" target="#foot_7">9</ref> , formally stated as follows. Assumption 5.1 (Smoothness of Activation Function). The activation function σ(•) is smooth, that is, there exists an absolute constant C &gt; 0 such that for any z, z ∈ R</p><formula xml:id="formula_18">|σ (z) -σ (z )| ≤ C|z -z |.</formula><p>Prior to proving the approximation results, we would like to first provide a version of convergence theorem similar to Theorem 4.1, but for this two-layer setting. It is encouraged that the reader can read Appendix B for the proof of the following Theorem 5.1 first, since it is relatively cleaner than that of the deep setting but the proof logic is analogous. Theorem 5.1 (Convergence of Gradient Descent without Projection for Optimizing Surrogate Loss for Two-layer Networks). Suppose the loss function satisfies Assumption 3.1 and the activation function satisfies Assumption 5.1. With high probability, using the two-layer network defined above, for any &gt; 0, if we run gradient descent with step size α = O ( ), and if m = Ω R 4 2 , we have</p><formula xml:id="formula_19">min t=1,••• ,T L A (W t ) -L * (W * ) ≤ ,<label>(5)</label></formula><p>where W * = min W∈B(R) L * (W) and T = Θ( √ m α ). Remark. Compared to Theorem 4.1, we do not need the projection step for this two-layer theorem. We believe using a smooth activation function can also eliminate the need of the projection step in the deep setting from a technical perspective, and from a practical sense we conjecture that the projection step is not needed anyway. Now we're ready to proceed to the approximation results, i.e. proving that L * (W * ) is also small, and combined with Equation ( <ref type="formula" target="#formula_19">5</ref>) we can give an absolute bound on min t L A (W t ). For the reader's convenience, we first introduce the Neural Tangent Kernel (NTK) <ref type="bibr" target="#b24">[25]</ref> w.r.t. our two-layer network. Definition 5.1 (NTK <ref type="bibr" target="#b24">[25]</ref>). The NTK with activation function σ (•) and initialization distribution w ∼ N (0, I d ) is defined as K σ (x, y) = E w∼N (0,I d ) xσ (w x), yσ (w y) .</p><p>For a given kernel K, there is a reproducing kernel Hilbert space (RKHS) introduced by K. We denote it as H(K). We refer the readers to <ref type="bibr" target="#b35">[36]</ref> for an introduction of the theory of RKHS.</p><p>We formally make the following assumption on the universality of NTK. Assumption 5.2 (Existence of Robust Classifier in NTK). For any &gt; 0, there exists f ∈ H(K σ ), such that |f (x i ) -y i | ≤ , for every i ∈ [n] and x i ∈ B(x i ).</p><p>Also, we make an additional assumption on the activation function σ(•):</p><formula xml:id="formula_20">Assumption 5.3 (Lipschitz Property of Activation Function). The activation function σ(•) satisfies |σ (z)| ≤ C, ∀z ∈ R for some constant C.</formula><p>Under these assumptions, by applying the strategy of approximating the infinite situation by finite sum of random features, we can get the following theorem: , with probability at least 0.99 over the initialization there exists W such that</p><formula xml:id="formula_21">L * (W) ≤ and W ∈ B(R D,B, ).</formula><p>Combining Theorem 5.1 and 5.2 we finally know that </p><p>Remark 5.1. We point out that Assumption 5.2 is rather general and can be verified for a large class of activation functions by showing their induced kernel is universal as done in <ref type="bibr" target="#b31">[32]</ref>. Also, here we use an implicit expression of the radius B D,B, , but the dependence on can be calculated under specific activation function with or without the smoothness assumptions. As an example, using quadratic ReLU as activation function, we solve the explicit dependency on in Appendix C.2 that doesn't rely on Assumption 5.2.</p><p>Therefore, adversarial training is guaranteed to find a robust classifier under a given attack algorithm when the network width is sufficiently large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Capacity Requirement of Robustness</head><p>In this section, we will show that in order to achieve adversarially robust interpolation (which is formally defined below), one needs more capacity than just normal interpolation. In fact, empirical evidence have already shown that to reliably withstand strong adversarial attacks, networks require a significantly larger capacity than for correctly classifying benign examples only <ref type="bibr" target="#b30">[31]</ref>. This implies, in some sense, that using a neural network with larger width is necessary.</p><formula xml:id="formula_23">Let S δ = {(x 1 , • • • , x n ) ∈ (R d ) n : x i -x j 2 &gt; 2δ} and B δ (x) = {x : x -x 2 ≤ δ},</formula><p>where δ is a constant. We consider datasets in S δ and use B δ as the perturbation set function in this section.</p><p>We begin with the definition of the interpolation class and the robust interpolation class. Definition 6.1 (Interpolation class). We say that a function class F of functions f : R d → {1, -1}is an n-interpolation class<ref type="foot" target="#foot_8">10</ref> , if the following is satisfied:</p><formula xml:id="formula_24">∀(x 1 , • • • , x n ) ∈ S δ , ∀(y 1 , • • • , y n ) ∈ {±1} n , ∃f ∈ F, s.t. f (x i ) = y i , ∀i ∈ [n].</formula><p>Definition 6.2 (Robust interpolation class). We say that a function class F is an n-robust interpolation class, if the following is satisfied:</p><formula xml:id="formula_25">∀(x 1 , • • • , x n ) ∈ S δ , ∀(y 1 , • • • , y n ) ∈ {±1} n , ∃f ∈ F, s.t.f (x i ) = y i , ∀x i ∈ B δ (x i ), ∀i ∈ [n].</formula><p>We will use the VC-Dimension of a function class F to measure its complexity. In fact, as shown in <ref type="bibr" target="#b5">[6]</ref> (Equation( <ref type="formula" target="#formula_10">2</ref>)), for neural networks there is a tight connection between the number of parameters W , the number of layers H and their VC-Dimension Ω(HW log(W/H)) ≤ VC-Dimension ≤ O(HW log W ).</p><p>In addition, combining with the results in <ref type="bibr" target="#b51">[52]</ref>  </p><p>For a general hypothesis class F, we can evidently see that when F is an n-interpolation class, F has VC-Dimension at least n. For a neural network that is an n-interpolation class, without further architectural constraints, this lower bound of its VC-dimension is tight up to logarithmic factors as indicated in Equation ( <ref type="formula" target="#formula_26">7</ref>). However, we show that for a robust-interpolation class we will have a much larger VC-Dimension lower bound: Theorem 6.1. If F is an n-robust interpolation class, then we have the following lower bound on the VC-Dimension of F VC-Dimension ≥ Ω(nd),</p><p>where d is the dimension of the input space.</p><p>For neural networks, Equation <ref type="bibr" target="#b7">(8)</ref> shows that any architecture that is an n-robust interpolation class should have VC-Dimension at least Ω(nd). Compared with Equation ( <ref type="formula" target="#formula_26">7</ref>) which shows an n-interpolation class can be realized by a network architecture with VC-Dimension O(n log n), we can conclude that robust interpolation by neural networks needs more capacity, so increasing the width of neural network is indeed in some sense necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion on Limitations and Future Directions</head><p>This work provides a theoretical analysis of the empirically successful adversarial training algorithm in the training of robust neural networks. Our main results indicate that adversarial training will find a network of low robust surrogate loss, even when the maximization is computed via a heuristic algorithm such as projected gradient ascent. However, there are still some limitations with our current theory, and we also feel our results can lead to several thought-provoking future work, which is discussed as follows.</p><p>Removal of projection. It is also natural to ask whether the projection step can be removed, as it is empirically unnecessary and also unnecessary for our two-layer analysis. We believe using smooth activations might resolve this issue from a technical perspective, although practically it seems the projection step in the algorithm is unnecessary in any case.</p><p>Generalizing to different attacks. Firstly, our current guarantee of the surrogate loss is based on the same perturbation function as that used during training. It is natural to ask that whether we can ensure the surrogate loss is low with respect to a larger family of perturbation functions than that used during training.</p><p>Exploiting structures of network and data. Same as the recent proof of convergence on overparameterized networks in the non-robust setting, our analysis fails to further incorporate useful network structures apart from being sufficiently wide, and as a result increasing depth can only hurt the bound. It would be interesting to provide finer analysis based on additional assumptions on the alignment of the network structure and data distribution.</p><p>Improving the approximation bound. On the expressivity side, the current argument utilizes that a neural net restricted to a local region can approximate its induced RKHS. Although the RKHS is universal, they do not avoid the curse of dimensionality (see Appendix C.2). However, we believe in reality, the required radius of region R to achieve robust approximation is not as large as the theorem demands. So an interesting question is whether the robust expressivity of neural networks can adapt to structures such as low latent dimension of the data mechanism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>, thereby reducing the approximation bound.</p><p>Capacity requirement of robustness and robust generalization. Apart from this paper, there are other works supporting the need for capacity including the perspective of network width <ref type="bibr" target="#b30">[31]</ref>, depth <ref type="bibr" target="#b48">[49]</ref> and computational complexity <ref type="bibr" target="#b34">[35]</ref>. It is argued in <ref type="bibr" target="#b50">[51]</ref> that robust generalization is also harder using Rademacher complexity. In fact, it appears empirically that robust generalization is even harder than robust training. It is observed that increasing the capacity, though benifiting the dacay of training loss, has much less effect on robust generalization. There are also other factors behind robust generalization, like the number of training data <ref type="bibr" target="#b39">[40]</ref>. The questions about robust generalization, as well as to what extent capacity influnces it, are still subject to much debate.</p><p>The above are several interesting directions of further improvement to our current result. In fact, many of these questions are largely unanswered even for neural nets in the non-robust setting, so we leave them to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 ,</head><label>7</label><figDesc>Theorem 4.1 (Convergence of Projected Gradient Descent for Optimizing Surrogate Loss). Given &gt; 0, suppose R = Ω(1), and m ≥ max Θ R 9 H 16 Θ(d 2 ) . Let the loss function satisfy Assumption 3.1. 5 If we run projected gradient descent based on the convex constraint set B(R) with stepsize</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 5 . 2 ( 4 D</head><label>524</label><figDesc>Existence of Robust Classifier near Initialization). Given data set D = {(x i , y i )} n i=1 and a compatible perturbation set function B with x i and its allowed perturbations taking value on S, for the two-layer network defined in (4), if Assumption 3.1, 5.1, 5.2, 5.3 hold, then for any , δ &gt; 0, there exists R D,B, such that when the width m satisfies m = Ω R ,B, 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Corollary 5 . 1 ( 4 D,B, 2 ) 2 D</head><label>51422</label><figDesc>Adversarial Training Finds a Network of Small Robust Training Loss). Given data set on the unit sphere equipped with a compatible perturbation set function and an associated perturbation function A, which also takes value on the unit sphere. Suppose Assumption 3.1, 5.1, 5.2, 5.3 are satisfied. Then there exists a R D,B, which only depends on dataset D, perturbation B and , such that for any 2-layer fully connected network with width m = Ω( R , if we run gradient descent with stepsize α = O ( ) for T = Θ( R ,B, α ) steps, then with probability 0.99, min t=1,••• ,T L A (W t ) ≤ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(Theorem 3)  which shows the existence of a 4-layer neural network with O(n) parameters that can interpolate any n data points, i.e. an n-interpolation class, we have that an n-interpolation class can be realized by a fixed depth neural network with VC-Dimension upper bound</figDesc><table /><note><p><p>VC-Dimension</p>≤ O(n log n).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We only consider the setting when the network output is scalar. However, it is not hard to extend out results to the setting of vector outputs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We assume intermediate layers are square matrices of size m for simplicity. It is not difficult to generalize our analysis to rectangular weight matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>It is also not hard to extend our analysis to perturbation functions involving randomness.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We actually didn't use the assumption (y, y) = 0 in the proof, so common loss functions like the crossentropy loss works in this theorem. Also, with some slight modifications, it is possible to prove for other loss functions including the square loss.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p><ref type="bibr" target="#b5">6</ref> It is not jointly smooth in W, which is part of the subtlety of the analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>This makes f (W, x) = 0 at initialization, which helps eliminate some unnecessary technical nuisance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Note that we have taken out the term 1 √ m explicitly in the network expression for convenience, so in this section there is a difference of scaling by a factor of √ m from the W used in the previous section.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Similar approximation results also hold for other activation functions like ReLU.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>Here we let the classification output be ±1, and a usual classifier f outputting a number in R can be treated as sign(f ) here.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowlegements</head><p>We acknowlegde useful discussions with Siyu Chen, Di He, Runtian Zhai, and Xiyu Zhai. RG and TC are partially supported by the elite undergraduate training program of School of Mathematical Sciences in Peking University. LW acknowledges support by Natioanl Key R&amp;D Program of China (no. 2018YFB1402600), BJNSF (L172037). JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0303, the Sloan Research Fellowship, and NSF CCF #1900145.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03962</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Breaking the curse of dimensionality with convex neural networks</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="629" to="681" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the equivalence between kernel quadrature rules and random feature expansions</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="714" to="751" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nearly-tight vcdimension and pseudodimension bounds for piecewise linear neural networks</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Peter L Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">63</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12173</idno>
		<title level="m">On the inductive bias of neural tangent kernels</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04248</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10204</idno>
		<title level="m">Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A gram-gauss-newton method learning overparameterized deep neural networks for regression problems</title>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jikai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11675</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalization bounds of stochastic gradient descent for wide and deep neural networks</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13210</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Zoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Elan</forename><surname>Jeremy M Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02918</idno>
		<title level="m">Certified adversarial robustness via randomized smoothing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SGD learns the conjugate kernel class of the network</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2422" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the power of over-parametrization in neural networks with quadratic activation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01206</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03804</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Robust physical-world attacks on deep learning models</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08945</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07362</idno>
		<title level="m">Learning in non-convex games with an optimization oracle</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Countering adversarial images using input transformations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<title level="m">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01204</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rob-gan: Generator, discriminator, and adversarial attacker</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal kernels</title>
		<author>
			<persName><forename type="first">Yuesheng</forename><surname>Charles A Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2651" to="2667" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">New analysis and algorithm for learning with drifting distributions</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><forename type="middle">Munoz</forename><surname>Medina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="124" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<title level="m">Foundations of Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00532</idno>
		<title level="m">Adversarial robustness may be at odds with simplicity</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An introduction to the theory of reproducing kernel Hilbert spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Vern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Paulsen</surname></persName>
		</author>
		<author>
			<persName><surname>Raghupathi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uniform approximation of functions with random bases</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 46th Annual Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="555" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A convex relaxation barrier to tight robust verification of neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5014" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free! arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast and effective robustness certification</title>
		<author>
			<persName><forename type="first">Gagandeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10802" to="10813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.3027</idno>
		<title level="m">Introduction to the non-asymptotic analysis of random matrices</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the convergence and robustness of adversarial training</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6586" to="6595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards fast computation of certified robustness for relu networks</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Tsui-Wei Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5273" to="5282" />
		</imprint>
	</monogr>
	<note>Duane Boning, and Inderjit Dhillon</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5283" to="5292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Intriguing properties of adversarial training</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03787</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Optimal approximation of continuous functions by very deep relu networks</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03620</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rademacher complexity for adversarially robust generalization</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11914</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Finite sample expressive power of small-width relu networks</title>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07770</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">You only propagate once: Painless adversarial training using maximal principle</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient neural network robustness certification with general activation functions</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4939" to="4948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent optimizes over-parameterized deep ReLU networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08888</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
