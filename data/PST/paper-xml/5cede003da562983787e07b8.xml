<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Survey on deep learning with class imbalance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<addrLine>777 Glades Road</addrLine>
									<settlement>Boca Raton</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<addrLine>777 Glades Road</addrLine>
									<settlement>Boca Raton</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Survey on deep learning with class imbalance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0C04D5AE30D0625B139B89F93FC6FE9</idno>
					<idno type="DOI">10.1186/s40537-019-0192-5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Deep neural networks</term>
					<term>Class imbalance</term>
					<term>Big data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems containing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the implementation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several traditional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Supervised learning methods require labeled training data, and in classification problems each data sample belongs to a known class, or category <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In a binary classification problem with data samples from two groups, class imbalance occurs when one class, the minority group, contains significantly fewer samples than the other class, the majority group. In many problems <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, the minority group is the class of interest, i.e., the positive class. A well-known class imbalanced machine learning scenario is the medical diagnosis task of detecting disease, where the majority of the patients are healthy and detecting disease is of greater interest. In this example, the majority group of healthy patients is referred to as the negative class. Learning from these imbalanced data sets can be very difficult, especially when working with big data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and non-standard machine learning methods are often required to achieve desirable results. A thorough understanding of the class imbalance problem and the methods available for addressing it is indispensible, as such skewed data exists in many real-world applications.</p><p>When class imbalance exists within training data, learners will typically over-classify the majority group due to its increased prior probability. As a result, the instances belonging to the minority group are misclassified more often than those belonging to the majority group. Additional issues that arise when training neural networks with imbalanced data will be discussed in the "Deep learning methods for class imbalanced data" section. These negative effects make it very difficult to accomplish the typical objective of accurately predicting the positive class of interest. Furthermore, some evaluation metrics, such as accuracy, may mislead the analyst with high scores that incorrectly indicate good performance. Given a binary data set with a positive class distribution of 1%, a naïve learner that always outputs the negative class label for all inputs will achieve 99% accuracy. Many traditional machine learning techniques, which are summarized in the "Machine learning techniques for class imbalanced data" section, have been developed over the years to combat these adverse effects.</p><p>Methods for handling class imbalance in machine learning can be grouped into three categories: data-level techniques, algorithm-level methods, and hybrid approaches <ref type="bibr" target="#b9">[10]</ref>. Data-level techniques attempt to reduce the level of imbalance through various data sampling methods. Algorithm-level methods for handling class imbalance, commonly implemented with a weight or cost schema, include modifying the underlying learner or its output in order to reduce bias towards the majority group. Finally, hybrid systems strategically combine both sampling and algorithmic methods <ref type="bibr" target="#b9">[10]</ref>.</p><p>Over the last 10 years, deep learning methods have grown in popularity as they have improved the state-of-the-art in speech recognition, computer vision, and other domains <ref type="bibr" target="#b10">[11]</ref>. Their recent success can be attributed to an increased availability of data, improvements in hardware and software <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, and various algorithmic breakthroughs that speed up training and improve generalization to new data <ref type="bibr" target="#b16">[17]</ref>. Despite these advances, very little statistical work has been done which properly evaluates techniques for handling class imbalance using deep learning and their corresponding architectures, i.e. deep neural networks (DNNs). In fact, many researchers agree that the subject of deep learning with class imbalanced data is understudied <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. For this reason, our survey is limited to just 15 deep learning methods for addressing class imbalance.</p><p>A comprehensive literature review was performed in order to identify a broad range of deep learning methods for addressing class imbalance. We have documented the specific details of the literature search process so that other scholars may more confidently use this survey in future research, an essential step in any literature review <ref type="bibr" target="#b23">[24]</ref>. Candidate papers were first discovered through the Google Scholar <ref type="bibr" target="#b24">[25]</ref> and IEEE Xplore <ref type="bibr">[26]</ref> databases. Keyword searches included combinations of query terms such as: "class imbalance", "class rarity", "skewed data", "deep learning", "neural networks" and "deep neural networks". Search results were reviewed and filtered, removing those that did not demonstrate learning from class imbalanced data with neural networks containing two or more hidden layers. No restrictions were placed on the date of publication. The matched search results were then used to perform backward and forward searches, i.e. reviewing the references of matched articles and additional sources that have cited these articles. This was repeated until all relevant papers were identified, to the best of our knowledge.</p><p>Additional selection criteria were applied to exclude papers that only tested low levels of class imbalance, that did not compare proposed methods to other existing class imbalance methods, or that only used a single data set for evaluation. We discovered that papers meeting these criteria are very limited. Therefore, in order to increase the total number of selected works, these additional requirements were relaxed. The final set of 15 publications includes journal articles, conference papers, and student theses that employ deep learning methods with class imbalanced data.</p><p>We explore a variety of data-level, algorithm-level, and hybrid deep learning methods designed to improve the classification of imbalanced data. Implementation details, experimental results, data set details, network topologies, class imbalance levels, performance metrics, and any known limitations are included in each surveyed work's discussion. Tables <ref type="table" target="#tab_19">17</ref> and<ref type="table" target="#tab_21">18</ref>, in the "Discussion of surveyed works" section, summarize all of the surveyed deep learning methods and the details of their corresponding data sets. This survey provides the most current analysis of deep learning methods for addressing class imbalance, summarizing and comparing all related work to date, to the best of our knowledge.</p><p>The remainder of this paper is organized as follows. The "Class imbalance background" section provides background information on the class imbalance problem, reviews performance metrics that are more sensitive to class imbalanced data, and discusses some of the more popular traditional machine learning (non-deep learning) techniques for handling imbalanced data. The "Deep learning background" section provides necessary background information on deep learning. The neural network architectures used throughout the survey are introduced, along with several important milestones and the use of deep learning in solving big data analytics challenges. The "Deep learning methods for class imbalanced data" section surveys 15 published studies that analyze deep learning methods for addressing class imbalance. The "Discussion of surveyed works" section summarizes the surveyed works and offers further insight into their various strengths and weaknesses. The "Conclusion" section concludes the survey and discusses potential areas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class imbalance background</head><p>The task of binary classification, comprised of one positive group and one negative group, is used to discuss class imbalance and various techniques for addressing its challenges in this section. These concepts can be extended to the multi-class problem, because it is possible to convert multi-class problems into a set of two-class problems through class decomposition <ref type="bibr" target="#b25">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The class imbalance problem</head><p>Skewed data distributions naturally arise in many applications where the positive class occurs with reduced frequency, including data found in disease diagnosis <ref type="bibr" target="#b2">[3]</ref>, fraud detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, computer security <ref type="bibr" target="#b5">[6]</ref>, and image recognition <ref type="bibr" target="#b6">[7]</ref>. Intrinsic imbalance is the result of naturally occurring frequencies of data, e.g. medical diagnoses where the majority of patients are healthy. Extrinsic imbalance, on the other hand, is introduced through external factors, e.g. collection or storage procedures <ref type="bibr" target="#b26">[28]</ref>.</p><p>It is important to consider the representation of the minority and majority classes when learning from imbalanced data. It was suggested by <ref type="bibr">Krawczyk [10]</ref> that good results can be obtained, regardless of class disproportion, if both groups are well represented and come from non-overlapping distributions. Japkowicz <ref type="bibr" target="#b27">[29]</ref> examined the effects of class imbalance by creating artificial data sets with various combinations of complexity, training set size, and degrees of imbalance. The results show that sensitivity to imbalance increases as problem complexity increases, and that non-complex, linearly separable problems are unaffected by all levels of class imbalance.</p><p>In some domains, there is a genuine lack of data due to the low frequency with which events occur, e.g. detecting oil spills <ref type="bibr" target="#b6">[7]</ref>. Learning from extreme class imbalanced data, where the minority class accounts for as few as 0.1% of the training data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">30]</ref>, is of great importance because it is typically these rare occurrences that we are most interested in. Weiss <ref type="bibr" target="#b29">[31]</ref> discusses the difficulties of learning from rare events and various machine learning techniques for addressing these challenges.</p><p>The total number of minority samples available is of greater interest than the ratio or percentage of the minority. Consider a minority group that is just 1% of a data set containing 1 million samples. Regardless of the high level of imbalance, there are still many positive samples <ref type="bibr" target="#b9">(10,</ref><ref type="bibr">000)</ref> available to train a model. On the other hand, an imbalanced data set where the minority class displays rarity or under-representation is more likely to compromise the performance of the classifier <ref type="bibr" target="#b28">[30]</ref>.</p><p>For the purpose of comparing experimental results across all works presented in this survey, a ratio ρ (Eq. 1) <ref type="bibr" target="#b22">[23]</ref> will be used to indicate the maximum between-class imbal- ance level. C i is a set of examples in class i, and max i {|C i |} and min i {|C i |} return the max- imum and minimum class size over all i classes, respectively. For example, if a data set's largest class has 100 samples and its smallest class has 10 samples, then the data has an imbalance ratio of ρ = 10 . Since the actual number of samples may prove more impor- tant than the ratio, Table <ref type="table" target="#tab_21">18</ref> also includes the maximum and minimum class sizes for all experiments in this survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance metrics</head><p>The confusion matrix in Table <ref type="table" target="#tab_0">1</ref> summarizes binary classification results. The FP and FN errors correspond to Type I and Type II errors, respectively. All of the performance metrics listed in this section can be derived from the confusion matrix.</p><p>(1) ρ = max i {|C i |} min i {|C i |} Accuracy (Eq. 2) and error rate (Eq. 3) are the most frequently used metrics when evaluating classification results. When working with class imbalance, however, both are insufficient, as the resulting value is dominated by the majority group, i.e. the negative class.</p><p>As mentioned previously, when given a data set whose positive group distribution is just 1% of the data set, a naïve classifier can achieve a 99% accuracy score by simply labeling all examples as negative. Of course, such a model would provide no real value. For this reason, we review several popular evaluation metrics commonly used with imbalanced data problems.</p><p>Precision (Eq. 4) measures the percentage of the positively labeled samples that are actually positive. Precision is sensitive to class imbalance because it considers the number of negative samples incorrectly labeled as positive. Precision alone is insufficient, however, because it provides no insight into the number of samples from the positive group that were mislabeled as negative. On the other hand, Recall (Eq. 5), or the True Positive Rate (TPR), measures the percentage of the positive group that was correctly predicted to be positive by the model. Recall is not affected by imbalance because it is only dependent on the positive group. Recall does not consider the number of negative samples that are misclassified as positive, which can be problematic in problems containing class imbalanced data with many negative samples. There is a trade-off between precision and recall, and the metric of greater importance varies from problem to problem. Selectivity (Eq. 6), or the True Negative Rate (TNR), measures the percentage of the negative group that was correctly predicted to be negative.</p><p>( The F-Measure (Eq. 7), or F1-score, combines precision and recall using the harmonic mean, where coefficient β is used to adjust the relative importance of precision versus recall. The G-Mean (Eq. 8) measures performance by combining both the TPR and the TNR metrics using the square root of their product. Similar to the G-Mean, the Balanced Accuracy (Eq. 9) metric also combines TPR and TNR values to compute a metric that is more sensitive to the minority group <ref type="bibr" target="#b17">[18]</ref>. Although F-Measure, G-Mean, and Balanced Accuracy are improvements over Accuracy and Error Rate, they are still not entirely effective when comparing performance between classifiers and various distributions <ref type="bibr" target="#b26">[28]</ref>.</p><formula xml:id="formula_0">) Accuracy = TP + TN TP + TN + FP + FN (3) Error Rate = 1 -Accuracy (4) Precision = TP TP + FP (5) Recall = TPR = TP TP + FN (6) Selectivity = TNR = TN TN + FP (7) F-Measure = (1 + β 2 ) × Recall × Precision β 2 × Recall + Precision (8) G-Mean = √ TPR × TNR (9) Balanced Accuracy = 1 2 × (TPR + TNR)<label>2</label></formula><p>The receiver operating characteristics (ROC) curve, first presented by Provost and Fawcett <ref type="bibr" target="#b30">[32]</ref>, is another popular assessment which plots true positive rate over false positive rate, creating a visualization that depicts the trade-off between correctly classified positive samples and incorrectly classified negative samples. For models which produce continuous probabilities, thresholding can be used to create a series of points along ROC space <ref type="bibr" target="#b26">[28]</ref>. From this a single summary metric, the area under the ROC curve (AUC), can be computed and is often used to compare performance between models. A weighted-AUC metric, which takes cost biases into consideration when calculating the area, was introduced by Weng and Poon <ref type="bibr" target="#b31">[33]</ref>.</p><p>According to Davis and Goadrich <ref type="bibr" target="#b32">[34]</ref>, ROC curves can present overly optimistic results on highly skewed data sets and Precision-Recall (PR) curves should be used instead. The authors claim that a curve can only dominate in ROC space if it also dominates in PR space. This is justified by the fact that the false positive rate used by ROC, FPR = FP FP+TN , will be less sensitive to changes in FP as the size of the negative class grows.</p><p>According to Seliya et al. <ref type="bibr" target="#b33">[35]</ref>, learners should be evaluated with a set of complementary performance metrics, where each individual metric captures a different aspect of performance. In their comprehensive study, 22 different performance metrics were used to evaluate two classifiers across 35 unique data sets. Common factor analysis was then used to group the metrics, identifying sets of unrelated performance metrics that can be used in tandem to reduce redundancy and improve performance interpretation. One example set of complementary performance metrics discovered by Seliya et al. is AUC, Brier Inaccuracy <ref type="bibr" target="#b34">[36]</ref>, and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine learning techniques for class imbalanced data</head><p>Addressing class imbalance with traditional machine learning techniques has been studied extensively over the last two decades. The bias towards the majority class can be alleviated by altering the training data to decrease imbalance, or by modifying the model's underlying learning or decision process to increase sensitivity towards the minority group. As such, methods for handling class imbalance are grouped into data-level techniques, algorithm-level methods, and hybrid approaches. This section summarizes some of the more popular traditional machine learning methods for handling class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-level methods</head><p>Data-level methods for addressing class imbalance include over-sampling and undersampling. These techniques modify the training distributions in order to decrease the level of imbalance or reduce noise, e.g. mislabeled samples or anomalies. In their simplest forms, random under-sampling (RUS) discards random samples from the majority group, while random over-sampling (ROS) duplicates random samples from the minority group <ref type="bibr" target="#b35">[37]</ref>.</p><p>Under-sampling voluntarily discards data, reducing the total amount of information the model has to learn from. Over-sampling will cause an increased training time due to the increased size of the training set, and has also been shown to cause over-fitting <ref type="bibr" target="#b36">[38]</ref>. Over-fitting, characterized by high variance, occurs when a model fits too closely to the training data and is then unable to generalize to new data. A variety of intelligent sampling methods have been developed in an attempt to balance these trade-offs.</p><p>Intelligent under-sampling methods aim to preserve valuable information for learning. Zhang and Mani <ref type="bibr" target="#b37">[39]</ref> present several Near-Miss algorithms that use a K-nearest neighbors (K-NN) classifier to select majority samples for removal based on their distance from minority samples. One-sided selection was proposed by Kubat and Matwin <ref type="bibr" target="#b38">[40]</ref> as a method for removing noisy and redundant samples from the majority class as they are discovered through a 1-NN rule and Tomek links <ref type="bibr" target="#b39">[41]</ref>. <ref type="bibr">Barandela et al. [42]</ref> use Wilson's editing <ref type="bibr" target="#b41">[43]</ref>, a K-NN rule that removes misclassified samples from the training set, to remove majority samples from class boundaries.</p><p>A number of informed over-sampling techniques have also been developed to strengthen class boundaries, reduce over-fitting, and improve discrimination. Chawla et al. <ref type="bibr" target="#b42">[44]</ref> introduced the Synthetic Minority Over-sampling Technique (SMOTE), a method that produces artificial minority samples by interpolating between existing minority samples and their nearest minority neighbors. Several variants to SMOTE, e.g. Borderline-SMOTE <ref type="bibr" target="#b43">[45]</ref> and Safe-Level-SMOTE <ref type="bibr" target="#b44">[46]</ref>, improve upon the original algorithm by also taking majority class neighbors into consideration. Borderline-SMOTE limits over-sampling to the samples near class borders, while Safe-Level-SMOTE defines safe regions to prevent over-sampling in overlapping or noise regions.</p><p>Supervised learning systems usually define a concept with several disjuncts, where each disjunct is a conjunctive definition describing a subconcept <ref type="bibr" target="#b45">[47]</ref>. The size of a disjunct corresponds to the number of samples that the disjunct correctly classifies. Small disjuncts, often corresponding to rare cases in the domain, are learned concepts that correctly classify only a few data samples. These small disjuncts are problematic, as they often contain much higher error rates than large disjuncts, and they cannot be removed without compromising performance <ref type="bibr" target="#b46">[48]</ref>.</p><p>Jo and Japkowicz <ref type="bibr" target="#b47">[49]</ref> proposed cluster-based over-sampling to address the presence of small disjuncts in the training data. Minority and majority groups are first clustered using the K-means algorithm, then over-sampling is applied to each cluster separately. This improves both within-class imbalance and between-class imbalance.</p><p>Van Hulse et al. <ref type="bibr" target="#b35">[37]</ref> compared seven different sampling techniques with 11 commonly-used machine learning algorithms. Each model was evaluated with 35 benchmark data sets using six different performance metrics to compare results. It was shown that sampling results were highly dependent on both the learner and the evaluation performance metric. Experiments revealed that RUS resulted in good performance overall, outperforming ROS and intelligent sampling methods in most cases. The results suggest that, although RUS performs well in most cases, no sampling method is guaranteed to perform best in all problem domains, and multiple performance metrics should be used when evaluating results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm-level methods</head><p>Unlike data sampling methods, algorithmic methods for handling class imbalance do not alter the training data distribution. Instead, the learning or decision process is adjusted in a way that increases the importance of the positive class. Most commonly, algorithms are modified to take a class penalty or weight into consideration, or the decision threshold is shifted in a way that reduces bias towards the negative class.</p><p>In cost-sensitive learning, penalties are assigned to each class through a cost matrix. Increasing the cost of the minority group is equivalent to increasing its importance, decreasing the likelihood that the learner will incorrectly classify instances from this group <ref type="bibr" target="#b9">[10]</ref>. The cost matrix of a binary classification problem is shown in Table <ref type="table">2</ref>  <ref type="bibr" target="#b48">[50]</ref>. A given entry of the table, c ij , is the cost associated with predicting class i when the true class is j. Usually, the diagonal of the cost matrix, where i = j , is set to 0. The costs corre- sponding to false positive and false negative errors are then adjusted for desired results.</p><p>Ling and Sheng <ref type="bibr" target="#b49">[51]</ref> categorize cost-sensitive methods as either a direct method, or a meta-learning method. Direct methods are methods that have cost-sensitive capabilities within themselves, achieved through modification of the learner's underlying algorithm such that costs are taken into consideration during learning. The optimization process changes from one of minimizing total error, to one of minimizing total cost. Meta-learning methods utilize a wrapper to convert cost-insensitive learners into cost-sensitive systems. If a cost-insensitive classifier produces posterior probability estimates, the cost matrix can be used to define a new threshold p * such that: Usually, thresholding methods use p * (Eq. 10) to redefine the output decision threshold when classifying samples <ref type="bibr" target="#b49">[51]</ref>. Threshold moving, or post-processing the output class probabilities using Eq. 10, is one meta-learning approach that converts a cost-insensitive learner into a cost-sensitive system.</p><p>One of the biggest challenges in cost-sensitive learning is the assignment of an effective cost matrix. The cost matrix can be defined empirically, based on past experiences, or a domain expert with knowledge of the problem can define them. Alternatively, the false negative cost can be set to a fixed value while the false positive cost is varied, using a validation set to identify the ideal cost matrix. The latter has the advantage of exploring <ref type="bibr" target="#b9">(10)</ref> p * = c 10 c 10 + c 01</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 Cost matrix</head><p>a range of costs, but can be expensive and even impractical if the size of the data set or number of features is too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid methods</head><p>Data-level and algorithm-level methods have been combined in various ways and applied to class imbalance problems <ref type="bibr" target="#b9">[10]</ref>. One strategy includes performing data sampling to reduce class noise and imbalance, and then applying cost-sensitive learning or thresholding to further reduce the bias towards the majority group. Several techniques which combine ensemble methods with sampling and cost-sensitive learning were presented in <ref type="bibr" target="#b26">[28]</ref>. Liu et al. <ref type="bibr" target="#b50">[52]</ref> proposed two algorithms, EasyEnsemble and BalanceCascade, that learn multiple classifiers by combining subsets of the majority group with the minority group, creating pseudo-balanced training sets for each individual classifier. SMOTEBoost <ref type="bibr" target="#b51">[53]</ref>, DataBoost-IM <ref type="bibr" target="#b52">[54]</ref>, and JOUS-Boost <ref type="bibr" target="#b53">[55]</ref> all combine sampling with ensembles. Sun <ref type="bibr" target="#b54">[56]</ref> introduced three cost-sensitive boosting methods, namely AdaC1, AdaC2, and AdaC3. These methods iteratively increase the impact of the minority group by introducing cost items into the AdaBoost algorithm's weight updates. Sun showed that the cost-sensitive boosted ensembles outperformed plain boosting methods in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction to deep learning</head><p>Deep learning is a sub-field of machine learning that uses artificial neural networks (ANNs) containing two or more hidden layers to approximate some function f * , where f * can be used to map input data to new representations or make predictions. The ANN, inspired by the biological neural network, is a set of interconnected neurons, or nodes, where connections are weighted and each neuron transforms its input into a single output by applying a non-linear activation function to the sum of its weighted inputs. In a feedforward network, input data propagates through the network in a forward pass, each hidden layer receiving its input from the previous layer's output, producing a final output that is dependent on the input data, the choice of activation function, and the weight parameters <ref type="bibr" target="#b0">[1]</ref>. Gradient descent optimization is then used to adjust the network's weight parameters in order to minimize the loss function, i.e. the error between expected output and actual output.</p><p>The multilayer perceptron (MLP) is a fully-connected feedforward neural network containing at least one hidden layer. A shallow and deep MLP are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The deep MLP is the simplest deep learning model in terms of implementation, but it quickly becomes very resource intensive as the number of weighted connections quickly increases.</p><p>The convolutional neural network (CNN) is a specialized feedforward neural network that was designed to process multi-dimensional data, e.g. images <ref type="bibr" target="#b56">[58]</ref>. It was inspired by the brain's visual cortex and its origins date back to the Neocognitron presented by Fukushima in 1980 <ref type="bibr" target="#b57">[59]</ref>. A CNN architecture is typically comprised of convolutional layers, pooling (subsampling) layers, and fully-connected layers. Figure <ref type="figure">2</ref> illustrates the LeNet-5 CNN architecture proposed by LeCun et al. <ref type="bibr" target="#b56">[58]</ref> in 1998 for the purpose of character recognition. Unlike fully-connected layers, a single unit of a convolutional layer is only connected to a small receptive field of its input, where the weights of its connections define a filter bank <ref type="bibr" target="#b10">[11]</ref>. The convolution operation is used to slide the filter bank across the input, producing activations at each receptive field that combine to form a feature map <ref type="bibr" target="#b58">[60]</ref>. In other words, the same set of weights are used to detect a specific feature, e.g. a horizontal line, at each receptive field of the input, and the output feature map indicates the presence of this feature at each location. The concept of local connections and shared weights take advantage of the fact that input signals in close proximity of each other are usually highly correlated, and that input signals are often invariant to location. By combining multiple filter banks in a single convolutional layer, the layer can learn to detect multiple features in the input, and the resulting feature maps become the input of the next layer. Pooling layers are added after one or more convolutional layers in order to merge semantically similar features and reduce dimensionality <ref type="bibr" target="#b10">[11]</ref>. After the convolutional and pooling layers, the multi-dimensional output is flattened and fed to fully-connected layers for classification. Similar to the MLP, output activations are fed from Fig. <ref type="figure">2</ref> LeNet-5 CNN architecture of 1998 <ref type="bibr" target="#b56">[58]</ref> one layer to the next in a forward pass, and the weights are updated through gradient descent.</p><p>The MLP and CNN are just two of many alternative DNN architectures that have been developed over the years. Recurrent neural networks (RNNs), autoencoders, and stochastic networks are explained thoroughly in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b59">61]</ref>. They also present advanced optimization techniques that have been shown to improve training time and performance, e.g. regularization methods, parameter initialization, improved optimizers and activation functions, and normalization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation learning</head><p>The success of a conventional machine learning algorithm is highly dependent on the representation of the input data, making feature engineering a critical step in the machine learning workflow. This is very time consuming and for many complex problems, e.g. image recognition, it can be extremely difficult to determine which features will yield the best results. Deep learning offers a solution to this problem by building upon the concept of representation learning <ref type="bibr" target="#b10">[11]</ref>.</p><p>Representation learning is the process of using machine learning to map raw input data features into a new representation, i.e. a new feature space, for the purpose of improving detection and classification tasks. This mapping from raw input data to new representations is achieved through non-linear transformations of the input data. Composing multiple non-linear transformations creates hierarchical representations of the input data, increasing the level of abstraction through each transformation. This automatic generation of new features saves valuable time by removing the need for experts to manually hand engineer features, and improves overall performance in many complex problem domains, such as image and speech, where it is otherwise difficult to determine the best features. As data passes through the hidden layers of a DNN, it is transformed by each layer into a new representation. Given sufficient data, DNNs are able to learn high-level feature representations of inputs through the composition of multiple hidden layers. These learned representations amplify components of the input which are important for discrimination, while suppressing those that are unimportant <ref type="bibr" target="#b10">[11]</ref>. Deep learning architectures achieve their power through this composition of increasingly complex abstract representations <ref type="bibr" target="#b58">[60]</ref>. This approach to problem solving intuitively makes sense, as composing simple concepts into complex concepts is analogous to many real-world problem domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning milestones</head><p>The first DNNs date back to the 1960's, but they were largely abandoned in favor of traditional machine learning methods due to difficulties in training and inadequate performance <ref type="bibr" target="#b60">[62]</ref>. In 1986, Rumelhart et al. <ref type="bibr" target="#b61">[63]</ref> presented backpropagation, a method for efficiently updating neural network weight parameters by propagating the gradient of the loss function through multiple layers. It was believed by most, however, that gradient descent would be unable to escape poor local minima during optimization, preventing neural networks from converging to a global acceptable solution. Today, we believe this to be untrue, as theoretical results suggest that local minima are generally not an issue and that systems nearly always reach solutions of similar quality <ref type="bibr" target="#b10">[11]</ref>. Despite some early successes in the late 1980s <ref type="bibr" target="#b62">[64]</ref> and 1990s <ref type="bibr" target="#b56">[58]</ref>, DNNs were mostly forsaken in practice and research due to these challenges.</p><p>In 2006, interests in deep learning were revived as research groups presented methods for sensibly initializing DNN weights with an unsupervised layer-wise pre-training procedure <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b64">66]</ref>. These pre-trained Deep Belief Networks (DBNs) can then be efficiently fine-tuned through supervised learning. They proved to be very effective in image and speech tasks, and led to record breaking results on a speech recognition task in 2009 and the deployment of deep learning speech systems in Android mobile devices by 2012 <ref type="bibr" target="#b10">[11]</ref>.</p><p>In 2012, Krizhevsky et al. <ref type="bibr" target="#b16">[17]</ref> submitted a deep CNN to the Large Scale Visual Recognition Challenge (LSVRC) <ref type="bibr" target="#b65">[67]</ref> that nearly halved the top-5% error rate, reducing from the previous year's 26% down to just 16%. The work by Krizhevsky et al. included several crucial methods which have since become common practice in deep learning work. The CNN was implemented on multiple graphics processing units (GPUs). The drastic speedup provided by parallel GPU computing allows for the training of deeper networks with larger data sets, and increased research productivity. A new non-saturating activation function, the rectified linear unit (ReLU), alleviated the vanishing gradient problem and allowed for faster training. Dropout was introduced as a regularization method to decrease over fitting in high capacity networks with many layers. Dropout simulates the ensembling of many models by randomly disabling neurons with a probability P ∈ [0, 1] during each iteration, forcing the model to learn more robust features. Data augmentation, artificially enlarging the data set by applying transformations to data samples, was also applied as a regularization technique. This event marked a major turning point and sparked new interest in deep learning and computer vision.</p><p>This newfound interest in deep learning drove leading technological companies to increase research efforts, producing many advances in deep learning and pushing the state-of-the-art in deep learning to new levels. Deep learning frameworks which abstract tensor computation <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> and GPU compatibility libraries <ref type="bibr" target="#b15">[16]</ref> have been made available to the community through open source software <ref type="bibr" target="#b66">[68]</ref> and cloud services <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b68">70]</ref>. Combined with an increasing amount of available data and public attention, deep learning is growing at a faster pace than ever before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning with big data</head><p>Many organizations are being faced with the challenges of big data, as they are exploring large volumes of data to extract value and guide decisions <ref type="bibr" target="#b69">[71]</ref>. Big data refers to data which exceeds the capabilities of standard data storage and data processing systems <ref type="bibr" target="#b70">[72]</ref>. This forces practitioners to adopt new techniques for storing, manipulating, and analyzing data. The rise of big data can be attributed to improvements in hardware and software, increased internet and social media activity, and a growing abundance of sensor-enabled interconnected devices, i.e. the internet of things (IoT).</p><p>More specifically, big data can be characterized by the four Vs: volume, variety, velocity, and veracity <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b71">73]</ref>. The large volumes of data being collected require highly scalable hardware and efficient analysis tools, often demanding distributed implementations. In addition to adding architecture and network overhead, distributed systems have been shown to exacerbate the negative effects of class imbalanced data <ref type="bibr" target="#b72">[74]</ref>. Advanced techniques for quickly processing incoming data streams and maintaining appropriate turnaround times are required to keep up with the rate at which data is being generated, i.e. data velocity. The variety of big data corresponds to the mostly unstructured, diverse, and inconsistent representations that arise as data is consumed from multiple sources over extended periods of time. This variety further increases the computational complexity of data preprocessing and machine learning. Finally, the veracity of big data, i.e. its accuracy and trustworthiness, must be regularly validated to ensure results do not become corrupted by invalid input. Some additional machine learning challenges that are magnified by big data include high-dimensionality, distributed infrastructures, realtime requirements, feature engineering, and data cleansing <ref type="bibr" target="#b73">[75]</ref>.</p><p>Najafabadi et al. <ref type="bibr" target="#b73">[75]</ref> discuss the use of deep learning in solving big data challenges. The ability of DNNs to extract meaningful features from large sets of unlabeled data is particularly important, as this is commonly encountered in big data analytics. The automatic extraction of features from mostly unstructured and diverse data, e.g. image, text and audio data, is therefore extremely useful. With abstract features extracted from big data through deep learning methods, simple linear models can often be used to complete machine learning tasks more efficiently. Advanced semantic-based information storage and retrieval systems, e.g. semantic indexing and hashing <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b75">77]</ref>, are also made possible with these high-level features. In addition, deep learning has been used to tag incoming data streams, helping to group and organize fast-moving data <ref type="bibr" target="#b73">[75]</ref>. In general, high-capacity DNNs are well suited for learning from the large volumes of data encountered in big data analytics.</p><p>As the presence of big data within organizations continues to increase, new methods will be required to keep up with the influx of data. Despite being relatively immature, deep learning methods are proving effective in solving many big data challenges. We believe that advances in deep learning, especially in learning from unsupervised data, will play a critical role in the future of big data analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning methods for class imbalanced data</head><p>Anand et al. <ref type="bibr" target="#b76">[78]</ref> explored the effects of class imbalance on the backpropagation algorithm in shallow neural networks in the 1990's. The authors show that in class imbalanced scenarios, the length of the minority class's gradient component is much smaller than the length of the majority class's gradient component. In other words, the majority class is essentially dominating the net gradient that is responsible for updating the model's weights. This reduces the error of the majority group very quickly during early iterations, but often increases the error of the minority group and causes the network to get stuck in a slow convergence mode.</p><p>This section analyzes a number of deep learning methods for addressing class imbalance, organized by data-level, algorithm-level, and hybrid methods. For each surveyed work, we summarize the implementation details and the characteristics of the data sets used to evaluate the method. We then discuss various strengths and weaknesses, considering topics such as class imbalance levels, interpretation of results, relative performance, difficulty of use, and generalization to other architectures and problem domains. Known limitations are highlighted and suggestions for future work are offered. For consistency, class imbalance is presented as the maximum between-class ratio, ρ (Eq. 1), for all surveyed works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-level methods</head><p>This section includes four papers that explore data-level methods for addressing class imbalance with DNNs. Hensman and Masko <ref type="bibr" target="#b77">[79]</ref> first show that balancing the training data with ROS can improve the classification of imbalanced image data. Then RUS and augmentation methods are used by Lee et al. <ref type="bibr" target="#b19">[20]</ref> to decrease class imbalance for the purpose of pre-training a deep CNN. Pouyanfar et al. <ref type="bibr" target="#b20">[21]</ref> introduce a new dynamic sampling method that adjusts sampling rates according to class-wise performance. Finally, Buda et al. <ref type="bibr" target="#b22">[23]</ref> compare RUS, ROS, and two-phase learning across multiple imbalanced image data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Balancing training data with ROS</head><p>Hensman and Masko <ref type="bibr" target="#b77">[79]</ref> explored the effects of class imbalance and ROS using deep CNNs. The CIFAR-10 [80] benchmark data set, comprised of 10 classes with 6000 images per class, was used to generate 10 imbalanced data sets for testing. These 10 generated data sets contained varying class sizes, ranging between 6% and 15% of the total data set, producing a max imbalance ratio ρ = 2.3 . In addition to varying the class size, the different distributions also varied the number of minority classes, where a minority class is any class smaller than the largest class. For example, a major 50-50 split (Dist. 3) reduced five of the classes to 6% of the data set size and increased five of the classes to 14%. As another example, a major singular over-representation (Dist. 5) increased the size of the airplane class to 14.5%, reducing the other nine classes slightly to 9.5%.</p><p>A variant of the AlexNet <ref type="bibr" target="#b16">[17]</ref> CNN, which has proven to perform well on CIFAR-10, was used for all experiments by Hensman and Masko. The baseline performance was defined by training the CNN on all distributions with no data sampling. The ROS method being evaluated consisted of randomly duplicating samples from the minority classes until all classes in the training set had an equal number of samples.</p><p>Hensman and Masko presented their results as the percentage of correct answers per class, and included the mean score for all classes, denoted by Total. To ensure results were valid, a total of three runs were completed for each experiment and then averaged. Table <ref type="table" target="#tab_1">3</ref> shows the results of the CNN without any data sampling. These results demonstrate the impact of class imbalance when training a CNN model. Most of the imbalanced distributions saw a loss in performance. Dist. 6 and Dist. 7, which contained very slight imbalance and no over-representation, performed just as well as the original balanced distribution. Some of the imbalanced distributions that contained overrepresented classes, e.g. Dist. 5 and Dist. 9, yielded useless models that were completely biased towards the majority group.</p><p>Table <ref type="table" target="#tab_2">4</ref> includes the results of training the CNN with balanced data that was generated through ROS. It shows that over-sampling performs significantly better than the baseline results from Table <ref type="table" target="#tab_1">3</ref>. Dist. 1 is excluded from Table <ref type="table" target="#tab_2">4</ref> because it is already balanced, i.e. ROS is not applicable. In this experiment, ROS improved the classification results for all distributions. Dist. 5 and Dist. 9 saw the largest performance gains, increasing from baseline Total F1-scores of 0.10 up to 0.73 and 0.72, respectively. The ROS classification results for distributions Dist. 2-Dist. 11 are comparable to the results achieved by the baseline CNN on Dist. 1, suggesting that ROS has completely restored model performance.</p><p>The experiments by Hensman and Masko show that applying ROS to the level of class balance can be effective in addressing slight class imbalance in image data. It is also made clear by some of the results in Table <ref type="table" target="#tab_1">3</ref> that small levels of imbalance are able to prevent a CNN from converging to an acceptable solution. We believe that the low imbalance levels tested ( ρ = 2.3 ) is the biggest limitation of this experiment, as imbal- ance levels are typically much higher in practice. Besides exploring additional data sets and higher levels of imbalance, one area worth pursuing further is the total number of epochs completed during training on the imbalanced data. In these experiments, only 10 epochs over the training data were completed because the authors were more interested in comparing performance than they were in achieving high performance. Running additional epochs would help to rule out whether or not the poor performance was due to the slow convergence phenomenon described by Anand et al.  At the time of writing, May 2015, Hensman and Masko observed no existing research that examined the impact of class imbalance on deep learning with popular benchmark data sets. Our literature review also finds this to be true, confirming that addressing class imbalance with deep learning is still relatively immature and understudied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-phase learning</head><p>Lee et al. <ref type="bibr" target="#b19">[20]</ref> combined RUS with transfer learning to classify highly-imbalanced data sets of plankton images, WHOI-Plankton <ref type="bibr" target="#b79">[81]</ref>. The data set contains 3.4 million images spread over 103 classes, with 90% of the images comprised of just five classes and the 5th largest class making up just 1.3% of the entire data set. Imbalance ratios of ρ &gt; 650 are exhibited in the data set, with many classes making up less than 0.1% of the data set. The proposed method is the two-phase learning procedure, where a deep CNN is first pre-trained with thresholded data, and then fine-tuned using all data. The thresholded data sets for pre-training are constructed by randomly under-sampling large classes until they reach a threshold of N examples. The authors selected a threshold of N = 5000 through preliminary experiments, then down-sampled all large classes to N samples. The proposed model (G) was compared to six alternative methods (A-F), a combination of transfer learning and augmentation techniques, using unweighted average F1-scores to compare results. Unlike plain RUS, which completely removes potentially useful information from the training set, the two-phase learning method only removes samples from the majority group during the pre-training phase. This allows the minority group to contribute more to the gradient during pre-training, and still allows the model to see all of the available data during the fine-tuning phase. The authors did not include details on the pre-training phase, such as the number of pre-training epochs or the criteria used to determine when pre-training was complete. These details should be considered in future works, as pre-training that results in high bias or high variance will certainly impact the final model's class-wise performance. Future work can also consider a hybrid approach, where the model is pre-trained with data that is generated through a combination of under-sampling majority classes and augmenting minority classes.</p><p>The previous year, Havaei et al. <ref type="bibr" target="#b80">[82]</ref> used a similar two-phase learning procedure to manage class imbalance when performing brain tumor image segmentation. The brain tumor data contains minority classes that make up less than 1% of the total data set. Havaei et al. stated that the two-phase learning procedure was critical in dealing with the imbalanced distribution in their image data. The details of this paper are not included in this survey because the two-phase learning is just one small component of their domain-specific experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic sampling</head><p>Pouyanfar et al. <ref type="bibr" target="#b20">[21]</ref> used a dynamic sampling technique to perform classification of imbalanced image data with a deep CNN. The basic idea is to over-sample the low performing classes and under-sample the high performing classes, showing the model less of what it has already learned and more of what it does not understand yet. This is somewhat analogous to how humans learn, by moving on from easy tasks once learned and focusing attention on the more difficult tasks. The author's self-collected data set contains over 10,000 images captured from publicly available network cameras, including a total of 19 semantic concepts, e.g. intersection, forest, farm, sky, water, playground, and park. From the original data set, 70% is used for training models, 20% is used for validation, and 10% is set aside for testing. The authors report imbalance ratios in the data set as high as ρ = 500 . Average F1-scores and weighted average F1-scores are used to compare the proposed model to a baseline CNN (A) and four alternative methods for handling class imbalance (B-E). The system presented by Pouyanfar et al. includes three core components: real time data augmentation, transfer learning, and a novel dynamic sampling method. Real time data augmentation improves generalization by applying various transformations to select images in each training batch. Transfer learning is achieved by fine-tuning an Inception-V3 network <ref type="bibr" target="#b81">[83]</ref> that was pre-trained using ImageNet <ref type="bibr" target="#b82">[84]</ref> data. The dynamic sampling method is the main contribution relative to class imbalance.</p><p>F 1 i is a vector containing all individual class F1-scores after iteration i, and f 1 i,j denotes the F1-score for class j on iteration i, where F1-scores are calculated for each class in a one-versus-all manner. During the next iteration, classes with lower F1-scores are sampled at a higher rate, forcing the learner to focus more on examples previously misclassified. Eq. 11 is used to obtain the next iteration's sample size for a given class c j , where N * is the average class size. To prevent over-fitting of the minority group, a second model is trained through transfer learning without sampling. At time of inference, the output label is computed as a function of both models.</p><p>The proposed model is compared with the following alternative methods: Average class-wise F1-scores are compared across all 19 concepts, showing that the basic CNN performs the worst in all cases. The basic CNN was unable to classify a single instance correctly for several concepts with very high imbalance ratios, including Playground and Airport with imbalance ratios of ρ = 200 and ρ = 500 , respectively. Transfer learning methods (C-E) performed significantly better than the baseline CNN, increasing the weighted average F1-score from 0.630 to as high as 0.779. Results in Table <ref type="table" target="#tab_5">6</ref> show that the proposed method (F) outperforms all other models tested on the given data set. Compared to transfer learning with basic augmentation (D), the dynamic sampling method (F) improved the weighted average F1-score from 0.779 to 0.794. The dynamic sampling method's ability to self-adjust sampling rates is its most attractive feature. This allows the method to adapt to different problems containing varying levels of complexity and class imbalance, with little to no hyperparameter tuning. By removing samples that have already been captured by the network <ref type="bibr" target="#b10">(11</ref></p><formula xml:id="formula_1">) Sample size (F 1 i , c j ) = 1 -f 1 i,j c k ∈C (1 -f 1 i,k ) × N *</formula><p>parameters, gradient updates will be driven by the more difficult positive class samples. The dynamic sampling method outperforms a hybrid of over-sampling and under-sampling (E) according to F1-scores, but the details of the sampling method are not included in the description. We also do not know how dynamic sampling performs against plain RUS and ROS, as these methods were not tested. This should be examined closely in future works to determine if dynamic sampling can be used as a general replacement for RUS and ROS. One area of concern is the method's dependency on a validation set to calculate the class-wise performance metrics that are required to determine sampling rates. This will certainly be problematic in cases of class rarity, where very few positive samples exist, as setting aside data for validation may deprive the model of valuable training data. Methods for maximizing the total available training data should be included in future research. In addition, future research should extend the dynamic sampling method to non-CNN architectures and other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROS, RUS, and two-phase learning</head><p>Buda et al. <ref type="bibr" target="#b22">[23]</ref> compare ROS, RUS, and two-phase learning using three multiclass image data sets and deep CNNs. MNIST <ref type="bibr" target="#b84">[86]</ref>, CIFAR- A different CNN architecture was empirically selected for each data set based off recent state-of-the-art results. For the MNIST and CIFAR-10 experiments, a version of the LeNet-5 <ref type="bibr" target="#b56">[58]</ref> and the All-CNN <ref type="bibr" target="#b85">[87]</ref> architectures were used for classification, respectively. Baseline results were established for each CNN architecture by performing classification on the data sets without any form of class imbalance technique, i.e. no data sampling or thresholding. Next, seven different methods for addressing class imbalance were integrated with the CNN architectures and tested. ROC AUC was extended to the multi-class problem by averaging the one-versus-all AUC for each class, and used to compare the methods. A portion of their results are presented in Fig. <ref type="figure">3</ref>. While it does provide a reasonable high-level view of each method's performance, the multi-class ROC AUC score provides no insight into the underlying class-wise performance trade-offs. It is not clear if there is high variance in the class-wise scores, or if one extremely low class-wise score is causing a large drop in the average AUC score. We believe that additional performance metrics, including class-wise scores, will better explain the effectiveness of each method in addressing class imbalance and help guide practitioners in model selection.</p><p>Buda et al. conclude that ROS should be performed until all class imbalance is eliminated. Despite their experimental results, we do not readily agree with this blanket statement, and argue that this is likely problem-dependent and requires further exploration.</p><p>The MNIST data set, both relatively low in complexity and small in size, was used to demonstrate that over-sampling until all classes are balanced performs best. We do not know how well over-sampling to this level will perform on more complex data sets, or in problems containing big data or class rarity. Furthermore, over-sampling to this level of class balance in a big data problem can be extremely resource intensive, drastically increasing training time by introducing large volumes of redundant data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of data-level methods</head><p>Two of the surveyed works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b77">79]</ref>  is difficult to understand the efficacy of two-phase learning. The dynamic sampling method outperformed baseline CNNs, but we do not know how it compares to ROS, RUS, or two-phase learning. Despite this limitation, the dynamic sampling method's ability to automatically adjust sampling rates throughout the training process is very appealing. Methods that can automatically adjust to varying levels of complexity and imbalance levels are favorable, as they reduce the number of tunable hyperparameters.</p><p>The experimental results suggest the use of ROS to eliminate class imbalance during the training of DNNs. This may be true for relatively small data sets, but we believe this will not hold true for problems containing big data or extreme class imbalance. Applying ROS until classes are balanced in very-large data sets, e.g. WHOI-Plankton data, will result in the duplication of large volumes of data and will drastically increase training times. RUS, on the other hand, reduces training time and may therefore be more practical in big data problems. We believe that RUS methods that remove redundant samples, reduce class noise, and strengthen class borders will prove helpful in these big data problems. Future work should explore these scenarios further.</p><p>All of the data-level methods presented were tested on class imbalanced image data with deep CNNs. In addition, differences in performance metrics and problem complexity make it difficult to compare methods directly. Future works should test these data-level methods on a variety of data types, imbalance levels, and DNN architectures. Multiple complementary performance metrics should be used to compare results, as this will better illustrate method trade-offs and guide future practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm-level methods</head><p>This section includes surveyed works that modify deep learning algorithms for the purpose of addressing class imbalance. These methods can be further divided into new loss functions, cost-sensitive learning, and threshold moving. Wang et al. <ref type="bibr" target="#b17">[18]</ref> and Lin et al. <ref type="bibr" target="#b86">[88]</ref> introduced new loss functions that allow the minority samples to contribute more to the loss. Wang et al. <ref type="bibr" target="#b87">[89]</ref>, Khan et al. <ref type="bibr" target="#b18">[19]</ref>, and Zhang et al. <ref type="bibr" target="#b88">[90]</ref> experimented with cost-sensitive DNNs. The methods proposed by <ref type="bibr">Khan et</ref>   <ref type="bibr">Buda et al.</ref> in the "ROS, RUS, and twophase learning" section has also been included in this section, as they experimented with threshold adjusting. Zhang et al. <ref type="bibr" target="#b89">[91]</ref> combine transfer learning, CNN feature extraction, and a cluster-based nearest neighbor rule to improve the classification of imbalanced image data. Finally, Ding et al. <ref type="bibr" target="#b90">[92]</ref> experiment with very-deep CNNs to determine if increasing neural network depth improves convergence rates with imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean false error (MFE) loss</head><p>Wang et al. <ref type="bibr" target="#b17">[18]</ref> found some success in modifying the loss function as they experimented with classifying imbalanced data with deep MLPs. A total of eight imbalanced binary data sets, including three image data sets and five text data sets, were generated from the CIFAR-100 <ref type="bibr" target="#b91">[93]</ref> and 20 Newsgroup <ref type="bibr" target="#b92">[94]</ref> collections. The data sets are all relatively small, with most training sets containing fewer than 2000 samples and the largest training set containing just 3500 samples. For each data set generated, imbalance ratios ranging from ρ = 5 to ρ = 20 were tested.</p><p>The authors first show that the mean squared error (MSE) loss function poorly captures the errors from the minority group in cases of high class imbalance, due to many negative samples dominating the loss function. They then propose two new loss functions that are more sensitive to the errors from the minority class, mean false error (MFE) and mean squared false error (MSFE). The proposed loss functions were derived by first splitting the MSE loss into two components, mean false positive error (FPE) and mean false negative error (FNE). The FPE (Eq. 12) and FNE (Eq. 13) values are then combined to define the total system loss, MFE (Eq. 14), as the sum of the mean error from each class.</p><p>Wang et al. introduce the MSFE loss (Eq. 15) as an improvement to the MFE loss, asserting that it better captures errors from the positive class. The MSFE can be expanded into the form 1  2 ((FPE + FNE) 2 + (FPE -FNE) 2 ) , demonstrating how the optimization process minimizes the difference between FPE and FNE. The authors believe this improved version will better balance the error rates between the positive and negative classes.</p><p>A deep MLP trained with the standard MSE loss is used as the baseline model. This same MLP architecture is then used to evaluate both the MFE and MSFE loss. Image classification results (Table <ref type="table" target="#tab_8">7</ref>) and text classification results (Table <ref type="table" target="#tab_9">8</ref>) show that the proposed models outperform the baseline in nearly all cases, with respect to F-measure and AUC scores.</p><p>(</p><formula xml:id="formula_2">) FPE = 1 N N i=1 n 1 2 (d (i) n -y (i) n ) 2 (13) FNE = 1 P P i=1 n 1 2 (d (i) n -y (i) n ) 2 (14) MFE = FPE + FNE (15) MSFE = FPE 2 + FNE 2<label>12</label></formula><p>Results show that the MFE and MSFE loss functions outperform MSE loss in almost all cases. Improvements over the baseline MSE loss are most apparent when class imbalance is greatest, i.e. imbalance levels of 5%. The MFE and MSFE performance gains are also more pronounced on the image data than on the text data. For example, the MSFE loss improved the classification of Household image data, increasing the F1-score from 0.1143 to 0.2353 when the class imbalance level was 5%.</p><p>Being relatively easy to implement and integrate into existing models is one of the biggest advantages of using custom loss functions for addressing class imbalance. Unlike data-level methods that increase the size of the training set, the loss function is less likely to increase training times. The loss functions should generalize to other domains with ease, but as seen in comparing the image and text performance results, performance gains will vary from problem to problem. Additional experiments should be performed to validate MFE and MSFE effectiveness, as it is currently unclear how many rounds were conducted per experiment and the F1-score and AUC gains over the baseline are only minor improvements. On the image data experiments, for example, the average AUC gain over the baseline is just 0.025, with a median AUC gain over the baseline of only 0.008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Focal loss</head><p>Lin et al. <ref type="bibr" target="#b86">[88]</ref> proposed a model that effectively addresses the extreme class imbalance commonly encountered in object detection problems, where positive foreground samples are heavily outnumbered by negative background samples. Two-stage and onestage detectors are well-known methods for solving such problems, where the two-stage detectors typically achieve higher accuracy at the cost of increased computation time. Lin et al. set out to determine whether a fast single-stage detector was capable of achieving state-of-the-art results on par with current two-stage detectors. Through analysis of various two-stage detectors (e.g. R-CNN <ref type="bibr" target="#b93">[95]</ref> and its successors) and one-stage detectors (e.g. SSD <ref type="bibr" target="#b94">[96]</ref> and YOLO <ref type="bibr" target="#b95">[97]</ref>), class imbalance was identified as the primary obstacle preventing one-stage detectors from achieving state-of-the-art performance. The overwhelming number of easily classified negative background candidates create imbalance ratios commonly in the range of ρ = 1000 , causing the negative class to account for the majority of the system's loss.</p><p>To combat these extreme imbalances, Lin et al. presented the focal loss (FL) (Eq. 16), which re-shapes the cross entropy (CE) loss in order to reduce the impact that easily classified samples have on the loss. This is achieved by multiplying the CE loss by a modulating factor, α t (1p t ) γ . Hyper parameter γ ≥ 0 adjusts the rate at which easy exam- ples are down weighted, and α t ≥ 0 is a class-wise weight that is used to increase the importance of the minority class. Easily classified examples, where p t → 1 , cause the modulating factor to approach 0 and reduce the sample's impact on the loss. ( <ref type="formula">16</ref>) The proposed one-stage focal loss model, RetinaNet, is evaluated against several stateof-the-art one-stage and two-stage detectors. The RetinaNet model is composed of a backbone model and two subnetworks, where the backbone model is responsible for producing feature maps from the input image, and the two subnetworks then perform object classification and bounding box regression. The authors selected the feature pyramid network (FPN) <ref type="bibr" target="#b86">[88]</ref>, built on top of the ResNet architecture <ref type="bibr" target="#b96">[98]</ref>, as the backbone model and pre-trained it on ImageNet data. They found that the FPN CNN, which produces features at different scales, outperforms the plain ResNet in object detection. Two additional CNNs with separate parameters, the subnetworks, are then used to perform classification and bounding box regression. The proposed focal loss function is applied to the classification subnet, where the total loss is computed as the sum of the focal loss over all ≈ 100, 000 candidates. The COCO <ref type="bibr" target="#b97">[99]</ref> data set was used to evaluate the pro- posed model against its competitors. The first attempt to train RetinaNet using standard CE loss quickly fails and diverges due to the extreme imbalance. By initializing the last layer of the model such that the prior probability of detecting an object is π = 0.01 , results improve significantly to an average precision (AP) of 30.2. Additional experiments are used to determine appropriate hyper parameters for the focal loss, selecting γ = 2.0 and α = 0.25 for all remaining experiments.</p><formula xml:id="formula_3">FL(p t ) = -α t (1 -p t ) γ log(p t )</formula><p>Experiments by Lin et al. show that the RetinaNet, with the proposed focal loss, is able to outperform existing one-stage and two-stage object detectors. It outscores the runner-up one-stage detector (DSSD513 <ref type="bibr" target="#b98">[100]</ref>) and the best two-stage detector (Faster R-CNN with TDM <ref type="bibr" target="#b99">[101]</ref>) by 7.6-point and 4.0-point AP gains, respectively. When compared to several online hard example mining (OHEM) <ref type="bibr" target="#b100">[102]</ref> methods, RetinaNet outscores the best method with an increase in AP from 32.8 to 36.0. Table <ref type="table" target="#tab_10">9</ref> compares results between RetinaNet and seven state-of-the-art one-stage and two-stage detectors.</p><p>Lin et al. provide additional information to illustrate the effectiveness of the focal loss method. In one experiment, they use a trained model to compute the focal loss over ≈ 10 7 negative images and ≈ 10 5 positive images. By plotting the cumulative distribution function for positive and negative samples, they show that as γ increases, more and more weight is placed onto a small subset of negative samples, i.e. the hard negatives. In fact, with γ = 2 , they show that most of the loss is derived from a very small fraction of sam- ples, and that the focal loss is indeed reducing the impact of easily-classified negative samples on the loss. Run time statistics were included in the report to demonstrate that they were able to construct a fast one-stage detector capable of outperforming accurate two-stage detectors.</p><p>The new FL method lends itself to not just class imbalance problems, but also hard sample problems. It addresses the primary minority class gradient issue defined by Anand et al. by preventing the majority group from dominating the loss and allowing the minority group to contribute more to the weight updates. Similar to the MFE and MSFE loss functions, advantages include being relatively easy to integrate into existing models and having minimal impact on training time. We believe that the FL method's ability to down-weight easily-classified samples will allow it to generalize well to other domains. The authors compared FL directly to the CE loss, but we do not know how FL compares to other existing class imbalance methods. Future works should compare this Figure <ref type="figure">4</ref> shows the training validation loss and accuracy of the focal loss method. Nemoto et al. conclude that focal loss improves problems related to class imbalance and over-fitting by adjusting the per-class learning speed. By comparing the loss plots, it is clear that as γ increases, the total loss decreases faster and the model is slower to over- fit. It is not clear, however, if FL with γ &gt; 0 is producing better classification results. First, the results from the first experiment cannot be compared to the results of the second experiment, because the total number of classes has been reduced from six to three, significantly reducing the complexity of the classification problem. Second, the results from the no change and laying solar panel classes in Fig. <ref type="figure">4</ref> show that the highest accuracy is achieved when γ = 0 , where the laying solar panel class is the smallest class. The focal loss, with γ = 0 , reduces to the standard cross entropy loss, suggesting that the CE Fig. <ref type="figure">4</ref> Focal loss and building change detection <ref type="bibr" target="#b101">[103]</ref> loss outperforms the focal loss on two of the three classes in this experiment. To better understand the effectiveness of FL, future work should include a baseline with consistent training data, several alternative methods for addressing class imbalance, and additional performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost-sensitive deep neural network (CSDNN)</head><p>Wang et al. <ref type="bibr" target="#b87">[89]</ref> employed a cost-sensitive deep neural network (CSDNN) method to detect hospital readmissions, a class imbalanced problem where a small percentage of patients are readmitted to a hospital shortly after their original visit. Two data sets were provided by the Barnes-Jewish Hospital, containing patient records spanning from 2007 to 2011. The first data set, general hospital wards (GHW), contains vital signs, clinical processes, demographics, real-time bedside monitoring, and other electronic data sources. Of the 2565 records in GHW, 406 patients were readmitted within 30 days and 538 patients were readmitted within 60 days, producing imbalance ratios of ρ = 5.3 and ρ = 3.8 , respectively. The second data set, the operating room pilot data (ORP), contains vital signs, pre-operation data, laboratory tests, medical history, and procedure details. There are 700 records in the ORP data set, with 157 readmissions within 1 year and 124 readmissions within 30 days. The imbalance ratio of the ORP data is unclear; the authors merely state that the ORP data is less imbalanced than the GHW data ( ρ &lt; 3.8 ). A vari- ety of performance metrics, including ROC, AUC, accuracy, recall, precision, positive predictive value (PPV), and negative predictive value (NPV) are used for evaluation. The PPV and NPV metrics are equivalent to positive and negative class precision scores. The proposed CSDNN method was found to outperform existing hospital readmission prediction systems, and has been deployed at the Barnes-Jewish Hospital.</p><p>Instead of representing categorical values with traditional one-hot encoding, Wang et al. use a categorical feature embedding approach to create more meaningful representations. In addition, they employed a CNN for automatic feature extraction from time series data, i.e. the patient vital signs data. The extracted features and categorical embeddings are concatenated, forming a final input feature vector that is fed to a DNN for classification. The DNN was composed of two hidden layers, with 128 and 64 neurons per layer. The CE loss function was modified to incorporate a pre-defined cost matrix, forcing the network to minimize misclassification cost. For the GHW data set, false negative errors were given a cost 2× the cost of false positive errors. Similarly for the ORP data set, the false negative cost was set to 1.5× the cost of false positive errors.</p><p>Wang et al. 's CSDNN method is compared to five baseline classifiers, including three decision tree methods, a SVM, and an ANN. Only one of the baseline methods addresses class imbalance, using under-sampling with a Random Forest (RF) classifier. The proposed method outperformed all of the baseline classifiers across all performance metrics except for NPV. We were able to observe the class-wise performance trade-off because multiple complementary performance metrics were reported. On the GHW data set, the CSDNN's AUC of 0.70 outscored the runner up's (ANN classifier) AUC of 0.62. Unfortunately, however, it is not possible to determine if the cost-sensitive loss function is the cause of the improved performance, as there are several other factors that may have contributed to improvements, e.g. categorical feature embedding and time-series feature extraction. A baseline that includes the CNN feature extractor and the categorical embeddings is required to isolate the efficacy of the cost-sensitive loss function.</p><p>Incorporating the cost matrix into the CE loss is a minor implementation detail that should generalize well to other domains and architectures with minimal impact on training times. The process of identifying an ideal cost matrix is likely the biggest limitation. In traditional machine learning problems with relatively small data sets, models can be validated across a range of costs and the best cost matrix can be selected for the final model. When working with DNNs and large data sets, however, this process of searching for the best cost parameters can be very time consuming and even impractical. Including results from a range of cost matrices in future work will help to demonstrate the classwise performance trade-offs that occur as costs vary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning cost matrices with cost-sensitive CNN (CoSen)</head><p>Khan et al. <ref type="bibr" target="#b18">[19]</ref> introduced an effective cost-sensitive deep learning procedure which jointly learns network weight parameters and class misclassification costs during training. The proposed method, CoSen CNN, is evaluated against six multi-class data sets with varying levels of imbalance: MNIST, CIFAR-100, Caltech-101 <ref type="bibr" target="#b102">[104]</ref>, MIT-67 <ref type="bibr" target="#b103">[105]</ref>, DIL <ref type="bibr" target="#b104">[106]</ref>, and MLC <ref type="bibr" target="#b105">[107]</ref>. Class imbalance ratios of ρ = 10 are tested for the MNIST, CIFAR-100, Caltech-101, and MIT-67 data sets. The DIL and MLC data sets have imbalance ratios of ρ = 13 and ρ = 76 , respectively. The VGG-16, pre-trained on ImageNet data, is used as a feature extractor and the baseline CNN throughout the experiments.</p><p>The cost matrix that is learned by the CoSen CNN is used to modify the output of the VGG-16 CNN's last layer, giving higher importance to samples with higher cost. The group presents three modified loss functions, incorporating the learned cost parameters into MSE loss, SVM hinge loss, and CE loss. The training process learns network weight parameters and misclassification cost parameters by keeping one fixed at a time, and minimizing cost with respect to the other during training. The cost matrix update is dependent on the current classification errors, the overall classification error, and the class-to-class separability (C2C). The C2C separability measures relationships between within-class sample distances and the size of class-separating boundaries. The CoSen CNN is evaluated against the baseline CNN, multiple sampling methods, and multiple cost-sensitive methods. A two-layered neural network was used for the sampling classification methods, and a SVM and RF were used for the cost-sensitive methods. The SVM and RF baseline classifiers used the features that were extracted by the pre-trained VGG-16 CNN as input. The SOSR CNN is a cost-sensitive deep learning method that incorporates a fixed cost matrix into the loss function <ref type="bibr" target="#b106">[108]</ref>.</p><p>Overall accuracy was used to show that the proposed CNN outperforms all seven alternative techniques across all data sets. Table <ref type="table" target="#tab_11">10</ref> shows that the CoSen CNN performed exceptionally well, outperforming the runner-up classifier by more than 5% on CIFAR-100, Caltech-101, and MIT-67. The second best classifier listed in the experimental results is between the SOSR CNN and the baseline CNN. In all cases SMOTE outperformed RUS, and hybrid sampling method SMOTE-RSB <ref type="bibr" target="#b107">[109]</ref> outperformed SMOTE. Unfortunately, with accuracy being the only performance metric reported between all seven class imbalance methods, and accuracy being unreliable in cases of class imbalance, these results may be misleading.</p><p>F1 and G-Mean scores were reported to show that the proposed CoSen CNN outperforms the baseline CNN on all data sets, e.g. increasing the F1-score from 0.389 to 0.416 on the Caltech-101 data set. Khan et al. also included sample network training times, showing that the added cost parameter training increases each training epoch by several seconds but has little to no impact on the inference step. In another experiment, the authors defined three fixed cost matrices using class representation, data separability, and classification errors to derive the costs, and showed that the dynamic cost matrix method outperforms all three cases.</p><p>It is interesting that the baseline CNN, with no class imbalance modifications, is a close runner-up to the CoSen CNN, outperforming the sampling methods, SVM, and RF classifiers in all cases. This does not imply that a deep CNN with no class imbalance modifications is better equipped to address class imbalance than traditional sampling methods. Rather, this demonstrates the power of re-using strong feature extractors that have been trained on large volumes of data, e.g. over 1.3 million images in this experiment.</p><p>As mentioned previously, one of the difficulties of cost-sensitive learning is choosing an appropriate cost matrix, often requiring a domain expert or a grid search procedure. The CoSen method proposed by Khan et al. removes this requirement, and offers more of an end-to-end deep learning framework capable of learning from class imbalanced data. The authors report results on a number of experiments across a variety of data sets, and the joint learning of network parameters and cost parameters appears to be an excellent candidate for learning from class imbalanced data. Future experiments should explore this cost-sensitive method's ability to learn from problems containing alternative data types, big data, and class rarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost-sensitive DBN with differential evolution (CSDBN-DE)</head><p>Zhang et al. <ref type="bibr" target="#b88">[90]</ref> set out to automatically learn costs for the purpose of cost-sensitive deep learning. More specifically, they use a differential evolutionary algorithm <ref type="bibr" target="#b108">[110]</ref> to improve the cost matrix each training iteration, and incorporate these learned costs into a DBN. The proposed cost-sensitive deep belief network with differential evolution (CSDBN-DE) is evaluated against 42 data sets from the Knowledge Extraction based on Evolutionary Learning (KEEL) <ref type="bibr" target="#b109">[111]</ref> repository. The training data sets are structured data sets, ranging in size from 171 samples to 3339 samples and containing between 6 and 13 attributes each. Imbalance ratios range from ρ = 9.28 up to ρ = 128.21.</p><p>The DBN pre-training phase follows the original layer-wise training method first proposed by Hinton et al. <ref type="bibr" target="#b63">[65]</ref>. The cost matrix is then incorporated into the output layer's softmax probabilities during the fine-tuning phase. Cost matrices are first randomly initialized, then training set evaluation scores are used to select a new cost matrix for the next population. Mutation and cross-over operations are applied to evolve and generate the next population of cost matrices. Once training is complete, the best cost matrix is selected and applied to the output layer of the DBN, forming the final version of the cost-sensitive DBN to be used for inference.</p><p>Accuracy and error rate are used to compare the proposed method to an extreme learning machine (ELM) <ref type="bibr" target="#b110">[112]</ref>. The ELM network is a type of single-hidden layer feedforward network that does not use backpropagation and is known to train thousands of times faster than neural networks. Experiments are repeated 20 times and results are averaged. The proposed CSDBN-DE model outperformed the ELM network on 28 out of 42 data sets, i.e. 66% of the experiments.</p><p>Similar to the CoSen CNN <ref type="bibr" target="#b18">[19]</ref>, the biggest advantage of the CSDBN-DE is its ability to learn an effective cost matrix throughout training, as this is often a difficult and timeconsuming process. Unfortunately, it is not very clear how well the CSDBN-DE handles class imbalanced data, because it is compared to a single baseline built on a completely different architecture. Also, the accuracy performance metric provides no real insight into the network's true performance when class imbalance is present. The concept of incorporating an evolutionary algorithm to iteratively update a cost matrix shows promise, but more robust experiments are required to validate its ability to classify imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output thresholding</head><p>In addition to the data sampling methods discussed in the "ROS, RUS, and two-phase learning" section, Buda et al. <ref type="bibr" target="#b22">[23]</ref> experimented with adjusting CNN output thresholds to improve overall performance. They used the MNIST and CIFAR-10 data sets with varying levels of class imbalance ratios in the range of ρ ∈ [1, 5000] and ρ ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">50]</ref> , respectively. Accuracy scores were used to compare thresholding with the baseline CNN, ROS, and RUS methods as they were described in the "ROS, RUS, and two-phase learning" section.</p><p>The authors applied thresholding by dividing the network outputs for each class by its estimated prior probability, effectively reducing the likelihood of misclassifying examples from the minority group. They also considered hybrid methods by combining threshold moving with RUS and ROS. The thresholding method outperformed the baseline CNN for all levels of class imbalance on the MNIST and CIFAR-10 data sets. Thresholding outperformed ROS and RUS for all levels of imbalance on the MNIST data, but on the CIFAR-10 data there were several instances were ROS outperformed thresholding. Thresholding combined with ROS performed especially well, outperforming all other methods in nearly all cases.</p><p>As discussed in the "ROS, RUS, and two-phase learning" section, Buda et al. explored a variety of deep learning methods for addressing a wide range of class imbalance levels. They have shown that overall accuracy can be improved with threshold moving, and that it can be implemented relatively easily with prior class probabilities. Unfortunately, the accuracy score does not explain individual class-wise performances and trade-offs. Since thresholding is only applied during inference, it does not affect training times. This also means that thresholding does not impact weight tuning and therefore does not improve a model's ability to discriminate between classes. Regardless, it is still an appropriate method for reducing majority class bias that can be quickly implemented on top of an already trained network to improve classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category centers</head><p>Zhang et al. <ref type="bibr" target="#b89">[91]</ref>  Zhang et al. observe that similar images of the same class tend to cluster well in CNN deep feature space. They discuss the decision boundary that is created by the final layer of the CNN, the classification layer responsible for separating these deep feature clusters, stating that there is a greater chance for large errors in boundary placement when class imbalance is present. To avoid this boundary error in scenarios of class imbalance, they propose using high-level features extracted by the CNN to calculate each class's centroid in deep feature space. These category centers in deep feature space can then be used to classify new images, by assigning new images to their nearest deep feature  <ref type="table" target="#tab_12">11</ref>. The authors tried using two different CNN layers for feature extraction, the last convolutional layer (B) and the last fully connected layer (C), to determine if one set of features performed better. Finally, over-sampling was applied to the baseline and the category centers methods to determine the impact of data sampling on CC (D-F). These results show that the CC methods (B, C) outperform the baseline CNN (A) on mean precision for all three imbalance scenarios. Results also show that the addition of over-sampling (D-F) led to improved results on the CIFAR-10 distributions, with the exception of Dist. C. For example, the CC method with over-sampling (E) increased mean precision on Dist. A from a baseline (A) of 0.779 to 0.830. CIFAR-100 results were similar, in the sense that the proposed method (B, C) outperformed the baseline (A) for all three distributions. Unlike the CIFAR-10 data, interestingly, over-sampling did not improve the results of the proposed method when classifying the CIFAR-100 distributions, but it did improve the baseline CNN.</p><p>We believe that the biggest limitation to the CC method is that it is highly dependent on the DNN's ability to generate discriminative features that cluster well. If a large volume of class balanced, labelled training data is not available to pre-train the DNN, deep feature boundaries may not be strong enough to apply the CC method. The specifics of the over-sampling method used in methods (D-F) are unknown, so we do not know if over-sampling was applied to a level of class balance or some other ratio. In addition, we do not know if these results are the average of several rounds of experiments, or are just results from a single instance. With no balanced distribution results, an unclear oversampling method, and only one performance metric, it is difficult to understand how well the proposed CC method does in learning from class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Very-deep neural networks</head><p>Ding et al. <ref type="bibr" target="#b90">[92]</ref> experimented with very-deep CNN architectures, e.g. 50 layers, to determine if deeper networks perform better on imbalanced data. The authors observe in literature that the error surfaces of deeper networks have better qualities for training convergence than smaller sized networks <ref type="bibr" target="#b111">[113,</ref><ref type="bibr" target="#b112">114]</ref>. They claim that larger networks contain more local minimum with good performance, making acceptable solutions easier to locate with gradient descent. The networks are tested on the problem of Facial Action Units (FAUs) recognition, i.e. the detection of basic facial actions as defined by the Facial Action Coding System <ref type="bibr" target="#b113">[115]</ref>.</p><p>The EmotioNet Challenge Track 1 data set <ref type="bibr" target="#b114">[116]</ref> is used to compare methods. From the original data set, with over one million images, 450,000 images were randomly selected for training and 40,000 images were randomly selected for validation. The images in this data set contain 11 possible FAUs. Since an image can be positive for more than one FAU, the authors treated the classification problem as a set of 11 binary problems. Several FAUs are present in less than 1% of the data set, e.g. nose wrinkler, chin raiser, and upper lid raiser. The lip stretcher FAU is only present in 0.01% of the data, creating a max imbalance ratio to ρ = 10, 000 . Six deep CNN architectures are compared, all of which include 3 × 3 convolution filter layers and are trained for 100 epochs: F1-scores from the first experiment are presented in Table <ref type="table" target="#tab_13">12</ref>. The shallow network (A) was unable to capture classes containing high imbalance, e.g. AU2-AU5, during the 100 training epochs. The non-residual 34-layer CNN (B) saw a large boost in performance compared to the shallow network (A), with average F1-score increasing from 0.29 to 0.48. It can also be observed that the 10-layer ResNet (C) achieved an equal F1-score of 0.48. The FAU with the largest imbalance, i.e. lip stretcher (AU20), received an F1-score of 0.0 in all experiments. There is no noticeable difference in F1-scores between the networks with 10 layers or more (B-F). The 34-layer ResNet (E) won first place at the first track EmotioNet Challenge in 2017 <ref type="bibr" target="#b114">[116]</ref>.</p><p>In a second experiment, Ding et al. compare the convergence rate of a very-deep 18-layer CNN to a shallower 6-layer CNN across 100 epochs. The experiment shows that the very-deep 18-layer CNN converges faster than the shallower network, as the error decreases at a faster rate. The authors suggest that the shallower network will continue to converge, provided additional epochs.</p><p>The experiments by <ref type="bibr">Ding et al.</ref> show that additional hidden layers can increase the convergence rate on facial action recognition. Training very-deep neural networks With the convergence rate in question, other methods that have been shown to impact convergence rates should be included in future studies, e.g. alternative optimization methods, dynamic learning rates, and weight initialization. Unlike data sampling methods, the algorithm-level methods presented do not alter the training data and do not require any pre-processing steps. Compared to ROS, which was recommended by multiple authors in the "Data-level methods" section, algorithm-level methods are less likely to impact training times. This suggests that algorithm-level methods may be better equipped for big data problems. With the exception of defining misclassification costs, the algorithm-level methods require little to no tuning. Fortunately, two methods were presented for automatically learning cost parameters. Methods which are able to adapt to different problems with minimal tuning are preferred, as they can be quickly applied to new problems and do not require specific domain knowledge. The focal loss function and CoSen CNN demonstrate this flexibility, and we believe they will generalize well to many complex problem domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of algorithm-level methods</head><p>In general, there is a lack of research that appropriately compares deep learning algorithm-level methods to alternative class imbalance methods. This is due to poor choices in baseline models, insufficient performance metrics, and domain-specific experiments that fail to isolate the proposed class imbalance method. Similar to the surveyed deep learning data-level methods, most of the methods in this section were evaluated on image data with deep CNNs. We are most interested in understanding how deep learning methods for addressing class imbalance compare to each other and to traditional machine learning techniques for class imbalance. We believe that filling these research gaps and properly evaluating these methods will have a great impact on future deep learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid-methods</head><p>The deep learning techniques for addressing class imbalance in this section combine algorithm-level and data-level methods. Huang et al. <ref type="bibr" target="#b21">[22]</ref> use a novel loss function and sampling method to generate more discriminative representations in their Large Margin Local Embedding (LMLE) method. Ando and Huang <ref type="bibr" target="#b115">[117]</ref> presented the first deep feature over-sampling method, Deep Over Sampling (DOS). Finally, class imbalance in large-scale image classification is addressed by Dong et al. <ref type="bibr" target="#b116">[118]</ref> with a novel loss function and hard sample mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large Margin Local Embedding (LMLE)</head><p>Huang et al. <ref type="bibr" target="#b21">[22]</ref> proposed the LMLE method for learning more discriminative deep representations of imbalanced image data. The method is motivated by the observation that minority groups are sparse and typically contain high variability, allowing the local neighborhood of these minority samples to be easily invaded by samples of another class. By combining a new informed quintuplet sampling method with a new tripleheader hinge loss function, deep feature representations that preserve same class locality and increase inter-class discrimination are learned from imbalanced image data. These deep feature representations, which form well-defined clusters, are then used to label new samples with a fast cluster-wise K-NN classification method. The proposed LMLE method is shown to achieve state-of-the-art results on the CelebA <ref type="bibr" target="#b117">[119]</ref> data set, which contains high imbalance levels up to ρ = 49.</p><p>The quintuplet sampling method selects an anchor and four additional samples based on inter-class and intra-class cluster distances. During training, each mini-batch selects an equal number of quintuplets from both the minority and majority classes. The five samples obtained by the quintuplet sampling method are fed to five identical CNNs, and their outputs are aggregated into a single result. The triple-header hinge loss is then used to compute the error and update the network parameters accordingly. This regularized loss function constrains the deep feature representation such that clusters collapse into small neighborhoods with appropriate margins between inter-class and intra-class clusters.</p><p>The CelebA data set contains facial images annotated with 40 attributes, with imbalance levels as high as ρ = 49 (Bald vs not Bald). A total of 160,000 images are used to train a CNN, and the learned deep representations are then fed to a modified K-NN classifier. The Balanced Accuracy (Eq. 9) metric is calculated for each facial attribute. The LMLE-kNN model is compared to three state-of-the-art models in the facial attribute recognition domain: Triplet-kNN <ref type="bibr" target="#b118">[120]</ref>, PANDA <ref type="bibr" target="#b119">[121]</ref>, and ANet <ref type="bibr" target="#b120">[122]</ref>. Results in Table <ref type="table" target="#tab_1">13</ref> show that the LMLE-kNN performs as good as, or better than, alternative methods for all facial attributes. Performance gains over alternative methods increased as levels of imbalance increased, e.g. LMLE increased accuracy on the Bald attribute from 75 to 90 when compared to its runner-up. The authors achieved similar results when comparing the LMLE-kNN to 14 state-of-the-art models on the BSDS500 <ref type="bibr" target="#b121">[123]</ref> edge detection data set. Huang et al. were one of the first to study deep representation learning with class imbalanced data. The proposed LMLE method combines several powerful concepts, achieves state-of-the-art results on a popular image data benchmark, and shows potential for future works in other domains. The high-performance results do come with a cost, however, as the system is both complex and computationally expensive. The data must be pre-clustered before quintuplet sampling can take place, which may be difficult if a suitable feature extractor is not available for the given data set. The authors do suggest that the initial clustering is not important, because the learning procedure can be used to re-cluster the data throughout training. Furthermore, each forward pass through the system requires computation from five CNNs, one for each of the quintuplets sampled. We are in agreement with the statement made by Ando and Huang <ref type="bibr" target="#b115">[117]</ref>, that adapting LMLE to new problems will require many task and model specific configurations. This will likely deter many practitioners from using this method when trying to solve class imbalanced problems, as most will resort to simpler and faster methods. Results provided by Dong et al. in the "Class rectification loss (CRL) and hard sample mining" section will show that LMLE outperforms ROS, RUS, thresholding, and costsensitive learning on the CelebA data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep over-sampling (DOS)</head><p>Ando and Huang <ref type="bibr" target="#b115">[117]</ref> introduced over-sampling to the deep feature space produced by CNNs in their DOS framework. The proposed method is extensively evaluated by generating imbalanced data sets from five popular image benchmark data sets, including MNIST, MNIST-back-rot, SVHN <ref type="bibr" target="#b122">[124]</ref>, CIFAR-10, and STL-10 <ref type="bibr" target="#b123">[125]</ref>. Class-wise precision and recall, F1-scores, and AUC are used to evaluate the DOS framework against alternative methods.</p><p>The DOS framework consists of two simultaneous learning procedures, optimizing the lower layer and upper layer parameters separately. The lower layers are responsible for acquiring the embedding function, while the upper layers learn to discriminate between classes using the generated embeddings. In order to learn the embedding features, the CNN's input is presented with both a class label and a set of deep feature targets, an in-class nearest neighbor cluster from deep feature space. Then the micro-cluster loss computes the distances between each of the deep feature targets and their mean, constraining the optimization of the lower layers to shift deep feature embeddings towards the class mean. Ando and Huang state that shifting target representations to their corresponding local mean will induce smaller in-class variance and strengthen class distinction in the learned representations. The upper layers used for discriminating between classes are trained by taking the weighted sum of the CE losses, i.e. the CE loss for each deep feature target.</p><p>The deep over-sampling component is the process of selecting k in-class neighbors from deep feature space. In order to address class imbalance, the number of inclass neighbors to select should vary between classes. For example, using k = 3 for the minority class, and k = 0 for the majority class, will supplement the minority class with additional embeddings while leaving the majority class as is.</p><p>In the first DOS experiment, they compare the DOS framework with two alternative methods for handling class imbalance, Large Margin Local Embedding (LMLE) <ref type="bibr" target="#b21">[22]</ref> and Triplet re-sampling with cost-sensitive learning (TL-RS-CSL). MNIST back-rotation images were used to create three data sets with class reduction rates of 0%, 20%, and 40%. It was shown that the DOS framework outperformed TL-RS-CSL and LMLE for reduction rates of 20% and 40%, and that DOS performance deteriorates slower than both TL-RS-CSL and LMLE as imbalance levels increase. For example, on imbalanced MNIST-back-rot data (40% reduction), DOS scored an average class-wise recall of 75.43, compared to the LMLE score of 70.13.  In a second experiment, the DOS framework is compared to a basic CNN and a K-NN classifier that is trained with CNN deep features (CNN-CL). Three different data sets are used to generate imbalanced data sets with reduction rates of 90%, 95%, and 99%, producing imbalance ratios up to ρ = 100 . The results in Table <ref type="table" target="#tab_15">14</ref> show again that the improved performance of the DOS framework becomes more apparent as the level of class imbalance increases. The third level of imbalance, with reduction rates of 99%, show that the DOS framework clearly outperforms both the CNN and CNN-CL across all data sets and all performance metrics.</p><p>The final experiment uses balanced data sets to examine the sensitivity of DOS parameter k, which defines the number of deep feature in-class neighbors to sample. Table <ref type="table" target="#tab_16">15</ref> shows that when k is increased to 10 the performance on the minority group begins to deteriorate. These results also show that the proposed DOS framework outperforms the baseline CNN and CNN-CL on balanced data sets.</p><p>The DOS method displayed no visible performance trade-offs between the majority and minority groups, or between precision and recall. Its ability to outperform alternative methods when class imbalance is not present is also very appealing, as this quality is not common among class imbalance learning methods. Most importantly, some of the performance gains observed when comparing the CNN or CNN-CL to the DOS model on the minority group were very large, e.g. MNIST-back-rot and SVHN F1-scores increasing from 0.43 to 0.66 and 0.42 to 0.64, respectively, under an imbalance reduction rate of 99%. Like other hybrid methods, DOS is more complex than more common data-level and algorithm-level methods, which may discourage statisticians from using it. We agree with Ando and Huang, however, that the DOS method is generally extendable to other domains and deep learning architectures. Future works should evaluate the DOS method in these new contexts and should compare results to other class imbalance methods presented throughout this survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class rectification loss (CRL) and hard sample mining</head><p>Dong et al. <ref type="bibr" target="#b116">[118]</ref> present an end-to-end deep learning method for addressing high class imbalance in large-scale image classification. They explicitly distinguish their work from more traditional small-scale class imbalance problems containing small levels of imbalance, suggesting that this method may not generalize to small-scale problems. Experimental results compare the proposed method against data sampling, cost-sensitive learning, threshold moving, and several relevant state-of-the-art methods. Mean sensitivity scores are used as the primary performance metric by averaging together class-wise recall scores. The proposed method combines hard sample mining from the minority groups with a regularized objective function, the class rectification loss (CRL).</p><p>The hard sample mining selects minority samples which are expected to be more informative for each mini-batch, allowing the model to learn more effectively with less data. Dong et al. strive to rectify, or correct, the class distribution bias in an incremental manner, resulting in progressively stronger minority class discrimination through training. Unlike LMLE <ref type="bibr" target="#b21">[22]</ref>, which attempts to strengthen the structure of both majority and minority groups, this method only enhances minority class discrimination. To identify the minority classes, all classes are sorted by their size and then selected from smallest to largest until they collectively sum up to at most half the size of the data set, ensuring     </p><note type="other">69 89 88 89 83 95 89 77 72 72 76 86 66 76 24 73 81 76 76 15 93 LMLE-kNN 88 96 99 99 92 99 98 83 68 72 79 92 60 80 87 73 87 73 83 96 98 CRL 81 94 92 95 87 98 90 79 66 71 80 88 67 77 83 72 84 79 84</note><note type="other">92 84 62 71 82 83 76 95 82 89 81 89 78 95 83 85 91 86 93 79 LMLE-kNN 99 82 59 59 82 76 90 98 78 95 79 88 59 99 74 80 91 73 90 84 CRL 95 84 73 73 89 88 87 99 90 95 87 95 86 99 89 92 96 93 99 87</note><p>that the minority classes account for at most half the training batch. Both class-level and instance-level hardness metrics are considered when performing hard sample mining. At the class-level, hard positives refer to weak recognitions, the correctly labelled samples with low prediction scores. Alternatively, hard negatives are the obvious mistakes, the samples incorrectly labelled with high prediction scores. At the instance-level, hard positives are defined by correctly labelled images with far distances in feature space distance, while hard negatives are the images incorrectly labelled which are close in feature space.</p><p>The CRL loss function (Eq. 17), where α = η� imb is linearly proportional to the level of class imbalance, places a batch-wise class balancing constraint on the optimization process. This reduces the learning bias caused by the majority group's over-representation. The CRL regularization imposes an imbalance-adaptive learning mechanism, applying more weight to the more highly imbalanced labels, while reducing the weight for the less imbalanced labels. Three different loss criteria L crl are explored through experimentation, where the Triplet Ranking loss <ref type="bibr" target="#b124">[126]</ref> is selected as the default for many of the experiments.</p><p>Experiments are conducted by extending state-of-the-art CNN architectures with the proposed method and performing classification on three benchmark data sets. The Cel-ebA data set contains a max imbalance level of ρ = 49 . The X-Domain <ref type="bibr" target="#b125">[127]</ref> data set contains 245,467 retail store clothing images that are annotated with 9 multi-class attribute labels, 165,467 of which are set aside for training. The X-Domain contains extreme class imbalance ratios of ρ &gt; 4000 . Several imbalanced data sets are generated from the CIFAR-100 set, with imbalance ratios up to ρ = 20 , to demonstrate how CRL handles increasing levels of imbalance.</p><p>Table <ref type="table" target="#tab_17">16</ref> contains the CelebA facial attribute recognition results. All class imbalance methods presented were implemented on top of the 5-layer CNN, DeepID2, <ref type="bibr" target="#b126">[128]</ref>. Referring to the bottom half of the data attributes, with imbalance ratios ρ ≥ 6 , under-sampling almost always performs worse than over-sampling, cost-sensitive learning, and threshold moving. According to the mean sensitivity scores, over-sampling and cost-sensitive learning perform the same, outperforming thresholding and under-sampling by 3% and 4%, respectively.</p><p>When compared to the best non-imbalanced learning method (DeepID2) and the best imbalanced learning method (LMLE), the proposed CRL method improves the mean sensitivity by 6% and 3%, respectively. By analyzing the attribute-level scores, we can see that LMLE outperforms CRL in many cases where class imbalance levels are low, e.g. outperforming CRL on the attractive attribute by 7%. Dong et al. acknowledge that LMLE appears to handle low-level imbalance better than CRL. For many of the high-imbalance attributes, the non-imbalance method DeepID2 outperforms the LMLE method. It is when class imbalance is highest that the proposed CRL method performs its best and achieves its most significant performance gains over alternative methods.</p><p>In another experiment with the X-Domain data set, CRL outperforms LMLE on all categories, achieving a mean sensitivity performance gain over LMLE of almost 5%. Cost-sensitive learning, thresholding, and ROS all score roughly 6% below CRL, <ref type="bibr" target="#b16">(17)</ref> L bln = αL crl + (1α)L ce , α = η� imb while RUS again performs very poorly. When experimenting with balanced distributions from the CIFAR-100 data set, the CRL method is applied to three state-of-theart CNN models: CifarNet <ref type="bibr" target="#b127">[129]</ref>, ResNet32 <ref type="bibr" target="#b96">[98]</ref>, and DenseNet <ref type="bibr" target="#b128">[130]</ref>. For each model, the CRL method consistently improves upon the baseline, increasing mean sensitivity by 3.6%, 1.2%, and 0.8%, respectively. Additional cost analysis by Dong et al. shows that CRL is significantly faster than LMLE, taking just 27.2 h to train versus LMLE's 166.6 h on the given test case.</p><p>The advantages of CRL over alternative methods on large-scale class imbalanced image data were demonstrated through comprehensive experiments. It was shown to efficiently outperform many popular class imbalanced methods, including ROS, RUS, thresholding, cost-sensitive learning, and LMLE. It was also shown to train significantly faster than the LMLE method. The authors were clear to distinguish this method as effective in large-scale image data containing high levels of imbalance, and results on facial attribute detection showed that LMLE performed better for many facial attributes with lower levels of imbalance. This suggests that the CRL method may not be a good fit for class imbalance problems containing low levels of class imbalance. Despite this initial observation, CRL should be evaluated on a wide range of data sets with varying levels of complexity to better understand when it should be used. Exploring the CRL method's ability to learn from non-image data is also of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of hybrid methods</head><p>This section analyzed three hybrid deep learning methods for addressing class imbalance. The LMLE method proposed by Huang et al. outperformed several state-of-the-art models on the CelebA and BSDS500 image data sets. Ando and Huang introduced the DOS method, which learns an embedding layer that produces more discriminative features, and then supplements the minority group by over-sampling in deep feature space. DOS was shown to outperform LMLE and baseline CNNs on multiple class imbalanced image data sets. Lastly, Dong et al. 's CRL loss function was shown to outperform four state-of-the-art models, ROS, RUS, cost-sensitive learning, output thresholding, and LMLE on the CelebA data set.</p><p>The results provided by Dong et al. are most informative, as they compare their CRL method to LMLE and several other data-level and algorithm-level deep learning methods for addressing class imbalance. Not only did they show that the CRL method outperforms alternative methods as class imbalance levels increase, but they also illustrated the effectiveness of RUS, ROS, thresholding, and cost-sensitive learning. From their results on the CelebA data set, we observe that ROS and cost-sensitive learning outperform RUS and threshold moving on average. These results are consistent with those from the "Data-level methods" section, suggesting that ROS is generally a good choice in addressing class imbalance with DNNs. Dong et al. also confirmed our original observation, that LMLE is rather complex and resource intensive, by showing that CRL is capable of training 6× faster than LMLE. Comparing DOS and CRL directly will prove useful, as both methods were shown to outperform the LMLE method.</p><p>In general, hybrid methods for learning from class imbalanced data will be more complex, and more difficult to implement than algorithm-level methods and data-level methods. This is expected, since both algorithm-level and data-level methods are being combined for the purpose of improving classification performance. As learners become more complex, their flexibility and ease of use will decrease, which may make them more difficult to adapt to new problems.</p><p>Class-wise performance scores on the CelebA data set in Table <ref type="table" target="#tab_17">16</ref> show that different methods perform better when subjected to different levels of class imbalance. This observation supports our demand for future research that evaluates multiple deep learning methods across a variety of class imbalance levels and problem complexities. We expect to see similar trade-offs between models when they are evaluated across a variety of data types and DNN architectures. Filling these gaps in current research will help to shape future deep learning applications that involve class imbalance, class rarity, and big data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of surveyed works</head><p>To provide a high-level summary and better compare the current deep learning methods for class imbalance, the surveyed works and their data sets have been summarized in Tables <ref type="table" target="#tab_19">17</ref> and<ref type="table" target="#tab_21">18</ref>. The methods presented by each group have been categorized in   <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b77">79]</ref>, RUS <ref type="bibr" target="#b22">[23]</ref>, two-phase learning with sampling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, and performance-based dynamic sampling <ref type="bibr" target="#b20">[21]</ref>. The ROS experiments reported that over-sampling to the level of class balance works best on imbalanced image data. The two-phase learning method uses RUS or ROS to pre-train a DNN with class balanced data sets, then fine-tunes the network using all data. The dynamic sampling method uses class-wise F1-scores to determine sampling rates, allowing the model to sample difficult classes at a higher rate. Algorithm-level methods were explored by nine studies, and can be further broken down into new loss functions, cost-sensitive methods, output thresholding, and a deep feature 1-NN rule. MFE and MSFE loss functions <ref type="bibr" target="#b17">[18]</ref> are modifications of the MSE loss that allow the positive and negative class to equally contribute to the loss. The focal loss function <ref type="bibr" target="#b86">[88,</ref><ref type="bibr" target="#b101">103]</ref> improves classification by down-weighting easy-to-classify samples, preventing easily-classified negative samples from dominating the loss. Wang et al. modified the CE loss slightly to incorporate pre-defined class costs into the learning process, creating an optimization process that minimizes total cost. The final two cost-sensitive methods evaluated are unique in the sense that they iteratively improve the cost matrix throughout training, instead of requiring pre-defined costs. The CoSen CNN <ref type="bibr" target="#b18">[19]</ref> uses an additional loss function to learn the cost parameters through backpropagation, while the CSDBN-DE <ref type="bibr" target="#b88">[90]</ref> utilizes an evolutionary algorithm to produce increasingly better cost values. Buda et al. used class prior probabilities to adjust deep CNN output thresholds and reduce bias towards the majority group. The category centers method <ref type="bibr" target="#b89">[91]</ref> uses CNN-generated features to calculate class centers in feature space, then uses the 1-NN rule to classify new data by its nearest class centroid. Very-deep CNNs (&gt; 10 layers) were explored by Ding et al. to determine if deeper networks would converge faster than shallow networks when trained on imbalanced data.</p><p>Three hybrid methods that combine data-level and algorithm-level changes to address the class imbalance problem were compared to baselines and alternative methods. LMLE <ref type="bibr" target="#b21">[22]</ref> combines quintuplet sampling with the triple-header hinge loss to learn more discriminative features. The CRL loss function combined with hard sample mining <ref type="bibr" target="#b116">[118]</ref> was shown to improve representation learning and outperform LMLE. Ando and Huang were the first to explore over-sampling in deep feature space, showing that DOS is able to outperform LMLE on imbalanced image data.</p><p>Two-phase learning <ref type="bibr" target="#b19">[20]</ref> and dynamic sampling <ref type="bibr" target="#b20">[21]</ref> both outperformed transfer learning and augmentation methods in classifying imbalanced image data. Buda et al. presented conflicting results, however, that suggested that plain ROS performs better than both two-phase learning and RUS. Dong et al. showed that ROS and cost-sensitive learning methods perform equivalently on the CelebA dataset, both outperforming RUS and thresholding methods. Experiments by Khan et al., evaluated over six data sets, showed that a cost-sensitive CNN can outperform data sampling methods with neural networks and cost-sensitive SVM and RF learners. Wang et al. deployed a state-of-theart hospital readmission predictor built using a cost-sensitive DNN that outperformed existing models. A class balanced version of the MSE loss, MFE and MSFE <ref type="bibr" target="#b17">[18]</ref>, showed improvements in classifying imbalanced image and text data. In <ref type="bibr" target="#b115">[117]</ref>, over-sampling in the deep feature space is used to perform classification on five data sets, showing that the DOS framework can outperform LMLE and TL-RS-CSL. The focal loss <ref type="bibr" target="#b86">[88]</ref> function was shown to outscore leading one-stage and two-stage detectors on the COCO data set. The CRL loss <ref type="bibr" target="#b116">[118]</ref> outperformed LMLE, data sampling, cost-sensitive learning, and thresholding on the CelebA facial attribute detection task.</p><p>Multiple authors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b77">79]</ref> suggested the use of deep learning with ROS to address class imbalance, showing that ROS outperforms RUS in almost all cases. Buda et al. and Hensman and Masko both suggest applying ROS until class imbalance is eliminated in the training set, but experiments did not consider big data scenarios. We believe that applying ROS to this level will not hold when subjected to big data or class rarity. Duplicating such large volumes of data may cause over-fitting and is very computationally expensive. In a contrastive example, Bauder et al. <ref type="bibr" target="#b129">[131]</ref> found RUS to be more effective than ROS when using traditional machine learning algorithms to detect fraud in big data with class rarity. Future work should consider these big data scenarios, and should experiment with RUS methods that remove duplicates and strengthen class boundaries.</p><p>Almost all (80%) of the surveyed works employed deep CNN architectures to address class imbalanced image data. Yet, all of the methods presented can be extended to non-CNN architectures and non-image data. The popularity of CNNs, image classification, and object detection in the research community can be partly attributed to popular benchmark data sets like MNIST and CIFAR, and the continuous improvements being driven by competitive events like LSVRC. Class imbalance is not limited to image data, and additional work must be done to evaluate the use of these deep learning class imbalance methods in other domains.</p><p>Approximately half of the studies used only one data set when evaluating their deep learning methods for addressing class imbalance. The authors are not at fault for this, as most were focused on solving a specific problem or benchmark task. However, more comprehensive studies that evaluate these methods across a wider range of data sets, with varying levels of class imbalance and complexity, will better demonstrate their strengths and weaknesses. Also, only a third of the studies indicate the number of rounds or repetitions executed for each experiment. In other words, the remaining groups either did not perform multiple runs, or failed to include those details and presented the most favorable results. To their defense, training a deep learning model on a large data set can take days or even weeks, making it difficult to conduct several rounds of experiments. This creates several avenues for future research, as comparing various deep learning methods on many data sets with repetition will increase confidence in results and help guide future practitioners in model selection.</p><p>Less than half of the surveyed works experimented with data sets larger than 100,000 samples. Lee et al. were the only ones to work with more than a million samples containing imbalanced data. Pouyanfar et al. provided a big data analytics workflow to classify fast-streaming network camera data, but their experiment was limited to just 10,000 images. This is an extremely important area of research that demands attention if we expect deep learning to provide big data analytics solutions. It is also very likely that the methods proposed throughout this survey, e.g. ROS and RUS, will behave very differently in the context of big data.</p><p>It is not currently possible to compare the methods which have been presented directly, as they are evaluated across a variety of data sets with varying levels of class imbalance, and results are reported with inconsistent performance metrics. In addition, some studies report contradictory results, further suggesting that performance is highly dependent on problem complexity, class representation, and the performance metrics reported. Overall, there is a lack of evidence which distinguishes any one deep learning method as superior for learning from class imbalanced data, and additional experiments are required before such conclusions can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>To the best of our knowledge, this survey provides the most comprehensive analysis of deep learning methods for addressing the class imbalance data problem. Fifteen studies published between 2015 and 2018 are summarized and discussed, exploring a number of advanced techniques for learning from imbalanced data with DNNs. It has been shown that traditional machine learning techniques for handling class imbalance can be extended to deep learning models with success. The survey also finds that nearly all research in this area has been focused on computer vision tasks with CNNs. Despite a growing demand for big data analytics solutions, there is very little research that properly evaluates deep learning in the context of class imbalance and big data. Deep learning from class imbalanced data is still largely understudied, and statistical evidence which compares newly published methods across a variety of data sets and imbalance levels does not exist.</p><p>Several areas for future work are apparent. Applying the newly proposed methods to a larger variety of data sets and class imbalance levels, comparing results with multiple complementary performance metrics, and reporting statistical evidence will help to identify the preferred deep learning method for future applications containing class imbalance. Experimenting with deep learning methods for addressing class imbalance in the context of big data and class rarity will prove valuable to the future of big data analytics. More work is required with non-convolutional DNNs to determine if the methods presented will generalize well to alternative architectures, e.g. MLPs and RNNs. Finally, research that evaluates the use of deep learning to address class imbalance in non-image data is limited and should be expanded upon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1Shallow MLP vs deep MLP<ref type="bibr" target="#b55">[57]</ref> </figDesc><graphic coords="10,128.22,95.21,340.08,114.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>A) Full: CNN trained with original imbalanced data set. (B) Noise: CNN trained with augmented data, where minority classes are duplicated through noise injection until all classes contain at least 1000 samples. (C) Aug: CNN trained with augmented data, where minority classes are duplicated through rotation, scaling, translation, and flipping of images until all classes contain at least 1000 samples. (D) Thresh: CNN trained with thresholded data, generated through random undersampling until all classes have at most 5000 samples. (E) Noise + full: CNN pre-trained with the noise-augmented data from (B), then finetuned using the full data set. (F) Aug + full: CNN pre-trained with the transform-augmented data from (C), then fine-tuned using the full data set. (G) Thresh + full: CNN pre-trained using the thresholded data set from (D), then finetuned using the full imbalanced data set. Results from Lee et al. 's experiments are presented in Table 5, where the L5 and Rest columns are the unweighted average F1-scores for the largest five classes and the remaining minority classes, respectively. Comparing methods (B-D) shows that under-sampling (D) outperforms both noise injection (B) and augmentation (C) oversampling methods on the given data set for all classes. Methods (B-D) saw moderate improvements on the Rest group when compared to the baseline (A), but the L5 group suffered sufficient loss in performance. Re-training the model with the full data set in (E-G) allowed the models to re-capture the distribution of the L5 group. The proposed model (G) achieved the highest F1-score (0.7791) on the L5 group, and greatly improved the F1-score of the Rest group from 0.1548 to 0.3262 with respect to the baseline. The two-phase learning procedure presented by Lee et al. has proven effective in increasing the minority class performance while still preserving the majority class performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>A) Basic CNN: VGGNet [85] CNN trained on entire data set. (B) Deep CNN features + SVM: Support vector machine (SVM) classifier trained with deep features generated by CNN. (C) Transfer learning without augmentation: Fine-tuned Inception-V3 with no data augmentation. (D) Transfer learning with augmentation: Fine-tuned Inception-V3 with data augmentation. (E) Transfer learning with balanced augmentation: Fine-tuned Inception-V3 with data augmentation that enforces class balanced training batches with over-sampling and under-sampling. (F) Proposed model: Dynamic sampling, data augmentation, and transfer learning on Inception-V3 network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>loss function to alternative class imbalance methods across a variety of data sets and class imbalance levels. Nemoto et al. [103] later used the focal loss in another image classification task, the automated detection of rare building changes, e.g. new construction. The airborne building images are annotated with the labels: no change, new construction, rebuilding, demolished, repaint roofs, and laying solar panel. The training data contains 203,358 images in total, where 200,000 comprise the negative class, i.e. no change. The repaint roofs and laying solar panel classes contain just 326 and 222 images, respectively, yielding class imbalance ratios as high as ρ = 900. The experiments by Nemoto et al. utilize the VGG-16 [85] CNN architecture, where the baseline CNN uses the standard CE loss. Images were augmented by rotation and inversion, creating 20,000 samples per class. Class-wise accuracy on the validation set was used to compare the focal loss to the CE loss. The first experiment uses the CE loss function and shows that the accuracy on the repaint roof and laying solar panel classes begins to deteriorate after 25,000 iterations, suggesting over-fitting. In the second experiment, focal loss was evaluated using the same VGG-16 architecture and the same image augmentation procedure, generating 20,000 images per class. Unlike the first experiment, however, only three classes were selected for training and validation: no change, repaint roof, and laying solar panel images. The value of the focal loss's down-weighting parameter γ was varied in the range γ ∈ [0, 5] to better understand its impact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>experimented with deep representation learning of class imbalanced image data from the CIFAR-10 and CIFAR-100 data sets. They present a method for addressing class imbalance, category centers (CC), that combines transfer learning, deep CNN feature extraction, and a nearest neighbor discriminator. Three class imbalanced distributions are created from each original data set through random under-sampling, i.e. Dist. A, Dist. B, and Dist. C. In Dist. A and Dist. B, half of the classes are reduced in size, creating imbalance levels of ρ = 10 and ρ = 20 , respectively. In Dist. C, class reduc- tion levels increase linearly across all classes with a max imbalance of ρ = 20 . For exam- ple, Dist. C for the CIFAR-100 data set contains 25 images in each of the first 10 classes, 75 images in each of the next 10 classes, then 125 images in each of the next 10 classes, etc. The proposed model is compared to both a baseline CNN and an over-sampling method. The mean precision performance metric is used to compare results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>A) Handcraft: 6-layer CNN (B) Plain34: 34-layer CNN (C) Res10: 10-layer ResNet CNN (D) Res18: 18-layer ResNet CNN (E) Res34: 34-layer ResNet CNN (F) Res50: 50-layer ResNet CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 13 LMLE</head><label>13</label><figDesc>CelebA facial attribute recognition (Balanced Accuracy)<ref type="bibr" target="#b21">[22]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>and italic scores indicate best and second best class scores, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Confusion matrix Actual positive Actual negative</head><label>1</label><figDesc></figDesc><table><row><cell>Predicted positive</cell><cell>True positive (TP)</cell><cell>False positive (FP)</cell></row><row><cell>Predicted negative</cell><cell>False negative (FN)</cell><cell>True negative (TN)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 Imbalanced CIFAR-10 classification [79] Total Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck</head><label>3</label><figDesc></figDesc><table><row><cell>Dist. 1</cell><cell>0.73</cell><cell>0.78</cell><cell>0.84</cell><cell>0.62 0.57 0.70</cell><cell>0.62</cell><cell>0.80</cell><cell>0.76</cell><cell>0.84</cell><cell>0.80</cell></row><row><cell>Dist. 2</cell><cell>0.69</cell><cell>0.74</cell><cell>0.75</cell><cell>0.58 0.33 0.58</cell><cell>0.65</cell><cell>0.84</cell><cell>0.78</cell><cell>0.87</cell><cell>0.79</cell></row><row><cell>Dist. 3</cell><cell>0.66</cell><cell>0.71</cell><cell>0.75</cell><cell>0.59 0.30 0.52</cell><cell>0.61</cell><cell>0.79</cell><cell>0.77</cell><cell>0.85</cell><cell>0.73</cell></row><row><cell>Dist. 4</cell><cell>0.27</cell><cell>0.78</cell><cell>0.24</cell><cell>0.12 0.08 0.19</cell><cell>0.24</cell><cell>0.33</cell><cell>0.27</cell><cell>0.21</cell><cell>0.27</cell></row><row><cell>Dist. 5</cell><cell>0.10</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00 0.00 0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Dist. 6</cell><cell>0.73</cell><cell>0.74</cell><cell>0.86</cell><cell>0.65 0.53 0.71</cell><cell>0.63</cell><cell>0.81</cell><cell>0.76</cell><cell>0.83</cell><cell>0.79</cell></row><row><cell>Dist. 7</cell><cell>0.73</cell><cell>0.75</cell><cell>0.86</cell><cell>0.66 0.52 0.71</cell><cell>0.63</cell><cell>0.80</cell><cell>0.78</cell><cell>0.84</cell><cell>0.79</cell></row><row><cell>Dist. 8</cell><cell>0.66</cell><cell>0.63</cell><cell>0.75</cell><cell>0.55 0.35 0.51</cell><cell>0.58</cell><cell>0.82</cell><cell>0.74</cell><cell>0.84</cell><cell>0.80</cell></row><row><cell>Dist. 9</cell><cell>0.10</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00 0.00 0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell></row><row><cell cols="2">Dist. 10 0.69</cell><cell>0.75</cell><cell>0.77</cell><cell>0.56 0.42 0.66</cell><cell>0.63</cell><cell>0.76</cell><cell>0.70</cell><cell>0.81</cell><cell>0.79</cell></row><row><cell cols="2">Dist. 11 0.69</cell><cell>0.74</cell><cell>0.82</cell><cell>0.58 0.44 0.59</cell><cell>0.64</cell><cell>0.80</cell><cell>0.69</cell><cell>0.83</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 Imbalanced CIFAR-10 classification with ROS [79] Total Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck</head><label>4</label><figDesc></figDesc><table><row><cell>Dist. 2</cell><cell>0.72</cell><cell>0.77</cell><cell>0.80</cell><cell>0.57 0.51 0.68</cell><cell>0.64</cell><cell>0.81</cell><cell>0.78</cell><cell>0.84</cell><cell>0.82</cell></row><row><cell>Dist. 3</cell><cell>0.73</cell><cell>0.73</cell><cell>0.80</cell><cell>0.59 0.53 0.63</cell><cell>0.65</cell><cell>0.82</cell><cell>0.81</cell><cell>0.87</cell><cell>0.83</cell></row><row><cell>Dist. 4</cell><cell>0.73</cell><cell>0.76</cell><cell>0.82</cell><cell>0.60 0.54 0.68</cell><cell>0.63</cell><cell>0.81</cell><cell>0.78</cell><cell>0.85</cell><cell>0.83</cell></row><row><cell>Dist. 5</cell><cell>0.73</cell><cell>0.80</cell><cell>0.84</cell><cell>0.61 0.55 0.68</cell><cell>0.63</cell><cell>0.82</cell><cell>0.79</cell><cell>0.82</cell><cell>0.81</cell></row><row><cell>Dist. 6</cell><cell>0.73</cell><cell>0.75</cell><cell>0.86</cell><cell>0.65 0.51 0.65</cell><cell>0.66</cell><cell>0.81</cell><cell>0.78</cell><cell>0.85</cell><cell>0.80</cell></row><row><cell>Dist. 7</cell><cell>0.73</cell><cell>0.73</cell><cell>0.85</cell><cell>0.62 0.51 0.71</cell><cell>0.66</cell><cell>0.81</cell><cell>0.79</cell><cell>0.86</cell><cell>0.80</cell></row><row><cell>Dist. 8</cell><cell>0.73</cell><cell>0.78</cell><cell>0.84</cell><cell>0.62 0.56 0.66</cell><cell>0.64</cell><cell>0.81</cell><cell>0.77</cell><cell>0.83</cell><cell>0.80</cell></row><row><cell>Dist. 9</cell><cell>0.72</cell><cell>0.73</cell><cell>0.84</cell><cell>0.58 0.55 0.68</cell><cell>0.59</cell><cell>0.79</cell><cell>0.78</cell><cell>0.83</cell><cell>0.82</cell></row><row><cell cols="2">Dist. 10 0.73</cell><cell>0.78</cell><cell>0.85</cell><cell>0.63 0.50 0.67</cell><cell>0.63</cell><cell>0.82</cell><cell>0.75</cell><cell>0.87</cell><cell>0.80</cell></row><row><cell cols="2">Dist. 11 0.72</cell><cell>0.82</cell><cell>0.87</cell><cell>0.58 0.64 0.64</cell><cell>0.49</cell><cell>0.78</cell><cell>0.69</cell><cell>0.84</cell><cell>0.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 Two-phase learning with WHOI-Plankton (Avg. F1-score) [20]</head><label>5</label><figDesc></figDesc><table><row><cell>Classifier</cell><cell>All classes</cell><cell>L5</cell><cell>Rest</cell></row><row><cell>(A) Full</cell><cell>0.1773</cell><cell>0.7773</cell><cell>0.1548</cell></row><row><cell>(B) Noise</cell><cell>0.2465</cell><cell>0.5409</cell><cell>0.3599</cell></row><row><cell>(C) Aug</cell><cell>0.2726</cell><cell>0.5776</cell><cell>0.3700</cell></row><row><cell>(D) Thresh</cell><cell>0.3086</cell><cell>0.6510</cell><cell>0.4044</cell></row><row><cell>(E) Noise + full (F) Aug + full (G) Thresh + full</cell><cell>0.3038 0.3212 0.3339</cell><cell>0.7531 0.7668 0.7791</cell><cell>0.2971 0.3156 0.3262</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 Dynamic sampling with network camera image data [21]</head><label>6</label><figDesc>All minority classes were over-sampled until class balance was achieved, where any class smaller than the largest class size is considered a minority class. In almost all experiments, over-sampling displayed the best performance, and never showed a decrease in performance when compared to the baseline. (B) RUS: All majority classes were under-sampled until class balance was achieved, where any class larger than the smallest class size is considered a majority class. RUS performed poorly when compared to the baseline models, and never displayed a notable advantage to ROS. RUS was comparable to ROS only when the total number of minority classes was very high, i.e. 80-90%. (C) Two-phase training with ROS: The model is first pre-trained on a balanced data set which is generated through ROS, and then fine-tuned using the complete data set. In general, this method performed worse than strict ROS (A). (D) Two-phase training with RUS: Similar to (C), except the balanced data set used for pre-training is generated through RUS. Results show that this approach was less effective than RUS (B). (E) Thresholding with prior class probabilities: The network's decision threshold is adjusted during the test phase based on the prior probability of each class, effectively shifting the output class probabilities. Thresholding showed improvements to overall accuracy, especially when combined with ROS.</figDesc><table><row><cell>Classifier</cell><cell>Acc.</cell><cell>Avg. F1</cell><cell>WAvg. F1</cell></row><row><cell>(A) Basic CNN</cell><cell>0.649</cell><cell>0.254</cell><cell>0.630</cell></row><row><cell>(B) Deep CNN features + SVM (C) TL + no aug. (D) TL + basic aug. (E) TL + balanced aug. (F) Proposed model</cell><cell>0.746 0.765 0.792 0.759 0.802</cell><cell>0.528 0.432 0.502 0.553 0.599</cell><cell>0.747 0.755 0.779 0.766 0.794</cell></row></table><note><p><p><p><p><p><p>Fig.</p>3</p>ROS, RUS, and two-phase learning with MNIST (a-c) and CIFAR-10 (d-f)</p><ref type="bibr" target="#b22">[23]</ref> </p>(A) ROS: (F) ROS and thresholding: The thresholding method (E) is applied after training the model with a balanced data set, where the balanced data set is generated through ROS. Thresholding combined with ROS performed better than the baseline thresholding (E) in most cases. (G) RUS and thresholding: The thresholding method (E) is applied after training the model with a balanced data set, where the balanced data set is produced through RUS. Thresholding with RUS performed worse than (E) and (F) in all cases.</p>The work by Buda et al., which varies levels of class imbalance and problem complexity, is comprehensive, having trained almost 23,000 deep CNNs across three popular data sets. The impact of class imbalance was validated on a set of baseline CNNs, showing that classification performance is severely compromised as imbalance increases and that the impact of class imbalance seems to increase as problem complexity increases, e.g. CIFAR-10 versus MNIST. The authors conclude that ROS is the best overall method for addressing class imbalance, that RUS generally performs poorly, and that two-phase learning with ROS or RUS is not as effective as their plain ROS and RUS counterparts.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>have shown that eliminating class imbalance in the training data with ROS significantly improves classification results. Lee et al. have shown that pre-training DNNs with semi-balanced data generated through RUS or augmentation-based over-sampling improves minority group performance. In contrast to Lee et al., Buda et al. found that plain ROS and RUS generally perform better than two-phase learning. Unlike Lee et al., however, Buda et al. pre-trained their networks with data that was sampled until class balance was achieved. Since Lee et al. and Buda et al. used different imbalance levels for pre-training, and reported results with different performance metrics, it</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>al. and Zhang et al. have the advantage of learning cost matrices during training. The work by</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 CIFAR-100 classification with MFE and MSFE [18]</head><label>7</label><figDesc></figDesc><table><row><cell>Data set</cell><cell>Imbalance</cell><cell>F-measure</cell><cell></cell><cell></cell><cell>AUC</cell><cell></cell><cell></cell></row><row><cell></cell><cell>level (%)</cell><cell>MSE</cell><cell>MFE</cell><cell>MSFE</cell><cell>MSE</cell><cell>MFE</cell><cell>MSFE</cell></row><row><cell>Household</cell><cell>20</cell><cell>0.3913</cell><cell>0.4138</cell><cell>0.4271</cell><cell>0.7142</cell><cell>0.7397</cell><cell>0.7354</cell></row><row><cell></cell><cell>10</cell><cell>0.2778</cell><cell>0.2797</cell><cell>0.3151</cell><cell>0.7125</cell><cell>0.7179</cell><cell>0.7193</cell></row><row><cell></cell><cell>5</cell><cell>0.1143</cell><cell>0.1905</cell><cell>0.2353</cell><cell>0.6714</cell><cell>0.6950</cell><cell>0.6970</cell></row><row><cell>Tree 1</cell><cell>20</cell><cell>0.5500</cell><cell>0.5500</cell><cell>0.5366</cell><cell>0.8100</cell><cell>0.8140</cell><cell>0.8185</cell></row><row><cell></cell><cell>10</cell><cell>0.4211</cell><cell>0.4211</cell><cell>0.4211</cell><cell>0.7960</cell><cell>0.7990</cell><cell>0.7990</cell></row><row><cell></cell><cell>5</cell><cell>0.1667</cell><cell>0.2353</cell><cell>0.2353</cell><cell>0.7920</cell><cell>0.8000</cell><cell>0.8000</cell></row><row><cell>Tree 2</cell><cell>20</cell><cell>0.4348</cell><cell>0.4255</cell><cell>0.4255</cell><cell>0.8480</cell><cell>0.8450</cell><cell>0.8440</cell></row><row><cell></cell><cell>10</cell><cell>0.1818</cell><cell>0.2609</cell><cell>0.2500</cell><cell>0.8050</cell><cell>0.8050</cell><cell>0.8060</cell></row><row><cell></cell><cell>5</cell><cell>0.0000</cell><cell>0.1071</cell><cell>0.1481</cell><cell>0.5480</cell><cell>0.6520</cell><cell>0.7000</cell></row><row><cell cols="4">Italic scores indicate MFE/MSFE loss outperforming MSE loss</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 20 Newsgroup classification with MFE and MSFE[18]</head><label>8</label><figDesc></figDesc><table><row><cell>Italic scores indicate MFE/MSFE loss outperforming MSE loss</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 RetinaNet (focal loss) on COCO [88]</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell>Backbone</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>Two-stage methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN+++</cell><cell>ResNet-101-C4</cell><cell>34.9</cell><cell>55.7</cell><cell>37.4</cell><cell>15.6</cell><cell>38.7</cell><cell>50.9</cell></row><row><cell>Faster R-CNN w FPN</cell><cell>ResNet-101-FPN</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell>Faster R-CNN by G-RMI</cell><cell>Inception-ResNet-v2</cell><cell>34.7</cell><cell>55.5</cell><cell>36.7</cell><cell>13.5</cell><cell>38.1</cell><cell>52.0</cell></row><row><cell>Faster R-CNN w TDM</cell><cell>Inception-ResNet-v2-TDM</cell><cell>36.8</cell><cell>57.7</cell><cell>39.2</cell><cell>16.2</cell><cell>39.8</cell><cell>52.1</cell></row><row><cell>One-stage methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLOv2</cell><cell>DarkNet-19</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4</cell><cell>35.5</cell></row><row><cell>SSD513</cell><cell>ResNet-101-SSD</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell>49.8</cell></row><row><cell>DSSD513</cell><cell>ResNet-101-DSSD</cell><cell>33.2</cell><cell>53.3</cell><cell>35.2</cell><cell>13.0</cell><cell>35.4</cell><cell>51.1</cell></row><row><cell>RetinaNet</cell><cell>ResNet-101-FPN</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>RetinaNet</cell><cell>ResNeXt-101-FPN</cell><cell>40.8</cell><cell>61.1</cell><cell>44.1</cell><cell>24.1</cell><cell>44.2</cell><cell>51.2</cell></row><row><cell cols="2">Italic scores indicate top AP performances</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 Cost-sensitive CoSen CNN results (accuracy) [19]</head><label>10</label><figDesc>Italic scores indicate the top performance for each data set</figDesc><table><row><cell>Dataset</cell><cell>Imbalance</cell><cell>SMOTE</cell><cell cols="2">RUS (%) SMOTE</cell><cell>CoSen</cell><cell>CoSen</cell><cell>SOSR</cell><cell>Baseline</cell><cell>CoSen</cell></row><row><cell></cell><cell>protocol</cell><cell>(%)</cell><cell></cell><cell>RSB (%)</cell><cell>SVM</cell><cell>RF (%)</cell><cell>CNN</cell><cell>CNN (%)</cell><cell>CNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell>(%)</cell><cell></cell><cell>(%)</cell></row><row><cell>MNIST</cell><cell>10% of odd</cell><cell>94.5</cell><cell>92.1</cell><cell>96.0</cell><cell>96.8</cell><cell>96.3</cell><cell>97.8</cell><cell>97.6</cell><cell>98.6</cell></row><row><cell></cell><cell>classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-100</cell><cell>10% of odd</cell><cell>32.2</cell><cell>28.8</cell><cell>37.5</cell><cell>39.9</cell><cell>39.0</cell><cell>55.8</cell><cell>55.0</cell><cell>60.1</cell></row><row><cell></cell><cell>classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Caltech-101 10% of odd</cell><cell>67.7</cell><cell>61.4</cell><cell>68.2</cell><cell>70.1</cell><cell>68.7</cell><cell>77.4</cell><cell>77.4</cell><cell>83.2</cell></row><row><cell></cell><cell>classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIT-67</cell><cell>10% of odd</cell><cell>33.9</cell><cell>28.4</cell><cell>34.0</cell><cell>35.5</cell><cell>35.2</cell><cell>49.8</cell><cell>50.4</cell><cell>56.9</cell></row><row><cell></cell><cell>classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DIL</cell><cell>Standard</cell><cell>50.3</cell><cell>46.7</cell><cell>52.6</cell><cell>55.3</cell><cell>54.7</cell><cell>68.9</cell><cell>69.5</cell><cell>72.6</cell></row><row><cell></cell><cell>split</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLC</cell><cell>Standard</cell><cell>38.9</cell><cell>31.4</cell><cell>43.0</cell><cell>47.7</cell><cell>46.5</cell><cell>65.7</cell><cell>66.1</cell><cell>68.6</cell></row><row><cell></cell><cell>split</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 Category centers with CIFAR-10 data (AP) [91]</head><label>11</label><figDesc>The VGG-16 network, pre-trained on ImageNet data, is used throughout the experiments as the baseline CNN. The proposed method fine-tunes the CNN on the imbalanced distribution, maps all images to their corresponding deep feature representations, then calculates the centroid for each class. At test time the trained CNN is used to extract features from new images, and each new image is assigned to the class of its nearest category center in feature space, defined by the Euclidean distance between the features. Zhang et al. claim that the category center is significantly more stable than the boundary generated by a CNN's classification layer, but no evidence was provided to support this.A portion of Zhang et al. 's results are displayed in Table</figDesc><table><row><cell>Classifier</cell><cell>Dist A ( ρ = 10)</cell><cell>Dist B ( ρ = 20)</cell><cell>Dist C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>( ρ ∈ [1, 20])</cell></row><row><cell>(A) Baseline CNN</cell><cell>0.779</cell><cell>0.747</cell><cell>0.857</cell></row><row><cell>(B) CC last conv. layer</cell><cell>0.824</cell><cell>0.775</cell><cell>0.859</cell></row><row><cell>(C) CC last FC layer</cell><cell>0.826</cell><cell>0.772</cell><cell>0.865</cell></row><row><cell>(D) Baseline CNN + over-sampling (E) CC last conv. layer + over-sampling (F) CC last FC layer + over-sampling</cell><cell>0.787 0.831 0.830</cell><cell>0.770 0.792 0.796</cell><cell>0.850 0.861 0.862</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 Comparing very-deep CNNs on FAU recognition (F1-score) [92] FAU class Imbalance level (A) Handcraft (B) Plain34 (C) Res10 (D) Res18 (E) Res34 (F) Res50</head><label>12</label><figDesc>comes at a cost, however, as it increases the total number of matrix operations and the memory footprint. Additional experiments are required to determine if this increased convergence rate is observed with alternative deep learning architectures and data sets.</figDesc><table><row><cell>AU1</cell><cell>2.5%</cell><cell>0.05</cell><cell>0.43</cell><cell>0.46</cell><cell>0.44</cell><cell>0.44</cell><cell>0.46</cell></row><row><cell>AU2</cell><cell>1.5%</cell><cell>0.00</cell><cell>0.42</cell><cell>0.44</cell><cell>0.45</cell><cell>0.45</cell><cell>0.46</cell></row><row><cell>AU4</cell><cell>1.1%</cell><cell>0.00</cell><cell>0.44</cell><cell>0.42</cell><cell>0.41</cell><cell>0.40</cell><cell>0.42</cell></row><row><cell>AU5</cell><cell>0.9%</cell><cell>0.00</cell><cell>0.42</cell><cell>0.41</cell><cell>0.42</cell><cell>0.40</cell><cell>0.46</cell></row><row><cell>AU6</cell><cell>3.6%</cell><cell>0.41</cell><cell>0.52</cell><cell>0.51</cell><cell>0.55</cell><cell>0.52</cell><cell>0.51</cell></row><row><cell>AU9</cell><cell>0.6%</cell><cell>0.00</cell><cell>0.41</cell><cell>0.36</cell><cell>0.38</cell><cell>0.40</cell><cell>0.44</cell></row><row><cell>AU12</cell><cell>45.7%</cell><cell>0.89</cell><cell>0.89</cell><cell>0.90</cell><cell>0.89</cell><cell>0.90</cell><cell>0.90</cell></row><row><cell>AU17</cell><cell>0.4%</cell><cell>0.00</cell><cell>0.19</cell><cell>0.20</cell><cell>0.26</cell><cell>0.27</cell><cell>0.24</cell></row><row><cell>AU20</cell><cell>0.01%</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>AU25</cell><cell>29.9%</cell><cell>0.79</cell><cell>0.85</cell><cell>0.86</cell><cell>0.86</cell><cell>0.86</cell><cell>0.85</cell></row><row><cell>AU26</cell><cell>25.6%</cell><cell>0.64</cell><cell>0.73</cell><cell>0.74</cell><cell>0.73</cell><cell>0.74</cell><cell>0.73</cell></row><row><cell>Average</cell><cell>-</cell><cell>0.29</cell><cell>0.48</cell><cell>0.48</cell><cell>0.49</cell><cell>0.49</cell><cell>0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>This section included eight algorithm-level methods for addressing class imbalance with DNNs. The MFE and MSFE loss functions presented byWang et  al. outperform the standard MSE loss on several image and text data sets. Lin et al. 's focal loss, which down-weights easy-to-classify samples, was used to outperform several state-of-theart one-stage and two-stage object detectors on the COCO data set. Nemoto et al. also explored focal loss for the purpose of detecting rare building changes, but the results were somewhat contradictory. A cost-sensitive method that incorporates pre-defined misclassification costs into the CE loss function was used by Wang et al. to predict hospital readmissions. Khan et al. proposed a cost-sensitive deep CNN (CoSen) that jointly learns network parameters and cost matrix parameters during training. Overall accuracy was used to show that the CoSen CNN outperforms a baseline CNN, multiple sampling methods, and multiple cost-sensitive methods consistently across six image data sets. Similarly, Zhang et al. combined a DBN with an evolutionary algorithm that searches for optimal misclassification costs, but results were again reported with the accuracy metric and are difficult to interpret. Buda et al. demonstrated how prior class probabilities can be used to adjust DNN output thresholds to improve overall accuracy in classifying image data. Zhang et al. presented category centers, a method that addresses class imbalance by combining transfer learning, deep CNN feature extraction, and a nearest deep feature cluster discrimination rule. Finally, Ding et al. experimented with very-deep neural networks ( &gt; 10 layers) and showed that deeper networks may converge faster due to changes in the error surfaces that allow for faster optimization.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 DOS with varying imbalance [117] Model Reduction rate Class MNIST MNISTbr SVHN</head><label>14</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Pr</cell><cell>Re F1 AUC Pr</cell><cell>Re F1 AUC Pr</cell><cell>Re F1 AUC</cell></row><row><cell>CNN</cell><cell>0.90</cell><cell cols="3">mnr 0.98 0.93 0.96 0.99 0.31 0.76 0.43 0.68 0.62 0.77 0.55 0.78</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.96 0.99 0.97 1.00 0.77 0.56 0.65 0.77 0.80 0.76 0.75 0.89</cell></row><row><cell></cell><cell>0.95</cell><cell cols="3">mnr 0.99 0.89 0.94 0.99 0.27 0.76 0.23 0.57 0.13 0.86 0.21 0.61</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.93 0.98 0.95 0.99 0.52 0.69 0.58 0.67 0.89 0.61 0.71 0.87</cell></row><row><cell></cell><cell>0.99</cell><cell cols="3">mnr 0.65 0.98 0.77 0.96 0.31 0.71 0.43 0.68 0.50 0.72 0.42 0.74</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.99 0.82 0.89 0.99 0.78 0.56 0.65 0.77 0.73 0.67 0.60 0.81</cell></row><row><cell cols="2">CNN-CL 0.90</cell><cell cols="3">mnr 0.99 0.90 0.94 1.00 0.22 0.77 0.31 0.69 0.59 0.60 0.42 0.78</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.94 0.99 0.96 0.98 0.78 0.53 0.63 0.78 0.77 0.75 0.73 0.86</cell></row><row><cell></cell><cell>0.95</cell><cell cols="3">mnr 0.99 0.83 0.90 0.97 0.28 0.77 0.24 0.60 0.04 0.68 0.07 0.61</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.89 0.99 0.94 1.00 0.52 0.68 0.57 0.69 0.89 0.60 0.70 0.84</cell></row><row><cell></cell><cell>0.99</cell><cell cols="3">mnr 0.75 0.98 0.85 0.95 0.22 0.72 0.31 0.69 0.47 0.71 0.37 0.72</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.99 0.86 0.92 0.99 0.78 0.53 0.63 0.78 0.71 0.57 0.56 0.78</cell></row><row><cell>DOS</cell><cell>0.90</cell><cell cols="3">mnr 0.99 0.97 0.98 1.00 0.66 0.77 0.71 0.79 0.71 0.82 0.72 0.84</cell></row><row><cell>(k = 5)</cell><cell></cell><cell>mjr</cell><cell cols="2">0.98 0.99 0.98 1.00 0.75 0.68 0.71 0.81 0.85 0.79 0.81 0.92</cell></row><row><cell></cell><cell>0.95</cell><cell cols="3">mnr 0.98 0.96 0.97 1.00 0.56 0.75 0.63 0.72 0.40 0.89 0.55 0.73</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.97 0.99 0.98 1.00 0.64 0.74 0.69 0.78 0.90 0.69 0.78 0.91</cell></row><row><cell></cell><cell>0.99</cell><cell cols="3">mnr 0.91 0.99 0.95 0.99 0.61 0.73 0.66 0.75 0.51 0.91 0.64 0.80</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="2">0.98 0.94 0.96 1.00 0.77 0.70 0.73 0.82 0.89 0.68 0.77 0.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 DOS with varying k [117]</head><label>15</label><figDesc></figDesc><table><row><cell cols="2">Classifier k</cell><cell cols="2">Class MNIST</cell><cell></cell><cell></cell><cell cols="2">MNISTbr</cell><cell></cell><cell>SVHN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pr</cell><cell>Re</cell><cell>F1</cell><cell>AUC Pr</cell><cell>Re</cell><cell>F1</cell><cell>AUC Pr</cell><cell>Re</cell><cell>F1</cell><cell>AUC</cell></row><row><cell>CNN</cell><cell></cell><cell>mnr</cell><cell cols="8">0.65 0.98 0.77 0.96 0.31 0.76 0.43 0.68 0.50 0.72 0.42 0.74</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="8">0.99 0.82 0.89 0.99 0.78 0.56 0.65 0.77 0.73 0.67 0.60 0.81</cell></row><row><cell>CNN-CL</cell><cell></cell><cell>mnr</cell><cell cols="8">0.75 0.98 0.85 0.95 0.22 0.77 0.31 0.69 0.47 0.71 0.37 0.72</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="8">0.99 0.86 0.92 0.99 0.78 0.53 0.63 0.78 0.71 0.57 0.56 0.78</cell></row><row><cell>DOS</cell><cell>3</cell><cell>mnr</cell><cell cols="8">0.91 0.98 0.95 0.99 0.65 0.77 0.70 0.78 0.67 0.77 0.66 0.83</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="8">0.99 0.94 0.96 1.00 0.75 0.68 0.71 0.80 0.80 0.74 0.74 0.86</cell></row><row><cell></cell><cell>5</cell><cell>mnr</cell><cell cols="8">0.91 0.99 0.95 0.99 0.66 0.77 0.71 0.79 0.51 0.91 0.64 0.80</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="8">0.98 0.94 0.96 1.00 0.75 0.68 0.71 0.81 0.89 0.68 0.77 0.90</cell></row><row><cell></cell><cell>10</cell><cell>mnr</cell><cell cols="8">0.91 0.99 0.95 0.99 0.61 0.73 0.66 0.75 0.40 0.89 0.55 0.73</cell></row><row><cell></cell><cell></cell><cell>mjr</cell><cell cols="8">0.99 0.94 0.96 1.00 0.77 0.70 0.73 0.82 0.90 0.69 0.78 0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 CRL with CelebA Data (Balanced Accuracy) [118] Attractive Mouth open Smiling Wear lipstick High cheekbones Male Heavy makeup Wavy hair Oval face Pointy nose Arched eyebrows Black hair Big lips</head><label>16</label><figDesc></figDesc><table><row><cell>Bangs</cell><cell></cell></row><row><cell>No</cell><cell>beard</cell></row><row><cell>Wear</cell><cell>earings</cell></row><row><cell>Bags</cell><cell>under eyes</cell></row><row><cell>Brown</cell><cell>hair</cell></row><row><cell>Young Straight</cell><cell>hair</cell></row><row><cell>Big</cell><cell>nose</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 (continued) Blond hair Bushy eyebrows Wear necklace Narrow eyes 5 o'clock shadow Receding hairline Wear necktie Eyeglasses Rosy cheeks Goatee Chubby Sideburns Blurry Wear hat Double chin Pale skin Gray hair Mustache Bald Mean ANet 90</head><label>16</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 Summary of deep learning class imbalance methods Method Network type Method type Description</head><label>17</label><figDesc></figDesc><table><row><cell>ROS [23, 79]</cell><cell>CNN</cell><cell>Data</cell><cell>ROS of minority classes until class balance is</cell></row><row><cell></cell><cell></cell><cell></cell><cell>achieved</cell></row><row><cell>RUS [23]</cell><cell>CNN</cell><cell>Data</cell><cell>RUS of majority classes until class balance is</cell></row><row><cell></cell><cell></cell><cell></cell><cell>achieved</cell></row><row><cell cols="2">Two-phase learning [20, 23] CNN</cell><cell>Data</cell><cell>Pre-training with RUS or ROS, then fine-tuning</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with all data</cell></row><row><cell>Dynamic sampling [21]</cell><cell>CNN</cell><cell>Data</cell><cell>Sampling rates adjust throughout training based</cell></row><row><cell></cell><cell></cell><cell></cell><cell>on previous iteration's class-wise F1-scores</cell></row><row><cell>MFE and MSFE loss [18]</cell><cell>MLP</cell><cell>Algorithm</cell><cell>New loss functions allow positive and negative</cell></row><row><cell></cell><cell></cell><cell></cell><cell>classes to contribute to loss equally</cell></row><row><cell>Focal loss [88, 103]</cell><cell>CNN</cell><cell>Algorithm</cell><cell>New loss function down-weights easy-to-classify</cell></row><row><cell></cell><cell></cell><cell></cell><cell>samples, reducing their impact on total loss</cell></row><row><cell>CSDNN [89]</cell><cell>MLP</cell><cell>Algorithm</cell><cell>CE loss function modified to incorporate a pre-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>defined cost matrix</cell></row><row><cell>CoSen CNN [19]</cell><cell>CNN</cell><cell>Algorithm</cell><cell>Cost matrix is learned through backpropagation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and incorporated into output layer</cell></row><row><cell>CSDBN-DE [90]</cell><cell>DBN</cell><cell>Algorithm</cell><cell>Cost matrix is learned through evolutionary algo-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rithm and incorporated into output layer</cell></row><row><cell>Threshold moving [23]</cell><cell>CNN</cell><cell>Algorithm</cell><cell>Decision threshold is adjusted by dividing output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>probabilities by prior class probabilities</cell></row><row><cell>Category centers [91]</cell><cell>CNN</cell><cell>Algorithm</cell><cell>Class centroids are calculated in deep feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>space and K-NN method discriminates</cell></row><row><cell>Very-deep NNs [92]</cell><cell>CNN</cell><cell>Algorithm</cell><cell>CNN network depths of up to 50 layers are used to</cell></row><row><cell></cell><cell></cell><cell></cell><cell>examine convergence rates</cell></row><row><cell>LMLE [22]</cell><cell>CNN</cell><cell>Hybrid</cell><cell>Triple-header hinge loss and quintuplet sampling</cell></row><row><cell></cell><cell></cell><cell></cell><cell>generate more discriminative features</cell></row><row><cell>DOS [117]</cell><cell>CNN</cell><cell>Hybrid</cell><cell>Minority class over-sampled in deep feature space</cell></row><row><cell></cell><cell></cell><cell></cell><cell>using K-NN and micro-cluster loss</cell></row><row><cell>CRL loss [118]</cell><cell>CNN</cell><cell>Hybrid</cell><cell>Class Rectification loss and hard sample mining</cell></row><row><cell></cell><cell></cell><cell></cell><cell>produce more discriminative features</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Table 17 as one of three types: data, algorithm, or hybrid. All of the general methods for handling class imbalance in traditional machine learning have been extended to deep learning, including: random sampling, informed sampling, cost-sensitive learning, thresholding, and hybrid methods. Data-level techniques for addressing class imbalance in deep learning include ROS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18 Summary of data sets and class imbalance levels</head><label>18</label><figDesc>Images from CelebA and EmotioNet are treated as a set of binary classification problems, because they are each annotated with 40 and 11 binary attributes, respectively. The COCO data class imbalance arises from the extreme imbalance between background and foreground concepts</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Paper Data sets Data type Class count Data set size Min class size Max class size ρ (Eq. 1)</head><label></label><figDesc></figDesc><table><row><cell>[79]</cell><cell>CIFAR-10</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>2340</cell><cell>3900</cell><cell>2.3</cell></row><row><cell>[20]</cell><cell>WHOI-Plankton</cell><cell>Image</cell><cell>103</cell><cell>3,400,000</cell><cell>&lt; 3500</cell><cell>2,300,000</cell><cell>657</cell></row><row><cell>[21]</cell><cell>Public cameras</cell><cell>Image</cell><cell>19</cell><cell>10,000</cell><cell>14</cell><cell>6986</cell><cell>499</cell></row><row><cell>[18]</cell><cell>CIFAR-100 (1)</cell><cell>Image</cell><cell>2</cell><cell>6000</cell><cell>150</cell><cell>3000</cell><cell>20</cell></row><row><cell></cell><cell>CIFAR-100 (2)</cell><cell>Image</cell><cell>2</cell><cell>1200</cell><cell>30</cell><cell>600</cell><cell>20</cell></row><row><cell></cell><cell>CIFAR-100 (3)</cell><cell>Image</cell><cell>2</cell><cell>1200</cell><cell>30</cell><cell>600</cell><cell>20</cell></row><row><cell></cell><cell cols="2">20 News Group (1) Text</cell><cell>2</cell><cell>1200</cell><cell>30</cell><cell>600</cell><cell>20</cell></row><row><cell></cell><cell cols="2">20 News Group (2) Text</cell><cell>2</cell><cell>1200</cell><cell>30</cell><cell>600</cell><cell>20</cell></row><row><cell>[88]</cell><cell>COCO</cell><cell>Image</cell><cell>2</cell><cell>115,000</cell><cell>10</cell><cell>100,000</cell><cell>10,000</cell></row><row><cell cols="3">[103] Building changes Image</cell><cell>6</cell><cell>203,358</cell><cell>222</cell><cell>200,000</cell><cell>900</cell></row><row><cell>[89]</cell><cell>GHW</cell><cell cols="2">Structured 2</cell><cell>2565</cell><cell>406</cell><cell>2159</cell><cell>5.3</cell></row><row><cell></cell><cell>ORP</cell><cell cols="2">Structured 2</cell><cell>700</cell><cell>124</cell><cell>576</cell><cell>4.6</cell></row><row><cell>[19]</cell><cell>MNIST</cell><cell>Image</cell><cell>10</cell><cell>70,000</cell><cell>600</cell><cell>6000</cell><cell>10</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>Image</cell><cell>100</cell><cell>60,000</cell><cell>60</cell><cell>600</cell><cell>10</cell></row><row><cell></cell><cell>CALTECH-101</cell><cell>Image</cell><cell>102</cell><cell>9144</cell><cell>15</cell><cell>30</cell><cell>2</cell></row><row><cell></cell><cell>MIT-67</cell><cell>Image</cell><cell>67</cell><cell>6700</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell></cell><cell>DIL</cell><cell>Image</cell><cell>10</cell><cell>1300</cell><cell>24</cell><cell>331</cell><cell>13</cell></row><row><cell></cell><cell>MLC</cell><cell>Image</cell><cell>9</cell><cell>400,000</cell><cell>2600</cell><cell>196,900</cell><cell>76</cell></row><row><cell>[90]</cell><cell>KEEL</cell><cell cols="2">Structured 2</cell><cell>3339</cell><cell>26</cell><cell>3313</cell><cell>128</cell></row><row><cell>[91]</cell><cell>CIFAR-10</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>250</cell><cell>5000</cell><cell>20</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>Image</cell><cell>100</cell><cell>60,000</cell><cell>25</cell><cell>500</cell><cell>20</cell></row><row><cell>[22]</cell><cell>CelebA</cell><cell>Image</cell><cell>2</cell><cell>160,000</cell><cell>3200</cell><cell>156,800</cell><cell>49</cell></row><row><cell cols="2">[117] MNIST</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>50</cell><cell>5000</cell><cell>100</cell></row><row><cell></cell><cell>MNIST-back-rot</cell><cell>Image</cell><cell>10</cell><cell>62,000</cell><cell>12</cell><cell>1200</cell><cell>100</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>5000</cell><cell>5000</cell><cell>1</cell></row><row><cell></cell><cell>SVHN</cell><cell>Image</cell><cell>10</cell><cell>99,000</cell><cell>73</cell><cell>7300</cell><cell>100</cell></row><row><cell></cell><cell>STL-10</cell><cell>Image</cell><cell>10</cell><cell>13,000</cell><cell>500</cell><cell>500</cell><cell>1</cell></row><row><cell cols="2">[118] CelebA</cell><cell>Image</cell><cell>2</cell><cell>160,000</cell><cell>3200</cell><cell>156,800</cell><cell>49</cell></row><row><cell>[92]</cell><cell>EmotioNet</cell><cell>Image</cell><cell>2</cell><cell>450,000</cell><cell>45</cell><cell>449,955</cell><cell>10,000</cell></row><row><cell>[23]</cell><cell>MNIST</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>1</cell><cell>5000</cell><cell>5000</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>Image</cell><cell>10</cell><cell>60,000</cell><cell>100</cell><cell>5000</cell><cell>50</cell></row><row><cell></cell><cell>ImageNet</cell><cell>Image</cell><cell>1000</cell><cell>1,050,000</cell><cell>10</cell><cell>1000</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their constructive evaluation of this paper, and the various members of the Data Mining and Machine Learning Laboratory, Florida Atlantic University, for assistance with the reviews.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>Not applicable.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>Not applicable.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>ANN: artificial neural network; AP: average precision; AUC : area under the curve; C2C: class-to-class; CE: cross entropy; CNN: convolutional neural network; CRL: class rectification loss; CSDBN-DE: cost-sensitive deep belief network with differential evolution; CSDNN: cost-sensitive deep neural network; DBN: deep belief network; DNN: deep neural network; DOS: deep over-sampling; ELM: extreme learning machine; FAU: Facial Action Unit; FL: focal loss; FN: false negative; FNE: false negative error; FP: false positive; FPE: false positive error; FPN: feature pyramid network; GHW: general hospital wards; GPU: graphics processing unit; IoT: internet of things; K-NN: K-nearest neighbors; KEEL: Knowledge Extraction based on Evolutionary Learning; LMLE: Large Margin Local Embedding; LSVRC: Large Scale Visual Recognition Challenge; MFE: mean false error; MLP: multilayer perceptron; MSE: mean squared error; MSFE: mean squared false error; NPV: negative predictive value; OHEM: online hard example mining; ORP: operating room pilot; PPV: positive predictive value; PR: precision-recall; ReLU: rectified linear unit; RF: random forest; RNN: recurrent neural network; ROC: receiver operating characteristics; ROS: random over-sampling; RUS: random under-sampling; SMOTE: Synthetic Minority Over-sampling Technique; SVM: support vector machine; TL-RS-CSL: triplet re-sampling with cost-sensitive learning; TN: true negative; TNR: true negative rate; TP: true positive; TPR: true positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' contributions</head><p>JMJ performed the literature review and drafted the manuscript. TMK worked with JMJ to develop the article's framework and focus. TMK introduced this topic to JMJ. Both authors read and approved the final manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval and consent to participate</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data mining, Fourth Edition: Practical machine learning tools and techniques</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>th ed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised machine learning: a review of classification techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kotsiantis</surname></persName>
		</author>
		<idno>cfm?id=15667 70.15667 73</idno>
		<ptr target="http://dl.acm.org/citation" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 conference on emerging artificial intelligence applications in computer engineering: Real Word AI Systems with applications in eHealth, HCI, Information Retrieval and Pervasive Technologies</title>
		<meeting>the 2007 conference on emerging artificial intelligence applications in computer engineering: Real Word AI Systems with applications in eHealth, HCI, Information Retrieval and Pervasive Technologies<address><addrLine>Amsterdam; The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2007-10-15">2007. 15 Oct 2018</date>
			<biblScope unit="page" from="3" to="24" />
		</imprint>
	</monogr>
	<note>The Netherlands</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data mining for improved cardiac care</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Niculescu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1147234.1147236</idno>
		<ptr target="https://doi.org/10.1145/1147234.1147236" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective detection of sophisticated online banking fraud on extremely imbalanced data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11280-012-0178-0</idno>
		<idno>1128 0-012-0178-0</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="475" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Big data fraud detection using multiple medicare data sources</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bauder</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-018-0138-3</idno>
		<ptr target="https://doi.org/10.1186/s40537-018-0138-3" />
	</analytic>
	<monogr>
		<title level="j">J Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combating imbalance in network intrusion datasets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Striegel</surname></persName>
		</author>
		<idno type="DOI">10.1109/GRC.2006.1635905</idno>
		<ptr target="https://doi.org/10.1109/GRC.2006.16359" />
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on granular computing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning for the detection of oil spills in satellite radar images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007452223027</idno>
		<ptr target="https://doi.org/10.1023/A:1007452223" />
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">17th IEEE international conference on machine learning and applications (ICMLA)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasanin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2018.00125</idno>
		<ptr target="https://doi.org/10.1109/ICMLA.2018.00125" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="785" to="790" />
		</imprint>
	</monogr>
	<note>An empirical study on class rarity in big data</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effects of varying class distribution on learner behavior for medicare fraud detection with imbalanced big data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13755-018-0051-3</idno>
		<idno>1375 5-018-0051-3</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Health Inf Sci Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data: open challenges and future directions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13748-016-0094-0</idno>
		<idno>1374 8-016-0094-0</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Prog Artif Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">TensorFlow: large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<imprint>
			<date type="published" when="2015-11-01">2015. 1 Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Theano: a Python framework for fast computation of mathematical expressions</title>
		<author>
			<orgName type="collaboration">Theano Development Team</orgName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015-11-01">2015. 1 Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><surname>Cudnn</surname></persName>
		</author>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training deep neural networks on imbalanced data sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727770</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2016" />
	</analytic>
	<monogr>
		<title level="m">2016 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">77277</biblScope>
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Plankton classification on imbalanced large scale database via convolutional neural networks with transfer learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2016.7533053</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2016.7533053" />
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on image processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="3713" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic sampling in convolutional neural networks for imbalanced data classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kaseb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gauen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aghajanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shyu</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIPR.2018.00027</idno>
		<ptr target="https://doi.org/10.1109/MIPR.2018.00027" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE conference on multimedia information processing and retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="112" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.580</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.580" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2018.07.011</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2018.07.011" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reconstructing the giant: on the importance of rigour in documenting the literature search process</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niehaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plattfaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cleven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><surname>Scholar</surname></persName>
		</author>
		<ptr target="https://scholar.google.com/" />
		<imprint>
			<date type="published" when="2018-11-15">15 Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiclass imbalance problems: analysis and potential solutions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMCB.2012.2187280</idno>
		<ptr target="https://doi.org/10.1109/TSMCB.2012" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B (Cybern)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1119" to="1130" />
			<date type="published" when="2012">2012. 21872 80</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2008.239</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2008.239" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The class imbalance problem: Significance and strategies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 2000 international conference on artificial intelligence (ICAI)</title>
		<meeting>the 2000 international conference on artificial intelligence (ICAI)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining data with rare events: a case study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICTAI.2007.130</idno>
		<ptr target="https://doi.org/10.1109/ICTAI.2007.130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th IEEE international conference on tools with artificial intelligence</title>
		<meeting>the 19th IEEE international conference on tools with artificial intelligence<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
	<note>ICTAI &apos;07</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining with rarity: a unifying framework</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007730.1007734</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis and visualization of classifier performance: comparison under imprecise class and cost distributions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third international conference on knowledge discovery and data mining</title>
		<meeting>the third international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new evaluation measure for imbalanced datasets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Australasian data mining conference</title>
		<meeting>the 7th Australasian data mining conference<address><addrLine>Darlinghurst, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
	<note>cfm?id=24492 88.24492 95</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143874</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on machine learning. ICML &apos;06</title>
		<meeting>the 23rd international conference on machine learning. ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A study on the relationships of classifier performance metrics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICTAI.2009.25</idno>
		<ptr target="https://doi.org/10.1109/ICTAI.2009.25" />
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0493(1950)078&lt;0001:VOFEIT&gt;2.0.CO;2</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Mon Weather Rev</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Experimental perspectives on learning from imbalanced data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273614</idno>
		<ptr target="https://doi.org/10.1145/1273496" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on machine learning. ICML &apos;07</title>
		<meeting>the 24th international conference on machine learning. ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 12736 14</date>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Editorial: Special issue on learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kotcz</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007730.1007733</idno>
		<ptr target="https://doi.org/10.1145/1007730" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2004">2004. 10077</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">KNN approach to unbalanced data distributions: a case study involving information extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML&apos;2003 workshop on learning from imbalanced datasets</title>
		<meeting>the ICML&apos;2003 workshop on learning from imbalanced datasets</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Addressing the curse of imbalanced training sets: one-sided selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth international conference on machine learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two modifications of cnn</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tomek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="769" to="772" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The imbalanced training sample problem: under or over sampling?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural, syntactic, and statistical pattern recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Caelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rpw</forename><surname>Duin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Campilho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>De Ridder</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Asymptotic properties of nearest neighbor rules using edited data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.1972.4309137</idno>
		<ptr target="https://doi.org/10.1109/TSMC.1972.4309137" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern SMC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="421" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Int Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B-H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv Intell Comput</title>
		<editor>
			<persName><forename type="first">D-S</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X-P</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Safe-level-smote: safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bunkhumpornpat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinapiromsaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lursinsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in knowledge discovery and data mining</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Theeramunkong</surname></persName>
		</editor>
		<meeting><address><addrLine>Kijsirikul B, Cercone N, Ho T-B; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Concept learning and the problem of small disjuncts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Acker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A quantitative study of small disjuncts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the seventeenth national conference on artificial intelligence and twelfth conference on innovative applications of artificial intelligence</title>
		<meeting>the seventeenth national conference on artificial intelligence and twelfth conference on innovative applications of artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="665" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Class imbalances versus small disjuncts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007730.1007737</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth international joint conference on artificial intelligence</title>
		<meeting>the seventeenth international joint conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="973" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Cost-sensitive learning and the class imbalanced problem</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<editor>Sammut C</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploratory undersampling for class-imbalance learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMCB.2008.2007853</idno>
		<ptr target="https://doi.org/10.1109/TSMCB" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B (Cybern)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2008">2009. 2008. 20078</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Smoteboost: improving prediction of the minority class in boosting</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge discovery in databases: PKDD 2003</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Todorovski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="107" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data sets with boosting and data generation: the databoost-im approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Viktor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007730.1007736</idno>
		<ptr target="https://doi.org/10.1145/1007730.1007736" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Boosted classification trees and class probability/quantile estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="409" to="439" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno>AAINR34548</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Waterloo, Ont., Canada, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">deep-learn ingmade-easy-with-deep-cogni tion-403fb e4453 51</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vázquez</surname></persName>
		</author>
		<ptr target="https://becominghuman.ai/" />
		<imprint>
			<date type="published" when="2017-10-10">2017. 10 Oct 2018</date>
		</imprint>
	</monogr>
	<note>Deep Learning made easy with Deep Cognition</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neocognitron: a self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00344251</idno>
		<ptr target="https://doi.org/10.1007/BF003" />
	</analytic>
	<monogr>
		<title level="j">Biol Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44251</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Recent advances in deep learning: an overview</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Minar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08169</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: an overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.4.541</idno>
		<ptr target="https://doi.org/10.1162/neco.1989.1.4.541" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
		<ptr target="https://doi.org/10.1162/neco.2006.18.7.1527" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on neural information processing systems. NIPS&apos;06</title>
		<meeting>the 19th international conference on neural information processing systems. NIPS&apos;06<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note>cfm?id=29764 56.29764 76</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A comparative study of open source deep learning frameworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shatnawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Albdour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Qurran</surname></persName>
		</author>
		<idno type="DOI">10.1109/IACS.2018.8355444</idno>
		<ptr target="https://doi.org/10.1109/IACS.2018.8355444" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An incorporation of artificial intelligence capabilities in cloud computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18535/ijecs/v5i11.63</idno>
		<ptr target="https://doi.org/10.18535/ijecs/v5i11.63" />
	</analytic>
	<monogr>
		<title level="j">Int J Eng Comput Sci</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cloud computing for deep learning analytics: a survey of current trends and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saiyeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mir</surname></persName>
		</author>
		<idno type="DOI">10.26483/ijarcs.v8i2.2931</idno>
		<ptr target="https://doi.org/10.26483/ijarc" />
	</analytic>
	<monogr>
		<title level="j">Int J Adv Res Comput Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="72" />
			<date type="published" when="2017">2017. 8i2.2931</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Advances in data mining. Applications and theoretical aspects</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elgendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elragal</surname></persName>
		</author>
		<editor>Perner P</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="214" to="227" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Big data analytics: a literature review paper</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">What is big data? : an introduction to the big data landscape</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dumbill</surname></persName>
		</author>
		<ptr target="http://radar.oreilly.com/2012/01/what-is-big-data.html" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Perspectives on Big Data analysis: methodologies and applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Providence</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A survey on addressing high-class imbalance in big data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Leevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-018-0151-6</idno>
		<ptr target="https://doi.org/10.1186/s40537-018-0151-6" />
	</analytic>
	<monogr>
		<title level="j">J Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep learning applications and challenges in big data analytics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villanustre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muharemagic</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-014-0007-7</idno>
		<ptr target="https://doi.org/10.1186/s40537-014-0007-7" />
	</analytic>
	<monogr>
		<title level="j">J Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Discovering binary codes for documents by learning deep generative models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Top Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="91" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijar.2008.11.006</idno>
		<ptr target="https://doi.org/10.1016/j.ijar.2008.11.006" />
	</analytic>
	<monogr>
		<title level="j">Int J Approx Reason</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An improved algorithm for neural network classification of imbalanced training sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.286891</idno>
		<ptr target="https://doi.org/10.1109/72.286891" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="962" to="969" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">The impact of imbalanced training data for convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Masko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Cifar</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Canadian Institute for Advanced Research)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Whoi-plankton-a large scale fine grained visual recognition benchmark dataset for plankton classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Orenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sosik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-M</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.05.004</idno>
		<ptr target="https://doi.org/10.1016/j.media.2016.05.004" />
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010-11">2010. Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Striving for simplicity: the all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Predicting hospital readmission via cost-sensitive deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kronzer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCBB.2018.2827029</idno>
		<ptr target="https://doi.org/10.1109/TCBB.2018.2827029" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Training cost-sensitive deep belief networks on imbalance data problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727769</idno>
		<ptr target="https://doi.org/10.1109/IJCNN" />
	</analytic>
	<monogr>
		<title level="m">international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
			<biblScope unit="volume">77277</biblScope>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">33rd Youth Academic annual conference of Chinese Association of Automation (YAC)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/YAC.2018.8406400</idno>
		<ptr target="https://doi.org/10.1109/YAC.2018.8406400" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="359" to="363" />
		</imprint>
	</monogr>
	<note>Image classification with category centers in class imbalance situation</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Facial action recognition using very deep networks for highly imbalanced class distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><forename type="middle">W</forename></persName>
		</author>
		<idno type="DOI">10.1109/APSIPA.2017.8282246</idno>
		<ptr target="https://doi.org/10.1109/APSIPA.2017" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">82822</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
	<note>Asia-Pacific signal and information processing association annual summit and conference</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Cifar-100</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Canadian Institute for Advanced Research)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<ptr target="http://people.csail" />
		<title level="m">20 Newsgroups Dataset</title>
		<imprint>
			<date type="published" when="2018-10-15">15 Oct 2018</date>
		</imprint>
	</monogr>
	<note>mit.edu/jrenn ie/20New sgrou ps</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">You only look once: unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.91" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><forename type="middle">M</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">DSSD: deconvolutional single shot detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Beyond skip connections: top-down modulation for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Classification of rare building change using cnn with multi-class focal loss</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGARSS.2018.8517563</idno>
		<ptr target="https://doi.org/10.1109/IGARSS.2018" />
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE international geoscience and remote sensing symposium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">85175</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Caltech101 image dataset</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2009.5206537</idno>
		<ptr target="https://doi.org/10.1109/CVPRW" />
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="volume">52065</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">fit-image -libra ry</title>
		<ptr target="https://licensing.eri.ed.ac.uk/i/software/dermo" />
		<imprint>
			<date type="published" when="2018-11-05">5 Nov 2018</date>
		</imprint>
		<respStmt>
			<orgName>The University of Edinburgh: Edinburgh Dermofit Image Library</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Automated annotation of coral reef survey images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting><address><addrLine>Providence, Rhode Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Cost-aware pre-training for multiclass cost-sensitive deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Smote-rsb*: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using smote and rough sets theory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ramentol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-011-0465-6</idno>
		<ptr target="https://doi.org/10.1007/s10115-011-0465-6" />
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="265" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Adaptive cross-generation differential evolution operators for multiobjective optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Abbass</surname></persName>
		</author>
		<idno type="DOI">10.1109/TEVC.2015.2433672</idno>
		<ptr target="https://doi.org/10.1109/TEVC.2015.2433672" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Evol Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Keel: a software tool to assess evolutionary algorithms for data mining problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alcalá-Fdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Garrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bacardit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-008-0323-y</idno>
		<ptr target="https://doi.org/10.1007/s00500-008-0323-y" />
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="318" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Siew</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2005.12.126</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2005.12.126" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Facial action coding system: a technique for the measurement of facial movement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Emotionet challenge: recognition of facial expressions of emotion in the wild</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01210</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Deep over-sampling framework for classifying imbalanced data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning and knowledge discovery in databases</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ceci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hollmén</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Todorovski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Imbalanced deep learning by minority class incremental rectification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2832629</idno>
		<ptr target="https://doi.org/10.1109/TPAMI" />
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 28326</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on computer vision (ICCV)</title>
		<meeting>international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Facenet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Panda: pose aligned networks for deep attribute modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.212</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.212" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the IEEE computer society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.425" />
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.161</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2010.161" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature Learning 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics. Proceedings of machine learning research</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dudík</surname></persName>
		</editor>
		<meeting>the fourteenth international conference on artificial intelligence and statistics. machine learning research<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">T-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000016</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Found Trends Inf Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Deep domain adaptation for describing people based on finegrained clothing attributes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299169</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299169" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5315" to="5324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Data sampling approaches with severely imbalanced big data for medicare fraud detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasanin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICTAI.2018.00030</idno>
		<ptr target="https://doi.org/10.1109/ICTAI.2018.00030" />
	</analytic>
	<monogr>
		<title level="m">IEEE 30th international conference on tools with artificial intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
