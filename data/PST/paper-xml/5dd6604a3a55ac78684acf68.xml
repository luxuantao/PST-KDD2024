<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Using SpecAugment for End-to-End Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-20">20 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Parnia</forename><surname>Bahar</surname></persName>
							<email>bahar@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AppTek</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
							<email>zeyer@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AppTek</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
							<email>schlueter@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<email>ney@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AppTek</orgName>
								<address>
									<postCode>52062</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Using SpecAugment for End-to-End Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-20">20 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.08876v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2% BLEU on LibriSpeech Audiobooks En→Fr and +1.2% on IWSLT TED-talks En→De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Traditional speech-to-text translation (ST) systems have been build in a cascaded fashion comprised of an automatic speech recognition (ASR) model trained on paired speechtranscribed data and a machine translation (MT) model trained on bilingual text data. Recent advancements in both ASR <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> and MT <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> have inspired the end-to-end direct ST models which can be trained using a translation speech corpus <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Some appealing advantages of the direct models are: <ref type="bibr" target="#b0">(1)</ref> no error accumulation from the recognizer, (2) faster decoding throughput and (3) less computational power in total by training all parameters jointly. In spite of these properties, training such end-to-end ST models requires a moderate amount of paired translated speech-totext data which is not easy to acquire. Therefore these models tend to overfit easily.</p><p>In the absence of an adequate volume of training data, one remedy is generating synthetic data like back-translation (BT) <ref type="bibr" target="#b13">[14]</ref> as the most common data augmentation method to leverage monolingual data. The idea is to use a pretrained model to convert weakly supervised data into speechto-translation pairs for ST training <ref type="bibr" target="#b14">[15]</ref>. One way is to use a pre-trained source-to-target MT model to translate ASR transcription into the target language. Another method is the use of a pre-trained text-to-speech (TTS) model to generate speech data from a monolingual text. However, these methods require some effort to train an additional model, as well as computational power to generate a moderate amount of (noisy) synthetic data, which in some cases can be too ex-pensive to be obtained.</p><p>Another method is data augmentation by which new synthetic training samples are generated by corrupting the initial audio data and conserving the same label as the original training sample. Audio-level speech augmentation can be done in different ways such as noise injection (adding random noise), shifting time (transmitting time series forward/backward with a few seconds), speed perturbation (expanding time series by a speed rate) and changing the frequency pitch randomly. Besides increasing the quantity of training data, data augmentation often make the model invariant to the applied noise and enhance its ability to generalize.</p><p>Inspired by the success of augmentation methods in ASR <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, as a remedy to avoid overfitting while using lowresource translated speech data, we study the use of spectrogram augmentation (SpecAugment) for direct ST model. SpecAugment <ref type="bibr" target="#b15">[16]</ref> is a simple and low-implementation cost approach. Unlike traditional speech augmentation methods that directly manipulate the input signal, SpecAugment is applied on the audio features, which are usually mel spectrogram of the input signal. We utilize two kinds of deformations of the spectrogram which are time and frequency masking, where we mask a block of consecutive time steps and/or mel frequency channels.</p><p>Our main motivation of using SpecAugment is the potential avoidance of overfitting, better generalization beyond low-resource training data and improving robustness of the end-to-end models. In this paper, we aim to shed light on the following questions. First, does SpecAugment strategy help the direct ST model? Second, what is the effect of the approach concerning the different amount of training data? Our first contribution is an extensive empirical investigation of SpecAugment on top-performing ST systems to validate or disprove the above conjectures. Our aim is not to compare with other data augmentation strategies, but the effectiveness of the SpecAugment as a stand-alone method. We hope that this method might overcome the data efficiency issue and therefore, as our second contribution, we explore the effect of that on a various amount of training data. Our experimental results on LibriSpeech Audiobooks En→Fr and IWSLT TED-talks En→De show that the method not only greatly outperforms direct ST model up to +1.7% BLEU on average, but also diminishes the overfitting problem. We also show that our improvements are valid in different data scenarios irrespective of the amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spectrogram Augmentation</head><p>In spectrogram augmentation (SpecAugment) <ref type="bibr" target="#b15">[16]</ref>, we randomly apply masking in consecutive frames in the time axis as well as consecutive dimensions in the feature axis. Since the author stated that the time warping is the most expensive and the least influential, we do not explore it here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Time Masking</head><p>Time masking is masking of τ successive time steps [t,t + τ), where we set (x t , . . . , x t+τ ) := 0, where τ is the masking window which is selected from a uniform distribution from 0 to the maximum time mask parameter R. (x 1 , . . . , x T ) are the input audio features, and T is the length of the input signal. The time position t is picked from another uniform distribution over [0, T ) 1 such that we never exceed the maximum sequence length T (i.e. if t + τ &gt; T , we set it to T ).</p><p>We apply the time masking procedure for m R ∈ N 0 times. We also ensure that if m R &gt; 1, the same time position t is not selected more than once (i.e. without replacement) 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Frequency Masking</head><p>Frequency masking can be also applied such that φ consecutive frequency channels [ f , f + φ ) are masked, where φ is chosen from a uniform distribution from 0 to the frequency mask parameter F, and f is chosen from [0, ν) 3 . ν is the input feature dimension, e.g. the number of mel frequency channels. Similar to time masking, we do not allow for already selected f and check if f + φ &gt; ν, we set it to ν.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows examples of the individual augmentations applied to a single input. Multiple frequency and time masks might overlap. m F ∈ N 0 refers to the number of times we apply the frequency masks. We note that we standardize the 1 We choose time position differently from the original paper where they select t in the interval of [0, T − τ) <ref type="bibr" target="#b15">[16]</ref>. 2 It is not clear whether the original paper allows replacement or not. 3 Again we note the difference between our implementation and the original paper where the selection interval is [0, ν − φ ) <ref type="bibr" target="#b15">[16]</ref>. features to zero mean and variance of one. Therefore, masking to zero is equivalent to setting it to the mean value. In this work, we mainly investigate a series of combinations to find a reliable recipe for direct ST model. We only apply the SpecAugment during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network</head><p>In ST, given an input observation (audio feature vectors) of variable length T , x T 1 , a sequence of discrete label of unknown length J (source sequence/transcribed words), f J 1 and a sequence of target words of unknown length I, e I 1 , the posterior probability of a target sequence is defined as:</p><formula xml:id="formula_0">p(e I 1 |x T 1 ) = I ∏ i=1 p(e i |e i−1 1 , x T 1 )<label>(1)</label></formula><p>where usually T &gt; I, J. This posterior can be modeled directly in an end-to-end fashion. Here, we only address the direct end-to-end architecture that is used in our experiments. The direct model is based on the attention sequence-tosequence model <ref type="bibr" target="#b7">[8]</ref> composed of long short-term memories (LSTMs) <ref type="bibr" target="#b17">[18]</ref> similar to <ref type="bibr" target="#b18">[19]</ref>. We only focus on LSTMbased models and leave the transformer architecture as our future study <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. An abstract overview of the network and a summary of the model are shown in Figure <ref type="figure" target="#fig_1">2</ref> and written in Equation 2 respectively. A bidirectional LSTM (BLSTM) scans the input sequence once from left to right and once from right to left. To handle the long speech utterances, we apply max-pooling in the time-dimension at multiple steps inside the speech encoder. For the input sequence x T 1 , we end up with the encoder states h T 1 , where T = T /red with the time reduction factor red. Then, the decoder LSTM generates an output sequence conditioned on the encoder representations. While computing e i at each time step, an additive attention function is used to produce normalized attention weights α i,t . The context vector c i is then computed as a weighted sum of encoder representations. A transformation followed by a softmax operation predicts e i . Finally, the decoder state is updated to s i . Here, L e and L d are the number of encoder and decoder layers respectively. • is the concatenation operator of functions.</p><formula xml:id="formula_1">h T 1 = (BLSTM L e • • • • • max-pool 1 • BLSTM 1 )(x T 1 ) α i,t = softmax t linear(tanh(linear(s i−1 , h t ))) c i = T ∑ t=1 α i,t h t p(e i |e i−1 1 , x T 1 ) = softmax e linear(e i−1 , s i−1 , c i ) s i = LSTM L d • • • • • LSTM 1 (e i , s i−1 , c i )<label>(2)</label></formula><p>4. Experiments LibriSpeech En→Fr: As suggested by <ref type="bibr" target="#b22">[23]</ref>, to double the training data size, we concatenate the original translation and the Google Translate reference which have been provided in the dataset package. Hence, we end up to 200h of clean speech corresponding to 94.5k segments for the ST task. We apply 40-dimensional Gammatone features <ref type="bibr" target="#b25">[26]</ref> using the RASR feature extractor <ref type="bibr" target="#b26">[27]</ref>. For MT training, we utilize no additional data and only use the source-target data from the ST task, i.e. 94.5k. For ASR training, we take both the ASR and ST data resulting in 330h. The dev and test sets contain 2h and 4h of speech, 1071 and 2048 segments respectively. Here, the dev set is used as our cross-validation set as well as checkpoint selection.</p><p>IWSLT En→De: Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, we extract 80dimensional Mel-frequency cepstral coefficients (MFCC) features. We automatically recompute the provided audio-tosource-sentence alignments to reduce the problem of speech segments without a translation. We use the TED-LIUM corpus including 207h and the IWSLT speech translation TED corpus with 272h of speech data for ASR training. For MT training, we use the TED, and the OpenSubtitles2018 corpora, as well as the data provided by the WMT 2018 evaluation (Europarl, ParaCrawl, CommonCrawl, News Commentary, and Rapid), a total of 65M lines of parallel sentences. We filter these data based on several heuristics resulting in 32M samples. We randomly select a part of our segments as our cross-validation set and choose dev2010 and test2015 as our dev and test sets with 888 and 1080 segments respec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Models</head><p>In our experiments, we build ASR, MT, and ST models all based on the network described in Section 3. The ASR and MT models are used for building the cascade pipeline as well as pre-training the components of the ST model. Thus, the ASR and ST models use the same speech encoder architecture, whilst the MT and ST models use the same text decoder topology, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>For both tasks, we apply separate byte pair encoding (BPE) <ref type="bibr" target="#b32">[33]</ref> with 20k symbols on both side of the MT data, whereas 10k merge operations on the ASR transcriptions. ASR model: All tokens are mapped into a 512-dimensional embedding space. The encoder is composed of 6 stacked BLSTM layers with 1024 nodes. The decoder is a 1-layer unidirectional LSTM of size 1024. A single head additive attention with alignment feedback <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> is used as our attention component. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, we apply layer-wise pre-training for the encoder, where we start with two encoder layers and a single max-pool in between BLSTM layers. We apply 2 max-pooling layers with pool-size of 3 and 2 , i.e. we get a total time reduction factor of 6. We also use CTC auxiliary loss function <ref type="bibr" target="#b35">[36]</ref> on top of the speech encoder only during training <ref type="bibr" target="#b36">[37]</ref>. MT model: Our MT model follows the ASR model with a 6layer BLSTM encoder without max-pooling, with a cell size of 1024. The decoder is a 1-layer unidirectional LSTM with cell size 1024, with single head additive attention equipped with alignment feedback. ST model: The encoder has a similar architecture to the ASR encoder, and the decoder is similar to the MT decoder.</p><p>The models are trained end-to-end using the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with a learning rate of 0.0008, and a dropout of 0.3 <ref type="bibr" target="#b38">[39]</ref>. We warm-up the learning rate by linearly increasing it for a few training steps. Label smoothing <ref type="bibr" target="#b39">[40]</ref> with a ratio of 0.1 is utilized. We employ a learning rate scheduling scheme, where we lower the learning rate with a decay factor of 0.9 if the perplexity on the dev set does not improve for 5 consecutive checkpoints and save the checkpoints every fifth of an epoch. We remove sequences longer than 75 tokens All batch sizes are specified to be as big as possible to fit in a single GPU memory. A beam size of 12 is used in inference. In order to explore the impact of SpecAugment, we choose to have relatively large models, as explained above. These models include 181M and 192M free parameters for Librispeech and IWSLT tasks respectively. The models are built using our in-house NNtoolkit software that relies on TensorFlow <ref type="bibr" target="#b40">[41]</ref>. The code is open source and the configurations of the setups are available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Table <ref type="table" target="#tab_1">2</ref> and 3 present the results for the ASR and MT models (described in Section 4.2) on LibriSpeech and IWSLT tasks respectively. On the test sets, we achieve 6.47% and 13.80% WER. We note that one might gain better WER using the conventional hybrid hidden Markov model (HMM) -neural network (NN) approach on phoneme level <ref type="bibr" target="#b41">[42]</ref>, which is out of the scope of this paper.</p><p>The MT task on LibriSpeech seems more challenging as both scores are lower. We obtain 18.2% BLEU on the LibriSpeech and respectively 31.5% BLEU on the IWSLT by pure MT. Table <ref type="table" target="#tab_2">4</ref> shows the traditional cascade pipeline where the output of our ASR model, a sequence of tokens is fed as the input to our MT system. We gain 15.7% BLEU and 70.6% TER on the LibriSpeech and 24.4% BLEU and 62.5% TER on IWSLT test set. As expected, the ST systems are behind the pure MT models (cf. Table <ref type="table" target="#tab_2">3 and Table 4</ref>). In the rest of the paper, we only focus on the results of direct models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Using SpecAugment</head><p>In this section, we explore different types of masking. In the first set of experiments, we deactivate either time or frequency masking. The results are listed in Table <ref type="table" target="#tab_3">5</ref> and 6 for LibriSpeech and IWSLT tasks respectively. As it is shown in Table <ref type="table" target="#tab_3">5</ref>, we apply various types of frequency masking with different values of F, between 2 and 35, and m F while the time masking is disabled. As listed, the optimum value of F is around 4 and 5 with 1.5% in BLEU and 1.4% in TER on average of dev and test sets. A further increase of F until 20 hurts the performance by 0.4% in both BLEU and TER on the test set. To verify the effect of SpecAugment, we also employ a coarse policy where we randomly mask 35 frequency channels out of 40 and apply it 5 times. As expected, the performance drops behind the direct baseline. Interestingly enough, even a small value of frequency masking (F = 2) leads to improvements. It is important to highlight that since we randomly select the masking window between zero and maximum value of F, the results are close to each other. The best results of each set of experiments are highlighted in bold. We also disable the frequency masking by setting F and m F set to zero and vary the time mask parameter R and the number of times it has been called. Again, there is a limit of how much data augmentation can be applied. Enlarging the time masking window R to 100 leads to lower BLEU and TER scores. Furthermore, we drastically increase the time masking window R into 400 steps and apply it 5 times which fails due to unstable training. If the initial convergence is stable, in all cases, adding some data augmentation improves the setup, however, at some point, the performance degrades by more augmentation. Moreover, observe that in many cases applying small window several times gives slightly more improvements compared to the policy in which a large window applied once (cf. row 4 and 7 in Table <ref type="table" target="#tab_3">5</ref>). After finding the optimum of both time and frequency masking, we have done some combinations of both masking as shown in the table. As expected, the combination of best of two masking gives the largest boost.</p><p>We verify the influence of SpecAugment on IWSLT En→De task by an improvement up to 1.2% in BLEU and 1.8% in TER (see Table <ref type="table" target="#tab_4">6</ref>). For IWSLT, we also apply a policy similar to the LD policy of main paper <ref type="bibr" target="#b15">[16]</ref> as listed in the last row of Table <ref type="table" target="#tab_4">6</ref>. As shown, this setup is not the optimum case for our task, which leads to the conclusion that SpecAugment might be working better by fine-tuning on a specific task, however, adding some data augmentation improves the setup. Moreover, based on the above results, SpecAugment performs quite well regardless of the features and their dimensions. In our experiments, we use 40-dimensional Gammatone features for LibriSpeech respectively 80-dimensional MFCC features for IWSLT. In both cases, augmentation helps the performance. In the rest of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Importance of SpecAugment on overfitting</head><p>We also study the effect of SpecAugment on overfitting. Figure <ref type="figure" target="#fig_2">3</ref> shows log-perplexity plots on training and dev set with and without augmentation. It is seen that SpecAugment leads to better generalization, as measured from the difference between the perplexity of training and dev data. The model trained with SpecAugment still has a training data likelihood which is higher than the baseline system. Therefore, we can confirm that the method reduces overfitting up to some degree. In this case, we need to train the SpecAugment system for few more epochs. A corresponding increase in the number of epochs for the baseline system deteriorated the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect on training data size</head><p>We go further and show that SpecAugment can be leveraged to improve the performance of a direct ST model, when the amount of training data is limited. To do so, we have conducted studies on a different portion of training data to see how the method performs in different data conditions. Table The minimal gains, even hurting TER when 23k samples are used. It could be attributed to the fact that 23k segments are not sufficient to train a reliable deep model. We believe that as long as a moderate amount of data is available, SpecAugment helps data efficiency more. The augmentation policy compensates lack of data when we half the training data size to 47k segments. It achieves 1.8% of absolute improvement difference in BLEU and 2.8% in TER compared to the model trained using 94k samples with 1.5% of BLEU and 1.6% of TER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Pre-training</head><p>We also consider the effect of data augmentation on the top of pre-training. We pre-train the encoder using our pre-trained ASR model, and the decoder using our MT model as described in Section 4.2 which use more training data compared to the direct ST model (see Table <ref type="table" target="#tab_0">1</ref>). After initialization with pre-trained components, the ST model is fine-tuned using the ST data. Here, we add an additional BLSTM layer (adaptor layer) to adapt the output of speech encoder and the input of text decoder without freezing the parameters (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>As shown in Table <ref type="table" target="#tab_6">8</ref>, SpecAugment slightly outperforms the pre-training. It outperforms the pre-trained models by 0.5% and 0.2% BLEU on LibriSpeech and IWSLT respectively, and no TER improvements. This results confirm that SpecAugment can be used along with pre-training. By comparing Table <ref type="table" target="#tab_6">5 and 8</ref>, one can argue that the SpecAugment might compensate the effect of pre-training strategy by its own for LibriSpeech (compare 18.0% vs. 17.9% in BLEU on the dev set and 15.8% vs. 16.1% on the test set).</p><p>We finally compare our model with the other works in the literature in Table <ref type="table">9</ref>. On the LibriSpeech test set, our model outperforms both the LSTM-based end-to-end models and the Transformer-based. Contrary to <ref type="bibr" target="#b22">[23]</ref> in which character decoder is used, we apply BPE that obtain improvements. Both our direct model and the cascade model outperform the models in <ref type="bibr" target="#b22">[23]</ref>. We also beat the Transformer models without augmentation. Our recipe works as good as knowledge distillation method where an MT model is exploited to teach the ST model. Table <ref type="table">9</ref>: Comparison on LibriSpeech En→Fr test set with the literature. In order to be comparable with other works, the results in this table are case-insensitive BLEU computed using multi-bleu.pl script <ref type="bibr" target="#b28">[29]</ref>. 1 : the evaluation is without punctuation. 2 : it correspond to 16.2% BLEU from Table <ref type="table" target="#tab_6">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method BLEU [%]</head><p>other works direct <ref type="bibr" target="#b22">[23]</ref> 13.3 multi-task <ref type="bibr" target="#b22">[23]</ref> 13.4 cascade pipeline <ref type="bibr" target="#b22">[23]</ref> 14.6 unsupervised 1 <ref type="bibr" target="#b29">[30]</ref> 12.2 Transformer <ref type="bibr" target="#b19">[20]</ref> 13.8 Transformer+pretraining <ref type="bibr" target="#b20">[21]</ref> 14.3 + knowledge distillation <ref type="bibr" target="#b20">[21]</ref> 17.0 this work direct+pretraining+SpecAugment 2 17.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have studied SpecAugment, a simple and low-cost data augmentation for end-to-end direct speech translation. There is a limit of how much data augmentation can be applied. Adding some data augmentation improves the performance in terms of BLEU and TER, however, at some point, the performance degrades by more augmentation. We have also shown that the method avoids overfitting to some extent and it requires longer training. A common criticism of many techniques for low-resource applications is that the improvements go away once we have lots of synthetic parallel data. Therefore, we believe that comparing the SpecAugment approach with generated data using TTS model is a crucial next step. We also aim to explore the effectiveness of the approach on Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 694537, project "SEQ-CLAS"), the Deutsche Forschungsgemeinschaft (DFG; grant agreement NE 572/8-1, project "CoreTec") and from a Google Focused Award. The work reflects only the authors' views and none of the funding parties is responsible for any use that may be made of the information it contains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: From top to bottom, the figures depict the spectrogram of the input with no augmentation, time masking, frequency masking and both masking applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the direct speech translation model. Shallow grey blocks correspond to pre-trained components, and dark grey blocks are fine-tuned on the ST task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average log perplexity of training and dev sets across epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>4.1. Datasets and MetricsWe have conducted our experiments on two ST tasks which are publicly available: the LibriSpeech Audio-books En→Fr<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> 4 and the IWSLT TED-talks En→De<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> 5 . The training data statistics are listed in Table1. Training data statistics.</figDesc><table><row><cell>Task</cell><cell cols="4">LibriSpeech En→Fr IWSLT En→De # of seg. hours # of seg. hours</cell></row><row><cell>ASR</cell><cell>61.3k</cell><cell>130h</cell><cell cols="2">92.9k 207h</cell></row><row><cell>ST</cell><cell>94.5k</cell><cell>200h</cell><cell cols="2">171.1k 272h</cell></row><row><cell>MT</cell><cell>94.5k</cell><cell>-</cell><cell>32M</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ASR results measured in WER [%].</figDesc><table><row><cell>Task</cell><cell></cell><cell cols="2">WER [↓] dev test</cell></row><row><cell cols="2">LibriSpeech En→Fr</cell><cell>6.47</cell><cell>6.47</cell></row><row><cell>IWSLT En→De</cell><cell></cell><cell cols="2">12.36 13.80</cell></row><row><cell cols="4">Table 3: MT results measured in BLEU [%] and TER [%]</cell></row><row><cell cols="3">trained using ground truth source text.</cell></row><row><cell>Task</cell><cell cols="3">BLEU [↑] dev test dev test TER [↓]</cell></row><row><cell cols="4">LibriSpeech En→Fr 20.1 18.2 65.3 67.7</cell></row><row><cell>IWSLT En→De</cell><cell cols="3">30.5 31.5 50.6 -</cell></row><row><cell cols="2">before batching them together.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>ST results using cascaded pipeline of independent ASR and MT models measured in BLEU [%] and TER [%]. We highlight that the cascaded pipeline used more training data compared to the direct model.</figDesc><table><row><cell>Task</cell><cell>BLEU [↑] dev test dev test TER [↓]</cell></row><row><cell cols="2">LibriSpeech En→Fr 17.3 15.7 69.1 70.6</cell></row><row><cell>IWSLT En→De</cell><cell>24.7 24.4 58.9 62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>SpecAugment results on LibriSpeech En→Fr using various augmentation parameters. "-": failed training. The bold augmentation parameters are used in the rest of our experiments.</figDesc><table><row><cell cols="2">F m F</cell><cell cols="2">R m R</cell><cell cols="4">BLEU [%] dev test dev test TER [%]</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">0 15.8 15.2 74.1 75.8</cell></row><row><cell>2</cell><cell>1</cell><cell>0</cell><cell cols="5">0 17.3 16.1 72.5 75.0</cell></row><row><cell>4</cell><cell>1</cell><cell>0</cell><cell cols="5">0 18.0 16.1 72.2 74.9</cell></row><row><cell>4</cell><cell>2</cell><cell>0</cell><cell cols="5">0 17.6 16.1 72.3 74.6</cell></row><row><cell>4</cell><cell>4</cell><cell>0</cell><cell cols="5">0 17.1 16.1 73.2 74.6</cell></row><row><cell>5</cell><cell>1</cell><cell>0</cell><cell cols="5">0 17.6 16.1 72.4 74.7</cell></row><row><cell>8</cell><cell>1</cell><cell>0</cell><cell cols="5">0 17.4 15.7 72.8 75.2</cell></row><row><cell>8</cell><cell>2</cell><cell>0</cell><cell cols="5">0 17.5 15.7 73.0 75.9</cell></row><row><cell>20</cell><cell>1</cell><cell>0</cell><cell cols="5">0 17.2 15.7 72.2 75.0</cell></row><row><cell>20</cell><cell>2</cell><cell>0</cell><cell cols="5">0 16.7 15.4 73.0 75.9</cell></row><row><cell>35</cell><cell>5</cell><cell>0</cell><cell cols="5">0 16.1 15.0 74.1 76.8</cell></row><row><cell>0</cell><cell>0</cell><cell>20</cell><cell cols="5">1 17.2 15.7 73.3 75.5</cell></row><row><cell>0</cell><cell>0</cell><cell>20</cell><cell cols="5">2 17.3 16.2 72.9 75.2</cell></row><row><cell>0</cell><cell>0</cell><cell>40</cell><cell cols="5">1 17.4 15.6 73.2 75.8</cell></row><row><cell>0</cell><cell>0</cell><cell>40</cell><cell cols="5">2 17.3 16.2 72.5 74.7</cell></row><row><cell>0</cell><cell>0</cell><cell>40</cell><cell cols="5">4 16.6 15.6 73.1 75.9</cell></row><row><cell>0</cell><cell>0</cell><cell>80</cell><cell cols="5">1 17.5 16.1 72.5 75.1</cell></row><row><cell>0</cell><cell cols="2">0 100</cell><cell cols="5">1 17.3 15.9 73.8 75.7</cell></row><row><cell>0</cell><cell cols="2">0 100</cell><cell cols="5">2 16.8 15.4 74.1 76.6</cell></row><row><cell>0</cell><cell cols="2">0 200</cell><cell cols="5">2 16.6 15.3 73.8 76.0</cell></row><row><cell>0</cell><cell cols="2">0 400</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>5</cell><cell>1</cell><cell>40</cell><cell cols="5">2 17.9 16.1 72.2 74.6</cell></row><row><cell>5</cell><cell>1</cell><cell>80</cell><cell cols="5">1 17.6 15.6 72.6 75.5</cell></row><row><cell>4</cell><cell>1</cell><cell>40</cell><cell cols="5">2 17.8 16.1 72.6 75.1</cell></row><row><cell>4</cell><cell>2</cell><cell>40</cell><cell cols="5">2 16.8 15.7 72.3 74.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>SpecAugment results on IWSLT En→De using various augmentation parameters.</figDesc><table><row><cell cols="2">F m F</cell><cell cols="2">R m R</cell><cell cols="2">BLEU [%] dev test dev test TER [%]</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">0 16.9 16.5 67.3 70.6</cell></row><row><cell>4</cell><cell>1</cell><cell>0</cell><cell cols="3">0 17.8 17.0 66.3 69.5</cell></row><row><cell>4</cell><cell>2</cell><cell>0</cell><cell cols="3">0 17.3 17.4 66.5 69.3</cell></row><row><cell>5</cell><cell>1</cell><cell>0</cell><cell cols="3">0 18.1 17.5 66.3 68.8</cell></row><row><cell>8</cell><cell>1</cell><cell>0</cell><cell cols="3">0 17.6 17.1 66.2</cell><cell>-</cell></row><row><cell>10</cell><cell>1</cell><cell>0</cell><cell cols="3">0 16.9 16.9 66.9 70.2</cell></row><row><cell>20</cell><cell>1</cell><cell>0</cell><cell cols="2">0 17.0 17.1</cell><cell>-70.3</cell></row><row><cell>0</cell><cell>0</cell><cell>20</cell><cell cols="2">1 17.7 17.6</cell><cell>-70.1</cell></row><row><cell>0</cell><cell>0</cell><cell>20</cell><cell cols="3">2 17.5 17.5 65.8 69.2</cell></row><row><cell>0</cell><cell>0</cell><cell>40</cell><cell cols="3">1 17.5 17.1 66.1 70.0</cell></row><row><cell>0</cell><cell>0</cell><cell>40</cell><cell cols="3">2 17.9 17.5 65.8 69.5</cell></row><row><cell>0</cell><cell>0</cell><cell>60</cell><cell cols="3">1 17.6 17.3 66.6</cell><cell>-</cell></row><row><cell>0</cell><cell>0</cell><cell>80</cell><cell cols="3">1 16.9 17.1 68.6 69.9</cell></row><row><cell>0</cell><cell>0</cell><cell>80</cell><cell cols="3">2 16.8 16.6 68.0 72.0</cell></row><row><cell>0</cell><cell cols="2">0 100</cell><cell cols="3">1 17.7 17.0 66.2</cell><cell>-</cell></row><row><cell>5</cell><cell>1</cell><cell>40</cell><cell cols="3">2 17.8 16.8 66.2 70.9</cell></row><row><cell>4</cell><cell>1</cell><cell>40</cell><cell cols="3">1 17.5 17.2 66.5 69.4</cell></row><row><cell>4</cell><cell>1</cell><cell>40</cell><cell cols="3">2 17.7 18.0 66.0 69.2</cell></row><row><cell>27</cell><cell cols="2">2 100</cell><cell cols="3">2 16.6 16.7 67.6 70.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>SpecAugment results with with a varying amount of training data on LibriSpeech En→Fr. ∆ indicates the absolute difference on average of dev and test sets.</figDesc><table><row><cell cols="2"># segments</cell><cell cols="2">BLEU [%] dev test</cell><cell>∆</cell><cell>TER [%] dev test</cell><cell>∆</cell></row><row><cell cols="5">23k + SpecAug 13.3 7.3 11.6 7.0 1.0</cell><cell>90.2 92.3 -1.2 91.5 93.4</cell></row><row><cell cols="5">47k + SpecAug 17.3 11.2 14.8 10.2 1.8</cell><cell>87.2 89.1 2.8 83.4 87.4</cell></row><row><cell cols="5">71k + SpecAug 16.7 14.4 15.4 13.2 1.1</cell><cell>78.9 81.0 2.1 76.7 79.1</cell></row><row><cell cols="5">94k + SpecAug 17.9 16.1 15.8 15.2 1.5</cell><cell>74.1 75.8 1.6 72.2 74.6</cell></row><row><cell cols="6">our experiments, we use the augmentation parameters which</cell></row><row><cell cols="6">is bold in the tables. For IWSLT, we choose F = 4 rather</cell></row><row><cell cols="6">F = 5, but the rest of the parameters are the same.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dev -w SpecAug</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>dev -w/o SpecAug train -w SpecAug</cell></row><row><cell>log PPL</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>train -w/o SpecAug</cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell></cell><cell>15</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>SpecAugment results with pre-training, which makes use of more training data.</figDesc><table><row><cell></cell><cell cols="2">LibriSpeech En→Fr</cell><cell cols="2">IWSLT En→De</cell></row><row><cell>method</cell><cell>BLEU [%]</cell><cell>TER [%]</cell><cell>BLEU [%]</cell><cell>TER [%]</cell></row><row><cell></cell><cell cols="4">dev test dev test dev test dev test</cell></row><row><cell>direct</cell><cell cols="4">15.8 15.2 74.1 75.8 16.9 16.5 67.3 70.6</cell></row><row><cell>+ pretraining</cell><cell cols="4">18.0 15.8 71.3 73.9 21.1 20.7 62.1 65.5</cell></row><row><cell cols="5">+ SpecAugment 18.5 16.2 71.0 74.5 21.3 20.9 61.9 65.7</cell></row><row><cell cols="2">7 compares the performance improvement of SpecAugment</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with a varying amount of training data. It can be seen that it is</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">helpful in all scenarios irrespective of the amount of training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>data.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://persyval-platform.univ-grenoble-alpes.fr/DS91/detaildataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">https://sites.google.com/site/iwsltevaluation2018/Lectures-task tively. We select our checkpoints based on dev2010 set.For both tasks, we remove the punctuation from the transcriptions (i.e. the English text) and keep the punctuation on the target side. After tokenization using Moses toolkit<ref type="bibr" target="#b28">[29</ref></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">]<ref type="bibr" target="#b5">6</ref> , we apply frequent casing for the IWSLT tasks while lowercase for the LibriSpeech data. Therefore, the evaluation of the IWSLT En→De is case-sensitive, while that of the LibriSpeech is case-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">insensitive<ref type="bibr" target="#b6">7</ref> . The translation models are evaluated using the official scripts of WMT campaign, i.e. BLEU<ref type="bibr" target="#b30">[31]</ref> computed by</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4">mteval-v13a 8 and TER<ref type="bibr" target="#b31">[32]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5">computed by tercom<ref type="bibr" target="#b8">9</ref> . WER is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6">computed by sclite 10 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7">http://www.statmt.org/moses/?n=Moses.SupportTools</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8">We do the case-insensitive evaluation to be comparable with the other works, however, it is not clear which BLEU script they used<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9">ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10">http://www.cs.umd.edu/ snover/tercom/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_11">http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
				<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">Mar. 20-25, 2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inter. Conf. ICASSP</title>
				<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">Mar. 20-25, 2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conf. NIPS</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 7-12. 2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-06">2-6 September 2018</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On using 2d sequence-to-sequence models for speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of transformer and lstm encoder decoder models for asr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<meeting><address><addrLine>Sentosa, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13. 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. September 17-21, 2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards twodimensional sequence to sequence model in neural machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="3009" to="3015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2017, 18th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 20-24, 2017</date>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<idno>abs/1612.01744</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">August 7-12. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging weakly supervised data to improve end-to-end speechto-text translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1811.02050</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH 2015, 16th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 6-10, 2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparative study on end-to-end speech to text translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bieschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<meeting><address><addrLine>Sentosa, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhancing transformer for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A D</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit XVII</title>
				<meeting>Machine Translation Summit XVII<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>European Association for Machine Translation</publisher>
			<date type="published" when="2019-08-23">19-23 Aug. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end speech translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904">1904.08075, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kraif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">May 7-12. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-15">2018. April 15-20. 2018</date>
			<biblScope unit="page" from="6224" to="6228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A corpus of spontaneous speech in lectures: The KIT lecture corpus for spoken language processing and translation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fünfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC</title>
				<meeting>the Ninth International Conference on Language Resources and Evaluation, LREC<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-26">2014. May 26-31, 2014</date>
			<biblScope unit="page" from="1554" to="1559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The iwslt 2018 evaluation campaign</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S M C M T M F J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Workshop on Spoken Language Translation (IWSLT 2018)</title>
				<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">October 29 -30, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gammatone features and feature combination for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bezrukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">Apr. 15-20, 2007</date>
			<biblScope unit="page" from="649" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RASR/NN: the RWTH neural network toolkit for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
				<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">May 4-9, 2014</date>
			<biblScope unit="page" from="3281" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural speech translation at apptek</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wilken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Silvestre-Cerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez-Villaronga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Spoken Language Translation</title>
				<meeting>the 15th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
				<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards unsupervised speech-to-text translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno>abs/1811.01307</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Study of Translation Edit Rate with Targeted Human Annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas</title>
				<meeting>the 7th Conference of the Association for Machine Translation in the Americas<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">August 7-12. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
				<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The rwth aachen machine translation systems for iwslt 2017</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rossenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Spoken Language Translation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
				<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">June 25-29, 2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">6980</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1701.06548</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Connectionist speech recognition: a hybrid approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
