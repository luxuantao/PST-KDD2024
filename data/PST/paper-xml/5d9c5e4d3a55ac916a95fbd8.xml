<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 NEGATIVE SAMPLING IN VARIATIONAL AUTOENCODERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 NEGATIVE SAMPLING IN VARIATIONAL AUTOENCODERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose negative sampling as an approach to improve the notoriously bad out-of-distribution likelihood estimates of Variational Autoencoder models. Our model pushes latent images of negative samples away from the prior. When the source of negative samples is an auxiliary dataset, such a model can vastly improve on baselines when evaluated on OOD detection tasks. Perhaps more surprisingly, we present a fully unsupervised version of employing negative sampling in VAEs: when the generator is trained in an adversarial manner, using the generator's own outputs as negative samples can also significantly improve the robustness of OOD likelihood estimates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning semantically meaningful and useful representations for downstream tasks in an unsupervised manner is a big promise of generative modeling. While a plethora of work demonstrates the effectiveness of deep generative models in this regard, recent work of <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi et al. (2018)</ref> show that these models often fail even at a task that is supposed to be close to their original goal of learning densities. Variational Autoencoders, PixelCNN and flow-based models cannot distinguish common objects like cats and dogs from house numbers. That is, when trained e.g., on CIFAR-10, the models consistently assign higher likelihoods to the elements of the SVHN test set than for the elements of the CIFAR-10 test set or even the elements of the CIFAR-10 train set. As generative models are becoming more and more ubiquitous due to the massive progress in this area in recent years, it is of fundamental importance to understand these phenomena.</p><p>In this work we study Variational Autoencoder (VAE) models, and besides the likelihood, we also investigate to what extent the latent representation of a data point can be used to identify out-ofdistribution (OOD) samples (points that are not from the true data distribution). For this purpose, we consider the KL divergence between the prior and the posterior distribution of a data point as a score to distinguish inliers and outliers. Our contributions are summarized as follows:</p><p>• We demonstrate empirically that the extent of this notorious phenomenon -of bad outof-distribution likelihood estimates -present in VAEs largely depends on the observation model of the VAE. In particular, our experiments show that it diminishes when a Gaussian noise model is considered (with a reasonably sized fixed or learned variance) instead of a Bernoulli. Meanwhile, when examining only the KL divergence between the prior and the posterior distributions in the latent space (instead of the full likelihood), the weak separating capability more consistently prevails between inliers and outliers. • We propose negative sampling in Variational Autoencoders as an approach to alleviate the above weaknesses of the model family. In this method, we introduce an additional prior distribution p(z) in the latent space, where the representations of negative samples are meant to be mapped by the inference model of the VAE machinery. Negative samples can be obtained from an auxiliary dataset, or -to remain completely in the unsupervised setting -from a generative model trained on the ground truth distribution itself. • We present empirical evidence that utilizing negative samples either from an auxiliary dataset or from an adversarial training scheme (using the adversarially trained generative model itself to provide the negative samples) significantly and consistently improves the discriminative power of VAE models regarding out-of-distribution samples.</p><p>The general intuition behind our approach is that if the posterior distribution of each and every point is pulled towards the prior then it is rather natural to expect that the system will map outof-distribution samples close to the prior, as well. This viewpoint suggests that providing negative signals throughout the learning process would be beneficial to enhance the OOD discriminative power of the system. <ref type="bibr" target="#b3">Hendrycks et al. (2019)</ref> demonstrate that utilizing auxiliary datasets as OOD examples (as a supervised signal) significantly improves the performance of existing anomaly detection models on image and text data. First, we study how this approach can be employed in the VAE setting. Beyond that, we also propose a method which remains completely in the unsupervised learning paradigm (without using an auxiliary dataset for supervised signal). The core idea of this unsupervised approach is to use a generative model to provide near-manifold negative samples throughout the training process for which the model is either implicitly or explicitly encouraged to give low likelihood estimates.</p><p>In our proposed method, these negative samples are obtained from the currently trained VAE model itself by utilizing the generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>The generative modeling task aims to model a ground truth data density p * (x) on a space X by learning to generate samples from the corresponding distribution. The learning is done in an unsupervised manner with sampled observables X = {x (i) } N i=1 as training points assumed to be drawn independently from p * (x), where N is the sample size. In latent variable models, the observables are modeled together with hidden variables z on which a prior distribution p(z) is imposed.</p><p>The Variational Autoencoder (VAE) <ref type="bibr" target="#b4">(Kingma &amp; Welling, 2013)</ref> is a latent variable model that takes the maximum likelihood approach and maximizes a lower bound of the sample data log likelihood N i=1 log p θ (x (i) ), where θ are the generator parameters. The utilized lower bound L(θ, φ, x (i) ) (called the ELBO) comes from a variational approximation q φ (z|x (i) ) of the intractable posterior p θ (z|x (i) ), where φ are the variational parameters:</p><formula xml:id="formula_0">log p θ (x (i) ) = log p θ (x (i) |z)p(z) ≥ ≥ E q φ (z|x (i) ) log p θ (x (i) |z) Reconstruction term − D KL (q φ (z|x (i) ) p(z)) KL divergence term L(θ, φ, x (i) ).</formula><p>In the VAE model the parametrized distributions p θ and q φ are modeled with neural networks and are trained jointly to maximize L with some variant of the SGD. The prior is often chosen to be the multivariate standard normal distribution, and a Bernoulli or Gaussian noise model is considered in the observable space to define the likelihood.</p><p>To give likelihood estimates for unseen data points at test time, one can use the trained inference model q φ (z|x (i) ) (also referred to as encoder) and generative model p θ (x (i) |z) (also referred to as decoder) to estimate the ELBO, thus giving a lower bound of the likelihood. Throughout our paper, we are considering these ELBO estimates to measure the likelihood of data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEGATIVE SAMPLING IN VARIATIONAL AUTOENCODERS</head><p>To incorporate negative samples in the VAE training process, we introduce an additional prior distribution p(z) (called the negative prior) on the latent variables z into which the representations of negative samples X = {x (i) } M i=1 are meant to be mapped by the inference model. This is encouraged in the training process by adding to the regular ELBO a new loss term: the KL divergence of the posterior distributions of negative samples to this negative prior. Thus the joint loss function (to be minimized) is as follows:</p><formula xml:id="formula_1">L(θ, φ, x (i) , x(i) ) −L(θ, φ, x (i) ) + D KL (q φ (z|x (i) ) p(z)) = = −E q φ (z|x (i) ) log p θ (x (i) |z) + D KL (q φ (z|x (i) ) p(z)) −1• ELBO for x (i) + D KL (q φ (z|x (i) ) p(z))</formula><p>KL term for negative sample x(i)</p><p>.</p><p>(1)</p><p>Motivating our loss function The loss function defined in equation 1 is still an upper bound of the negative data log likelihood (for the positive samples) as the added loss term is non-negative. The new loss term explicitly imposes the discriminative task for the inference model: to distinguish inliers and outliers in the latent space. With these two components, while still preserving the aim of maximizing the likelihood for inliers, we also expect the implicit behavior of reducing likelihood estimates for outliers. For the outliers, a trained inference model produces latent representations that are close to the negative prior p(z), thus, supposedly far from the prior p(z). Also, the system is not encouraged to learn to generate from the vicinity of the negative prior, therefore not only the KL term of the likelihood, but the reconstruction part of a negative sample is affected when inferring the likelihood estimate of an outlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE CHOICE OF THE NEGATIVE PRIOR</head><p>One has numerous options to choose the positive and negative priors. In this paper, we simply choose to use a standard normal for the positive prior, and a shifted standard normal for the negative prior. With a rotationally symmetric posterior distribution, the distance between the two priors would be the only unspecified hyperparameter of such a model. The assumption of diagonal covariance matrix posterior breaks rotational symmetry in principle, but our exploratory experiments have demonstrated that the magnitude of the shift is a more significant modeling choice than the direction/sparsity of the shift.</p><p>The role of D KL (p(z) p(z)) The magnitude of KL divergence between the negative and the positive prior plays an important role. Larger D KL (p(z) p(z)) values result in larger D KL (q φ (z|x (i) ) p(z)) terms when evaluating the KL divergence term of the likelihood in a trained model, and also result in heavier weighted KL divergence terms during the optimization process. E.g., with a farther shifted negative prior mean, a larger penalty is given for a wrong inference.</p><p>The role of the latent dimension The above argument gives rise to an interesting side effect: increasing the latent dimension also increases D KL (p(z) p(z)), thus resulting in a larger weight of the discriminative KL terms.</p><p>With the above simple choice of the shifted normal for the negative prior, our experiments already demonstrate the effectiveness of our proposed method. One possible direction for further improvement would be to explore positive-negative prior pairs that better reflect the inlier-outlier structure of different datasets. We leave this investigation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SOURCE OF NEGATIVE SAMPLES</head><p>Negative samples can also be obtained in different ways. The task of our models is to generalize from the negative samples as much as possible to all possible out-of-distribution samples, so that they can push down likelihood estimates of those. Depending on the source of negative samples, this generalization can be easier or harder. We conduct experiments with several variants:</p><p>• samples from an auxiliary dataset,</p><p>• the data with isotropic Gaussian noise added, • generated samples from the trained model itself,</p><p>• generated samples utilizing an adversarial training scheme.</p><p>Negative samples that are very far from the data manifold do not facilitate generalization. Noise added to data points is a simple and principled way to sample from the vicinity of the data manifold, but as we will see, it does not provide good generalization. We argue that the reason for this is that discriminating between noisy and noiseless points is too easy for the encoder, so "semantically" the noisy versions are far from the data manifold. In contrast, utilizing samples produced by a generative model (which could be the trained generative model itself) is a more suitable way to acquire nearmanifold negative samples, as we will experimentally demonstrate.</p><p>Why using generated data as negative samples could help? An immediate counterargument against utilizing generated samples as negatives could be the following: for a well-trained model, the generated images are indistinguishable from the ground truth images, so training the model to discriminate them is nonsensical. There are several reasons why such an unsupervised method could still work. First, in practice, a trained generative model is typically not perfect. True data samples and generated samples can be distinguished even for fully trained models. Second, even assuming a perfect generator at the end, during the training process, the generated samples might still help to guide the model toward an equilibrium that promotes a lower likelihood for OOD samples. Moreover, when utilizing auxiliary datasets, we have to choose the auxiliary dataset (or multiple datasets) carefully to wedge in between the training set and a potential out-of-distribution data point, otherwise the weak separating capability could prevail. In contrast, learning to discriminate the generated near-manifold examples from the ground truth data is a harder task, and could result in discriminating a more diverse set of potential out-of-distribution samples.</p><p>Our experiments also confirm this. When utilizing an adversarial training scheme (to be introduced later in this section), generated images not only facilitate our discriminative training procedure, but even achieve a higher level of generalization in the following sense: utilizing generated images improves on the baseline in all permutations of the roles for the grayscale datasets when considering discrimination in the latent space, while utilizing auxiliary datasets fails to achieve notable improvement in some cases. (See the results in Table <ref type="table" target="#tab_0">1</ref> in rows with AUC KL, and more details in the experiments section.)</p><p>In our preliminary experiments, we observed that in some examined cases, utilizing simply the generated images of the currently trained VAE model fail to provide a good signal for the discriminative task. We achieved greater success when we augmented our model with adversarial training. We hypothesize that the reason behind this is that obtained negative samples are richer in features and semantically more meaningful for the task. (See <ref type="bibr" target="#b6">Lee et al. (2018)</ref> for an incarnation of this idea in the context of classification and generative adversarial networks.)</p><p>The utilized adversarial training scheme When experimenting with generated samples as negatives, we utilize an adversarial training scheme where the generator (and only the generator) gets an additional gradient signal through the encoder to map the randomly generated images into the prior. This is encouraged via the following additional loss term:</p><formula xml:id="formula_2">D KL (q φ (z|x (i) ) p(z)),</formula><p>where x(i) denotes a generated image obtained from the generator p θ (x (i) |z), where z is sampled from the prior p(z). Together with the fact that the encoder also gets the generated images as negative samples, this results in an adversarial training procedure. In this setup, the separate loss functions of the encoder and generator are:</p><formula xml:id="formula_3">L adv enc (θ, φ, x (i) , x(i) ) = L(θ, φ, x (i) , x(i) ), L adv gen (θ, φ, x (i) , x(i) ) = L(θ, φ, x (i) , x(i) ) + D KL (q φ (z|x (i) ) p(z)).</formula><p>Our utilized scheme is simple yet effective. However, it is just one of the options. Another choice would be to use a separate generative model with the specific task to provide negative samples. We invite the research community to develop methods that can provide near-manifold examples that can facilitate the training of models with better OOD likelihood properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>The general setup Our main concern is the discriminative power of VAE models regarding outof-distribution samples. Following the conventions of related work, the general experimental setup in this section is as follows: we train a model on a train set of a dataset (e.g. train set of Fashion-MNIST) and then require the model to discriminate between the test set of the train dataset (e.g. test set of Fashion-MNIST) and the test set of an out-of-distribution dataset (e.g. test set of MNIST).</p><p>During the training phase, the models do not encounter examples from the OOD dataset, only at test time are they expected to able to distinguish between inliers and out-of-distribution samples.</p><p>Quantitative assessment For quantitative assessment, we use the threshold independent AUC metric calculated with the bits-per-dimension score (denoted by AUC BPD) and also with the KL divergence of the posterior distribution of a data point to the prior (denoted by AUC KL). All reported numbers in this section are averages of 5 runs with standard deviations denoted in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and experimental details</head><p>We conduct experiments on two sets of datasets: color images of size 32x32 (CIFAR-10, SVHN, downscaled ImageNet) and grayscale images of size 28x28 (MNIST, Fashion-MNIST, Kuzushiji-MNIST, EMNIST-Letters). For both cases, the (positive) prior is chosen to be standard normal, and the latent dimension is set to 100 for color images, and to 10 for grayscale images. For a more detailed description of the utilized datasets, models, and training methodology, see Appendix A. We present generated samples from the models in Appendix D. The samples demonstrate that the models preserve their generative capability even after adding the extra loss terms.</p><p>The choice of the negative prior In our experiments, the negative prior is a standard normal with a shifted mean. For color images it is centered at 25 • 1, for grayscale images it is centered at 8 • 1.</p><p>The magnitude of the shift is set based on a parameter sweep, which was evaluated using Fashion-MNIST and MNIST in the range of {2, 4, 6, 8, 10} for grayscale images, and using CIFAR-10 and SVHN in the range of {5, 10, 15, 20, 25, 30} for color images. After observing a clear trend, we have chosen the mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THE EFFECTIVENESS OF NEGATIVE SAMPLING</head><p>To demonstrate the effectiveness of negative sampling we present two different sets of experiments: first we incorporate negative samples from an auxiliary dataset, second we explore the use of adversarially generated negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Almost perfect discrimination with auxiliary datasets</head><p>The AUC scores in Table <ref type="table" target="#tab_0">1</ref> show that using the auxiliary dataset as a source of negative samples in most cases proved to result in models that are capable to distinguish nearly perfectly between inliers and OOD samples. This is also the case with color images, as experimental results in Table <ref type="table" target="#tab_1">2</ref> show.</p><p>Failure modes with auxiliary datasets One can observe in Table <ref type="table" target="#tab_0">1</ref> that -despite the above mentioned improvements -there are cases when utilizing an auxiliary dataset fails to improve on the OOD separating capability. One example for this is when the inlier set is the EMNIST-Letters, the OOD test set is MNIST, and the utilized auxliary dataset is Fashion-MNIST (the results for this setup are in the last row of Table <ref type="table" target="#tab_0">1</ref>). Showing skirts and boots for the model in training time does not help discriminating between letters and numbers at test time. We hypothesize, that this as an example of the case, when the auxiliary dataset (regarding its features) does not wedge in between the inlier and the outlier test set. One possible way of improvement in this regard is to utilize several auxiliary datasets to present a more diverse set of examples for possible out-of-distribution samples in terms of features and semantic content.</p><p>Of course, the most beneficial would be to train the system to distinguish between the inliers and every possible outlier data points instead of just learning to separate only one or a specific set of auxiliary dataset. This motivates our experiments utilizing generated samples as negative, with the idea that learning to separate from near-manifold examples could facilitate a better generalization in terms of OOD detection.</p><p>Unsupervised method: improvements in all permutations in AUC KL In the case of the grayscale images, the last column in Table <ref type="table" target="#tab_0">1</ref> shows the effectiveness of the fully unsupervised approach: regardless of whether using a Gaussian and a Bernoulli noise model<ref type="foot" target="#foot_0">1</ref> , the trained models achieve higher AUC KL scores than the baseline in all permutations. The method also shows better AUC BPD scores than the baseline in most of the cases where the baseline fails (i.e., baselines with below 0.6 AUC BPD scores). One can observe that when the train set is EMNIST-Letters and the OOD set is MNIST, the separation is still not achieved with this method either. The possible reason behind this is that the visual features of these two datasets are very close to each other and it is a hard task to switch the default relation between them (note that when these two datasets switch roles, the likelihood estimates are correct). Table <ref type="table" target="#tab_1">2</ref> shows that in the case of color images, the unsupervised method also achieves notable discriminative performance improving on the baseline.</p><p>Random noise and additive isotropic Gaussian noise does not help We also investigated how the choice of negative samples influences the performance of the trained model. We conducted further experiments with the following negative samples: 1) Kuzushiji-MNIST<ref type="foot" target="#foot_1">2</ref> (KMNIST) as an another auxiliary dataset, 2) random noise (in which we sample each pixel intensity from the uniform distribution on [0, 1] -modeling a dataset with less structure), 3) with an additive isotropic Gaussian noise added to the inlier dataset.</p><p>The results in Table <ref type="table" target="#tab_2">3</ref> show that utilizing either KMNIST or MNIST-Letters results in perfect separation of the inliers (Fashion-MNIST) and outliers (MNIST). The weak results with random noise as negative samples show the significance of the choice of negative samples. We also experimented with utilizing the training set itself with an additive isotropic Gaussian noise as negative samples -a rather natural choice to provide near-manifold examples. With an additive noise of σ = 0.25,   the results for the AUC BPD metric is 0.44 (0.01) and 0.70 (0.09) for the AUC KL, showing weak discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">THE EFFECT OF THE NOISE MODEL</head><p>Examining the results for baseline VAE models (i.e., models without negative sampling) in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, we can observe great variability in the OOD detection performance.</p><p>The noise model greatly influences the phenomenon The results suggest that the intriguing phenomenon in VAEs discussed by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> and <ref type="bibr" target="#b0">Choi et al. (2018)</ref> is highly dependent on modelling choices. In the case of grayscale images, when changing the noise model from Bernoulli to Gaussian (and otherwise remaining in the same experimental setting as <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>), the issue of assigning higher likelihood estimates to OOD samples simply does not occur. However, one can observe that discrimination between inliers and OOD samples based on the KL divergence between approximate posterior and prior is hardly feasible, with below-1/2 AUC scores. Meanwhile, with a Bernoulli noise model (also used in <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>) both the likelihoodestimates and the KL divergences fail to discriminate. The other results in the table (where models are trained on MNIST) confirm the asymmetric behaviour already described by <ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref>, that is, switching the roles of the inlier and outlier dataset affects the presence of the phenomenon. Concerning experiments with color images, the corresponding rows of Table <ref type="table" target="#tab_1">2</ref> again show the importance of modelling choices. When CIFAR-10 is the training set, the phenomenon persistently occurs with Bernoulli, Gaussian and Quantized Gaussian noise models. When SVHN is the training set, one can observe again a great variability in the AUC scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Our investigations are mostly inspired by and related to recent work on the evaluation of generative models on OOD data <ref type="bibr" target="#b12">(Shafaei et al., 2018;</ref><ref type="bibr" target="#b8">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b0">Choi et al., 2018;</ref><ref type="bibr" target="#b3">Hendrycks et al., 2019)</ref>. These works report that despite intuitive expectations, generative models -including but not limited to VAEs -consistently fail at distinguishing OOD data from the training data, yielding higher likelihood estimates on unseen OOD samples. The ominous observation is presented also by <ref type="bibr" target="#b3">Hendrycks et al. (2019)</ref>, but they concentrate on improving the OOD data detection with Outlier Exposure. Their work demonstrates that utilizing samples from an auxiliary data set as OOD examples, i.e., training models to discriminate between training and auxiliary samples, significantly improves on the performance of existing OOD detection models on image and text data. However, they do not investigate the VAE model, and their general setup always requires an auxiliary dataset. Our work also sheds light on an issue with this approach: one should choose the auxiliary datasets carefully to obtain robust OOD detection.</p><p>Within the context of uncertainty estimation, <ref type="bibr" target="#b6">Lee et al. (2018)</ref> demonstrate that adversarially generated samples improve the confidence of classifiers in their correct predictions. They train a classifier simultaneously with a GAN and require it to have lower confidence on GAN samples. For each class distribution, they tune the classifier and GAN using samples from that OOD dataset. Their method of utilizing generated samples of GANs is closest to our approach of using generated data points as negative samples, but <ref type="bibr" target="#b6">Lee et al. (2018)</ref> work within a classification setting. <ref type="bibr" target="#b9">Nalisnick et al. (2019b)</ref> propose a solution that can alleviate the issue without modifying existing generative models, but the issue they aim to address (distributional shift) is very different from the standard concerns of OOD sample detection. Their model works by using the likelihood estimates coming from likelihood-based models as inputs to detect distributional shift, as opposed to using them as raw OOD sample detectors. The model operates under the assumption that at evaluation time, samples come in batches, and thus can be the inputs of statistical tests differentiating between likelihood estimates for inlier datasets and likelihood estimates for evaluation datasets. In the limiting case where the evaluation dataset has batch-size 1, the performance of this model can be compared meaningfully with our unsupervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this work, we studied Variational Autoencoder models and investigated to what extent the latent representations of data points or the likelihood estimates given by the model can be used to identify out-of-distribution samples. We demonstrated empirically that the extent of the notorious phenomenon of wrong out-of-distribution likelihood estimates present in VAEs is highly dependent on the observation model. We introduced negative sampling as an approach to alleviate the above weakness of the Variational Autoencoder model family. We presented empirical evidence that utilizing negative samples either from an auxiliary dataset or from an adversarial training scheme significantly and consistently improves the discriminative power of VAE models regarding out-ofdistribution samples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RECONSTRUCTION OF NEGATIVES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Results for the baseline (i.e., VAE without negative sampling) are indicated again in the first column for comparison. Samples from the different data sets are also depicted in the last row to show their general visual characteristics. 08) 0.32 (0.04) 1.10 (0.09) 1.44 (0.20) 10 18 (10 19 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Nalisnick et al. (2019a)</ref> examine the phenomenon in detail, focusing on finding the cause of it by analyzing flow-based models that allow exact likelihood calculation.Choi et al. (2018) also notice the above-mentioned phenomenon, while they address the task of OOD sample detection with Generative Ensembles. They decrease the weight of the KL divergence term in the ELBO (contrarily to what is promoted by the β-VAE loss function) to encourage a higher distortion penalty during training, resulting in a better performing model. This observation also confirms the importance of the noise model and the balance between the KL and the reconstruction term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing the out-of-distribution discriminative power of baseline VAE models and VAE models with negative sampling on grayscale images. Numbers for all permutations with the different possible roles of the three datasets (MINST, Fashion-MNIST and EMNIST-Letters) are reported. When an auxiliary dataset is utilized, the auxiliary dataset is the one out of the three that is not utilized neither as inlier nor for OOD testing purposes.</figDesc><table><row><cell></cell><cell>Inlier</cell><cell>OOD</cell><cell>Noise model</cell><cell>Baseline VAE Negative: Negative: (no negative) auxiliary adversarial</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>MNIST</cell><cell>Bernoulli</cell><cell>0.46 (0.05) 1.00 (0.00) 0.70 (0.13)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>MNIST</cell><cell>Gaussian</cell><cell>0.98 (0.00) 1.00 (0.00) 0.80 (0.04)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>Letters</cell><cell>Bernoulli</cell><cell>0.61 (0.01) 0.99 (0.00) 0.78 (0.07)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>Letters</cell><cell>Gaussian</cell><cell>0.97 (0.00) 1.00 (0.00) 0.85 (0.04)</cell></row><row><cell>AUC BPD</cell><cell>MNIST MNIST MNIST MNIST</cell><cell cols="2">Fashion-MNIST Bernoulli Fashion-MNIST Gaussian Letters Bernoulli Letters Gaussian</cell><cell>1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 0.97 (0.00) 1.00 (0.00) 0.98 (0.01) 0.99 (0.00) 0.99 (0.00) 0.98 (0.00) 0.78 (0.14) 0.93 (0.08) 0.79 (0.04)</cell></row><row><cell></cell><cell>Letters</cell><cell cols="2">Fashion-MNIST Bernoulli</cell><cell>0.98. (0.00) 0.98 (0.00) 0.99 (0.00)</cell></row><row><cell></cell><cell>Letters</cell><cell cols="2">Fashion-MNIST Gaussian</cell><cell>0.80 (0.07) 0.76 (0.08) 0.93 (0.04)</cell></row><row><cell></cell><cell>Letters</cell><cell>MNIST</cell><cell>Bernoulli</cell><cell>0.58 (0.02) 0.58 (0.02) 0.73 (0.07)</cell></row><row><cell></cell><cell>Letters</cell><cell>MNIST</cell><cell>Gaussian</cell><cell>0.67 (0.17) 0.58 (0.20) 0.65 (0.04)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>MNIST</cell><cell>Bernoulli</cell><cell>0.61 (0.09) 1.00 (0.00) 0.88 (0.07)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>MNIST</cell><cell>Gaussian</cell><cell>0.26 (0.03) 1.00 (0.00) 0.74 (0.05)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>Letters</cell><cell>Bernoulli</cell><cell>0.68 (0.07) 1.00 (0.00) 0.84 (0.04)</cell></row><row><cell></cell><cell>Fashion-MNIST</cell><cell>Letters</cell><cell>Gaussian</cell><cell>0.38 (0.04) 0.99 (0.00) 0.79 (0.05)</cell></row><row><cell>AUC KL</cell><cell>MNIST MNIST MNIST MNIST</cell><cell cols="2">Fashion-MNIST Bernoulli Fashion-MNIST Gaussian Letters Bernoulli Letters Gaussian</cell><cell>0.73 (0.14) 1.00 (0.00) 0.94 (0.10) 0.71 (0.04) 1.00 (0.00) 0.98 (0.01) 0.64 (0.03) 0.76 (0.03) 0.89 (0.02) 0.54 (0.07) 0.75 (0.08) 0.74 (0.04)</cell></row><row><cell></cell><cell>Letters</cell><cell cols="2">Fashion-MNIST Bernoulli</cell><cell>0.66 (0.14) 0.54 (0.09) 0.98 (0.00)</cell></row><row><cell></cell><cell>Letters</cell><cell cols="2">Fashion-MNIST Gaussian</cell><cell>0.54 (0.10) 0.49 (0.23) 0.91 (0.05)</cell></row><row><cell></cell><cell>Letters</cell><cell>MNIST</cell><cell>Bernoulli</cell><cell>0.37 (0.05) 0.45 (0.03) 0.75 (0.06)</cell></row><row><cell></cell><cell>Letters</cell><cell>MNIST</cell><cell>Gaussian</cell><cell>0.36 (0.08) 0.43 (0.10) 0.64 (0.04)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparing baseline VAEs and VAEs with negative sampling with Bernoulli, Gaussian, and Quantized Gaussian (Q. Gaussian) noise models on color image datasets.</figDesc><table><row><cell></cell><cell>Inlier</cell><cell>OOD</cell><cell>Noise model</cell><cell>Baseline VAE Negative: Negative: (no negative) auxiliary adversarial</cell></row><row><cell>AUC BPD</cell><cell cols="4">CIFAR-10 SVHN CIFAR-10 SVHN CIFAR-10 SVHN Q. Gaussian 0.19 (0.00) 0.92 (0.03) 0.82 (0.03) Bernoulli 0.59 (0.00) 0.90 (0.05) 0.81 (0.04) Gaussian 0.25 (0.02) 0.93 (0.01) 0.84 (0.03) SVHN CIFAR-10 Bernoulli 0.51 (0.00) 1.00 (0.00) 0.70 (0.03) SVHN CIFAR-10 Gaussian 0.92 (0.00) 1.00 (0.00) 0.75 (0.11)</cell></row><row><cell>AUC KL</cell><cell cols="4">CIFAR-10 SVHN CIFAR-10 SVHN CIFAR-10 SVHN Q. Gaussian 0.28 (0.01) 0.92 (0.03) 0.82 (0.03) Bernoulli 0.29 (0.00) 0.90 (0.06) 0.81 (0.04) Gaussian 0.25 (0.01) 0.93 (0.01) 0.84 (0.03) SVHN CIFAR-10 Bernoulli 0.87 (0.00) 1.00 (0.00) 0.70 (0.03) SVHN CIFAR-10 Gaussian 0.74 (0.01) 1.00 (0.00) 0.74 (0.11)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparing baseline model and negative sampling with different sources for negatives. Columns correspond to different sources for negative samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparing the discriminative power of VAE models with negative sampling with Bernoulli noise model, with Fashion-MNIST and MNIST as inlier and OOD datasets, respectively, when reconstruction of negative samples from EMNIST-Letters is also taken into account, with α weight. One can observe that the models are able to reconstruct OOD samples, while the discriminative power does not diminish. The same was observed when using Gaussian noise model. Test BPD 0.305 0.304 0.305 0.306 0.307 0.307 0.308 0.308 0.308 0.309 0.31 (0.00)OOD BPD 1.521 0.681 0.636 0.639 0.641 0.632 0.638 0.634 0.638 0.645 0.59 (0.03)AUC KL 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 (0.00) Test KL 15.12 15.73 15.26 15.85 16.38 16.53 16.59 16.66 16.94 17.04 15.86 (0.23) OOD KL 325.1 325.4 325.5 340.2 350.4 343.0 354.3 350.1 353.6 363.2 321.9 (19.9)</figDesc><table><row><cell>α</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell cols="12">AUC BPD 0.999 0.999 0.998 0.998 0.998 0.998 0.998 0.997 0.997 0.998 0.99 (0.01)</cell></row><row><cell>α Inlier</cell><cell></cell><cell></cell><cell>OOD</cell><cell></cell><cell cols="7">Generated Reconstructed Reconstructed Reconstructed</cell></row><row><cell cols="2">Fashion-MNIST</cell><cell></cell><cell>MNIST</cell><cell></cell><cell>samples</cell><cell></cell><cell cols="5">train samples test samples OOD samples</cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparing the discriminative performance of baseline VAE models with different latent dimension sizes, trained on Fashion-MNIST, and MNIST used as OOD dataset, with Bernoulli noise model. First column corresponds to our default setup. Reconstructed training samples and generated samples from the models are also provided. Our exploratory experiments indicate that simply increasing the latent dimension size does not help to overcome the problem of assigning higher likelihoods to OOD data, and even the generative performance is diminishing.</figDesc><table><row><cell>Latent dimension</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>250</cell><cell>500</cell></row><row><cell>AUC BPD</cell><cell>0.46 (0.05)</cell><cell>0.35</cell><cell>0.35</cell><cell>0.39</cell><cell>0.34</cell></row><row><cell>AUC KL</cell><cell>0.61 (0.09)</cell><cell>0.76</cell><cell>0.73</cell><cell>0.42</cell><cell>0.64</cell></row><row><cell>Test BPD</cell><cell>0.30 (0.00)</cell><cell>0.31</cell><cell>0.32</cell><cell>0.33</cell><cell>0.34</cell></row><row><cell>OOD BPD</cell><cell>0.35 (0.08)</cell><cell>0.27</cell><cell>0.27</cell><cell>0.29</cell><cell>0.29</cell></row><row><cell>Test KL</cell><cell>15.61 (0.55)</cell><cell>16.06</cell><cell>16.14</cell><cell>18.02</cell><cell>17.65</cell></row><row><cell>OOD KL</cell><cell>31.91 (16.89)</cell><cell>19.37</cell><cell>18.70</cell><cell>17.43</cell><cell>18.75</cell></row><row><cell>Reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Generated samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>First two coordinates of the latent space of baseline VAE and VAE with negative sampling, with Bernoulli noise model, trained on Fashion-MNIST, and MNIST used as OOD dataset.</figDesc><table><row><cell>Epoch 1</cell><cell>Epoch 10</cell><cell>Epoch 100</cell></row><row><cell>Baseline VAE</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Even though the Bernoulli noise model might not be a particularly good choice for modeling grayscale or color images, here we follow the literature when considering it as a baseline.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">EMNIST-Letters, Kuzushiji-MNIST and Fashion-MNIST are datasets that can be utilized as drop-in replacements for MNIST.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL DETAILS A.1 DATASETS AND PREPROCESSING</head><p>We conduct experiments with two types of data set: color images of size 32x32 and grayscale images of size 28x28. The utilized datasets are listed below.</p><p>Datasets of grayscale images of size 28x28:</p><p>• MNIST <ref type="bibr" target="#b5">(LeCun et al., 2010)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes • Fashion-MNIST <ref type="bibr" target="#b14">(Xiao et al., 2017)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes</p><p>• Kuzushiji-MNIST <ref type="bibr" target="#b1">(Clanuwat et al., 2018)</ref>: 28x28x1, 60.000 train + 10.000 test, 10 classes  <ref type="bibr">(2018)</ref>, all of the models are trained with the RMSProp optimizer with learning rate set to 10 −4 . We train the models for 100 epochs with mini-batch size of 50. We update the parameters of the encoder and decoder network in an alternating fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details for color images</head><p>For color images we use a DCGAN-style CNN architecture with Conv-BatchNorm-ReLU modules for both the encoder and the decoder. The size of the kernels are 4 × 4, and the number of filters are 32, 64, 128 for the encoder; and 128, 64, 1 for the decoder. All of the models are trained with the Adam optimizer (β 1 = 0.9, β 2 = 0.999) for 100 epochs with mini-batch size 50. The learning rate is set to 10 −4 . We update the parameters of the encoder and decoder network in an alternating fashion. When generated images are used as negative samples, we employ spectral normalization <ref type="bibr" target="#b7">(Miyato et al., 2018)</ref> for the convolutional weights of the encoder in order to stabilize and enhance the performance of the respective models, and in this case the models are trained for 300 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D GENERATED SAMPLES</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Waic, but why? generative ensembles for robust anomaly detection</title>
		<author>
			<persName><forename type="first">Hyunsun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2017.7966217</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxCxhRcY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Alex Krizhevsky. Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2009">2013. 2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Auto-encoding variational bayes</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>ATT Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryiAv2xAZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1xwNhCcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution inputs to deep generative models using a test for typicality</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02994</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06847</idno>
		<title level="m">Distribution matching in variational inference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Does your model know the digit 6 is not a cat? a less biased evaluation of</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04729</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">outlier&quot; detectors. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR, abs/1601.06759</idno>
		<ptr target="http://arxiv.org/abs/1601.06759" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
