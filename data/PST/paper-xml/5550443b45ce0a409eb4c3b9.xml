<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards End-to-End Speech Recognition with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
							<email>graves@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
							<email>ndjaitly@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards End-to-End Speech Recognition with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in algorithms and computer hardware have made it possible to train neural networks in an endto-end fashion for tasks that previously required significant human expertise. For example, convolutional neural networks are now able to directly classify raw pixels into high-level concepts such as object categories <ref type="bibr" target="#b17">(Krizhevsky et al., 2012)</ref> and messages on traffic signs <ref type="bibr" target="#b3">(Ciresan et al., 2011)</ref>, without using hand-designed feature extraction algorithms. Not only do such networks require less human effort than traditional approaches, they generally deliver superior performance. This is particularly true when very large amounts of training data are available, as the bene- fits of holistic optimisation tend to outweigh those of prior knowledge.</p><p>While automatic speech recognition has greatly benefited from the introduction of neural networks <ref type="bibr" target="#b2">(Bourlard &amp; Morgan, 1993;</ref><ref type="bibr" target="#b13">Hinton et al., 2012)</ref>, the networks are at present only a single component in a complex pipeline. As with traditional computer vision, the first stage of the pipeline is input feature extraction: standard techniques include mel-scale filterbanks <ref type="bibr" target="#b4">(Davis &amp; Mermelstein, 1980)</ref> (with or without a further transform into Cepstral coefficients) and speaker normalisation techniques such as vocal tract length normalisation <ref type="bibr" target="#b18">(Lee &amp; Rose, 1998)</ref>. Neural networks are then trained to classify individual frames of acoustic data, and their output distributions are reformulated as emission probabilities for a hidden Markov model (HMM). The objective function used to train the networks is therefore substantially different from the true performance measure (sequence-level transcription accuracy). This is precisely the sort of inconsistency that end-to-end learning seeks to avoid. In practice it is a source of frustration to researchers, who find that a large gain in frame accuracy can translate to a negligible improvement, or even deterioration in transcription accuracy. An additional problem is that the frame-level training targets must be inferred from the alignments determined by the HMM. This leads to an awkward iterative procedure, where network retraining is alternated with HMM re-alignments to generate more accurate targets. Full-sequence training methods such as Maximum Mutual Information have been used to directly train HMM-neural network hybrids to maximise the probability of the correct transcription <ref type="bibr" target="#b0">(Bahl et al., 1986;</ref><ref type="bibr" target="#b16">Jaitly et al., 2012)</ref>. However these techniques are only suitable for retraining a system already trained at frame-level, and require the careful tuning of a large number of hyper-parameterstypically even more than the tuning required for deep neural networks.</p><p>While the transcriptions used to train speech recognition systems are lexical, the targets presented to the networks are usually phonetic. A pronunciation dictionary is therefore needed to map from words to phoneme sequences. Creating such dictionaries requires significant human effort and often proves critical to overall performance. A further complication is that, because multi-phone contextual models are used to account for co-articulation effects, 'state tying' is needed to reduce the number of target classesanother source of expert knowledge. Lexical states such as graphemes and characters have been considered for HMMbased recognisers as a way of dealing with out of vocabulary (OOV) words <ref type="bibr" target="#b6">(Galescu, 2003;</ref><ref type="bibr" target="#b1">Bisani &amp; Ney, 2005)</ref>, however they were used to augment rather than replace phonetic models.</p><p>Finally, the acoustic scores produced by the HMM are combined with a language model trained on a text corpus. In general the language model contains a great deal of prior information, and has a huge impact on performance. Modelling language separately from sound is perhaps the most justifiable departure from end-to-end learning, since it is easier to learn linguistic dependencies from text than speech, and arguable that literate humans do the same thing. Nonetheless, with the advent of speech corpora containing tens of thousands of hours of labelled data, it may be possible to learn the language model directly from the transcripts.</p><p>The goal of this paper is a system where as much of the speech pipeline as possible is replaced by a single recurrent neural network (RNN) architecture. Although it is possible to directly transcribe raw speech waveforms with RNNs <ref type="bibr">(Graves, 2012, Chapter 9)</ref> or features learned with restricted Boltzmann machines <ref type="bibr" target="#b15">(Jaitly &amp; Hinton, 2011)</ref>, the computational cost is high and performance tends to be worse than conventional preprocessing. We have therefore chosen spectrograms as a minimal preprocessing scheme.</p><p>The spectrograms are processed by a deep bidirectional LSTM network <ref type="bibr" target="#b10">(Graves et al., 2013)</ref> with a Connectionist Temporal Classification (CTC) output layer <ref type="bibr" target="#b9">(Graves et al., 2006;</ref><ref type="bibr">Graves, 2012, Chapter 7)</ref>. The network is trained directly on the text transcripts: no phonetic representation (and hence no pronunciation dictionary or state tying) is used. Furthermore, since CTC integrates out over all possible input-output alignments, no forced alignment is required to provide training targets. The combination of bidirectional LSTM and CTC has been applied to characterlevel speech recognition before <ref type="bibr" target="#b5">(Eyben et al., 2009)</ref>, however the relatively shallow architecture used in that work did not deliver compelling results (the best character error rate was almost 20%).</p><p>The basic system is enhanced by a new objective function that trains the network to directly optimise the word error rate.</p><p>Experiments on the Wall Street Journal speech corpus demonstrate that the system is able to recognise words to reasonable accuracy, even in the absence of a language model or dictionary, and that when combined with a language model it performs comparably to a state-of-the-art pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network Architecture</head><p>Given an input sequence x = (x 1 , . . . , x T ), a standard recurrent neural network (RNN) computes the hidden vector sequence h = (h 1 , . . . , h T ) and output vector sequence y = (y 1 , . . . , y T ) by iterating the following equations from t = 1 to T :</p><formula xml:id="formula_0">h t = H (W ih x t + W hh h t−1 + b h ) (1) y t = W ho h t + b o (2)</formula><p>where the W terms denote weight matrices (e.g. W ih is the input-hidden weight matrix), the b terms denote bias vectors (e.g. b h is hidden bias vector) and H is the hidden layer activation function.</p><p>H is usually an elementwise application of a sigmoid function. However we have found that the Long Short-Term Memory (LSTM) architecture <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref>, which uses purpose-built memory cells to store information, is better at finding and exploiting long range context. Fig. <ref type="figure" target="#fig_1">1</ref> illustrates a single LSTM memory cell. For the version of LSTM used in this paper <ref type="bibr" target="#b7">(Gers et al., 2002)</ref> H is implemented by the following composite function:</p><formula xml:id="formula_1">i t = σ (W xi x t + W hi h t−1 + W ci c t−1 + b i ) (3) f t = σ (W xf x t + W hf h t−1 + W cf c t−1 + b f ) (4) c t = f t c t−1 + i t tanh (W xc x t + W hc h t−1 + b c ) (5) o t = σ (W xo x t + W ho h t−1 + W co c t + b o ) (6) h t = o t tanh(c t ) (7)</formula><p>where σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors, all of which are the same size as the hidden vector h. The weight matrix subscripts have the obvious meaning, for example W hi is the hidden-input gate matrix, W xo is the input-output gate matrix etc. The weight matrices from the cell to gate vectors (e.g. W ci ) are diagonal, so element m in each gate vector only receives input from element m of the cell vector. The bias terms (which are added to i, f , c and o) have been omitted for clarity.</p><p>One shortcoming of conventional RNNs is that they are only able to make use of previous context. In speech recognition, where whole utterances are transcribed at once, there is no reason not to exploit future context as well. Bidirectional RNNs (BRNNs) <ref type="bibr" target="#b21">(Schuster &amp; Paliwal, 1997</ref>)  do this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer. As illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, a BRNN computes the forward hidden sequence − → h , the backward hidden sequence ← − h and the output sequence y by iterating the backward layer from t = T to 1, the forward layer from t = 1 to T and then updating the output layer:</p><formula xml:id="formula_2">− → h t = H W x − → h x t + W− → h − → h − → h t−1 + b− → h (8) ← − h t = H W x ← − h x t + W← − h ← − h ← − h t+1 + b← − h (9) y t = W− → h y − → h t + W← − h y ← − h t + b o<label>(10)</label></formula><p>Combing BRNNs with LSTM gives bidirectional LSTM <ref type="bibr" target="#b8">(Graves &amp; Schmidhuber, 2005)</ref>, which can access long-range context in both input directions.</p><p>A crucial element of the recent success of hybrid systems is the use of deep architectures, which are able to build up progressively higher level representations of acoustic data.</p><p>Deep RNNs can be created by stacking multiple RNN hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Assuming the same hidden layer function is used for all N layers in the stack, the hidden vector sequences h n are iteratively computed from n = 1 to N and t = 1 to T :</p><formula xml:id="formula_3">h n t = H W h n−1 h n h n−1 t + W h n h n h n t−1 + b n h<label>(11)</label></formula><p>where h 0 = x. The network outputs y t are</p><formula xml:id="formula_4">y t = W h N y h N t + b o<label>(12)</label></formula><p>Deep bidirectional RNNs can be implemented by replacing each hidden sequence h n with the forward and backward sequences − → h n and ← − h n , and ensuring that every hidden layer receives input from both the forward and backward layers at the level below. If LSTM is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM <ref type="bibr" target="#b10">(Graves et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Connectionist Temporal Classification</head><p>Neural networks (whether feedforward or recurrent) are typically trained as frame-level classifiers in speech recognition. This requires a separate training target for every frame, which in turn requires the alignment between the audio and transcription sequences to be determined by the HMM. However the alignment is only reliable once the classifier is trained, leading to a circular dependency between segmentation and recognition (known as Sayre's paradox in the closely-related field of handwriting recognition). Furthermore, the alignments are irrelevant to most speech recognition tasks, where only the word-level transcriptions matter. Connectionist Temporal Classification (CTC) <ref type="bibr">(Graves, 2012, Chapter 7</ref>) is an objective function that allows an RNN to be trained for sequence transcription tasks without requiring any prior alignment between the input and target sequences.</p><p>The output layer contains a single unit for each of the transcription labels (characters, phonemes, musical notes etc.), plus an extra unit referred to as the 'blank' which corresponds to a null emission. Given a length T input sequence x, the output vectors y t are normalised with the softmax function, then interpreted as the probability of emitting the label (or blank) with index k at time t:</p><formula xml:id="formula_5">Pr(k, t|x) = exp y k t k exp y k t (13)</formula><p>where</p><formula xml:id="formula_6">y k t is element k of y t .</formula><p>A CTC alignment a is a length T sequence of blank and label indices. The probability Pr(a|x) of a is the product of the emission probabilities at every time-step:</p><formula xml:id="formula_7">Pr(a|x) = T t=1 Pr(a t , t|x)<label>(14)</label></formula><p>For a given transcription sequence, there are as many possible alignments as there different ways of separating the labels with blanks. For example (using '−' to denote blanks) the alignments Denoting by B an operator that removes first the repeated labels, then the blanks from alignments, and observing that the total probability of an output transcription y is equal to the sum of the probabilities of the alignments corresponding to it, we can write</p><formula xml:id="formula_8">Pr(y|x) = a∈B −1 (y) Pr(a|x)<label>(15)</label></formula><p>This 'integrating out' over possible alignments is what allows the network to be trained with unsegmented data. The intuition is that, because we don't know where the labels within a particular transcription will occur, we sum over all the places where they could occur. Eq. ( <ref type="formula" target="#formula_8">15</ref>) can be efficiently evaluated and differentiated using a dynamic programming algorithm <ref type="bibr" target="#b9">(Graves et al., 2006)</ref>. Given a target transcription y * , the network can then be trained to minimise the CTC objective function:</p><formula xml:id="formula_9">CT C(x) = − log Pr(y * |x) (16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expected Transcription Loss</head><p>The CTC objective function maximises the log probability of getting the sequence transcription completely correct. The relative probabilities of the incorrect transcriptions are therefore ignored, which implies that they are all equally bad. In most cases however, transcription performance is assessed in a more nuanced way. In speech recognition, for example, the standard measure is the word error rate (WER), defined as the edit distance between the true word sequence and the most probable word sequence emitted by the transcriber. We would therefore prefer transcriptions with high WER to be more probable than those with low WER. In the interest of reducing the gap between the objective function and the test criteria, this section proposes a method that allows an RNN to be trained to optimise the expected value of an arbitrary loss function defined over output transcriptions (such as WER).</p><p>The network structure and the interpretation of the output activations as the probability of emitting a label (or blank) at a particular time-step, remain the same as for CTC.</p><p>Given input sequence x, the distribution Pr(y|x) over transcriptions sequences y defined by CTC, and a real-valued transcription loss function L(x, y), the expected transcription loss L(x) is defined as</p><formula xml:id="formula_10">L(x) = y Pr(y|x)L(x, y)<label>(17)</label></formula><p>In general we will not be able to calculate this expectation exactly, and will instead use Monte-Carlo sampling to approximate both L and its gradient. Substituting Eq. ( <ref type="formula" target="#formula_8">15</ref>) into Eq. ( <ref type="formula" target="#formula_10">17</ref>) we see that</p><formula xml:id="formula_11">L(x) = y a∈B −1 (y)</formula><p>Pr(a|x)L(x, y) </p><p>Eq. ( <ref type="formula" target="#formula_7">14</ref>) shows that samples can be drawn from Pr(a|x) by independently picking from Pr(k, t|x) at each time-step and concatenating the results, making it straightforward to approximate the loss:</p><formula xml:id="formula_14">L(x) ≈ 1 N N i=1 L(x, B(a i )), a i ∼ Pr(a|x)<label>(20)</label></formula><p>To differentiate L with respect to the network outputs, first observe from Eq. ( <ref type="formula">13</ref>) that</p><formula xml:id="formula_15">∂ log P r(a|x) ∂ Pr(k, t|x) = δ atk Pr(k, t|x)<label>(21)</label></formula><p>Then substitute into Eq. ( <ref type="formula" target="#formula_13">19</ref>), applying the identity This expectation can also be approximated with Monte-Carlo sampling. Because the output probabilities are independent, an unbiased sample a i from Pr(a|x) can be converted to an unbiased sample from Pr(a|x, a t = k) by setting a i t = k. Every a i can therefore be used to provide a gradient estimate for every Pr(k, t|x) as follows:</p><formula xml:id="formula_16">∇ x f (x) = f (x)∇ x log f (x), to yield: ∂L(x) ∂ Pr(k, t|x) = a ∂ Pr(a|x) ∂ Pr(k, t|x) L(x, B<label>(</label></formula><formula xml:id="formula_17">∂L(x) ∂ Pr(k, t|x) ≈ 1 N N i=1 L(x, B(a i,t,k ))<label>(22)</label></formula><p>with a i ∼ Pr(a|x) and a i,t,k t = a i t ∀t = t, a i,t,k t = k. The advantage of reusing the alignment samples (as opposed to picking separate alignments for every k, t) is that the noise due to the loss variance largely cancels out, and only the difference in loss due to altering individual labels is added to the gradient. As has been widely discussed in the policy gradients literature and elsewhere <ref type="bibr" target="#b19">(Peters &amp; Schaal, 2008)</ref>, noise minimisation is crucial when optimising with stochastic gradient estimates. The Pr(k, t|x) derivatives are passed through the softmax function to give:</p><formula xml:id="formula_18">∂L(x) ∂y k t ≈ Pr(k, t|x) N N i=1 L(x, B(a i )) − Z(a i , t)</formula><p>where</p><formula xml:id="formula_19">Z(a i , t) = k Pr(k , t|x)L(x, B(a i,t,k ))</formula><p>The derivative added to y k t by a given a i is therefore equal to the difference between the loss with a i t = k, and the expected loss with a i t sampled from Pr(k , t|x). This means the network only receives an error term for changes to the alignment that alter the loss. For example, if the loss function is the word error rate and the sampled alignment yields the character transcription "WTRD ERROR RATE" the gradient would encourage outputs changing the second output label to 'O', discourage outputs making changes to the other two words and be close to zero everywhere else.</p><p>For the sampling procedure to be effective, there must be a reasonable probability of picking alignments whose variants receive different losses. The vast majority of alignments drawn from a randomly initialised network will give completely wrong transcriptions, and there will therefore be little chance of altering the loss by modifying a single output. We therefore recommend that expected loss minimisation is used to retrain a network already trained with CTC, rather than applied from the start.</p><p>Sampling alignments is cheap, so the only significant computational cost in the procedure is recalculating the loss for the alignment variants. However, for many loss functions (including word error rate) this could be optimised by only recalculating that part of the loss corresponding to the alignment change. For our experiments, five samples per sequence gave sufficiently low variance gradient estimates for effective training.</p><p>Note that, in order to calculate the word error rate, an endof-word label must be used as a delimiter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Decoding</head><p>Decoding a CTC network (that is, finding the most probable output transcription y for a given input sequence x) can be done to a first approximation by picking the single most probable output at every timestep and returning the corresponding transcription: argmax y Pr(y|x) ≈ B (argmax a Pr(a|x))</p><p>More accurate decoding can be performed with a beam search algorithm, which also makes it possible to integrate a language model. The algorithm is similar to decoding methods used for HMM-based systems, but differs slightly due to the changed interpretation of the network outputs. In a hybrid system the network outputs are interpreted as posterior probabilities of state occupancy, which are then combined with transition probabilities provided by a language model and an HMM. With CTC the network outputs themselves represent transition probabilities (in HMM terms, the label activations are the probability of making transitions into different states, and the blank activation is the probability of remaining in the current state). The situation is further complicated by the removal of repeated label emissions on successive time-steps, which makes it necessary to distinguish alignments ending with blanks from those ending with labels.</p><p>The pseudocode in Algorithm 1 describes a simple beam search procedure for a CTC network, which allows the integration of a dictionary and language model. Define Pr − (y, t), Pr + (y, t) and Pr(y, t) respectively as the blank, non-blank and total probabilities assigned to some (partial) output transcription y, at time t by the beam search, and set Pr(y, t) = Pr − (y, t) + Pr + (y, t). Define the extension probability Pr(k, y, t) of y by label k at time t as follows:</p><formula xml:id="formula_20">Pr(k, y, t) = Pr(k, t|x) Pr(k|y) Pr − (y, t − 1) if y e = k Pr(y, t − 1) otherwise</formula><p>where Pr(k, t|x) is the CTC emission probability of k at t, as defined in Eq. ( <ref type="formula">13</ref>), Pr(k|y) is the transition probability from y to y + k and y e is the final label in y. Lastly, define ŷ as the prefix of y with the last label removed, and ∅ as the empty sequence, noting that Pr + (∅, t) = 0 ∀t. such knowledge is present (as with standard CTC) then all Pr(k|y) are set to 1. Constraining the search to dictionary words can be easily implemented by setting Pr(k|y) = 1 if (y + k) is in the dictionary and 0 otherwise. To apply a statistical language model, note that Pr(k|y) should represent normalised label-to-label transition probabilities. To convert a word-level language model to a label-level one, first note that any label sequence y can be expressed as the concatenation y = (w + p) where w is the longest complete sequence of dictionary words in y and p is the remaining word prefix. Both w and p may be empty. Then we can write</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The transition probabilities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pr(k|y) =</head><p>w ∈(p+k) * Pr γ (w |w)</p><formula xml:id="formula_21">w ∈p * Pr γ (w |w)<label>(23)</label></formula><p>where Pr(w |w) is the probability assigned to the transition from the word history w to the word w , p * is the set of dictionary words prefixed by p and γ is the language model weighting factor.</p><p>The length normalisation in the final step of the algorithm is helpful when decoding with a language model, as otherwise sequences with fewer transitions are unfairly favoured; it has little impact otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The experiments were carried out on the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B and LDC94S13B). The RNN was trained on both the 14 hour subset 'train-si84' and the full 81 hour set, with the 'test-dev93' development set used for validation. For both training sets, the RNN was trained with CTC, as described in Section 3, using the characters in the transcripts as the target sequences. The RNN was then retrained to minimise the expected word error rate using the method from Section 4, with five alignment samples per sequence.</p><p>There were a total of 43 characters (including upper case letters, punctuation and a space character to delimit the words). The input data were presented as spectrograms derived from the raw audio files using the 'specgram' function of the 'matplotlib' python toolkit, with width 254 Fourier windows and an overlap of 127 frames, giving 128 inputs per frame.</p><p>The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights. It was trained using stochastic gradient descent with one weight update per utterance, a learning rate of 10 −4 and a momentum of 0.9.</p><p>The RNN was compared to a baseline deep neural network-HMM hybrid (DNN-HMM). The DNN-HMM was created using alignments from an SGMM-HMM system trained using Kaldi recipe 's5', model 'tri4b' <ref type="bibr" target="#b20">(Povey et al., 2011)</ref>.</p><p>The 14 hour subset was first used to train a Deep Belief Network (DBN) <ref type="bibr" target="#b12">(Hinton &amp; Salakhutdinov, 2006)</ref> with six hidden layers of 2000 units each. The input was 15 frames of Mel-scale log filterbanks (1 centre frame ±7 frames of context) with 40 coefficients, deltas and accelerations. The DBN was trained layerwise then used to initialise a DNN.</p><p>The DNN was trained to classify the central input frame into one of 3385 triphone states. The DNN was trained with stochastic gradient descent, starting with a learning rate of 0.1, and momentum of 0.9. The learning rate was divided by two at the end of each epoch which failed to reduce the frame error rate on the development set. After six failed attempts, the learning rate was frozen. The DNN posteriors were divided by the square root of the state priors during decoding.</p><p>The RNN was first decoded with no dictionary or language model, using the space character to segment the character outputs into words, and thereby calculate the WER.</p><p>The network was then decoded with a 146K word dictionary, followed by monogram, bigram and trigram language models. The dictionary was built by extending the default WSJ dictionary with 125K words using some augmentation rules implemented into the Kaldi recipe 's5'. The languge models were built on this extended dictionary, using data from the WSJ CD (see scripts 'wsj extend dict.sh' and 'wsj train lms.sh' in recipe 's5'). The language model weight was optimised separately for all experiments. For the RNN experiments with no linguistic information, and those with only a dictionary, the beam search algorithm in Section 5 was used for decoding. For the RNN experiments with a language model, an alternative method was used, partly due to implementation difficulties and partly to ensure a fair comparison with the baseline system: an N-best list of at most 300 candidate transcriptions was extracted from the baseline DNN-HMM and rescored by the RNN using Eq. ( <ref type="formula">16</ref>). The RNN scores were then combined with the language model to rerank the N-best lists and the WER of the best resulting transcripts was recorded. The best results were obtained with an RNN score weight of 7.7 and a language model weight of 16.</p><p>For the 81 hour training set, the oracle error rates for the monogram, bigram and trigram candidates were 8.9%, 2% and 1.4% resepectively, while the anti-oracle (rank 300) error rates varied from 45.5% for monograms and 33% for trigrams. Using larger N-best lists (up to N=1000) did not yield significant performance improvements, from which we concluded that the list was large enough to approximate the true decoding performance of the RNN.</p><p>An additional experiment was performed to measure the effect of combining the RNN and DNN. fering with the explicit model. Nonetheless the difference was small, considering that so much more prior information (audio pre-processing, pronunciation dictionary, statetying, forced alignment) was encoded into the baseline system. Unsurprisingly, the gap between 'RNN-CTC' and 'RNN-WER' also shrank as the LM became more dominant.</p><p>The baseline system improved only incrementally from the 14 hour to the 81 hour training set, while the RNN error rate dropped dramatically. A possible explanation is that 14 hours of transcribed speech is insufficient for the RNN to learn how to 'spell' enough of the words it needs for accurate transcription-whereas it is enough to learn to identify phonemes.</p><p>The combined model performed considerably better than either the RNN or the baseline individually. The improvement of more than 1% absolute over the baseline is considerably larger than the slight gains usually seen with model averaging; this is presumably due to the greater difference between the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>To provide character-level transcriptions, the network must not only learn how to recognise speech sounds, but how to transform them into letters. In other words it must learn how to spell. This is challenging, especially in an orthographically irregular language like English.  , where the underscores are end-of-word markers. The network was trained with WER loss, which tends to give very sharp output decisions, and hence sparse error signals (if an output probability is 1, nothing else can be sampled, so the gradient is 0 even if the output is wrong). In this case the only gradient comes from the extraneous apostrophe before the 'S'. Note that the characters in common sequences such as 'IS', 'RI' and 'END' are emitted very close together, suggesting that the network learns them as single sounds.</p><p>latter problem may be harder than usual to fix with a language model, as words that are close in sound can be quite distant in spelling. Unlike phonetic systems, the network also makes lexical errors-e.g. 'bootik' for 'boutique'and errors that combine the two, such as 'alstrait' for 'illustrate'.</p><p>It is able to correctly transcribe fairly complex words such as 'campaign', 'analyst' and 'equity' that appear frequently in financial texts (possibly learning them as special cases), but struggles with both the sound and spelling of unfamiliar words, especially proper names such as 'Milan' and 'Dukakis'. This suggests that out-of-vocabulary words may still be a problem for character-level recognition, even in the absence of a dictionary. However, the fact that the network can spell at all shows that it is able to infer significant linguistic information from the training transcripts, paving the way for a truly end-to-end speech recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper has demonstrated that character-level speech transcription can be performed by a recurrent neural network with minimal preprocessing and no explicit phonetic representation. We have also introduced a novel objective function that allows the network to be directly optimised for word error rate, and shown how to integrate the network outputs with a language model during decoding. Finally, by combining the new model with a baseline, we have achieved state-of-the-art accuracy on the Wall Street Journal corpus for speaker independent recognition.</p><p>In the future, it would be interesting to apply the system to datasets where the language model plays a lesser role, such as spontaneous speech, or where the training set is sufficiently large that the network can learn a language model from the transcripts alone. Another promising direction would be to integrate the language model into the CTC or expected transcription loss objective functions during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&amp;CP volume 32. Copyright 2014 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Long Short-term Memory Cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Bidirectional Recurrent Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Deep Recurrent Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a, −, b, c, −, −) and (−, −, a, −, b, c) both correspond to the transcription (a, b, c). When the same label appears on successive time-steps in an alignment, the repeats are removed: therefore (a, b, b, b, c, c) and (a, −, b, −, c, c) also correspond to (a, b, c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) ∂ log Pr(a|x) ∂ Pr(k, t|x) L(x, B(a)) = a:at=k Pr(a|x, a t = k)L(x, B(a))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Network outputs.The figure shows the frame-level character probabilities emitted by the CTC layer (different colour for each character, dotted grey line for 'blanks'), along with the corresponding training errors, while processing an utterance. The target transcription was 'HIS FRIENDS ', where the underscores are end-of-word markers. The network was trained with WER loss, which tends to give very sharp output decisions, and hence sparse error signals (if an output probability is 1, nothing else can be sampled, so the gradient is 0 even if the output is wrong). In this case the only gradient comes from the extraneous apostrophe before the 'S'. Note that the characters in common sequences such as 'IS', 'RI' and 'END' are emitted very close together, suggesting that the network learns them as single sounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pr − (∅, 0) ← 1 for t = 1 . . . T do B ← the W most probable sequences in B B ← {} for y ∈ B do if y = ∅ then Pr + (y, t) ← Pr + (y, t − 1) Pr(y e , t|x) if ŷ ∈ B then Pr + (y, t) ← Pr + (y, t) + Pr(y e , ŷ, t) Pr − (y, t) ← Pr(y, t − 1) Pr(−, t|x)</figDesc><table><row><cell>Add y to B</cell><cell></cell></row><row><cell cols="2">for k = 1 . . . K do</cell></row><row><cell cols="2">Pr − (y + k, t) ← 0</cell></row><row><cell cols="2">Pr + (y + k, t) ← Pr(k, y, t)</cell></row><row><cell cols="2">Add (y + k) to B</cell></row><row><cell></cell><cell>1</cell></row><row><cell>Return: maxy∈B Pr</cell><cell>|y| (y, T )</cell></row></table><note>Pr(k|y) can be used to integrate prior linguistic information into the search. If no Algorithm 1 CTC Beam Search Initalise: B ← {∅};</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Wall Street Journal Results. All scores are word error rate/character error rate (where known) on the evaluation set. 'LM' is the Language model used for decoding. '14 Hr' and '81 Hr' refer to the amount of data used for training.</figDesc><table><row><cell>SYSTEM</cell><cell>LM</cell><cell>14 HR</cell><cell>81 HR</cell></row><row><cell>RNN-CTC</cell><cell>NONE</cell><cell cols="2">74.2/30.9 30.1/9.2</cell></row><row><cell>RNN-CTC</cell><cell cols="3">DICTIONARY 69.2/30.0 24.0/8.0</cell></row><row><cell>RNN-CTC</cell><cell>MONOGRAM</cell><cell>25.8</cell><cell>15.8</cell></row><row><cell>RNN-CTC</cell><cell>BIGRAM</cell><cell>15.5</cell><cell>10.4</cell></row><row><cell>RNN-CTC</cell><cell>TRIGRAM</cell><cell>13.5</cell><cell>8.7</cell></row><row><cell>RNN-WER</cell><cell>NONE</cell><cell cols="2">74.5/31.3 27.3/8.4</cell></row><row><cell>RNN-WER</cell><cell cols="3">DICTIONARY 69.7/31.0 21.9/7.3</cell></row><row><cell>RNN-WER</cell><cell>MONOGRAM</cell><cell>26.0</cell><cell>15.2</cell></row><row><cell>RNN-WER</cell><cell>BIGRAM</cell><cell>15.3</cell><cell>9.8</cell></row><row><cell>RNN-WER</cell><cell>TRIGRAM</cell><cell>13.5</cell><cell>8.2</cell></row><row><cell>BASELINE</cell><cell>NONE</cell><cell>-</cell><cell>-</cell></row><row><cell>BASELINE</cell><cell cols="2">DICTIONARY 56.1</cell><cell>51.1</cell></row><row><cell>BASELINE</cell><cell>MONOGRAM</cell><cell>23.4</cell><cell>19.9</cell></row><row><cell>BASELINE</cell><cell>BIGRAM</cell><cell>11.6</cell><cell>9.4</cell></row><row><cell>BASELINE</cell><cell>TRIGRAM</cell><cell>9.4</cell><cell>7.8</cell></row><row><cell cols="2">COMBINATION TRIGRAM</cell><cell>-</cell><cell>6.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to thank Daniel Povey for his assistance with Kaldi. This work was partially supported by the Canadian Institute for Advanced Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum mutual information estimation of hidden markov model parameters for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.1986.1169179</idno>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="1986-04">Apr 1986</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open vocabulary speech recognition with flat hybrid models</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="725" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Connectionist Speech Recognition: A Hybrid Approach</title>
		<author>
			<persName><forename type="first">Herve</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>ISBN 0792393961</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><surname>Ueli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08">August 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From speech to letters -using a novel neural network architecture for grapheme based asr</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>17.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Automatic Speech Recognition and Understanding Workshop</title>
				<meeting>Automatic Speech Recognition and Understanding Workshop<address><addrLine>Merano, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition of out-of-vocabulary words with sub-lexical language models</title>
		<author>
			<persName><forename type="first">Lucian</forename><surname>Galescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Precise Timing with LSTM Recurrent Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005-07">June/July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<meeting><address><addrLine>Pittsburgh, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICASSP 2013</title>
				<meeting>ICASSP 2013<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Abdel</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a better representation of speech soundwaves using restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5884" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Application of pretrained deep neural networks to large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A frequency warping approach to speaker normalization. Speech and Audio Processing</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1998-01">Jan 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning of motor skills with policy gradients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, number 4</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="682" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society</title>
				<imprint>
			<date type="published" when="2011-12">December 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
