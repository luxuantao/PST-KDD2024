<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Complexity of Event Ordering for Shared-Memory Parallel Program Executions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1990-01-19">January 19, 1990</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Robert</forename><forename type="middle">H B</forename><surname>Netzer</surname></persName>
							<email>netzer@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison</orgName>
								<address>
									<addrLine>1210 W. Dayton Street Madison</addrLine>
									<postCode>53706</postCode>
									<region>Wisconsin</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barton</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison</orgName>
								<address>
									<addrLine>1210 W. Dayton Street Madison</addrLine>
									<postCode>53706</postCode>
									<region>Wisconsin</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Complexity of Event Ordering for Shared-Memory Parallel Program Executions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="1990-01-19">January 19, 1990</date>
						</imprint>
					</monogr>
					<idno type="MD5">680A543E8DB731F76A638DE3F7F68212</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Event ordering</term>
					<term>nondeterminacy</term>
					<term>parallel processing</term>
					<term>race conditions</term>
					<term>synchronization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents results on the complexity of computing event orderings for sharedmemory parallel program executions. Given a program execution, we formally define the problem of computing orderings that the execution must have exhibited or could have exhibited, and prove that computing such orderings is an intractable problem.</p><p>We present a formal model of a shared-memory parallel program execution on a sequentially consistent processor, and discuss event orderings in terms of this model. Programs are considered that use fork/join and either counting semaphores or event style synchronization. We define a feasible program execution to be an execution of the program that performs the same events as an observed execution, but which may exhibit different orderings among those events. Any program execution exhibiting the same data dependences among the shared data as the observed execution is feasible. We define several relations that capture the orderings present in all (or some) of these feasible program executions. The happened-before, concurrent-with, and ordered-with relations are defined to show events that execute in a certain order, that execute concurrently, or that execute in either order but not concurrently. Each of these ordering relations is defined in two ways. In the must-have sense they show the orderings that are guaranteed to be present in all feasible program executions, and in the could-have sense they show the orderings that could potentially occur in at least one feasible program execution due to timing variations. We prove that computing any of the must-have ordering relations is a co-NP-hard problem and that computing any of the could-have ordering relations is an NP-hard problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In an execution of a shared-memory parallel program, the order in which some events execute may not be enforced by (explicit or implicit) synchronization, but instead may occur by chance. Even if two events are explicitly synchronized to force a certain order during one execution, the same events may occur in a different order during another execution. Due to nondeterministic timing variations, the program may, on different occasions, execute exactly the same events but exhibit different orderings among those events. We formally define several relations that capture the orderings present in all (or some) such alternate executions, and prove that computing these relations is an intractable problem. We consider programs executing on sequentially consistent processors that use fork/join and either counting semaphores or event variables.</p><p>We present a formal model of a shared-memory parallel program execution, and define event orderings in terms of this model. Given a program execution, P, we characterize other program executions that execute exactly the same events as P but which may exhibit different event orderings. Any program execution exhibiting the same data dependences among the shared data as P will execute the same events as P. The set of all such alternate program executions, called feasible program executions, is defined by considering all the different orderings that could allow the data dependences exhibited by P to occur. The various orderings that must have been exhibited (or could have been exhibited) by all such feasible program executions are captured by defining several ordering relations: happened-before, concurrent-with, and ordered-with. Each of these relations is defined in the must-have sense and in the could-have sense. The must-have relations show orderings that are guaranteed to occur in all feasible program executions, while the could-have relations show orderings that could potentially occur due to timing variations. We prove that computing the must-have relations is a co-NP-hard problem, and that computing the could-have relations is an NP-hard problem. These results are shown to hold for programs that use fork/join and either counting semaphores or event style synchronization (using the Post, Wait, and Clear primitives). We also show that these results hold when computing the orderings occurring in all program executions exhibiting the same events as a given execution, regardless of whether the original shared-data dependences occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Program Execution Model</head><p>In this section, we briefly present a formal model of shared-memory parallel program executions. The model contains the objects that represent a program execution (such as which statements were executed and in what order), and axioms that characterize properties those objects can possess. In subsequent sections, we use the model to characterize behavior that an execution might have exhibited (such as alternate event orderings) and behavior that an execution must have exhibited (such as obeying the semantics of its synchronization operations).</p><p>Our model † provides a formalism for reasoning about shared-memory parallel program executions that does † This section is a brief presentation of the model that was first presented by us in an earlier paper <ref type="bibr" target="#b9">[10]</ref>, and is based on Lamport's theory of concurrent systems <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TR 908</head><p>January <ref type="bibr">19,</ref><ref type="bibr">1990</ref> not assume the existence of atomic operations. We consider the class of shared-memory parallel programs that execute on sequentially consistent processors <ref type="bibr" target="#b6">[7]</ref> and that use fork/join and either counting semaphores or event style synchronization. A program execution is described by a collection of events and two relations over those events. Each event represents an execution instance of a set of (consecutively executed) program statements. We distinguish between two types of events: a synchronization event is an instance of some synchronization operation, and a computation event is an instance of a group of statements belonging to the same process, none of which are We define a program execution, P, to be a triple, 〈E, T , D 〉, where E is a finite set of events, and T and D are the relations over E described above. The temporal ordering and shared-data dependence relations must satisfy several axioms that describe properties a valid program execution must possess <ref type="bibr" target="#b9">[10]</ref>. We omit these axioms here as they are not required to prove our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>In a given program execution, the temporal ordering between some events is not always ''guaranteed''.</p><p>Another execution of the program could perform exactly the same events, but due to nondeterministic timing variations, could exhibit a different temporal ordering among those events. In this section, we characterize program executions exhibiting such alternate temporal orderings, and define several relations that capture the orderings present in all (or some) of these program executions. In subsequent sections, we prove that computing these relations is an intractable problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feasible Program Executions</head><p>Given a program execution, P, a feasible program execution for P (or just a feasible program execution, when P is implied) describes an execution of the program that performs exactly the same events as P, but which may exhibit different temporal orderings. Any execution that exhibits the same shared-data dependences as P will † Throughout this paper we use superscripted arrows to denote relations, and write a / b as a shorthand for ¬(a b), and a / b as a shorthand for ¬(a b) ∧ ¬(b a). ‡ This definition of data dependence is different from the standard ones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> in that our definition combines the notions of flow-, anti-, and output-dependence, and does not explicitly state the variable involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TR 908</head><p>January <ref type="bibr">19,</ref><ref type="bibr">1990</ref> execute exactly the same events as P † This result can be proven by showing that the execution result of each statement instance depends only upon the values of the variables it reads, and that the program's input and the shareddata dependences uniquely characterize these values for each step in the computation <ref type="bibr" target="#b8">[9]</ref>. Therefore, a program</p><formula xml:id="formula_0">execution P′ = 〈E ′, T′ , D′ 〉 is a feasible program execution for P = 〈E, T , D 〉 if (F1) E′ = E, and</formula><p>(F2) P′ satisfies the axioms of the model <ref type="bibr" target="#b9">[10]</ref>, and</p><formula xml:id="formula_1">(F3) a D b ⇒ a D′ b.</formula><p>These conditions state that any valid program execution (i.e., a program execution obeying the axioms mentioned in Section 2) possessing the same events and shared-data dependences as P describes an execution that is guaranteed to have potentially occurred. This statement holds even if the program executes nondeterministic statements, since P′ is still capable of executing the same events as P. We denote the set of feasible program executions for P, as characterized above, by F (P) (or just F, when P is implied).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ordering Relations</head><p>Given a program execution, P = 〈E, T , D 〉, and the set, F, of feasible program executions for P, we define several relations (shown in Table <ref type="table">1</ref>) that summarize the temporal orderings present in the feasible program executions in F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Must-Have Could-Have</head><p>Happened-Before</p><formula xml:id="formula_2">a MHB b ⇔ ∀〈E, T , D 〉 ∈ F, a T b a CHB b ⇔ ∃〈E, T , D 〉 ∈ F, a T b Concurrent-With a MCW b ⇔ ∀〈E, T , D 〉 ∈ F, a / T b a CCW b ⇔ ∃〈E, T , D 〉 ∈ F, a / T b Ordered-With a MOW b ⇔ ∀〈E, T , D 〉 ∈ F, ¬(a / T b) a COW b ⇔ ∃〈E, T , D 〉 ∈ F, ¬(a / T b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Ordering Relations For A Set, F, Of Feasible Program Executions</head><p>Each relation type (happened-before, concurrent-with, or ordered-with) ‡ is defined to capture both the must-have and could-have orderings. The must-have relations describe orderings that are guaranteed to be present in all feasible program executions in F, while the could-have relations describe orderings that could potentially occur in † For this statement to hold, interactions with the external environment must be modeled as shared-data dependences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TR 908</head><p>January <ref type="bibr">19,</ref><ref type="bibr">1990</ref> at least one of the feasible program executions in F. The happened-before relations show events that execute in a specific order, the concurrent-with relations show events that execute concurrently, and the ordered-with relations show events that execute in either order but not concurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>The problem of computing event orderings has been previously addressed by several researchers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Given a program execution, Emrath, Ghosh, and Padua <ref type="bibr" target="#b1">[2]</ref>, and Helmbold, McDowell, and Wang <ref type="bibr" target="#b4">[5]</ref> attempt to compute orderings that are guaranteed to occur in any other execution exhibiting the same events, irrespective of the original shared-data dependences. Callahan and Subhlok <ref type="bibr" target="#b0">[1]</ref> consider event orderings in the context of static analysis of parallel FORTRAN programs. Emrath, Ghosh, and Padua <ref type="bibr" target="#b1">[2]</ref> attempt to compute a graph that shows the must-have and could-have orderings for programs that use event style synchronization. However, since they do not consider the ordering constraints imposed by the shared-data dependences, their method sometimes overlooks some orderings. Helmbold, McDowell, and Wang <ref type="bibr" target="#b4">[5]</ref> consider programs that use counting semaphores, and present algorithms for computing only some of the must-have orderings. Callahan and Subhlok <ref type="bibr" target="#b0">[1]</ref> prove that computing orderings that are guaranteed to occur in all executions of a given program is a co-NP-hard problem, and present a data-flow framework for computing some of these orderings.</p><p>Emrath, Ghosh, and Padua <ref type="bibr" target="#b1">[2]</ref> describe a method for computing the ''guaranteed run-time ordering'' between events in program executions that use fork/join and event style synchronization (using Post, Wait, and ing between two events iff there is a path between the nodes representing those events. However, since their method does not account for the orderings imposed by the shared-data dependences, the graph sometimes shows no ordering when indeed an ordering is enforced by a shared-data dependence.</p><p>Consider the program fragment shown in Figure <ref type="figure" target="#fig_3">1a</ref>. Its task graph, for the case when the first created task completely executes before the other two created tasks, is shown in Figure <ref type="figure" target="#fig_3">1b</ref>. In this execution, there is a shared-data dependence from the statement instance ''X := 1'' to ''if X=1 then''. The dotted lines represent orderings imposed by the fork operation, and the solid line represents a guaranteed ordering (drawn from the closest common ancestor of the two Post nodes to the Wait node). In this graph, there is no path between the two Post nodes. However, the two Posts cannot execute in either order. Due to the shared-data dependence from the statement instance ''X := 1'' to ''if X=1 then'', it is not possible for the right-most Post to execute before the left-most Post. If this shared-data dependence does not occur, the else clause will execute, causing a Wait to be issued instead of the right-most Post. This example illustrates that even if the programmer does not intentionally introduce synchronization with shared variables, some events are nevertheless ordered by the shared-data dependences.</p><p>Any method that attempts to compute could-have orderings must therefore consider these dependences. We prove in the next section that exhaustively computing the orderings for program executions that use event style synchronization (with Clear operations) is an intractable problem.</p><p>Given a trace of a program that uses counting semaphores, Helmbold, McDowell, and Wang <ref type="bibr" target="#b4">[5]</ref> present an algorithm to compute some of the must-have orderings by computing safe orderings. A ordering is safe if the orderings it contains are guaranteed to occur in all executions that exhibit the same events (regardless of the shareddata dependences). Their algorithm operates in three phases. First, for each semaphore, they order the i th V event before the i th P event in the trace. The transitive closure of the union of this ordering with the intra-process orderings defines their happened before relation. This relation is unsafe because another execution (performing the same events) might exhibit a different pairing among the semaphore operations. In the second phase, their happened before relation is altered so that each V event is ordered before all P events on the same semaphore. The resulting relation is safe, but overly conservative. To sharpen the relation, the third phase adds additional safe orderings by considering that only some P events can actually execute after certain V events. Their algorithms run in TR 908 January <ref type="bibr">19,</ref><ref type="bibr">1990</ref> polynomial time since they compute only some of the must-have-happened-before orderings. The resulting ordering relation is therefore a subset of our MHB relation. In the next section we prove that computing the entire MHB relation is a co-NP-hard problem.</p><p>Callahan and Subhlok <ref type="bibr" target="#b0">[1]</ref> consider the problem of static analysis of FORTRAN programs without loops that use parallel DO and CASE and event style synchronization (without Clear operations). They prove that determining the orderings that are guaranteed to occur in all executions of such a program is a co-NP-hard problem. They also present a data-flow framework for computing some of these guaranteed orderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Complexity of Computing Ordering Relations</head><p>In this section, we describe the complexity of computing the ordering relations defined in Section 3. We prove that the problem of computing any of the must-have ordering relations (i.e., MHB  ) is NP-hard. These results are shown to hold for programs that use counting semaphores, and for programs that use event style synchronization (using the Post, Wait, and Clear primitives). Finally, we briefly comment on intractability results for computing the ordering relations when the shared-data dependences are not considered. Proof. We present a proof only for the must-have-happened-before relation ( MHB ); proofs for the other relations are analogous. We give a reduction † from 3CNFSAT <ref type="bibr" target="#b3">[4]</ref> such that any Boolean formula is not satisfiable iff a MHB b for two events, a and b, defined in the reduction. Let an arbitrary instance of 3CNFSAT be given by a set of n variables, V = {X 1 ,X 2 , . . . ,X n }, and a Boolean formula B consisting of m clauses,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Counting Semaphores</head><formula xml:id="formula_3">C 1 ∧ C 2 ∧ . . . ∧ C m ,</formula><p>where each clause is a disjunction of three literals (a literal is any variable or its negation, from V). From such an instance of 3CNFSAT, we construct a program consisting of 3n+3m+2 processes that uses 3n +m +1 semaphores (all semaphores are assumed to be initialized to zero). The execution of this program simulates a nondeterministic evaluation of the Boolean formula B. Semaphores are used to represent the truth values of each variable and clause. As we will show, the execution exhibits certain orderings iff B is not satisfiable.</p><p>For each variable, X i , construct the following three processes:</p><p>TR 908</p><p>January 19, 1990</p><formula xml:id="formula_4">P(A i ) P(A i ) V(A i ) V(X i ) V(X i ) P(Pass 2) . . V(A i ) . . . . V(X i ) V(X i )</formula><p>where '' . . . '' indicates as many V(X i ) (or V(X i )) operations as occurrences of the literal X i (or X i ) in the formula B. The semaphores X i and X i are used to represent the truth value of variable X i ; a signaling of semaphore X i (or X i ) represents the assignment of True (or False) to variable X i . The above processes operate in two passes. The first pass is a nondeterministic guessing phase in which each variable used in the Boolean formula is assigned a unique truth value. This assignment is accomplished by allowing either the V(X i ) operations or the V(X i ) operations to proceed, but not both. The second pass, which begins after semaphore Pass 2 is signaled, is used only to ensure that the program does not deadlock; the semaphore operations that were not allowed to execute during the first pass are allowed to proceed.</p><p>For each clause, C j , construct the following three processes:</p><formula xml:id="formula_5">P(L 1 ) P(L 2 ) P(L 3 ) V(C j ) V(C j ) V(C j )</formula><p>where L 1 , L 2 , and L 3 are the semaphores corresponding to the literals in clause C j . The semaphore C j represents the truth value of clause C j . This semaphore will be signaled if the truth assignments guessed during the first pass cause clause C j to evaluate to True.</p><p>Finally, create the following two processes:</p><formula xml:id="formula_6">a : skip P(C 1 ) V(Pass 2) . . . . . . P(C m ) V(Pass 2) b : skip</formula><p>where there are n V(Pass 2) operations (one for each variable). Event b is reached only after semaphore C j , for each clause j, has been signaled.</p><p>Since the program contains no conditional statements or shared variables, every execution of the program executes the same events and exhibits the same shared-data dependences (i.e., none). For any execution, we claim that a MHB b iff B is not satisfiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TR 908</head><p>January <ref type="bibr">19,</ref><ref type="bibr">1990</ref> To show the ''if'' part, assume that B is not satisfiable. Then there is always some clause, C j , that is not satisfied by the truth values guessed during the first pass. Therefore, no V(C j ) operation is issued during the first pass.</p><p>Event b cannot execute until this V operation is issued, which can then only be done during the second pass. The second pass does not occur until after event a executes, so event a must precede event b.</p><p>To show the ''only if'' part, assume that a MHB b. That is, there is no execution in which b either precedes a or executes concurrently with a. For a contradiction, assume that B is satisfiable. Then some truth assignment can be guessed during the first pass that satisfies all of the clauses. Event b can then execute before event a, contradicting the assumption. Therefore, B cannot be satisfiable. The above proofs do not make use of the general counting ability of counting semaphores, and therefore also hold for programs that use binary semaphores. In addition, the above results can be shown to hold for a program execution that uses a single counting semaphore by a reduction from the problem of sequencing to minimize maximum cumulative cost <ref type="bibr" target="#b3">[4]</ref>. Proof. The proof is similar to the proof of Theorem 1 (for programs that use semaphores), and hinges on the ability to implement two-process mutual exclusion using only synchronization operations (i.e., no shared variables).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Event Style Synchronization</head><p>We give a reduction from 3CNFSAT that produces a program operating in the same manner as described in Theorem 1. For each variable, X i , construct the following process:</p><p>TR 908</p><p>January 19, 1990</p><formula xml:id="formula_7">Post(A i ) Post(B i ) fork Clear(A i ) Wait(B i ) Post(X i ) Clear(B i ) Wait(A i ) Post(X i ) join</formula><p>Although these processes can deadlock, when they do not exactly one of Post(X i ) or Post(X i ) will be issued.</p><p>For each clause, C j , construct the following three processes:</p><formula xml:id="formula_8">Wait(L 1 ) Wait(L 2 ) Wait(L 3 ) Post(C j ) Post(C j ) Post(C j )</formula><p>where L 1 , L 2 , and L 3 are the event variables corresponding to the literals in clause j.</p><p>Finally, create the following two processes:</p><formula xml:id="formula_9">a : skip Wait(C 1 ) Post(A 1 ) . . . Post(B 1 ) Wait(C m ) . . . b : skip Post(A n ) Post(B n )</formula><p>These processes operate in a manner analogous to those created using semaphores, described in Theorem 1. We claim that a MHB b iff the Boolean formula is not satisfiable. The proof is analogous to the argument given in Theorem 1. Proof. The proof is analogous to the proof of Theorem 2 (for programs that use semaphores). The required reductions are similar to the reduction given in Theorem 3 above.</p><p>The above proofs rely on the implementation of two-process mutual exclusion with only synchronization primitives (i.e., no shared variables). Theorems 3 and 4 made use of the Clear primitive to implement two-process</p><note type="other">TR 908 January 19, 1990</note><p>mutual exclusion. We are currently investigating the complexity of computing the ordering relations for program executions that do not use the Clear primitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ordering Relations Ignoring Shared-Data Dependences</head><p>The related work outlined in Section 4 addresses the event ordering problem by using a different notion of feasibility than we presented here. These methods attempt to compute some (or all) of the orderings exhibited by all program executions exhibiting the same events as a given program execution, regardless of whether the original shared-data dependences occur. The complexity results given in this section extend directly to this case.</p><p>Since the programs constructed by the various reductions contain no shared-data dependences, the proofs suffice to show that even when the original shared-data dependences are ignored and all alternate program executions are considered, computing the ordering relations is still an intractable problem. As we showed, these results hold for programs that use counting semaphores or event style synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has presented results showing that, given a program execution, computing event orderings that the execution might have exhibited (or must have exhibited) is an intractable problem. Given a program execution, P, we defined a feasible program execution to be an execution of the program that performs the same events as P but which may exhibit different temporal orderings among those events. Any program execution that possess the same shared-data dependences as P is a feasible program execution. To capture the orderings exhibited by all (or some) of these program executions, we defined several ordering relations. The happened-before, concurrentwith, and ordered-with relations were defined to show events that execute in a certain order, that execute concurrently, or that execute in either order but not concurrently. Each of these ordering relations was defined in two ways. In the must-have sense they show the orderings that are guaranteed to be present in all feasible program executions, and in the could-have sense they show the orderings that could potentially occur at least one feasible program execution due to timing variations.</p><p>We proved that computing any of the must-have ordering relations is a co-NP-hard problem and that computing any of the could-have ordering relations is an NP-hard problem. These results were shown to hold for programs that use counting semaphores and for programs that use event style synchronization (using the Post, Wait, and Clear primitives). In addition, these results also hold for programs that use only a single counting semaphore or multiple binary semaphores. It is currently an open problem whether these results hold for program executions that use only a single binary semaphore or event style synchronization without Clear operations. We also showed that these results hold when computing the orderings occurring in all program executions exhibiting the same events as a given execution, regardless of whether the original shared-data dependences occur. An implication of these results is that exhaustively detecting all data races potentially exhibited by a given program execution <ref type="bibr" target="#b9">[10]</ref> is an intractable problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>synchronization operations. The temporal ordering relation † , T , describes the temporal ordering among events in the program execution; a T b means that a completes before b begins (in the sense that the last action of a can affect the first action of b), and a / T b means that a and b execute concurrently (i.e., neither completes before the other begins). The shared-data dependence relation, D , indicates when one event causally affects another; a D b means that a accesses a shared variable that b later accesses, where at least one of the accesses is a modification to the variable. ‡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Clear operations).They construct a graph (called a task graph) that contains a single node for each synchronization event in the program execution. Task Start and Task End edges are added to represent orderings imposed by fork and join operations, and Machine edges are added to represent orderings of events belonging to the same process. Synchronization edges are also added to represent guaranteed orderings imposed by synchronization. For each Wait node, all Post nodes that might have triggered that Wait are identified. A Post might trigger a Wait if there is no path from the Wait to the Post (which would indicate that the Wait must have preceded the Post), and no path from the Post to the Wait that includes a Clear node. Synchronization edges are then added from the closest common ancestors of these Posts to the Wait. The resulting graph is intended to show a guaranteed order-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example Program Fragment and a Task Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 .</head><label>1</label><figDesc>Given a program execution, P = 〈E, T , D 〉, that uses counting semaphores, the problem of deciding whether a MHB b, a MCW b, or a MOW b (i.e., any of the must-have ordering relations) is co-NPhard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Since a MHB b iff B is not satisfiable, the problem of deciding a MHB b is co-NP-hard. By similar reductions, programs can be constructed such that the non-satisfiability of B can be determined from the MCW or MOW relations. The problem of deciding these relations is therefore also co-NP-hard. Theorem 2. Given a program execution, P = 〈E, T , D 〉, that uses counting semaphores, the problem of deciding whether a CHB b, a CCW b, or a COW b (i.e., any of the could-have ordering relations) is NP-hard. Proof. The reduction used in the proof of Theorem 1 can be used to prove that deciding CHB is an NP-hard problem. Using the events defined in the above reduction, we can show that b CHB a iff B is satisfiable, showing that deciding CHB is NP-hard. As above, similar reductions can be constructed to prove that deciding the CCW and COW relations is also NP-hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3 .</head><label>3</label><figDesc>Given a program execution, P = 〈E, T , D 〉, that uses event style synchronization (with Post, Wait, and Clear primitives), the problem of deciding whether a MHB b, a MCW b, or a MOW b (i.e., any of the must-have ordering relations) is co-NP-hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 4 .</head><label>4</label><figDesc>Given a program execution, P = 〈E, T , D 〉, that uses event style synchronization, the problem of deciding whether a CHB b, a CCW b, or a COW b (i.e., any of the could-have ordering relations) is NPhard.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>† This reduction was motivated by the ones Taylor<ref type="bibr" target="#b10">[11]</ref> constructed to prove that certain static analysis problems are NP-complete.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research supported in part by National Science Foundation grant CCR-8815928, Office of Naval Research grant N00014-89-J-1222, and a Digital Equipment Corporation External Research Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Static Analysis of Low-Level Synchronization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspal</forename><surname>Subhlok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging</title>
		<meeting>of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-05">May 1988. January 1989</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>Also appears in SIGPLAN Notices</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Event Synchronization Analysis for Debugging Parallel Programs</title>
		<author>
			<persName><forename type="first">Perry</forename><forename type="middle">A</forename><surname>Emrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Sanjoy Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Padua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-11">November 1989</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="580" to="588" />
			<pubPlace>Reno, NV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Program Dependence Graph and its Use in Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">D</forename><surname>Ottenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="349" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>W. H. Freeman and Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing Traces with Anonymous Synchronization</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Zhong</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1990 Intl. Conf. on Parallel Processing</title>
		<meeting>of the 1990 Intl. Conf. on Parallel essing<address><addrLine>St. Charles, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependence Graphs and Compiler Optimizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leasure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Record of the Eighth ACM Symp. on Principles of Programming Languages</title>
		<meeting><address><addrLine>Williamsburg, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981-01">January 1981</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Computers C</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="690" to="691" />
			<date type="published" when="1979-09">September 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Mutual Exclusion Problem: Part I -A Theory of Interprocess Communication</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1986-04">April 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Debugging and Analysis of Large-Scale Parallel Programs</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-09">September 1989</date>
		</imprint>
		<respStmt>
			<orgName>also available as Computer Science Dept ; Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 312</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting Data Races in Parallel Program Executions</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">H B</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Also appears in Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gelernter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gross</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nicolau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</editor>
		<meeting><address><addrLine>Irvine, CA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1990">1991. Aug. 1990</date>
		</imprint>
	</monogr>
	<note>Languages and Compilers for Parallel Computing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Complexity of Analyzing the Synchronization Structure of Concurrent Programs</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">N</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="1983">19. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
