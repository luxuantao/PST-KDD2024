<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Learning-based Multi-model Ensemble Method for Cancer Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yawen</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="laboratory">and Key Laboratory of System Control and Information Processing of Ministry of Education</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongli</forename><surname>Lin</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaodong</forename><surname>Zhao</surname></persName>
							<email>xiaodong122@yahoo.com</email>
							<affiliation key="aff4">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Comm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Methods and Programs in Biomedicine</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<postBox>P.O. Box 400743</postBox>
									<postCode>22904-4743</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Learning-based Multi-model Ensemble Method for Cancer Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A4F88A24378B78FC82F2F2BA52C8B9EA</idno>
					<idno type="DOI">10.1016/j.cmpb.2017.09.005</idno>
					<note type="submission">Received date: 26 April 2017 Revised date: 7 August 2017 Accepted date: 6 September 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-model ensemble</term>
					<term>deep learning</term>
					<term>gene expression</term>
					<term>feature selection</term>
					<term>cancer prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background and Objective: Cancer is a complex worldwide health problem associated with high mortality. With the rapid development of the high-throughput sequencing technology and the application of various machine learning methods that have emerged in recent years, progress in cancer prediction has been increasingly made based on gene expression, providing insight into effective and accurate treatment decision making. Thus, developing machine learning methods, which can successfully distinguish cancer patients from healthy persons, is of great current interest. However, among the classification methods applied to cancer prediction so far, no one method outperforms all the others. Methods: In this paper, we demonstrate a new strategy, which applies deep learning to an ensemble approach that incorporates multiple different machine learning models. We supply informative gene data selected by differential gene expression analysis to five different classification models. Then, a deep learning method is employed to ensemble the outputs of the five classifiers. Results: The proposed deep learning-based multi-model ensemble method was tested on three public RNAseq data sets of three kinds of cancers, Lung Adenocarcinoma, Stomach Adenocarcinoma and Breast Invasive Carcinoma. The test results indicate that it increases the prediction accuracy of cancer for all the tested RNA-seq data sets as compared to using a single classifier or the majority voting algorithm. Conclusions: By taking full advantage of different classifiers, the proposed deep learning-based multi-model ensemble method is shown to be accurate and effective for cancer prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T Highlights</head><p>• An ensemble of multiple machine learning models outperforms single classifiers.</p><p>• We propose a deep learning-based ensemble method for cancer prediction.</p><p>• We select differentially expressed genes from gene expression data.</p><p>• We present prediction results on lung, stomach and breast cancer data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T A Deep Learning-based Multi-model Ensemble Method for Cancer Prediction 1 Introduction</head><p>Cancer has been characterized as a collection of related diseases involving abnormal cell growth with the potential to divide without stopping and spread into surrounding tissues <ref type="bibr">[1]</ref>. According to the GLOBOCAN project <ref type="bibr" target="#b0">[2]</ref>, in 2012 alone, about 14.1 million new cases of cancer occurred globally (not including skin cancer other than melanoma), which caused about 14.6% of the death. Since cancer is a major cause of morbidity and mortality, diagnosis and detection of cancer in its early stage is of great importance for its cure. Over the past decades, a continuous evolution of cancer research has been performed <ref type="bibr" target="#b1">[3]</ref>. Among the diverse methods and techniques developed for cancer prediction, the utilization of gene expression level is one of the research hotspots in this field.</p><p>Data analysis on gene expression level has facilitated cancer diagnosis and treatment to a great extent. Accurate prediction of cancer is one of the most critical and urgent tasks for physicians <ref type="bibr" target="#b2">[4]</ref>.</p><p>With the rapid development of computer-aided techniques in recent years, application of machine learning methods is playing an increasingly important role in the cancer diagnosis, and various prediction algorithms are being explored continuously by researchers. Sayed et al. <ref type="bibr" target="#b3">[5]</ref> conducted a comparative study on feature selection and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T classification using data collected from the central database of the National Cancer Registry Program of Egypt, and three classifiers were applied, including support vector machines (SVMs), k -nearest neighbour (k NN) and Naive Bayes (NBs). The results showed that SVMs with polynomial kernel functions yielded higher classification accuracy compared with k NN and NBs. Statnikov et al. <ref type="bibr" target="#b4">[6]</ref> carried a comprehensive comparison of random forests (RFs) and SVMs for cancer diagnosis. The results were obtained that SVMs outperformed RFs in fifteen data sets, RFs outperformed SVMs in four data sets, and the two algorithms performed the same in three data sets.</p><p>These results were obtained by using full set of genes. Similar results were derived based on the gene selection method. From a large body of literature in cancer prediction research, none of these machine learning methods is fully accurate and each method may be lacking in different facets in the classification procedure. For instance, it is difficult for SVMs to figure out an appropriate kernel function, and although RFs have solved the over-fitting of decision trees (DTs), RFs may lead the classification result to the category with more samples.</p><p>In view of the fact that each machine learning method may outperform others or have defects in different cases, it is thus natural to expect that a method that takes advantages of multiple machine learning methods would lead to superior performance. To this end, several studies have been reported in the literature that aim to integrate models to increase the accuracy of the prediction. For example, Breiman <ref type="bibr" target="#b5">[7]</ref> introduced Bagging, which combines outputs from decision trees generated by several randomly selected subsets of the training data and votes for the final outcome. Freund and Schapire <ref type="bibr" target="#b6">[8]</ref> introduced Boosting, which updates the weights of training samples after each iteration of training and combines the classification outputs by weighted votes. Wolpert <ref type="bibr" target="#b7">[9]</ref> proposed to use linear regression to combine outputs of the neural networks, which was later known as Stacking.</p><p>Aik et al. <ref type="bibr" target="#b8">[10]</ref> applied Bagging and Boosting on cancerous microarray data for cancer classification. Cho and Won <ref type="bibr" target="#b9">[11]</ref> applied the majority voting algorithm to combine four classifiers using three benchmark cancer data sets. The</p><p>Stacking and majority voting take advantages of different machine learning methods. Although the majority voting algorithm is the most common in classification tasks, it is still too simple a combination strategy to discover complex information from different classifiers. Stacking, through the use of a learning method in the combination stage, is a much more powerful ensemble technique. Given that the small number of deep learning studies in biomedicine have shown success with this method <ref type="bibr" target="#b10">[12]</ref>, deep learning has become a strong learning method with many advantages.</p><p>Unlike the majority voting which only considers the linear relationships among classifiers and requires for manual participation, deep learning has the ability to "learn" the intricate structures, especially nonlinear structures, from the original large data sets automatically. Thus, in order to better describe the unknown relationships among different classifiers, we adopt deep learning in the Stacking-based ensemble learning of multiple classifiers.</p><p>In this paper, we attempt to use deep neural networks to ensemble five classification models, which are k NN, SVMs, DTs, RFs and gradient boosting decision trees (GBDTs), to construct a multi-model ensemble model to predict cancer in normal and tumor conditions. To avoid over-fitting, we employ the differential gene expression analysis to select important and informative genes. The selected genes are then supplied to the five classification models. After that, a deep neural network is used to ensemble the outputs of the five classification models to obtain the final prediction result. We evaluate the proposed method on three public RNA-seq data sets from lung tissues, stomach tissues and breast tissues, respectively. The final results indicate that the proposed deep learning-based multi-model ensemble method makes more effective use of the information of the limited clinical data and generates more accurate prediction than single classifiers or the majority voting algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 2 Methods</head><p>The flowchart of the proposed deep learning-based ensemble strategy is shown in Fig. <ref type="figure">1</ref>. Initially, differential expression analysis is used to select the significantly differentially expressed genes, namely the most informative features, which are then fed to the following classification process. Then, we employ the technique of S -fold cross validation to divide the initial data into S groups of training and testing data sets. After that, multiple classifiers (first-stage models) are learned from the training sets, each of which consists of S -1 of the S groups, and then applied to the corresponding test set, which is the remaining group of the S groups, to output the predicted class of the samples. Finally, we use a deep neural network classifier (second-stage ensemble model) to combine the predictions in the first stage with the aim of reducing the generalization error and procuring a more accurate outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First stage classification models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second stage ensemble model</head><p>Figure <ref type="figure">1</ref>: Flowchart of the proposed deep learning-based multi-model ensemble method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature selection</head><p>The use of gene expression data with an increasing number of features (e.g., genes) and information makes it more challenging to develop classification models. In clinical practice, the number of cancer samples available is rather small in comparison with the number of features, resulting in higher risk of over-fitting and degradation of the classification performance. Feature selection is a good way to address these challenges <ref type="bibr" target="#b11">[13]</ref>. By reducing the entire feature space to a subset of features, over-fitting of the classification model can be avoided, thus mitigating the challenges arising from a small sample size and a high data dimensionality.</p><p>In this paper, we employ the DESeq <ref type="bibr" target="#b12">[14]</ref> method to select informative genes for the downstream classification.</p><p>The DESeq method is usually used to decide whether, for a given gene, an observed difference in read count is significant, that is, whether it is greater than what would be expected just due to natural random variation <ref type="bibr" target="#b12">[14]</ref>.</p><p>In differential expression analysis, by setting the thresholds of the BH-adjusted p-value and the fold change level, the significantly differentially expressed genes are screened and selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross validation</head><p>For many classification models, the complexity may be governed by multiple parameters. In order to achieve the best prediction performance on new data, we wish to find appropriate values of the complexity parameters that lead to the optimal model for a particular application.</p><p>If data are plentiful, then a simple way for model selection is to divide the entire data into three subsets, the training set, the validation set and the test set. A range of models are trained on the training set, compared and selected on the validation set, and finally evaluated on the test set. Among the diverse complex models that have been trained, the one having the best predictive performance is selected, which is an effective model validated by the data in the validation set. In a practical application, however, the supply of data for training and testing is limited, leading to an increase of the generalization error. An approach to reducing the generalization error and preventing over-fitting is to use cross validation <ref type="bibr" target="#b13">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The technique of S -fold cross validation <ref type="bibr" target="#b13">[15]</ref> used in this paper is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> for the case of S = 4.</p><p>S -fold cross validation partitions the available data set D into S disjoint groups, D 1 , D 2 , . . . , D S , with all subsets maintaining consistency in the data distribution. After that, S -1 groups are used as the training set and the remaining group is used as the test set. The procedure is then repeated for all S possible choices of the S -1 groups, and the performance scores resulting from the S runs are then averaged. In our study, we not only utilize S -fold cross validation to implement model selection for every single classifier separately, but also generate new data sets for the ensemble stage by using S -fold cross validation on the initial data sets in order to avoid over-fitting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification methods</head><p>After preprocessing of the data sets, we assess the prediction performance of five popular classification methods towards the discrimination between normal and tumor samples. Specifically, we apply k -nearest-neighbor (k NN), support vector machines (SVMs), decision trees (DTs), random forests (RFs), and gradient boosting decision trees (GBDTs) as first-stage classification models. All of these five classification methods are of high accuracy in practical applications and are reviewed as follows <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr">17]</ref>.</p><p>k NN is a non-parametric classification method that is used when there is little or no prior knowledge about the distribution of the data. k NN classifiers transform samples to a metric space, where distances between samples are determined. The distance function between a test sample and the training samples is the basis, by calculating which, k NN classifies a test sample based upon the most common class in its k -nearest training samples.</p><p>SVMs initially map the input vector into a feature space of higher dimensionality and identify a hyperplane that separates the data points into two categories. The gap between the two categories is as wide as possible. New samples are then mapped into the same space and predicted to belong to a category based on which side of the gap they fall on with higher confidence.</p><p>DTs have tree-like structures in which the nodes represent the input variables and the leaves correspond to decision outcomes. When traversing the tree for the classification of a new sample, we are able to predict the category of the data with adequate reasoning due to the specific architecture.</p><p>RFs have only recently been applied in the field of cancer prediction. RFs are an ensemble learning method that combines tree predictors, each of which depends on the values of a random vector sampled independently and with the same distribution. The final outcome is the most popular class that receives the majority of votes from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the trees in the forest, hence yielding an overall better model.</p><p>GBDTs are a machine learning technique that combines an ensemble of decision trees into a stronger prediction model. GBDTs build the model in a stage-wise fashion like other boosting methods do, and implement a generalization by allowing the optimization of an arbitrary differentiable loss function.</p><p>Three classical methods (i.e., k NN, SVMs and DTs) and two advanced algorithms (i.e., RFs and GBDTs) are introduced. As suggested in the literature, k NN is one of the simplest classification methods especially for distribution-unknown data. But k NN is sensitive to redundant features and requires effective feature selection prior to classification, and the choice of the number k can greatly affect the performance of classifier. SVMs can be considered as the most effective and common algorithm for cancer classification. Nevertheless, it is a challenge for SVMs to figure out an appropriate kernel for specific issues. In particular, there is no general solution for nonlinear cases, thus the prediction accuracy can not be guaranteed. DTs, as the the most fundamental and widely used classification method in various fields, however, tend to be a weak classifier to distinguish normal and cancer samples since it often over-fits the model. The latter two methods, RFs and GBDTs are evolutionary approaches which are ensembles of DTs, overcoming the over-fitting problem to an extent, but may lead to the classification result tending to the category with more samples. Considering that each method has its own shortcomings relative to others, we come up with an ensemble strategy to make use of the advantages of the multiple methods and avoid the shortcomings. Here, we select both the fundamental and evolutionary methods in order to increase the diversity of our ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-model ensemble based on deep learning</head><p>In practice, several classification models are available for cancer prediction, but none of these is fully accurate and each method may be making mistakes in different facets. Stacking of multiple different classification methods may lead to performance improvement over individual models. Multi-model ensemble is a technique in which the predictions of a collection of models are given as inputs to a second-stage learning model. The second-stage model is trained to combine the predictions from first stage models optimally to form a final set of predictions.</p><p>In this paper, we adopt deep learning as the ensemble model to stack the multiple classifiers. Neural networks are inspired by how the brain works and is widely used in many applications. A neural network is trained to generate an output as a combination among the input variables. Given a set of features and a target, it can learn to be a nonlinear function approximator, where, between the input and output layers, there can be one or more nonlinear layers, called hidden layers. Deep learning involves deep neural networks with many hierarchical hidden layers of nonlinear information processing which endow the capabilities to learn complex patterns from high dimensional raw data with little guidance <ref type="bibr" target="#b10">[12]</ref>. Shown in Fig. <ref type="figure" target="#fig_3">3</ref> is a neural network. The leftmost layer is the input layer with neurons being called input neurons. The rightmost layer is called the output layer with an output neuron. The middle layers are hidden layers that are made up of hidden neurons. In order to classify samples correctly, we define an objective function, which computes the error between the predicted scores and the actual scores. Then, by training with the training samples, the machine modifies the values of its internal adjustable parameters that define the input-output function to reduce the error. In practice, the stochastic gradient descent (SGD) algorithm is most commonly used in this machine learning procedure. In a deep neural network, we denote the number of layers as n l and layer l as L l , so layer L 1 is the input layer and layer L n l is the output layer. We also let s l denote the number of neurons in layer l. The neural network has parameters W = {W 1 , W 2 , . . . , W n l } and b = {b 1 , b 2 , . . . , b n l }, where W l ij , j = 1, 2, . . . , s l-1 , i = 1, 2, . . . , s l , l = 2, 3, . . . , n l , denotes the weight associated the connection between unit j in  layer l -1 and unit i in layer l, and b l i , i = 1, 2, . . . , s l , l = 2, 3, . . . , n l , denotes the bias of unit i in layer l. Suppose that we have a training set {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x m , y m )} of m samples, with which we train the neural network using the SGD. We define the cost function (the objective function mentioned above) as,</p><formula xml:id="formula_0">J(W, b) = 1 m m i=1 J(W, b; x i , y i ) + λ 2 n l l=2 s l-1 j=1 s l i=1 (W l ij ) 2 = 1 m m i=1 1 2 h W,b (x i ) -y i 2 + λ 2 n l l=2 s l-1 j=1 s l i=1 (W l ij ) 2 ,<label>(1)</label></formula><p>where the first term is a mean square error term and the second term is a regulation term used to constrain the magnitudes of the weights and prevent over-fitting, and λ is the weight decay parameter that regulates the relative importance of the two terms. The nonlinear hypothesis h W,b (x) of the neural network is defined as,</p><formula xml:id="formula_1">h W,b (x) = f (W T x + b),<label>(2)</label></formula><p>where f : R → R is called the activation function. In recent years, the most popular nonlinear function used here is the rectified linear unit (ReLU) f (z) = max{0, z} which typically learns much faster in multi-layer deep neural networks than the more conventional hyperbolic tangent and logistic sigmoid function <ref type="bibr" target="#b15">[18]</ref>. For one sample, we define the activation (output value) of unit i in layer l as a l i and the weighted sum as z l i , so that</p><formula xml:id="formula_2">a l i = f (z l i ) = f W l-1 i1 a l-1 1 + W l-1 i2 a l-1 2 + . . . + W l-1 is l-1 a l-1 s l-1 + b l-1 i ,<label>(3)</label></formula><p>and with x i as the unit i in the input layer L 1 , i.e.,</p><formula xml:id="formula_3">a 1 i = x i .<label>(4)</label></formula><p>Thus the activation of the unit in the output layer is</p><formula xml:id="formula_4">h W,b (x) = a n l i = f W n l -1 i1 a n l -1 1 + W n l -1 i2 a n l -1 2 + . . . + W n l -1 isn l -1 a n l -1 sn l -1 + b n l -1 i .<label>(5)</label></formula><p>This step to compute the activation of each unit is called the forward propagation. In SGD, our goal is to minimize J(W, b) by adjusting parameters W and b. We first initialize each W l ij and b l i to a small random value near zero</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>and then update the parameters in each iteration of SGD as,</p><formula xml:id="formula_5">W l ij = W l ij -α ∂ ∂W l ij J(W, b),<label>(6)</label></formula><formula xml:id="formula_6">b l i = b l i -α ∂ ∂b l i J(W, b),<label>(7)</label></formula><p>where α is the learning rate. Then we employ the back propagation algorithm to compute the partial derivatives.</p><p>In detail, given a training sample (x, y), the back propagation algorithm can be described as follows.</p><p>1. Conduct the forward propagation calculations to compute the activation of each unit in layer L 2 up to the output layer L n l .</p><p>2. For each unit i in layer n l , calculate the residual</p><formula xml:id="formula_7">δ n l i = ∂ ∂z n l i 1 2 y -h W,b (x) 2 = -(y i -a n l i )f (z n l i ).<label>(8)</label></formula><p>3. For each unit i in layer l, l = n l -1, n l -2, ..., 2, calculate the residual</p><formula xml:id="formula_8">δ l i =   s l+1 j=1 W l ji δ l+1 j   f (z l i ).<label>(9)</label></formula><p>4. Calculate the desired partial derivatives</p><formula xml:id="formula_9">∂ ∂W l ij J(W, b; x, y) = a l j δ l+1 i ,<label>(10)</label></formula><formula xml:id="formula_10">∂ ∂b l i J(W, b; x, y) = δ l+1 i .<label>(11)</label></formula><p>The process of deriving from back forward is the intention of back propagation. By repeating the iterative steps of the SGD, we can decrease the cost function J(W, b) so as to train the neural network.</p><p>In our study, we have proposed a deep learning-based multi-model ensemble method with a 5-fold stacking. The overall algorithm is shown in Fig. <ref type="figure">4</ref>. In the first stage, we divide the given data set D into five subsets, D 1 , D 2 , . . . , D 5 , where D i = {x i , y i }, i = 1, 2, . . . , 5, contains labeled points drawn independent and identically distributed according to the same distribution. In the first round, the union of D 2 , D 3 , D 4 and D 5 is used as the training set, and D 1 = {x 1 , y 1 } is used as the test set. Given the input x 1 , five classification models in this stage propose corresponding hypotheses h 1 (x 1 ), h 2 (x 1 ), . . . , h 5 (x 1 ), where h i (x i ) is a binary variable, and the subscript i of h i (x 1 ) is referred to as the ith model. After the classifications in the first round, we assemble the predictions of each model into H 1 = [h 1 (x 1 ), h 2 (x 1 ), . . . , h 5 (x 1 )], which is merged with the corresponding label y 1 to form a new data set D 1 , for use in the second stage. This procedure is then repeated for five times, according to the S -fold cross validation technique discussed in the section of Materials and methods. After all this, we obtain five new data sets, D 1 , D 2 , . . . , D 5 , where D i = {H i , y i }, i = 1, 2, . . . , 5. In the second stage, we apply a deep neural network as the ensemble model. To classify normal and tumor samples, we use a five-layer neural network. The input layer of the network contains five neurons, which represent features of samples in the new data set. In the hidden layers, we experiment with different numbers of nodes in each layer for better classification performance. The output layer of the network contains one neuron whose output was 0 or 1, denoting normal or tumor, respectively. In this stage, we also employ 5-fold cross validation and take the mean value to obtain the outcome. 3 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collection</head><p>We evaluated the proposed method on three RNA-seq data sets of three kinds of cancers, including Lung Adenocarcinoma (LUAD), Stomach Adenocarcinoma (STAD) and Breast Invasive Carcinoma (BRCA). The gene expression data were obtained from the TCGA project web page <ref type="bibr" target="#b16">[19]</ref>. These data sets, which include all stages of cancers, were collected from subjects of various clinical conditions and different ages, genders and races. As described in the profile <ref type="bibr" target="#b17">[20]</ref>, the tumor tissues from patients not treated with prior chemotherapy or radiotherapy were selected. The specific information of the data sets is shown in Table <ref type="table" target="#tab_0">1</ref>. In our procedure, we used both the raw count data and the normalized fragments per kilobase per million (FPKM) data. The raw count data were used to selected the significantly differentially expressed genes and the normalized FPKM data were used to the following classification and ensemble procedure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gene selection of normal and tumor samples</head><p>We analyzed gene differential expression between normal samples and cancer samples in three data sets. In order to filter the significantly differentially expressed features, the technique of DESeq was employed. Through the analysis and comparison, the genes that satisfied the following conditions were considered most differentially expressed <ref type="bibr" target="#b18">[21]</ref>: (1) the BH-adjusted p-value less than 0.01; (2) the fold change threshold of 4; (3) the mean FPKM of each gene in all the samples larger than 2. For the LUAD data, 1385 differentially expressed genes were selected.</p><p>For the STAD data, 801 genes were selected. For the BRCA data, 934 genes were selected. The selected genes satisfying the estimate threshold settings were significantly differentially expressed in different cancer data sets where the difference was greater than that in randomly selected genes.</p><p>We utilized the three mostly used evaluation metrics to compare the prediction performance with the entire set of genes and the differentially expressed genes identified previously: precision, recall and accuracy. Precision is defined as the fraction of correctly identified cancer patients, recall measures the proportion of predicted cancer patients to all the people sampled, and accuracy is the weighted average of precision and recall, denoting the overall correctness. Both the mean values and the standard deviations were calculated for multiple test sets in cross validation. These evaluation metrics were also used in the following assessment of classification models.</p><p>We applied a DT classifier to train and test the entire data and selected data separately. The obtained results are shown in Table <ref type="table" target="#tab_1">2</ref>. We observe that better accuracy and a better tradeoff between recall and precision are represented by feature selection. Besides, the classification performance is more stable on the selected data. The computation time is also compared, revealing the importance of selecting features. Therefore, we would use the selected data as the input of the subsequent procedure, in consideration of a more accurate prediction of cancer as well as a greatly reduced running time of classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-model ensemble based on neural networks</head><p>We first applied five classification methods in the first stage individually, which were k -nearest neighbor, support vector machines, decision trees, random forests and gradient boosting decision trees, and then averaged the predictions derived from these methods after using 5-fold cross validation technique. Then, we went a step further to employ the multi-model ensemble method to integrate all the first-stage predictions using a deep neural network. With 5-fold cross validation, the dimensionality of the new data set has decreased and the sample size has increased significantly, which provides the possibility for the application of deep neural networks and also provides more information for the prediction. We compare the predictive accuracy of each individual method, the majority voting and our proposed ensemble method on three data sets, which are the LUAD, STAD and BRCA data sets. The receiver operating characteristic (ROC) curve is also used to compare the performance of different methods. In statistic, the ROC curve is a graphical plot that illustrates the performance of a binary classifier as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (i.e., true positive rate) against the fraction of false positives out of the negatives (i.e., false positive rate) at various thresholds. Generally, the area under the curve (AUC) is estimated as an important measurement for model comparison, which reflects the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. Because of the imbalance of the samples we used, the precision-recall (PR) curve is required to deal with the highly skewed data. The area under the PR curve is typically used to measure the relationship between the precision and recall and the performance of a classifier. A large area represents both high precision and high recall.</p><p>The predictive accuracy results are shown in Table <ref type="table" target="#tab_2">3</ref>. In the table, we describe the recognition rate of our proposed ensemble method compared with five single classifiers and the majority voting algorithm for each cancer data set. From the table, it is clear that the integration strategy of multiple models significantly outperforms classification models using in isolation and results in more stable performance. In addition, our deep learningbased multi-model ensemble method obtains better predictions than the majority voting, raising the accuracy to 99.20%, 98.78% and 98.41%, for the LUAD, STAD and BRCA data sets, respectively.</p><p>The three ROC curves are shown in Fig. <ref type="figure" target="#fig_6">5</ref> for the three cancer data sets, respectively. According to the results in the figures, the integration strategy gets higher AUC scores by combining multiple different classifiers, thus obtains better classification performance than each classifier operating alone. Furthermore, the AUC scores of the proposed deep learning-based ensemble method are higher than that of the majority voting for all three data sets, owing to its ability to learn and discover hidden structure automatically.</p><p>The three corresponding PR curves are shown in Fig. <ref type="figure" target="#fig_7">6</ref>. In the figure, we observe that the proposed ensemble</p><formula xml:id="formula_11">A C C E P T E D M A N U S C R I P T</formula><p>method obtains an area that is larger than or the same as that of each single classifier and the majority voting.</p><p>We also observe that the proposed ensemble method deals well with skewed data, which reflects the imbalance of clinical samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Based on the results, we observe that the proposed deep learning-based multi-model ensemble method yields satisfactory results that are superior to single classifiers and the majority voting algorithm in cancer prediction.</p><p>Due to the complexity and high mortality of cancer, timely and accurate diagnosis is critical. Thus, improving the prediction accuracy by applying computer-aided techniques is of great help to cancer treatment.</p><p>In the study, we made a comparison between the multi-model ensemble method we have proposed in this paper and five different classification models acting solo. The five classifiers are classical and advanced ones that have been widely used in cancer prediction. According to the observations in a previous study <ref type="bibr" target="#b4">[6]</ref>, for SVMs and RFs, each classifier may outperform the other on different data sets. The same situation may happen for other classifiers, which indicates that each method has its own shortcomings relative to others. It is this observation that has motivated us to propose the strategy of integrating different classifiers in order to obtain a more accurate and unbiased classification model. Our results on three data sets show that the multi-model ensemble method leads to higher accuracy than all the five classifiers acting solo on all the data sets. In addition, the ROC curves indicate that a single classifier exhibits unstable prediction performance for different data sets. This is probably a consequence of different sensitivities of classifiers to different data distributions, sample sizes and redundant features. However, by going a step further to ensemble the outputs of the five classifiers, our proposed method continues to train the weight of each classifier. In this procedure, classifiers with higher accuracy have a greater role to play and interference information of the classifiers with lower accuracy is excluded. Therefore, the advantages of each classifier are fully considered and utilized, and better prediction performance is obtained.</p><p>Additional tests were performed to compare the prediction accuracy of the proposed deep learning-based ensemble method with the majority voting algorithm. As a commonly used ensemble approach in various fields, the majority voting algorithm is also employed in cancer prediction. Cho and Won <ref type="bibr" target="#b9">[11]</ref> observed a better classification performance of the majority voting than SVMs and k NN on cancer data sets, which is also confirmed by our results.</p><p>Furthermore, we observed that our deep learning-based ensemble method obtains a higher accuracy and AUC score than the majority voting algorithm. The results may be attributed to the fact that the majority voting algorithm does not regard the weights of different classifiers and only considers linear relationships. As compared to the majority voting algorithm, the deep learning used in the ensemble stage in our proposed method automatically learns hidden intricate structures, including nonlinear structures. Through the training of deep learning, the unknown relationships among classifiers and the label of samples are discovered and fitted to the best, so that both the outputs of different classifiers and the relationships among them are fully taken into account. Consequently, higher accuracy of cancer prediction is achieved. Our results also confirm that deep learning, with the ability to fit complex relationships, especially nonlinear relationships, and with very little engineering by hand, will be of great use in taking advantages of increases in the amount of information and data. We believe that the application of deep learning in the field of disease diagnosis will be very promising with a broad development space.</p><p>We have to point out that the deep learning-based multi-model ensemble method incurs a higher computational cost. To overcome this limitation to a certain extent, we applied the feature selection technique in the data preprocessing phase, which greatly reduces the running time and improves the prediction accuracy in the same time. With the rapid increase in the amount of gene expression data and the variety of features, feature selection   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The technique of S -fold cross validation (S = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of the neural network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>D 2 D 3 D 4 D 5 D 1 D 3 D 4 D 5 D 1 D 2 D 3 D 2 'Figure 4 :</head><label>2345134512324</label><figDesc>Figure 4: The deep learning-based ensemble method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The ROC curves for individual and ensemble methods on three data sets: (a) LUAD; (b) STAD; (c) BRCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The precision-recall curves for individual and ensemble methods on three data sets: (a) LUAD; (b) STAD; (c) BRCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data sets information</figDesc><table><row><cell cols="2">Data set Genes</cell><cell></cell><cell>Samples</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Tumor Normal Total</cell></row><row><cell>LUAD</cell><cell>20532</cell><cell>125</cell><cell>37</cell><cell>162</cell></row><row><cell>STAD</cell><cell>29699</cell><cell>238</cell><cell>33</cell><cell>271</cell></row><row><cell>BRCA</cell><cell>20532</cell><cell>775</cell><cell>103</cell><cell>878</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The precision, recall, accuracy and CPU time of the entire data and selected data analyzed by DTs</figDesc><table><row><cell>Data set</cell><cell>Data size</cell><cell>Precision(%)</cell><cell>Recall(%)</cell><cell>Accuracy(%)</cell><cell>CPU time(s)</cell></row><row><cell>LUAD</cell><cell>Entire set</cell><cell>97.10(±4.29)</cell><cell>97.37(±2.63)</cell><cell>95.60(±2.19)</cell><cell>0.0532</cell></row><row><cell></cell><cell cols="4">Selected set 98.46(±2.29) 97.37(±2.63) 96.80(±2.28)</cell><cell>0.0039</cell></row><row><cell>STAD</cell><cell>Entire set</cell><cell>96.69(±1.22)</cell><cell>97.22(±1.39)</cell><cell>94.63(±2.04)</cell><cell>0.8608</cell></row><row><cell></cell><cell cols="2">Selected set 99.42(±0.80)</cell><cell>96.67(±3.75)</cell><cell>96.59(±3.80)</cell><cell>0.0182</cell></row><row><cell>BRCA</cell><cell>Entire set</cell><cell>96.60(±1.26)</cell><cell>97.34(±0.98)</cell><cell>94.62(±1.64)</cell><cell>0.0591</cell></row><row><cell></cell><cell cols="4">Selected set 97.77(±0.96) 97.42(±0.61) 95.76(±0.94)</cell><cell>0.0040</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The predictive accuracy (%) of individual and ensemble methods on three data sets</figDesc><table><row><cell>Classification algorithm</cell><cell>LUAD</cell><cell>STAD</cell><cell>BRCA</cell></row><row><cell>k NN</cell><cell>88.00(±5.10)</cell><cell>93.90(±2.99)</cell><cell>95.08(±0.89)</cell></row><row><cell>SVM</cell><cell>97.20(±2.28)</cell><cell cols="2">81.22(±22.50) 79.55(±19.22)</cell></row><row><cell>DT</cell><cell>96.80(±2.28)</cell><cell>96.59(±3.80)</cell><cell>95.76(±0.94)</cell></row><row><cell>RF</cell><cell>93.20(±1.79)</cell><cell>96.83(±1.85)</cell><cell>94.17(±1.53)</cell></row><row><cell>GBDT</cell><cell>96.80(±2.28)</cell><cell>96.59(±2.64)</cell><cell>95.76(±4.46)</cell></row><row><cell>Majority voting</cell><cell>97.20(±1.79)</cell><cell>98.54(±1.34)</cell><cell>98.18(±0.73)</cell></row><row><cell>Proposed method</cell><cell cols="3">98.80(±1.79) 98.78(±1.44) 98.41(±0.41)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Natural Science Foundation of China under grant No. 31270210. The authors would like to thank the reviewers in advance for their comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>is very important and necessary. Overall, feature selection in the discovery of important genes and in the study of pathology deserves more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Cancer is a major health problem worldwide. Although the machine learning methods have been more and more widely used in cancer prediction, no one method outperforms all the others. In this paper, we presented a deep learning-based multi-model ensemble approach to the prediction of cancer. Specifically, we analyzed gene expression data obtained from three kinds of tissues, lung, stomach and breast. In order to avoid over-fitting in classification, we identified differentially expressed gene data between normal and tumor phenotypes with the DESeq technique.</p><p>The results show that differential expression analysis is necessary to reduce the dimensionality of data and to select effective information, thus increasing the accuracy of the prediction and reducing the computational time to large extent. The multi-model ensemble method then utilizes the predictions of multiple different models as inputs to a deep neural network, which is trained to combine the model predictions to form an optimal final prediction. The majority voting algorithm combines the predictions from different classifiers as a contrast. We analyzed the three kinds of cancer data on five classifiers separately as well as on the majority voting method and our proposed multimodel ensemble method. The results show that the proposed ensemble model outperforms every other classifier as well as the majority voting in various evaluation metrics. The deep learning-based multi-model ensemble method reduces the generation error and obtains more information by using the first-stage predictions as features than it is trained in isolation. Moreover, by using deep learning, the intricate relationships among the classifiers are learned automatically, thus enabling the ensemble method to achieve better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors do not have financial and personal relationships with other people or organizations that could inappropriately influence (bias) their work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimated Cancer Incidence, Mortality and Prevalence Worldwide in 2012</title>
		<author>
			<persName><surname>Globocan</surname></persName>
		</author>
		<ptr target="http://globocan.iarc.fr/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hallmarks of cancer: the next generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Weinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="646" to="674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine learning applications in cancer prognosis and prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kourou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Karamouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis ; A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Structural Biotechnology Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="8" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature selection for cancer classification: An SVM based approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Emam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="20" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive comparison of random forests and support vector machines for microarray-based cancer classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble machine learning on gene expression data for cancer classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Bioinformatics</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine learning in dna microarray analysis for cancer classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Bioinformatics Conference</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Applications of deep learning in biomedicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mamoshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Pharmaceutics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A classification framework applied to cancer gene expression profiles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hijazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="255" to="284" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differential expression analysis for sequence count data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>Science+Business Media</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random forest</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://cancergenome.nih.gov/" />
		<title level="m">The TCGA Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cancer Genome Atlas Research Network</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">513</biblScope>
			<biblScope unit="issue">7517</biblScope>
			<biblScope unit="page" from="202" to="209" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A system level analysis of gastric cancer across tumor stages with RNA-seq data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular BioSystems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1925" to="1932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
