<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C720B93BC8AB6D1EBC7D42A9CD197D05</idno>
					<idno type="DOI">10.1109/TNNLS.2018.2861991</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person Reidentification via Structural Deep Metric Learning</head><p>Xun Yang, Peicheng Zhou, and Meng Wang , Senior Member, IEEE Abstract-Despite the promising progress made in recent years, person reidentification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. This paper proposes to tackle this task by jointly learning feature representation and distance metric in an end-to-end manner. Existing deep metric learning-based re-ID methods usually encounter the following two weaknesses: 1) most works based on pairwise or triplet constraints often suffer from slow convergence and poor local optima, partially because they use very limited samples for each update and 2) hard negative sample mining has been widely applied in existing works. However, hard positive samples, which also contribute to the training of network, have not received enough attention. To alleviate these problems, we develop a novel structural metric learning objective for person re-ID, in which each positive pair is allowed to be compared against all negative pairs in a minibatch and each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution. The introduced positive pair weighting strategy enables the algorithm to focus more on the hard positive samples. Furthermore, we propose to enhance the proposed loss function by adding a global loss term to reduce the variances of positive/negative pair distances, which is able to improve the generalization capability of the network model. By this approach, person images can be nonlinearly mapped into a low-dimensional embedding space where similar samples are kept closer and dissimilar samples are pushed farther apart. We implement the proposed algorithm using the inception architecture and evaluate it on three large-scale re-ID data sets. Experiment results demonstrate that our approach is able to outperform most state of the arts while using much lower dimensional deep features. Index Terms-Computer vision, deep metric learning, deep neural network, machine learning, person re-identification (re-ID).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N RECENT years, person reidentification (re-ID) [1]- [4]   has attracted increasing attention in the computer vision community for its critical role in security surveillance applications. It aims to recognize the person of interest across multiple nonoverlapping camera views. Given a probe person image (query), the task is to rank all the person images in the gallery set by the similarity between the query and candidate images and return the most relevant images as retrieval results.</p><p>To tackle this problem, massive efforts <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b19">[20]</ref> have been made over the last decade. However, it remains a challenging problem since a person's appearance usually undergoes dramatic variations across camera views due to the changes in view angle, body pose, illumination, and background clutter.</p><p>Traditional methods mainly consist of two parts: feature extraction and metric learning. The first part focuses on designing robust hand-crafted features <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>. The second part <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> aims to learn a suitable distance/similarity function. Despite the promising progress, they optimize the two parts either separately or sequentially, which may result in a suboptimal performance. Once useful information has been lost in the feature extraction stage, it can hardly be recovered later.</p><p>More recently, deep convolutional neural networks (CNNs) have gained increasing popularity in person re-ID <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b40">[41]</ref>. Different from traditional works, deep CNN-based approaches are able to learn feature representation and distance metric jointly in an end-to-end manner, where the major task is to learn a nonlinear discriminative mapping from person images to low-dimensional embeddings, where similar examples are mapped close to each other, while dissimilar examples are pushed farther apart.</p><p>Existing deep CNN-based re-ID approaches can roughly be classified into two groups. The first group <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref> considers each sample independently using an identification loss, which directly casts person re-ID as a multiclass recognition task and usually learns a nonlinear mapping from an input person image to its person identity using a cross-entropy loss. Despite the simplicity, the first group of methods may be less effective on small data sets, and the identification loss usually suffers from the large intrapersonal variance. The second group <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b39">[40]</ref> prefers to learn discriminative feature embeddings by employing pairwise or triplet constraints. Pairwise constraint-based methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b35">[36]</ref> take paired person images (positive/negative pairs) as an input to minimize a verification/contrastive loss. They usually focus on both reducing interpersonal variations and enlarging interpersonal variations and result in an absolute distance. Triplet constraint-based methods <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b39">[40]</ref> take image triplets for each update and minimize a triplet loss that encourages the network to find an embedding space where the distance between positive pair (anchor, positive) is smaller than that between negative pair (anchor, negative) by a fixed margin. Triplet constraints result in a relative distance which usually matters more than absolute distance for most tasks <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b46">[47]</ref>, e.g., face verification, image clustering, and retrieval. The second group has received much more attention in recent years. This paper is more related to the second group.</p><p>Despite yielding promising progress, pairwise or triplet constraint-based works often suffer from slow convergence and poor local optima <ref type="bibr" target="#b45">[46]</ref>, partially because they only employ two samples (pairwise constraints) or three samples (triplet constraints) for each update. Moreover, most existing works focus more on mining hard negative samples for the optimization of network, while they pay less attention to hard positive samples that also contribute to the optimization.</p><p>To alleviate these problems, this paper proposes to learn feature representation and distance metric jointly in an end-toend manner for person re-ID with a hardness-aware structural metric learning objective. The proposed learning objective improves the triplet loss by allowing each positive pair to be compared against all the corresponding negative pairs within minibatch for each update. To better leverage hard positive samples, each positive pair is adaptively assigned a hardness-aware weight in the proposed learning objective, which enables the algorithm to focus more on hard positive samples rather than treat all positive pairs equally. By this way, the learning performance can be effectively improved. In addition, the second-order statistics of positive/negative pair distances is incorporated into our learning objective to improve the generalization capability of the network. Specifically, we introduce a global loss term that penalizes the large variances of positive/negative pair distances. By the proposed approach, we expect to learn more discriminative deep features that are robust to person appearance variation. Extensive experiments on several large-scale data sets have demonstrated the effectiveness of the proposed structural feature embedding learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Metric Learning</head><p>Distance metric learning <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> aims to learn a mapping from the input data space to a low-dimensional embedding space, where similar instances are kept closer, while dissimilar instances are pushed farther apart. In recent years, with the remarkable development of deep learning techniques <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b53">[54]</ref>, deep metric learning has shown promising results on multiple computer vision tasks, e.g., face recognition, image retrieval, and fine-grained image recognition. The major difference with standard metric learning is that deep metric learning optimizes feature and metric jointly in an endto-end manner. Existing works can be roughly classified into the following three groups.</p><p>The first group of deep metric learning methods trains Siamese networks with a contrastive loss <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>, in which paired data are fed into neural networks. They usually minimize intraclass distance and penalize interclass distance for being smaller than a data-independent threshold. The contrastive loss usually results in the absolute distance.</p><p>The second group of methods aims to learn deep embeddings using the triplet loss <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b57">[58]</ref>, which takes triplets as an input. Each triplet consists of three samples (anchor, positive, and negative), where the former two samples share the same class label and the third one is from a different class. Triplet loss encourages the network to find an embedding space where the anchor sample is closer to the positive sample than the negative sample. The triplet loss results in a relative distance that has been shown to be better than an absolute distance in most tasks. For the triplet-loss-based methods, it is crucial to mine hard samples, e.g., the semihard or hardest negative samples, to select triplets violating the triplet constraints for fast convergence.</p><p>The third group of methods focuses on improving the performance by exploiting more negative samples for each update <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> or exploiting the global structure of embedding space <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Song et al. <ref type="bibr" target="#b46">[47]</ref> proposed a lifted structured embedding loss by lifting the vector of pairwise distances within the minibatch to the dense matrix of pairwise distances. Sohn <ref type="bibr" target="#b45">[46]</ref> designed a N-pair loss that optimizes the log probability of identification loss directly, which is a special case of the lifted structured loss <ref type="bibr" target="#b44">[45]</ref>. Then, a structured prediction framework <ref type="bibr" target="#b46">[47]</ref> is applied to ensure that the score of the ground-truth clustering assignment is higher than the score of any other clustering assignment, which exploits the global structure of embedding space and results in an impressive performance.</p><p>Our proposed loss function can be seen an improvement of lifted structured loss <ref type="bibr" target="#b44">[45]</ref> and N-pair loss <ref type="bibr" target="#b45">[46]</ref>. However, our loss can directly process the 2 -normalized features. It is an important architecture design in our model. Usually, an 2 -normalization layer is not adopted in most state-of-the-art deep metric learning methods, since it bounds the Euclidean distance in a small range and makes the optimization difficult. Besides, the lifted structured loss and N-pair loss <ref type="bibr" target="#b45">[46]</ref> treat all positive pairs equally. We argue that hard positive pairs contribute more to the training of network than easy positive pairs. For this reason, we improve the lifted structured loss by adaptively assigning larger weights to hard positive pairs. Our loss function also includes a global loss term that minimizes the variances of positive/negative pair distances. It can improve the generalization capability of the network. By this way, the learning performance can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Person Reidentification</head><p>This paper focuses on tackling person re-ID with the proposed deep metric learning scheme. We roughly categorize most existing works into two types: traditional methods and deep model-based methods. In this section, we only briefly introduce some representative works.</p><p>Traditional methods usually cope with two subproblems, i.e., feature learning and metric learning, separately. Some works focus on designing hand-crafted features <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> that are expected to be robust to complex variations in human appearances from different camera views, e.g., local maximal occurrence feature <ref type="bibr" target="#b6">[7]</ref> and Gaussian of Gaussian <ref type="bibr" target="#b5">[6]</ref>. Some works focus on learning an optimal distance/similarity function <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref> using hand-crafted features to better characterize the similarity between a pair of person images. Zheng et al. <ref type="bibr" target="#b61">[62]</ref> formulated re-ID as a relative distance learning problem by maximizing the probability that relevant samples have a smaller distance than the irrelevant ones. Kostinger et al. <ref type="bibr" target="#b13">[14]</ref> developed a simple and effective metric learning method by computing the difference between the intraclass and the interclass covariance matrix. As an improvement, Liao et al. <ref type="bibr" target="#b6">[7]</ref> proposed a cross-view quadratic discriminant analysis method by learning a more discriminative distance metric and a low-dimensional subspace simultaneously. Generally, traditional re-ID models can be easily trained and have shown a promising performance on small data sets, e.g. VIPeR <ref type="bibr" target="#b62">[63]</ref>.</p><p>Recently, more researchers in re-ID <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> prefer to learn feature and metric jointly in an end-to-end manner by training deep neural networks. For the deep re-ID methods, an identification loss is the first choice, which has been exploited in multiple pioneering works <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>. For example, Xiao et al. <ref type="bibr" target="#b28">[29]</ref> proposed to jointly handle person detection and identification in an endto-end framework. Verification or contrastive loss functions are also widely used <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b35">[36]</ref>, which are usually employed to train Siamese-like networks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> using paired person images as an input. Ahmed et al. <ref type="bibr" target="#b34">[35]</ref> improved the Siamese model by computing the cross-input neighborhood difference, which can capture local relationships between two input images. Later, Varior et al. <ref type="bibr" target="#b32">[33]</ref> incorporated long short-term memory modules into the Siamese network, which can process person image parts sequentially so that the spatial connections can be memorized. More recently, triplet-based losses have attracted more attention in re-ID <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b39">[40]</ref> for its great success in face recognition (FaceNet <ref type="bibr" target="#b43">[44]</ref>). Compared with pairwise constraint-based losses, triplet-based loss functions take triplets as an input and result in a relative distance. Some works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref> also proposed to combine multiple loss functions to further enhance the performance. For example, Wang et al. <ref type="bibr" target="#b2">[3]</ref> presented a deep learning framework to jointly optimize single-image representation and cross-image representation for re-ID, which achieves a satisfying performance. Both pairwise and triplet constraints are exploited in <ref type="bibr" target="#b2">[3]</ref>.</p><p>We propose a deep person re-ID scheme with a hardnessaware structural metric learning objective. It allows each positive pair to be compared against all negative pairs within minibatch and can adaptively assign large weights to hard positive pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>This paper addresses the problem of person re-ID by training a deep convolutional network discriminatively. We aim to find a suitable distance function between two person images x i and x j , which is expected to be small if x i and x j are from the same class or large if they are from different classes. In this paper, it is defined as a squared Euclidean distance between deep embeddings of person images: 2  2 , where f θ (•) is a nonlinear feature mapping parameterized by the network parameters θ (weight matrices, bias vectors, and so on). Therefore, the key step of this problem Overview of the proposed person re-ID framework. The basic idea is to learn a nonlinear mapping from person images to discriminative embeddings based on a deep CNN. For each iteration, in the training stage, we feed the network with an identity-balanced minibatch generated by an online random sampling strategy and generate triplets online. The output of the last fully connected layer is 2 -normalized and passed into the loss layer. The network parameters are updated by backpropagation supervised by a hardness-aware structural metric learning objective. In the testing stage, given a probe image, we compute the Euclidean distances between the probe and gallery images using the learned deep embeddings. Our network architecture is simpler yet effective.</p><formula xml:id="formula_0">d 2 (x i , x j ) = f θ (x i ) - f θ (x j )</formula><p>is to learn the nonlinear feature mapping function f θ (•) that can transform a person image x i to a low-dimensional and discriminative embedding f θ (x i ) ∈ R d in the Euclidean space. We expect to directly output 2 -normalized deep embeddings in the training stage rather than only normalize the embeddings in the testing stage. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, an 2 -normalization layer is added after the fully connected layer. During testing, we use the 2 -normalized low-dimensional embeddings to compute the distance between a query person image (probe) and candidate person images (galleries) in the database. If a true match to the probe exists in the database, it should have a small distance with the probe and will be top-ranked. To learn f θ (•), the task is to design a discriminative loss function to supervise the training of neural network. For simplicity, in this section, we omit θ from f θ (•) and use</p><formula xml:id="formula_1">d 2 i j to replace d 2 (x i , x j ).</formula><p>In this section, we first review several widely used loss functions in Section III-A, followed by the proposed loss function in Section III-B. The optimization procedure is described in Section III-C, followed by the implementation details in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Review</head><p>In this section, we briefly review three commonly used loss functions for embedding learning: identification loss, contrastive loss, and triplet loss.</p><p>1) Identification Loss: The most popular loss function for deep embedding learning is the identification loss <ref type="bibr" target="#b66">[67]</ref>, which formulates the learning task as a multiclass classification problem. It usually corresponds an n-way softmax layer in a neural network, which yields a probability distribution over n classes and minimizes the softmax loss function, denoted by</p><formula xml:id="formula_2">L(x i , y i ) = -log e W T y i f (x i )+b y i n j =1 e W T j f (x i )+b j (1)</formula><p>where y i is the class label of sample x i . f (x i ) is the deep feature of sample x i and the input of the softmax layer, {W, b} is the softmax layer parameters. W j denotes the j th column of the weight matrix W and b is the bias term.</p><p>The number of class is n. For its simplicity, it has been applied for person re-ID in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and <ref type="bibr" target="#b67">[68]</ref>.</p><p>Zheng et al. <ref type="bibr" target="#b67">[68]</ref> presented a competitive baseline method for person re-ID, termed ID-discriminative embedding (IDE). However, the largest person re-ID data set at present has only less than 1000 identities in the training set. When the number of identities increases, this loss necessitates a growing number of network parameters, most of which will be discarded after training.</p><p>2) Contrastive Loss: The second option is to take paired images with a binary label (x i , x j , y i j ) as an input to formulate a contrastive loss <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref> based on pairwise constraints. It aims to minimize the positive pair distance and penalize the negative pair distance that is smaller than a margin α. It is usually defined as</p><formula xml:id="formula_3">L(x i , x j , y i j ) = (1 -y i j )[α -d i j ] 2 + + y i j d 2 i j (2)</formula><p>where y i j = 1 for a positive pair (x i , x j ) and y i j = 0 for a negative pair. The operator [•] + denotes the hinge loss. This loss results in an absolute distance that can answer the question "How similar/dissimilar are these two person images?" It has been investigated for person re-ID in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and <ref type="bibr" target="#b68">[69]</ref>.</p><p>3) Triplet Loss: Triplet loss <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b57">[58]</ref> has attracted much more attention in recent years for its impressive performance in face recognition <ref type="bibr" target="#b43">[44]</ref>. It can be seen as an improvement of contrastive loss, and it is measured on the triplets {(x i , x j , x k )}, where x i is the anchor of triplet that shares the same class label with the positive x j and has a different class label with the negative x k . It encourages the network to find an embedding space where the distance between x i and x k is larger than the distance between x i and x j by a positive margin α. The cost function that is being minimized is defined as</p><formula xml:id="formula_4">L(x i , x j , x k ) = d 2 i j + α -d 2 ik + . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Hard negative sample mining in large minibatches <ref type="bibr" target="#b43">[44]</ref> is usually employed to select sufficient nontrivial triplets. In fact, the performance of triplet loss-based methods highly depends on the hard sample mining strategy <ref type="bibr" target="#b46">[47]</ref>. This loss has been exploited in <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b39">[40]</ref> for person re-ID. Although the triplet loss has yielded a promising performance, it usually suffers from slow convergence and poor local optima <ref type="bibr" target="#b45">[46]</ref>, partially due to the reason that it only employs three samples (one positive pair (x i , x j ) and one negative pair (x i , x k )) for distance comparison in each update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Loss Function</head><p>In this paper, we aim to design a loss function that is able to enhance the contrastive loss and triplet loss. To address this issue, the lifted structured loss <ref type="bibr" target="#b44">[45]</ref> is developed to improve the standard triplet loss by comparing the positive pair with all negative pairs. However, it is formulated with unnormalized deep features, yielding an unbounded Euclidean distance. We found that the lifted structured loss is sensitive to input features with large norm x i 2 . 2 normalization can be a natural solution to avoid such situation. However, it is too stringent for the lifted structured loss <ref type="bibr" target="#b44">[45]</ref> as well as other state-of-theart triplet-based methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, since it bounds the Euclidean distance to vary in a small range [0, 2], thus making the optimization difficult and very slow.</p><p>In this paper, we introduce a structural loss that can effectively utilize the 2 -normalized features for deep embedding learning. Following the merit of the lifted structured loss <ref type="bibr" target="#b44">[45]</ref>, we expect that our loss can allow each positive pair to be compared with more than one negative pairs. We use the minibatches training strategy and generate triplets online. For each iteration, given a triplet set T = {x i , x j , {x - ik } N k=1 }, where (x i , x j ) is a positive pair and {x i , x - ik } N k=1 are all the corresponding negative pairs, indexed by k, along the direction of anchor x i , we formulate the structural loss as</p><formula xml:id="formula_6">L(T ) = 1 |P| (i, j )∈P log 1 + N k=1 e d 2 i j -d 2 ik +α ξ (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where α is a margin parameter. P is a set of positive pairs, indexed by (i, j ), consisting of |P| positive pairs in a minibatch.</p><formula xml:id="formula_8">d 2 ik = f (x i ) -f (x - ik ) 2 2</formula><p>is the squared Euclidean distance. All the input features are 2 -normalized. The parameter ξ &lt; 1 is a scale factor introduced to yield softer triplet loss, which can accelerate the training. Without this scale factor, the optimization is more prone to collapsing unexpectedly. Similar to lifted structured loss <ref type="bibr" target="#b44">[45]</ref>, (4) is a generalization of widely used triplet loss. Its main improvement on <ref type="bibr" target="#b44">[45]</ref> is that it can effectively utilize the 2 -normalized features for training. As it employs all negative pairs for each update, we term it as 2 Apair loss.</p><p>In large-scale person re-ID data sets, e.g., Market-1501 <ref type="bibr" target="#b71">[72]</ref>, each identity usually has dozens of images, manually/automatically selected from video sequences captured by multiple cameras. We found that some images of the same identity look very similar, thus resulting in many easy positive pairs shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. Generally, the easy positive pairs contribute less to the training of network, since they can be recognized as the same person very easily. Existing methods mainly focus on hard negative mining, but they pay less attention on hard positive mining. In this paper, we design a technique to pay more attention on the hard positive pairs for person re-ID. For a hard positive pair (x i , x j ) from the cth class (identity), we compare its squared Euclidean distance with a class-specific threshold to compute a hardness-aware weight β i j as</p><formula xml:id="formula_9">β i j = exp d 2 i j -τ c (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where τ c is a hard threshold of the cth class. The positive pair (x i , x j ) will be treated as a hard positive pair if d 2 i j &gt; τ c , which will result in larger contribution to the overall objective. The primary task is how to set the hard threshold. It is unwise to directly employ the average positive distance of class c as the threshold, since it will make nearly half of positive pairs be treated as hard and the network will be more prone to overfitting. In this paper, the hard threshold is computed as</p><formula xml:id="formula_11">τ c = 2 1 |P c | (i, j )∈P c d 2 i j -min (i, j )∈P c d 2 i j (6)</formula><p>where |P c | is the set of positive pairs of the cth class. Using the hard threshold in ( <ref type="formula">6</ref>), only a small proportion of positive pairs will be assigned a large weight (β i j &gt; 1), which can avoid excessive attention on hard pairs. Then, the hardnessaware 2 Apair loss in (4) can be reformulated as</p><formula xml:id="formula_12">L(T ) = 1 B (i, j )∈P β i j log 1 + N k=1 e d 2 i j -d 2 ik +α ξ<label>(7)</label></formula><p>where B = (i, j )∈P β i j is the sum of all positive pair weights.</p><p>The introduced positive pair weight β i j in (6) enables the hard positive pair to contribute more than the easy positive pair. Although some previous works have adopted an offline hard positive samples selection strategy, easy positive samples are filtered directly, thus resulting in a waste of training samples. In this paper, all positive pairs are used, while each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution.</p><p>Our proposed loss in <ref type="bibr" target="#b6">(7)</ref> supervises the network to optimize the embedding in a local manner. It has been shown in <ref type="bibr" target="#b46">[47]</ref> that the local manner may result in a failure that the gradient signal from the positive pair gets outweighed by the negative pairs, which leads to groups of examples with the same class label being separated into partitions in the embedding space that are far apart from each other. To alleviate this problem, we introduce a global loss term that explores the global structure of the embedding space to enhance the local loss in <ref type="bibr" target="#b6">(7)</ref>. Our global loss term takes into consideration the second-order statistics of positive/negative pair distances, defined by</p><formula xml:id="formula_13">L global = 1 2 σ 2 p -α p + + σ 2 n -α n + (<label>8</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">σ 2 p = 1 |P| (i, j )∈P d 2 i j -μ p 2 (<label>9</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">σ 2 n = 1 |N | (i,l)∈N d 2 il -μ n 2 (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where |N | denotes the number of all negative pairs in the negative pair set N , and σ 2 p and σ 2 n denote the variances of positive pair distances and negative pair distances, respectively. μ p and μ n denote the mean value of positive pair distance and negative pair distance, respectively, in the normalized Euclidean distance space. α p and α n are two margin parameters. The values of μ p and μ n are hard to be estimated. It is too stringent to set them as fixed values. In this paper, the values of μ p and μ n are updated for each iteration as</p><formula xml:id="formula_19">μ t p = γ μ t -1 p + (1 -γ )μ t p (<label>11</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">μ t u = γ μ t -1 u + (1 -γ )μ t u (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where μ t p (μ t u ) denotes the mean value of positive (negative) pair distances estimated using the tth minibatch. γ is a parameter restricted in [0, 1] that controls the updating of μ t p and μ t u . The introduced global loss term in ( <ref type="formula" target="#formula_13">8</ref>) is designed to penalize the large variance of positive/negative pair distances in the normalized Euclidean distance space. This global loss term is able to regularize the network and improve its generalization capability by combining the global loss term in <ref type="bibr" target="#b7">(8)</ref> and the local loss term in (4).</p><p>The final objective function is formulated by</p><formula xml:id="formula_23">L(T ) = 1 B (i, j )∈P β i j log 1 + N k=1 e d 2 i j -d 2 ik +α ξ + λ 2 σ 2 p -α p + + σ 2 n -α n + (13)</formula><p>where the parameter λ is used to balance the two terms. Note that the global loss term is not a hard constraint. If λ is too large, the signal of gradient from the second term will outweigh the signal of the first term, which makes the network prone to overfitting. The mathematical formulation of our final objective in ( <ref type="formula">13</ref>) is a generalization of triplet loss. It is designed with 2 normalized features and allows the positive pair to be compared against all the negative pairs. It adaptively assigns larger weights to hard positive pairs, which facilitates the training. It also includes a global loss term that controls the variance of positive/negative pair distances, thus improving the generalization capability of the network. By this way, the learning performance can be improved effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization</head><p>To solve the optimization problem in (13), we apply the stochastic gradient descent scheme to update the gradient of objective L with respect to the feature embedding f (•). We summarize the backpropagation procedure in Algorithm 1, where F i j = log(1 + N k=1 e (d 2 i j -d 2 ik +α)/ξ ) for simple expression, and</p><formula xml:id="formula_24">∂L ∂σ 2 p = λ 2 ½ σ 2 p &gt; α p (14) ∂L ∂σ 2 n = λ 2 ½ σ 2 n &gt; α n (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>Algorithm 1 Backpropagation Input: A mini-batch {x i } m i=1 with multiclass labels; margin α; parameter λ; pair weights {β i j }; dense pairwise squared distance matrix D = {d 2 i j };</p><formula xml:id="formula_26">Output: The gradients ∂L ∂ f (x i ) , ∀i ∈ [1, m]. Initialization: ∂L ∂ f (x i ) = 0, ∀i ∈ [1, m]; for i = 1, • • • , m do for j = 1, • • • , m, s.t. (i, j ) ∈ P do ∂L ∂ f (x i ) ← ∂L ∂ f (x i ) + λ 2 ∂L ∂σ 2 p ∂σ 2 p ∂d 2 i j ∂d 2 i j ∂ f (x i ) ; ∂L ∂ f (x j ) ← ∂L ∂ f (x j ) + λ 2 ∂L ∂σ 2 p ∂σ 2 p ∂d 2 i j ∂d 2 i j ∂ f (x j ) ; for k = 1, • • • , m, s.t. (i, k) ∈ N do ∂L ∂ f (x i ) ← ∂L ∂ f (x i ) + β i j B ( ∂F i j ∂d 2 i j ∂d 2 i j ∂ f (x i ) + ∂F i j ∂d 2 ik ∂d 2 ik ∂ f (x i ) ); ∂L ∂ f (x j ) ← ∂L ∂ f (x j ) + β i j B ∂F i j ∂d 2 i j ∂d 2 i j ∂ f (x j ) ; ∂L ∂ f (x k ) ← ∂L ∂ f (x k ) + β i j B ∂F i j ∂d 2 ik ∂d 2 ik ∂ f (x k ) ; for l = 1, • • • , m, s.t. (i, l) ∈ N do ∂L ∂ f (x i ) ← ∂L ∂ f (x i ) + λ 2 ∂L ∂σ 2 n ∂σ 2 n ∂d 2 il ∂d 2 il ∂ f (x i ) ; ∂L ∂ f (x l ) ← ∂L ∂ f (x l ) + λ 2 ∂L ∂σ 2 n ∂σ 2 n ∂d 2 il ∂d 2 il ∂ f (x l ) ; ∂σ 2 p ∂d 2 i j = 2 |P| d 2 i j -μ p (<label>16</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">∂σ 2 n ∂d 2 il = 2 |N | d 2 il -μ n (<label>17</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">∂F i j ∂d 2 i j = e F i j -1 ξ e F i j (<label>18</label></formula><formula xml:id="formula_31">)</formula><formula xml:id="formula_32">∂F i j ∂d 2 ik = - e d 2 i j -d 2 ik +α ξ ξ e F i j (<label>19</label></formula><formula xml:id="formula_33">)</formula><p>where ½[•] is an indicator function which outputs 1 if the expression evaluates to true and outputs 0 otherwise. The gradients of positive pair distance with respect to the feature embeddings are computed as</p><formula xml:id="formula_34">∂d 2 i j ∂ f (x i ) = 2( f (x i ) -f (x j )) (<label>20</label></formula><formula xml:id="formula_35">)</formula><formula xml:id="formula_36">∂d 2 i j ∂ f (x j ) = 2( f (x j ) -f (x i )). (<label>21</label></formula><formula xml:id="formula_37">)</formula><p>D. Implementation Details 1) Network Architecture: We use a subnetwork of GoogLeNet architecture (Inception v1 <ref type="bibr" target="#b72">[73]</ref>), from the image input to the output of inception-4e, followed by a global average pooling layer, a fully connected layer, an 2 -normalization layer, and finally the loss layer. Specifically, the person images are resized to 160 × 80 as an input. In the testing stage, the output of the 2 -normalization layer is directly used to compute the distance for person retrieval. For data preprocessing, we use the standard horizontal flips of the resized images.</p><p>2) Network Training: We use the Caffe <ref type="bibr" target="#b73">[74]</ref> package for the implementation. We pretrain the network on the ImageNet ILSVRC data set and fine-tune it on the person re-ID data sets. In this paper, we adopt an online sampling strategy for minibatch generation. For each sequentially selected identity, we randomly select at most K images into a minibatch. When all identities have been traversed, we shuffle the identity list for next round. The batch size is 150 and each identity has at most five images in a minibatch. Besides, the triplet set is also generated online. We set the initial learning rate as 0.01 and divide it by 10 after 10 000 iterations and 12 500 iterations. The weight decay is 0.0002 and the momentum for gradient update is 0.9. Each model is trained for 15 000 iterations. The margin parameter α, parameter λ, and scale factor ξ are set to 0.2, 0.5, and 0.05, respectively. The parameter γ is set to 0.95 for stable updating of mean distance. The parameters α p and α n are set to 0.01 and 0.1, respectively. The final embedding dimension of our approach is 128, which facilitates person retrieval on large-scale person re-ID data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets and Evaluation Protocols</head><p>The evaluation is carried out on three large-scale person re-ID data sets: Market-1501 <ref type="bibr" target="#b71">[72]</ref>, DukeMTMC-reID <ref type="bibr" target="#b25">[26]</ref>, and CUHK03 <ref type="bibr" target="#b31">[32]</ref>.</p><p>Market-1501 <ref type="bibr" target="#b71">[72]</ref> is one of the largest person re-ID data sets, containing 32 668 bounding boxes (cropped images) of 1501 identities. All the bounding boxes are detected by the deformable part model (DPM) pedestrian detector <ref type="bibr" target="#b74">[75]</ref>. Each identity has multiple images captured by at least two cameras and at most six cameras. Specifically, the training set contains 12 936 bounding boxes of 750 identities. The testing set contains 19 732 bounding boxes of 751 identities, where only one image of each identity is randomly selected as a query image for each camera. In total, the testing set contains 3368 query images. There are 2793 images included as distractors in the original gallery set for testing.</p><p>DukeMTMC-reID <ref type="bibr" target="#b25">[26]</ref> is a new large-scale person re-ID data set, derived from a multicamera pedestrian tracking data set (DukeMTMC <ref type="bibr" target="#b75">[76]</ref>). It contains 36 411 hand-drawn bounding boxes of 1812 identities, taken from eight different camera views, in which 1404 identities appear in more than two camera views and 408 identities appear in only one camera view whose images are used as distractors; 16 522 bounding boxes of 702 identities are randomly selected for training and the rest are used for testing, including 2228 query images and 17 661 gallery images.</p><p>CUHK03 <ref type="bibr" target="#b31">[32]</ref> is a widely used re-ID data set, containing 13 164 bounding boxes of 1360 identities, captured by two disjoint cameras. In the original evaluation protocol <ref type="bibr" target="#b31">[32]</ref>, 1160 identities are employed for training, and the remaining identities are used for validation and testing. In this paper, we adopt a new data set split <ref type="bibr" target="#b19">[20]</ref> for CUHK03, where 767 identities are used for training and the remaining 700 identities are employed for testing. The new protocol is more Fig. <ref type="figure" target="#fig_2">3</ref> shows some sample images from these data sets. We use the widely used evaluation protocol <ref type="bibr" target="#b34">[35]</ref>. In the matching process, we calculate the similarities between each query and all the gallery images and return the rank list according to the similarities. All the experiments are under the single query setting. The performance is evaluated by the cumulative matching characteristics (CMC) curve that is an estimation of the expectation of finding the correct match in the top-K matches and the mean average precision (mAP) score. We use the evaluation codes provided by Zhao et al. <ref type="bibr" target="#b76">[77]</ref> for the empirical evaluation in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Empirical Analysis 1) Effect of the Hardness-Aware Weights:</head><p>In Section III-B, we present a hard positive mining strategy that aims to pay more attention on hard positive pairs during optimization. As shown in <ref type="bibr" target="#b4">(5)</ref>, for each positive pair, we compare its squared Euclidean distance d 2 i j with a class-specific hard threshold τ c . Then, the positive pair with a large distance (d 2 i j &gt; τ c ) will result in larger contribution to the overall objective. In this section, we evaluate its effect by ablation study on the three benchmark data sets. The learning objective in <ref type="bibr" target="#b6">(7)</ref> that has the hardness-aware weight β i j is termed 2 Apair + HA. In Table <ref type="table">I</ref>, we compare its performance with the 2 Apair loss in <ref type="bibr" target="#b3">(4)</ref>. Note that we compare their performance using the networks trained with 10 000 iterations and 15 000 iterations.</p><p>We can observe in Table I that, by 10 000 iterations, 2 Apair + HA improves 2 Apair by 1.7%, 1.9%, and 0.4% rank-1, and 1.2%, 1%, and 1.2% mAP score on the three data sets, respectively. By 15 000 iterations, 2 Apair + HA improves 2 Apair by 0.8%, 0.7%, and 0.8% rank-1 and 0.4%, 0.5%, and 1% mAP. It shows that the effect of the proposed hard positive mining technique is more obvious in the early optimization stage. By exploiting this hardness-aware weighting technique, hard positive points can be pulled closer to the anchor points, thus accelerating the optimization, especially in the early learning stage. After 10 000 iterations, the intraclass distance has been significantly reduced. The margin between the hard positive pairs and the easy positive pairs becomes much smaller. Then, the penalization on the hard positive pair becomes weak. It can avoid excessive focus on the hard samples.</p><p>2) Effect of the Global Loss Term: This paper presents a global loss term that aims to regularize the network by utilizing the second-order statistics of positive/negative distance distribution. In detail, we expect the the learned distance has a low variance in the final embedding space. It is mainly inspired by the observation in <ref type="bibr" target="#b46">[47]</ref> that local learning manner may result in a failure: the gradient signal from positive pairs may get outweighed by negative pairs. It will lead to groups of examples with the same class label being separated into partitions in the embedding space.</p><p>By the proposed regularization term, we observe in Table <ref type="table">I</ref> that the performance of 2 Apair + HA can be further improved. By 15 000 iterations, rank-1 is improved by 0.9%, 0.6%, and 1% and mAP is improved by 1.1%, 1%, and 0.4%, respectively, on the three data sets. The improvement is actually not trivial, since 2 Apair loss is already a strong baseline which exploits the structural relationship in the data space. It is indicated in Section IV-C that our method has outperformed most state-of-the-art methods while using much lower dimensional features and a less sophisticated base network structure.</p><p>3) Effect of the Scale Factor ξ : In (4), the 2 Apair loss introduces a scale factor ξ to make the triplet loss softer in the 2 -normalized distance space, which can avoid the optimization to be collapsing. To evaluate its impact on the convergence, we plot the training loss curves with ξ = 1, 1/5, and 1/20, respectively, in Fig. <ref type="figure" target="#fig_3">4</ref>. The loss curve of triplet loss is also shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We can see that without this scale factor (ξ = 1), the training loss drops very slow due to the collapsing of optimization. When we set ξ = 1/5, the loss drops fast since the loss has been fivefolds amplified. In the experiment, we set ξ = 1/20, which shows a faster convergence speed than ξ = 1/5. It is recommended to set this scale factor as 1/10 or 1/20 for a faster optimization. Too small value of ξ will make the output of exp(•) beyond the limit of precision of double/float type, thus making the optimization unstable. Besides, we also observe in Fig. <ref type="figure" target="#fig_3">4</ref> that 2 Apair loss (ξ = 1/20) converges much faster than standard triplet loss, mainly benefiting from the structural distance comparison and the soft-margin design.</p><p>4) Effect of the Parameter λ: The parameter λ in (13) makes a tradeoff between the 2 Apair loss term and the proposed global regularization term. In this section, we investigate its effect by comparing the performance of our loss [see <ref type="bibr" target="#b12">(13)</ref>] at varying λ = {0, 0.1, 0.5, 1, 10}. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, we can observe a clear improvement when λ = 0.5 or λ = 1. When λ is small, its effect is not significant. A too large λ will also make the gradient signal from the second term (global loss) outweigh the first term (local loss), which makes the optimization slow.</p><p>5) On the Embedding Dimension d: In Table <ref type="table" target="#tab_0">II</ref>, we evaluate the performance of our approach [based on <ref type="bibr" target="#b12">(13)</ref>] at varying  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison With the State of the Arts</head><p>In this section, we compare our method with the recent state-of-the-art results on the Market-1501, DukeMTMC-reID, and CUHK03 data sets. Note that, for a fair comparison, we train the network using the same input image size (224 × 224) with that of most state-of-the-art methods, and use the standard evaluation codes of <ref type="bibr" target="#b71">[72]</ref>. Experimental results are presented in Tables III-V.</p><p>1) Results on Market-1501: In Table <ref type="table" target="#tab_1">III</ref>, we compare our result with state-of-the-art methods on the Market-1501 data set. Note that, all the state-of-the-art methods in Table <ref type="table" target="#tab_1">III</ref> are based on deep learning techniques. As shown in Table <ref type="table" target="#tab_1">III</ref>, the competitive methods, such as IDE <ref type="bibr" target="#b67">[68]</ref>, SVDNet <ref type="bibr" target="#b79">[80]</ref>, and PAN <ref type="bibr" target="#b80">[81]</ref>, are font of using identification loss together with the ResNet architecture. IDE outputs a strong baseline result: 73.8% rank-1 and 47.9% mAP. The SVDNeT incorporates singular vector decomposition into IDE to orthogonalize the base components in the fully connected layer. It significantly improves the performance of IDE, but it also largely increases the computational complexity. The PAN <ref type="bibr" target="#b80">[81]</ref> method introduces the pedestrian alignment into the IDE model. It can presented a generalized triplet loss with extremely hard sample mining. It obtains a better performance than our method. However, it is implemented using ResNet50 and outputs 1024-D features. The Dem <ref type="bibr" target="#b78">[79]</ref> method employs the complementation of identification loss and verification loss and also reports its result using GoogleNet: 73.52% rank-1 and 51.15% mAP. Our method surpasses it by nearly 10% rank-1 and over 12% mAP. Zhao et al. <ref type="bibr" target="#b76">[77]</ref> designed a part-aligned network module for pedestrian feature learning based on GoogleNet and triplet loss. Without the part detector, its base network is the same as ours. We improve its baseline (GoogleNet+Triplet loss) by over 8% rank-1 and 13% mAP under the same evaluation protocol. Even with the part-detector module, its performance is still inferior to ours. The competitive performance on the Market-1501 data set has demonstrated the effectiveness of the proposed approach.  It can be mainly attributed to the proposed hardness-aware structural learning objective and the global loss term. Fig. <ref type="figure" target="#fig_5">6</ref> shows the Barnes-Hut t-distributed stochastic neighbor embedding (t-SNE) visualization <ref type="bibr" target="#b81">[82]</ref> on a subset of the testing set using the learned deep features. It is clearly shown that most visually similar person images are grouped together. Our method is able to learn semantically meaningful features.</p><p>2) Results on DukeMTMC-reID: The DukeMTMC-reID data set is a newly released large-scale person re-ID data set, derived from a multicamera pedestrian tracking data set (DukeMTMC <ref type="bibr" target="#b75">[76]</ref>). We compare our method with several state-of-the-art DNN-based methods on this data set in Table <ref type="table" target="#tab_2">IV</ref>. All the compared methods listed in Table <ref type="table" target="#tab_2">IV</ref> are based on the identification loss. The IDE model <ref type="bibr" target="#b67">[68]</ref> reports its results using two network architectures: 46.9% rank-1 and 28.3% mAP based on CaffeNet, and 65.5% rank-1 and 44% mAP based on ResNet50. It shows that as a state-of-theart network architecture, ResNet50 can bring a significant performance improvement on the simple CNN architecture, such as CaffeNet. Another similar case is the Online Instance Matching (OIM) <ref type="bibr" target="#b28">[29]</ref> model which obtains 61.7% rank-1 with GoogleNet and 68.1% rank-1 with ResNet50. Nearly, 7% improvement is observed. The other three methods all adopt ResNet50 as their base network, in which Zheng et al. <ref type="bibr" target="#b25">[26]</ref> utilized the generative adversarial network technique to generate a large amount of unlabeled pedestrian images. These unlabeled images are further employed to augment the training The main reason is that the identification loss aims to learn a nonlinear mapping from person image to person ID and does not consider the interpersonal relationship directly. That is, the value of training data has not been fully exploited. Our loss aims to optimize a relative distance by comparing the positive pair with all the corresponding negative pairs. The structural relationship among samples has been exploited effectively.</p><p>3) Results on CUHK03: In this paper, we apply a new testing protocol in <ref type="bibr" target="#b19">[20]</ref> for the evaluation on the CUHK03 data set. This new protocol uses a smaller training set and a larger testing set (767 persons for training and 700 persons for testing). It is more challenging than the original setting <ref type="bibr" target="#b31">[32]</ref> where 1367 persons are employed for training and only 100 persons are used for testing. This data set includes two subsets in which one is manually labeled and the other one is detected by the DPM pedestrian detector. We only report our result on the detected set, since we observe a similar performance on the two sets. Under the new data set split, only 7365 images can be used for training, thus making the network prone to overfitting. In each iteration, we randomly sample a minibatch and generate large numbers of triplets online for our learning objective, which can reduce the risk of overfitting. It can be observed from the performance comparison in Table <ref type="table" target="#tab_3">V</ref>, the IDE model <ref type="bibr" target="#b67">[68]</ref> can only obtain 30.50% rank-1 and 21.1% mAP using ResNet50, while our method achieves 39.64% rank-1 and 35.67% mAP using GoogleNet. It improves the mAP of IDE by over 14%. It can be mainly attributed to the structural learning objective and the global loss term in this paper. A low-dimensional but discriminative deep embedding can be learned by our method. In Table V, the SVDNet <ref type="bibr" target="#b79">[80]</ref> obtains the best result on this data set by orthogonalizing the base components of fully connected layer. However, if taking the network structure and feature dimension into consideration, our method is more competitive than SVDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present an effective person re-ID framework by discriminatively learning a nonlinear deep feature mapping from person images to low-dimensional embeddings, where similar samples are mapped closer to each other, while dissimilar samples are pushed farther apart. The proposed approach jointly learns feature representation and distance metric in an end-to-end manner. The main contribution of this paper is that we develop a hardness-aware structural metric learning objective where each positive pair is allowed to be compared with all the corresponding negative pairs within minibatch and each positive pair is assigned a hardness-aware weight to adaptively modulate its contribution. Moreover, we incorporate a global loss term that penalizes large variance of positive/negative pair distances into the proposed objective function, which improves the generalization capability of the network effectively. Extensive experimental evaluations on three large-scale data sets have demonstrated the effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.Overview of the proposed person re-ID framework. The basic idea is to learn a nonlinear mapping from person images to discriminative embeddings based on a deep CNN. For each iteration, in the training stage, we feed the network with an identity-balanced minibatch generated by an online random sampling strategy and generate triplets online. The output of the last fully connected layer is 2 -normalized and passed into the loss layer. The network parameters are updated by backpropagation supervised by a hardness-aware structural metric learning objective. In the testing stage, given a probe image, we compute the Euclidean distances between the probe and gallery images using the learned deep embeddings. Our network architecture is simpler yet effective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of (a) easy positive pairs and (b) hard positive pairs. Images are sampled from the Market-1501 [72] data set.</figDesc><graphic coords="4,338.51,135.77,204.14,61.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sample images from three large-scale person re-ID data sets. (a) Market-1501 [72]. (b) DukeMTMC-reID [26]. (c) CUHK03 [32].</figDesc><graphic coords="7,431.03,58.37,108.14,139.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Convergence curves of 2 Apair loss with ξ = 1, 1/5, and 1/20, and triplet loss.</figDesc><graphic coords="8,91.91,61.37,189.38,129.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. MAP comparison at varying λ = {0, 0.1, 0.5, 1, 10}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Barnes-Hut t-SNE visualization<ref type="bibr" target="#b81">[82]</ref> of our learned embeddings on a subset of the testing set in the Market-1501 data set<ref type="bibr" target="#b71">[72]</ref>. Best viewed in color.</figDesc><graphic coords="9,318.47,58.25,237.62,135.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON (CMC@RANK-R AND MAP, %) ON THE MARKET-1501 DATA SET AT VARYING EMBEDDING DIMENSIONS embedding sizes {32, 64, 128, 256, 512, 1024} on the Market-1501 data set. As shown in TableII, our approach has the ability of learning much lower dimensional discriminative embeddings. Our approach can obtain a very impressive performance: 73.96% rank-1 and 56.47% mAP, even when d = 32. When 32 ≤ d ≤ 128, the performance increases significantly. When 128 ≤ d ≤ 1024, the performance increases</figDesc><table><row><cell>much slower. Rank-1 starts to drop when d &gt; 512. A larger</cell></row><row><cell>embedding size may not guarantee a better performance, but</cell></row><row><cell>it undoubtedly incurs a higher time complexity in the testing</cell></row><row><cell>stage. Therefore, we set the final embedding size as 128, which</cell></row><row><cell>works very well in the experiments. Such a low-dimensional</cell></row><row><cell>embedding facilitates the large-scale person retrieval in the</cell></row><row><cell>real-world scenarios.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON (CMC@RANK-R AND MAP, %) ON THE MARKET-1501 DATA SET. A LARGER NUMBER INDICATES A BETTER RESULT. † DENOTES THE RESULTS FROM CONCURRENT WORK ONLY PREPUBLISHED ON ARXIV. * DENOTES RESULTS USING THE EVALUATION CODES OF [77]. I, V, C, AND T STAND FOR IDENTIFICATION, VERIFICATION, CONTRASTIVE, AND TRIPLET, RESPECTIVELY. DIM MEANS THE DIMENSION OF THE LEARNED DEEP FEATURES reduce the negative effect of the background. It obtains 82.81% rank-1 and 63.4% mAP. Although they yield a promising performance on Market-1501, they rely on the ResNet50 architecture that is deeper and complicated than GoogleNet. ResNet-50 has a much higher memory and computation requirements than GoogleNet, especially while training. The size of the final trained model becomes an important concern to consider if we are looking to deploy a model to run locally on mobile. Besides, they usually use the output of the last pooling layer as the deep feature for testing, thus resulting in a 2048-D embedding which is unsuitable for fast person retrieval. However, this paper aims to learn low-dimensional deep embeddings using a relatively small network architecture: GoogleNet(V1). As shown in Table III, our method obtains 83.43% rank-1 and 63.66% mAP using only 128-D features. It has surpassed most state-of-the-art results listed in Table III. Xiao et al. [71]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON (CMC@RANK-R AND MAP, %) ON THE DUKEMTMC-REID DATA SET. A LARGER NUMBER INDICATES A BETTER RESULT. † DENOTES RESULTS FROM CONCURRENT WORK ONLY PREPUBLISHED ON ARXIV. I STANDS FOR IDENTIFICATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON (CMC@RANK-R AND MAP, %) ON THECUHK03 (DETECTED) DATA SET USING THE NEW EVALUATION PROTOCOL<ref type="bibr" target="#b19">[20]</ref>. THIS PROTOCOL USES A LARGER TESTING SET (767 PERSONS FOR TRAINING AND 700 PERSONS FOR TESTING). A LARGER NUMBER INDICATES A BETTER RESULT. † DENOTES RESULTS FROM CONCURRENT WORK ONLY PREPUBLISHED ON ARXIV. I STANDS FOR IDENTIFICATION set of the IDE model, which yields 2% performance improvement. Our method adopts a much smaller network architecture: GoogleNet, but achieves a state-of-the-art result on DukeMTMC-reID. Our rank-1 accuracy is 75.4% that surpasses the strong baseline [IDE (ResNet50)] by nearly 10%. It experimentally demonstrates the effectiveness of our loss.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Nature Science Foundation of China under Grant 61732008 and Grant 61725203.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person re-identification by multihypergraph fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2763" to="2774" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person reidentification based on elastic projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1314" to="1327" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person reidentification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical Gaussian descriptor for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1363" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color invariants for person reidentification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kviatkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1622" to="1634" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning invariant color features for person reidentification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3395" to="3410" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient PSD constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3656" to="3670" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput</title>
		<meeting>Eur. Conf. Comput<address><addrLine>Vis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PCCA: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shrinkage expansion adaptive metric learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep ranking for person reidentification via joint representation learning</title>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2353" to="2367" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="3346" to="3355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artif. Intell</title>
		<meeting>Int. Joint Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2194" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 22nd Int. Conf. Pattern Recognit</title>
		<meeting>IEEE 22nd Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-08">Aug. 2014</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepReID: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep neural networks with inexact matching for person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2667" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based CNN with improved triplet loss function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.07737" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale triplet CNN for person re-identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf</title>
		<meeting>ACM Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="192" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput., Commun., Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.07220" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class N-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1849" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2206" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Manifold preserving: An intrinsic approach for semisupervised distance metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2731" to="2742" />
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised metric fusion over multiview data by graph random walk-based crossview diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A kernel classification framework for metric learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1950" to="1962" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Face alignment with deep regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="194" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Signature verification using a &apos;Siamese&apos; time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Metric learning with adaptive density discrimination</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Person re-identification with metric learning using privileged information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="791" to="805" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification in a self-trained subspace</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput., Commun., Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.05244" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1601.07255" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Deep person reidentification with improved embedding and efficient training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.03332" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.02984" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Margin sample mining loss: A deep learning based method for person re-identification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.00478" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection with deformable part models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rybski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Vehicles Symp</title>
		<meeting>IEEE Intell. Vehicles Symp</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1035" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput., Commun., Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="3820" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.00408" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">His current research interests include person reidentification, multimedia content analysis, computer vision, and pattern recognition. Peicheng Zhou received the B.S. degree from the Xi&apos;an University of Technology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2011">Oct. 2014. 2015 to 2017. 2011. 2014</date>
			<pubPlace>Hefei, China; Ultimo NSW, Australia; Xi&apos;an, China; Xi&apos;an</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Hefei University of Technology ; Faculty of Engineering and Information Technology, University of Technology Sydney ; Northwestern Polytechnical University</orgName>
		</respStmt>
	</monogr>
	<note>Xun Yang is currently pursuing the Ph.D. degree with the School of Computer and Information Engineering. where he is currently pursuing the Ph.D. degree. His current research interests include computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">He is currently a Professor with the Hefei University of Technology, Hefei. His current research interests include multimedia content analysis, computer vision, and pattern recognition. He has authored over 200 book chapters and journal and conference papers in these areas</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wang was a recipient of the ACM SIGMM Rising Star Award 2014. He is an Associate Editor of the IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, the IEEE TRANSACTIONS ON CIRCUITS AND SYS-TEMS FOR VIDEO TECHNOLOGY, the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, and the IEEE TRANSACTIONS ON MULTIMEDIA</title>
		<meeting><address><addrLine>Hefei, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003 and 2008</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electronic Engineering and Information Science, University of Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>SM&apos;17) received the B.E. and Ph.D. degrees from the Special Class for the Gifted Young. respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
