<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GenIE: Generative Information Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-13">13 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
							<email>martin.josifoski@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
							<email>nicola.decao@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
							<email>maxime.peyrard@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
							<email>fabiopetroni@fb.com</email>
							<affiliation key="aff3">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>West</surname></persName>
							<email>robert.west@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GenIE: Generative Information Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-13">13 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.08340v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to extract structured semantic information from unstructured texts is crucial for many AI tasks such as knowledge discovery <ref type="bibr" target="#b28">(Ji and Grishman, 2011;</ref><ref type="bibr" target="#b54">Trisedya et al., 2019)</ref>, knowledge maintenance <ref type="bibr" target="#b52">(Tang et al., 2019)</ref>, symbolic representation, and reasoning <ref type="bibr" target="#b29">(Ji et al., 2021)</ref>. The interface between free text and structured knowledge is formalized by knowledge base population (KBP; <ref type="bibr" target="#b28">Ji and Grishman, 2011)</ref>, which proposes to represent the information contained in text using (subject, relation, object) fact triplets. In this work, we focus We use a transformer encoder-decoder model that takes unstructured text as input and autoregressively generates a structured semantic representation of the information expressed in it, in the form of (subject, relation, object) triplets. GenIE employs constrained beam search with: (i) a high-level constraint which asserts that the output corresponds to a set of triplets; (ii) lower-level constraints which use prefix tries to force the model to only generate valid entity or relation identifiers (from a predefined schema).</p><p>on closed information extraction (cIE), the problem of extracting exhaustive sets of fact triplets expressible under the relation and entity constraints defined by a Knowledge Base (KB) schema.</p><p>Traditionally, cIE was approached with pipelines that sequentially combine named entity recognition <ref type="bibr" target="#b53">(Tjong Kim Sang, 2002)</ref>, entity linking <ref type="bibr" target="#b40">(Milne and Witten, 2008)</ref>, and relation extraction <ref type="bibr" target="#b39">(Miller et al., 1998)</ref>. Entity linking and relation extraction serve as grounding steps, matching entities and relations to numerical identifiers in a KB, e.g., QIDs and PIDs for Wikidata. <ref type="bibr">Recently, Trisedya et al. (2019)</ref> pointed out that such pipeline architectures suffer from the accumulation of errors and proposed an end-to-end alternative. Nevertheless, existing methods are still only practical for small schemas with unrealistically small numbers of relations and entities.</p><p>Alternatively, some works have focused on a simpler syntactic task: open information extraction (oIE), which produces free-form triplets from texts. In this setup, the entities and relations are not grounded in a KB and, usually, do not represent facts <ref type="bibr" target="#b20">(Gashteovski et al., 2020)</ref>. As oIE triplets contain only surface relations, they have ambiguous semantics, making them hard to use in downstream tasks <ref type="bibr" target="#b5">(Broscheit et al., 2017)</ref> if not first aligned with a KB <ref type="bibr" target="#b20">(Gashteovski et al., 2020)</ref>. Since, in practice, oIE often consists of structured substring selection, it has recently been framed as an endto-end sequence-to-sequence problem with great success <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021;</ref><ref type="bibr" target="#b14">Dognin et al., 2021)</ref>. Indeed, such autoregressive formulations can exploit the language knowledge already encoded in pre-trained transformers <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>. For example, some tokens can be more easily recognized as possible entities or relations thanks to the pre-training information.</p><p>Inspired by recent successes in oIE, we propose the first autoregressive end-to-end formulation of cIE that scales to many entities and relations, making cIE practical for more realistic KB schemas (i.e. schemas with millions of entities). <ref type="foot" target="#foot_0">1</ref> We employ a sequence-to-sequence BART model <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref>, and exploit a novel bi-level constrained generation strategy operating on the space of possible triplets (from a fixed schema induced by Wikidata) to ensure that only valid triplets are generated. Our resulting model, GenIE, performs Generative Information Extraction and combines the advantages of a known schema with an autoregressive formulation. The high-level overview of GenIE is provided in Fig. <ref type="figure" target="#fig_0">1</ref>. The constrained generation encodes the known schema and enables the autoregressive decoder to generate textual tokens but only from the set of allowed entities or relations. constrained beam search can be applied on large, structured, and compositional spaces.</p><p>• We propose a model that achieves state-of-theart performance on the cIE task and scales to previously unmanageable numbers of entities (6M) and relations (more than 800). • We point out and address weaknesses in the evaluation methodologies of recent previous works stemming from their small scale and the large imbalances in the available data per relation. We demonstrate the importance of reporting performance as a function of the number of relation occurrences in the data. • We release pre-processed data, pre-trained models, and code within a general template designed to facilitate future research at https: //github.com/epfl-dlab/GenIE.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Closed Information Extraction</head><p>In this work, we address the task of closed information extraction (cIE), which aims to extract the exhaustive set of facts from natural language, expressible under the relation and entity constraints defined by a knowledge base (KB).</p><p>Most of the existing methods address the problem with a pipeline solution. One line of work starts by first extracting the entity mentions and the relations between them from raw text. This is followed by a disambiguation step in which the entity and relation predicates are mapped to their corresponding items in the KB. The sub-task of extracting the free-form triplets was originally proposed by <ref type="bibr" target="#b4">Banko et al. (2007)</ref>, and it is commonly referred to as open information extraction (oIE) or text-to-graph in the literature <ref type="bibr" target="#b23">(Guo et al., 2020;</ref><ref type="bibr" target="#b6">Castro Ferreira et al., 2020;</ref><ref type="bibr" target="#b27">Huguet Cabot and Navigli, 2021;</ref><ref type="bibr" target="#b44">Shen et al., 2015)</ref>. Another line of work employs a pipeline of models for (i) named entity recognition (NER) -detecting the entity mentions; (ii) entity linking (EL) -mapping the mentions to specific entities from the KB; (iii) relation classification (RC) -detecting the relations that are expressed between the entities <ref type="bibr" target="#b16">(Galárraga et al., 2014;</ref><ref type="bibr" target="#b2">Angeli et al., 2015b;</ref><ref type="bibr" target="#b7">Chaganty et al., 2017)</ref>. Due to their architecture, pipeline methods are plagued by error propagation, which significantly affects their performance <ref type="bibr" target="#b38">(Mesquita et al., 2019;</ref><ref type="bibr" target="#b54">Trisedya et al., 2019)</ref>.</p><p>End-to-end systems that jointly perform the extraction and the disambiguation of entities and rela-tions have been proposed to address the error propagation <ref type="bibr" target="#b54">(Trisedya et al., 2019;</ref><ref type="bibr" target="#b46">Sui et al., 2021;</ref><ref type="bibr" target="#b37">Liu et al., 2018)</ref>. To mitigate the propagation of errors, these systems are endowed with the ability to leverage entity information in the relation extraction and vice-versa, which has resulted in significant performance gains. Conceptually, for producing the output triplets, existing methods all rely on atomic, multi-class classification-based ranking of relations and entities. Classification methods particularly suffer from imbalances in the data. On the contrary, our proposition, GenIE, is autoregressive and deals better with imbalances.</p><p>While cIE requires the constituent elements of the output triplets to be entities and relations associated with the KB, the output triplets in oIE are free-text. This makes the cIE task fundamentally harder than oIE and renders the majority, if not all, oIE methods inapplicable to the cIE setting. We report an additional discussion on relevant, but not fundamental, related work on oIE in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoregressive Entity Linking</head><p>The tasks of entity linking (EL) and entity disambiguation (ED) have been extensively studied in the past <ref type="bibr" target="#b26">(Huang et al., 2015;</ref><ref type="bibr" target="#b57">Wu et al., 2020;</ref><ref type="bibr" target="#b33">Le and Titov, 2018;</ref><ref type="bibr" target="#b31">Kolitsas et al., 2018;</ref><ref type="bibr" target="#b3">Arora et al., 2021)</ref>. Most existing approaches associate entities with unique atomic labels and cast the retrieval problem as multi-class classification across them. The match between the context and the label can then be represented as the dot product between the dense vector encodings of the input and the entity's meta information <ref type="bibr" target="#b57">(Wu et al., 2020)</ref>. This general approach has led to large performance gains. <ref type="bibr">Recently, De Cao et al. (2021a</ref><ref type="bibr">,b, 2022)</ref> have suggested that the classification-based paradigm for retrieval comes with several shortcomings such as (i) the failure to capture fine-grained interactions between the context and the entities; (ii) the necessity of tuning an appropriately hard set of negative samples during training. Building on these observations, they propose an alternative solution that casts the entity retrieval problem as one of autoregressive generation in which the entity names are generated token-by-token in an autoregressive fashion. The (freely) generated output will not always be a valid entity name, and to solve this problem De <ref type="bibr" target="#b11">Cao et al. (2021b)</ref> propose a constrained decoding strategy that enforces this by employing a prefix trie. Their method scales to millions of entities, achieving state-of-the-art performance on monolingual and multilingual entity linking.</p><p>Inspired by the intuition that language models are well suited for predicting entities, we propose a novel approach for cIE by framing the problem in an autoregressive generative formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we formalize GenIE, an autoregressive end-to-end model for closed information extraction. Let us assume a knowledge base (KB) consisting of a collection of entities E , a collection of relations R, and a set of facts (s, r, o) ∈ E × R × E stored as (subject, relation, object) triplets. Additionally, we assume that each entity e ∈ E and relation r ∈ R is assigned to a textual label (corresponding to its name). The Wikidata KB (Vrandečić, 2012), with Wikipedia page titles as entity names, and the Wikidata relation labels as relation names satisfy these assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We cast the task of information extraction as one of autoregressive generation. More concretely, given some text input x, GenIE strives to generate the linearized sequence representation y of the exhaustive set of facts expressed in x. The conditional probability (parameterized by θ) assigned to the output y is computed in the autoregressive formulation: p θ (y | x) = |y| i=1 p θ (y i | y &lt;i , x). This can be seen as translating the unstructured text to a structured, unambiguous representation in a sequence-to-sequence formulation. GenIE employs the BART <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref> transformer architecture. It is trained to maximize the target sequence's conditional log-likelihood with teacher forcing <ref type="bibr" target="#b48">(Sutskever et al., 2011</ref><ref type="bibr" target="#b49">(Sutskever et al., , 2014))</ref>, using the cross-entropy loss. We use dropout <ref type="bibr" target="#b45">(Srivastava et al., 2014)</ref> and label smoothing for regularization <ref type="bibr" target="#b50">(Szegedy et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Output Linearization</head><p>To represent the output with a sequence of symbols that is compatible with sequence-to-sequence architectures, we introduce the special tokens &lt;sub&gt;, &lt;rel&gt;, &lt;obj&gt; to demarcate the start of the subject entity, the relation type and the object entity for each triplet. The special token &lt;et&gt; is introduced to demarcate the end of the object entity, which is also the end of the triplet. We construct the sequence representation by concatenating the textual representations of its constituent triplets. While the sequence representation has an intrinsic notion of order, the output set of triplets does not. To mitigate the effects of this discrepancy, we enforce a consistent ordering of the target triplets during training. Concretely, whenever the triplets' entities are linked to the entity mentioned in the textual input, we consider first the triplets for which the subject entity appears earlier in the text. Ties are resolved by considering the appearance position of the object entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference with Constrained Beam Search</head><p>The space of triplets corresponds to T = E × R × E , and the target space, which consists of triplet sets of arbitrary cardinality, is equivalent to</p><formula xml:id="formula_0">S = ∞ i=0 [E × R × E ] i .</formula><p>At inference time, GenIE tackles the task of retrieving the linearized representation y S ∈ S of a set of facts S = {t 1 , . . . ,t n } constituted by triplets t i ∈ T expressed in the input text x. Ideally, we would consider every element y ∈ S in the target space, assign it a score p θ (y | x), and retrieve the most probable y. Unfortunately, this is prohibitively expensive since we are dealing with a compositional target space whose size is gigantic (e.g., if we consider a Wikidata entity catalog of |E | ≈ 6M elements and a relation catalog of |R| ≈ 1000 relations, that can express a total of |T | ≈ 10 15 triplets; even if we limit ourselves to sentences that express only two facts, this provides us with ≈ 10 30 different output options).</p><p>On the other hand, the output needs to follow a particular structure, and contain only valid entity and relation identifiers. This does not necessarily hold for an arbitrary generation from a sequenceto-sequence model.</p><p>GenIE employs constrained beam search (BS; <ref type="bibr" target="#b49">Sutskever et al., 2014;</ref><ref type="bibr" target="#b11">De Cao et al., 2021b)</ref> to resolve both of these problems. Instead of explicitly scoring all of the elements in the target space S , the idea is to search for the top-k eligible options, using BS with k beams and a prefix trie. BS considers one step ahead -the next token to be generated -conditioned on the previous ones. The prefix trie restricts the BS to candidate tokens that could lead to valid identifiers. However, for the cIE setting we are interested in, the target space is prohibitively large to pre-compute the necessary trie. Therefore, we enforce a bi-level constraint on the output that allows for compositional, dynamic generation of the valid prefixes. More specifically, GenIE en-forces: (i) a high-level structural constraint which asserts that the output follows the linearization schema defined in Sec. 3.2; (ii) lower level validity constraints which use an entity trie and a relation trie to force the model to only generate valid entity or relation identifiers, respectively -depending on the specific element of the structure that is being generated. This outlines a general approach for applying BS to search through large compositional structured spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Base: Wikidata</head><p>We use Wikidata<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b56">(Vrandečić, 2012)</ref> as the target KB to link to, filtering out all entities that do not have an English Wikipedia page associated with them. The filtering guarantees that all entity names are unique. Our final entity set E contains 5,891,959 items. We also define our relation set R as the union of all the relations considered in the datasets described below, resulting in 857 relations. For different datasets, we consider only the subset of annotated relations to better compare with baselines. Although large, the number of entity (and relation) names is not a memory bottleneck as the generated prefix trie occupies ≈200MB of storage (e.g., the entity linking system proposed by <ref type="bibr" target="#b57">Wu et al. 2020</ref> needs &gt;20 times more storage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Evaluation Metrics</head><p>In this work, we further annotate and adapt REBEL <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021)</ref> and Wiki-NRE for training, validation and testing. Additionally, we use Geo-NRE <ref type="bibr" target="#b54">(Trisedya et al., 2019)</ref>, and FewRel <ref type="bibr" target="#b24">(Han et al., 2018)</ref> for testing purposes only. Appendix B contains descriptions of these datasets and their statistics. We measure the performance in terms of micro and macro precision, recall and F1. See Appendix C for a detailed and formal description of these metrics. We also report a 1-standard-deviation confidence interval constructed from 50 bootstrap samples of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare GenIE against Set Generation Networks (SetGenNet; <ref type="bibr" target="#b46">Sui et al., 2021)</ref> which is, to the best of our knowledge, the strongest model on Wiki-NRE and Geo-NRE. Note that the authors did not release code or the model and there is no other model from the literature trained and evaluated on REBEL for cIE. SetGenNet <ref type="bibr" target="#b46">(Sui et al., 2021)</ref> is an end-to-end state-of-the-art model for triplet extraction. It consists of a transformer encoder <ref type="bibr" target="#b55">(Vaswani et al., 2017</ref>) that encodes the input followed by a non-autoregressive transformer decoder <ref type="bibr" target="#b22">(Gu et al., 2018)</ref>. The decoder generates embeddings that are used to predict entities and relations. SetGenNet further uses candidate selection <ref type="bibr" target="#b17">(Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b31">Kolitsas et al., 2018)</ref> to reduce the output space and a bipartite matching loss that handles different prediction orderings (i.e., it generates a set). Note that there are weaker baselines (e.g., <ref type="bibr" target="#b54">Trisedya et al. 2019)</ref> we could have used to compare on REBEL, but we were not able to reproduce their code. We report details on the effort made to use these baselines in Appendix D.</p><p>We also implement a pipeline baseline, consisting of 4 independent steps, namely: (i) named entity recognition (NER), which selects the spans in the input source likely to be entity mentions; (ii) entity disambiguation (ED), which links mentions to their corresponding identifiers in the KB; (iii) relation classification (RC), which predicts the relation between a given pair of entities, and finally; (iv) triplet classification (TC), which predicts whether a given triplet is actually entailed by the context. TC is necessary because the previous step (RC) predicts a relation for every pair of entities. Each step needs to be trained independently with a specific architecture tailored for the task, and we made an optimal choice for each step. For the NER component we used the state-of-the-art tagger FLAIR<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b0">(Akbik et al., 2019)</ref>, while for ED we used the GENRE linker 4 (De Cao et al., 2021b). These two models were already trained, and we use them for inference only. For RC and TC, we trained a RoBERTa <ref type="bibr" target="#b36">(Liu et al., 2019)</ref> model with a linear classification layer on top (as these two sub-tasks are typically cast as classification problems). Trisedya et al. ( <ref type="formula">2019</ref>) also proposed many other pipeline baselines but ours outperforms them (see Table <ref type="table" target="#tab_6">5</ref> in Appendix G for comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Evaluation</head><p>Models performing cIE can base their predictions on different schemas. In this section, we distinguish between small and large evaluation schema.</p><p>The small evaluation schema is consistent with previous approaches where models only have to decide between a small set of relations and entities (the schema induced by Wiki-NRE). In the large evaluation schema, models use the schema induced by REBEL. Models also use the large evaluation schema of REBEL when tested on FewRel, as a high-quality and challenging recall-based evaluation. We consider 3 training setups for GenIE and the pipeline baseline comprised of SotA components: (i) the training set of Wiki-NRE (W) only, (ii) the training set of REBEL (R) only, and (iii) pre-training on REBEL and fine-tuning on Wiki-NRE (R+W). The implementation details are given in Appendix E. We report the macro and micro precision, recall, and F1 in Table <ref type="table" target="#tab_0">1</ref>. Unfortunately, as the code for SetGenNet is not available, we cannot compute its macro performance, thus we report the micro scores only.</p><p>First, we observe a large and significant F1 improvement of 8 and 28 absolute points obtained by GenIE over SetGenNet and the pipeline baseline, respectively, when trained on the same dataset (W). Despite the much bigger schema employed by REBEL, pre-training on it and then fine-tuning (R+W), improves the performance on Wiki-NRE and Geo-NRE for 3% and 5%, respectively. This highlights that: (i) GenIE can effectively transfer knowledge across datasets/schemas; (ii) GenIE can quickly adapt to new schemas. Due to its rigid, monolithic relation classifier, the pipeline baseline does not possess these qualities. However, the pretraining does improve its macro scores.</p><p>Only the newly developed pipeline baseline and GenIE can scale-up to the larger schema<ref type="foot" target="#foot_4">5</ref> , and as expected, this setting is more challenging for both models. However, GenIE still preserves a good F1 score of 68 micro and 34 macro, which is a relative increase of 60% and 320%, respectively, over the baseline. While the pipeline has a steeper drop from micro to macro scores, in general, a significant difference between the two is observed in every setting. This suggests that the models perform better for relations associated with many training examples and significantly worse for the rest. These findings call for the fine-grained analysis of performance in Sec. 5.2 that partitions the relations according to their occurrence count in the training data. For completeness, we also provide an analysis of performance as a function of the number of relations considered, in Appendix F.1.</p><p>Finally, on FewRel, recall is the only welldefined metric (see Appendix B). In this setting as well, GenIE greatly outperforms the baseline by 13 (micro) and 11 (macro) recall points (micro and macro are close as the dataset is class-balanced).</p><p>Ablation study. In Table <ref type="table" target="#tab_1">2</ref> we summarize the results of an ablation study considering the pretraining and the constrained generation. We consider three different starting points: (i) a random initialization; (ii) BART <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref> pretrained language model (PLM); (iii) a pre-trained autoregressive entity retrieval model GENRE <ref type="bibr" target="#b11">(De Cao et al., 2021b)</ref>. The pre-trained models are better in terms of recall and exhibit a better outof-domain generalization on FewRel. In contrast, they are slightly worse in terms of precision, which translates to maximum improvement of a single point in F1 on REBEL. Another salient advantage of pre-training is reducing the training steps necessary for achieving good results. Indeed, when starting from GENRE, 3-5k steps are sufficient for competitive performance; starting from a PLM necessitates 5-10k steps; while a random initialization requires 40-50k steps for competitive performance. Additionally, the pre-trained versions converge to a lower validation loss (see Fig. <ref type="figure" target="#fig_4">5</ref> in Appendix G).</p><p>To quantify the benefits from the constrained generation, we compare the results attained by the randomly initialized model with and without constraints. In addition to ensuring a structure on the output, the constrained generation strategy results in an increase of 2-3 absolute points in terms of F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Performance as a Function of the Relation Occurrence Count</head><p>The datasets naturally present large imbalances, where few relations occur a lot, but most relations are rare. In the previous section, we already observed a large difference between macro and micro F1 scores of models, indicating that the number of occurrences impacts model performances. Thus, we now measure F1 scores after bucketing relations according to their number of occurrences in the training dataset. In Fig. <ref type="figure" target="#fig_1">2</ref>  The histogram first confirms that most of the relations occur in only a few triplets from the training data. Models thus need to perform few-shot learning for most of the relations. GenIE is significantly better than the pipeline baseline for all the buckets. Finally, it is important to highlight that even though the performance of both methods, unsurprisingly, declines for relations that appear less often in the training data, GenIE already performs well for relations with at least 2 6 = 64 occurrences. On the contrary, the baseline needs 2 14 = 16,384 samples to reach a comparable level of performance, and scores better than GenIE does for the 2 6 = 64 bucket only after seeing at least 2 19 = 524,288 samples. This confirms that GenIE is not only better at macro and micro F1, but it is also capable of performing fewer-shot learning than the baseline. It further shows that, contrary to the baseline, GenIE's good scores do not come solely from its ability to perform well on the few most frequent relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Disentangling the Errors</head><p>The task of cIE, explicitly or implicitly, encompasses NER, NEL and RC as its subtasks. Failure in any of them directly translates to failure on the original task. Therefore, to effectively compare different cIE models and accurately characterize their behavior, we need to evaluate their performance on each of the subtasks.</p><p>The separation of responsibility between the pipeline components leads to a natural error attribution for the SotA pipeline model. To estimate the NER error, we take the entity mentions predicted by the NER component and compare them with the corresponding mentions of the constituent entities of the output triplets. All triplets that concern an entity whose mention was not retrieved by the NER component are considered erroneous. We differentiate between two settings: (i) exact, which requires that the generated mention exactly matches the target mention; and (ii) partial, for which any overlap between the generated and the target triplet is sufficient. The NEL error is calculated by considering the output of the NEL component and comparing the linked entities to those in the output triplets. Again, all of the triplets that concern an incorrectly linked entity are considered erroneous. Finally, for the pipeline, every correctly predicted relation label translates to a correctly extracted triplet. Therefore, we calculate the RC error using the cIE definition of recall given in Appendix C. End-to-end systems tackle all of the sub-tasks jointly, which makes the error attribution, in this setting, less obvious. To estimate errors corresponding to a particular target triplet, we need a reference triplet -among the predicted ones -for comparison. We start by outlining a bipartite matching procedure. Let each triplet be a node in a graph. We add an edge between each target-prediction pair of triplets. The edges are assigned a weight determined as a function of the pair of triplets they connect. Concretely, an edge e that connects the target triplet t T = (s T , r T , o T ) and the predicted triplet t P = (s P , r P , o P ) will be assigned a weight of: 1, if the triplets are the same; 2, if they express the same relation, but either the subject or the object differ; 3, if they concern the same entities, but the relation differs; 4, if they share only a single entity; 5, if they share only a single relation; 6 if they have nothing in common. We construct the matching by selecting edges in a greedy fashion until all of the target triplets have been matched. The procedure ensures that every target triplet is paired with its closest match. Finally, we estimate the NEL error as the portion of edges that were assigned a weight w ∈ {2, 4, 5, 6}, and the RC error as the portion of edges that where assigned a weight w ∈ {3, 4, 6}.</p><p>The results of this analysis are summarized in Fig. <ref type="figure" target="#fig_2">3</ref>. Immediately, the NER component in the pipeline method introduces an 18% error by completely missing on relevant entity mentions. An additional 12 absolute points hinge on a partial matching. The NEL component matches most of the entity mentions that are retrieved, but at this point, even with a (hypothetical) perfect RC, the performance of the pipeline will be only on par with GenIE. In practice, the RC component adds 30% to the inherited error, effectively doubling it.</p><p>On another note, the absolute error attributed to NEL by the pipeline and GenIE differs in a few absolute points only, while the difference for the (non-inherited) error stemming from RC is less than 10%. Adding these two together leaves us much shorter than the actual gap of 28 absolute points in performance on the cIE task. This suggests a strong correlation between the performance on NEL and RC for GenIE, which is fueled by the increased flow of information between the subtasks. The previous allows for more fine-grained interactions between the entities, the relations, and the context to be captured, consequently improving the overall performance. Alternatively, whenever the model captures a misleading correlation/interaction, it is amplified and hinders the performance on both subtasks. This result is echoed by the fact that the sum of the errors attributed to NEL and RC is significantly smaller than the error on the cIE task. Based on this observation, we hypothesize that any improvement on NEL will overflow to the RC subtask -and vice-versa -thereby directly translating to performance gains on the overall task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Unifying the cIE spectrum. There is a full spectrum of tasks that are closely related to cIE and are central to the field of information extraction. The typical setup assumes a KB associated with entities and relations, and the goal is to either annotate the text with information from the KB, or extract structured unambiguous information from the text. The tasks of entity linking and relation classification, already discussed in Sec. 2 and Sec. 4.3, are two such examples. Another example is slot filling (SF), the task of extracting information for a specific entity and relation (e.g., entity Mick Jagger, relation member of ) from natural language <ref type="bibr" target="#b47">(Surdeanu, 2013;</ref><ref type="bibr" target="#b43">Petroni et al., 2021)</ref>.</p><p>All of these problems rely on the same set of logical tasks: identifying entities from the KB in text and understanding how they interact. Therefore, it would be beneficial to assume a single model, or a set of models that share parts of the weights and collectively solve all of the tasks. This would allow for the information from a dataset collected for one task (e.g., RC) to be leveraged for improving the performance of another (e.g., SF or cIE).</p><p>Bridging the gap between oIE and cIE. In this work, we only considered triplets for which both entities are element in the entity catalog. However, for many useful relations one of the objects is a literal <ref type="bibr" target="#b38">(Mesquita et al., 2019)</ref>, e.g., date of birth, length, size, number of employees or others. GenIE can be readily extended to accommodate for this, by adapting the decoding strategy allowing that for specific relations the entity can be a substring from the input. This is subtle connection to oIE which has thus far been treated as a separate problem. Current state-of-the-art methods on the oIE task address the problem in a similar autoregressive formulation (see Appendix A for more discussion).</p><p>Real world implications. Generative models have been shown to be very effective even in massive multilingual settings-e.g., <ref type="bibr" target="#b12">De Cao et al. (2022)</ref> proposed mGENRE, a multilingual version of GENRE trained and tested on more than 100 language. Our GenIE formulation would not need substantial modifications to adapt to such setting. Having a single model that works in hundreds of languages would be extremely useful and a very promising direction for future work.</p><p>While autoregressive models have a nonnegligible computation footprint, De <ref type="bibr" target="#b10">Cao et al. (2021a)</ref> show that autoregressive EL can be sped up 70x with no cost on performance. The fact that this solution can be adapted to GenIE makes the practical impact of our method even greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper provides a new view on closed information extraction (cIE) by casting the problem as autoregressive sequence-to-sequence generation. Our method, GenIE, leverages the autoregressive formulation to capture the fine-grained interactions expressed in the text and employs a bi-level constrained generation strategy to effectively retrieve the target representation from a large, structured, compositional predefined space of outputs. Experiments show that GenIE achieves state-of-the-art performance on cIE and can scale to a previously unmanageable number of entities and relations. We believe that our autoregressive formulation of cIE, coupled with constrained decoding, is a stepping stone towards a unified approach for addressing the core tasks in information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Background and Related Work A.1 Generative Open Information Extraction</head><p>Early work had focused on pipeline architecture for oIE. In general, these methods first detect the entity mentions present in the text and then, for pair of entities, in a classification setting, predict the existence of a relation between the two entities and the relation type <ref type="bibr" target="#b1">(Angeli et al., 2015a;</ref><ref type="bibr" target="#b9">Corro and Gemulla, 2013)</ref>. The advent of transformers <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr" target="#b32">Lan et al., 2020;</ref><ref type="bibr" target="#b36">Liu et al., 2019)</ref> and pipeline architectures that allow for information to flow between the two subtasks -usually by sharing some parameters of the encoder -have allowed these models to do well on the oIE task. However, they do come with some general limitations: (i) assuming the existence of a single relation between a pair of entities; (ii) inability to capture the interactions between triplets. Much of the current research is focused on studying the oIE problem in the autoregressive generative setting, which seamlessly mitigates the limitations mentioned above <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021;</ref><ref type="bibr" target="#b14">Dognin et al., 2021;</ref><ref type="bibr" target="#b42">Nayak and Ng, 2020)</ref>. For instance, ReGen <ref type="bibr" target="#b14">(Dognin et al., 2021)</ref> significantly improves upon published results and establishes state-of-the-art results on the dataset used in the WebNLG 2020+ Challenge <ref type="bibr" target="#b6">(Castro Ferreira et al., 2020)</ref>. REBEL <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021)</ref>, on the other hand, achieves state-of-the-art performances across a suite of oIE benchmarks. Moreover, both of these methods address the problem in a similar formulation that takes the text as input context and generates the output triplets tokenby-token in an autoregressive fashion.</p><p>The output triplets in oIE are free-text, while cIE requires the constituent elements of the output triplets to come from the entity and relation sets associated with the KB. This makes the cIE task fundamentally harder than oIE, and renders these methods not applicable to the cIE setting.</p><p>Finally, <ref type="bibr" target="#b51">Taillé et al. (2020)</ref> make an effort to describe the many issues with the evaluation of oIE systems in literature and call for a unified evaluation setting for a fair comparison between systems. These problems get only exacerbated in cIE where the performance of a model would highly depend on the entity and relation catalogue considered. To alleviate some of these issues, we annotate the REBEL dataset <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021)</ref> with unique textual entity identifiers and textual relation labels, and propose a suite of meaningful evaluation settings while considering an approximately 6 million long entity catalogue comprised of all the entities in the English Wikipedia, and 857 long relation catalogue supported by the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>Table <ref type="table" target="#tab_3">3</ref> summarizes the statistics of all datasets used in this work. For each dataset, we remove datapoints containing triplets with entities that do not have an associated Wikipedia page (i.e., entities not associated to a unique name). This filtering removes a negligible portion of the data in most cases (i.e., &lt;0.5%) except for REBEL where 3.4% of datapoints were removed.</p><p>We evaluate the models in a standard setups for Wiki-NRE and Geo-NRE. For these datasets, the schema is unrealistically small: ≈300K entities with 157 relations for Wiki-NRE and 124 entities with 11 relations for Geo-NRE. Therefore, we scale to previously unexplored schema sizes for cIE using the REBEL dataset ( ≈6M entities and 857 relations). We also use FewRel as a high-quality dataset for recall evaluation using the large schema from REBEL.</p><p>REBEL <ref type="bibr" target="#b27">(Huguet Cabot and Navigli, 2021</ref>) is a dataset created from Wikipedia abstracts. It consists of an alignment between sentences, Wikipedia hyperlinks and their corresponding Wikidata entities, and relations. REBEL proposed an alignment expanding on <ref type="bibr" target="#b15">Elsahar et al. (2018)</ref>, a pipeline of mention detection, coreference resolution, entity disambiguation and then mapping triplets to each sentence. Huguet Cabot and Navigli (2021) further filtered false positives using an Natural Language Inference (NLI) model to check if the relation was truly entailed by the text. In this setting, we consider the full ≈6M long entity and 857 long relation catalog. We use this dataset for both training and testing. Additionally, we employ REBEL to analyze the performance as a function of the number of relations, by simulating different environments pertaining to subsets of the top-n most frequent relations.</p><p>Wiki-NRE <ref type="bibr" target="#b54">(Trisedya et al., 2019</ref>) is a dataset created from Wikipedia. Authors aligned hyperlinks to Wikidata entities as in REBEL but they applied a different filtering: they (i) extracted sentences that contain implicit entity names using co-reference resolution <ref type="bibr" target="#b8">(Clark and Manning, 2016)</ref> filtered and assigned relations to sentences using paraphrase detection from different sources <ref type="bibr" target="#b41">(Nakashole et al., 2012;</ref><ref type="bibr" target="#b18">Ganitkevitch et al., 2013;</ref><ref type="bibr" target="#b21">Grycner and Weikum, 2016)</ref>. We used this dataset for both training and testing.</p><p>Geo-NRE <ref type="bibr" target="#b54">(Trisedya et al., 2019)</ref> is constructed in the same way as Wiki-NRE but from a collection of user reviews on 100 popular landmarks in Australia, instead of Wikipedia. Due to its small size and to compare with the literature, we used this dataset only for testing.</p><p>FewRel <ref type="bibr" target="#b24">(Han et al., 2018)</ref> is also extracted from Wikipedia where Wikidata is the KB. Contrary to the other datasets, FewRel does not provide distant supervision but it is fully annotated by humans.</p><p>The dataset was first automatically constructed and then filtered as annotators were asked to judge whether the relations are explicitly expressed in the sentences. Each input in FewRel is associated with a single triplet only, and not all of the triplets entailed by it. Therefore, this dataset can be used for precisely measuring recall (but not precision or F1). We employ it only for testing. To simulate a more realistic scenario, we train the models on many relations, and leverage the high quality FewRel data to calculate the performance metrics for the subset of relations annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Performance Metrics</head><p>We measure standard precision, recall and F1 for all settings. A fact is regarded as correct if the relation and the two corresponding entities are all correct. More precisely, we denote the set of all predicted triplets of a document d ∈ D as P d , and the set of gold triplets as G d . Then:</p><formula xml:id="formula_1">micro-precision = d∈D |P d ∩ G d | d∈D |P d | ,<label>(1)</label></formula><p>and micro-recall</p><formula xml:id="formula_2">= d∈D |P d ∩ G d | d∈D |G d | . (2)</formula><p>Micro scores are useful for measuring the overall performance of a model but they are less informative for imbalanced datasets (e.g., when some entities or relations are disproportionately more present in both training and test sets). Indeed, micro scores assign equal weight to every sample while macro scores assign equal weight to every class. Thus, we also measure macro scores by aggregating per relation type. If we denote P </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Note on End-to-End Baselines</head><p>We invested a considerable amount of time trying to use a strong end-to-end baseline to compare GenIE with. Unfortunately, most works do not have available or directly usable code. In particular, we first concentrated on SetGenNet <ref type="bibr" target="#b46">(Sui et al., 2021)</ref> as, to the best of our knowledge, it is the strongest model on the task of cIE. However, the authors do not report a link to the code<ref type="foot" target="#foot_5">6</ref> in neither the arXiv nor the ACL Antology version of the paper. We could not find any related repository on GitHub either. For these reasons we were unable to use their method as a baseline for REBEL.</p><p>We then focused on the work most similar to Set-GenNet, that is the system proposed by <ref type="bibr" target="#b54">Trisedya et al. (2019)</ref>. They released code and we were able to run it. However, the code was incomplete: they included code for training only a part of their system. They start with pre-trained word, entity and relation embeddings, but did not release code for pre-training them. The closest solution we found was using Wikipedia2Vec <ref type="bibr" target="#b58">(Yamada et al., 2020)</ref>, which does not include relation embeddings. Besides, the pre-trained word embeddings on the official Wikipedia2Vec website<ref type="foot" target="#foot_6">7</ref> do not match the dimensionality used by <ref type="bibr" target="#b54">Trisedya et al. (2019)</ref>. Finally, the authors did not include code to train the "triple classifier" of their model. The classifier is instead directly loaded in their code. For these reasons we were unable to use their method as a baseline for REBEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Implementation Details</head><p>Data. The train, test and validation splits are either inherited from the original dataset (see Appendix B for details) or sampled at random. To facilitate reproducibility, we release the exact splits employed in our experiments.</p><p>Additionally, we release the curated entity and relation catalogs for both the large and the small schema, in which the redirects have been resolved and each of the QID/PID is paired with a unique, semantically meaningful textual identifier. We hope that this will allow for a fair comparison of future work in which the same evaluation setup can be maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 GenIE</head><p>Infrastructure. For training we used a single machine with 24 Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz processor cores and 441 GB of RAM, equipped with 4 Tesla V100-PCIE-16GB GPUs.</p><p>Training. The models were trained using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 3e-5, 0.1 gradient clipping and a varying weight decay (cf. Table <ref type="table" target="#tab_4">4</ref>). The learning rate is updated using a polynomial decay schedule with an end value of 0. While most of the parameters were left at their default values for BART, the rest were tuned on the respective datasets' valida-Inference. At test time, we use Constrained Beam Search with 10 beams. We restrict the input and the output sequence to be at most 256 tokens, cutting from the right side if the input is too long. We normalize the log-probabilities by sequence length, and allow for any number of n-gram repetition. The other parameters are kept to their default values for inference with BART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 SotA Pipeline</head><p>We described our SotA pipeline system baseline in Sec. 4.3. We release code to both train and run inference with the proposed pipeline. The named entity recognition and the entity disambiguation components were not trained. The relation classification module is a linear layer on top of RoBERTa <ref type="bibr" target="#b36">(Liu et al., 2019)</ref>. We trained it learning rate 3e-4 using the Adam optimizer <ref type="bibr" target="#b30">(Kingma and Ba, 2015)</ref>. We trained for a maximum number of steps using early stopping on the validation sets. We restrict the input sequence to be at most 128 tokens cutting from the right side if the input is too long. All other hyperparameters are reported in Table <ref type="table" target="#tab_4">4</ref>. The triple classification module is also a linear layer on top of RoBERTa <ref type="bibr" target="#b36">(Liu et al., 2019)</ref> with the same hyperparameters of the relation classification module but we trained for less steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Analysis of Performance as a Function of the Number of Relations</head><p>Previous works focus on small schemas meaning that few relations were considered. Indeed, classification problems on a large set of possible classes become particularly difficult under large class imbalances, which is the case here as shown by Fig. <ref type="figure" target="#fig_1">2</ref>. However, scaling up to larger schemas with more relations is crucial for the models to be useful in downstream tasks. To measure the scaling ability of GenIE, we create different setups with variable numbers of relations. To create such setups, we start with the REBEL dataset and schema (857 relations) and choose subsets of relations with their associated training data. In Fig. <ref type="figure">4</ref>, we report Ge-nIE and the pipeline baseline F1 for schemas with 100, 400, and 857 relations. To choose a subset of n relations, we take the n most frequent relations to mimic the strategies used by previous works to reduce the schemas <ref type="bibr" target="#b46">(Sui et al., 2021)</ref>. We first observe that GenIE is always largely better than the baseline. The baseline suffers from the same difficulty as previous works; classifying among a large set of relations is hard with large imbalances. GenIE and the baseline have similar absolute decrease in performance when the number of relations increases, corresponding to a more considerable relative decrease for the baseline. More concretely, GenIE's micro F1-score goes from 70.36 % for the top 100 relations, to 68.82 % and 68.93 % for the top 400 and 857 relation setups, respectively. This translates to a relative decrease of 2 % only in the first step. For the baseline, the absolute score of 47.67 % first falls to 44.25 % and subsequently to 42.5 % as the number of relations grows. This in turn, is an overall relative drop of almost 11 %.</p><p>Notably, when looking at precision and recall separately (cf. Table <ref type="table" target="#tab_7">6</ref> in Appendix G), GenIE has a slight proportional decrease of 1-2 absolute points, both in precision and recall, which reflects the increased difficulty of the task due to larger number of relations. The baseline exhibits a similar drop in precision, but a much more significant drop in the recall of almost 10 absolute or 16 point relative. This suggests that the baseline simply ignores most of the relations with lower occurrence counts, which is consistent with the results in Sec. 2, and the hypothesis that the relation classification task is a bottleneck for effectively scaling the baseline system to a large number of relations.</p><p>We already have to deploy several techniques to help the baseline better deal with these issues (see Sec. 4.3), while GenIE, thanks to its generative autoregressive formulation, can effectively scale and manage the inherent imbalances of the task much more naturally.   <ref type="bibr">(2019)</ref>. Encoder-decoder baseline are proposed by the authors and other pipeline baseline include an NER and an ED system AIDA <ref type="bibr" target="#b25">(Hoffart et al., 2011)</ref> or NeuralEL <ref type="bibr" target="#b31">(Kolitsas et al., 2018)</ref> and then a relation extraction system CNN <ref type="bibr" target="#b35">(Lin et al., 2016)</ref>, MiniE <ref type="bibr">(Gashteovski et al., 2017), or ClausIE (Corro and</ref><ref type="bibr" target="#b9">Gemulla, 2013)</ref>. Best results are highlighted in bold and second best are underlined. Our pipeline baseline scores the best or on pair among these other methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiki</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of GenIE. We use a transformer encoder-decoder model that takes unstructured text as input and autoregressively generates a structured semantic representation of the information expressed in it, in the form of (subject, relation, object) triplets. GenIE employs constrained beam search with: (i) a high-level constraint which asserts that the output corresponds to a set of triplets; (ii) lower-level constraints which use prefix tries to force the model to only generate valid entity or relation identifiers (from a predefined schema).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Impact of the number of relation occurrences. Relations are bucketed based on their number of occurrences; bucket 2 i contains relations occurring between 2 i and 2 i+1 times. The histogram shows the number of relations per bucket. The line plots depict the F1 scores of GenIE and the baseline per bucket together with confidence intervals computed per bucket with bootstrap resampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attribution of error to each of the cIE subtasks. The dashed lines equal the overall recall error of the system. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>predicted and gold set only containing the relation r ∈ R of a document d, then macro-precision is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training and validation loss curves for different initialization of our model. GenIE starts from a random initialization, GenIE -PLM fine-tunes a BART pre-trained language model, while GenIE -GENRE is initialized with a pre-trained autoregressive entity linking model by De Cao et al. (2021b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Main results. "R" indicates training on REBEL, and "W" indicates training on Wiki-NRE. .68 43.33 ± 0.63 33.85 ± 0.58 46.96 ± 0.25 GenIE -GENRE 32.02 ± 0.67 39.14 ± 0.68 33.40 ± 0.62 46.63 ± 0.24 GenIE unconstrained 32.25 ± 0.66 27.59 ± 0.53 28.20 ± 0.50 26.14 ± 0.24</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Small Evaluation Schema</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Large Evaluation Schema</cell></row><row><cell></cell><cell></cell><cell>Wiki-NRE</cell><cell></cell><cell></cell><cell>Geo-NRE</cell><cell></cell><cell></cell><cell>REBEL</cell><cell></cell><cell>FewRel</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell></row><row><cell>Micro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SetGenNet (W)</cell><cell cols="6">82.75 ± 0.11 77.55 ± 0.27 80.07 ± 0.27 86.89 ± 0.51 85.31 ± 0.47 86.10 ± 0.34</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SotA Pipeline (W)</cell><cell cols="6">67.43 ± 0.28 54.22 ± 0.21 60.11 ± 0.22 64.60 ± 1.46 64.05 ± 1.46 64.32 ± 1.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SotA Pipeline (R)</cell><cell cols="10">50.78 ± 0.20 62.17 ± 0.24 55.90 ± 0.20 60.28 ± 1.45 60.78 ± 1.49 60.53 ± 1.45 43.30 ± 0.15 41.73 ± 0.13 42.50 ± 0.13 17.89 ± 0.24</cell></row><row><cell cols="7">SotA Pipeline (R+W) 65.17 ± 0.27 54.40 ± 0.20 59.30 ± 0.21 66.65 ± 1.47 66.22 ± 1.46 66.43 ± 1.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GenIE (W)</cell><cell cols="6">88.18 ± 0.13 88.31 ± 0.16 88.24 ± 0.13 86.46 ± 1.05 87.14 ± 1.03 86.80 ± 1.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GenIE (R)</cell><cell cols="10">27.98 ± 0.13 67.16 ± 0.20 39.50 ± 0.14 39.69 ± 1.65 59.01 ± 1.56 47.45 ± 1.62 68.02 ± 0.15 69.87 ± 0.14 68.93 ± 0.12 30.77 ± 0.27</cell></row><row><cell>GenIE (R+W)</cell><cell cols="6">91.39 ± 0.15 91.58 ± 0.14 91.48 ± 0.12 91.77 ± 0.98 93.20 ± 0.83 92.48 ± 0.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Macro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SotA Pipeline (W)</cell><cell cols="6">11.96 ± 0.72 10.73 ± 0.46 10.56 ± 0.43 24.82 ± 3.61 22.54 ± 3.67 20.39 ± 2.72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SotA Pipeline (R)</cell><cell cols="10">19.39 ± 1.18 17.41 ± 0.99 15.93 ± 0.93 28.80 ± 3.86 30.24 ± 4.46 25.24 ± 3.21 12.20 ± 0.35 10.44 ± 0.22 9.48 ± 0.21 19.67 ± 0.26</cell></row><row><cell cols="7">SotA Pipeline (R+W) 24.12 ± 1.46 16.55 ± 1.00 17.76 ± 1.01 38.67 ± 5.72 34.49 ± 5.99 35.14 ± 5.09</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GenIE (W)</cell><cell cols="6">44.22 ± 2.40 36.79 ± 1.62 38.39 ± 1.71 57.13 ± 6.83 52.83 ± 6.84 52.79 ± 6.27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GenIE (R)</cell><cell cols="10">30.63 ± 1.40 41.97 ± 1.92 29.27 ± 1.26 32.38 ± 5.86 40.39 ± 5.17 30.67 ± 5.23 33.90 ± 0.73 30.48 ± 0.65 30.46 ± 0.62 30.78 ± 0.26</cell></row><row><cell>GenIE (R+W)</cell><cell cols="6">52.55 ± 2.12 45.95 ± 1.67 47.08 ± 1.68 75.77 ± 7.80 71.60 ± 7.95 72.59 ± 7.32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>REBEL</cell><cell></cell><cell>FewRel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE</cell><cell cols="4">68.02 ± 0.15 69.87 ± 0.14 68.93 ± 0.12 30.77 ± 0.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE -PLM</cell><cell cols="4">59.32 ± 0.13 77.78 ± 0.12 67.31 ± 0.10 46.95 ± 0.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE -GENRE</cell><cell cols="4">64.14 ± 0.14 76.58 ± 0.11 69.81 ± 0.10 46.62 ± 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">GenIE unconstrained 65.30 ± 0.14 67.12 ± 0.12 66.20 ± 0.11 26.15 ± 0.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Macro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE</cell><cell cols="4">33.90 ± 0.73 30.48 ± 0.65 30.46 ± 0.62 30.78 ± 0.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE -PLM</cell><cell>30.66 ± 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the weights initialization and the constrained generation strategy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we create buckets i ∈ {0, . . . , 20}, where bucket i contains all the relations occurring at least 2 i times and less than 2 i+1 times in the REBEL dataset. The height of the histogram for bucket i shows how many relations are contained in this bucket. Finally, we report the F1 scores of GenIE and the pipeline of SotA components per bucket. Note that micro F1 from Table1is equivalent to putting all relations in one single bucket (equal weight to each data point), and macro F1 is equivalent to averaging the F1 with one bucket per relation (equal weight to each relation).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>, and (ii) they Statistics of the datasets. † With an abuse of notation here we indicate the amount of unique entities and relations for each dataset and not the size of the Knowledge Base associated with it (see Section 4 for more details).Note that we do not use the validation FewRel data in our experiment, but we release this split as well.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Documents</cell><cell></cell><cell></cell><cell>Triplets</cell><cell></cell><cell cols="2">|E |  † |R|  †</cell></row><row><cell></cell><cell cols="2">training validation</cell><cell>test</cell><cell cols="2">training validation</cell><cell>test</cell><cell></cell><cell></cell></row><row><cell>REBEL</cell><cell>1,899,331</cell><cell cols="3">104,960 105,516 5,147,836</cell><cell cols="4">284,268 284,936 1,498,143 857</cell></row><row><cell>Wiki-NRE</cell><cell>223,536</cell><cell cols="2">980 29,619</cell><cell>298,489</cell><cell cols="2">1,317 39,678</cell><cell cols="2">278,204 157</cell></row><row><cell>Geo-NRE</cell><cell>-</cell><cell>-</cell><cell>1,000</cell><cell>-</cell><cell>-</cell><cell>1,000</cell><cell>124</cell><cell>11</cell></row><row><cell>FewRel</cell><cell>-</cell><cell>26,892</cell><cell>27,650</cell><cell>-</cell><cell>26,892</cell><cell>27,650</cell><cell>64,762</cell><cell>80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for the different models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Max steps Warm-up steps Batch size Dropout Weight decay</cell><cell>Training time</cell></row><row><cell cols="2">GenIE (W)</cell><cell></cell><cell>60,000</cell><cell>1,000</cell><cell>32</cell><cell>0.1</cell><cell>0.01</cell><cell>0.5 GPU days</cell></row><row><cell cols="2">GenIE (R)</cell><cell></cell><cell>100,000</cell><cell>5,000</cell><cell>384</cell><cell>0.1</cell><cell>0.01</cell><cell>18.5 GPU days</cell></row><row><cell cols="2">GenIE (R + W)</cell><cell></cell><cell>100,000</cell><cell>5,000</cell><cell>384</cell><cell>0.1</cell><cell>0.01</cell><cell>20.5 GPU days</cell></row><row><cell cols="2">GenIE -Genre</cell><cell></cell><cell>50,000</cell><cell>3,000</cell><cell>2,048</cell><cell>0.3</cell><cell>0.50</cell><cell>11 GPU days</cell></row><row><cell cols="2">GenIE -PLM</cell><cell></cell><cell>50,000</cell><cell>3,000</cell><cell>2,048</cell><cell>0.3</cell><cell>0.50</cell><cell>17 GPU days</cell></row><row><cell cols="3">SoTA Rel-class (W)</cell><cell>20,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>0.2 GPU days</cell></row><row><cell cols="3">SoTA Rel-class (R)</cell><cell>250,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>2.5 GPU days</cell></row><row><cell cols="3">SoTA Rel-class (R + W)</cell><cell>250,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>2.5 GPU days</cell></row><row><cell cols="3">SoTA Tri-class (R)</cell><cell>50,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>0.3 GPU days</cell></row><row><cell cols="3">SoTA Tri-class (W)</cell><cell>5,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1 GPU days</cell></row><row><cell cols="3">SoTA Tri-class (R + W)</cell><cell>50,000</cell><cell>500</cell><cell>128</cell><cell>0.1</cell><cell>0.01</cell><cell>0.3 GPU days</cell></row><row><cell>0.8 1.0</cell><cell></cell><cell></cell><cell cols="2">GenIE (REBEL) Micro GenIE (REBEL) Macro SotA Pipeline (REBEL) Micro SotA Pipeline (REBEL) Macro</cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>100</cell><cell cols="2">400 Number of relations considered</cell><cell>857</cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 4: Impact of the number of relations in the</cell><cell></cell><cell></cell></row><row><cell cols="5">schema on REBEL. Micro and macro F1 of both Ge-</cell><cell></cell><cell></cell></row><row><cell cols="5">nIE and the pipeline of SotA components for 3 schema</cell><cell></cell><cell></cell></row><row><cell cols="5">sizes: 100, 400, and 857 relations. The schema is con-</cell><cell></cell><cell></cell></row><row><cell cols="5">strained at both training and testing time. Full results</cell><cell></cell><cell></cell></row><row><cell cols="5">(i.e., precision and recall) are reported in Table 6 in Ap-</cell><cell></cell><cell></cell></row><row><cell cols="2">pendix G.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Baselines comparison. All results are taken from from Trisedya et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>± 0.12 72.05 ± 0.13 70.36 ± 0.10 67.10 ± 0.13 70.62 ± 0.15 68.82 ± 0.12 68.02 ± 0.15 69.87 ± 0.14 68.93 ± 0.12 SotA Pipeline 44.76 ± 0.17 50.99 ± 0.17 47.67 ± 0.16 38.98 ± 0.13 51.18 ± 0.12 44.25 ± 0.11 43.30 ± 0.15 41.73 ± 0.13 42.50 ± 0.13MicroGenIE 52.26 ± 0.25 54.13 ± 0.27 52.75 ± 0.24 41.50 ± 0.66 38.53 ± 0.57 38.12 ± 0.51 33.90 ± 0.73 30.48 ± 0.65 30.46 ± 0.62 SotA Pipeline 27.41 ± 0.27 31.05 ± 0.18 25.87 ± 0.15 16.94 ± 0.63 19.00 ± 0.36 14.73 ± 0.37 12.20 ± 0.35 10.44 ± 0.22 9.48 ± 0.21 Impact of the number of relations in the schema on REBEL. The schema is constrained at both training and testing time.</figDesc><table><row><cell></cell><cell cols="3">REBEL (top 100 Relations)</cell><cell cols="3">REBEL (top 400 Relations)</cell><cell cols="3">REBEL (857 Relations)</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Micro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GenIE</cell><cell>68.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that current methods, due to the atomic classification, have high memory requirements, and suffer from performance deterioration as the number of entities or/and relations grows.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Dumps from 2019/08/01</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/flairNLP/flair</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/facebookresearch/ GENRE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">See notes on reproducibility in Appendix D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">As of October 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://wikipedia2vec.github.io/ wikipedia2vec/pretrained tion set, and their corresponding optimal values are given in Table4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Nicola De Cao is supported by SAP Innovation Center Network. Authors want to thank Ivan Titov and Wilker Aziz for insightful discussions and help. With support from Swiss National Science Foundation (grant 200021_185043), European Union (TAILOR, grant 952215), and gifts from Microsoft, Facebook, Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLAIR: An easy-to-use framework for state-of-theart NLP</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><forename type="middle">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bootstrapped self training for knowledge base population</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Tejasvi Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Text Analysis Conference</title>
				<meeting>the 2015 Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-rank subspaces for unsupervised entity linking</title>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.634</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8037" to="8054" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence, IJCAI&apos;07</title>
				<meeting>the 20th International Joint Conference on Artificial Intelligence, IJCAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OpenIE for Slot Filling at TAC KBP 2017 -System Description</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Achenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Text Analysis Conference</title>
				<meeting>the Text Analysis Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="55" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Importance sampling for unbiased on-demand evaluation of knowledge base population</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1038" to="1048" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2256" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ClausIE: clause-based open information extraction</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488388.2488420</idno>
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference, WWW &apos;13</title>
				<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-13">2013. May 13-17, 2013</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee / ACM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Highly parallel autoregressive entity linking with discriminative correction</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="7662" to="7669" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021b. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multilingual Autoregressive Entity Linking</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Plekhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00460</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="274" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ReGen: Reinforcement learning for text and knowledge base generation using pretrained language models</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1084" to="1099" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Canonicalizing open knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661829.2662073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-11-03">2014. 2014. November 3-7, 2014</date>
			<biblScope unit="page" from="1679" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName><forename type="first">Octavian-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MinIE: Minimizing facts in open information extraction</title>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corro</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On aligning OpenIE extractions with knowledge bases: A case study</title>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.eval4nlp-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</title>
				<meeting>the First Workshop on Evaluation and Comparison of NLP Systems</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">POLY: Mining relational paraphrases from multilingual sentences</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2183" to="2192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>UK. Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<author>
			<persName><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno>abs/1504.07678</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">REBEL: Relation extraction by end-to-end language generation</title>
		<author>
			<persName><forename type="first">Pere-Lluís</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cabot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
				<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2370" to="2381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2021.3070843</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
				<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end neural entity linking</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
				<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="1907">2019. 1907.11692</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Seq2RDF: An Endto-end Application for Deriving Triples from Natural Language Text</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISWC 2018 Posters &amp; Demonstrations, Industry and Blue Sky Ideas Tracks co-located with 17th International Semantic Web Conference (ISWC 2018)</title>
				<meeting>the ISWC 2018 Posters &amp; Demonstrations, Industry and Blue Sky Ideas Tracks co-located with 17th International Semantic Web Conference (ISWC 2018)<address><addrLine>Monterey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-08">2018. October 8th -to -12th. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">KnowledgeNet: A benchmark dataset for knowledge base population</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1069</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
	<note>Paramita Mirza, and Denilson Barbosa</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Algorithms that learn to extract information BBN: TIPSTER phase III</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Crystal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidi</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<idno type="DOI">10.3115/1119089.1119107</idno>
	</analytic>
	<monogr>
		<title level="m">TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop</title>
				<meeting><address><addrLine>Baltimore, Maryland; Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10-13">1998. October 13-15, 1998</date>
			<biblScope unit="page" from="75" to="89" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
				<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PATTY: A taxonomy of relational patterns with semantic types</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2523" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Set generation networks for end-to-end knowledge base population</title>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Overview of the tac2013 knowledge base population evaluation: English slot filling and temporal slot filling. Theory and Applications of Categories</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML 2011</title>
				<meeting>the 28th International Conference on Machine Learning, ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011-06-28">2011. June 28 -July 2, 2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. 2014. December 8-13 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
				<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Let&apos;s Stop Incorrect Comparisons in End-to-end Relation Extraction!</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Taillé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3689" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to update knowledge graphs by reading news</title>
		<author>
			<persName><forename type="first">Jizhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1265</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2632" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
				<imprint>
			<date type="published" when="2002">2002. 2002. CoNLL-2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural relation extraction for knowledge base enrichment</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wikidata: A new platform for collaborative data collection</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<idno type="DOI">10.1145/2187980.2188242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web, WWW &apos;12 Companion</title>
				<meeting>the 21st International Conference on World Wide Web, WWW &apos;12 Companion<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
