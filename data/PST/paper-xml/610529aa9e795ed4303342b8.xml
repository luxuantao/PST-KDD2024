<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ye</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xi&apos;an Jiaotong University 4 Shanghai Tree-Graph Blockchain Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangbin</forename><surname>Dong</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">International Digital Economy Academy at Guangdong-Hong Kong-Macau Greater</orgName>
								<address>
									<settlement>Bay Area 8</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingzhong</forename><surname>Mao</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">International Digital Economy Academy at Guangdong-Hong Kong-Macau Greater</orgName>
								<address>
									<settlement>Bay Area 8</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Long</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institute for Interdisciplinary Information Core Technology</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">International Digital Economy Academy at Guangdong-Hong Kong-Macau Greater</orgName>
								<address>
									<settlement>Bay Area 8</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
							<email>gsun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ISCA52012.2021.00040</idno>
					<note type="submission">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:12:17 UTC from IEEE Xplore. Restrictions apply.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-knowledge proof (ZKP) is a promising cryptographic protocol for both computation integrity and privacy. It can be used in many privacy-preserving applications including verifiable cloud outsourcing and blockchains. The major obstacle of using ZKP in practice is its time-consuming step for proof generation, which consists of large-size polynomial computations and multi-scalar multiplications on elliptic curves. To efficiently and practically support ZKP in real-world applications, we propose PipeZK, a pipelined accelerator with two subsystems to handle the aforementioned two intensive compute tasks, respectively. The first subsystem uses a novel dataflow to decompose large kernels into smaller ones that execute on bandwidth-efficient hardware modules, with optimized off-chip memory accesses and on-chip compute resources. The second subsystem adopts a lightweight dynamic work dispatch mechanism to share the heavy processing units, with minimized resource underutilization and load imbalance. When evaluated in 28 nm, PipeZK can achieve 10x speedup on standard cryptographic benchmarks, and 5x on a widely-used cryptocurrency application, Zcash.</p><p>* Mingyu Gao and Guangyu Sun are the co-corresponding authors.</p><p>as a promising use case of ZKP, allows a weak client to outsource computations to the powerful cloud, and also efficiently verify the correctness of the returned results [52], <ref type="bibr" target="#b52">[53]</ref>. Another widely deployed application of ZKP is blockchains and cryptocurrencies. With ZKP, the intensive computations can be moved off-chain and each node only needs to verify the integrity of a much more lightweight proof on the critical path [1], <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>Since its birth <ref type="bibr" target="#b29">[30]</ref>, tremendous effort has been made by cryptography researchers to make ZKP more practical. Among newly invented ones, zk-SNARK, which stands for Zero-Knowledge Succinct Non-Interactive Argument of Knowledge, is widely considered as a promising candidate. As its name suggests, zk-SNARK generates succinct proofs -often within hundreds of bytes regardless of the complexity of the program, and these proofs are very fast to verify. Because of these two properties, we are seeing more and more deployments of zk-SNARK in real-world applications, especially in the blockchain community.</p><p>Although zk-SNARK proofs are succinct and fast to verify, their generation remains an obstacle in large-scale zk-SNARK adoption. To generate proofs for a program, it is typical to first translate the program into a constraint system, the size of which is usually several times larger than the initial program, and could be up to a few millions. The prover then performs a number of arithmetic operations over a large finite field. The actual number of operations required is protocolspecific, but is always super-linear comparing to the number of constraints, hence even larger. As a result, it takes much longer to generate the zk-SNARK proof of a program than verifying it, sometimes up to hundreds of times, and could be up to a few minutes just for a single payment transaction <ref type="bibr" target="#b46">[47]</ref>.</p><p>In this paper, we present PipeZK, an efficient pipelined architecture for accelerating zk-SNARK. PipeZK mainly involves two subsystems, for the polynomial computations with large-size number theoretic transforms (NTTs), and for the multi-scalar multiplications that execute vector inner products on elliptic curves (ECs). These two phases are the most 416</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Zero-knowledge proof (ZKP) blossoms rapidly in recent years, drawing attentions from both researchers and practitioners. In short, it is a family of cryptographical protocols that allow one party (called the prover) to convince the others (called the verifiers) that a computational statement is true, without leaking any information. For example, if a program P outputs the result y on a public input x and a secret input w, using a ZKP protocol, the prover can assure the verifiers that she knows the secret w that satisfies P (x, w) = y without revealing the value of w.</p><p>As a fundamental primitive in modern cryptography, ZKP has the potential to be widely used in many privacy-critical applications to enable secure and verifiable data processing, including electronic voting <ref type="bibr" target="#b53">[54]</ref>, online auction <ref type="bibr" target="#b25">[26]</ref>, anonymous credentials <ref type="bibr" target="#b22">[23]</ref>, verifiable database outsourcing <ref type="bibr" target="#b51">[52]</ref>, verifiable machine learning <ref type="bibr" target="#b50">[51]</ref>, privacy-preserving cryptocurrencies <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and various smart contracts on blockchains <ref type="bibr" target="#b36">[37]</ref>. More specifically, verifiable outsourcing, compute-intensive parts. We implement them as specialized hardware accelerators, and combine with the CPU to realize a heterogeneous end-to-end system for zk-SNARK.</p><p>For the polynomial computation subsystem, we notice that the large-size NTTs (up to a million elements) result in sigificant challenges for both off-chip memory accesses and on-chip compute resources, due to the irregular strided access patterns similar to classical FFTs, and the large bitwidth (up to 768 bits) of each element. We propose a novel high-level dataflow that recursively decomposes the large NTT kernels into smaller ones, which can then be efficiently executed on a bandwidth-efficient NTT hardware module that uses lightweight FIFOs internally to realize the strided accesses. We also leverage data tiling and on-chip matrix transpose to improve off-chip bandwidth utilization.</p><p>For the multi-scalar multiplication subsystem, rather than simply replicating multiple processing units for EC operations, we exploit the large numbers of EC multiplications in the vector inner products, and use Pippenger algorithm <ref type="bibr" target="#b42">[43]</ref> to share the dominant EC processing units with a lightweight dynamic work dispatch mechanism. This alleviates the resource underutilization and load imbalance issues when the input data have unpredictable value distributions. Furthermore, we scale the system in a coarse-grained manner to allow each processing unit to work independently from each other, while guaranteeing that there are no stragglers even when data distributions are highly pathological.</p><p>In summary, our contributions in this paper include:</p><p>• We designed a novel module, which decomposes a largescale polynomial computation into small tiles and processes them in a pipeline style. It achieves high efficiency in both off-chip memory bandwidth and on-chip logic resource utilization. • We designed a novel module for multi-scalar point multiplications on elliptic curves. It leverages an optimized algorithm and a pipelined dataflow to achieve high processing throughput. • We implemented a prototype of the proposed architecture in RTL and synthesized our design as a 28 nm ASIC, and evaluated it in an end-to-end heterogeneous system with a host CPU. Compared to state-of-the-art approaches, the overall system can achieve 10x speedup for small-size standard cryptographic benchmarks on average, and 5x for a real-world large-scale application, Zcash <ref type="bibr" target="#b46">[47]</ref>. When individually executed, the two subsystems of PipeZK can achieve 197x to 77x speedup, respectively.</p><p>Beyond our accelerator design, both subsystems in PipeZK could be of independent interest to a wider range of applications. The NTT module is the key building block in homomorphic encryption (HE) <ref type="bibr" target="#b28">[29]</ref> and modern public-key encryption schemes <ref type="bibr" target="#b37">[38]</ref> based on Ring Learning With Errors (R-LWE) problems <ref type="bibr" target="#b43">[44]</ref>. The multi-scalar multiplication module is commonly used in vector commitments <ref type="bibr" target="#b18">[19]</ref> and many pairing-based proof systems <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b41">[42]</ref>. We expect our architecture insights would inspire more opportunities in making modern cryptographic algorithms more practical to use towards general-purpose secure computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>Zero-knowledge proof (ZKP) is a powerful cryptographic primitive that has recently been adopted to many real-world applications <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b53">[54]</ref>, and drawn a lot of attentions in both academia <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref> and industry <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b38">[39]</ref>. ZKP allows the prover to prove to the verifier that a given statement of the following form is true: "given a function F and an input x, I know a secret witness w that makes F (x, w) = 0." More specifically, the prover can generate a proof, whose validity can be checked by the verifier. However, even though the verifier gets the proof and is able to verify its validity, she cannot obtain any information about w itself. The prover's secret remains secure after the proving process. As a result, the zero-knowledge property of ZKP provides a strong guarantee for the prover's privacy, as she can prove to others that she knows some private information (i.e., w) without leaking it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applications of Zero-Knowledge Proof</head><p>As one of the fundamental primitives in modern cryptography, ZKP can be widely used in many security applications as a basic building block to enable real-world secure and verifiable data processing. Generally speaking, ZKP allows two or multiple parties to perform compute tasks in a cooperative but secure manner, in the sense that one party can convince the others that her result is valid without accidentally leaking any sensitive information. Many real-world applications can benefit from these properties, including electronic voting <ref type="bibr" target="#b53">[54]</ref>, online auction <ref type="bibr" target="#b25">[26]</ref>, anonymous credentials <ref type="bibr" target="#b22">[23]</ref>, verifiable database outsourcing <ref type="bibr" target="#b51">[52]</ref>, verifiable machine learning <ref type="bibr" target="#b50">[51]</ref>, privacy-preserving cryptocurrencies <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and various smart contracts on blockchains <ref type="bibr" target="#b36">[37]</ref>.</p><p>A promising example application of ZKP is verifiable outsourcing <ref type="bibr" target="#b26">[27]</ref>, in which case a client with only weak compute power outsources a compute task to a powerful server, e.g., a cloud datacenter, who computes on potentially sensitive data to generate a result that is returned to the client. Examples include database SQL queries <ref type="bibr" target="#b51">[52]</ref> and machine learning jobs <ref type="bibr" target="#b50">[51]</ref>. In such a scenario, the client would like to ensure the result is indeed correct, while the server is not willing to expose any sensitive data. ZKP allows the server to also provide a proof associated with the result, which the client can use to efficiently check the integrity. The zero-knowledge property allows the prover to make arbitrary statements about (i.e., to compute functions on) the sensitive data without worrying about exposing them, therefore naturally supporting theoretically general-purpose outsourcing computations.</p><p>Another widely deployed application of ZKP is blockchains and cryptocurrencies. Conventional blockchain-based applications require every node in the system to execute the same on-chain computations to update the states, which brings a large overhead with long latency. ZKP enables private decentralized verifiable computations which are moved offchain, and each node only checks the integrity of a lightweight proof to discover illegal state transitions. For instance, zk-Rollup <ref type="bibr" target="#b0">[1]</ref> packs many transactions in one proof and allows the nodes to check their integrity by efficiently verifying the proof. Other work even enables verifying the integrity of the whole blockchain using one succinct proof <ref type="bibr" target="#b38">[39]</ref>. This feature greatly increases the blockchain scalability. Furthermore, the zero-knowledge property allows users to make confidential transactions while still being able to prove the validity of each transaction. Zcash <ref type="bibr" target="#b46">[47]</ref> and Pinocchio Coin <ref type="bibr" target="#b21">[22]</ref> are such examples, where the transaction details including the amount of money and the user addresses are hidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computation Requirements of Zero-Knowledge Proof</head><p>It is natural to imagine that realizing such a counter-intuitive ZKP functionality would require huge computation and communication costs. Since its first introduction by Goldwasser et al. <ref type="bibr" target="#b29">[30]</ref>, there have been significant improvements in the computation efficiency of ZKP to make it more practical. zk-SNARK <ref type="bibr" target="#b31">[32]</ref>, as the state-of-the-art ZKP protocol, allows the prover to generate a succinct proof, which greatly reduces the verification cost. Formally speaking, the proof of zk-SNARK has three important properties: correctness, zero-knowledge, and succinctness. Correctness means that if the verification passes, then the prover's statement is true, i.e., the prover does know the secret witness w. Zero-knowledge means that the proof does not leak any information about w. And succinctness means that the size of the proof is small (e.g., 128 bytes) and it is also fast to verify (e.g., within 2 milliseconds), regardless of how complicated the original statement might be.</p><p>Unfortunately, although the proof verification is fast, generating such a proof at the prover side with zk-SNARK has considerable computation overheads and can take a great amount of time, which hinders zk-SNARK from wide adoption in real-world applications. Therefore, this work focuses on the workflow and the key components of the prover's computation <ref type="bibr" target="#b31">[32]</ref>, which is our target for hardware acceleration.</p><p>For a specific implementation of zk-SNARK, a security parameter λ is first decided to trade off the computation complexity and the security strength, by specifying the data width used. A larger λ provides stronger security guarantees but also introduces significantly higher computation cost. Typically, λ ranges from 256-bit to 768-bit. <ref type="foot" target="#foot_1">1</ref>As illustrated in Figure <ref type="figure">1</ref>, the prover first goes through a pre-processing phase, during which the function F , typically written in some high-level programming languages, is first compiled into a set of arithmetic constraints, called "rank-1 constraint system (R1CS)". The constraint system contains a number of linear or polynomial equations of the input x and the witness w. Determined by the complexity of the function F , the number of equations in the constraint system could be as many as up to millions for real-world applications. Meanwhile, various random parameters are set up, including the proving keys. With the prover's secret witness, the constraint system, the proving keys, and other parameters, the pre-processing phase subsequently outputs two sets of data (Figure <ref type="figure">1</ref>), which are later used in the computation phase.</p><p>• Scalar vectors S n , A n , B n , C n . Each vector includes n λ-bit numbers. The dimension n is determined by the size of the constraint system. Note that n could be extremely large for real-world applications. For example, Zcash has n as large as a few millions <ref type="bibr" target="#b34">[35]</ref>. • Point vectors P n , Q n . Each vector includes n points on a pre-determined elliptic curve (EC) <ref type="bibr" target="#b32">[33]</ref>. EC is commonly used in cryptographic primitives. It supports several basic operations including point addition (PADD), point double (PDBL) and point scalar multiplication (PMULT). By leveraging the binary representation of the scalar, PMULT can be broken down into a series of PADD and PDBL in the scalar's bit-serial order. Both PADD and PDBL operations contain a bunch of arithmetic operations over a large finite field, as shown in Figure <ref type="figure">2</ref>. Fast algorithms for EC operations typically use projective coordinates to avoid modular inverse <ref type="bibr" target="#b12">[13]</ref>. They also adopt Montgomery representations for basic arithmetic operations over the finite field <ref type="bibr" target="#b39">[40]</ref>.</p><p>With these data, the prover can now generate the proof Π. This is the most computation-heavy phase, and therefore is our main target for hardware acceleration. It involves largesize number theoretic transforms (NTTs) and complicated EC operations, as illustrated in Figure <ref type="figure">2</ref>. More specifically, the computation phase mainly includes the following two tasks:  <ref type="figure">2</ref>.</p><p>And each NTT/INTT also has considerable computation cost, given that n could be quite large (up to millions) and each coefficient is a very wide integer number (e.g., λ = 768 bits). • Multi-scalar multiplication (MSM). This part includes the calculation of the "vector inner products" between S n and Q n , and between H n (the output of POLY) and P n , respectively. Note that the inner products are performed on EC, i.e., using the PADD and PMULT operations defined above to multiply the scalar vector and the point vector together. MSM is computation-intensive, because the cost of the inner products is proportional to n, and the PADD/PMULT operations on EC are also quite expensive, with arithmetic operations between wide integer numbers on a large finite field. As Figure <ref type="figure">1</ref> shows, the prover's witness, after pre-processed, is used as the input for both POLY and MSM. The output of POLY will be included in the input of MSM. The final proof is the output of MSM composed of several EC points. The proof can be verified by the verifier within a few milliseconds through pairing, a special operation on the EC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware Acceleration Opportunities</head><p>As we can see from the workflow in Section II-B, the prover's computations are particularly complicated and require significant compute time. In Zcash <ref type="bibr" target="#b34">[35]</ref>, the size n of the constraint system is about two million. It takes over 30 seconds to generate a proof for each anonymous transaction. As a result, ordinary users sometimes prefer sending transparent transactions instead to avoid the high cost of generating proofs, which trades off privacy for better performance. In Filecoin <ref type="bibr" target="#b23">[24]</ref>, the function F is even larger. It contains over 128 million constraints and requires an hour to generate a proof. Actually, these blockchain applications usually use cryptofriendly functions that have well-crafted arithmetic computation flows, which are easier to transfer into smaller constraint systems. For real-world, general-purpose applications such as those in Section II-A, the problem sizes will be even larger, with extremely high computation overheads. This is the primary reason that hinders the wide adoption of ZKP. It is therefore necessary to consider hardware acceleration for ZKP workloads, especially on the prover side.</p><p>In the proving process, the pre-processing typically takes less than 5% time <ref type="bibr" target="#b7">[8]</ref>. We hence focus mostly on the POLY and MSM computations. The POLY part takes about 30% of the proving time. As shown in Figure <ref type="figure">2</ref>, it mostly invokes the NTT/INTT modules for seven times. Other computations like multiplications and subtractions only contribute less than 2% time. These large-size NTTs are extremely expensive. Similar to FFTs, NTTs have complicated memory access patterns with different strides in each stage. Moreover, all the arithmetic operations (multiplications, exponentiations, etc.) inside NTTs are performed over a large finite field, making them also compute-intensive. Thus, the main focus of hardware acceleration in POLY is the large-size NTTs/INTTs (Section III).</p><p>The MSM part takes about 70% of the proving time, which makes it the most computation-intensive part in proving. It requires many expensive PMULT operations on EC. Though several previous proposals have accelerated a single PMULT <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>, MSM additionally requires adding up the PMULT result points, i.e., an inner product. This brings the opportunity to use more efficient algorithms rather than simply duplicating multiple PMULT units. Also, in zk-SNARK, the scalar vectors exhibit certain distributions that we can take advantage of to improve performance. We propose a new hardware framework for MSM which can make full use of the hardware resources (Section IV).</p><p>Why not just CPUs/GPUs? The basic operations of both POLY and MSM are arithmetics over large finite fields, which are not friendly to traditional general-purpose computing platforms like CPUs and GPUs. CPUs have insufficient computation throughput and they cannot exploit the parallelism inside these operations well enough. GPUs, on the other hand, have high computation throughput but mostly for floatingpoint numbers. Moreover, the memory architecture of modern GPUs is also not efficient for POLY and MSM operations. Each thread can only access a very limited software cache (i.e., shared memory) and the irregular global memory access patterns in each component will slow down the operations in GPUs significantly. In contrast, large integer arithmetic operations have been well studied in specialized circuit design. It is also more flexible to generate customized designs for different memory access patterns. Thus, a domain-specific accelerator is more promising to achieve better performance and energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Prior Work</head><p>Prior work has achieved significant performance improvement for polynomial computations in homomorphic encryption using customized hardware <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Accelerating EC operations has also been well studied in the literature of circuit design <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>. However, it is inefficient to directly employ the prior designs for zk-SNARK due to two issues. First, the scale of polynomial computations in zk-SNARK is much larger than those needed in homomorphic encryption. Thus, it induces intensive off-chip memory accesses, which cannot be satisfied in prior design. In addition, the data bitwidth in zk-SNARK is much larger, thus it is inefficient to use large-scale multiplexers to select proper input elements for different butterfly operations like before <ref type="bibr" target="#b44">[45]</ref>. Second, directly duplicating EC hardware cannot leverage state-ofthe-art algorithm optimizations for zk-SNARK. Besides, the sparsity in scalars may cause a lot of resource underutilization in the pipelines that compute MSM. Detailed discussions are in Section III and Section IV.</p><p>A recent work called DIZK has proposed to leverage Apache Spark for distributing the prover's computation to multiple machines <ref type="bibr" target="#b49">[50]</ref>. Though it can reduce the latency for the proving process, the primary goal for DIZK is supporting zk-SNARK for super large-scale applications, such as machine learning models. Large cloud computing is inefficient for ordinary-size applications like anonymous payment and privacy-preserving smart contracts due to network latency and computation cost. Therefore DIZK can be regarded as a complementary work to ours, while our design achieves better efficiency for each distributed machine.</p><p>Recently, a few approaches in industry try to accelerate the prover with dedicated hardware (GPU <ref type="bibr" target="#b10">[11]</ref> or FPGA <ref type="bibr" target="#b4">[5]</ref>) by leveraging the parallelism inside zk-SNARK. For example, Coda held a global competition for accelerating the proving process using GPU with high rewards ($100k) <ref type="bibr" target="#b10">[11]</ref>. However, the final acceleration result of the competition is even worse than our CPU benchmark (See Section VI for more details). And the FPGA one does not contain a complete end-to-end implementation <ref type="bibr" target="#b4">[5]</ref>. In summary, there is still a considerable gap between the existing performance and the requirement in practical usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ACCELERATING POLYNOMIAL COMPUTATION</head><p>The POLY part of zk-SNARK mainly consists of multiple NTTs and INTTs. To overcome the design challenges of largesize NTTs, we introduce a recursive NTT algorithm with an optimized overall dataflow. We also design efficient hardware NTT modules to alleviate the off-chip bandwidth and on-chip resource requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NTT Computations</head><p>The NTT computation â def = NTT(a) is defined on two Nsize arrays a and â, with their elements â</p><formula xml:id="formula_0">[i] = N −1 j=0 a[j]ω ij N .</formula><p>Here a[j] and â[i] are λ-bit scalars in a finite field. And ω N is the N th root of unity in the same field. All possible exponents of ω N are called twiddle factors, which are constant values for a specific size of N . Since we use off-chip memory to store them, we assume all twiddle factors for all possible N s are precomputed. This may only introduce tens of MB storage for N up to several millions. Typical implementations of NTT utilize the property of the twiddle factors to compute the results recursively. The access patterns are similar to the standard FFT algorithms, as shown in Figure <ref type="figure">3</ref>. In this example, the NTT size is N = 2 n . In stage i, two elements with a fixed stride 2 n−i perform a butterfly operation and output two elements to the next stage. The overall NTT computations complete in n stages. The different strides in different stages result in</p><formula xml:id="formula_1">[0] Stage 1 Stage 2 Stage 3 [1] [2] [3] [4] [5] [6] [7] [0] [4] [2] [6] [1] [5] [3] [7]</formula><p>Fig. <ref type="figure">3</ref>. The data access pattern of NTT (similar to FFT) with size 2 3 = 8. complicated data access patterns, which makes it challenging to design an efficient hardware accelerator. As shown in Figure <ref type="figure">3</ref>, the output elements on the right side are out-of-order and need to be reordered through an operation called bit-reverse. Alternatively, we can reorder the input elements and generate the outputs in order <ref type="bibr" target="#b20">[21]</ref>. If we need to perform multiple NTTs in a sequence, it is possible to properly chain the two styles alternately and eliminate the need for the bit-reverse operations in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design Challenges</head><p>NTT is an important kernel commonly used in cryptography. As a result, there exist many hardware accelerator designs for NTT. One state-of-the-art NTT hardware design is HEAX <ref type="bibr" target="#b44">[45]</ref>, which is specialized for homomorphic encryption. However, the POLY computations in zk-SNARK have substantially larger scales than those addressed in HEAX. It requires multiple NTTs of up to a few million elements, with the data width normally more than 256-bit. Such large sizes can hardly be satisfied by any previous NTT hardware design and pose new challenges that must be properly addressed.</p><p>First, the total size of zk-SNARK NTT data can be too large to keep on-chip and should be stored in off-chip memory. For example, a million-size NTT with 256-bit data width will need over 64 MB data storage for the input data and the twiddle factors. If we need to access 1024 elements in each cycle from the off-chip memory to feed a 1024-size NTT module, the accelerator has to support at least 2.98 TB/s bandwidth, even with a relatively low 100 MHz frequency. This is unrealistically high in existing systems, let alone that the complicated stride accesses may further reduce the effective bandwidth. Therefore, it is critical to optimize the off-chip data access patterns of the NTT hardware modules to minimize bandwidth requirements and balance between computations and data transfers. In contrast, prior work like HEAX assumes data can be buffered on-chip in most cases and does not specially design for off-chip data accesses <ref type="bibr" target="#b44">[45]</ref>.</p><p>Second, the large bitwidth of NTT elements also requires significant on-chip resources on the computation side. The original HEAX design only works with data no wider than 54bit. It, therefore, adopts an approach that uses a set of on-chip multiplexers before the computation units to choose the correct input elements for each butterfly operation <ref type="bibr" target="#b44">[45]</ref>. If we naively Step(1)</p><p>Step(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>Step scale up the bitwidth beyond 256 as required in zk-SNARK, the area and energy overheads of such multiplexers will increase significantly. Furthermore, the required computation resources for the butterfly operation itself in the NTT module also scale in a super-linear fashion. Both make it inefficient to support large NTTs with high throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recursive NTT Algorithm</head><p>To overcome the above challenges, we adopt a parallel NTT algorithm from <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b48">[49]</ref> to recursively decompose a large NTT of arbitrary size (e.g., 1 million) into multiple smaller NTT kernels (e.g., 1024). This allows us to only implement smaller NTT modules, which can fit into the onchip compute resources and also satisfy the off-chip bandwidth limitation. We then iteratively use the smaller NTT modules to calculate the original large NTT. The hardware NTT module in Section III-D can work with different NTT kernel sizes, therefore supporting flexible decomposition.</p><p>We give a high-level overview of the algorithm as shown in Figure <ref type="figure" target="#fig_2">4</ref>. A more precise description can refer to the literature <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b48">[49]</ref>. In this example, the large NTT size is N = I × J. We can then decompose the N -size NTT into several I-size and J-size smaller NTTs. For convenience, we represent the original 1D input array a as a row-major I × J matrix in Figure <ref type="figure" target="#fig_2">4</ref>. We first do an I-size NTT for each of the J columns (step 1). Then we multiply the output with the corresponding twiddle factors (step 2). Next, we do a Jsize NTT for each of the I rows (step 3). Finally, we output each element in the column-major order, as the final output 1D array â.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Bandwidth-Efficient NTT Hardware Module</head><p>With the above decomposition, we only need to design a relatively small-size NTT hardware module that works on I and J array elements. Previous work like HEAX <ref type="bibr" target="#b44">[45]</ref> implemented such NTT modules following the data access pattern in Figure <ref type="figure">3</ref>, using a set of on-chip multiplexers to deliver each input element to the corresponding multiplier. However, recall that I and J could still be large (e.g., 1024). Directly fetching these data from off-chip memory in every cycle would result in significant bandwidth consumption, as described in Section III-B. Therefore, we adopt a bandwidthefficient pipelined architecture. We choose a design similar to <ref type="bibr" target="#b33">[34]</ref> as the basic building block. It is a fully pipelined design that reads one input element and produces one output element sequentially in each clock. Instead of using many multiplexers, we use FIFOs with different depths to deal with the different strides in each stage.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the simplified design for a 1024-size NTT pipeline module. It contains 10 stages. Each stage has an NTT core that does the butterfly operation between two elements with a certain stride, as in Figure <ref type="figure">3</ref>, and generates two new elements for the next stage. The core has a 13-cycle latency for the arithmetic operations inside. The depth of the FIFO in each stage matches the stride needed, i.e., 512 for the first stage, 256 for the second stage, and so on. The pipeline keeps reading one element per cycle from the memory. In the first 512 cycles, the 512 elements are stored in the FIFO in the first stage. In the next 512 cycles, we enable the NTT core, which uses the newly read element and pops the head of the FIFO as its two inputs, with the desired stride 512. In this way, the stride is correctly enforced with a FIFO instead of multiplexers. The NTT core generates two output elements in each cycle, one of which is directly sent to the next stage. The other output needs to be buffered and sent to the next stage at a later point (see the orders in Figure <ref type="figure">3</ref>). We reuse the FIFO in the first stage for this purpose, as the input elements in the FIFO can be discarded after use. The next stage follows the same behavior but with a different FIFO depth to realize a different stride. The last stage writes the output back to the memory.</p><p>With the above design, we reduce the bandwidth needed to only one element read and one element write per cycle. With 256-bit elements and 100 MHz, this is just 5.96 GB/s, much more practical to satisfy than before. Also, we reduce the superlinear multiplexer cost to linear memory cost. Not only the resources scale better now, but also the resource type changes from complex logic units to regular RAM.</p><p>The total latency for an N -size NTT includes 13 log N cycles for the log N stages, and N cycles for buffering the data across all stages. It requires another N cycles to fully process all elements, which can be overlapped with the next NTT kernel if any. If there are t modules, it takes 13 log N +N + NT t cycles to compute T NTT kernels in parallel.</p><p>Supporting INTT. We also need to support INTT in POLY. An INTT module is almost the same as NTT, except that (1) the execution order in the butterfly NTT core is different; (2) the control unit operates in the reversed stage order; and (3) the twiddle factors are inversed. We design one butterfly core for both NTT and INTT with different control logic, but shared computation resources such as the expensive multipliers which are the dominant components. In POLY, NTTs and INTTs are chained together as in Figure <ref type="figure">2</ref>. Thus we can alternately adopt the two reordering styles of input and output arrays in our modules as described in Section III-A to eliminate the need for the bit-reverse operations.  Various-size kernels. Our NTT module can also easily support various-size NTT kernels that are smaller than N . The NTT kernels in POLY are always padded by software to power-of-two sizes. For a power-of-two size smaller than N , we can bypass the previous stages in the module and start from a later stage. For example, a 512-size NTT starts from the second stage. Thus the module can flexibly support different I-size and J-size NTTs after decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overall NTT Dataflow</head><p>We follow the recursive algorithm in Figure <ref type="figure" target="#fig_2">4</ref> to process large-size NTT kernels in a decomposed manner on the small NTT hardware modules in Section III-D. However, the overall data access pattern in each of the steps does not match well with the data layout stored in the off-chip memory. This would result in inefficient large-stride accesses that poorly utilize the available bandwidth. To illustrate this issue, we consider the original input matrix in Figure <ref type="figure" target="#fig_2">4</ref>, whose layout in memory is row-major, generated from the 1D array a (up to a million elements). In step 1, each I-size column NTT kernel needs to process one column of data. This would make J-strided accesses (up to 1024) on the row-major layout. The output data of this step naturally form a column-major matrix. Step 2 is a simple pass of element-wise multiplication. However, in step 3, each J-size row NTT kernel should access the data in a row, again resulting in large strides on the column-major layout. Finally, the output of step 3, which is in row-major after the row NTT kernels, should go through another transpose to be read out in the column order, leading to another round of strided accesses.</p><p>To alleviate the problem and make better use of the bandwidth, we effectively block the data to balance between the two choices of layouts (row-major and column-major) and initiate on-chip SRAM buffers to improve input data reuse and aggregate output data before storing back. We also implement multiple NTT modules to process in parallel and to fully utilize the data fetched together from memory each time.</p><p>For simplicity, suppose I = J and the original NTT size N = I × I. We implement t NTT modules of size I as shown in Figure <ref type="figure">6</ref>. Data are still stored in the off-chip memory in a row-major order, as resulted from the original 1D array. First, we fetch t columns together from the off-chip memory and process them in the t NTT modules. Each memory access reads a t-size range of elements, resulting in better sequential access bandwidth. Recall from Section III-D that each NTT module only reads one new input element at each cycle, and outputs one element per cycle after the initial pipeline filling.</p><formula xml:id="formula_3">t elements 0 … … … … … I t elements I-sized … … … 0 … clk 1 clk t Output: … … … … … … … … … … … … … … … … … I Read I-1 t-1 I-sized - 0 t-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDR memory</head><p>On-chip Transpose Fig. <ref type="figure">6</ref>. The overall dataflow of NTT processing. In each cycle, we read t sequential elements from each row of the original 2D array as the input of the t I-size column NTTs. Or equivalently, we read from t columns simultaneously. The marked read happens in the (I + t)-th cycle to fetch that sub-row to the NTT modules. The green block shows the buffered output data on-chip to be transposed. The grey elements are currently being processed in the t NTT module pipelines.</p><p>We use an on-chip buffer of size t×t to resolve the data layout issue, by performing a small matrix transpose before writing data back to off-chip memory. In each cycle, the t modules output t elements and write a column in the on-chip buffer. When the buffer is filled up, we write back each row to off-chip memory, resulting in at least t-size access granularity. This allows us to always keep the data in the off-chip memory in row-major formats, while still achieving at least t-size access granularity for high effective bandwidth. Figure <ref type="figure">6</ref> shows the details during the processing. The green block of t × t elements are already processed and the results are written to the on-chip buffer on the right side. They were pushed into the buffer by columns and popped out to the memory by rows. The gray elements, including the beginning of the second group of t columns, are being processed in the NTT module pipelines. In such a way, we see that the t NTT modules are fully pipelined and well utilized. The pressure on the off-chip bandwidth is also alleviated with our bandwidthefficient NTT module design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ACCELERATING MULTI-SCALAR MULTIPLICATION</head><p>In this section, we first introduce the computation task and design challenges for MSM. Then, we present the algorithm and the corresponding architecture to accelerate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MSM Computations</head><p>As illustrated in Section II-B, the MSM computations are defined as Q = n i=1 k i P i , where each P i is a point on a predetermined EC and each k i is a λ-bit scalar on a large finite field. Each pair k i P i is a point scalar multiplication (PMULT), and MSM needs to add up (PADD) these products to get one final point. zk-SNARK requires several times of MSM with different scalar vectors. One is from the result of POLY (H n ) and the other is from the witness (S n ). Note that the point vectors are known ahead of time as fixed parameters for a certain application problem; only the scalar vectors change according to different witnesses.</p><p>As we can see, the most expensive operations in MSM are PMULT and PADD on the EC. Similar to the fast exponentiation algorithm <ref type="bibr" target="#b30">[31]</ref>, the more expensive PMULT can be decomposed into a series of PADD and PDBL in a bit-serial fashion. An example is shown in Figure <ref type="figure" target="#fig_5">7</ref>, where we want to compute 37P. We represent 37 in its binary form (100101) 2 . At each bit position, we execute a PDBL to double the point. If the bit is 1, we add it to the result using a PADD. We can find that PMULT invokes PADD and PDBL sequentially according to each bit of the scalar k i . Thus, the sparsity of the scalar k i impacts the overall latency. If the binary form of k i contains more 1's, then the ith PMULT needs more PADD operations and thus more time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design Challenges</head><p>While EC is a commonly used kernel in a wide range of cryptographic applications, most of them only need a single PMULT to encrypt values. Thus, none of the previous accelerators or ASICs has been specially designed for MSM, which involves a large number of PMULT operations whose results are finally accumulated with a PADD. For such a pattern, directly duplicating existing PMULT accelerators is inefficient. Because the computation demands of PADD and PDBL depend on each input scalar, not only the utilization of each PMULT module would be quite low for sparse scalars, but the multiple PMULT modules would also suffer from load imbalance issues, further decreasing the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimized Algorithm and Hardware Module Design</head><p>Instead of directly replicating PMULT modules, we adopt the Pippenger algorithm <ref type="bibr" target="#b42">[43]</ref> to achieve high resource utilization and better load balancing. We firstly represent the scalar k under radix 2 s , where s is a chosen window size. This is equivalent to dividing the λ-bit scalar k into λ s chunks with s bits each. An example is shown in Figure <ref type="figure" target="#fig_6">8</ref>, where λ = 12 and s = 4. Computing Q can be done with the following steps: First, sum up the elements in each chunk i (s-bit wide) to get G i . Then, sum up 2 i×s G i to get the final result, with 2 i×s as the weights.</p><p>In this way, we convert the original computation to a set of smaller sub-tasks of computing G i . For each sub-task, the Pippenger algorithm groups the elements in the s-bit chunk by the scalars, and put those P's with the same corresponding scalar value into the same bucket, as shown in Figure <ref type="figure" target="#fig_6">8</ref> right side. Since the scalar bitwidth is s, there are 2 s − 1 different buckets in total. Note that if the scalar is zero, we can directly skip the corresponding points. Then we add up all the points assigned to the same bucket, to get one sum point B i per each bucket. Then G i can be computed by adding up B i weighted by the corresponding scalar i to that bucket. As long as the number of original PMULT operations (i.e., the length of the point and scalar vectors) is much larger than the number of buckets (2 s − 1), by doing this we can convert the many expensive PMULT operations into the more lightweight PADD within each bucket. The detailed maths is shown below, where b i [j] represents the j-th radix-2 s chunk of a i . n i=1</p><formula xml:id="formula_4">a i P i = λ s −1 j=0 [ n i=1 (b i [j] * P i )] * 2 js = λ s −1 j=0 G j * 2 js G j = n i=1 (b i [j] * P i ) = 2 s −1 k=0 k * [ n i=1 (b i [j] == k) * P i ]</formula><p>With the Pippenger algorithm, MSM becomes PADDintensive. We design efficient PADD modules. The PADD module is heavily pipelined with 74 stages for expensive arithmetic modular operations such as modular multiply. Since the datapath of PADD is deterministic, we alleviate resource underutilization and load imbalance issues. The remaining few PDBL operations when summing up iB i and 2 i×s G i have only negligible cost, less than 0.1% in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall Architecture</head><p>While we convert expensive PMULT into cheaper PADD operations, the overall architecture still faces a few challenges. First, the group-by phase requires an efficient implementation, especially considering that the size of the MSM (i.e., the length of the scalar and point vectors) could be very large, up to a few millions. The control logic is also non-trivial. Second, while each PADD operation is deterministic, the number of points assigned to each bucket, and hence the number of PADD operations needed, can be skewed. The workloads between buckets can therefore still be possibly imbalanced.</p><p>To solve the above problems, we propose a novel architecture for the Pippenger algorithm. First, we divide a large MSM into smaller segments to fit in the on-chip memory. For example, as shown at the top of Figure <ref type="figure" target="#fig_7">9</ref>, an MSM with 1 million scalars and points can be divided, and each time we load one segment of 1024 scalars (256-bit each) and points (768-bit each using projective coordinates) to the onchip global buffer from the off-chip memory. Then, in each cycle, we read two scalars and two corresponding points from the on-chip buffer. We put the points into different buckets according to the last four bits of the corresponding scalars. The depth for the bucket buffer is only one. Once there are two points that would appear in the same bucket, they will be transferred into a centralized FIFO together with the bucket index as their label, as the green area shown in Figure <ref type="figure" target="#fig_7">9</ref>. Each entry of the FIFO contains a 4bit label (bucket index) and two points from the same bucket waiting to be added together. There are two 15-entry FIFOs prepared for the two scalar-point pairs read in the same cycle.</p><p>The entries in the FIFOs are sent to a shared, pipelined PADD module to be processed. When the resultant sum is ready, it should be written back to the corresponding bucket according to the label for further accumulation. However, these results also need another 15-entry FIFO to buffer, in case of conflicts with the already existing data in the destination buckets. Basically, the newly obtained sum can be immediately sent to this FIFO together with the existing data in the bucket for another PADD operation. The PADD module hence can read from three FIFOs in total, two for newly loaded data and one for PADD results. After 512 cycles, the last 4-bit chunks of all 1024 scalars are processed. We then move forward to the next 4 bits and repeat the above workflow.</p><p>Overall, our architecture for the Pippenger algorithm uses a centralized and shared PADD module among all buckets, and dynamically dispatches work from these buckets to achieve load balance. Because the PADD module is the performance and area dominant part, sharing it results in a much better resource utilization than having separate private PADD modules in each bucket. Our work dispatch mechanism is also lightweight. We avoid physically sorting the points as typical group-by algorithms require. We mostly rely on a small number of buffers and FIFOs to stash the data to be accumulated. Carefully provisioning the buffer and FIFO sizes allows us to avoid most stalls and achieves high throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Exploiting Parallelism and Balancing Loads</head><p>The PADD module in our architecture in Section IV-D is clearly a performance bottleneck. Now, we extend the design to use multiple PADD modules in parallel. A straightforward way to make use of those PADDs is to provision multiple PADD for the same set of FIFOs and distribute work from these FIFOs among them. However, this will result in complicated synchronization control logic. Also, increasing the number of PADD modules may lead to more idle cycles when the FIFOs are empty, thus decreasing resource utilization.</p><p>We use a different way to balance the workloads among different PADD modules. Notice that we only read 4 bits of a scalar in one round and then read the next 4 bits in the next round. Each round is independent of each other, and thus can be processed in parallel. Therefore, we replicate the entire design in Section IV-D as multiple processing elements (PEs), each with a separate set of buckets, the FIFOs, and a PADD module. For t PEs, we can read 4t bits of the scalar each time in one round. Each PE works exactly the same as previously described, and processes its own 4-bit chunk with the same set of points. The control logic is greatly simplified in this way.</p><p>We next consider the detailed workload balance among different PEs. The worst situation is that all points in one PE are put into a single bucket. Thus, it has the longest PADD dependency chain, with 1023 PADD operations to get the final result. The best situation is that all points in one round have a uniform distribution and they are put into the 15 buckets evenly, each with 64 or 65 points. This requires 1024 − 15 = 1009 PADD operations. As the PADD module is shared across all buckets in a PE and is not aware of which bucket the pair of points is from, the end-to-end latency difference between these two cases with similar numbers of required PADD operations is negligible. Therefore, load balance among multiple PEs is well maintained.</p><p>As shown in Figure <ref type="figure">2</ref>, one scalar H n is from the polynomial computation, and the other S n is from the expended witness directly. H n is dense and can be regarded as approximately uniformly distributed, since doing NTT brings uncertainty to the data. Consequently, the possibility of the worst case is extremely low. S n is very sparse. In fact, more than 99% of the scalars are 0 and 1. This is because the arithmetic circuit usually has a lot of bound checks and range constraints. It uses the binary form of values, and brings 0 and 1 to the expended witness vector. Note that the cases for 0 and 1 can be directly computed without sending into the pipelined acceleration hardware. We process those cases separately. <ref type="foot" target="#foot_2">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OVERALL SYSTEM</head><p>The overall architecture of PipeZK is shown in Figure <ref type="figure" target="#fig_8">10</ref>. The CPU first expands the witness and transfers the data to the accelerator's DDR memory. Next, the accelerator reads from its memory to execute NTT/INTTs for the POLY phase. After the POLY is done, the MSM subsystem processes the scalar and point vectors. It outputs the partial sums of B i from each bucket (see Figure <ref type="figure" target="#fig_7">9</ref>), and the CPU deals with the remaining additions, which is less than 0.1% of the execution time.</p><p>Note that there are two types of ECs (G1 and G2) in the actual MSM implementation of zk-SNARK <ref type="bibr" target="#b8">[9]</ref>. Both G1 and G2 have exactly the same high-level algorithm, so they could have benefited from the same architecture as we introduced in Section IV. The difference is that G2 has different basic units, i.e., the multiplication on G2 needs four modular multiplications whereas G1 only needs one. It needs more resources to implement G2. However, G2 often takes less than 10% of the overall MSM time and the scalar vectors for this part are very sparse. Therefore we move the G2 part to the host CPU, to achieve better trade-off between resources and performance. In summary, the CPU generates the witness and processes the MSM for G2, and the accelerator processes the POLY and the MSM for G1. This results in a heterogeneous system with few interactions. And the computations on both sides can happen in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>The evaluation consists of two parts. First, we present the microbenchmark results with various input sizes (i.e., constraint system sizes) for the NTT/MSM modules, along with the results of typical workloads shown in Table <ref type="table" target="#tab_5">V</ref>. Second, a real-world application, Zcash, is showcased with three end-to-end workloads to demonstrate the practicality of our design and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>For POLY and MSM, we have a full-stack Verilog implementation, which includes the low-level operations such as PADD, PDBL, and PMULT (with Montgomery optimizations <ref type="bibr" target="#b39">[40]</ref> and projective coordinates <ref type="bibr" target="#b12">[13]</ref>). We synthesize our design using Synopsys Design Compiler under UMC 28 nm library (details in Table <ref type="table" target="#tab_1">I</ref>), and use Ramulator to simulate the performance of off-chip DDR memory. The ASIC-based POLY and MSM modules are integrated along with other modules (such as trusted setup and witness generation) from libsnark <ref type="bibr" target="#b8">[9]</ref> running on the host CPU, to derive an end-to-end prototype, as Figure <ref type="figure" target="#fig_8">10</ref> illustrates.</p><p>We compare our design (denoted as "ASIC") against the state-of-the-arts, including a single GPU implementation <ref type="bibr" target="#b5">[6]</ref> (denoted as "1GPU"), an 8-GPU implementation <ref type="bibr" target="#b2">[3]</ref> (denoted as "8GPUs"), and libsnark <ref type="bibr" target="#b8">[9]</ref> and bellman <ref type="bibr" target="#b1">[2]</ref> on a CPU  server (denoted as "CPU"), respectively. Note that due to the limitations of the baseline implementations, in the rest of the paper, we only show corresponding results for supported curves (details in Table <ref type="table" target="#tab_1">I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluating NTT and MSM with Different Input Sizes</head><p>This section presents the microbenchmark results for our NTT and MSM implementations on ASICs. We vary the input size from 2 14 to 2 20 to demonstrate the scalability of our design. For both NTT and MSM, we evaluate them with different underlying elliptic curves: BN-128, BLS12-381, and MNT4753, where the bitwidth λ = 256, 384, and 768, respectively. For BN-128 and MNT4753, we use libsnark <ref type="bibr" target="#b8">[9]</ref> on CPUs while bellperson <ref type="bibr" target="#b2">[3]</ref> for BLS12-381 on GPUs. <ref type="foot" target="#foot_3">3</ref>The results for NTT and MSM are shown <ref type="foot" target="#foot_4">4</ref> in Tables II and III. The speedup over the baseline is also attached to each latency number of the ASIC, which equals to the latency ratio between the baseline and the ASIC. Compared to the CPU/GPU implementations, our ASIC design demonstrates a speedup up to 197.5x and 77.7x for NTT and MSM, respectively. Even with increasing input sizes, our implementation still shows superiority.</p><p>We carefully tailor the tradeoffs between resource consumption and speed in our ASIC implementations. For the 256-bit curve BN-128, we implement 4 NTT pipelines and 4 PEs for MSM, while use only 1 PE for MSM/NTT in the 768bit MNT4753 curve. For BLS12-381, we implement 4 NTT   <ref type="table" target="#tab_5">IV</ref>). Large integer modular multiplication plays a dominant role in the resource utilization. We expect the performance will be further improved with more careful resource-efficient design for modular multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluating zk-SNARK Workloads</head><p>We also evaluate POLY and MSM of zk-SNARK<ref type="foot" target="#foot_5">5</ref> over typical workloads <ref type="bibr" target="#b7">[8]</ref>, as shown in Table <ref type="table" target="#tab_5">V</ref>.</p><p>We present the end-to-end proof time, which includes the time of loading parameters through PCIe, computing POLY and MSM on chip, as well as other processing on CPU. These workloads are compiled with jsnark <ref type="bibr" target="#b7">[8]</ref> and executed with libsnark as our backend. Both CPU and GPU baselines <ref type="bibr" target="#b5">[6]</ref> are evaluated with the curve MNT4753 where λ = 768. And as described in Section V, MSM G2 is offloaded to CPU in the GPU baseline and our design. We list the time for POLY, MSM, and MSM G2, respectively. We only provide the overall proof time for "1GPU" without the breakdown due to their heterogeneous architecture with intertwined timings of MSM/POLY on GPU/CPU.</p><p>For our ASIC implementation, the latency for proof without G2 (which runs on ASIC) and the latency for MSM G2 (which runs on CPU) are both presented. The final proof time is determined by the maximum latency of the two parts, since they can execute in parallel. However, MSM G2 usually dominates in the overall latency. In summary, Table V shows significant acceleration rates of our implementation over the baselines (50x faster). If we could have additional support for MSM G2 part, the speedup would be even higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluating Zcash</head><p>Last, we evaluate a real-world industrial application, Zcash, and compare the end-to-end results with a CPU implementation (currently, there are no available GPU implementations for Zcash). The results are shown in Table <ref type="table" target="#tab_5">VI</ref>.</p><p>There are three kinds of workloads (sprout, sapling spend, sapling output) in Zcash. To make a shielded transaction, a compound proof is required (i.e., a combination of those workloads). The time for the transaction adds up the proving time for different types of proofs. Other latencies in a transaction such as generating signatures occupy less than 0.5% portion. For the largest workload, sprout, we can accelerate the time to generate shielded transactions by 6x. For circuits sapling spend and sapling output , we can reduce the latency of making shielded transactions over 4x.</p><p>We can see that the overall acceleration rate is much lower compared to the acceleration rate of each single module (POLY, MSM). This is because the latencies for MSM G2 and generating witness on CPU ("MSM G2" and "Gen Witness") start to dominate after our acceleration for other parts. As we mentioned in the previous section, MSM G2 can use exactly the same architecture as G1 and get a similar acceleration rate if needed. In addition, generating witness is highly parallelizable with software optimizations, which takes 10% of the overall time and one only needs to accelerate this part for 3 or 4 times to match the overall speedup achieved by our implementation. Therefore, we expect the effort to be technically trivial for ASIC-based MSM G2 and softwareoptimized witness generating. We leave these for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>Zero-knowledge proof has been introduced for decades and are widely considered as one of the most useful weapons for establishing trust and preserving privacy. However, its limited performance has impeded its wider applications in practice. In this paper, we propose PipeZK, the first architectural effort to significantly accelerate zk-SNARK, the state-of-the-art zeroknowledge proof protocol. We introduce and implement various techniques to efficiently streamline key operations (NTTs, MSMs, etc.) in zk-SNARK. Our empirical results demonstrate considerable speedups compared to state-of-the-art solutions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. The workflow of the prover. The illustrated F (x, w) has a constraint system size of five (i.e. n = 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The recursive NTT algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>421</head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:12:17 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The architecture of a 1024-size bandwidth-efficient NTT module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>422Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An example of bit-serial PMUT computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Pippenger algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIFOFig. 9 .</head><label>9</label><figDesc>Fig. 9. Overall architecture of the Pippenger algorithm for MSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The overall architecture of PipeZK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CONFIGURATIONS</head><label>I</label><figDesc>AND SUPPORTED CURVES ON EACH PLATFORM.</figDesc><table><row><cell>Platforms</cell><cell>Detailed Configurations</cell><cell>Supported Curves</cell></row><row><cell>ASIC (ours)</cell><cell>Synopsys DC, UMC 28nm library,</cell><cell>BN-128, BLS12-381,</cell></row><row><cell></cell><cell>DDR4 @2400MHz (4 channels, 2 ranks)</cell><cell>MNT4753</cell></row><row><cell>CPU [9]</cell><cell>Intel(R) Xeon(R) Gold 6145 @2.00G Hz,</cell><cell>BN-128, MNT4753 [9]</cell></row><row><cell></cell><cell>80 logical cores, 377G RAM</cell><cell>BLS12-381 [2]</cell></row><row><cell>8GPUs [3]</cell><cell>eight Nvidia GTX 1080 TI cards</cell><cell>BLS12-381</cell></row><row><cell>1GPU [6]</cell><cell>single Nvidia GTX 1080 TI card</cell><cell>MNT4753</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>FOR DIFFERENT WORKLOADS (LATENCIES IN SECONDS).</figDesc><table><row><cell></cell><cell></cell><cell>CPU</cell><cell cols="2">1GPU</cell><cell></cell><cell cols="2">ASIC</cell><cell></cell><cell></cell><cell>Acceleration Rate</cell><cell>Acceleration Rate (w/o G2)</cell></row><row><cell cols="5">Application Size POLY MSM Proof Proof POLY</cell><cell>MSM w/o G2</cell><cell cols="2">Proof w/o G2</cell><cell>MSM G2</cell><cell cols="2">Proof ASIC/CPU ASIC/GPU ASIC/CPU ASIC/GPU</cell></row><row><cell>AES</cell><cell cols="10">16384 0.301 0.835 1.137 1.393 0.002 0.021 0.023 0.097 0.097 11.768</cell><cell>14.420</cell><cell>49.791</cell><cell>61.012</cell></row><row><cell>SHA</cell><cell cols="10">32768 0.545 0.984 1.529 1.983 0.003 0.027 0.030 0.102 0.102 14.935</cell><cell>19.365</cell><cell>50.330</cell><cell>65.261</cell></row><row><cell cols="11">RSA-Enc 98304 1.882 3.403 5.290 5.157 0.014 0.080 0.094 1.230 1.230</cell><cell>4.302</cell><cell>4.193</cell><cell>56.297</cell><cell>54.878</cell></row><row><cell cols="11">RSA-SHA 131072 1.935 3.578 5.514 5.958 0.014 0.105 0.119 0.822 0.822</cell><cell>6.705</cell><cell>7.246</cell><cell>46.481</cell><cell>50.228</cell></row><row><cell cols="11">Merkle Tree 294912 6.623 8.071 14.695 16.287 0.063 0.226 0.289 2.697 2.697</cell><cell>5.449</cell><cell>6.040</cell><cell>50.869</cell><cell>56.381</cell></row><row><cell cols="11">Auction 557056 13.875 10.817 24.692 30.573 0.139 0.445 0.585 2.053 2.053 12.025</cell><cell>14.890</cell><cell>42.243</cell><cell>52.306</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE VI</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">RESULTS FOR ZCASH (LATENCIES IN SECONDS).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CPU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASIC</cell><cell>Acceleration Rate</cell></row><row><cell></cell><cell>Application</cell><cell>Size</cell><cell>Gen Witness</cell><cell cols="3">POLY MSM Proof</cell><cell>MSM G2</cell><cell cols="2">POLY</cell><cell>MSM w/o G2</cell><cell>Proof w/o G2</cell><cell>Proof ASIC/CPU</cell><cell>ASIC/CPU w/o G2</cell></row><row><cell></cell><cell>Zcash Sprout</cell><cell cols="9">1956950 1.010 3.652 5.147 9.809 0.677 0.076 0.136 0.211 1.687</cell><cell>5.815</cell><cell>8.031</cell></row><row><cell></cell><cell cols="2">Zcash Sapling Spend 98646</cell><cell cols="8">0.187 0.441 0.766 1.393 0.167 0.004 0.014 0.018 0.354</cell><cell>3.937</cell><cell>6.817</cell></row><row><cell></cell><cell cols="2">Zcash Sapling Output 7827</cell><cell cols="8">0.043 0.107 0.115 0.266 0.034 0.254ms 0.001 0.002 0.077</cell><cell>3.480</cell><cell>5.982</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:12:17 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Here we abuse the notion of security parameter for simplicity, since it is usually directly related to the bit width of parameters and the underlying elliptic curve.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The cases of 0 and 1 can be filtered when fetching from the scalar and processed in parallel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Since the eight-GPU implementation<ref type="bibr" target="#b2">[3]</ref> on BLS12-381 is much faster than that of CPU<ref type="bibr" target="#b1">[2]</ref>, we omit corresponding latency results of CPU for simplicity. However, the one-GPU-card implementation<ref type="bibr" target="#b5">[6]</ref> demonstrates weaker performance than that of our 80-core CPU server. Thus, we only list the CPU results for BN-128 and MNT4753 in Tables II and III.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">For BLS381 where λ = 384, the scalar field is still 256-bit. Thus we only compare the performance of 256 and 768-bit for NTT part in TableII.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Note that in the rest of the paper, MSM of zk-SNARK (or MSM for short) denotes the computations of four G1-type MSMs and one G2-type MSM, which differ from "MSM" in Section VI-B that consists of only one G1-type MSM.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank the anonymous reviewers for their valuable comments. This work is partially supported by National Key R&amp;D Program of China (2020AAA0105200), National Natural Science Foundation of China (62032001 and 62072262).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">barrywhitehat. roll up: Scale ethereum with snarks</title>
		<ptr target="https://github.com/barryWhiteHat/rollup/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">bellman: a crate for building zk-snark circuits</title>
		<ptr target="https://github.com/zkcrypto/bellman" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">bellperson: Gpu parallel acceleration for zk-snark</title>
		<ptr target="https://github.com/filecoin-project/bellperson" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Filecoin company</title>
		<ptr target="https://filecoin.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fpga snark prover targeting the bn128 curve</title>
		<ptr target="https://github.com/bsdevlin/fpgasnarkprover" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gpu groth16 prover</title>
		<ptr target="https://github.com/CodaProtocol/gpu-groth16-prover-3x" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<ptr target="https://www.goquorum.com/" />
	</analytic>
	<monogr>
		<title level="j">J.p. morgan quorum</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">jsnark: A java library for building snarks</title>
		<ptr target="https://github.com/akosba/jsnark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">libsnark: a c++ library for zksnark proofs</title>
		<ptr target="https://github.com/scipr-lab/libsnark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Qed-it</title>
		<ptr target="https://qed-it.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The snark challenge: A global competition to speed up the snark prover</title>
		<ptr target="https://coinlist.co/build/coda" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Zcash company</title>
		<ptr target="https://z.cash/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ieee standard specifications for public-key cryptography</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Std 1363-2000</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and flexible hardware support for ecc over multiple standard prime fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alrimeih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rakhmatov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2661" to="2674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-Z ECC scalar multiplications for hardware, software and hardwaresoftware co-design on embedded systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Goundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Marnane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptographic Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="240" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational integrity with a public random string from quasilinear pcps</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bentov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gabizon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pergament</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riabzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="551" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive oracle proofs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Spooner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Succinct non-interactive arguments via linear interactive proofs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bitansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Paneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vector commitments and their applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Catalano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Public Key Cryptography</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Marlin: Preprocessing zksnarks with universal and updatable srs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="738" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Inside the FFT black box: serial and parallel fast Fourier transform algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pinocchio coin: building zerocoin from a succinct pairing-based proof system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohlweiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACM workshop on Language support for privacy-enhancing technologies</title>
				<meeting>the First ACM workshop on Language support for privacy-enhancing technologies</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cinderella: Turning shabby x. 509 certificates into elegant anonymous credentials with the magic of verifiable computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delignat-Lavaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohlweiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy (SP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="235" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling proof-ofreplication for filecoin mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Benet//Technical report</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Plonk: Permutations over lagrange-bases for oecumenical noninteractive arguments of knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gabizon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciobotaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACR Cryptol. ePrint Arch</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">953</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Verifiable sealed-bid auction on the ethereum blockchain</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Galal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Youssef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Financial Cryptography and Data Security</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-interactive verifiable computing: Outsourcing computation to untrusted workers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Cryptology Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="465" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quadratic span programs and succinct nizks without pcps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raykova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="626" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully homomorphic encryption using ideal lattices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gentry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-first annual ACM symposium on Theory of computing</title>
				<meeting>the forty-first annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The knowledge complexity of interactive proof systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rackoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="208" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of fast exponentiation methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of algorithms</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the size of pairing-based non-interactive arguments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="305" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Elliptic curve cryptography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hankerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menezes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new approach to pipeline fft processor</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Torkelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Parallel Processing</title>
				<meeting>International Conference on Parallel Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="766" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zcash protocol specification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hopwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hornby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wilcox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>GitHub</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Downloaded on December 31,2022 at 09:12:17 UTC from IEEE Xplore. Restrictions apply. point multiplication on elliptic curves over gf (p)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Javeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Circuit Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="228" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Low latency flexible fpga implementation of 427 Authorized licensed use limited to: Tsinghua University</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hawk: The blockchain model of cryptography and privacy-preserving smart contracts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kosba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papamanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE symposium on security and privacy (SP)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On ideal lattices and learning with errors over rings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lyubashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peikert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Coda: Decentralized cryptocurrency at scale</title>
		<author>
			<persName><forename type="first">I</forename><surname>Meckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-05">May. 2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>O (1) Labs whitepaper</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Modular multiplication without trial division</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="519" to="521" />
		</imprint>
	</monogr>
	<note>Mathematics of computation</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A scalable gf (p) elliptic curve processor architecture for programmable hardware</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Cryptographic Hardware and Embedded Systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="348" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pinocchio: Nearly practical verifiable computation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raykova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the evaluation of powers and related problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pippenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Symposium on Foundations of Computer Science (sfcs</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1976">1976. 1976</date>
			<biblScope unit="page" from="258" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On lattices, learning with errors, random linear codes, and cryptography</title>
		<author>
			<persName><forename type="first">O</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HEAX: An Architecture for Computing on Encrypted Data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Riazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1295" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fpga-based high-performance parallel architecture for homomorphic computing on encrypted data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jarvinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Verbauwhede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="387" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zerocash: Decentralized anonymous payments from bitcoin</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Miers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Virza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spartan: Efficient and general-purpose zksnarks without trusted setup</title>
		<author>
			<persName><forename type="first">S</forename><surname>Setty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Cryptology Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="704" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Schönhage-strassen algorithm with mapreduce for multiplying terabit integers</title>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Workshop on Symbolic-Numeric Computation</title>
				<meeting>the 2011 International Workshop on Symbolic-Numeric Computation</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DIZK: A distributed zero knowledge proof system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/usenixsecurity18/presentation/wu" />
	</analytic>
	<monogr>
		<title level="m">27th USENIX Security Symposium (USENIX Security 18)</title>
				<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="675" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Zero knowledge proofs for decision tree predictions and accuracy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2020 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2039" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">vsql: Verifying arbitrary sql queries over dynamic outsourced databases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papamanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A zero-knowledge version of vsql</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papamanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACR Cryptol. ePrint Arch</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">1146</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How to vote privately using bitcoin</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Communications Security</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
