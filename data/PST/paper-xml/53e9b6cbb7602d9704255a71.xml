<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stage Language Models for Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stage Language Models for Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">571185869B799D1D11C14313A2F3E24E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval Modelslanguage models</term>
					<term>parameter setting risk minimization</term>
					<term>two-stage language models</term>
					<term>two-stage smoothing</term>
					<term>Dirichlet prior</term>
					<term>interpolation</term>
					<term>parameter estimation</term>
					<term>leave-oneout</term>
					<term>mixture model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The optimal settings of retrieval parameters often depend on both the document collection and the query, and are usually found through empirical tuning. In this paper, we propose a family of two-stage language models for information retrieval that explicitly captures the different influences of the query and document collection on the optimal settings of retrieval parameters. As a special case, we present a two-stage smoothing method that allows us to estimate the smoothing parameters completely automatically. In the first stage, the document language model is smoothed using a Dirichlet prior with the collection language model as the reference model. In the second stage, the smoothed document language model is further interpolated with a query background language model. We propose a leave-one-out method for estimating the Dirichlet parameter of the first stage, and the use of document mixture models for estimating the interpolation parameter of the second stage. Evaluation on five different databases and four types of queries indicates that the twostage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close toor better than-the best results achieved using a single smoothing method and exhaustive parameter search on the test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>It is well-known that the optimal settings of retrieval parameters generally depend on both the document collection and the query.</p><p>For example, specialized term weighting for short queries was studied in <ref type="bibr" target="#b2">[3]</ref>. Salton and Buckley studied many different term weighting methods used in the vector-space retrieval model; their recommended methods strongly depend on the type of the query and the characteristics of the document collection <ref type="bibr" target="#b12">[13]</ref>. It has been a great challenge to find the optimal settings of retrieval parameters automatically and adaptively accordingly to the characteristics of the collection and queries, and empirical parameter tuning seems to be inevitable in order to achieve good retrieval performance. This is evident in the large number of parameter-tuning experiments reported in virtually every paper published in the TREC proceedings <ref type="bibr" target="#b14">[15]</ref>.</p><p>The need for empirical parameter tuning is due in part from the fact that most existing retrieval models are based on certain preassumed representation of queries and documents, rather than on a direct modeling of the queries and documents. As a result, the "adaptability" of the model is restricted by the particular representation assumed, and reserving free parameters for tuning becomes a way to accommodate any difference among queries and documents that has not been captured well in the representation. In order to be able to set parameters automatically, it is necessary to model queries and documents directly. This goal has been explored recently in the language modeling approach to information retrieval, which has attracted significant attention since it was first proposed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>The first uses of the language modeling approach focused on its empirical effectiveness using simple models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>. Recent work has begun to develop more sophisticated models and a systematic framework for this new family of retrieval methods. In <ref type="bibr" target="#b3">[4]</ref>, a risk minimization retrieval framework is proposed that incorporates language modeling as natural components, and that unifies several existing retrieval models in a framework based on Bayesian decision theory. One important advantage of the risk minimization retrieval framework over the traditional models is its capability of modeling both queries and documents directly through statistical language models, which provides a basis for exploiting statistical estimation methods to set retrieval parameters automatically. Several special language models are explored in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>, and in all uses of language modeling in IR, smoothing plays a crucial role. The empirical study in <ref type="bibr" target="#b16">[17]</ref> reveals that not only is retrieval performance generally sensitive to the setting of smoothing parameters, but also that this sensitivity depends on the type of queries that are input to the system.</p><p>In this paper, we propose a family of language models for information retrieval that we refer to as two-stage models. The first stage involves the estimation of a document language model independent of the query, while the second stage involves the computation of the likelihood of the query according to a query language model, which is based on the estimated document language model. Thus, the two-stage strategy explicitly captures the different influences of the query and document collection on the optimal settings of retrieval parameters.</p><p>We derive the two-stage models within the general risk minimization retrieval framework, and present a special case that leads to a two-stage smoothing method. In the first stage of smoothing, the document language model is smoothed using a Dirichlet prior with the collection language model as the reference model. In the second stage, the smoothed document language model is further interpolated with a query background language model. we propose a leave-one-out method for estimating the first-stage Dirichlet parameter and make use of a mixture model for estimating the second-stage interpolation parameter. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methodwhich is fully automatic-consistently gives retrieval performance that is close to, or better than, the result of using a single smoothing method and exhaustive parameter search on the test data.</p><p>The proposed two-stage smoothing method represents a step toward the goal of setting database-specific and query-specific retrieval parameters fully automatically, without the need for tedious experimentation. The effectiveness and robustness of the approach, along with the fact that there is no ad hoc parameter tuning involved, make it very useful as a solid baseline method for the evaluation of retrieval models.</p><p>The rest of the paper is organized as follows. We first derive the two-stage language models in Section 2, and present the two-stage smoothing method as a special case in Section 3. We then describe, in Section 4, methods for estimating the two parameters involved in the two-stage smoothing method. We report our experimental results in Section 5. Section 6 presents conclusions and suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TWO-STAGE LANGUAGE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The risk minimization framework</head><p>The risk minimization retrieval framework is a general probabilistic retrieval framework based on Bayesian decision theory <ref type="bibr" target="#b3">[4]</ref>. In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem. The framework unifies several existing retrieval models within one general probabilistic framework, and facilitates the development of new principled approaches to text retrieval.</p><p>In traditional retrieval models, such as the vector-space model <ref type="bibr" target="#b11">[12]</ref> and the BM25 retrieval model <ref type="bibr" target="#b10">[11]</ref>, the retrieval parameters have almost always been introduced heuristically. The lack of a direct modeling of queries and documents makes it hard for these models to incorporate, in a principled way, parameters that adequately address special characteristics of queries and documents. For example, the vector-space model assumes that a query and a document are both represented by a term vector. However, the mapping from a query or a document to such a vector can be somehow arbitrary. Thus, because the model "sees" a document through its vector representation, there is no principled way to model the length of a document. As a result, heuristic parameters must be used (see, e.g., the pivot length normalization method <ref type="bibr" target="#b13">[14]</ref>). Similarly, in the BM25 retrieval formula, there is no direct modeling of queries, making it necessary to introduce heuristic parameters to incorporate query term frequencies <ref type="bibr" target="#b10">[11]</ref>.</p><p>One important advantage of the risk minimization retrieval framework <ref type="bibr" target="#b3">[4]</ref> over these traditional models is its capability of modeling both queries and documents directly through statistical language modeling. Although a query and a document are similar in the sense that they are both text, they do have important differences. For example, queries are much shorter and often contain just a few keywords. Thus, from the viewpoint of language modeling, a query and a document require different language models. Practically, separating a query model from a document model has the important advantage of being able to introduce different retrieval parameters for queries and documents when appropriate. In general, using statistical language models allows us to introduce all parameters in a probabilistic way, and also makes it possible to set the parameters automatically through statistical estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Derivation of two-stage language models</head><p>The original language modeling approach as proposed in <ref type="bibr" target="#b8">[9]</ref> involves a two-step scoring procedure: (1) Estimate a document language model for each document; (2) Compute the query likelihood using the estimated document language model (directly). The twostage language modeling approach is a generalization of this twostep procedure, in which a query language model is introduced so that the query likelihood is computed using a query model that is based on the estimated document model, instead of using the estimated document model directly. The use of an explicit and separate query model makes it possible to factor out any influence of queries on the smoothing parameters for document language models.</p><p>We now derive the family of two-stage language models for information retrieval formally using the risk minimization framework.</p><p>In the risk minimization framework presented in <ref type="bibr" target="#b3">[4]</ref>, documents are ranked based on the following risk function:</p><formula xml:id="formula_0">R(di; q) = R∈{0,1} Θ Q Θ D L(θQ, θD, R) × p (θQ | q, U) p (θD | di, S) p (R | θQ, θD) dθD dθQ</formula><p>Let us now consider the following special loss function, indexed by a small constant ǫ, Lǫ(θQ, θD, R) = 0 if ∆(θQ, θD) ≤ ǫ c otherwise where ∆ : ΘQ × ΘD → Ê is a model distance function, and c is a constant positive cost. Thus, the loss is zero when the query model and the document model are close to each other, and is c otherwise. Using this loss function, we obtain the following risk:</p><formula xml:id="formula_1">R(d; q) = c - Θ D θ Q ∈Sǫ(θ D ) p(θ Q | q, U ) p(θ D | d, S) dθ Q dθ D</formula><p>where Sǫ(θD) is the sphere of radius ǫ centered at θD in the parameter space. Now, assuming that p (θD | d, S) is concentrated on an estimated value θD, we can approximate the value of the integral over ΘD by the integrand's value at θD. Note that the constant c can be ignored for the purpose of ranking. Thus, using A ∼ B to mean that A and B have the same effect for ranking, we have that</p><formula xml:id="formula_2">R(d; q) ∼ - θ Q ∈Sǫ( θD ) p (θQ | q, U) dθQ</formula><p>When θQ and θD belong to the same parameter space (i.e., ΘQ = ΘD) and ǫ is very small, the value of the integral can be approximated by the value of the function at θD times a constant (the volume of Sǫ( θD)), and the constant can again be ignored for the purpose of ranking. That is,</p><formula xml:id="formula_3">R(d; q) ∼ -p ( θD | q, U)</formula><p>Therefore, using this risk we will be actually ranking documents according to p ( θD | q, U), i.e., the posterior probability that the user used the estimated document model as the query model. Applying Bayes' formula, we can rewrite this as</p><formula xml:id="formula_4">p ( θD | q, U) ∝ p (q | θD, U)p ( θD | U) (1)</formula><p>Equation 1 is our basic two-stage language model retrieval formula. Similar to the model discussed in <ref type="bibr" target="#b0">[1]</ref>, this formula has the following interpretation: p (q | θD, U) captures how well the estimated document model θD explains the query, whereas p ( θD | U) encodes our prior belief that the user would use θD as the query model. While this prior could be exploited to model different document sources or other document characteristics, in this paper we assume a uniform prior.</p><p>The generic two-stage language model can be refined by specifying a concrete model p (d | θD, S) for generating documents and a concrete model p (q | θQ, U) for generating queries; different specifications lead to different retrieval formulas. If the query generation model is the simplest unigram language model, we have the scoring procedure of the original language modeling approach proposed in <ref type="bibr" target="#b8">[9]</ref>; that is, we first estimate a document language model and then compute the query likelihood using the estimated model. In the next section, we present the generative models that lead to the two-stage smoothing method suggested in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE TWO-STAGE SMOOTHING METHOD</head><p>Let d = d1d2...dn denote a document, q = q1q2...qm denote a query, and V = {w1, ..., w |V | } denote the words in the vocabulary. We consider the case where both θQ and θD are parameters of unigram language models, i.e., multinomial distributions over words in V .</p><p>The simplest generative model of a document is just the unigram language model θD, a multinomial. That is, a document would be generated by sampling words independently according to p (• | θD), or</p><formula xml:id="formula_5">p (d | θD, S) = n i=1 p(di | θD)</formula><p>Each document is assumed to be generated from a potentially different model as assumed in the general risk minimization framework. Given a particular document d, we want to estimate θD. We use a Dirichlet prior on θD with parameters α = (α1, α2, . . . , α |V | ), given by</p><formula xml:id="formula_6">Dir (θ | α) = Γ( È |V | i=1 αi) É |V | i=1 Γ(αi) |V | i=1 θ α i -1 i<label>(2)</label></formula><p>The parameters αi are chosen to be αi = µ p(wi | S) where µ is a parameter and p (• | S) is the "collection language model," which can be estimated based on a set of documents from a source S. The posterior distribution of θD is given by</p><formula xml:id="formula_7">p (θD | d, S) ∝ w∈V p (w | θD) c(w,d)+µp (w | S)-1</formula><p>and so is also Dirichlet, with parameters αi = c(wi, d)+µp (wi | S).</p><p>Using the fact that the Dirichlet mean is αj/ È k α k , we have that</p><formula xml:id="formula_8">pµ(w | θD) = θ D p(w | θD)p(θD | d, S)dθD = c(w, d) + µ p(w | S) |d| + µ</formula><p>where |d| = È w∈V c(w, d) is the length of d. This is the Dirichlet prior smoothing method described in <ref type="bibr" target="#b16">[17]</ref>.</p><p>We now consider the query generation model. The simplest model is again the unigram language model θQ, which will result in a retrieval model with the Dirichlet prior as the single smoothing method. However, as observed in <ref type="bibr" target="#b16">[17]</ref>, such a model will not be able to explain the interactions between smoothing and the type of queries. In order to capture the common and non-discriminative words in a query, we assume that a query is generated by sampling words from a two-component mixture of multinomials, with one component being θQ and the other some query background language model p (• | U). That is,</p><formula xml:id="formula_9">p (q | θQ, λ, U) = m i=1 ((1 -λ)p(qi | θQ) + λp(qi | U))</formula><p>where λ is a parameter, roughly indicating the amount of "noise" in q.</p><p>Combining our estimate of θD with this query model, we have the following retrieval scoring formula for document d and query q.</p><formula xml:id="formula_10">p (q | θD, λ, U) = = m i=1 (1 -λ)p(qi | θD) + λ p(qi | U) = m i=1 (1 -λ) c(qi, d) + µ p(qi | S) |d| + µ + λ p(qi | U)</formula><p>In this formula, the document language model is effectively smoothed in two steps. First, it is smoothed with a Dirichlet prior, and second, it is interpolated with a query background model. Thus, we refer to this as two-stage smoothing.</p><p>The above model has been empirically motivated by the observation that smoothing plays two different roles in the query likelihood retrieval method. One role is to improve the maximum likelihood estimate of the document language model, at the very least assigning non-zero probabilities to words that are not observed in the document. The other role is to "explain away" the common/nondiscriminative words in the query, so that the documents will be discriminated primarily based on their predictions of the "topical" words in the query. The two-stage smoothing method explicitly decouples these two roles. The first stage uses Dirichlet prior smoothing method to improve the estimate of a document language model; this method normalizes documents of different lengths appropriately with a prior sample size parameter, and performs well empirically <ref type="bibr" target="#b16">[17]</ref>. The second stage is intended to bring in a query background language model to explicitly accommodate the generation of common words in queries.</p><p>The query background model p(• | U) is in general different from the collection language model p(• | S). With insufficient data to estimate p(• | U), however, we can assume that p(• | S) would be a reasonable approximation of p(• | U). In this form, the two-stage smoothing method is essentially a combination of Dirichlet prior smoothing with Jelinek-Mercer smoothing <ref type="bibr" target="#b16">[17]</ref>. Indeed, it is very easy to verify that when λ = 0, we end up with just the Dirichlet prior smoothing, whereas when µ = 0, we will have Jelinek-Mercer smoothing. Since the combined smoothing formula still follows the general smoothing scheme discussed in <ref type="bibr" target="#b16">[17]</ref>, it can be implemented very efficiently. In the next section, we present methods for estimating µ and λ from data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PARAMETER ESTIMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Estimating µ</head><p>The purpose of the Dirichlet prior smoothing at the first stage is to address the estimation bias due to the fact that a document is an extremely small amount of data with which to estimate a unigram language model. More specifically, it is to discount the maximum likelihood estimate appropriately and assign non-zero probabilities to words not observed in a document; this is the usual role of language model smoothing. A useful objective function for estimating smoothing parameters is the "leave-one-out" likelihood, that is, the sum of the log-likelihoods of each word in the observed data computed in terms of a model constructed based on the data with the target word excluded ("left out"). This criterion is essentially based on cross-validation, and has been used to derive several well-known smoothing methods including the Good-Turing method <ref type="bibr" target="#b7">[8]</ref>.</p><p>Formally, let C = {d1, d2, ..., dN } be the collection of documents. Using our Dirichlet smoothing formula, the leave-one-out log-likelihood can be written as</p><formula xml:id="formula_11">ℓ-1(µ | C) = N i=1 w∈V c(w, di) log c(w, di) -1 + µp(w|C) |di| -1 + µ</formula><p>Thus, our estimate of µ is</p><formula xml:id="formula_12">μ = arg max µ ℓ-1(µ|C)</formula><p>which can be easily computed using Newton's method. The update formula is</p><formula xml:id="formula_13">µ (k+1) = µ (k) -g(µ (k) )/g ′ (µ (k) )</formula><p>where the first and second derivatives of ℓ-1 are given by</p><formula xml:id="formula_14">g(µ) = ℓ ′ -1 (µ) = N i=1 w∈V c(w, di)((|di| -1)p(w | C) -c(w, di) + 1) (|di| -1 + µ)(c(w, di) -1 + µp(w | C))</formula><p>and</p><formula xml:id="formula_15">g ′ (µ) = ℓ ′′ -1 (µ) = - N i=1 w∈V c(w, di)((|di| -1)p(w | C) -c(w, di) + 1) 2 (|di| -1 + µ) 2 (c(w, di) -1 + µp(w | C)) 2</formula><p>Since g ′ ≤ 0, as long as g ′ = 0, the solution will be a global maximum. In our experiments, starting from value 1.0 the algorithm always converges. The estimated values of µ for three databases are shown in Table 1. There is no clear correlation between the database characteristics shown in the table and the estimated value of µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimating λ</head><p>With the query model hidden, the query likelihood is</p><formula xml:id="formula_16">p (q | λ, U) = Θ Q m i=1 ((1 -λ)p(qi | θQ) + λp(qi | U)) p(θQ | U) dθQ</formula><p>In order to estimate λ, we approximate the query model space by the set of all N estimated document language models in our collection. That is, we will approximate the integral with a sum over all the possible document language models estimated on the collection, or</p><formula xml:id="formula_17">p (q | λ, U) = N i=1 πi m j=1 ((1 -λ)p(qj | θd i ) + λp(qj | U))</formula><p>where πi = p( θd i | U), and p(• | θd i ) is the smoothed unigram language model estimated based on document di using the Dirichlet prior approach.</p><p>Thus, we assume that the query is generated from a mixture of N document models with unknown mixing weights {πi} N i=1 . Leaving {πi} N i=1 free is important, because what we really want is not to maximize the likelihood of generating the query from every document in the collection, instead, we want to find a λ that can maximize the likelihood of the query given relevant documents. With {πi} N i=1 free to estimate, we would indeed allocate higher weights on documents that predict the query well in our likelihood function; presumably, these documents are also more likely to be relevant.</p><p>With this likelihood function, the parameters λ and {πi} N i=1 can be estimated using the EM algorithm. The update formulas are</p><formula xml:id="formula_18">π (k+1) i = π (k) i É m j=1 ((1 -λ (k) ) p(qj | θd i ) + λ (k) p(qj | U)) È N i ′ =1 π (k) i ′ É m j=1 ((1 -λ (k) )p(qj | θd i ′ ) + λ (k) p(qj | U)) and λ (k+1) = 1 m N i=1 π (k+1) i m j=1 λ (k) p (qj | U) (1 -λ (k) )p (qj | θd i ) + λ (k) p (qj | U)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section we first present experimental results that confirm the dual-role of smoothing, which provides an empirical justification for using the two-stage smoothing method for retrieval. We then present results of the two-stage smoothing method using the estimated parameters, comparing it to the optimal performance from using single smoothing methods and an exhaustive parameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Influence of Query Length and Verbosity on Smoothing</head><p>In <ref type="bibr" target="#b16">[17]</ref>, strong interactions between smoothing and the type of queries have been observed. However, it is unclear whether the high sensitivity observed on long queries is due to a higher density of common words in such queries, or to just the length. The two-stage smoothing method assumes that it is the former. In order to clarify this, we design experiments to examine two query factors-length and "verbosity." Specifically, we consider four different types of queries, i.e., short keyword, long keyword, short verbose, and long verbose queries, and compare how they each behave with respect to smoothing. As we will show, the high sensitivity is indeed caused by the presence of common words in the query, and this provides an empirical justification for the two-stage smoothing method.</p><p>We generate the four types of queries from TREC topics 1-150. These 150 topics are special because, unlike other TREC topics, they all have a "concept" field, which contains a list of keywords related to the topic; these keywords serve well as the "long keyword" version of our queries. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of such a topic (topic 52).</p><p>Title: South African Sanctions Description: Document discusses sanctions against South Africa. Narrative:</p><p>A relevant document will discuss any aspect of South African sanctions, such as: sanctions declared/proposed by a country against the South African government in response to its apartheid policy, or in response to pressure by an individual, organization or another country; international sanctions against Pretoria imposed by the United Nations; the effects of sanctions against S. Africa; opposition to sanctions; or, compliance with sanctions by a company. The document will identify the sanctions instituted or being considered, e.g., corporate disinvestment, trade ban, academic boycott, arms embargo.  We use all of the 150 topics, and generate the four versions of queries in the following way:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts</head><p>1. short keyword: Using only the title of the topic description (usually a noun phrase) <ref type="foot" target="#foot_0">1</ref>2. short verbose: Using only the description field (usually one sentence).</p><p>3. long keyword: Using the concept field (about 28 keywords on average).</p><p>4. long verbose: Using the title, description and the narrative field (more than 50 words on average).</p><p>The relevance judgments available for these 150 topics are mostly on the documents in TREC disk 1 and disk 2. In order to observe any possible difference in smoothing caused by the types of documents, we partition the documents in disks 1 and 2 and use the three largest subsets of documents, accounting for a majority of the relevant documents for our queries. The three databases are AP88-89, WSJ87-92, and ZIFF1-2, each about 400MB-500MB in size. The queries without relevance judgments for a particular database were ignored for all of the experiments on that database. Four queries do not have judgments on AP88-89, and 49 queries do not have judgments on ZIFF1-2. Preprocessing of the documents is minimized; only a Porter stemmer is used, and no stop words are removed. Combining the four types of queries with the three databases gives us a total of 12 different testing collections.</p><p>To understand the interaction between different query factors and smoothing, we examine the sensitivity of retrieval performance to smoothing on each of the four different types of queries. For both Jelinek-Mercer and Dirichlet smoothing, on each of our 12 testing collections we vary the value of the smoothing parameter and record the retrieval performance at each parameter value. The results are plotted in Figure <ref type="figure" target="#fig_1">2</ref>. In each case, we show how the average precision varies according to different values of the smoothing parameter.</p><p>From these figures, we see that the two types of keyword queries behave similarly, as do the two types of verbose queries. The retrieval performance is generally much less sensitive to smoothing in the case of the keyword queries than for the verbose queries, whether long or short. Therefore, the sensitivity is much more correlated with the verbosity of the query than with the length of the query. Indeed, the short verbose queries are clearly more sensitive than the long keyword queries. In all cases, insufficient smoothing is much more harmful for verbose queries than for keyword queries. This confirms that smoothing is indeed responsible for "explaining" the common words in a query, and provides an empirical justification for the two-stage smoothing approach.</p><p>We also see a consistent order of performance among the four types of queries. As expected, long keyword queries are the best and short verbose queries are the worst. Long verbose queries are worse than long keyword queries, but better than short keyword queries, which are better than the short verbose queries. This appears to suggest that queries with only (presumably good) keywords tend to perform better than more verbose queries. Also, longer queries are generally better than short queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness of the Two-stage Smoothing Method</head><p>To evaluate the two-stage smoothing method, we first test it on the same 12 testing collections as described earlier. These collections represent a very good diversity in the types of queries, but the databases are all homogeneous and relatively small. In order to further test the robustness of the two-stage smoothing method, we then test it on three much bigger and more heterogeneous TREC collections. These are the official ad hoc retrieval collections used in TREC-7, TREC-8, and the TREC-8 small web track. The official TREC-7 and TREC-8 ad hoc tasks have used the same document database (i.e., TREC disk4 and disk5 excluding the Congressional Record data), but different topics (topics 351-400 for TREC-7 and 401-450 for TREC-8). The TREC-8 web track and the TREC-8 official ad hoc task share the same 50 topics. Since these topics do not have a concept field, we have only three types of queries: shortkeyword, short-verbose, and long-verbose. The size of these large collections is about 2GB in the original source. Again, we perform minimum pre-processing -only a Porter stemmer was used, and no stop words were removed.</p><p>For each testing collection, we compare the retrieval performance of the estimated two-stage smoothing parameters with the best results achievable using a single smoothing method. The best results of a single smoothing method are obtained through an exhaustive search on its parameter space, so are the ideal performance of the smoothing method. In all our experiments, we use the collection language model to approximate the query background model.</p><p>The results are shown in Table <ref type="table" target="#tab_2">2</ref>. The four types of queries are abbreviated with the two initial letters (e.g., SK for Short-Keyword). The standard TREC evaluation procedure for ad hoc retrieval is followed, and we have considered three performance measuresnon-interpolated average precision, initial precision (i.e., precision at 0.0 recall), and precision at five documents. In all the results, we see that, the performance of two-stage smoothing with the estimated parameter values is consistently very close to, or better than the best performance of a single method by all the three measures. Only in a few cases, the difference is statistically significant (indicated with a star).</p><p>To quantify the sensitivity of the retrieval performance to the smoothing parameter for single smoothing methods, we also show (in parentheses) the median average precision at all the parameter values that are tried<ref type="foot" target="#foot_1">2</ref> . We see that, for Jelinek-Mercer, the sensitivity is clearly higher on verbose queries than on keyword queries; the median is usually much lower than the best performance for verbose queries. This means that, it is much harder to tune the λ in Jelinek-Mercer for verbose queries than for keyword queries. Interestingly, for Dirichlet prior, the median is often just slightly below the best, even when the queries are verbose. (The worst cases are significantly lower, though.) From the sensitivity curves in Figure <ref type="figure" target="#fig_1">2</ref>, we see that as long as we set a relatively large value for µ in Dirichlet prior, the performance will not be much worse than the best performance, and there is a great chance that the median is at a large value for µ. This immediately suggests that we can expect to perform reasonably well if we simply set µ to some "safe" large value. However, it is clear, from the results in Table 2, that such a simple approach would not perform so well as our parameter estimation methods. Indeed, the two-stage performance is always better than the median, except for three cases of short-keyword queries when it is slightly worse. Since the Dirichlet prior smoothing dominates the two-stage smoothing effect for these short-keyword queries (due to "little noise"), this somehow suggests that the leave-one-out method might have under-estimated µ.</p><p>Note that, in general, Jelinek-Mercer has not performed as well as Dirichlet prior in all our experiments. But, in two cases of verbose queries (Trec8-SV and Trec8-LV on the Trec7/8 database), it does outperform Dirichlet prior. In these two cases, the twostage smoothing method performs either as well as or better than the Jelinek-Mercer. Thus, the two-stage smoothing performance appears to always track the best performing single method at its optimal parameter setting.</p><p>The performance of two-stage smoothing does not reflect the performance of a "full-fledged" language modeling approach, which would involve more sophisticated feedback models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref>. Thus, it is really not comparable with the performance of other TREC systems. Yet some of the performance figures shown here are actually competitive when compared with the performance of the official TREC submissions (e.g., the performance on the TREC-8 ad hoc task and the TREC-8 web track).</p><p>These results of the two-stage smoothing method are very encouraging, especially because there is no ad hoc parameter tuning involved in the retrieval process with the approach. Both µ and λ are automatically estimated based on a specific database and query; µ is completely determined by the given database, and λ is deter- mined by the database and the query together. The method appears to be quite robust according to our experiments with all the different types of queries and different databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper we derive general two-stage language models for information retrieval using the risk minimization retrieval framework, and present a concrete two-stage smoothing method as a special case. The two-stage smoothing strategy explicitly captures the different influences of the query and document collection on the optimal settings of smoothing parameters. In the first stage, the document language model is smoothed using a Dirichlet prior with the collection model as the reference model. In the second stage, the smoothed document language model is further interpolated with a query background model.</p><p>We propose a leave-one-out method for estimating the first-stage Dirichlet prior parameter and a mixture model for estimating the second-stage interpolation parameter. These methods allow us to set the retrieval parameters automatically, yet adaptively according to different databases and queries. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation scheme consistently gives retrieval performance that is close to, or better than, the best results attainable using a single smoothing method, achievable only through an exhaustive parameter search. The effectiveness and robustness of the two-stage smoothing approach, along with the fact that there is no ad hoc parameter tuning involved, make it a solid baseline approach for evaluating retrieval models.</p><p>While we have shown that the automatic two-stage smoothing gives retrieval performance close to the best results attainable using a single smoothing method, we have not yet analyzed the optimality of the estimated parameter values in the two-stage parameter space. For example, it would be important to see the relative optimality of the estimated µ and λ when fixing one of them. It would also be interesting to explore other estimation methods. For example, µ might be regarded as a hyperparameter in a hierarchical Bayesian approach. For the estimation of the query model parameter λ, it would be interesting to try different query background models. One possibility is to estimate the background model based on resources such as past queries, in addition to the collection of documents. Another interesting future direction is to exploit the query background model to address the issue of redundancy in the retrieval results. Specifically, a biased query background model may be used to represent/explain the sub-topics that a user has already encountered (e.g., through reading previously retrieved results), in order to focus ranking on the new sub-topics in a relevant set of documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example topic, number 52. The keywords are used as the "long keyword" version of our queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sensitivity of Precision for Jelinek-Mercer smoothing (top) and Dirichlet prior smoothing (bottom) on AP88-89 (left), WSJ87-92 (center), and Ziff1-2 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Estimated values of µ along with database characteristics.</head><label>1</label><figDesc>Collection avg. doc length max. doc length vocab. size</figDesc><table><row><cell>μ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Comparison of the estimated two-stage smoothing with the best single stage smoothing methods on small collections (top) and large collections (bottom). The best number for each measure is shown in boldface. An asterisk (*) indicates that the difference between the two-stage smoothing performance and the best single smoothing performance is statistically significant according to the Wilcoxin signed rank test at the level of 0.05.</head><label>2</label><figDesc></figDesc><table><row><cell>Database</cell><cell>Query</cell><cell>Best Jelinek-Mercer</cell><cell></cell><cell>Best Dirichlet</cell><cell>Two-Stage</cell></row><row><cell></cell><cell></cell><cell cols="2">AvgPr (med) InitPr Pr@5d</cell><cell>AvgPr (med) InitPr Pr@5d</cell><cell>AvgPr Initpr Pr@5d</cell></row><row><cell>AP88-89</cell><cell>SK</cell><cell cols="2">0.203 (0.194) 0.573 0.341</cell><cell>0.230 (0.224) 0.623 0.381</cell><cell>0.222* 0.611 0.375</cell></row><row><cell></cell><cell>LK</cell><cell cols="2">0.368 (0.362) 0.767 0.530</cell><cell>0.376 (0.368) 0.755 0.533</cell><cell>0.374 0.754 0.533</cell></row><row><cell></cell><cell>SV</cell><cell cols="2">0.188 (0.158) 0.569 0.342</cell><cell>0.209 (0.195) 0.609 0.379</cell><cell>0.204 0.598 0.368</cell></row><row><cell></cell><cell>LV</cell><cell cols="2">0.288 (0.263) 0.711 0.463</cell><cell>0.298 (0.285) 0.704 0.490</cell><cell>0.292 0.689 0.473</cell></row><row><cell>WSJ87-92</cell><cell>SK</cell><cell cols="2">0.194 (0.188) 0.629 0.392</cell><cell>0.223 (0.218) 0.660 0.438</cell><cell>0.218* 0.662 0.450</cell></row><row><cell></cell><cell>LK</cell><cell cols="2">0.348 (0.341) 0.814 0.597</cell><cell>0.353 (0.343) 0.834 0.608</cell><cell>0.358 0.850* 0.607</cell></row><row><cell></cell><cell>SV</cell><cell cols="2">0.172 (0.158) 0.615 0.377</cell><cell>0.196 (0.188) 0.638 0.413</cell><cell>0.199 0.660 0.425</cell></row><row><cell></cell><cell>LV</cell><cell cols="2">0.277 (0.252) 0.768 0.533</cell><cell>0.282 (0.270) 0.750 0.504</cell><cell>0.288* 0.762 0.520</cell></row><row><cell>ZF1-2</cell><cell>SK</cell><cell cols="2">0.179 (0.170) 0.455 0.248</cell><cell>0.215 (0.210) 0.514 0.301</cell><cell>0.200 0.494 0.299</cell></row><row><cell></cell><cell>LK</cell><cell cols="2">0.306 (0.290) 0.675 0.404</cell><cell>0.326 (0.316) 0.681 0.438</cell><cell>0.322 0.696 0.434</cell></row><row><cell></cell><cell>SV</cell><cell cols="2">0.156 (0.139) 0.450 0.224</cell><cell>0.185 (0.170) 0.456 0.255</cell><cell>0.181 0.487 0.271*</cell></row><row><cell></cell><cell>LV</cell><cell cols="2">0.267 (0.242) 0.593 0.339</cell><cell>0.279 (0.273) 0.606 0.378</cell><cell>0.279* 0.618 0.384</cell></row><row><cell>Database</cell><cell>Query</cell><cell>Best Jelinek-Mercer</cell><cell></cell><cell>Best Dirichlet</cell><cell>Two-Stage</cell></row><row><cell></cell><cell></cell><cell cols="2">AvgPr (med) InitPr Pr@5d</cell><cell>AvgPr (med) InitPr Pr@5d</cell><cell>AvgPr Initpr Pr@5d</cell></row><row><cell></cell><cell>Trec7-SK</cell><cell cols="2">0.167 (0.165) 0.632 0.400</cell><cell>0.186 (0.182) 0.688 0.432</cell><cell>0.182 0.673 0.420</cell></row><row><cell></cell><cell>Trec7-SV</cell><cell cols="2">0.173 (0.138) 0.646 0.416</cell><cell>0.182 (0.168) 0.656 0.436</cell><cell>0.181 0.655 0.416</cell></row><row><cell></cell><cell>Trec7-LV</cell><cell cols="2">0.222 (0.195) 0.723 0.496</cell><cell>0.224 (0.212) 0.763 0.524</cell><cell>0.230 0.760 0.516</cell></row><row><cell>Trec7/8</cell><cell>Trec8-SK</cell><cell>0.239 (0.237) 0.621</cell><cell>0.44</cell><cell>0.256 (0.244) 0.717 0.488</cell><cell>0.257 0.719 0.496</cell></row><row><cell></cell><cell>Trec8-SV</cell><cell cols="2">0.231 (0.192) 0.687 0.456</cell><cell>0.228 (0.222) 0.670 0.432</cell><cell>0.231 0.719 0.484</cell></row><row><cell></cell><cell>Trec8-LV</cell><cell cols="2">0.265 (0.234) 0.789 0.556</cell><cell>0.260 (0.252) 0.753 0.492</cell><cell>0.268 0.787 0.524</cell></row><row><cell></cell><cell>Trec8-SK</cell><cell cols="2">0.243 (0.212) 0.607 0.368</cell><cell>0.294 (0.281) 0.756 0.484</cell><cell>0.278* 0.730 0.488</cell></row><row><cell>Web</cell><cell>Trec8-SV</cell><cell cols="2">0.203 (0.191) 0.611 0.392</cell><cell>0.267 (0.249) 0.699 0.492</cell><cell>0.253 0.680 0.436</cell></row><row><cell></cell><cell>Trec8-LV</cell><cell cols="2">0.259 (0.234) 0.790 0.464</cell><cell>0.275 (0.248) 0.752 0.508</cell><cell>0.284 0.781 0.508</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Occasionally, a few function words were manually excluded, in order to make the queries purely keyword-based.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For Jelinek-Mercer, we tried 13 values {0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,0.95, 0.99}; for Dirichlet prior, we tried 10 values {100, 500, 800, 1000, 2000,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>3000, 4000, 5000, 8000, 10000}.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Rong Jin, Jamie Callan, and the anonymous reviewers for helpful comments on this work. This research was sponsored in full by the Advanced Research and Development Activity in Information Technology (ARDA) under its Statistical Language Modeling for Information Retrieval Research Program, contract MDA904-00-C-2106.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information retrieval as statistical translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Twenty-one at TREC-7: Ad-hoc and cross-language track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Seventh Text REtrieval Conference</title>
		<meeting>of Seventh Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving two-stage ad-hoc retrieval for short queries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;98</title>
		<meeting>SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">98</date>
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001a. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic IR models based on query and document generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Modeling and IR workshop</title>
		<meeting>the Language Modeling and IR workshop</meeting>
		<imprint>
			<date type="published" when="2001">2001b</date>
		</imprint>
	</monogr>
	<note>Extended abstract</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relevance-based language models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hidden Markov model information retrieval system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the estimation of &apos;small&apos; probabilities by leaving-one-out</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1202" to="1212" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR</title>
		<meeting>the ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relevance weighting of search terms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>TREC-3</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pivoted document length normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1996 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs.html" />
		<title level="m">Proceedings of Text REtrieval Conference (TREC1-9)</title>
		<meeting>Text REtrieval Conference (TREC1-9)</meeting>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-based feedback in the KL-divergence retrieval model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Information and Knowledge Management</title>
		<imprint>
			<publisher>CIKM</publisher>
			<date type="published" when="2001">2001a. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001b. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
