<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Design Tradeoffs in CXL-Based Memory Pools for Public Cloud Platforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Ernst</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pantea</forename><surname>Zardoshti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Monish</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samir</forename><surname>Rajadnya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Hsu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ishwar</forename><surname>Agarwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Azure</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Virginia</forename><surname>Tech</surname></persName>
						</author>
						<title level="a" type="main">Design Tradeoffs in CXL-Based Memory Pools for Public Cloud Platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DRAM is a key driver of performance and cost in public cloud servers. At the same time, a significant amount of DRAM is underutilized due to fragmented use across servers. Emerging interconnects such as CXL offer a path towards improving utilization through memory pooling. However, the design space of CXL-based memory systems is large, with key questions around the size, reach, and topology of the memory pool. At the same time, using pools requires navigating complex design constraints around performance, virtualization, and management. This paper discusses why cloud providers should deploy CXL memory pools, key design constraints, and observations in designing towards practical deployment. We identify configuration examples with significant positive return of investment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motivation. Many public cloud customers deploy their workloads via virtual machines (VMs). VMs enable performance comparable to on-premises datacenters without the need to manage datacenters. Cloud providers face the challenge of achieving excellent performance at a competitive hardware cost.</p><p>A key driver of both performance and cost is main memory. The gold standard for memory performance is to preallocate a VM with cores and memory on the same socket. This leads to memory latency below 100ns and facilitates virtualization acceleration. At the same time, DRAM has become a major portion of hardware cost due to its poor scaling properties with only nascent alternatives <ref type="bibr" target="#b0">[1]</ref>. For example, DRAM can be over 50% of server cost <ref type="bibr" target="#b1">[2]</ref>.</p><p>Through analysis of Azure VM traces, we identify memory stranding as a dominant source of memory waste and a potential source of cost savings. Stranding happens when all server cores are rented (i.e., allocated to customer VMs) but unallocated memory capacity remains and cannot be rented. We find that up to 30% of DRAM becomes stranded as more cores become allocated to VMs.</p><p>Limitations of the state-of-the-art. Reducing DRAM usage in the public cloud is challenging due to its stringent performance requirements. Pooling memory via memory disaggregation is a promising approach because stranded memory can be returned to the disaggregated pool and used by other servers. Unfortunately, existing pooling systems have microsecond access latencies and require page faults or changes to the VM guest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The emerging CXL interconnect. The emerging Compute Express Link (CXL) interconnect <ref type="bibr" target="#b4">[5]</ref> enables cacheable load/store (ld/st) accesses to pooled memory on many current processors. Pool-memory accesses via loads/stores is a game changer for cloud computing as it allows memory to remain statically preallocated while physically being located in a shared pool. However, CXL access latency depends on the overall system design, especially the pool size (the number of CPU sockets able to use a given pool) and topology. Larger pools require traversing switching levels, which adds significant latency. Additionally, each CXL component adds to the system cost, which must be balanced against stranding savings. This work. This work is motivated by the memory stranding problem identified in Pond <ref type="bibr" target="#b1">[2]</ref> and we paraphrase the stranding analysis in Section 3. While Pond focuses on system software policies and mechanisms for allocating/managing pooled memory, this work focuses on design tradeoffs in the pool's hardware configuration. First, we characterize pool components, possible topologies, and associate memory access latencies. We derive a set of design recommendations from this analysis. Second, we compare savings from memory pooling to the cost of its components for different pool sizes and CXL device types. We find that CXL-based memory pooling can yield significant positive returns on investment. Contrary to the focus of existing literature, smaller pools may be attractive. Third, we discuss future directions for the industry as well as academic research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Cloud resource allocation. Public cloud workloads run inside virtual machines (VMs). To offer performance close to dedicated (non-virtualized) resources, VM resources are statically allocated by reserving each resource (CPU, DRAM, network bandwidth, etc.) for a VM's lifetime. Additionally, providers optimize I/O performance with virtualization accelerators that bypass the hypervisor <ref type="bibr" target="#b5">[6]</ref>. For example, accelerated networking is enabled by default on AWS and Azure. Virtualization acceleration requires statically preallocating (or "pinning") a VM's entire address space <ref type="bibr" target="#b6">[7]</ref>.</p><p>Cloud resource scheduling. Scheduling VMs with heterogeneous multi-dimensional resource demands onto servers leads to a challenging binpacking problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Scheduling is further complicated by constraints such as spreading VMs across multiple failure domains.</p><p>A simplified view of Azure's VM scheduling is that VMs are first assigned to a compute cluster and then placed on a specific server within this cluster. A cluster roughly corresponds to a row of racks with homogenous server configurations. We use the unit of a cluster to characterize our workloads.</p><p>Memory stranding. It is often difficult to provision servers that closely match the resource demands of the incoming VM mix. A common reason is that the DRAM-to-core ratio of a server that will last years must be determined at platform design time and is statically fixed over its lifetime. Additionally, fixed-size DIMMs over limited freedom in determining the DRAM-to-core ratio.</p><p>When the DRAM-to-core ratio of VM arrivals and a cluster's server resources do not match, tight packing becomes especially difficult. We define a resource as stranded when it is technically available to be rented to a customer, but is practically unavailable as some other resource has been exhausted. The typical scenario for memory stranding is that all cores have been rented, but there is still memory available in the server.</p><p>Reducing stranding via pooling. This work proposes to break the fixed hardware configuration of servers by disaggregating memory into a pool that is accessible by multiple hosts <ref type="bibr" target="#b9">[10]</ref>. By dynamically reassigning memory to different hosts at different times, we can shift memory resources to where they are needed. Thus, we can provision servers close to the average DRAM-tocore ratios and tackle deviations via the memory pool.</p><p>Pooling via CXL. The CXL.mem protocol for ld/st memory semantics maps device memory to the system address space. Last-level cache (LLC) misses to CXL memory addresses translate into requests on a CXL port whose responses bring the missing cachelines. Similarly, LLC write-backs translate into CXL data writes. CXL memory is virtualized using hypervisor page tables and the memory-management unit and is thus compatible with virtualization acceleration.</p><p>CXL.mem uses PCIe's electrical interface with custom link and transaction layers for low latency. Intel measures CXL port latencies at 25ns round-trip <ref type="bibr" target="#b10">[11]</ref>. With PCIe 5.0, the bandwidth  of a bidirectional ?8-CXL port at a typical 2:1 read:write-ratio roughly matches an 80-bit DDR5-4800 channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cloud Workload Characterization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stranding at Azure</head><p>We summarize previous analysis on stranding <ref type="bibr" target="#b1">[2]</ref>.</p><p>Dataset. We measure stranding in 100 generalpurpose clusters over a 75-day period. A generalpurpose cluster hosts a mix of first-party and third-party VM workloads that do not require special hardware (such as GPUs). We select clusters with similar deployment years, spanning major regions on the planet. Each cluster trace contains millions of per-VM arrival/departure events. Memory stranding. Figure <ref type="figure" target="#fig_0">1a</ref> shows the hourly average amount of stranded DRAM across our cluster sample, bucketed by the percentage of scheduled CPU cores. In clusters where 75% of CPU cores are scheduled for VMs, 6% of memory is stranded. This grows to over 10% when ?85% of CPU cores are allocated to VMs. This makes sense since stranding is an artifact of highly utilized nodes, which correlates with highly utilized clusters. Outliers are shown by the error bars, representing 5 th and 95 th percentiles. At 95 th , stranding reaches 25% during high utilization periods. Individual outliers reach more than 30% stranding.</p><p>Figure <ref type="figure" target="#fig_0">1b</ref> shows stranding over time across eight adjacent racks. Every row shows a server within each rack. A workload change (around day 36) suddenly increased stranding significantly. Furthermore, stranding can affect many racks concurrently (e.g., racks 2, 4-7) and it is generally hard to predict which clusters/racks will have stranded memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VM Memory Utilization in Azure</head><p>Dataset. We perform measurements on the same 100 general-purpose production clusters. For untouched memory, we rely on guest-reported memory usage counters cross-referenced with hypervisor page table access bit scans. We sample memory bandwidth counters using Intel RDT <ref type="bibr" target="#b11">[12]</ref> for a subset of clusters with compatible hardware. Finally, we use hypervisor counters to measure non-uniform memory access (NUMA) spanning in dual-socket servers, where a VM has cores on one socket and some memory from another socket.</p><p>Memory bandwidth. Memory bandwidth usage of general-purpose workloads is generally low with average bandwidth utilization below 10 GB/s. VMs on a small number of hosts do, however, use 100% of memory bandwidth.</p><p>NUMA spanning. Most VMs are small and can fit on a single socket. Azure's hypervisor aims to schedule VMs on dual-socket servers such that they fit entirely (cores and memory) on a single NUMA node. We find that spanning occurs for only 2-3% of VMs.</p><p>Overall, untouched memory and low memory bandwidth requirements make VM workloads a good fit for memory pooling. However, with 97-98% of VMs using NUMA-local memory, performance parity for pooled memory will be challenging.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Workload Sensitivity to Memory Latency</head><p>We summarize previous experiments on latency sensitivity <ref type="bibr" target="#b1">[2]</ref>.</p><p>Dataset. We evaluate 158 workloads across proprietary workloads, in-memory stores, data processing, and benchmark suites. They run on dualsocket Intel Skylake 8157M, with a 182% latency increase for socket-remote memory, or AMD EPYC 7452, with 222% latency increase. We normalize performance as slowdown relative to NUMA-local performance.</p><p>Latency sensitivity. Figure <ref type="figure">2</ref> surveys workload slowdowns. Under a 182% increase in memory latency, we find that 26% of the 158 workloads experience less than 1% slowdown under CXL. At the same time, some workloads are severely affected with 21% of the workloads facing &gt;25% slowdowns. Overall, every workload class has at least one workload with less than 5% slowdown and one workload with more than 25% slowdown (except SPLASH2x). Our proprietary workloads are less impacted than the overall workload set with almost half seeing &lt;1% slowdown. These production workloads are NUMA-aware and often include data placement optimizations.</p><p>Under a 222% increase in memory latency, we find that 23% of the 158 workloads experience less than 1% slowdown under CXL. More than 37% of workloads face &gt;25% slowdowns -a significantly higher fraction than on the 182% emulated latency increase. We find that the processing pipeline for some workloads, like VoltDB, seems to have just enough slack to accomodate the smaller 182% latency increase with significant pipeline stalls for 222% latency increase. Other workload classes like graph processing (GAPBS) are sensitive to both latency and bandwidth, and both effects are worsened on the 222% system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Memory Pool Design Space</head><p>Designing a memory pool involves multiple hardware components and design choices that expand with every new CXL release. To limit complexity, we focus on two design aspects: 1) whether to provide connectivity via CXL switches or through CXL multi-headed devices (MHDs) [5, ?2.5] and 2) how large the constructed pool should be to maximize return-oninvestment (ROI). We discuss a particular set of choices suitable for general-purpose cloud computing. Other use cases may see different sets of choices and tradeoffs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Components</head><p>CXL memory controller (MC) devices act as a bridge between the CXL protocol and memory devices such as DDR5 DRAMs. Today's MCs typically bridge between 1-2 CXL ?8 ports and 1-2 80b channels of DDR5 (e.g., <ref type="bibr" target="#b12">[13]</ref>).</p><p>CXL switches behave similar to other network switches in that they forward requests and data, without serving as an endpoint. Physically, CXL switches will likely share many characteristics (e.g., port count) with PCIe switches, due to using the same physical interface. For the purposes of this analysis, we assume that switches with 128lanes (16-ports) of CXL are used to build a fabric layer.</p><p>A CXL MHD essentially combines a switch and a memory controller in a single device. Specifically, the MHD offers multiple CXL ports  and appears to each connected host as a single logical memory device <ref type="bibr" target="#b4">[5]</ref>. The most significant tradeoffs for MHD designs are the number of incoming CXL ports and DDR channels. A useful design comparison is a modern server CPU IOdie (IOD), such as the one in AMD Genoa <ref type="bibr" target="#b13">[14]</ref>. The Genoa IOD offers 128 PCIe5 lanes as well as 12 DDR5 channels. With the ?8-CXL requirement, this would be analogous to a 16-headed device with at least 8 channels of DDR5. In our analysis we consider both this 16-headed device as well as a smaller 8-headed device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pool size vs latency</head><p>At a high level, the first design decision is whether cloud compute servers can pool all of their memory. With 21-37% of workloads facing significant slowdowns on pool-only configurations ( ?3), we do not recommend fully disaggregating compute and memory. Servers need to retain significant amounts of local DRAM to maintain performance expectations, which will likely go beyond the scope of on-die memory. Further, achieving maximum memory bandwidth requires CPUs to populate all available local DDR channels, creating a practical minimum for local memory capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 1:</head><p>A significant percentage (more than 25%) of datacenter memory needs to remain local to compute servers.</p><p>To understand pool latencies, we first characterize the impact on latency of achievable topologies given viable components.</p><p>Observation 2: When using at least a ?8-CXL port for each host, pool sizes beyond 16-32 hosts will require at least one level of switches if MHDs are used or two levels of switches if using only individual MCs.</p><p>Access latencies derive from multiple parameters. Port latency plays a dominant role with initial measurements indicating 25ns <ref type="bibr" target="#b10">[11]</ref>. Retimers are devices used to maintain CXL/PCIe signal integrity over distances above roughly half a meter, depending on the implementation of the signal path. They add about 10ns of latency in each direction (e.g., <ref type="bibr" target="#b14">[15]</ref>). Each switch will add at least 70ns of latency due to ports, arbitration, and network-on-chip (NOC).</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows a range of CXL path types based on pool sizes and the use of MHDs versus  switches with single-headed devices. We find that small 8-and 16-socket pools using MHDs increase latencies to 182-212% (155-180ns) relative to NUMA-local DRAM. Latency when using only switches and single-headed memory controllers would further increase by 23-38%.</p><p>Rack-scale pooling with 64 sockets would increase latencies by 318-405% (270-345ns) and pooling across multiple racks would require yet another level of switching and potentially multiple retimers, increasing latencies by more than 465% (395ns). Comparing these latencies to the slowdowns observable at 182-222% ( ?3), we observe that large-scale pooling will likely be prohibitive from a performance perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 3:</head><p>The size of CXL-based memory pools will likely be a subset of a rack to minimize the performance impact of access latencies.</p><p>Modern CPUs can connect to multiple MHDs or switches, which allows scaling to meet bandwidth and capacity goals for different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pool size vs DRAM savings</head><p>We analyze VM-to-server traces from Azure ( ?3) to estimate the amount of DRAM that could be saved via pools of different sizes. The reduction in DRAM comes from averaging host's peak memory needs across the pool. Our simulation plays back VM traces while assigning a fixed percentage of pool memory. We repeatedly run cluster simulations while decreasing overall memory in 0.5% steps until the first VM is rejected. The minimum amount of cluster memory corresponds to the "required overall DRAM" reported below.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> presents cluster DRAM requirements when VMs are assigned either 10%, 30%, or 50% of pool DRAM. As the pool size increases, the figure shows that required overall DRAM decreases. However, this effect diminishes for larger pools. For example, with a fixed 50% pool DRAM, a pool with 32 sockets saves 12% of DRAM while a pool with 64 sockets saves 13% of DRAM. Note that allocating 50% of VM memory to pool DRAM require latency mitigation techniques ( ?6).</p><p>Besides low latency, feasible configurations also must be ROI positive, as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Pool size vs system cost</head><p>System cost depends on many factors. We consider a simplified model that focuses on key hardware components: DRAM, memory controllers, cables, and the memory blade enclosure/printed circuit board (PCB). Our model ignores factors of time, scale, and market competition. Specifically, our model calculates cost relative to a non-pooled server's bill of materials (BOM) based on the following set of parameters. MC cost of a typical 2x8 CXL memory controller (e.g., 0.4%) MHD8 cost of an 8-headed memory controller (e.g., 0.8%) MHD16 cost of a 16-headed memory controller (e.g., 2.0%) Switch cost of a 16-port CXL switch (e.g., 1.6%) Ret cost of a CXL retimer (e.g., 0.02%) Infra cost of the supporting memory enclosure, PCBs, and cables expressed as a multiplier applied to MHD or switch cost (e.g., 0.5-2?) The exemplary values for the parameters are roughly based on estimates of silicon area as well as connectivity and infrastructure necessary to support the memory pools. Note that there is significant room for these parameters to change between companies, server configurations, use cases, and over time.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> presents cost overheads for pool sizes from 2-64 sockets and for pools encompassing two different capacity points relative to total system memory. The baseline for comparison is the full cost of a non-pooled server, including CPU, DRAM, and other standard infrastructure (e.g. network interface cards (NICs), power delivery, management controllers, boards, etc.). Within this baseline, DRAM memory is assumed to account for approximately half of the total cost, with the CPU and other infrastructure splitting the other half. All other modeled configurations hold the total cost of the base system constant, but add the costs of the extra components required for pooling part of the memory. Our results are reported as a percentage of cost uplift versus the baseline configuration. We vary the infrastructure overhead cost to show that the overall costs are very sensitive to the ability for a design to costeffectively provide connectivity to the pool. The analysis also shows that overhead for switchbased designs versus MHD designs is significant. As an example, an 8-socket pool implemented with switches adds over two-and-a-half times the cost of an 8-socket pool based on MHDs.</p><p>This overhead is important, as the systemlevel goal is reaching a beneficial pooling configuration, which is one where the cost uplift of moving memory into the pool is less than the efficiency benefit of having flexible memory as outlined in the savings analysis above. In Figure <ref type="figure" target="#fig_5">5</ref>, the black line plots the savings estimate from the earlier analysis (Figure <ref type="figure" target="#fig_4">4</ref>). Configurations below this line are ROI positive, while those above the line are likely ROI negative unless further optimizations can be made to improve savings. Note in particular that most switch-based configurations are ROI negative, while many MHDbased configurations are ROI positive, especially for smaller pool sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 4:</head><p>Positive ROI requires pool designers to navigate a complex tradeoff between pool size, topology, and savings, which is workload dependent. Infrastructure overheads may become a major hurdle to adopting CXL-based pooling as expensivelydesigned configurations will not achieve beneficial ROI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Low memory resource utilization and stranding has been observed at Google <ref type="bibr" target="#b15">[16]</ref> and Microsoft <ref type="bibr" target="#b16">[17]</ref>. This motivated at least three lines of prior research on memory pooling prior to CXL. Hypervisor/OS level approaches such as <ref type="bibr" target="#b2">[3]</ref> rely on page faults and access monitoring to maintain the working set in local DRAM. In the context of general-purpose cloud computing, these OS-based approaches bring too much overhead and jitter. They are also incompatible with virtualization acceleration (e.g., DDA).</p><p>Runtime-based disaggregation designs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> propose customized application programming interfaces for remote memory access. While effective, this approach requires developers to explicitly use these mechanisms at the application level.</p><p>Hardware-based memory disaggregation have served as an inspiration for CXL but prior approaches were not available on commodity hardware <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Prior analysis of requirements for disaggregation are related to our goals. However, networkbased disaggregation <ref type="bibr" target="#b19">[20]</ref> lead to a different design space, e.g., with latency considered in the range of 1us to 40us, whereas we consider latencies lower by an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>CXL-based memory pooling promises to reduce DRAM needs for general-purpose cloud platforms. This paper outlines the design space for memory pooling and offers a framework to evaluate different proposals.</p><p>As cloud datacenters are quickly evolving, some key parameters will differ significantly even among cloud providers and over time. The fraction of VM memory that can be allocated on CXL pools depends largely on the type of latency mitigation employed. For example, the recent Pond <ref type="bibr" target="#b1">[2]</ref> system can allocate an average of 35-44% of DRAM on CXL pools while satisfying stringent cloud performance goals. Future techniques for performance management may lead to significantly higher CXL pool usage. Another difference comes from server and infrastructure cost breakdowns, which lead to entirely different cost curves (Figure <ref type="figure" target="#fig_5">5</ref>).</p><p>Regardless of the variability in system and cost parameters, we believe that Observations 1-4 broadly apply to general-purpose clouds. We highlight that small pools, spanning up to 16 sockets, can lead to significant DRAM savings. This requires keeping infrastructure cost overheads low, which reinforces the need for standardization of pooling infrastructure. Latency and cost increase quickly for larger pool sizes, while the efficiency benefits fall off, which may make large pools counterproductive in many scenarios.</p><p>Our savings model focuses on pooling itself, e.g., averaging peak DRAM demand across the pool, and for Azure specific workloads. CXL also enables other savings including using cheaper media behind a CXL controller, such as reusing DDR4 from decommissioned servers. We advise practitioners to create a savings model for their specific use cases, which might differ from ours.</p><p>CXL re-opens memory controller architecture as a research frontier. With memory controllers decoupled from CPU sockets, new controller features can be more quickly explored and deployed. Cloud providers need improved reliability, availability, and serviceability (RAS) capabilities including memory error correction, management, and isolation. Tighter integration between memory chips, modules, and controllers can enable improvements along the Pareto frontier of RAS, memory bandwidth, and latency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Memory stranding. (a) Stranding increases significantly as more CPU cores are scheduled; (b) Stranding changes dynamically over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>YCSB A?F ML/Web, etc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pool size and latency tradeoffs. Small pools of 8-16 sockets add only 75-90ns relative to NUMA-local DRAM. Latency increases for larger pools that require retimers and a switch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact of pool size. Small pools of 32 sockets are sufficient to significantly reduce overall memory needs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Pool system cost tradeoffs. Both cost and savings increase with pool size. Infrastructure overheads also play a key factor in cost. Cost savings (black line) from Figure 4 are workload dependent and may look significantly different for other use cases. We advise practitioners to evaluate savings for their workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>b] Stranding over time</head><label></label><figDesc></figDesc><table><row><cell cols="4">10 20 30 40 0 [a] Stranding vs. 5 th Percentile 95 th Percentile Stranded Memory [%] 60 70 CPU Cores Scheduled Outliers 80 in this Cluster [%] CPU utilization</cell><cell>90</cell></row><row><cell>Racks</cell><cell>7 6</cell><cell>Rack 7 Servers Rack 6 Servers</cell></row><row><cell></cell><cell></cell><cell>Whitespace: no stranding 0</cell><cell>15 Black pixel: stranding &gt; 1%</cell><cell>30</cell></row><row><cell cols="4">0 1 2 3 4 5 6 7 [Servers in racks 5/6: long periods Racks 0 15 30 45 60 75 Time [Days] (server strands of stranding &gt; 1% of memory &lt; 1% of memory)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Pool designs with multi-headed device (MHD) Pool designs with only switches and single-headed memory controller (MC)</head><label></label><figDesc></figDesc><table><row><cell>2-8 Socket Pool</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CPU CPU CPU</cell><cell cols="2">CXL .. .</cell><cell cols="3">MHD</cell><cell cols="2">DDR5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Core/LLC /Fabric 40ns</cell><cell>CXL Port 25ns</cell><cell>5ns</cell><cell>MHD ACL NOC 25ns 15ns CXL Port</cell><cell>MC &amp; DRAM 45ns</cell><cell>Load to use 155ns, 182%</cell></row><row><cell>16 Socket Pool</cell><cell></cell><cell></cell><cell cols="2">CPU CPU CPU</cell><cell cols="2">.. .</cell><cell>R R R</cell><cell>CXL</cell><cell cols="3">MHD</cell><cell></cell><cell cols="2">DDR5</cell><cell></cell><cell></cell><cell cols="2">Core/LLC /Fabric 40ns</cell><cell>CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>MHD 25ns 15ns CXL Port ACL NOC</cell><cell>MC &amp; DRAM 45ns</cell><cell>Load to use 180ns, 212%</cell></row><row><cell>32-64 Pool Socket</cell><cell cols="2">CPU CPU CPU .. . R R R</cell><cell></cell><cell cols="2">CXL</cell><cell cols="6">CXL R R R . . . Switch</cell><cell cols="2">MHD</cell><cell>DDR5</cell><cell>Core/LLC /Fabric 40ns</cell><cell cols="2">CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>CXL Port 25ns</cell><cell>Switch 20ns ARB NOC</cell><cell>CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>MHD 25ns 15ns CXL Port ACL NOC</cell><cell>MC &amp; DRAM 45ns</cell><cell>&gt;270ns, 318% Load to use</cell></row><row><cell>2-8 Socket Pool</cell><cell>CPU CPU CPU</cell><cell>.. .</cell><cell></cell><cell cols="2">CXL</cell><cell cols="4">Switch . . .</cell><cell></cell><cell cols="2">CXL</cell><cell>MC</cell><cell>DDR5</cell><cell cols="3">Core/LLC /Fabric 40ns</cell><cell>CXL Port 25ns</cell><cell>5ns</cell><cell>CXL Port 25ns</cell><cell>Switch 20ns ARB NOC</cell><cell>CXL Port 25ns</cell><cell>5ns</cell><cell>CXL Port</cell><cell>MC MC &amp; DRAM 45ns</cell><cell>&gt;190ns, 224% Load to use</cell></row><row><cell>16 Socket Pool</cell><cell>CPU CPU CPU</cell><cell cols="2">.. . R R R</cell><cell cols="2">CXL</cell><cell cols="4">. . . Switch</cell><cell>R R</cell><cell cols="2">CXL R</cell><cell>MC</cell><cell>DDR5</cell><cell cols="2">Core/LLC /Fabric 40ns</cell><cell cols="2">CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>CXL Port 25ns</cell><cell>Switch 20ns ARB NOC</cell><cell>CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>MC 25ns CXL Port MC &amp; DRAM 45ns</cell><cell>Load to use &gt;250ns, 294%</cell></row><row><cell>32-64 Socket Pool</cell><cell cols="2">CXL .. CPU CPU CPU . R R R</cell><cell cols="11">MC CXL R R R . . Switch . CXL R R R . . Switch .</cell><cell>DDR5</cell><cell>Core/LLC /Fabric 40ns</cell><cell></cell><cell>CXL Port 25ns</cell><cell>5+20+5ns Re-timer</cell><cell>CXL Port 25ns</cell><cell>Switch 20ns ARB NOC</cell><cell>CXL Port 25ns</cell><cell>...</cell><cell>5+20+5ns Re-timer</cell><cell>MC 25ns CXL Port MC &amp; DRAM 45ns</cell><cell>Load to use &gt;345ns, 405%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Published by the IEEE Computer Society ? 2023 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>March-April 2023</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scaling and Performance Challenges of Future DRAM</title>
		<author>
			<persName><forename type="first">Shigeru</forename><surname>Shiratake</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In IMW &apos;20</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pond: CXL-Based Memory Pooling Systems for Cloud Platforms</title>
		<author>
			<persName><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pantea</forename><surname>Zardoshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monish</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Rajadnya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishwar</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;23</title>
		<imprint>
			<biblScope unit="page" from="574" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient Memory Disaggregation with Infiniswap</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">AIFM: High-Performance, Application-Integrated Far Memory</title>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In OSDI &apos;20</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://www.computeexpresslink.org/download-the-specification" />
		<title level="m">CXL Specification</title>
		<imprint>
			<date type="published" when="2020-12">December 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LeapIO: Efficient and Portable Virtual NVMe Storage on ARM SoCs</title>
		<author>
			<persName><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Gogte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><surname>Badam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;20</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Page Fault Support for Network Controllers</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Lesokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagi</forename><surname>Grimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<idno>ASPLOS &apos;17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Protean: VM Allocation Service at Scale</title>
		<author>
			<persName><forename type="first">Ori</forename><surname>Hadary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Esaias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Greeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Star</forename><surname>Dion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailesh</forename><surname>Dorminey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><surname>Moscibroda</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In OSDI &apos;20</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;17</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ThymesisFlow: A Software-Defined, HW/SW Co-Designed Interconnect Stack for Rack-Scale Memory Disaggregation</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Syrivelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Gazzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Koutsovasilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Katrinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hofstee</surname></persName>
		</author>
		<idno>MICRO-53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Compute Express Link: An Open Industry-standard Interconnect Enabling Heterogenous Datacentric Computing</title>
		<author>
			<persName><forename type="first">Debendra</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharma</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>HotI29</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html" />
		<title level="m">Intel Resource Director Technology (Intel RDT)</title>
		<imprint>
			<date type="published" when="2015">September 2022, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Memory Connectivity Platform for CXL 1.1 and 2.0</title>
		<author>
			<persName><surname>Asteralabs</surname></persName>
		</author>
		<author>
			<persName><surname>Leo</surname></persName>
		</author>
		<ptr target="https://www.asteralabs.com/wp-content/uploads/2022/08/AsteraLabsLeoAuroraProductFINAL.pdf" />
		<imprint>
			<date type="published" when="2022-08">August 2022, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Su</surname></persName>
		</author>
		<ptr target="https://www.amd.com/en/press-releases/2021-11-08-amd-unveils-workload-tailored-innovations-and-products-the-accelerated" />
		<title level="m">AMD Unveils Workload-Tailored Innovations and Products at The Accelerated Data Center Premiere</title>
		<imprint>
			<date type="published" when="2021-11">November 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://www.microchip.com/en-us/about/blog/learning-center/cxl--use-cases-driving-the-need-for-low-latency-performance-reti" />
		<title level="m">CXL Use-cases Driving the Need For Low Latency Performance Retimers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">the Next Generation</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Tirmazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><surname>Zhijing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><surname>Wilkes</surname></persName>
		</author>
		<imprint>
			<pubPlace>Borg</pubPlace>
		</imprint>
	</monogr>
	<note>In EuroSys &apos;20</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Redy: Remote Dynamic Memory Cache</title>
		<author>
			<persName><forename type="first">Qizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badrish</forename><surname>Chandramouli</surname></persName>
		</author>
		<idno>VLDB &apos;22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rethinking Software Runtimes for Disaggregated Memory</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talha Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Kolli</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ASPLOS &apos;21</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Clio: A Hardware-Software Co-Designed Disaggregated Memory System</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ASPLOS &apos;22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Network Requirements for Resource Disaggregation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Shenker</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In OSDI &apos;16</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
