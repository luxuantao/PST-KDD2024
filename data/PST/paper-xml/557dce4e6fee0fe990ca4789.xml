<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Reconstruction Aspects of Moment Descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Miroslaw</forename><surname>Pawlak</surname></persName>
						</author>
						<title level="a" type="main">On the Reconstruction Aspects of Moment Descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">38CDAFA638D7D7D83F798DC096C0F70B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reconstruction from moments</term>
					<term>orthogonal moments</term>
					<term>reconstruction error</term>
					<term>optimal .number of moments</term>
					<term>cross-validation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of reconstruction of an image from discrete and noisy data by the method of moments is examined. The set of orthogonal moments based on Legendre polynomials is employed. A general class of signal-dependent noise models is taken into account. An asymptotic expansion for the global reconstruction error is established. This reveals mutual relationships between a number of moments, the image smoothness, sampling rate and noise model characteristics. The problem of an automatic (data-driven) selection of an optimal number of moments is studied. This is accomplished with the help of cross-validation techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>OMENT descriptors of various forms have been M extensively employed as pattern features in scene recognition, registration, object matching as well as data compression, see <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr">[lo]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[161, [22]</ref>, <ref type="bibr">[251, and references cited therein.</ref> Some examples of moments include geometric, complex, Fourier-Mellin, radial, and orthogonal moments, <ref type="bibr">[l]</ref>, <ref type="bibr">[31,</ref><ref type="bibr">[161,</ref><ref type="bibr">[221,</ref><ref type="bibr">[W,</ref><ref type="bibr">WI,</ref><ref type="bibr">[301.</ref> Their invariance properties along with numerous applications have been widely studied in the literature. However, the fundamental problems concerning the robustness for noise and digitizing have been rarely addressed <ref type="bibr">[l]</ref>, <ref type="bibr">[30]</ref>, <ref type="bibr">[31]</ref>. Such issues can be carried out by verifying the accuracy of a reconstruction method derived from moments. That is, how close can we recover the original image from a finite set of moments computed from observed data? Certainly, the higher order moments suffer greater degradation due to noise. On the other hand, they are able to supply the detail information about the image. These two opposite factors working against one another imply that there exists an optimal number of moments yielding the best possible representation (with respect to a certain reconstruction measure) of the image <ref type="bibr">[l]</ref>, <ref type="bibr">[301.</ref> In this paper, the inverse moment problem of reconstruction of the image from a finite set of moments derived from the discrete and noisy data is examined.</p><p>Manuscript received <ref type="bibr">August 30, 1990;</ref><ref type="bibr">revised January 17, 1992</ref>. This work was supported by NSERC Grant A8131. This work was presented in part at the IEEE International Symposium on Information Theory, San Diego, CA, January <ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">1990</ref>.</p><p>The author is with the Department of Electrical and Computer</p><p>Engineering, University of Manitoba, Winnipeg, MB, R3T 2N2, Canada.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Log Number 910822</head><p>The mean integrated-square reconstruction error between the image and its reconstructed version is evaluated in the case of the orthogonal Legendre moments. This reveals how the error, as well as, the optimal number or moments depend on the image smoothness, noise characteristic, and sampling rate. In particular, a relation between a number of moments and sampling rate yielding the convergence property (the error tends to zero) is established. The rate of convergence is computed and the optimal number of moments minimizing the error is determined. Also, the boundary effect producing the deterioration of the reconstruction procedure is exhibited. The further problem we address is how the optimal number of moments can be selected directly from the observed data record. That is, a data-driven method of selecting the optimal number of moments is proposed. This is carried out by employing the cross-validations technique <ref type="bibr" target="#b9">[12]</ref>, [261. The optimality result of that choice is demonstrated.</p><p>We use orthogonal moments due to a number of reasons. The classical geometric moments having the form of projection of the image function onto the monomials ( x P y 9 ) are not orthogonal. Consequently, the recovery of image from these moments is computationally expensive and strongly ill-posed [5], <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr">[28]</ref>, <ref type="bibr">[33]</ref>. Indeed, the fundamental reasons for the ill-posedness of the inverse moment problem is the serious lack of orthogonality of the sequence { x p y q } [281.</p><p>Hence, small perturbations in initial data lead to large variations in the solution of the reconstruction problem. To fix this deficiency a regularization approach has been applied <ref type="bibr">[281. Teaque [29]</ref> has introduced the orthogonal moments employing the theory of orthogonal polynomials of two variables to overcome the computational burden associated with classical moments. In particular, he has used the Legendre and Zernike orthogonal polynomials.</p><p>Furthermore, orthogonality implies the linear independence of the corresponding moments, i.e., the orthogonal moments attain a zero value of redundancy measures (they correspond to different characteristics of the image). In addition, the reconstruction procedure does not need regularization, it simply adds the individual contributions of each order moment to generate the reconstructed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">BACKGROUND</head><p>In this section, the definition of orthogonal moments is given and corresponding reconstruction algorithm derived 0018-9448/92$03.00 0 1992 IEEE from the discrete and noisy data is proposed. The broad class of image distortion models is assumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Orthogonal Moments</head><p>Without loss of generality, we can assume that an image f ( x , y) is defined over the region S = [ -1 , l l X [ -1,1]. The orthogonal moment of order k + 1 is of the form 0 I k , 1 I 00, where {@,Jx,y)) is a set of basis functions, orthogonal (or orthonormal after the normalization) over S. Here and throughout the paper, if not otherwise stated, all integrals are in S.</p><p>Two commonly used orthogonal functions are Zernike polynomials and Legendre system <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr">[291, [301.</ref> In this paper, the latter set of functions is employed. However, most of the presented results can be straightforwardly extended to other types of orthogonal functions. The Legendre moments of order k + 1 are defined as in (2.1)</p><formula xml:id="formula_0">with $kl(x,Y) =pk(x)p,(Y), I k , l 5</formula><p>where { p k ( x ) = ((2k + 1)/2)'/2P,(x)) is the orthonormal Legendre system and P,(x) is the kth Legendre polynomial.</p><p>It is known that {p,(x)p,(y)) constitutes an orthonormal basis of the space L2(S), where [23] L 2 ( S ) = {fl j/f2(x, y)dxdy &lt; 4.</p><p>The Legendre moment can be easily related to other types of moments, see <ref type="bibr">[30]</ref>. For example, an explicit relation between A,, and the geometric moments mkl = j jxky' f ( x , y) dwdy is given by The Legendre moments were proposed. by Teaque <ref type="bibr">[29]</ref> and examined later by <ref type="bibr">Teh and Chin [301.</ref> Only the case of continuous images has been considered and some restrictive assumptions concerning the image and noise models have been made. Also, the problem of a data-based selection of number of moments has not been addressed.</p><p>The reconstruction algorithm employing ( N + 112 moments { A k l ) is of the form 1111.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N N</head><p>f N ( X &gt; Y ) = c c A,,p,(x)p,(y).</p><p>(2.2)</p><formula xml:id="formula_1">k = O [ = O</formula><p>Hence, the reconstruction algorithm is just the linear combination of { p,(x)p,(y)} which is the best approximation in the L2(S) norm to f.</p><p>Clearly, the square reconstruction error goes to zero as N + a, i.e., by including higher order moments one can make the error arbitrarily small. This scheme breaks down if the image is contaminated by noise or/and only a finite number of observations is available and moments must be determined from imperfect data. Indeed, in that case an optimal number of moments exists, giving the minimal value of the reconstruction error, i.e., the error is initially decreasing (not necessarily in monotonic way) up to a certain number of moments and then it increases as N + W. In order to verify such properties one has to assume some models for the underlying image as well as the distortion process.</p><p>B, Image Distortion Models Let g ( x , y) be the measured, noisy degraded, version of f ( x , y). The following conditions on g ( x , y ) are assumed</p><formula xml:id="formula_2">x, Y 1 = f ( x, Y 1 &gt; (2.4) cov(g(x,Y),g(x',Y'))</formula><p>for (x,y), ( x ' , y f ) E {(xl,yl): 1 I i I n, 1 I j I m} the set of nm nodes of the grid. The function w(x,y) represents inhomogenity of var(g(x,y)) and often is of the form w(x, y) = &amp;(x, y)), for some nonnegative function</p><formula xml:id="formula_3">q (see examples next).</formula><p>That is, the noisy version g ( x , y) of f(x, y) at a finite number of points is observed. The model in <ref type="bibr">(2.41, (2.5</ref>) admits a great number of noise models encountered in the image processing literature, see <ref type="bibr">[19]</ref>, [201 and references cited therein. Simple examples include a) an addative signal-independent model,</p><formula xml:id="formula_4">g(x,Y) = f ( x &gt; Y ) +Z(X,Y&gt;, w ( x , y ) = I, a* = v a r z ( x , y ) ; b) the film-grain noise, g ( x , Y) = f ( x , Y) + f ' / " x , r &gt; z ( x , Y), w ( x , y ) =f'/'(x,y), a 2 = v a r z ( x , y ) ; c) the Poisson noise, g ( x , y ) = Poisson (Yf(X,Y))/Y,</formula><p>where y &gt; 0 and Poisson ( t ) stands for a Poisson random variable with parameter t . Here w ( x , y) = f ( x , y), a 2 =</p><p>In [30], the reconstruction error has been evaluated under the assumption that f ( x , y) is a real homogeneous random field with zero-mean value and exponentially decaying autocorrelation function. Note, however, that the l/Y.</p><p>presented simulation examples have been given in the deterministic image plus noise framework.</p><p>The difficulty in treating f ( x , y ) as a stochastic field lies in the fact that the reconstruction error cannot be expressed in terms of natural smoothness conditions like continuity, number of derivatives, etc. More precisely, the sample paths of the process { f b , y ) ) have zero probability of being in any class of smooth functions, see <ref type="bibr" target="#b9">[12,</ref><ref type="bibr">Section 5.51 and [33]</ref> for a thorough discussion of this problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reconstruction</head><formula xml:id="formula_5">AX)/^ = y l -( A y ) / 2 = -1, X, + AX)/^ = y , + A x 1 " Note also that - h k , ( x l , y j ) = 1 for k = I = 0 and zero, otherwise.</formula><p>It is worth to note that the integral in (2.7) can be approximated very acurately using well-known techniques for numerical integration [ 111.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">j = l j = ]</head><p>Two simple examples include:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h k / ( x i 7 ~, ) = P k ( x i ) P , ( Y , ) A X A Y and</head><p>These considerations yield the following reconstruction algorithm (2.8)   </p><formula xml:id="formula_6">N A f h Y ) = c c Ak,P,(x)P/(Y),</formula><formula xml:id="formula_7">k = O I = O</formula><p>where i , , is defined by (2.6) and (2.7). As a measure of reconstruction performance we choose the integratedsquare error</p><formula xml:id="formula_8">J ( N ) = / j [ f ( X 7 Y ) -f ( x 7 Y ) ] 2 k d Y *</formula><p>In the next section, we give the error decomposition and its asymptotic evaluation. The problem of data-driven selection of N minimizing J ( N ) is studied in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">THE RECONSTRUCTION ERROR</head><p>Since f E L,(S) it can be represented by c <ref type="figure">c A,</ref><ref type="figure">,</ref><ref type="figure">p,</ref><ref type="figure">(x)p,</ref><ref type="figure">(y</ref> where I( N ) is defined in (2.3), i.e., it is the reconstruction error of the noise and sampling free image. The first term on the right-hand side of (3.1) canAbe viewed as a matching measure between {A,,} and (Ak,} based on the total ( N + 1)2 moments. Clearly, this term either converges to a finite number or to infinity as N + 00. Its asymptotic behavior in the mean-sense is given in Theorem 1. Our results model the performance of the reconstruction procedure on grids which become increasingly fine, i.e., as</p><formula xml:id="formula_9">A x A y -+ 0. k=O [ = O</formula><p>Furthermore, it is plain that</p><formula xml:id="formula_10">E( i,, -A,,)' = var( i k l ) + ( E i k l -A,,)'. (3.2)</formula><p>Here, the first term is due to the noise, whereas the second one is caused by discrete approximation of A,,.</p><p>The asymptotic behavior of these terms is described by the following theorem.</p><p>Theorem 1: Let 0 ~f ( x , y ) I M and w ( x , y ) I W , ( x , y ) E S. Suppose that f is a function of bounded variation on S with the variation V ( f ) . Then, The theorem assumes that the image f is a bivariate function of bounded variation <ref type="bibr" target="#b1">[2]</ref>. This allows discontinuous functions (the natural requirement in image processing).</p><formula xml:id="formula_11">E J ( N ) a'[ a1 + a2 + a N 2 + W ( N + 1)2]AxAy + M I / ( ~) ( N + I ) ~A X A Y + z ( N ) , as A X A Y -0,</formula><p>A few special cases resulting from Theorem 1 are of a particular interest.</p><p>Remark 1: Let the image f be observed under the noise-free condition, i.e., g(x,, y,) = fh,, y,), 1 I i 5 n, 1 5 j I m . Then, under the conditions of Theorem 1, we have</p><formula xml:id="formula_12">J ( N ) = M V ( ~) ( N + I ) ~A X A Y + z ( N ) ,</formula><p>as AxAy -+ 0. Hence, not only the noise but also the discreteness of data yields the existence of an optimal N which corresponds to trading off between the first term and Z(N). Note that Z ( N ) -+ 0 as N + w.</p><p>Remark 2: Let w(x, y) = 1, i.e., let us consider an additive signal independent noise model. Then, under the conditions of Theorem 1, we get</p><formula xml:id="formula_13">E J ( N ) = a 2 ( ~ + I ) ' A X A ~ + M V ( f ) ( N + 1 ) 2 A ~A y + I( N ) , asAxAy -+ 0.</formula><p>Theorem 1 reveals that the higher order moments are vulnerable both to the noise and discrete approximation of the image. Also, the degradation due to the noise model complexity is observed, i.e., the error takes higher values for signal dependent noise models than for independent ones. The latter deficiency can be alleviated by utilizing techniques correcting inhomogenity of the noise variance [8].</p><p>For a fixed AxAy it is clear that E J ( N ) tends to infinity as N -+ W. Nevertheless, one can reduce the error by relating N with AxAy. Hence, let AxAy = cN-7, c &gt; 0. Clearly, if y &gt; 2 then E J ( N ) -+ 0 as N + CQ, i.e., the convergence property is established. For y &lt; 2, E J ( N ) + 03, while for y = 2 we have E J ( N ) -+ c(a + W + M V ( f ) ) , as N -+ w.</p><p>Furthermore, using techniques analogical as in the proof of Theorem 1 one can show that as AxAy -+ 0, for (x, y) E S.</p><p>This reveals that the reconstruction algorithm f N ( x , y ) is strongly disturbed towards the boundyy points of</p><formula xml:id="formula_14">[ -1,1] X [ -1,1].</formula><p>In fact, the variance of f,(x, y ) is unbounded at the boundary points. Clearly, the worst situation occurs for pixels approaching the corner points (-1, -l), ( -1, l), (1, -l), (1,l). In practice this problem can be easily overcome by defining the image plane over a subset of [ -1,1] x [ -1,1]. Similar issue in the context of probability density estimation has been discussed in [141.</p><p>To understand the rate at which E J ( N ) can converge to zero and consequently to determine the moment order minimizing E J ( N ) one has to evaluate the rate at which Z ( N ) goes to zero. The latter necessity requires some assumptions about the smoothness of f(x, y). The smoothness of f(x, y ) is most naturally expressed in terms of the order of magnitude of Legendre moments</p><formula xml:id="formula_15">{ A k l ] . Indeed, if only f E L2(S) then lAkrl --f 0 as k + I -+ CQ.</formula><p>Hence, let</p><formula xml:id="formula_16">(3.3)</formula><p>where q &gt; 0, p &gt; 1/2.</p><p>Functions which have " p derivatives" exhibit such a property, e.g., p = 1 corresponds to functions satisfying where</p><formula xml:id="formula_17">fl = ( 2 p + 1)(2p -1)-12-2/(2P+1)(q2a28-1 a = a 2 ( ff + W ) + M V ( f ) . l )</formula><p>The N * given in Theorem 2 is the moment order which minimizes EJ(N), while EJ(N*) is the corresponding error value.</p><p>Let us assume conditions as in Remark 2 (a signal independent noise model). Then, the optimal N is given by If the effect of discretization is negligible then we can get and where (Y is defined in Theorem 1.</p><formula xml:id="formula_18">a , = 0' + W ( f )</formula><p>Thus, the variable a is a measure of a relative deficiency of signal dependent noise models with respect to signal independent ones. Clearly, (Y = 1 for w ( x , y ) E 1.</p><p>The formula given in Theorem 2 reveals also that N* is a decreasing function of both the image smoothness ( p )</p><p>and the sampling rate. The latter denotes that a high-resolution image requires a larger number of moments for an optimal reconstruction. In practice, however, the N * just proposed cannot be used since it involves unknown characteristics of f ( x , y ) as well as the noise process. The problem of how to choose N directly from the observed data record { g ( x i , yj)} is addressed in the next section.</p><p>To illustrate these results let us consider a simple numerical example. Hence, let f ( x , y ) = f l for (x, y ) E [ -1,0] X [ -1,1] and f ( x , y ) = f 2 , otherwise, i.e., f represents the ideal step edge image along the y axis. Let us also assume_ that the perfect sampling has been designed, i.e., that EA,, = A,,. Consider the following noise models: and where Ez(x, y ) = 0, varz(x, y ) = U ' . A simple algebra shows that Z ( N ) = w -2 -f J 2 1 Furthermore, for the model in ( 3 3 , we have</p><formula xml:id="formula_19">rr N + 2 '</formula><p>Notice that (Y = ( f , + f 2 ) / 2 . The corresponding formula for the model in <ref type="bibr">(3.4)</ref> has the form u 2 ( N + 1 ) 2 A ~A y .</p><p>Thus, for the signal independent noise model, whereas</p><formula xml:id="formula_20">E J ( N ) = a 2 ( ( f l + f 2 ) / 2 ) ( N 2 + 3 ) A x A y</formula><p>for the signal dependent one. Fig. <ref type="figure" target="#fig_7">1</ref> shows the values of E J ( N ) for U' = 0.5, A x = A y = 1/64, f l = 0, f 2 = 5, and 0 5 N 5 130.</p><p>The E J ( N ) starts high for N = 0, decreases rapidly as N increases, reaches a shallow minimum region, and then gradually increases. The minimum for the model <ref type="bibr">(3.4</ref>) is reached at N = 62, while for the model (3.5) at N = 46. Clearly, the reconstruction error for the signal dependent model is greater than that for the signal independent one. This is illustrated in Fig. <ref type="figure" target="#fig_8">2</ref> where the ratio E J , ( N ) / E J , ( N ) is plotted versus N . Here J , ( N ) and J , ( N ) stand for the errors of signal dependent and signal independent noise models, respectively. I v . DATA-BASED SELECTION OF N The problem which we address in this section is how to select a "good" N directly from the available data { g ( x , , yj&gt;; 1 I i I n, 1 I j 5 m}. Ideally, one wishes to have N,* that minimizes the global error J ( N ) . Notice that N,* is a function of the data at hand. This, in turn, is equivalent to taking the minimizer of the following criteria (4.1)   k=O</p><formula xml:id="formula_21">J ( N ) -/ / f ' ( X &gt; Y ) h d Y N N N N = c C (i,,)2 -2 c A , , L = S ( N ) ,</formula><formula xml:id="formula_22">I = O k = O I = O</formula><p>say. This, however, is infeasible since_ A,,% are unknown. Note also that replacing A,, with A,, in (4.1) yields the unacceptable solution N = m. Nevertheless, this difficulty can be handled by some resampling techniques, <ref type="bibr" target="#b9">[12]</ref>, [261.</p><p>First, let us observe that the second summand on the right-hand side of (4.1) is equal to <ref type="figure">= E { / / L ( ~,</ref><ref type="figure">Y ) g ( x ,</ref><ref type="figure">Y &gt; h d y I{g(xi,</ref><ref type="figure">Y j ) } } &gt;</ref> where E{XIY} denotes the conditional expectation of ~ X given Y. This identity suggests to estimate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>//f&amp; y ) f ( x , Y ) h d Y</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ilf,(x, y ) f ( x , Y)hdY by</head><p>where fN i j ( x , y ) is defined similarly as f N ( x , y ) with the observation g(x,, y j ) being removed. That is, N.. (4.3)   k=O I = O Hence, an empirical counterpart of S ( N ) is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>f N , i j ( x , ~) = C C Ak/,ijPk(X)P,(Y),</head><p>N N  --9oting that Ak,,ij in (4.6) can be represented in terms of A,, as follows: where Here, h k l ( x r , y t ) is defined as in (2.7) except the term involving the observation g ( x i -yj), where the integration is carried out over two pixels  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S^(W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T ( A X A Y ) ~/ ~</head><p>-+ 0.</p><p>Then, it follows that D( N,*, fro) -+ 0, in probability as A x A y -+ 0.</p><p>The proof of Theorem 3 is given in the Appendix.</p><p>The meaning of Theorem 3 is that yo and N,* are equivalent if one confines the search for No to a finite set, whose cardinality is of the order ( A x A ~) -~/ ' + " , for arbitrarily small E &gt; 0. Hence, for the 64 X 64 image the total 289 moments should be taken into account.</p><p>There is a rich statistical literature concerning databased selection of various smoothing and regularization parameters, see <ref type="bibr" target="#b9">[12]</ref>, <ref type="bibr">[15]</ref> and references cited therein. They have been adapted recently to some computer vision and image processing problems as well, <ref type="bibr">[61 [24]</ref>.</p><p>One can conclude that no technique yields the best solution and in practice one should apply different algorithms. For instance, instead of the J ( N ) criterion, its discrete approximation can be used, <ref type="bibr">[121, [151.</ref> This measures the discrepancy between f N ( x , y ) and f ( x , y ) only on the grid points {(xi, yj); 1</p><formula xml:id="formula_23">I i I n, 1 I j I m), whereas J ( N ) is the global criterion.</formula><p>The empirical selectors corresponding to J D ( N ) are of the form i.e., it is a penalized yersion of the residual error AxAy Cy= Common prescriptions for the penalty factor @ ( N I are a) (1 -L(N)AxAy)-*, b) (1 -L(N)AxAy)-',&amp;where L ( N ) is the total number of moments used in f N , e.g., U N ) = ( N + 1)2 for the square summation, or L ( N ) = (N + 1)(N + 2)/2 for the triangular one (see Section VI.</p><p>Using results of this paper along with techniques worked out in <ref type="bibr">[15]</ref> one can show the asymptotic optimality of the selector Z ( N ) . Moreover, the relative variability between N$ (minimizer of J D ( N ) ) and Sd (minimizer of ?$NI) is of the order ( A X A ~) ' ' ~. To illustrate the presented techniques for selection of N let us consider a simple simulation example. Fig. <ref type="figure" target="#fig_4">3</ref> shows the original 21 X 21 binary image and its noise version. The image represents the so-called Abingdon cross <ref type="bibr">[21]</ref> with the cross value 17 and background 12 grey levels. A white Gaussian noise with variance 2 is added producing a noisy image. The reconstruction process is shown in Fig. <ref type="figure" target="#fig_14">4</ref> starting from the noisy image and reconstructed images employing successivley moments from order 1 to 23. A triangular summation has been used with the total (N + 1)(N + 2)/2 moments.</p><p>The corresponding reconstruction error J D ( N ) (averaged on 20 runs) is depicted in Fig. <ref type="figure" target="#fig_12">5</ref>. The error slowly decreases, reaches minimum at N : = 8, and then it gradually increases up to the value N = 18. From N = 18, the error increases rapidly being, e.g., for N = 20 six times greater than for N = 8.</p><p>Then, the automatic selectors of N have beenLalculated giving some estimates of N:. First, the J D ( N ) criterion with @ ( N I = is equal to 0.42%. For the seLector E(N) with a(?&gt; =</p><p>(1 -L(N)AxAy)-' we get Nd = 15 with err(N$, N d ) = 22.8%. Finally, the selector discussed i? Section IV, see (4.51, has been implemented giving No = 11, with the relative error 4.89%.</p><p>It is worth to note that the visual inspection of reconstructed images (Fig. <ref type="figure" target="#fig_14">4</ref>) suggests that acceptable solutions lie between N = 8 and N = 13, see Section V for a brief discussion on a subjective choice of N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>We have considered the problem of reconstructing an image function f ( x , y) from a finite number of orthogonal moments. The solution of this problem revealed fundamental issues concerning the sensitivity to image noise, digitizing effect, and power of image representation.</p><p>In particular, the exact analytical formulas for the integrated square reconstruction error have been established and as a result an optimal number of moments minimizing the error has been derived. That number has been found to be a function of some inherent properties of the image (variation, smoothness), the noise model characteristics, and sampling rates.</p><p>This property, in turn, brings a new challenging nonhomogenity to the problem of object recognition where moments used to form a feature space. That is, the data coming from different class patterns can have variable dimensionality. None of the classical classification techniques seem to be approapriate to this data structure. The classification tree method of Breiman et al. <ref type="bibr" target="#b7">[9]</ref> would be ideally suited to the previous problem.</p><p>By using some resampling technique, motivated by the cross-validation methodology, we have proposed the automatic (data driven) method for the slection of the optimal number of moments. The asymptotic optimality theorem of that selection has been proved. It should be borne in mind, however, that the quality of the reconstructed image based on that optimal number does not agree with the subjective human measure of the image fidelity. In [71, it has been noted that a small decrease in the reconstruction error yields a substantial improvement in the subjective image quality, see also Fig. <ref type="figure" target="#fig_12">5</ref>   <ref type="bibr">[30]</ref>. Clearly, this is smaller than ( N + 112. Nevertheless, the approach taken in (2.8) has a straightforward generalization to the rectangular summation, i.e., -sin ((2k + 1)e) -sin ((21 + l)cp)] de dcp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>r = l , = 1</head><p>Since the Fourier coefficient of the (k, 1)th order of bivariate trigonometric series converges to zero as k + 1 + 00, see <ref type="bibr">[17]</ref>,</p><p>[MI, one can conclude that the above expression is of the order BY virtue of Cauchy-Schwartz inequality, the right-hand side of the does not exceed Using the same arguments as in the proof of Lemma 3 and then applying Lemma 2 we can complete the proof of Lemma 4. 0 E , 6 &gt; 0.</p><p>In order to complete the proof one has to consider L e m m a 5 Let .Pf(cos e)P?(cos cp) sin e sin cp d e dcp. Here, +jkI is the quantity defined in (4.6) and the maximum with respect to N is taken over the set (0, l;.., TI, where T will be specified next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A x Ay</head><p>From this, we have Using, in turn, (4.8) similar equations can be written for i = 1 and (i, j ) # ( p , 4). Then, using successively Lemmas 1, 4, and 5, and after some tedious algebra, one can show that var(CN) = O ( N 4 ( A x A y ) ' ) .</p><p>Having this as well as (A.4), (A.5), and (A.31, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P { J ( f i o ) -J(N,*) 2 E ) I c T ' ( A x A Y ) ' / E ' ,</head><p>where c does not depend on A x A y .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>N</head><label></label><figDesc>/21 D/21 * C C ( -1 ) r + s ~r k ~s , m k -2 r . / -2 r r r=O s = O where C,, = (2k -Z r ) ! / r ! ( kr ) ! ( k -2r)!, see [23, ch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Algorithm the order 2N, i.e., A,, = / / p k ( x Let us observe that E&amp;, = A,, and consequently EfN(x, y ) = f N ( x , y ) . However, since only a finite number of observations is given, one has to use a certain discrete version of Xkl. Let n m be a discrete approximation of A,,, where (h,,(x, y ) ) is a That is, a linear weighted combination of { g ( x i , y j ) , Many perscriptions for {h,,(xi, y i ) } are possible. One weight sequence (see examples next). 1 5 i I n, 1 5 j I m} can serve as an estimate of Ak,. used in this paper has the form where A x = x i -x i -, , A y = y , -y j -, are sampling intervals in the x and y directions, respectively, and x1 -( A y ) / 2 = 1. The coefficient hk,(xi, y j ) represents the integration of polynomial p , ( x ) p , ( y ) over the pixel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) and by virtue of Parseval's formula we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ff = K 2 j-j-J ( lx')(l -y2) Here, writing a ( x ) = b(x) as x -+ 0, we mean that a ( x ) / b ( x ) -+ 1 as x -+ 0. The proof of Theorem 1 is in the Appendix. The terms var(ik/) and ( E i k l -&amp;,I2 from (3.2) are represented by the first two leading expressions in Theorem 1, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 )</head><label>3</label><figDesc>the local Lipschitz condition [18]. Then, by the fact that one can find that Z ( N ) is of order These considerations and Theorem 1 yield the following Theorem 2: Let all the conditions of Theorem 1 hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Furthermore, N,*/N* = ( u / u , ) ' / ( ~P + ~) and EJ(N* )/ EJ(N,* = ( U / U , ) ( ~~-~) / ( ~~+ ~) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>= C c {' L -2%) 7 %k, = C c~~~~~Y l ~~, , , l l ~, , ~~l ~Y l ~.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mean integrated-square error versus N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ratio of reconstruction errors for models (3.5) and (3.4) as a function of N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the boundary observations { g ( x , , yj&gt;, j = l;.*,m} one has to integrate over is the gradient operator in the direction of the x-axis. A natural generalization could replace A x g by a more sophisticated class of gradient masks. Let No denote the valu? of N minimizing S"(N). The relative distance betyeen No and N,* can b : measured by the difference J(N,) -J(N,*) = D(N,*, No), say. Note that this quanGty is always nonnFgative. The asymptotic optimality of No means D ( N $ , N o ) converges to zero (in probability) as A x A y -+ 0. The latter syas that Po will behave like N,* assuming that A x A y is small enougk. The conditions for asymptotic equivalence between No and N,* are exhibited in the following theorem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem 3 :</head><label>3</label><figDesc>Let all the conditions of Theorem 1 be satisfied. Moreover, let (3.3)&amp;hold with p &gt; 1. Suppose that E { g 3 ( x , y ) } &lt; w. Let No denote the value of N minimizing S ( N ) over the set (0, l;..,TI, where    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig.3. Original image and its noisy version with white Gaussian noise of variance 2 added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Discrete reconstruction error versus N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>in this paper. The selection of image reconstruction measures properly reflecting the subjective quality remains an open problem. The reconstruction technique defined in (2.8) employs a total of ( N + 112 moments. If one wishes to use the triangular type of summation C i k -L , p k -, ( ~) p I ( y ) , then the total number of moments is ( N + 1)(N + 2 ) / 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Lemma 4 :</head><label>4</label><figDesc>turn, can be important in situations where the f ( x , y ) has a different variability in components x and y .In fact, if f ( x , y ) = f l ( x ) f i ( y ) and f l ( x ) varies rapidly with x , while f 2 ( y ) varies slowly with y then N should be large, whereas M should be small. The selection rule in (4.5) extended to that case is of the formN M S^(N, M ) = c {ii/ -24,J k = O / = 0and TheoremA3 holds with(fi,,,h?o)  being the values minimizing S ( N , M ) over the set IO, l;.., T,} X IO, l;.., T J , whereT ( A X A ~) " ~ + 0, as A x A y + 0, T = max(T,, T2).Other extensions of a particular interest would include the spatially correlated noise models, and capability of the moment descriptors to incorporate the edge information of the image. These topics are left for future research. ACKNOWLEDGMENT X. Liao provided the programming support for the Note: In [14], the analogical result has been established in the Applying the inequality, case of univariate functions. &lt; 0 &lt; T , [27], we find the integral bounded by by choosing E , 6 arbitrarily small. This concludes the proof of Lemma 2. 0 L e t [ O , . r r l X [ O , a l = A , , 6 U ~, s , w h e r e A , , , = [ ~, . r r -By virtue of Darboux's formula [27, p. 1681, we have [6, .rr -61, for E , 6 &gt; 0, and A,, is a complement of A , , ,. The proof easily results from the fact that where bound for the error holds uniformly for 0 E [ E , r -E ] , E &gt; 0. Hence, diately from (3.21, Lemma 1, Lemma 3, (A.21, Lemma 2 and the fact that P,(x) = 1, P , ( x ) = x and p , ( x ) = -P,(x&gt;. of Theorem 3 To verify Theorem 3, we need some further lemmas. Let us denote 5 = / / w ( x , y ) [ ( l -x2)(1 -y 2 ) ] -' / * dxdy + W and p = ( m ( f ) ) 1 / 2 . Let the conditions of lemmas 2 and 3 be satisfied. + 0 7 + -. Since 2c0s2 a = cos2a + 1 , we can rewrite the integral on the right-hand side of this formula as (k, I ) # (s, t ) , we have .IAe s/w(cos 0,cos cp)[sin((2k + l)e) sin((21 + 1)p) Proof It is plain that n m C O V ( i , / , L) = a 2 c c ~( X , , Y , ) h k / ( X , , Y , ) h s t ( X , , y / ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>By this and Chebyshev's inequality, it is sufficient to evaluate (EAN)*, var(BN), and var(CN).Owing to identities in (4.7) and (4.81, we have the identity in (4.7) and some algebra we can get for i &gt; 2.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>To prove Thorem 1 we need some lemmas.</p><p>Lemma 1: Let f be a function of bounded variation on S. Then, ~i,) = A,, + ( M V ( ~) A ~A Y ) " ~, where V(f) is the variation of f and M = max f ( x , y ) .</p><p>x , Y Proofi By the Cauchy-Schwartz inequality</p><p>Furthermore, the expression in the brackets does not exceed where Osc,,(f) = sup{lf(w,,w2) -f(z,, ~2 1 1 : ( w , , w ~) , ( z I , z,) E [ x , --, x , + -]x[y, -T , y, + 2 1 ) is the oscillation of f over the pixel [ x iy , x l + y l x [ y , -~, y , + 7 1 .</p><p>It is now plain that (A.l) is bounded by</p><p>We refer to [Z] concerning definitions and properties of bivariate functions of bounded variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0</head><p>The proof of Lemma 1 has been completed.</p><p>Lemma 2: Let w be the measurable function satisfying Then, simulations.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[(I -x 2 ) ( 1 -Y ~) ] -" ~ dwdy</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition aspects of moment invariants</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Psaltis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Properties of functions f ( x , y ) of bounded variation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Digital pattem recognition by moments</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Alt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Assoc. Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="240" to="258" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A general moment-invariants/attributed-graph method for three-dimensional object recognition from a single image</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bamiech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J P</forename><surname>De Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">M</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Pike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J . Robotics Automation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="301" to="330" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note>Inverse Probl.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ill-posed problems in early vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="865" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moment invariants for pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Hossack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattem Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="456" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . R. Statist. Soc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="211" to="246" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Monterey, C A Wadsworth and Brooks</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advanced optical processors for multiple degree-offreedom object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Casasent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods of Numerical Integration</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spline Smoothing and Nonparametric Regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Eubank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Template matching in rotated images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtaby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="338" to="344" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparison of two orthogonal series methods of estimating a density and its derivatives on an interal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<idno>NOV. 1984. 711-730</idno>
	</analytic>
	<monogr>
		<title level="j">J . Multivariate Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="1934">1982. 1934. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How far are automatically chosen regression smoothing parameters from their optimum</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hardle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="86" to="101" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual problem recognition by moment invariants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1962-02">Feb. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Igari</surname></persName>
		</author>
		<title level="m">Lectures on Fourier Series of Several Variables. Madison, WI: Univ. Wisconsin Lecture Notes</title>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Theory of Approximation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930">1930</date>
			<publisher>Amer. Math. Soc., Colloq. Amer. Math. Soc</publisher>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Fundamentals of Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive noise smoothing filter for images with signal-dependent noise</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Sawchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="165" to="177" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Abingdon cross benchmark survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Preston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Radial and angular moment invariants for image identification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Anal. Machine Intell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal estimation of contour properties by cross-validated regularization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahraray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Orthogonal Functions</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Interscience</publisher>
			<date type="published" when="1955">1955. 1989</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="600" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experiments on pattern recognition using invariant Fourier-Mellin descriptions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Arsenault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="771" to="776" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . R . Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recovering a function from a finite number of moments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Szego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Talenti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Orthogonal Polynomials, 4</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society Colloquie Publications</publisher>
			<date type="published" when="1975">1975. 1987</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="517" />
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image analysis via the general theory of moments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Teague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="920" to="930" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On image analysis by the methods of moments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattem Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="496" to="512" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On digital approximation of moment invariants</title>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, Image Processing</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Solutions of Ill-Posed Problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Arsenin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian &apos;confidence intervals&apos; for the cross-validation smoothing spline</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="133" to="150" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zygmund</surname></persName>
		</author>
		<title level="m">Trigonometric Series</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pami-3</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="240" to="242" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
