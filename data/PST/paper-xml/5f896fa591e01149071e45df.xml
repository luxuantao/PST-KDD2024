<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 SELF-TRAINING FOR FEW-SHOT TRANSFER ACROSS EXTREME TASK DIFFERENCES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 SELF-TRAINING FOR FEW-SHOT TRANSFER ACROSS EXTREME TASK DIFFERENCES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most few-shot learning techniques are pre-trained on a large, labeled "base dataset". In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different "source" problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite progress in visual recognition, training recognition systems for new classes in novel domains requires thousands of labeled training images per class. For example, to train a recognition system for identifying crop types in satellite images, one would have to hire someone to go to the different locations on earth to get the labels of thousands of satellite images. The high cost of collecting annotations precludes many downstream applications.</p><p>This issue has motivated research on few-shot learners: systems that can rapidly learn novel classes from a few examples. However, most few-shot learners are trained on a large base dataset of classes from the same domain. This is a problem in many domains (such as medical imagery, satellite images), where no large labeled dataset of base classes exists. The only alternative is to train the fewshot learner on a different domain (a common choice is to use ImageNet). Unfortunately, few-shot learning techniques often assume that novel and base classes share modes of variation <ref type="bibr">(Wang et al., 2018)</ref>, class-distinctive features <ref type="bibr" target="#b35">(Snell et al., 2017)</ref>, or other inductive biases. These assumptions are broken when the difference between base and novel is as extreme as the difference between object classification in internet photos and pneumonia detection in X-ray images. As such, recent work has found that all few-shot learners fail in the face of such extreme task/domain differences, underperforming even naive transfer learning from ImageNet <ref type="bibr" target="#b12">(Guo et al., 2020)</ref>.</p><p>Another alternative comes to light when one considers that many of these problem domains have unlabeled data (e.g., undiagnosed X-ray images, or unlabeled satellite imagery). This suggests the possibility of using self-supervised techniques on this unlabeled data to produce a good feature representation, which can then be used to train linear classifiers for the target classification task using just a few labeled examples. Indeed, recent work has explored self-supervised learning on a variety of domains <ref type="bibr" target="#b44">(Wallace &amp; Hariharan, 2020)</ref>. However, self-supervised learning starts tabula rasa, and as such requires extremely large amounts of unlabeled data (on the order of millions of images). With more practical unlabeled datasets, self supervised techniques still struggle to outcompete naive ImageNet transfer <ref type="bibr" target="#b44">(Wallace &amp; Hariharan, 2020)</ref>. We are thus faced with a conundrum: on the one hand, few-shot learning techniques fail to bridge the extreme differences between ImageNet and domains such as X-rays. On the other hand, self-supervised techniques fail when they ignore inductive biases from ImageNet. A sweet spot in the middle, if it exists, is elusive.</p><p>In this paper, we solve this conundrum by presenting a strategy that adapts feature representations trained on source tasks to extremely different target domains, so that target task classifiers can then Figure <ref type="figure">1</ref>: Problem setup. In the representation learning phase (left), the learner has access to a large labeled "base dataset" in the source domain, and some unlabeled data in the target domain, on which to pre-train its representation. The learner must then rapidly learn/adapt to few-shot tasks in the target domain in the evaluation phase (right). be trained on the adapted representation with very little labeled data. Our key insight is that a pre-trained base classifier from the source domain, when applied to the target domain, induces a grouping of images on the target domain. This grouping captures what the pre-trained classifier thinks are similar or dissimilar in the target domain. Even though the classes of the pre-trained classifier are themselves irrelevant in the target domain, the induced notions of similarity and dissimilarity might still be relevant and informative. This induced notion of similarity is in contrast to current self-supervised techniques which often function by considering each image as its own class and dissimilar from every other image in the dataset <ref type="bibr" target="#b50">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref>. We propose to train feature representations on the novel target domain to replicate this induced grouping. This approach produces a feature representation that is (a) adapted to the target domain, while (b) maintaining prior knowledge from the source task to the extent that it is relevant. A discerning reader might observe the similarity of this approach to self-training, except that our goal is to adapt the feature representation to the target domain, rather than improve the base classifier itself.</p><p>We call our approach "Self Training to Adapt Representations To Unseen Problems", or STARTUP. In a recently released BSCD-FSL benchmark consisting of datasets from extremely different domains <ref type="bibr" target="#b12">(Guo et al., 2020)</ref>, we show that STARTUP provides significant gains (up to 2.9 points on average) over few-shot learning, transfer learning and self-supervision state-of-the-art. To the best of our knowledge, ours is the first attempt to bridge such large task/domain gaps and successfully and consistently outperform naive transfer in cross-domain few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM SETUP</head><p>Our goal is to build learners for novel domains that can be quickly trained to recognize new classes when presented with very few labeled data points ("few-shot"). Formally, the target domain is defined by a set of data points (e.g. images) X N , an unknown set of classes (or label space) Y N , and a distribution D N over X N × Y N . A "few-shot learning task" in this domain will consist of a set of classes Y ⊂ Y N , a very small training set ("support")</p><formula xml:id="formula_0">S = {(x i , y i )} n i=1 ∼ D n N , y i ∈ Y and a small test set ("query") Q = {x i } m i=1 ∼ D m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>When presented with such a few-shot learning task, the learner must rapidly learn the classes presented and accurately classify the query images.</p><p>As with prior few-shot learning work, we will assume that before being presented with few-shot learning tasks in the target domain, the learner has access to a large annotated dataset D B known as the base dataset. However, crucially unlike prior work on few-shot learning, we assume that this base dataset is drawn from a very different distribution. In fact, we assume that the base dataset is drawn from a completely disjoint image space X B and a disjoint set of classes Y B :</p><formula xml:id="formula_1">D B = {(x i , y i )} N B i=1 ⊂ X B × Y B</formula><p>where X B is the set of data (or the source domain) and Y B is the set of base classes. Because the base dataset is so different from the target domain, we introduce another difference vis-a-vis the conventional few-shot learning setup: the learner is given access to an additional unlabeled dataset from the target domain:</p><formula xml:id="formula_2">D u = {x i } Nu i=1 ∼ D N Nu</formula><p>Put together, the learner will undergo two phases. In the representation learning phase, the learner will pre-train its representation on D B and D u ; then it goes into the evaluation phase where it will be presented few-shot tasks from the target domain where it learns the novel classes (Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Few-shot Learning (FSL). This paper explores few-shot transfer, and as such the closest related work is on few-shot learning. Few-shot learning techniques are typically predicated on some degree of similarity between classes in the base dataset and novel classes. For example, they may assume that features that are discriminative for the base classes are also discriminative for the novel classes, suggesting a metric learning-based approach <ref type="bibr">(Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr">Qi et al., 2018;</ref><ref type="bibr" target="#b35">Snell et al., 2017;</ref><ref type="bibr" target="#b43">Vinyals et al., 2016;</ref><ref type="bibr" target="#b38">Sung et al., 2018;</ref><ref type="bibr" target="#b18">Hou et al., 2019)</ref> or transfer learning-based approach <ref type="bibr" target="#b2">(Chen et al., 2019a;</ref><ref type="bibr" target="#b46">Wang et al., 2019;</ref><ref type="bibr" target="#b19">Kolesnikov et al., 2020;</ref><ref type="bibr" target="#b39">Tian et al., 2020)</ref>. Alternatively, they may assume that model initializations that lead to rapid convergence on the base classes are also good initializations for the novel classes <ref type="bibr" target="#b6">(Finn et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr">Ravi &amp; Larochelle, 2017;</ref><ref type="bibr" target="#b28">Nichol &amp; Schulman;</ref><ref type="bibr" target="#b34">Rusu et al., 2019;</ref><ref type="bibr" target="#b37">Sun et al., 2019;</ref><ref type="bibr" target="#b21">Lee et al., 2019)</ref>. Other methods assume that modes of intra-class variation are shared, suggesting the possibility of learned, class-agnostic augmentation policies <ref type="bibr" target="#b13">(Hariharan &amp; Girshick, 2017;</ref><ref type="bibr">Wang et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2019b)</ref>. Somewhat related is the use of a class-agnostic parametric model that can "denoise" few-shot models, be they from the base or novel classes <ref type="bibr">(Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b60">2019)</ref>. In contrast to such strong assumptions of similarity between base and novel classes, this paper tackles few-shot learning problems where base and novel classes come from very different domains, also called cross-domain few-shot learning.</p><p>Cross-domain Few-shot Classification (CD-FSL). When the domain gap between the base and novel dataset is large, recent work <ref type="bibr" target="#b12">(Guo et al., 2020;</ref><ref type="bibr" target="#b2">Chen et al., 2019a)</ref> has shown that existing stateof-the-art few-shot learners fail to generalize. <ref type="bibr" target="#b40">Tseng et al. (2020)</ref> attempt to address this problem by simulating cross-domain transfer during training. However, their approach assumes access to an equally diverse array of domains during training, and a much smaller domain gap at test time: for example, both base and novel datasets are from internet images. Another relevant work <ref type="bibr" target="#b27">(Ngiam et al., 2018)</ref> seeks to build domain-specific feature extractor by reweighting different classes of examples in the base dataset based on the target novel dataset but their work only investigates transfer between similar domains (both source and target are internet images). Our paper tackles a more extreme domain gap. Another relevant benchmark for this problem is <ref type="bibr" target="#b55">(Zhai et al., 2019)</ref> but they assume access to more annotated examples (1k annotations) during test time than the usual FSL setup.</p><p>Few-shot learning with unlabeled data. This paper uses unlabeled data from the target domain to bridge the domain gap. Semi-supervised few-shot learning (SS-FSL) <ref type="bibr" target="#b32">(Ren et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2019;</ref><ref type="bibr" target="#b54">Yu et al., 2020;</ref><ref type="bibr" target="#b33">Rodríguez et al., 2020;</ref><ref type="bibr" target="#b47">Wang et al., 2020)</ref> and transductive few-shot learning (T-FSL) <ref type="bibr" target="#b23">(Liu et al., 2019;</ref><ref type="bibr" target="#b5">Dhillon et al., 2020;</ref><ref type="bibr" target="#b18">Hou et al., 2019;</ref><ref type="bibr" target="#b47">Wang et al., 2020;</ref><ref type="bibr" target="#b33">Rodríguez et al., 2020)</ref> do use such unlabeled data, but only during evaluation, assuming that representations trained on the base dataset are good enough. In contrast our approach leverages the unlabeled data during representation learning. The two are orthogonal innovations and can be combined. Knowledge distillation <ref type="bibr" target="#b16">(Hinton et al., 2015)</ref> is similar but aims to compress a large teacher network by training a student network to mimic the prediction of the teacher network. A key difference between these and our work is that self-training / knowledge distillation focus on a single task of interest, i.e, there is no change in label space. Our approach is similar, but we are interested in transferring to novel domains with a wholly different label space: an unexplored scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation.</head><p>Transfer to new domains is also in the purview of domain adaptation <ref type="bibr" target="#b41">(Tzeng et al., 2017;</ref><ref type="bibr" target="#b17">Hoffman et al., 2018;</ref><ref type="bibr" target="#b24">Long et al., 2018;</ref><ref type="bibr" target="#b52">Xu et al., 2019;</ref><ref type="bibr" target="#b20">Laradji &amp; Babanezhad, 2020;</ref><ref type="bibr" target="#b45">Wang &amp; Deng, 2018;</ref><ref type="bibr" target="#b49">Wilson &amp; Cook, 2020)</ref> where the goal is to transfer knowledge from the labelabundant source domain to a target domain where only unlabeled data is available. However, a key assumption in domain adaptation is that the source domain and target domain share the same label space which does not hold for few-shot learning.</p><p>Self-supervised Learning. Learning from unlabeled data has seen a resurgence of interest with advances in self-supervised learning. Early self-supervised approaches were based on handcrafted "pretext tasks" such as solving jigsaw puzzles <ref type="bibr" target="#b29">(Noroozi &amp; Favaro, 2016)</ref>, colorization <ref type="bibr" target="#b56">(Zhang et al., 2016)</ref> or predicting rotation <ref type="bibr">(Gidaris et al., 2018)</ref>. A more recent (and better performing) line of self-supervised learning is contrastive learning <ref type="bibr" target="#b50">(Wu et al., 2018;</ref><ref type="bibr" target="#b26">Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b15">He et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref> which aims to learn representations by considering each image together with its augmentations as a separate class. While self supervision has been shown to boost few-shot learning <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b36">Su et al., 2020)</ref>, its utility in cases of large domain gaps between base and novel datasets have not been evaluated. Our work focuses on this challenging scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>Consider a classification model f θ = C • φ where φ embeds input x into R d and C is a (typically linear) classifier head that maps φ(x) to predicted probabilities P (y|x). θ is a vector of parameters.</p><p>During representation learning, STARTUP performs the following three steps:</p><p>1. Learn a teacher model θ 0 on the base dataset D B by minimizing the cross entropy loss 2. Use the teacher model to construct a softly-labeled set</p><formula xml:id="formula_3">D * u = {(x i , ȳi )} Nu i=1 where ȳi = f θ0 (x i ) ∀x i ∈ D u .</formula><p>(1)</p><p>Note that ȳi is a probability distribution as described above. 3. Learn a new student model θ * on D B and D * u by optimizing:</p><formula xml:id="formula_4">min θ 1 N B (xi,yi)∈D B l CE (f θ (x i ), y i ) + 1 N u (xj ,ȳj )∈D * u l KL (f θ (x j ), ȳj ) + l unlabeled (D u )</formula><p>(2) where l CE is the cross entropy loss, l KL is the KL divergence and l unlabeled is any unsupervised/self-supervised loss function (See below).</p><p>The third term, l unlabeled , is intended to help the learner extract additional useful knowledge specific to the target domain. We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>. The SimCLR loss encourages two augmentations of the same image to be closer in feature space to each other than to other images in the batch. We refer the reader to the paper for the detailed loss formulation.</p><p>The first two terms are similar to those in prior self-training literature <ref type="bibr" target="#b51">(Xie et al., 2020)</ref>. However, while in prior self-training work, the second term (l KL ) is thought to mainly introduce noise during training, we posit that l KL has a more substantial role to play here: it encourages the model to learn feature representations that emphasize the groupings induced by the pseudo-labels ȳi on the target domain. We analyze this intuition in section 5.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVALUATION</head><p>STARTUP is agnostic to inference methods during evaluation; any inference methods that rely on a representation <ref type="bibr" target="#b35">(Snell et al., 2017;</ref><ref type="bibr">Gidaris &amp; Komodakis, 2018</ref>) can be used with STARTUP. For simplicity and based on results reported by <ref type="bibr" target="#b12">Guo et al. (2020)</ref>, we freeze the representation φ after performing STARTUP and train a linear classifier on the support set and evaluate the classifier on the query set. <ref type="bibr" target="#b51">Xie et al. (2020)</ref> found that training the student from scratch sometimes yields better results for Im-ageNet classification. To investigate, we focused on a variant of STARTUP where the SimCLR loss is omitted and experimented with three different initialization strategies -from scratch (STARTUP-Rand (no SS)), from teacher (STARTUP-T (no SS)) and using the teacher's embedding with randomly initialized classifier (STARTUP (no SS)). We found no conclusive evidence that one single initialization strategy is superior to the others across different datasets (See Appendix A.4) but we observe that (STARTUP (no SS)) is either the best or the second best in all scenarios. As such, we opt to use teacher's embedding with a randomly initialized classifier as the default student initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INITIALIZATION STRATEGIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We defer the implementation details to Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FEW-SHOT TRANSFER ACROSS DRASTICALLY DIFFERENT DOMAINS</head><p>Benchmark. We experiment with the challenging (BSCD-FSL) benchmark introduced in <ref type="bibr" target="#b12">Guo et al. (2020)</ref>. The base dataset in this benchmark is miniImageNet <ref type="bibr" target="#b43">(Vinyals et al., 2016)</ref>, which is an object recognition task on internet images. There are 4 novel datasets in the benchmark, none of which involve objects, and all of which come from a very different domain than internet images: CropDiseases (recognizing plant diseases in leaf images), EuroSAT (predicting land-use from satellite images), ISIC2018 (identifying melanoma from images of skin lesions) and ChestX (diagnosing chest X-rays). <ref type="bibr">Guo et al.</ref> found that state-of-the-art few-shot learners fail on this benchmark.</p><p>To construct our setup, we randomly sample 20% of data from each novel datasets to form the respective unlabeled datasets D u . We use the rest for sampling tasks for evaluation. Following <ref type="bibr" target="#b12">Guo et al. (2020)</ref>, we evaluate 5-way k-shot classification tasks (the support set consists of 5 classes and k examples per class) for k ∈ {1, 5} and report the mean and 95% confidence interval over 600 few-shot tasks (conclusions generalize to k ∈ {20, 50}. See Appendix A.2).</p><p>Baselines. We compare to the techniques reported in <ref type="bibr" target="#b12">Guo et al. (2020)</ref>, which includes most stateof-the-art approaches as well as a cross-domain few-shot technique <ref type="bibr" target="#b40">Tseng et al. (2020)</ref>. The top performing among these is naive Transfer which simply trains a convolutional network to classify the base dataset, and uses the resulting representation to learn a linear classifier when faced with novel few-shot tasks. These techniques do not use the novel domain unlabeled data.</p><p>We also compare to another baseline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>, and then uses the resulting representation to learn linear classifiers for few-shot tasks. This builds upon state-of-the-art self-supervised techniques.</p><p>To compare to a baseline that uses both sources of data, we establish Transfer + SimCLR. This baseline is similar to the SimCLR baseline except the embedding is initialized to Transfer's embedding before SimCLR training.</p><p>Following the benchmark, all methods use a ResNet-10 (He et al., 2016) unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">RESULTS</head><p>We present our main results on miniImageNet → BSCD-FSL in Table <ref type="table">1</ref>.</p><p>STARTUP vs Few-shot learning techniques. STARTUP performs significantly better than all fewshot techniques in most datasets (except ChestX, where all methods are similar). Compared to previous state-of-the-art, Transfer, we observe an average of 2.9 points improvement on the 1-shot case. The improvement is particularly large on CropDisease, where STARTUP provides almost a 6 point increase for 1-shot classification. This improvement is significant given the simplicity of our approach, and given that all meta-learning techniques underperform this baseline.</p><p>Table <ref type="table">1</ref>: 5-way k-shot classification accuracy on miniImageNet→BSCD-FSL. Mean and 95% confidence interval are reported. (no SS) indicates removal of SimCLR. ProtoNet: <ref type="bibr" target="#b35">(Snell et al., 2017)</ref>, MAML: <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>, MetaOpt: <ref type="bibr" target="#b21">(Lee et al., 2019)</ref> FWT: <ref type="bibr" target="#b40">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type="bibr" target="#b12">(Guo et al., 2020)</ref> We conjecture that the base embedding is not a good starting point for this dataset. However, we find that using SimCLR as an auxilliary loss to train the student (STARTUP vs STARTUP (no SS)) is beneficial.</p><p>STARTUP vs Transfer + SimCLR. STARTUP outperforms Transfer + SimCLR in most cases (except 5-shot in ChestX and 1-shot in ISIC). We stress that the strength of STARTUP is not solely from SimCLR but rather from both self-training and SIMCLR. This is especially evident in EuroSAT since the STARTUP (no SS) variant outperforms Transfer and Transfer + SimCLR.</p><p>Larger and stronger teachers. To unpack the impact of teacher quality, we experiment with a larger network and transfer from the full ILSVRC 2012 dataset <ref type="bibr" target="#b4">(Deng et al., 2009)</ref> to BSCD-FSL.</p><p>In particular, we used the publicly available pre-trained ResNet-18 <ref type="bibr" target="#b14">(He et al., 2016)</ref> as a teacher and train a student via STARTUP. We compare this to a transfer baseline that uses the same network and ImageNet as the training set. The result can be found in table <ref type="table" target="#tab_0">2</ref>. Surprisingly, larger, richer embeddings do not always transfer better, in contrast to in-domain results reported by <ref type="bibr" target="#b13">Hariharan &amp; Girshick (2017)</ref>. However, STARTUP is still useful in improving performance: the absolute improvement in performance for STARTUP compared to Transfer remains about the same in most datasets except EuroSAT and CropDisease where larger improvements are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">WHY SHOULD STARTUP WORK?</head><p>While it is clear that STARTUP helps improve few shot transfer across extreme domain differences, it is not clear why or how it achieves this improvement. Below, we look at a few possible hypotheses.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">HYPOTHESIS 1:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">HYPOTHESIS 2: STARTUP ENHANCES TEACHER-INDUCED GROUPINGS</head><p>The teacher produces a meaningful grouping of the data from the target domain. The predictions made by the teacher essentially induce a grouping on the target domain. Even though the base label space and novel label space are disjoint, the groupings produced by the teacher might not be entirely irrelevant for the downstream classification task. To test this, we first assign each example in the novel datasets to its most probable prediction by the teacher (ResNet 18 trained on ImageNet).</p><p>We then compute the adjusted mutual information (AMI) <ref type="bibr" target="#b42">(Vinh et al., 2010)</ref> between the resulting grouping and ground truth label. AMI ranges from 0 for unrelated groupings to 1 for identical groupings. From Table <ref type="table" target="#tab_2">3</ref>, we see that on EuroSAT and CropDisease, there is quite a bit of agreement between the induced grouping and ground truth label. Interestingly, these are the two datasets where we observe the best transfer performance and most improvement from STARTUP (Table <ref type="table" target="#tab_0">2</ref>), suggesting correlations between the agreement and the downstream classification task performance.</p><p>STARTUP enhances the grouping induced by the teacher. Even though the induced groupings by the teacher can be meaningful, one could argue that those groupings are captured in the teacher model already, and no further action to update the representation is necessary. However, we posit that STARTUP encourages the feature representations to emphasize the grouping. To verify, we plot the t-SNE <ref type="bibr" target="#b25">(Maaten &amp; Hinton, 2008)</ref> of the data prior to STARTUP and after STARTUP for the two datasets in figure <ref type="figure" target="#fig_1">2</ref>. From the t-SNE plot, we observe more separation after doing STARTUP, signifying a representation with stronger discriminability.</p><p>Put together, this suggests that STARTUP works by (a) inducing a potentially meaningful grouping on the target domain data, and (b) training a representation that emphasizes this grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">FEW-SHOT TRANSFER ACROSS SIMILAR DOMAINS</head><p>Is STARTUP still useful when the gap between the base and target is smaller? To answer this, we tested STARTUP on two popular within-domain few-shot learning benchmark: miniImageNet <ref type="bibr" target="#b43">(Vinyals et al., 2016)</ref> and tieredImageNet <ref type="bibr" target="#b32">(Ren et al., 2018)</ref>. For miniImageNet, we use 20% of the and tieredImageNet-more that uses 50% of the novel set as unlabeled data. We follow the same evaluation protocols in section 5.1.</p><p>We report the results in table 4. We found that on miniImageNet, STARTUP and its variants neither helps nor hurts in most cases (compared to Transfer), indicating that the representation is already well-matched. On both variants of tieredImageNet, we found that STARTUP, with the right initialization, can in fact outperform Transfer. In particular, in the less data case, it is beneficial to initialize the student with the teacher model whereas in the more data case, training the students from scratch is superior. In sum, these results show the potential of STARTUP variants to boost few-shot transfer even when the base and target domains are close.</p><p>Additional Ablation Studies: We conducted three additional ablation studies: (a) training the student with various amount of unlabeled data, (b) training the student without the base dataset and (c) using the rotation as self-supervision instead of SimCLR in STARTUP . We show that STARTUP benefits from more unlabeled data <ref type="bibr">(Appendix A.5)</ref>, training student without the base dataset can hurt performance in certain datasets but not all datasets <ref type="bibr">(Appendix A.6</ref>) and STARTUP (w/ Rotation) outperforms Transfer in certain datasets but underperforms its SimCLR counterparts <ref type="bibr">(Appendix A.3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We investigate the use of unlabeled data from novel target domains to mitigate the performance degradation of few-shot learners due to large domain/task differences. We introduce STARTUP -a simple yet effective approach that allows few-shot learners to adapt feature representations to the target domain while retaining class grouping induced by the base classifier. We show that STARTUP outperforms prior art on extreme cross-domain few-shot transfer.</p><p>Table <ref type="table">5</ref>: 5-way k-shot classification accuracy on miniImageNet→BSCD-FSL. Mean and 95% confidence interval are reported. (no SS) indicates removal of SimCLR. ProtoNet: <ref type="bibr" target="#b35">(Snell et al., 2017)</ref>, MAML: <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>, MetaOpt: <ref type="bibr" target="#b21">(Lee et al., 2019)</ref> FWT: <ref type="bibr" target="#b40">(Tseng et al., 2020)</ref>. * Numbers reported in <ref type="bibr" target="#b12">(Guo et al., 2020)</ref> ), from teacher model (STARTUP-T (no SS)). We repeated the experiment in section 5.1 on miniImageNet → BSCD-FSL and report the results in table 9. We found that not a single initialization is superior to the others (for instance random initialization is the best on CropDisease but the worst on ISIC) however we did find that initializing the student with the teacher's embedding with a randomly initialized classifier for STARTUP is either the best or second best in all scenarios so we set that as our default initialization. The verdict is clear -STARTUP benefits from more unlabeled data (Figure <ref type="figure">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 TRAINING THE STUDENT WITHOUT THE BASE DATASET</head><p>STARTUP requires joint training on both the base dataset as well as the target domain. But in many cases, the base dataset may not be available. Removing the cross entropy loss on the base dataset when training the student essentially boils down to a fine-tuning paradigm. For miniImageNet → BSCD-FSL (Table <ref type="table" target="#tab_6">10</ref>), we found no discernible difference between all datasets except on the ISIC where we observe significant degradation in 5-shot performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-Training. Our approach is closely related to self-training, which has been shown to be effective for semi-supervised training and knowledge distillation. In self-training, a teacher model trained on the labeled data is used to label the unlabeled data and another student model is trained on both the original labeled data and the unlabeled data labeled by the teacher. Xie et al. (2020) and Yalniz et al. (2019) have shown that using self-training can improve ImageNet classification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: t-SNE plot of EuroSAT and CropDisease prior to and after STARTUP.</figDesc><graphic url="image-3.png" coords="8,304.36,163.21,188.97,160.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,117.90,81.86,376.20,161.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>; our re-implementation of Transfer uses a different batch size and 80% of the original test set for evaluation. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05. 5-way k-shot classification accuracy on ImageNet(ILSVRC 2012)→BSCD-FSL. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell></row><row><cell></cell><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell></cell><cell>MAML  *</cell><cell>-</cell><cell>23.48 ± 0.96</cell><cell>-</cell><cell>40.13 ± 0.58</cell></row><row><cell></cell><cell>ProtoNet  *</cell><cell>-</cell><cell>24.05 ± 1.01</cell><cell>-</cell><cell>39.57 ± 0.57</cell></row><row><cell></cell><cell>ProtoNet + FWT  *</cell><cell>-</cell><cell>23.77 ± 0.42</cell><cell>-</cell><cell>38.87 ± 0.52</cell></row><row><cell>.</cell><cell>MetaOpt  *  Transfer  *</cell><cell>--</cell><cell>22.53 ± 0.91 25.35 ± 0.96</cell><cell>--</cell><cell>36.28 ± 0.50 43.56 ± 0.60</cell></row><row><cell></cell><cell>Transfer</cell><cell cols="2">22.71 ± 0.40 26.71 ± 0.46</cell><cell cols="2">30.71 ± 0.59 43.08 ± 0.57</cell></row><row><cell></cell><cell>SimCLR</cell><cell cols="2">22.10 ± 0.41 25.02 ± 0.42</cell><cell cols="2">26.25 ± 0.53 36.09 ± 0.57</cell></row><row><cell></cell><cell>Transfer + SimCLR</cell><cell cols="2">22.70 ± 0.40 26.95 ± 0.45</cell><cell cols="2">32.63 ± 0.63 45.96 ± 0.61</cell></row><row><cell></cell><cell>STARTUP (no SS)</cell><cell cols="2">22.87 ± 0.41 26.68 ± 0.45</cell><cell cols="2">32.24 ± 0.62 46.48 ± 0.61</cell></row><row><cell></cell><cell>STARTUP</cell><cell cols="2">23.09 ± 0.43 26.94 ± 0.44</cell><cell cols="2">32.66 ± 0.60 47.22 ± 0.61</cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell>CropDisease</cell></row><row><cell></cell><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell></cell><cell>MAML  *</cell><cell>-</cell><cell>71.70 ± 0.72</cell><cell>-</cell><cell>78.05 ± 0.68</cell></row><row><cell></cell><cell>ProtoNet  *</cell><cell>-</cell><cell>73.29 ± 0.71</cell><cell>-</cell><cell>79.72 ± 0.67</cell></row><row><cell></cell><cell>ProtoNet + FWT  *</cell><cell>-</cell><cell>67.34 ± 0.76</cell><cell>-</cell><cell>72.72 ± 0.70</cell></row><row><cell></cell><cell>MetaOpt  *</cell><cell>-</cell><cell>64.44 ± 0.73</cell><cell>-</cell><cell>68.41 ± 0.73</cell></row><row><cell></cell><cell>Transfer  *</cell><cell>-</cell><cell>75.69 ± 0.66</cell><cell>-</cell><cell>87.48 ± 0.58</cell></row><row><cell></cell><cell>Transfer</cell><cell cols="2">60.73 ± 0.86 80.30 ± 0.64</cell><cell cols="2">69.97 ± 0.85 90.16 ± 0.49</cell></row><row><cell></cell><cell>SimCLR</cell><cell cols="2">43.52 ± 0.88 59.05 ± 0.70</cell><cell cols="2">78.23 ± 0.83 92.57 ± 0.48</cell></row><row><cell></cell><cell>Transfer + SimCLR</cell><cell cols="2">57.18 ± 0.87 77.61 ± 0.66</cell><cell cols="2">76.90 ± 0.78 92.64 ± 0.44</cell></row><row><cell></cell><cell>STARTUP (no SS)</cell><cell cols="2">62.90 ± 0.83 81.81 ± 0.61</cell><cell cols="2">73.30 ± 0.82 91.69 ± 0.47</cell></row><row><cell></cell><cell>STARTUP</cell><cell cols="2">63.88 ± 0.84 82.29 ± 0.60</cell><cell cols="2">75.93 ± 0.80 93.02 ± 0.45</cell></row></table><note>STARTUP vs SimCLR. The SimCLR baseline in general tends to underperform naive transfer from miniImageNet, and consequently, STARTUP performs significantly better than SimCLR on ISIC and EuroSAT. The exception to this is CropDisease, where SimCLR produces a surprisingly good representation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>STARTUP ADDS NOISE WHICH INCREASES ROBUSTNESS.<ref type="bibr" target="#b51">Xie et al. (2020)</ref> posit that self-training introduces noise when training the student and thus yielding a more robust student. More robust students may be learning more generalizable representations, and this may be allowing STARTUP to bridge the domain gap. Under this hypothesis, the function of the unlabeled data is only to add noise during training. This in turn suggests that STARTUP should yield improvements on the target tasks even if trained on unlabeled data from a different domain.</figDesc><table /><note>To test this, we train a STARTUP ResNet-18 student on EuroSAT and ImageNet and evaluate it on CropDisease. This model yields a 5-way 1-shot performance of 70.40 ± 0.86 (88.78 ± 0.54 for 5-shot), significantly underperforming the naive Transfer baseline (Table2. See Appendix A.7 for different combinations of unlabeled dataset and target dataset). This suggests that while the hypothesis is valid in conventional semi-supervised learning, it is incorrect in the cross-domain few-shot learning setup: unlabeled data are not merely functioning as noise. Rather, STARTUP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Adjusted Mutual Information (AMI) of the grouping induced by the teacher and the ground truth label. AMI has value from 0 to 1 with higher value indicating more agreement.</figDesc><table><row><cell></cell><cell>ChestX</cell><cell>ISIC</cell><cell>EuroSAT</cell><cell>CropDisease</cell></row><row><cell>AMI</cell><cell>0.0075</cell><cell>0.0427</cell><cell>0.3079</cell><cell>0.2969</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>5-way 1-shot (top) and 5-way 5-shot (bottom) classification accuracy on miniImagenet and tieredImageNet. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05.</figDesc><table><row><cell>Methods (k=1)</cell><cell>miniImageNet</cell><cell>tieredImageNet-less</cell><cell>tieredImageNet-more</cell></row><row><cell>Transfer</cell><cell>54.18 ± 0.79</cell><cell>57.29 ± 0.83</cell><cell>57.68 ± 0.89</cell></row><row><cell>STARTUP-T (no SS)</cell><cell>53.91 ± 0.79</cell><cell>60.39 ± 0.86</cell><cell>61.00 ± 0.86</cell></row><row><cell>STARTUP (no SS)</cell><cell>53.74 ± 0.80</cell><cell>55.49 ± 0.85</cell><cell>57.19 ± 0.89</cell></row><row><cell cols="2">STARTUP-Rand (no SS) 54.15 ± 0.81</cell><cell>56.93 ± 0.91</cell><cell>63.29 ± 0.90</cell></row><row><cell>STARTUP-T</cell><cell>54.00 ± 0.80</cell><cell>60.19 ± 0.86</cell><cell>60.16 ± 0.86</cell></row><row><cell>STARTUP</cell><cell>54.20 ± 0.81</cell><cell>55.33 ± 0.85</cell><cell>53.88 ± 0.89</cell></row><row><cell>STARTUP-Rand</cell><cell>53.89 ± 0.87</cell><cell>56.93 ± 0.91</cell><cell>61.95 ± 0.93</cell></row><row><cell>Methods (k=5)</cell><cell>miniImageNet</cell><cell>tieredImageNet-less</cell><cell>tieredImageNet-more</cell></row><row><cell>Transfer</cell><cell>76.20 ± 0.64</cell><cell>79.05 ± 0.65</cell><cell>78.67 ± 0.69</cell></row><row><cell>STARTUP-T (no SS)</cell><cell>76.26 ± 0.64</cell><cell>80.14 ± 0.65</cell><cell>79.61 ± 0.68</cell></row><row><cell>STARTUP (no SS)</cell><cell>76.42 ± 0.63</cell><cell>78.36 ± 0.66</cell><cell>78.50 ± 0.68</cell></row><row><cell cols="2">STARTUP-Rand (no SS) 73.77 ± 0.66</cell><cell>79.58 ± 0.69</cell><cell>81.60 ± 0.66</cell></row><row><cell>STARTUP-T</cell><cell>76.21 ± 0.63</cell><cell>79.40 ± 0.67</cell><cell>79.04 ± 0.68</cell></row><row><cell>STARTUP</cell><cell>76.48 ± 0.63</cell><cell>77.78 ± 0.67</cell><cell>77.48 ± 0.68</cell></row><row><cell>STARTUP-Rand</cell><cell>71.08 ± 0.72</cell><cell>79.58 ± 0.69</cell><cell>81.03 ± 0.66</cell></row><row><cell cols="4">novel set as the unlabeled dataset and use the same teacher as in section 5.1. For tieredImageNet, we</cell></row><row><cell cols="4">use ResNet-12 (Oreshkin et al., 2018) as our model architecture and evaluate two different setups -</cell></row><row><cell cols="4">tieredImageNet-less that uses 10% of the novel set as unlabeled data (following Ren et al. (2018))</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>; our re-implementation of Transfer uses a different batch size and 80% of the original test set for evaluation. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05. 5-way k-shot classification accuracy on miniImageNet→BSCD-FSL for higher shots. Mean and 95% confidence interval are reported. * are methods reported in<ref type="bibr" target="#b12">(Guo et al., 2020)</ref>. Despite using their code, difference in batch size and test set (80% of the original test set) have resulted in discrepancies between our Transfer and their Transfer * . (no SS) indicates removal of SimCLR. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell></row><row><cell></cell><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell></cell><cell>MAML  *</cell><cell>-</cell><cell>23.48 ± 0.96</cell><cell>-</cell><cell>40.13 ± 0.58</cell></row><row><cell></cell><cell>ProtoNet  *</cell><cell>-</cell><cell>24.05 ± 1.01</cell><cell>-</cell><cell>39.57 ± 0.57</cell></row><row><cell></cell><cell>ProtoNet + FWT  *</cell><cell>-</cell><cell>23.77 ± 0.42</cell><cell>-</cell><cell>38.87 ± 0.52</cell></row><row><cell></cell><cell>MetaOpt  *</cell><cell>-</cell><cell>22.53 ± 0.91</cell><cell>-</cell><cell>36.28 ± 0.50</cell></row><row><cell></cell><cell>Transfer  *</cell><cell>-</cell><cell>25.35 ± 0.96</cell><cell>-</cell><cell>43.56 ± 0.60</cell></row><row><cell>.</cell><cell>Transfer</cell><cell cols="2">22.71 ± 0.40 26.71 ± 0.46</cell><cell cols="2">30.71 ± 0.59 43.08 ± 0.57</cell></row><row><cell></cell><cell>SimCLR</cell><cell cols="2">22.10 ± 0.41 25.02 ± 0.42</cell><cell cols="2">26.25 ± 0.53 36.09 ± 0.57</cell></row><row><cell></cell><cell>Transfer + SimCLR</cell><cell cols="2">22.70 ± 0.40 26.95 ± 0.45</cell><cell cols="2">32.63 ± 0.63 45.96 ± 0.61</cell></row><row><cell></cell><cell>STARTUP-T (no SS)</cell><cell cols="2">22.79 ± 0.41 26.03 ± 0.43</cell><cell cols="2">32.37 ± 0.61 45.20 ± 0.61</cell></row><row><cell></cell><cell>STARTUP (no SS)</cell><cell cols="2">22.87 ± 0.41 26.68 ± 0.45</cell><cell cols="2">32.24 ± 0.62 46.48 ± 0.61</cell></row><row><cell></cell><cell>STARTUP-T</cell><cell cols="2">22.75 ± 0.40 26.47 ± 0.43</cell><cell cols="2">32.16 ± 0.60 45.75 ± 0.60</cell></row><row><cell></cell><cell>STARTUP</cell><cell cols="2">23.09 ± 0.43 26.94 ± 0.44</cell><cell cols="2">32.66 ± 0.60 47.22 ± 0.61</cell></row><row><cell></cell><cell>STARTUP (w/ Rotation)</cell><cell cols="2">22.83 ± 0.42 26.38 ± 0.43</cell><cell cols="2">31.54 ± 0.61 45.68 ± 0.60</cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell>CropDisease</cell></row><row><cell></cell><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell></cell><cell>MAML  *</cell><cell>-</cell><cell>71.70 ± 0.72</cell><cell>-</cell><cell>78.05 ± 0.68</cell></row><row><cell></cell><cell>ProtoNet  *</cell><cell>-</cell><cell>73.29 ± 0.71</cell><cell>-</cell><cell>79.72 ± 0.67</cell></row><row><cell></cell><cell>ProtoNet + FWT  *</cell><cell>-</cell><cell>67.34 ± 0.76</cell><cell>-</cell><cell>72.72 ± 0.70</cell></row><row><cell></cell><cell>MetaOpt  *</cell><cell>-</cell><cell>64.44 ± 0.73</cell><cell>-</cell><cell>68.41 ± 0.73</cell></row><row><cell></cell><cell>Transfer  *</cell><cell>-</cell><cell>75.69 ± 0.66</cell><cell>-</cell><cell>87.48 ± 0.58</cell></row><row><cell></cell><cell>Transfer</cell><cell cols="2">60.73 ± 0.86 80.30 ± 0.64</cell><cell cols="2">69.97 ± 0.85 90.16 ± 0.49</cell></row><row><cell></cell><cell>SimCLR</cell><cell cols="2">43.52 ± 0.88 59.05 ± 0.70</cell><cell cols="2">78.23 ± 0.83 92.57 ± 0.48</cell></row><row><cell></cell><cell>Transfer + SimCLR</cell><cell cols="2">57.18 ± 0.87 77.61 ± 0.66</cell><cell cols="2">76.90 ± 0.78 92.64 ± 0.44</cell></row><row><cell></cell><cell>STARTUP-T (no SS)</cell><cell cols="2">63.00 ± 0.84 81.25 ± 0.62</cell><cell cols="2">71.11 ± 0.83 90.79 ± 0.49</cell></row><row><cell></cell><cell>STARTUP (no SS)</cell><cell cols="2">62.90 ± 0.83 81.81 ± 0.61</cell><cell cols="2">73.30 ± 0.82 91.69 ± 0.47</cell></row><row><cell></cell><cell>STARTUP-T</cell><cell cols="2">63.49 ± 0.85 81.54 ± 0.63</cell><cell cols="2">72.85 ± 0.83 91.49 ± 0.48</cell></row><row><cell></cell><cell>STARTUP</cell><cell cols="2">63.88 ± 0.84 82.29 ± 0.60</cell><cell cols="2">75.93 ± 0.80 93.02 ± 0.45</cell></row><row><cell></cell><cell>STARTUP (w/ Rotation)</cell><cell cols="2">62.18 ± 0.86 81.37 ± 0.65</cell><cell cols="2">70.53 ± 0.84 90.59 ± 0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>5-way k-shot classification accuracy on ImageNet(ILSVRC 2012)→BSCD-FSL. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05. (no SS) 70.08 ± 0.80 87.12 ± 0.45 80.13 ± 0.77 94.51 ± 0.38 STARTUP 73.83 ± 0.77 89.70 ± 0.41 85.10 ± 0.74 96.06 ± 0.33 Table 8: 5-way k-shot classification accuracy on ImageNet(ILSVRC 2012)→BSCD-FSL for higher shots. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05. IMPACT OF DIFFERENT AMOUNT OF UNLABELED EXAMPLES STARTUP uses unlabeled data to adapt feature representations to novel domains. As with all learning techniques, it should perform better with more unlabeled data. To investigate how the amount of unlabeled examples impacts STARTUP, we repeated the miniImageNet → ISIC experiments in 5.1 with various amount of unlabeled data (20% of the dataset (2003 examples) is set aside for evaluation).</figDesc><table><row><cell>Methods</cell><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell></row><row><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell>Transfer</cell><cell cols="2">21.97 ± 0.39 25.85 ± 0.41</cell><cell cols="2">30.27 ± 0.51 43.88 ± 0.56</cell></row><row><cell>STARTUP (no SS)</cell><cell cols="2">22.90 ± 0.40 26.74 ± 0.46</cell><cell cols="2">30.18 ± 0.56 44.19 ± 0.57</cell></row><row><cell>STARTUP</cell><cell cols="2">23.03 ± 0.42 27.24 ± 0.46</cell><cell cols="2">31.69 ± 0.59 46.02 ± 0.59</cell></row><row><cell>Methods</cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell>CropDisease</cell></row><row><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell>Transfer</cell><cell cols="2">66.08 ± 0.81 85.58 ± 0.48</cell><cell cols="2">74.17 ± 0.82 92.46 ± 0.42</cell></row><row><cell>Methods</cell><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell></row><row><cell></cell><cell>k=20</cell><cell>k=50</cell><cell>k=20</cell><cell>k=50</cell></row><row><cell>Transfer</cell><cell cols="2">30.28 ± 0.45 32.55 ± 0.46</cell><cell cols="2">55.14 ± 0.60 60.99 ± 0.60</cell></row><row><cell>STARTUP (no SS)</cell><cell cols="2">31.98 ± 0.47 34.22 ± 0.47</cell><cell cols="2">55.54 ± 0.57 61.54 ± 0.55</cell></row><row><cell>STARTUP</cell><cell cols="2">32.40 ± 0.45 34.95 ± 0.48</cell><cell cols="2">57.06 ± 0.58 62.94 ± 0.56</cell></row><row><cell>Methods</cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell>CropDisease</cell></row><row><cell></cell><cell>k=20</cell><cell>k=50</cell><cell>k=20</cell><cell>k=50</cell></row><row><cell>Transfer</cell><cell cols="2">91.78 ± 0.33 93.76 ± 0.29</cell><cell cols="2">96.96 ± 0.25 98.10 ± 0.19</cell></row><row><cell>STARTUP (no SS)</cell><cell cols="2">92.60 ± 0.31 94.53 ± 0.26</cell><cell cols="2">97.94 ± 0.20 98.62 ± 0.16</cell></row><row><cell>STARTUP</cell><cell cols="2">94.27 ± 0.26 95.61 ± 0.23</cell><cell cols="2">98.55 ± 0.17 99.07 ± 0.13</cell></row><row><cell>A.5</cell><cell></cell><cell></cell><cell></cell></row></table><note>STARTUP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 :</head><label>10</label><figDesc>5-way k-shot classification accuracy on miniImageNet→BSCD-FSL. We compare STARTUP to fine-tuning. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05. 63.88 ± 0.84 82.29 ± 0.60 75.93 ± 0.80 93.02 ± 0.45 Fine-tuning 62.86 ± 0.85 82.36 ± 0.61 76.13 ± 0.78 93.01 ± 0.44</figDesc><table><row><cell>Methods</cell><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell></row><row><cell>.</cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row><row><cell>STARTUP</cell><cell cols="2">23.09 ± 0.43 26.94 ± 0.44</cell><cell cols="2">32.66 ± 0.60 47.22 ± 0.61</cell></row><row><cell>Fine-tuning</cell><cell cols="2">22.76 ± 0.41 27.05 ± 0.45</cell><cell cols="2">32.45 ± 0.61 45.73 ± 0.61</cell></row><row><cell>Methods</cell><cell></cell><cell>EuroSAT</cell><cell></cell><cell>CropDisease</cell></row><row><cell></cell><cell>k=1</cell><cell>k=5</cell><cell>k=1</cell><cell>k=5</cell></row></table><note>STARTUP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 :</head><label>11</label><figDesc>Few-shot classification accuracy on ImageNet(ILSVRC 2012)→BSCD-FSL with STARTUP on different datasets. STARTUP-X represents the STARTUP student trained on ImageNet and dataset X. The top table presents the results for 5-way 1-shot and the bottom table presents the results for 5-way 5-shot. Bolded entries are top performing methods that are not different based on t-test at significant level 0.05.</figDesc><table><row><cell>Methods</cell><cell>ChestX</cell><cell>ISIC</cell><cell>EuroSAT</cell><cell>CropDisease</cell></row><row><cell>STARTUP-ChestX</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/IBM/cdfsl-benchmark</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/sthalles/SimCLR</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>model and pick the learning rate that yields lowest loss on the validation set as the starting learning rate. We reduce the learning rate by a factor of 2 when the training loss has not decreased by 20 epochs. The model that achieves the lowest loss on the internal validation set throughout the 1000 epochs of training is picked as the final model.</p><p>SimCLR. Our implementation of SimCLR's loss function is based on a publicly available implementation of SimCLR 2 . We added the two-layer projection head on top of the embedding function φ. The temperature of NT-Xent is set to 1 since there is no validation set for BSCD-FSL for hyperparameter selection and we use a temperature of 1 when inferring the soft label of the unlabeled set. For the stochastic image augmentations for SimCLR, we use the augmentations defined for each novel dataset in <ref type="bibr" target="#b12">Guo et al. (2020)</ref>. These augmentations include the commonly used "randomly resized crop", color jittering, random horizontal flipping. For tieredImageNet and miniImageNet, we use the stochastic transformation implemented for the BSCD-FSL benchmark. We refer readers to the BSCD-FSL implementation for more details.</p><p>When training the student on the base dataset, we use the augmentation used for training the teacher for fair comparison. The batchsize for SIMCLR is set to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 TRAINING LINEAR CLASSIFIER.</head><p>We use the implementation by BSCD-FSL, i.e training the linear classifier with standard cross entropy and SGD optimizer. The linear classifier is trained for 100 epochs with learning rate 0.01, momentum 0.9 and weight decay 1e-4. Transfer. This is implemented using the teacher model as feature extractor. Please see A.1.1 for details.</p><p>SimCLR. This is implemented similarly to the SimCLR loss described in A.1.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.5 T-SNE</head><p>We use the publicly available scikit-learn implementation of t-SNE <ref type="bibr" target="#b0">(Buitinck et al., 2013)</ref>. We used the default parameters except for the perplexity where we set to 50. To speed up the experiment, we randomly sampled 25% of the data used for sampling few-shot tasks (80 % of the full dataset) and run t-SNE on this subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 FULL RESULTS ON BSCD-FSL</head><p>We present the result on miniImageNet → BSCD-FSL for shot = 1, 5, 20, 50 in Table <ref type="table">5 and 6</ref>. In addition to STARTUP, we also reported results on using teacher model as student initialization (STARTUP-T and STARTUP-T (no SS)) in these tables for reference. Results on ImageNet → BSCD-FSL can be found in Table <ref type="table">7 and 8</ref>. The conclusions we found in 5.1 still hold for higher shots in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 USING ROTATION FOR SELF-SUPERVISION.</head><p>We use rotation <ref type="bibr">(Gidaris et al. (2018)</ref>) instead of SimCLR in STARTUPand report the results in in Table 5 and 6. We observe that STARTUP (w/ Rotation) is able to outperform Transfer in CropDisease and EuroSAT but generally underperforms its SimCLR counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 INITIALIZATION STRATEGIES FOR THE STUDENT</head><p>We investigate the impact of different initialization strategies for the student on STARTUP. For this experiment, we remove SimCLR from STARTUP and consider three initialization strategies for the student -from scratch (STARTUP-Rand (no SS)), from teacher embedding with a randomly  We consider the ImageNet → CD-FSL experiment. We perform STARTUP on unlabeled data different from the target domain and present the result in Table <ref type="table">11</ref>. We found that it is crucial that the unlabeled data to perform STARTUP on should be from the target domain of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">API design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaques</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Van-Derplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop: Languages for Data Mining and Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boosting fewshot visual learning with self-supervision</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new benchmark for evaluation of cross-domain few-shot learning</title>
		<author>
			<persName><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Noel Cf Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajana</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">M-adda: Unsupervised domain adaptation with deep metric learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><surname>Babanezhad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation for Visual Understanding</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName><forename type="first">Xinzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10276" to="10286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reptile: a scalable metalearning algorithm</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Rodríguez López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<editor>
			<persName><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</editor>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<publisher>Sachin Ravi and Hugo Larochelle</publisher>
			<date type="published" when="2017">2019. 2018. 2017</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName><forename type="first">Issam</forename><surname>Pau Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">When does self-supervision improve fewshot learning?</title>
		<author>
			<persName><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking fewshot image classification: a good embedding is all you need?</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-domain few-shot classification via learned feature-wise transformation</title>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extending and analyzing self-supervised learning across domains</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instance credibility inference for few-shot learning</title>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12836" to="12845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1426" to="1435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transmatch: A transfer-learning scheme for semi-supervised few-shot learning</title>
		<author>
			<persName><forename type="first">Zhongjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12856" to="12864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">1 IMPLEMENTATION DETAILS We implemented STARTUP by modifying the the publicly-available implementation 1 of BSCD-FSL by</title>
		<author>
			<persName><forename type="first">A</forename><surname>Appendix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<editor>Guo et al.</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">MiniImageNet: We train the teacher model using the code provided in the BSCD-FSL benchmark. We keep everything the same except setting the batch size from 16 to 256</title>
		<idno>A.1.1 TRAINING THE TEACHER 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">TieredImageNet: We used the same setup as miniImageNet except we reduce the number of epochs to 90. We do not use any image augmentation for tieredImageNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">ImageNet: We used the pretrained ResNet18 available on PyTorch</title>
		<author>
			<persName><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">We use a batch size of 256 on the unlabeled dataset and a batch size of 256 for the base dataset if applicable. We use the SGD with momentum optimizer with momentum 0.9 and weight decay 1e-4. To pick the suitable starting learning rate, 10% of the unlabeled data and 5% of the labeled data (1% when using ImageNet as the base dataset) are set aside as our internal validation set. We pick the starting learning rate by training the student with starting learning rate lr ∈ {1e-1</title>
		<idno>23.03 ± 0.42 31.02 ± 0.55 65.20 ± 0.87 70.36 ± 0.86</idno>
		<imprint>
			<date>5e-2, 3e-2, 1e-2, 5e-3, 3e-3, 1e</date>
		</imprint>
	</monogr>
	<note>-3} for k epochs where k is the smallest epoch that guarantees at least 50 updates to the</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
		<idno>27.24 ± 0.46 44.14 ± 0.59 83.76 ± 0.56 89.95 ± 0.49</idno>
	</analytic>
	<monogr>
		<title level="j">Methods ChestX ISIC EuroSAT CropDisease STARTUP-ChestX</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
