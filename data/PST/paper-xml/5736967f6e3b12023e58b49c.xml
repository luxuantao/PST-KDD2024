<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient Domain Guided Image Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Kou</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Weihai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Changyun</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior member, IEEE</roleName><forename type="first">Zhengguo</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Gradient Domain Guided Image Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFF16BAA100E85BB0C5E2607E58B6198</idno>
					<idno type="DOI">10.1109/TIP.2015.2468183</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2468183, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2468183, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Guided image filter</term>
					<term>gradient domain</term>
					<term>edgepreserving</term>
					<term>detail enhancement</term>
					<term>high dynamic range</term>
					<term>saliency detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Guided image filter (GIF) is a well-known local filter for its edge-preserving property and low computational complexity. Unfortunately, the GIF may suffer from halo artifacts because the local linear model used in the GIF cannot represent the image well near some edges. In this paper, a gradient domain guided image filter is proposed by incorporating an explicit first-order edge-aware constraint. The edge-aware constraint makes edges be preserved better. To illustrate efficiency of the proposed filter, the proposed gradient domain guided image filter is applied for single image detail enhancement, tone mapping of high dynamic range (HDR) images and image saliency detection. Both theoretical analysis and experimental results prove that the proposed gradient domain guided image filter can produce better resultant images, especially near the edges where halos appear in the original GIF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Edge preserving smoothing is required by lots of applications in image processing, computation photography and computer vision, such as image detail enhancement <ref type="bibr" target="#b0">[1]</ref>, tone mapping of high dynamic range (HDR) images <ref type="bibr" target="#b1">[2]</ref>, joint upsampling <ref type="bibr" target="#b2">[3]</ref>, structure extraction from texture <ref type="bibr" target="#b3">[4]</ref> and correspondence search <ref type="bibr" target="#b4">[5]</ref>. With an edge-preserving smoothing algorithm, the details in the input image will be smoothed while the edges be preserved. The detail layer of the input image can also be obtained by subtracting the smoothed image from the input image. By amplifying the detail layer, a detail enhanced image is produced. Therefore, edge-preserving smoothing algorithms can also be used as edge-preserving enhancing/decomposition algorithms.</p><p>All the edge-preserving decomposition algorithms can be separated into two categories: one is local filter based algorithms such as median filter <ref type="bibr" target="#b5">[6]</ref>, bilateral filter (BLF) <ref type="bibr" target="#b6">[7]</ref>, its accelerated versions <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and its iterative version <ref type="bibr" target="#b10">[11]</ref>, guided image filter (GIF) <ref type="bibr" target="#b9">[10]</ref> and weighted guided image filter(WGIF) <ref type="bibr" target="#b16">[17]</ref>, the other is global optimization based algorithms such as total varition (TV) <ref type="bibr" target="#b11">[12]</ref>, its iterative shrinkage approach <ref type="bibr" target="#b12">[13]</ref> and its extension <ref type="bibr" target="#b3">[4]</ref>, weighted least This work has been supported by National Nature Science Foundation of China under the research project 51475017 and the China Scholarship Council.</p><p>Fei Kou and Weihai Chen are with the School of Automation Science and Electrical Engineering, Beihang University, Beijing, China 100191. Fei Kou is also with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798 (e-mail: koufei@buaa.edu.cn, whchen@buaa.edu.cn).</p><p>Changyun Wen is with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798 (e-mail: ecywen@ntu.edu.sg).</p><p>Zhengguo Li is with Signal Processing Department, Institute for Infocomm Research, Singapore 138632 (email: ezgli@i2r.a-star.edu.sg).</p><p>(Corresponding author: W. Chen.)</p><p>squares (WLS) <ref type="bibr" target="#b13">[14]</ref> and its accelerated version, fast weighted least squares (FWLS) <ref type="bibr" target="#b14">[15]</ref>, and L 0 norm gradient minimization <ref type="bibr" target="#b15">[16]</ref>. The global optimization based filters always give better results. All these algorithms are obtained by solving an optimization problem. The optimization problem is formulated as combination of a fidelity term and a regularization smooth term. With different fidelity terms or different regularization terms, different methods are proposed and different results are established. All these problems are solved after a number of iterations, so these global optimization based algorithms are usually very time consuming. An interesting concept of texture removal filter was firstly proposed in <ref type="bibr" target="#b3">[4]</ref>, the structure texture of the image was removed via solving a total variation based optimization problem. In <ref type="bibr" target="#b20">[21]</ref>, a patch based solution was proposed. In <ref type="bibr" target="#b21">[22]</ref>, a bilateral texture filter was proposed to remove texture in images. In the rolling guidance filter <ref type="bibr" target="#b10">[11]</ref>, the joint bilateral filter was iteratively invoked a few times. As a result, the texture in the image are removed. The local filter based filters usually have better efficiency, but the resultant image may suffer from artifacts. Median filter, widely known as an image de-noise filter, can also be used as a simple edgepreserving decomposition filter. Weighted median filter <ref type="bibr" target="#b17">[18]</ref> can filter images with the weight from a guidance image, but the the speed could be an issue. In <ref type="bibr" target="#b18">[19]</ref>, an interesting constant time weighted median filter was proposed. In <ref type="bibr" target="#b19">[20]</ref>, a novel fast weighted median filter was proposed, the fast implementation makes the weighted median filter more practical. Bilateral filtering (BLF) <ref type="bibr" target="#b6">[7]</ref> processes images by combining a range filter with a domain filter to preserve edges. It is a simple and widely used weighted average filter, but it may suffer from gradient reversal artifacts near some edges when used for detail enhancement <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Guided image filter (GIF) <ref type="bibr" target="#b9">[10]</ref> was proposed to avoid gradient reversal artifacts and it is derived from a local linear model. The main idea is using a linear transform to represent the pixel values in a window. Different from other algorithms, the GIF computes the resulting image by taking the structure of a guidance image into consideration and it is one of the fastest edge-preserving smoothing filters. Nevertheless, the model can not represent the image well near some edges. As a result, there may be some halos in the images <ref type="bibr" target="#b9">[10]</ref>. This happens in some GIF based applications and it is most apparent in the detail enhanced images obtained by the GIF. The halos reduce the visual quality of the resulting images, and thus it is the main drawback of the GIF. In <ref type="bibr" target="#b16">[17]</ref>, a weighted guided image filter (WGIF) was proposed to reduce the halo artifacts of the GIF. An edge aware factor was introduced to the constraint term of the GIF, the factor makes the edges preserved better in the result images and thus reduces the halo artifacts. However, zeroth-order (intensity domain) constraints are specified to get desired pixel values and firstorder (gradient domain) constraints to smooth the pixel values in both the GIF and the WGIF. Since there are no explicit constraints to treat edges in both of them, they cannot preserve edges well in some cases because they consider image filtering process and edge-preserving process together. It is widely believed that gradients are integral to the way in which human beings perceive images, and human cortical cells could be hard wired to preferentially respond to high contrast stimulus in their receptive fields <ref type="bibr" target="#b22">[23]</ref>, which directly correlate with gradients in an image. It is thus desired to design a new local filter which has explicit constraints to treat edges so as to make the gradient of the input image and output image be more similar.</p><p>In this paper, a gradient domain guided image filter is proposed by incorporating an explicit first-order edge-aware constraint.</p><p>The proposed filter is based on local optimization and the cost function is composed of a zeroth order data fidelity term and a first order regularization term. The regularization term includes an explicit edge aware constraint which is different from the regularization terms in both the GIF <ref type="bibr" target="#b9">[10]</ref> and the WGIF <ref type="bibr" target="#b16">[17]</ref>. As a result, the factors in the new local linear model can represent the images more accurately near edges. Edges are preserved much better. In addition, compared with the WGIF in <ref type="bibr" target="#b16">[17]</ref>, the edge-aware factor is multi-scale while it is single scale in the WGIF. The large scale weight cooperates with the small scale weight proposed in the WGIF, becoming a multiscale weight. The multi-scale factor can separate edges of an image from fine details of the image better. So the performance is highly improved, especially when fine details of an image is enhanced. Similar to the GIF in <ref type="bibr" target="#b9">[10]</ref> and the WGIF in <ref type="bibr" target="#b16">[17]</ref>, the proposed filter also avoids gradient reversal. In addition, the complexity of the proposed filter is O(N ) for an image with N pixels which is the same as that of the GIF in <ref type="bibr" target="#b9">[10]</ref> and the WGIF in <ref type="bibr" target="#b16">[17]</ref>. These features allow many applications of the proposed filter in the fields of computational photography and image processing. The proposed filter is first applied for single image detail enhancement and tone mapping of HDR images. Experimental results of both applications show that the resultant algorithms produce images with better visual quality than both the GIF in <ref type="bibr" target="#b8">[9]</ref> and the WGIF in <ref type="bibr" target="#b13">[14]</ref>. Besides single image detail enhancement and tone mapping of HDR images, one new application is proposed in this paper, namely it is used as a post-processing tool for image saliency detection. Experimental results show the proposed gradient domain GIF can increase the accuracy of saliency detection.</p><p>The paper is organized as follows. Section II introduces the related works on guided image filtering. Then, the gradient domain guided image filtering is proposed in Section III. Followed by the applications and experimental results of the proposed filter in Section IV. Finally, Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS ON GUIDED IMAGE FILTERING</head><p>In the GIF, there are a guidance image G and an image to be filtered X. They could be identical. Let Ω ζ1 (p) be a square window centered at a pixel p of a radius ζ 1 . It is assumed that the output image Ẑ is a linear transform of the guidance image G in the window Ω ζ1 (p ) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_0">Ẑ(p) = a p G(p) + b p , ∀p ∈ Ω ζ1 (p ),<label>(1)</label></formula><p>where a p and b p are two constants in the window Ω ζ1 (p ).</p><p>Their values are obtained by minimizing a cost function E(a p , b p ) which is defined as</p><formula xml:id="formula_1">E = p∈Ω ζ 1 (p ) [(a p G(p) + b p -X(p)) 2 + λa 2 p ],<label>(2)</label></formula><p>where λ is a regularization parameter penalizing large a p . The optimal values of a p and b p are computed as</p><formula xml:id="formula_2">a p = µ G X,ζ1 (p ) -µ G,ζ1 (p )µ X,ζ1 (p ) σ 2 G,ζ1 (p ) + λ ,<label>(3)</label></formula><formula xml:id="formula_3">b p = µ X,ζ1 (p ) -a p µ G,ζ1 (p ),<label>(4)</label></formula><p>where is the element-wise product of two matrices. µ G X,ζ1 (p ), µ G,ζ1 (p ) and µ X,ζ1 (p ) are the mean values of G X, G and X in the window Ω ζ1 (p ), respectively.</p><p>The GIF is one of the fastest edge-preserving local filters and it outperforms the bilateral filter <ref type="bibr" target="#b6">[7]</ref> in the sense that the GIF can avoid gradient reversal artifacts. However, the value of λ in the GIF <ref type="bibr" target="#b9">[10]</ref> is fixed. As such, halos are unavoidable for the GIF in <ref type="bibr" target="#b9">[10]</ref> when it is forced to smooth edges. A content adaptive GIF was proposed in <ref type="bibr" target="#b16">[17]</ref> to overcome the problem. The cost function in Equation ( <ref type="formula" target="#formula_1">2</ref>) is replaced by the following one:</p><formula xml:id="formula_4">E = p∈Ω ζ 1 (p ) [(a p G(p) + b p -X(p)) 2 + λ Γ G (p ) a 2 p ],<label>(5)</label></formula><p>where Γ G (p ) is an edge aware weighting and it is defined by using local variances of 3×3 windows of all pixels as follows:</p><formula xml:id="formula_5">Γ G (p ) = 1 N N p=1 σ 2 G,1 (p ) + ε σ 2 G,1 (p) + ε ,<label>(6)</label></formula><formula xml:id="formula_6">σ 2 G,1 (p ) is the variance of G in the window Ω 1 (p )</formula><p>. ε is a small positive constant and its value is selected as (0.001×L) 2 while L is the dynamic range of the input image. All pixels in the guidance image are used in the computation of Γ G (p ). In addition, the weighting Γ G (p ) measures the importance of pixel p with respect to the whole guidance image. Due to the box filter in <ref type="bibr" target="#b9">[10]</ref>, the complexity of Γ G (p ) is O(N ) for an image with N pixels.</p><p>The optimal values of a p and b p are computed as</p><formula xml:id="formula_7">a p = µ G X,ζ1 (p ) -µ G,ζ1 (p )µ X,ζ1 (p ) σ 2 G,ζ1 (p ) + λ Γ G (p ) ,<label>(7)</label></formula><formula xml:id="formula_8">b p = µ X,ζ1 (p ) -a p µ G,ζ1 (p ).<label>(8)</label></formula><p>The WGIF in <ref type="bibr" target="#b16">[17]</ref> can be applied to reduce halo artifacts. However, both the GIF and the WGIF specify intensitydomain constraints (i.e., zeroth-order constraints) to obtain desired pixel values and gradient-domain constrains (i.e., firstorder constraints) to smooth the pixel values over space and time. There are no explicit constraints to treat edges in both methods. Image filtering is usually an image coarsening process accompanying with image smoothing. When image filtering and edge-preserving are considered together, edges may be smoothed inevitably. As a result, these edge-preserving methods cannot preserve edges well in some cases <ref type="bibr" target="#b25">[26]</ref>. In the next section, a gradient domain GIF is introduced which includes an explicit first-order edge-aware constraint. The new constraint can be seamlessly integrated into the WGIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRADIENT DOMAIN GUIDED IMAGE FILTERING</head><p>Inspired by the Gradientshop in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>, a gradient domain GIF is introduced in this section. The proposed filter includes an explicit first-order edge-aware constraint and it thus preserves edges better than both the GIF and the WGIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A New Edge-Aware Weighting</head><p>A new edge-aware weighting ΓG (p ) is defined by using local variances of 3 × 3 windows and (2ζ 1 + 1) × (2ζ 1 + 1) windows of all pixels as follows:</p><formula xml:id="formula_9">ΓG (p ) = 1 N N p=1 χ(p ) + ε χ(p) + ε ,<label>(9)</label></formula><p>where </p><formula xml:id="formula_10">χ(p ) is defined as σ G,1 (p )σ G,ζ1<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Filter</head><p>It is shown in the linear model (1) that ∇ Ẑ(p) = a p ∇G(p).</p><p>Clearly, the smoothness of Ẑ in Ω ζ1 (p ) depends on the value of a p . If the value of a p is 1, the edge is then well preserved. This is expected if the pixel p is at an edge. On the other hand, if the pixel p is in a flat region, it is then expected that the value of a p is 0 such that the flat region is well smoothed. Based on the observation, a new cost function is defined as</p><formula xml:id="formula_11">E = p∈Ω ζ 1 (p ) [(a p G(p)+b p -X(p)) 2 + λ ΓG (p ) (a p -γ p ) 2 ],<label>(10)</label></formula><p>where γ p is defined as</p><formula xml:id="formula_12">γ p = 1 - 1 1 + e η(χ(p )-µχ,∞) ,<label>(11)</label></formula><p>µ χ,∞ is the mean value of all χ(p). η is calculated as 4/(µ χ,∞ -min(χ(p))). It is worth noting that the value of γ p approaches 1 if the pixel p is at an edge and 0 if it is in a smooth region. In other words, the value of a p is expected to approach 1 if the pixel p is at an edge and 0 if it is in a smooth region. As such, the proposed filter is less sensitive to the selection of λ. Subsequently, edges could be preserved better by the proposed filter than both the GIF and the WGIF.</p><p>The optimal values of a p and b p are computed as</p><formula xml:id="formula_13">a p = µ G X,ζ1 (p ) -µ G,ζ1 (p )µ X,ζ1 (p ) + λ ΓG (p ) γ p σ 2 G,ζ1 (p ) + λ ΓG (p ) ,<label>(12)</label></formula><formula xml:id="formula_14">b p = µ X,ζ1 (p ) -a p µ G,ζ1 (p ). (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>The final value of Ẑ(p) is given as follows:</p><formula xml:id="formula_16">Ẑ(p) = āp G(p) + bp , (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>where āp and bp are the mean values of a p and b p in the window, respectively computed as <ref type="bibr" target="#b14">(15)</ref> and |Ω ζ1 (p )| is the cardinality of Ω ζ1 (p ).</p><formula xml:id="formula_18">āp = 1 |Ω ζ1 (p)| p ∈Ω ζ 1 (p) a p ; bp = 1 |Ω ζ1 (p)| p ∈Ω ζ 1 (p) b p ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of the Proposed Filter</head><p>For easy analysis, the images X and G are assumed to be the same. Two cases are studied as below.</p><p>1) The pixel p is at an edge. The value of γ p is usually 1. The value of a p is computed as</p><formula xml:id="formula_19">a p = σ 2 G,ζ1 (p ) + λ ΓG (p ) σ 2 G,ζ1 (p ) + λ ΓG (p ) = 1.<label>(16)</label></formula><p>The value of a p is 1 regardless of the value of λ. Clearly, the value of a p is closer to 1 than both a p in the GIF <ref type="bibr" target="#b9">[10]</ref> and a p in the WGIF <ref type="bibr" target="#b16">[17]</ref> if the pixel p is at an edge. This implies that sharp edges are preserved better by the proposed filter than both the GIF and the WGIF.</p><p>2) The pixel p is in a flat area. The value of γ p is usually 0 and the value of ΓG (p ) is usually smaller than 1. The value of a p is computed as</p><formula xml:id="formula_20">a p = σ 2 G,ζ1 (p ) σ 2 G,ζ1 (p ) + λ ΓG (p ) . (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>Since the value of a p is 1 regardless of the choice of λ if the pixel p is at an edge, a larger λ is selected in the proposed filter than the λ in the GIF and the WGIF because the selection will not affect the preservation of edges by the proposed filter. Obviously, this results in that the value of a p is closer to 0 if the pixel p is in a flat area. This means that the proposed filter smooth the flag area better than both the GIF and the WGIF. To verify the analysis above, one smoothing result is presented.</p><p>To better observe the difference, we only show the 1 dimension value. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, edges are preserved better by the proposed filter than both the GIF in <ref type="bibr" target="#b9">[10]</ref> and the WGIF in <ref type="bibr" target="#b16">[17]</ref>. From the zoomed-in patches showed in the figure, it is seen that the output values of the proposed filter are almost the same as the input values near edges while the output values of the GIF and the WGIF are far away from the input values. This proves our previous analysis that the gradient constraint can make the result more similar to the input data near edges. So the proposed gradient domain guided image filtering can preserve edges better than the GIF and the WGIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATIONS OF THE NEW FILTER</head><p>In this section, the proposed gradient domain guided image filter is adopted to study single image detail enhancement, tone mapping of HDR images and saliency detection. Readers are invited to read the electronic version with full-size figures in order to better appreciate the differences among images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Image Detail Enhancement</head><p>Single image detail enhancement is a typical example to compare performance of different filters from both the halo A detail enhanced image will be produced by amplifying the detail layer. In the following, we add four times of the detail layer to the input image to get the detail enhanced image.</p><p>First we compare the selection of parameter λ in Eq. 10 and the λ in the GIF. The results are shown in Fig. <ref type="figure">3</ref>. From left to right, it can be seen that with the increasing of λ, there will be more details in the detail layer and this results in a sharper detail enhanced image. On the other hand, it may cause more halo artifacts near the edges (e.g. around the flower, green, black artifacts) for larger λ.</p><p>At the same time, the difference between the proposed filter and the GIF can be seen in Fig. <ref type="figure">3</ref>. There are more edges in  the detail layer decomposed by the GIF than by the proposed gradient domain GIF. The same as the GIF, the results of the proposed filter are sharper with the increasing of λ. However, it can be seen that the result of proposed algorithm has less artifacts even with a larger λ. In this case, we can use a larger λ with the proposed filter without worrying about the halo artifacts.</p><p>Next we use the blind object image quality metric in <ref type="bibr" target="#b27">[28]</ref> to evaluate the detail enhanced image quality. The scores with this metric of the input and result images shown in Fig. <ref type="figure">3</ref> are given in the following table:</p><p>With this metric, a higher value represents a higher quality.</p><p>Clearly, the proposed gradient domain outperforms the original GIF in <ref type="bibr" target="#b9">[10]</ref>. From the table, we can also get that the score initially increases and then decreases as λ increases. This is because over-sharpened images may be resulted from excessively large values of λ, which are unnatural. At the same time, only the scores of two images generated by the original GIF are higher than the input image, whereas all the four images generated by the proposed gradient domain GIF have higher scores than the input image. Again, this shows we can use a larger λ with the proposed filter without worrying about the halo artifacts.</p><p>Now the proposed filter is compared with the GIF in <ref type="bibr" target="#b9">[10]</ref>, the WGIF in <ref type="bibr" target="#b16">[17]</ref>, the BLF in <ref type="bibr" target="#b6">[7]</ref>, the L 0 norm minimization in <ref type="bibr" target="#b15">[16]</ref> and the FWLS in <ref type="bibr" target="#b14">[15]</ref>. From the detail enhanced image shown in the first row of Fig. <ref type="figure" target="#fig_3">4</ref>, it is seen almost all the algorithms (except L 0 minimization because it is a sparse based algorithm) produce similar results with overall view, the differences are edges. From the detail layers shown in the second row of Fig. <ref type="figure" target="#fig_3">4</ref>, it is seen that the result of the WGIF is better than the original GIF, the BLF, the L 0 minimization and (g) Result image of L0 in <ref type="bibr" target="#b15">[16]</ref> (h) Result image of FWLS in <ref type="bibr" target="#b14">[15]</ref> (i) Zoom-in patch of (b) (j) Zoom-in patch of (c) (k) Zoom-in patch of (d) (l) Zoom-in patch of (f) (m) Zoom-in patch of (g) (n) Zoom-in patch of (h) (o) Zoom-in patch of (b) (p) Zoom-in patch of (c) (q) Zoom-in patch of (d) (r) Zoom-in patch of (f) (s) Zoom-in patch of (g) (t) Zoom-in patch of (h) the FWLS. There are less edges in the detail layer of the WGIF than the others, but they are still much more apparent than the detail layer generated by the proposed filter. It is worth noting that the λ value in the proposed method is larger than the values of λ in both the GIF and the WGIF. We can conclude from Fig. <ref type="figure">3</ref> that a larger λ may produce more artifacts, but a larger λ in the proposed gradient domain GIF produce less artifacts than the GIF and the WGIF. From the zoom-in patches shown in Fig. <ref type="figure" target="#fig_3">4</ref>, it is observed that the results of the proposed filter has less artifacts than all the other algorithms. There are halo artifacts in the results of the GIF, the WGIF and the BLF, while there are reversal artifacts in the results of the BLF, the l 0 minimization smoothing and the FWLS, but the proposed filter produces neither halo artifacts nor reversal artifacts.</p><p>From Figs. 5(a)-(t), the same conclusion can be drawn. The difference between Fig. <ref type="figure">5</ref> (c) and Fig. <ref type="figure">5 (d</ref>) is presented in Fig. <ref type="figure">5</ref> (e). It can be seen that the differences are mainly near edges. There are more halos in Fig. <ref type="figure">5</ref> (c) than Fig. <ref type="figure">5 (d)</ref>.</p><p>To better compare the WGIF and the proposed gradient domain GIF, one more set of images are shown in Figs. 5(u)-(x). These images are generated with a large λ, by setting it to 0.5 2 . It is seen that there are apparent black halos around the flowers in Fig. <ref type="figure">5</ref>(u). From the detail layers shown in Figs.</p><p>5(w)-(x), it is observed that lots of edges are separated into the detail layer by the WGIF. This is the reason of the halo  artifacts. With our proposed gradient domain constraint, the edges are preserved in the base layer even if λ is very large, so there is no halo in the detail enhanced image. This implies that the proposed filter outperforms the WGIF in the sense that the proposed filter is less sensitive to the value of λ.</p><p>We also use the blind object image quality metric in <ref type="bibr" target="#b27">[28]</ref> to compare different algorithms. The scores with this metric of the input and result images shown in Fig. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure"></ref>   The results prove that the proposed gradient domain GIF is better than the original GIF in <ref type="bibr" target="#b9">[10]</ref>, the WGIF in <ref type="bibr" target="#b16">[17]</ref>, the BLF in <ref type="bibr" target="#b6">[7]</ref>, the L 0 norm minimization in <ref type="bibr" target="#b15">[16]</ref> and the FWLS in <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tone mapping of HDR images</head><p>Similar to single image detail enhancement, tone mapping of HDR images is a widely studied application to verify the performance of an edge-preserving image filter. So we also apply the proposed filter in HDR image tone mapping to compare with the guided filter based algorithms.</p><p>HDR images are usually generated from several differently exposed images of the same scene, so an HDR image has more information than each of the differently exposed images.</p><p>Limited by the dynamic range of monitors and printers nowadays, an HDR image has to be tone mapped to a low dynamic range (LDR) image. In an HDR tone mapping algorithm, the HDR image is first decomposed into a base layer and a detail layer, then the base layer is compressed and the detail layer is amplified. By adding up the compressed base layer and the amplified detail layer, a tone mapped LDR image is produced. The produced LDR image keeps most of the information in the HDR image with a much lower dynamic range. Similar to other tone mapping algorithms, the HDR image is decomposed to two layers by the proposed filter. The large contrast of the HDR image makes the variance change tremendously, so Gaussian blur is used to the variance before calculating the weight to make the result more natural.</p><p>Two sets of HDR tone mapping results are shown in Fig. <ref type="figure" target="#fig_6">6</ref> and Fig. <ref type="figure">7</ref>. It is seen that the halo artifacts are very apparent in the results of the GIF in <ref type="bibr" target="#b9">[10]</ref>. Even though the halo artifacts are reduced by the WGIF in <ref type="bibr" target="#b16">[17]</ref>, there are still visible halo artifacts. It is further improved by the proposed filter. The halo artifacts are more apparent to be observed from the detail image. For example, the edges in the windows in both Fig. <ref type="figure" target="#fig_6">6</ref> and Fig. <ref type="figure">7</ref> are more apparent in the results of the GIF and the WGIF than the proposed filter although the λ in the proposed filter is larger than the other two filters. Whereas there are more details in the result image of the proposed filter, for example, there are more textures on the floor in Fig. <ref type="figure">7</ref>(c) than Fig. <ref type="figure">7</ref>(b) produced by the WGIF. It is easier to be observed in the zoom-in patches in Fig. <ref type="figure">7</ref>(b) and Fig. <ref type="figure">7(c</ref>). It has been proven previously that a larger λ can produce more detailed image, but may cause more halos in the guided filter based algorithm, so this demonstrates once again the proposed filter is better than the GIF and the WGIF. To observe the difference, the differences between the detail layer by the WGIF and by the proposed filter are also provided. For visualization purposes, the differences are amplified 5 times. It is seen that there are more halos in the results of the WGIF than the results of the proposed filter. We can conclude that the resultant image of the proposed filter generated with a larger λ has less halos but more details than the resultant image of the WGIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Salience Detection</head><p>The Gaussian filter is widely used in existing Saliency detection algorithms to refine Saliency maps. In this subsection, we will show that the proposed filter can be applied to improve the Saliency maps, even for the latest super-pixel based Saliency detection algorithm.</p><p>Visual saliency reflects how much a region stands out from the image. It has been a fundamental problem in image processing and computer vision with many applications, such as image compression <ref type="bibr" target="#b28">[29]</ref>, image cropping <ref type="bibr" target="#b29">[30]</ref>, tone mapping of HDR images <ref type="bibr" target="#b30">[31]</ref>, object detection and recognition <ref type="bibr" target="#b31">[32]</ref>.</p><p>In early stages, most saliency detection approaches are block based . Gaussian smoothing is widely used <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> as a postprocessing procedure to change the computed visual saliency image to a saliency map in salience detection algorithms. It can make the image smoother and reduce the effect of noises. Recently, many region-based approaches <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref> are proposed with the development of superpixel algorithms <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. The Gaussian filter is no longer a suitable postprocessing filter for saliency detection, because it may blur the edges of the saliency map. Superpixel algorithm is a roughly segmentation algorithm, so the objects may not be segmented correctly. In the following, a new post-processing method is introduced for the saliency detection algorithms which can improve the performance of these algorithms.</p><p>In <ref type="bibr" target="#b34">[35]</ref>, an optimization based saliency detection algorithm was proposed. The saliency optimization procedure can be adopted to many other superpixel based algorithm, such as <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>. Here the proposed filter is applied to further improve the algorithm in <ref type="bibr" target="#b34">[35]</ref>. As a post-processing procedure, it can be adopted to almost all the saliency detection algorithms, especially the superpixel based algorithms.</p><p>After obtaining the final saliency map with the algorithm in <ref type="bibr" target="#b34">[35]</ref>, the saliency map is filtered with our proposed gradient domain guided image filter. The luminance channel of the input image is selected as the guidance image. With the proposed filter, the structure of the input image can be moved to the saliency map. This makes the pixels near edge separated more accurately. In addition, as the computational complexity is very low, negligible time is added. The running time is about 0.04 seconds for a 400*300 image on the computer with a Intel Core i7-3770 CPU @3.2GHZ and 8GB of RAM. Similar results can be obtained with other edge aware joint image filtering, including the GIF in <ref type="bibr" target="#b9">[10]</ref>, the WGIF in <ref type="bibr" target="#b16">[17]</ref>, the BLF in <ref type="bibr" target="#b6">[7]</ref> and so on, but the GIF based algorithms usually have better computation efficiency. Two datasets are tested to verify the proposed port-processing algorithm. The first dataset is the ASD dataset <ref type="bibr" target="#b40">[41]</ref>, in which 1000 images from the MSRA-B datasets <ref type="bibr" target="#b41">[42]</ref> are labled with a binary pixel-wise object mask. The second dateset is the Berkeley Segmentation Dataset (BSD) dataset <ref type="bibr" target="#b42">[43]</ref> with more complex scene. We compare the following four recently proposed saliency detection approaches: Saliency Filter (SF) <ref type="bibr" target="#b35">[36]</ref>, Geodesic Saliency (GS) <ref type="bibr" target="#b36">[37]</ref>, Manifold Ranking (MR) <ref type="bibr" target="#b37">[38]</ref> and Saliency Optimization (SO) <ref type="bibr" target="#b34">[35]</ref>. Four sets of images from each dataset are shown in Fig. <ref type="figure" target="#fig_9">9</ref>. From all the images, it is seen that the edge shape of our algorithm is closer to the ground truth image, e.g., the lines of the cross are straighter.</p><p>Similar to many other saliency detection approaches, the precision-recall (PR) curves and the F-measure are adopted to quantitatively evaluate our contribution. Precision is the percentage of salient pixels correctly assigned, and recall is the percentage of the detected salient pixels compared with the ground truth image. The PR curves on ASD and BSD are shown in Fig. <ref type="figure" target="#fig_9">9</ref> and Fig. <ref type="figure" target="#fig_10">10</ref>, respectively. It is seen that both the precision and recall are higher with our post-processing.</p><p>F-measure was proposed in <ref type="bibr" target="#b40">[41]</ref>. In F-measure, an adaptive threshold is used. The threshold is calculated as</p><formula xml:id="formula_22">T α = 2 W × H W x=1 W y=1 S(x, y)<label>(18)</label></formula><p>where x and y are the spatial pixel indexes of the saliency map S, W and H are the width and height of S, receptively. In saliency detection, the F β is widely used. It is defined as</p><formula xml:id="formula_23">F β = (1 + β 2 ) • P recision • Recall β 2 • P recision + Recall<label>(19)</label></formula><p>Similar to other results <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref>, the value of β 2 is set as 0.3. The F-measures of the original and the proposed are 0.8784, 0.8789 on dataset ASD and they are 0.6448, 0.6480 on dataset BSD. This proves our post-processing algorithm can indeed improve the performance.</p><p>Finally, we compare several existing edge-preserving filters with the proposed filter as post-processing tool for saliency detection. The comparison algorithms are the bilateral filter (BLF) <ref type="bibr" target="#b6">[7]</ref>, the weighted median filter (WMF) in <ref type="bibr" target="#b19">[20]</ref> and the rolling guidance filter (RGF) in <ref type="bibr" target="#b10">[11]</ref>. The PR curves of these filters are shown in Fig. <ref type="figure" target="#fig_11">11</ref>. From the PR curves of the different algorithms, we can draw a conclusion that all these filters can yield comparable results in improving the saliency detection. This is a new application of edge-preserving filters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND REMARKS</head><p>In this paper, a new gradient domain guided image filter has been proposed by incorporating an explicit first-order edge-aware constraint into the existing guided image filter. Experimental results of image detail enhancement and HDR image tone mapping show that the proposed filter produces images with better visual appearance than the existing guided filter based algorithms, especially around edges. In addition, based on the new filter, a new saliency detection postprocessing method has been proposed, which can make the saliency detection algorithms more accurate. It is reported in <ref type="bibr" target="#b9">[10]</ref> that there are many applications of guided image filter such as the Flash/no-flash, RGB/NIR, dark-flash image restoration applications. We believe that the proposed filter is also applicable to those applications. One more interesting problem is on the extension of the proposed filter so as to extract fine details from multiple images simultaneously by the extended filter as in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. They will be studied in our future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Comparison of Γ G (p ) and ΓG (p ). The window size of (b) and (c) are 3×3 and 33×33, respectively. 33 is selected here because the default ζ 1 in GIF is 16.The comparison of Γ G (p ) and ΓG (p ) of an image are shown in Fig.1. It is seen that, with this new weighting, the edges are detected more accurately. With the new weighting, one pixel will be detected as an edge pixel when both of its two scale variances are large. Compared with the weighting of the WGIF in<ref type="bibr" target="#b16">[17]</ref>, fewer details are detected as edges in the proposed multi-scale weighting. For example, there are more dots on the petals in Fig.1(b) than Fig.1(d), and the edges are much wider in Fig.1(c) than Fig.1(d). As a result, fine details are enhanced better by the proposed weighting. In addition, σ G,ζ1 (p ) is already calculated in the original GIF algorithm. So the new edge aware factor is more accurate than the factor in the WGIF with negligible increment of the computation time.</figDesc><graphic coords="3,49.74,419.52,59.75,89.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: 1-D illustration of the GIF, the WGIF and the proposed gradient domain GIF. ζ 1 = 16, λ = 1 in all the three algorithm. The input data is obtained from the middle row of the red channel in Fig. 1 (a).</figDesc><graphic coords="4,320.73,333.74,61.20,91.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 3 :</head><label>23</label><figDesc>Fig. 3: Comparison of the selection of parameter λ. The images of each row are the detail layers of GIF, the detail enhancement results of GIF, the detail layers of the proposed filter, and the detail enhancement results of the proposed filter.</figDesc><graphic coords="4,379.12,333.74,61.20,91.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of detail enhancement results.The images of each row are the detail enhanced images, the detail layers and two sets of zoom-in patches of the detail enhanced image, respectively. λ = 0.1 2 in the GIF and in the WGIF, λ = 0.15 2 in the proposed filter, σ s = 16, σ r = 0.1 for the BLF, λ = 0.01 in the L0 smoothing, and σ = 0.01, λ = 30 2 in the FWLS. The window size in the GIF, WGIF, BLF and the proposed filter are 33 × 33.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Input image (b) Result image of GIF in [10] (c) Result image of WGIF in [17] (d) Result image of proposed filter (e) Difference of (c) and (d) (f) Result image of BLF in [7]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Fig. 5 :</head><label>25</label><figDesc>Fig. 5: Comparison of detail enhancement results.λ = 0.1 2 in the GIF and in the WGIF, λ = 0.15 2 in the proposed filter, σ s = 16, σ r = 0.1 for the BLF, λ = 0.01 in the L0 smoothing, and σ = 0.01, λ = 30 2 in the FWLS. The window size in the GIF, WGIF, BLF and the proposed filter are 33 × 33.</figDesc><graphic coords="6,48.96,401.85,125.99,94.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Comparison of tone mapping results of HDR image "office". The parameters are ζ 1 = 15, λ = 1 for the GIF and the WGIF, and the parameters are ζ 1 = 15, λ = 2 for the proposed filter.</figDesc><graphic coords="7,64.91,314.13,158.39,118.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )Fig. 7 :</head><label>a7</label><figDesc>Fig. 7: Comparison of tone mapping results of HDR image "belgium house". The parameters are ζ 1 = 15, λ = 1 for the GIF and the WGIF, and the parameters are ζ 1 = 15, λ = 2 for the proposed filter.</figDesc><graphic coords="7,63.17,451.73,118.79,89.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Comparison of saliency detection. The parameters are ζ 1 = 1, λ = 0.1 2 for the proposed filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: PR curves comparison of saliency detection on dataset ASD</figDesc><graphic coords="9,447.32,419.64,122.40,96.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: PR curves comparison of saliency detection on dataset BSD</figDesc><graphic coords="10,48.96,53.14,259.22,203.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Comparison of different filters. The parameters are ζ 1 = 1, λ = 0.1 2 for the proposed filter, σ s = 3, σ r = 0.05, iteration = 4 for the rolling guidance filter, r = 3, σ = 25.5 for the weighted median filter and σ s = 3, σ r = 0.05 for the bilateral filter.</figDesc><graphic coords="10,313.36,266.11,244.80,189.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Scores of detail enhanced images by the GIF and the proposed filter</figDesc><table><row><cell></cell><cell>Input 0.01 2 0.05 2</cell><cell>0.1 2</cell><cell>0.2 2</cell></row><row><cell>GIF</cell><cell cols="3">36.87 38.97 40.25 34.33 28.86</cell></row><row><cell>Proposed</cell><cell cols="3">36.87 37.69 43.26 43.21 41.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Scores of enhanced images by different filters</figDesc><table><row><cell></cell><cell cols="4">Input GIF WGIF Ours BLF</cell><cell>L0</cell><cell>FWLS</cell></row><row><cell>Fig.4</cell><cell>36.9</cell><cell>34.3</cell><cell>40.7</cell><cell cols="2">42.4 39.6 36.1</cell><cell>33.7</cell></row><row><cell>Fig.5</cell><cell>29.7</cell><cell>27.8</cell><cell>37.1</cell><cell cols="2">44.9 35.5 32.2</cell><cell>28.2</cell></row><row><cell>Average</cell><cell>33.1</cell><cell>31.1</cell><cell>38.9</cell><cell cols="2">43.7 37.6 34.2</cell><cell>31.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_0"><p>(a) PR curves</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edge-preserving decompositions for multi-scale tone and detail manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of highdynamic-range images</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2002-07">Jul. 2002</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2007</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">Jul. 2007</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive support-weight approach for correspondence search</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="656" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1998 6th IEEE Int. Conf. on Computer Vision (ICCV1998)</title>
		<meeting><address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
			<biblScope unit="page" from="836" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast approximation of the bilateral filter using a signal processing approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th European Conference Computer Vision (ECCV 2006)</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="568" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constant time O(1) bilateral filtering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">2008. June 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Pattern Analysis and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rolling Guidance Filter</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th European Conference Computer Vision (ECCV 2014)</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Iterative Shrinkage Approach to Total-Variation Image Restoration</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">V</forename><surname>Michailovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1281" to="1299" />
			<date type="published" when="2011-05">May. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edge-preserving decompositions for multi-scale tone and details manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast Global Image Smoothing Based on Weighted Least Squares</title>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5638" to="5653" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image smoothing via L 0 gradient minimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weighted guided image filtering</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="129" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weighted median filters: a tutorial</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Neuvo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and Systems II: Analog and Digital Signal Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="157" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constant time weighted median filtering for stereo matching and beyond</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">100+ Times Faster Weighted Median Filter (WMF)</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun 2014</date>
			<biblScope unit="page" from="2830" to="2837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-preserving image smoothing via region covariances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilateral texture filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interacting roles of attention and visual salience in V4</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Properties and applications of shape recipes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge-Aware Gradient Domain Optimization Framework for Image Filtering by Local Propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. Jun 2014</date>
			<biblScope unit="page" from="2838" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradientshop: a gradientdomain optimization framework for image and video filtering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Two-Step Framework for Constructing Blind Image Quality Indices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letter</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010-05">May. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transa. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision(ICCV)</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="page" from="2232" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual-salience-based tone mapping for high dynamic range images</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7076" to="7082" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust classification of objects, faces, and flowers using natural image statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2472" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual saliency: a biologically plausible contourlet-like frequency domain approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive neurodynamics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 European Conference on Computer Vision</title>
		<meeting><address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">2012. Oct. 2012</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Turbopixels: Fast superpixels using geometric flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">22902297</biblScope>
			<date type="published" when="2009-05">May. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004-05">May. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detail-enhanced exposure fusion</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4672" to="4676" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">He is currently working toward the Ph.D. degree at the School of Automation Science and Electrical Engineering</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">he has been working as a visiting Ph.D. student at the School of Electrical and Electronic Engineering</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Oct. 2014. 2010. July 2014</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4372" to="4382" />
		</imprint>
		<respStmt>
			<orgName>Electronic Information Engineering from University of Science and Technology Beijing ; Beihang University ; Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include image processing and computer vision</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Since August 1991, he has been with School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, where he is currently a Full Professor. His main research activities are in the areas of control systems and applications, intelligent power management system, smart grids, cyber-physical systems, complex systems and networks, model based online learning and system identification, signal and image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">General Advisor, Publicity Chair and so on. He received the IES Prestigious Engineering Achievement Award 2005 from the Institution of Engineers, Singapore (IES) in 2005. He is a Fellow of IEEE</title>
		<meeting><address><addrLine>China; China; Xi&apos;an, China; Newcastle, Australia; Adelaide, Australia; Shenyang, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982. 1988 and 1996. 2007. 1983. 1990. August 1989 to August 1991. January 2000 to December 2002. January 2011 to December 2013. February 2010 to February 2013. 1992 and 1995. 2001. 2002. 2014</date>
		</imprint>
		<respStmt>
			<orgName>Zhejiang University ; degrees from Beihang University ; Beihang University ; University of Newcastle ; Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>He is an elected Technical Committee of the IEEE Visual Signal Processing and Communication. He served a General Chair of IEEE ICIEA in 2011, a Technical Brief Co-Chair of SIGGRAPH Asia in 2012, a General Co-Chair of CCDC in 2013, and the Workshop Chair of IEEE ICME in 2013. He was an Associate Editor of IEEE Signal Processing Letters since</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
