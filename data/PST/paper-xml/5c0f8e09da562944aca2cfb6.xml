<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCSR: Dilated Convolutions for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinran</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi-an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCSR: Dilated Convolutions for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FC84D699366D2863D0A94D568DF6EB1</idno>
					<idno type="DOI">10.1109/TIP.2018.2877483</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2877483, IEEE Transactions on Image Processing ACCEPTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING 1 received December 16, 2017, revised July 17, 2018 and September 24, 2018, accepted October 17, 2018. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2877483, IEEE Transactions on Image Processing ACCEPTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Super-resolution</term>
					<term>dilated convolutions</term>
					<term>deep neural networks</term>
					<term>receptive field</term>
					<term>correlation analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dilated convolutions support expanding receptive field without parameter exploration or resolution loss, which turn out to be suitable for pixel-level prediction problems. In this paper, we propose multiscale single image super-resolution based on dilated convolutions (DCSR). We adopt dilated convolutions to expand the receptive field size without incurring additional computational complexity. We mix standard convolutions and dilated convolutions in each layer, called mixed convolutions, i.e. in the mixed convolutional layer, the feature extracted by dilated convolutions and standard convolutions are concatenated. We theoretically analyze the receptive field and intensity of mixed convolutions to discover their role in SR. Mixed convolutions remove blind spots and capture the correlation between lowresolution (LR) and high-resolution (HR) image pairs successfully, thus achieving good generalization ability. We verify those properties of mixed convolutions by training 5-layer and 10-layer networks. We also train a 20-layer deep network to compare the performance of the proposed method with those of the state-ofthe-art ones. Moreover, we jointly learn maps with different scales from a low-resolution image to its high-resolution one in a single network. Experimental results demonstrate that the proposed method outperforms the state-of-the-art ones in terms of PSNR and SSIM, especially for a large scale factor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INGLE image SR, which is a technique to generate a high- resolution (HR) image from a single low-resolution (LR) image, has attracted much attention in the computer vision community. It is widely used in video surveillance, remote sensing, medical imaging, and video conversion. Since 1980s, it has been actively studied so far. Example-based SR <ref type="bibr" target="#b32">[32]</ref>, a representative approach to single image SR, needs good prior information from low-and high-resolution image pairs. Researchers typically adopt models which learn this prior information <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b39">[39]</ref> to reconstruct the high frequency components of an LR image. Sparse coding-based SR <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref> has achieved a significant improvement in SR performance. In recent years, deep learning <ref type="bibr" target="#b19">[20]</ref> has been actively applied to image SR <ref type="bibr" target="#b4">[5]</ref>, compression artifact reduction <ref type="bibr" target="#b38">[38]</ref>, and other image restoration tasks. Dong et al. firstly proposed a three-layer convolutional neural network (CNN) for single image SR, called SRCNN <ref type="bibr" target="#b4">[5]</ref>. Based on SRCNN, Kim et al. introduced a much deeper CNN model, called very deep convolutional network for SR (VDSR) <ref type="bibr" target="#b15">[16]</ref>, which has 20 layers and uses residual learning. They also proposed a deeply-recursive convolutional network (DRCN) <ref type="bibr" target="#b16">[17]</ref> which recursively implements the same convolutional kernels several times to extend its receptive field, i.e. a region in the input affects the value of a given point in the output.</p><p>The receptive field is an important issue for pixel-level prediction problems <ref type="bibr" target="#b37">[37]</ref> <ref type="bibr" target="#b15">[16]</ref>. Prediction performance is highly influenced by rich contextual information in an image which is extracted by the large receptive field. For convolutional network based pixel-level prediction, it is meaningless when the input size is much larger than the receptive field size. For deep learning-based image SR, researchers often increase the receptive field of their models. SRCNN uses a 9 × 9 convolutional filter in the first layer; VDSR stacks 20 convolutional layers; and DRCN recursively implements the same layer several times. However, the expanded convolutional kernel size or layer depth would cause additional computational complexity and memory consumption.</p><p>It is extremely beneficial to expand the receptive field in an efficient way. Inspired by this, we propose a mixed convolutional layer for single image SR which is composed of standard convolutions and dilated convolutions. Dilated convolutions <ref type="bibr" target="#b37">[37]</ref> are a particular convolution operator that expands the receptive field without additional computational complexity and memory consumption. Dilated convolutions cause blind spots within the receptive field, while mixed convolutions remove the blind spots. Based on the mixed convolutional layer, we design a mixed residual block (MRblock) and a residual learning SR network <ref type="bibr" target="#b10">[11]</ref>. To highlight the use of dilated convolutions, we call the proposed method DCSR. An important issue here is how proportions of dilated convolutions influence the performance. To address this issue, we analyze the correlations between LR and HR image pairs. Our basic hypothesis is that if CNN recovers the correlations between LR-HR pairs more accurately, it would have better generalization ability and performance. Li et al. also used a very similar hypothesis in <ref type="bibr" target="#b21">[22]</ref> where they analyzed the correlations of natural images themselves instead of LR-HR pairs. We train 5-layer and 10-layer networks to verify the hypothesis. Also, we compare the performance of DCSR with those of the state-of-the-art methods by training a 20-layer network. Experimental results demonstrate that the proposed method outperforms the state-of-the-art ones in terms of quantitative measurements as well as remarkably improves visual quality especially for a large scale factor.</p><p>Compared with existing methods, our main contributions are as follows:</p><p>• We provide a network, named DCSR, whose convolutional layer is made up of standard and dilated convolutions to capture larger scale contextual information in an image. • We theoretically analyze the receptive field and the receptive intensity of mixed convolutional layers.</p><p>• We analyze the correlation between LR-HR pairs, and verify that mixed convolutional layers outperform standard convolutions or dilated convolutions alone. This is because mixed convolutions remove blind spots and capture the correlation of LR-HR image pairs more accurately. The rest of this paper is organized as follows. Section II briefly reviews the related work on deep learning-based SR and introduces dilated convolutions. The proposed method is described in Section III, while the analysis of mixed convolutions on SR is shown in Section IV. We present experimental results and their corresponding analysis in Section V. Finally, we draw conclusions of this paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Due to the strong modeling capability, easy implementation on GPU, and a large amount of training data, deep learning is expected to be a promising solution to computer vision tasks. In this section, we describe current state-of-the-art deep learning-based methods for single image SR. Also, we formally introduce the concept of dilated convolution and the receptive field.</p><p>SRCNN <ref type="bibr" target="#b4">[5]</ref> is the first work for single image SR based on deep learning. <ref type="bibr">Dong et al. discussed</ref> the relationship between CNNs and sparse-coding-based methods. They performed three main functions of patch extraction, non-linear mapping, and reconstruction for SR corresponding to three convolution layers. Unlike other computer vision tasks, SRCNN only had three layers, and Dong et al. <ref type="bibr" target="#b4">[5]</ref> indicated that it was hard to achieve convergence if the model is deeper. SRCNN achieved stable performance after 8×10 8 back propagations. Also, they noticed that sparse-coding-based methods could be viewed as a CNN, which supported high availability of CNN-based SR. To accelerate SRCNN, they proposed fast SRCNN(FSRCNN) <ref type="bibr" target="#b5">[6]</ref> which had more convolutional layers, smaller convolutional filters, and a deconvolutional layer at the top of the network to do up-sampling. FSRCNN used an LR image without interpolation or up-sampling as the input for the SR network, thus remarkably reducing the amount of network parameters. Shi et al. <ref type="bibr" target="#b26">[27]</ref> proposed a novel network with convolution and sub-pixel convolutional layers that achieved real-time SR in 1080p videos.</p><p>VDSR <ref type="bibr" target="#b15">[16]</ref> is a much deeper CNN which has 20 layers and uses residual learning for single image SR. It used a residual learning network which extracted high frequency information from the input image. Kim et al. discovered that the residual learning network was easier to converge. A gradient clipping technique with residual learning made the network converge within only 10 epoches. Also, they provided the performance  They are redrawn from <ref type="bibr" target="#b6">[7]</ref>.</p><p>of the residual network trained by multiscale images. They trained VDSR on a Titan Z GPU whose running time was relatively shorter than other methods. Kim et al. <ref type="bibr" target="#b16">[17]</ref> also proposed a deeply-recursive convolutional network (DRCN) which had very deep recursive layers. They showed that the increase of recursion depth could improve the performance and range of the receptive field without introducing new parameters. However, the increase of recursion depth made the forward propagation slow, thus causing much runtime. Recently, there have been proposed several VDSR variants such as SRResNet <ref type="bibr" target="#b20">[21]</ref> and Enhanced Deep Super-Resolution (EDSR) <ref type="bibr" target="#b22">[23]</ref>. They used much more convolutional layers than VDSR, i.e. more than 32 layers in SRResNet and 64 layers in EDSR. EDSR utilized some additional strategies for further improvement such as removal of batch normalization layer, residual scaling and geometric self-ensemble. In this work, we focus on the convolution itself and introduce dilated convolutions into SR task. Dilated convolutions, proposed by Yu and Koltun <ref type="bibr" target="#b37">[37]</ref>, are a convolution operator that uses the same filter at different ranges using different dilation factors. Thus, dilated convolutions expand the receptive field more effectively. Yu and Koltun <ref type="bibr" target="#b37">[37]</ref> showed that dilated convolutions were helpful for pixel-level segmentation tasks. Formally, we denote I as a discrete signal and w as a convolution kernel, respectively. Standard convolution * is defined as follows:</p><formula xml:id="formula_0">(I * w) (t) = p+q=t I (p) w (q)<label>(1)</label></formula><p>where subscript (•) is the position of a discrete signal. Dilated convolution is a generalization of standard discrete convolution which is formulated as follows:</p><formula xml:id="formula_1">(I * l w) (t) = p+lq=t I (p) w (q)<label>(2)</label></formula><p>where l ∈ Z + is the dilation factor. We denote * l as dilated convolutions with a factor l, as in <ref type="bibr" target="#b37">[37]</ref>. Compared with (1) and ( <ref type="formula" target="#formula_1">2</ref>), the only difference is a factor l multiplied by the position q of w. This means one pixel in w corresponds to l pixels (skip of l-1 pixels) in I. Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the difference between the standard and dilated convolutions. * l with n × n kernel is regarded as * with (n -1) × l + 1 kernel whose n 2 points are involved in computation, while others are set to zero according to their positions <ref type="bibr" target="#b6">[7]</ref> Receptive field of a pixel p in the output is a particular region (a set of pixels) in the input which changes the value of p. Denote s as the stride of convolutions and n as the number of layers. When s = 1, the receptive field growths linearly w.r.t n. When s &gt; 1, the receptive field growths exponentially w.r.t n. For image SR, we are not interested in the latter case because the spatial resolution is reduced when s &gt; 1. Thus, we focus on the case s = 1 in this work. Dilated convolutions have a larger receptive field than standard convolution. Specifically, the receptive field of * l with k × k kernel is (k -1) × l + 1, while the receptive field of * is k. When we stack n layers * l with k×k kernel, the receptive field becomes (k-1)×l×n+1. Appendix A describes how the receptive field increases when we stack convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we formulate SR problem and introduce a mixed convolutional layer to solve it. We also describe the overall network architecture and learning strategy of DCSR in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Single image super-resolution (SISR) is the process of recovering an unknown HR image y from an observed LR image x. The degradation model of the SISR problem is formulated as follows:</p><formula xml:id="formula_2">x = D s Hy (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where H is a blurring opreator applied to x, and D s is a downsampling operator with a scale factor s applied to the blured image. D s maps an M × N image into an M/s × N/s image. Although the blurring process and the downsampling process are complicated in practice, in this paper we simply model D s H via bicubic downsampling with a scale factor s, as in most of other SR literature. Then, we obtain x as follows:</p><formula xml:id="formula_4">x = B s x = B s B s y<label>(4)</label></formula><p>where B s and B s are bicubic downsample and upsample operators with s, respectively. Notice that x has the same size as y and is the input of our models. Let D = {x i , y i }, i = 1, 2, ..., N be a data set where xi and y i are ith LR and HR image pair. Note that although xi which is obtained by downsample and upsample y i has the same size of y i , we still call xi LR image because the high frequency information is lost during downsample and upsample procedures. Denote f w as the neural network parameterized by w and f w (x i ) is the output given xi . Then, the loss function is formed as follows:</p><formula xml:id="formula_5">min w 1 N N i=1 f w ( xi ) -y i 2 2 + λΩ(f w )<label>(5)</label></formula><p>where Ω(f w ) is the regularization term, and λ controls the degree of regularization. Roughly speaking, Ω(f w ) measures how complexity f ω is. We believe that such the complexity is coded in their intrinsic structure by the number of layers or the way they convolved. In this work, we minimize the complexity penalty term by predefining the convolution methods of our network model. If the convolution methods are determined, we need to minimize the MSE term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mixed Convolutional Layer</head><p>Although dilated convolutions support expanding receptive fields without parameter exploration or resolution reduction, blind spots would appear in the receptive field if we directly use dilated convolutions in SR, as shown in Fig. <ref type="figure" target="#fig_7">4</ref>(c). To address this problem, we propose a mixed convolutional layer which combines both dilated convolutions and standard convolutions in the same layer. Mixed convolutions successfully capture the correlation between LR-HR pairs more accurately. We prove it in the next section. The mixed convolutional layer is formed as follows:</p><formula xml:id="formula_6">[σ(W s * x); σ(W d * 2 x)]<label>(6)</label></formula><p>where W s and W d are parameters of standard convolutions and dilated convolutions, respectively; σ is a composite of batch normalization <ref type="bibr" target="#b13">[14]</ref> (BN) and rectified linear unit <ref type="bibr" target="#b9">[10]</ref> (ReLu).</p><p>Bias is hidden for simplicity. That is, features extracted by dilated convolutions and standard convolutions are concatenated together. Then, we apply BN and ReLu to those features (see Fig. <ref type="figure" target="#fig_2">2(c</ref>)). More specifically, suppose the output of a mixed convolution layer has N channels. The first p × N channels are obtained by dilated convolutions while others are obtained by standard convolutions, where p is the proportion of dilated convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>The proposed network architecture is based on a fully convolutional network, and the standard convolution layers are replaced by the mixed convolution layers. In the network, the number of channels of each layer except the first layer is set to be 64, while the kernel size is set to be 3×3 <ref type="bibr" target="#b15">[16]</ref>. Moreover, we introduce the technique of residual block into the architecture to achieve fast convergence. Residual block in deep neural networks was proposed by He et al. <ref type="bibr" target="#b10">[11]</ref> for image recognition. It has been reported that the residual block is helpful for training very deep neural networks. Each residual block in this paper is constructed by 3 mixed convolutional layers, which is called mixed residual block (MR-block). Specifically, the input of MR-block is applied to 3 mixed convolutional layers. Then, MR-block produces the sum of the input and features obtained by the last layer as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. We denote M as the function of a mixed layer and obtain the function of MR-block as follows:</p><formula xml:id="formula_7">x + M 3 M 2 M 1 (x)<label>(7)</label></formula><p>The mixed convolutional layer is the core block for the proposed network model. We determine the optimal proportion of the mixed convolutions in each layer as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multiscale Training</head><p>It is hard to handle different scales in a single model for many SR methods such as prior information-based methods. However, for CNN-based SR methods, it can be achieved in a straightforward way: Training a single network using image patches with different downsampling scales simultaneously. Formally, we obtain LR samples x from (4) with scales s ∈ {2, 3, 4}. Our motivations of multiscale training are threefold: 1) Parameters learned from different scales can be shared in some degree because parameters from a scale are helpful for reconstructing SR images of its similar scales; 2) Deep neural networks have enough capacity to handle multiscale SR problem; 3) Compared with multiple single-scale models, the total number of parameters in multiscale training is small. Kim et al. <ref type="bibr" target="#b15">[16]</ref> trained their networks with all possible combinations of three different downsampling scales: 2, 3, and 4. They tested the PSNR performance on 'Set5' and found that training multiple scales boosted the overall performance. However, their final models were trained with a single scale. In this work, we simultaneously train DCSR using image patches with scales 2, 3, and 4. Then, we compare its performance with the benchmark based on the learned single model. Fig. <ref type="figure" target="#fig_3">3</ref> shows the learned model works successfully even when the scale is not an integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Residual Learning</head><p>Residual learning for neural network-based SR was first introduced by Kim et al. <ref type="bibr" target="#b15">[16]</ref> which achieves significant speedup of the convergence. Residual learning is used for training DCSR. That is, the proposed network is trained to predict the residual of LR-HR pairs, instead of direct HR prediction. Thus, ( <ref type="formula" target="#formula_5">5</ref>) is rewritten as follows:</p><formula xml:id="formula_8">min w 1 N N i=1 f w ( xi ) + xi -y i 2 2<label>(8)</label></formula><p>IV. ANALYSIS In this section, we analyze the effects of mixed convolutions on SR by two points: (1) How the proportion of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Receptive Intensity of Dilated Convolutions</head><p>We provide a theoretical analysis on the receptive intensity of dilated convolutions and mixed convolutions similar to Luo et al. <ref type="bibr" target="#b23">[24]</ref>, where they analyzed the receptive intensity of standard convolutions. We define I n t as the number of computational paths from a pixel at the relative position t to the center pixel in the output plane after implementing n convolutional layers. We analyze the receptive intensity in one dimensional and single channel case which can be generalized to two dimensional and multi-channel case straightforwardly. Denote u(t) as a 1D signal which represents the center pixel in the output plane and v l (t) as the convolution kernel with a dilation factor l whose weights are all equal to 1. Formally, u(t) and v l (t) are defined as follows:</p><formula xml:id="formula_9">u(t) = δ(t), v l (t) = r m=-r δ(t -lm)<label>(9)</label></formula><p>where if t = 0, δ(t) = 1, else, δ(t) = 0, r is the radius of kernel and l is the dilation factor.</p><formula xml:id="formula_10">I n is simply u * v * v • • • * v,</formula><p>which is convolving u with n such v's. Then, we transform u(t) and v l (t) into the frequency domain as follows:</p><formula xml:id="formula_11">U (ω) = 1, V l (ω) = r m=-r e -jωlm<label>(10)</label></formula><p>According to the convolution theorem, we perform the fourier transform of I n as follows:</p><formula xml:id="formula_12">F(I n ) = U (ω)V l (ω) n = r m=-r e -jωlm n<label>(11)</label></formula><p>Then, we perform its inverse transform, I n t , as follows:</p><formula xml:id="formula_13">I n t = 1 2π π -π r m=-r e -jωlm n e jωt dω<label>(12)</label></formula><p>By expanding the first term in the integral, we have:</p><formula xml:id="formula_14">I n t = 1 2π π -π lrn s=-lrn</formula><p>A s e -jωs e jωt dω (13</p><formula xml:id="formula_15">)</formula><p>where A s is known as the extended binomial coefficients <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_16">1 2π π -π</formula><p>e -jωs e jωt dω = δ(s -t)</p><p>According to ( <ref type="formula">13</ref>) and ( <ref type="formula" target="#formula_17">14</ref>)</p><formula xml:id="formula_18">I n t = lrn s=-lrn A s δ(s -t) = A t<label>(15)</label></formula><p>I n t equals to the coefficient of e -jωt in the expansion of r m=-r e -jωlm n . When l = 1, it is the standard convolution. Fig. <ref type="figure" target="#fig_7">4</ref> shows I 3 with different convolutions. Note that when t ≡ 0 (mod l), I n t = 0. It means that when we use dilated convolutions with the same dilation factor without the collaboration of standard convolutions, there exist blind spots (receptive intensity equals to 0) within the receptive field as shown in Fig. <ref type="figure" target="#fig_7">4(c</ref>). This phenomenon is obviously not suitable for image SR because we lose information of pixels' neighbors. Now, we turn to mixed convolution case. Denote p as the proportion of dilated convolutions, i.e. if there are N channels in the mixed convolutional layer, then p × N channels are dilated convolutions. We would like to show that when using mixed convolutions in an appropriate setting, there are no blind spots within the receptive field. Here, we focus on the mixture of two convolutional opreators with same size of kernel. Without loss of generality, the mixed convolution is formed as follows:</p><formula xml:id="formula_19">v l1,l2 (t) = p r m=-r δ(t-l 1 m)+(1-p) r m=-r δ(t-l 2 m) (16)</formula><p>where p is the proportion of * l1 . In a similar way, we get the fourier transform of I n as:</p><formula xml:id="formula_20">F(I n ) = p r m=-r e -jωl1m + (1 -p) r m=-r e -jωl2m n (17)</formula><p>We provide its derivation in Appendix B. Then, it can be concluded that I t n is a coefficient of e -jωt in the expansion of the right term in <ref type="bibr" target="#b16">(17)</ref>. For the special case used in this work, i.e. l 1 = 2, l 2 = 1, and r = 1, I n t &gt; 0 for any integer -R ≤ t ≤ R where R is the radius of receptive field. That is, there are no blind spots for arbitrary stacks of the mixed convolutional layers under such settings. We provide its mathematical analysis in Appendix C. We are also interested in the distribution of computational paths within the receptive field. We define the normalized intensity of a position t as follows:</p><formula xml:id="formula_21">În t = I n t R t=-R I n t (18)</formula><p>1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  We show Î5 and Î10 with different p (0, 0.25, 0.5 and 0.75) in Fig. <ref type="figure" target="#fig_10">6</ref>. As can be expected, the receptive intensity is more concentrated when p becomes smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation Analysis between LR and HR pairs</head><p>Locality is a strong property of natural images. One of the reasons why CNN can successfully model natural images is that locality is coded in the intrinsic structure of CNN. Locality means that a pixel has high correlation with its neighbors while having low correlation with or is nearly independent of pixels far from it. Li et al. <ref type="bibr" target="#b21">[22]</ref> analyzed the correlation of neighbors in natural images, and drew a conclusion that if CNN reconstructs this correlation more accurately, it has better generalization ability because of its gaussian complexity <ref type="bibr" target="#b0">[1]</ref>. For image SR, it can be expected that a pixel in HR images has high correlation with its corresponding neighbors in LR images, but has low correlation with pixels in LR images far from it. Instead of analyzing correlation for natural images, we analyze correlation between LR and HR pairs in a similar way to <ref type="bibr" target="#b21">[22]</ref>. Denote L i and H i as ith LR patch and ith HR patch, respectively. Denote k ∈ Z 2 as a two dimensional shift vector. That is, H i (k) means shifting H i by k. Then, we get:</p><formula xml:id="formula_22">C(k) = N i=1 cov(L i , H i (k)) N i=1 cov(L i , L i )cov(H i , H i )<label>(19)</label></formula><p>where</p><formula xml:id="formula_23">cov(A, B) = i (A i -µ A )(B i -µ B</formula><p>) is covariance of two matrices; µ A and µ B are the average of A and B, respectively. We call C correlation map. We calculate correlation maps between HR and LR images with downsampling factors 2, 3 and 4 in 291 data set (91 images from Yang et al. <ref type="bibr" target="#b36">[36]</ref> and 200 images from Berkeley segmentation data set <ref type="bibr" target="#b24">[25]</ref>), denoted as C 2 , C 3 , and C 4 respectively. Fig. <ref type="figure" target="#fig_5">5</ref> shows C 3 and the horizontal lines in the center of C 2 , C 3 , and C 4 . As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, LR and HR images are much correlated. The correlation coefficient are larger than 0.3 even when two points are with a distance of 40 pixels. When the downsampling factor becomes larger, correlation coefficients slightly increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimal Proportion of Dilated Convolutions</head><p>Based on <ref type="bibr" target="#b16">(17)</ref> and <ref type="bibr" target="#b17">(18)</ref>, we obtain the receptive intensity of a CNN network which stacks an arbitrary number of mixed layers with arbitrary proportions of dilated convolutions. It is required to find an optimal configuration of mixed layers for SR task. To simplify the problem and achieve a fair compairson, we introduce the following constraints into the configurations of our network architecture:</p><p>• All layers except the last one have the same number of convolutional filters; • All layers except the last one have the same proportion of dilated convolutions (The last layer has one filter and we simply set it to standard convolution in all experiments); • The dilation factor of each dilated convolution is 2. Such setting prevents blind spots within the receptive field. To evaluate the accuracy of a network in recovering the true correlation map C, we use normalized receptive intensity to approximate the correlation between LR and HR pairs reconstructed by the proposed network. However, we set the intensity at center to 1 by multiplying a particular constant because the value of the correlation map is 1 at the center point. Then, the reconstructed correlation Ĉ is as follows:</p><formula xml:id="formula_24">Ĉ = În În 0 ⊗ În În 0 (<label>20</label></formula><formula xml:id="formula_25">)</formula><p>where n is the number of layers of a network and În is a vector which represents the one dimensional receptive intensity. Due to the symmetry, we obtain the two dimensional receptive intensity by outer production of the one dimensional receptive intensity. We aim to minimize the mean absolute error (MAE) between true correlation prior C obtained from <ref type="bibr" target="#b18">(19)</ref> and the reconstructed correlation Ĉ as follows:</p><formula xml:id="formula_26">1 r 2 r i=-r r j=-r | Ĉij -C ij |<label>(21)</label></formula><p>We calculate <ref type="bibr" target="#b20">(21)</ref> by varying the proportions of dilated convolutions p from 0 to 1. We find that the minimum value of MAEs between Ĉ and C is obtained when p = 0.74 for 5-layer network; p = 0.81 for 10-layer network; and p = 0.88 for 20-layer network. Fig. <ref type="figure" target="#fig_12">7</ref> shows the relationship between MAE, scale and p. It can be observed that MAE of mixed convolutions with a suitable proportion is smaller than standard (p = 0) and dilated convolutions (p = 1). The results indicate mixed convolutions capture the correlation between LR-HR pairs with better accuracy, thus achieving better SR performance. The receptive intensity of standard convolution   is too concentrated at center to match the true correlations. However, dilated convolutions make the receptive intensity more uniform as shown in Fig. <ref type="figure" target="#fig_10">6</ref>, and thus capture long term dependence better than standard convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We perform two sets of experiments: (1) Effects of the proportion p in dilated convolutions by training 5-layer and 10-layer networks; (2) Performance comparison with state-ofthe-arts in a 20-layer network. Prior to describing two sets of experiments, we introduce data generation and implementation details as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets for Training and Testing</head><p>To train a 20-layer network, we use 291 images which is the same as <ref type="bibr" target="#b25">[26]</ref>. This data set consists of two parts: the first 91 images from Yang et al. <ref type="bibr" target="#b36">[36]</ref> and the other additional 200 images from Berkeley segmentation data set <ref type="bibr" target="#b24">[25]</ref>. Compared with image classification tasks usually trained on large data sets, the size of the data set is relatively small. This is because SR methods are trained on image patches and do not need a lot of images for training. Given 291 images, we generate sufficiently enough patches for training DCSR. Thus, the generalization ability of DCSR is undoubted. The input images are obtained by down-and up-sampling procedures using bicubic interpolation. Thus, the input and ground truth images share the same size. This is because multi-scale training needs to unify the input size, otherwise, we need different models for different scales. Moreover, we use residual learning for SR instead of direct HR prediction. Thus, the size of input and output should be the same in the network model. Each image is divided into 65 × 65 patches. For the first 91 images, the stride is 40 (with overlapping) while for the last 200 images, the stride is 65 (without overlapping). For training 5-layer and 10-layer networks, we use the first 91 images. Each image is divided into 41 × 41 patches without overlapping. For tests, we use four data sets: 'Set5' <ref type="bibr" target="#b1">[2]</ref>, 'Set14' <ref type="bibr" target="#b39">[39]</ref>, 'Urban100' <ref type="bibr" target="#b12">[13]</ref>, and 'B100' <ref type="bibr" target="#b31">[31]</ref>. 'Urban100' has 100 urban building images which are full of various edges and textures with size of 1024 pixels in length. 'B100' is a more complex data set for image segmentation. This data set contains animals, humans, vehicles, and various textures. These 4 data sets are frequently used for performance evaluations. In the testing stage, we input the whole image into our DCSR without cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>All experiments are implemented in Python using Theano <ref type="bibr" target="#b27">[28]</ref> and Lasagne <ref type="bibr" target="#b8">[9]</ref> on a PC with GPU GTX-1080Ti, 11GB RAM in ubuntu 14.04 system. 2) Optimization and Initialization: We train DCSR via ADAM optimizer <ref type="bibr" target="#b18">[19]</ref> with β 1 = 0.9 and β 2 = 0.999. For 5-layer and 10-layer networks, batchsize is set to be 128. For the 20-layer network, the batch size is set to be 64. Weights are initialized by <ref type="bibr" target="#b11">[12]</ref> since it is theoretically sound for initializing convolutional layers with ReLU activations. We set the learning rate to 0.02, and then decrease it by a factor 10 when 50% and 75% of total epoches are achieved. We use <ref type="bibr" target="#b7">(8)</ref> as the loss function during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proportion of DC</head><p>To see the effect of proportions p of DC on the performance and verify the optimal proportions p opt in Section IV-C, we train 5-layer and 10-layer networks with the same configuration to expect the proportions of DC: 25%, 50%, p opt and 100%. Recall that p opt = 74% for 5-layer network and  p opt = 81% for 10-layer network. 5-layer network has 1 MRblock and 10-layer network has 2 MR-block. As mentioned earlier, all hidden layers have 64 channels. We train each network 10000 iterations and compare PSNR on 'Set5'. The results are shown in Table <ref type="table" target="#tab_0">I</ref>. Compared with the standard convolution case (p = 0%), PSNRs are gained by a relatively large margin when p ∈ {25%, 50%, p opt }. While PSNRs are decreased for when p = 100% (dilated convolution case). Roughly speaking, introducing mixed convolutional layers into network can improve PSNR performance compared with both standard convolution case and dilated convolution case. Also, it can be observed that we achieve the best PSNR performance for scales 3 and 4 when p = p opt , which is consistent with the analysis in Section IV-C. It seems that we need a smaller receptive field for scale 2 than scales 3 and 4. This is reasonable because for a small scale factor the surrounding pixels are less blurred and contain more information of the image content. Thus, a smaller region is enough to recover the center pixel. In fact, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>(b), the correlation map of scale 2 is consistently smaller than those of scale 3 and 4. That is, the long-term correlation for scale 2 is weaker, which indicates a smaller receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Arts</head><p>We also train a deeper architecture and provide the quantitative and qualitative performance of this model in comparison with state-of-the-art ones: RFL <ref type="bibr" target="#b25">[26]</ref>, SelfEx <ref type="bibr" target="#b12">[13]</ref>, SRCNN <ref type="bibr" target="#b4">[5]</ref>, DRCN <ref type="bibr" target="#b16">[17]</ref>, and VDSR <ref type="bibr" target="#b15">[16]</ref>. As illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, the proposed network consists of 20 layers which is concatenation of 1 mixed convolutional layer, 6 MR-block, and 1 standard convolutional layer. The proportion of DC in all mixed layers is set to be the optimal value, i.e. 0.88. Note that the number of layers and the number of network parameters are exactly the same as VDSR for a fair comparison. Different from other deep learning-based methods, we adopt a multiscale training strategy, and evaluate the performance in the benchmark data set using the learned single model. Among them, SRCNN, DRCN, and VDSR are deep learning-based methods which have achieved state-of-the-art performance in recent years. We use two widely used metrics, i.e. PSNR and SSIM, for performance evaluation. These metrics indicate pixel-level similarity and local structure similarity between the reconstructed image and its ground truth. As shown in Table <ref type="table" target="#tab_1">II</ref>, the proposed method achieves considerable performance improvement over DRCN and VDSR, especially when the scales are 3 and 4. Figs. 8-11 show SR reconstruction results by different methods. As shown in the figures, DCSR recovers edges and textures which contain large scale contextual information more accurately than the others: The wing in Butterfly, the stripes in Zebra and the surface of buildings in Img099. For SR reconstruction of small textures, DCSR also performs well, such as the dot patterns in Butterfly. This is because DCSR has a larger receptive field and effectively captures the correlations between LR-HR pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we have proposed DCSR for single image SR based on the combination of standard convolutions and dilated convolutions. We have provided a theoretical analysis on the receptive field of the mixed convolutions to verify that the mixed convolutions are able to extract large scale contextual information with limited layer depth and convolution kernel size without blind spots within the receptive field. We analyze the receptive intensity of mixed convolutional layers and use it to approximate the correlation of LR-HR pairs recovered by networks. We have discovered that mixed convolutional networks recovers LR-HR correlations more accurately and thus achieves better performance. We have verified them using 5-layer and 10-layer networks. Also, we have provided the   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(a) l = 1 (b) l = 2</head><label>12</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Standard convolution. (b) Dilated convolution with a factor 2.They are redrawn from<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of the proposed DCSR network is shown in (a). Each layer except the last layer consists of three operators: convolutions, batch normalization (BN), and ReLu while the last layer is linearly activated. The output of the last layer is the residual between LR inputs and HR targets. The proposed mixed residual block (MR-block) is shown in (c). MR-block consists of three mixed convolutional layers and outputs the sum of inputs and features of the final layers.</figDesc><graphic coords="4,319.09,396.20,71.97,71.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results of DCSR by 5 different scales. (a) Ground truth. (b)-(f) Results with scales 2, 2.5, 3, 3.5, and 4. The single network is trained via a multiscale training strategy using low-resolution inputs with scales 2, 3, and 4.</figDesc><graphic coords="4,401.52,491.51,71.97,71.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Correlation map between HR and LR images with the downsampling factor 3 obtained from 291 data set. (b) Horizontal lines in the center of correlation maps with different downsampling factors.</figDesc><graphic coords="5,80.20,69.39,67.99,67.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 × 3</head><label>33</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2877483, IEEE Transactions on Image Processing ACCEPTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING 6 Mixed Conv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a)-(d) Receptive field intensity maps after three 3 × 3 convolutional layers, three 5 × 5 convolutional layers, three 3 × 3 dilated convolutional layers, and three mixed convolutional layers. 3 × 3 mixed convolutional layer has the same reception field size as 5 × 5 standard convolutional layer, but their intensity distributions are different. For the dilated convolutional layer, there are blind spots whose intensity is zero within the receptive field and which are not suitable for the SR task.</figDesc><graphic coords="6,85.79,68.44,74.51,69.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Receptive field intensity distributions under different n (layer) and p (proportion of dilated convolutions). Notice that the ranges of x labels are different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 )</head><label>1</label><figDesc>Random Generation of Training Samples: We generate training samples in each epoch in a random way. Specifically, we randomly pick a point (a, b) to begin cropping each image in each epoch. a, b ∈ [1, 20] are two random integers which stand for the starting coordinates on two axis. Then, we crop each image into patches with fixed size and stride as mentioned above. For data augmentation, we define five kinds of operators: Flip horizontally, flip vertically, rotate 90 • , rotate 180 • , and rotate 270 • . For each image patch during each epoch, we randomly select an operator from the five operators. Besides, both LR inputs and HR groud-truth targets are uniformly re-scaled from [0, 255] to [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. MAE evaluation between true correlation prior of natural images C and the reconstructed correlation of networks Ĉ. The optimal proportions for 5 layer, 10 layer and 20 layer networks are 0.74, 0.81 and 0.88, respectively.</figDesc><graphic coords="8,69.44,60.48,154.22,117.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Super-resolution reconstruction results in Butterfly ('Set5') with scale factor 4</figDesc><graphic coords="9,161.89,365.07,82.25,69.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Super-resolution reconstruction results of Zebra ('Set14') with scale factor 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PSNR</head><label>I</label><figDesc>EVALUATION RESULTS IN FOUR DIFFERENT SETTINGS FOR THE MIXED CONVOLUTIONAL LAYER ON 'SET5' AFTER 10000 ITERATIONS 100% means dilated convolutions, 25%, 50%, and popt are the proportions (p) of dilated convolutions in the mixed convolutional layers. Recall that popt = 74% for 5-layer network and popt = 81% for 10-layer network. The performance improvements is mainly from a deeper architecture.</figDesc><table><row><cell></cell><cell></cell><cell>0%</cell><cell>25%</cell><cell>50%</cell><cell>popt</cell><cell>100%</cell></row><row><cell></cell><cell>×2</cell><cell>36.60</cell><cell>36.70</cell><cell>36.71</cell><cell>36.65</cell><cell>36.34</cell></row><row><cell>5</cell><cell>×3</cell><cell>32.81</cell><cell>32.96</cell><cell>33.00</cell><cell>33.06</cell><cell>32.94</cell></row><row><cell></cell><cell>×4</cell><cell>30.54</cell><cell>30.68</cell><cell>30.69</cell><cell>30.75</cell><cell>30.72</cell></row><row><cell></cell><cell>×2</cell><cell>36.98</cell><cell>37.07</cell><cell>37.05</cell><cell>37.02</cell><cell>36.67</cell></row><row><cell>10</cell><cell>×3</cell><cell>33.22</cell><cell>33.33</cell><cell>33.35</cell><cell>33.41</cell><cell>33.31</cell></row><row><cell></cell><cell>×4</cell><cell>30.83</cell><cell>31.02</cell><cell>31.04</cell><cell>31.11</cell><cell>31.05</cell></row><row><cell cols="4">0% means standard convolutions,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPAIRSON WITH THE STATE-OF-THE-ARTS FOR SCALE FACTOR ×2, ×3 AND ×4 ON DATA SETS 'SET5', 'SET14', 'B100' AND 'URBAN100' SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Set5 2 33.66/0.9299 36.54/0.9537 36.49/0.9537 36.66/0.9542 37.63/0.9588 37.53/0.9587 37.54/0.9587 3 30.39/0.8682 32.43/0.9057 32.58/0.9093 32.75/0.9090 33.82/0.9226 33.66/0.9213 33.94/0.9234 4 28.42/0.8104 30.14/0.8548 30.31/0.8619 30.48/0.8628 31.53/0.8854 31.35/0.8838 31.58/0.8870 Set14 2 30.24/0.8688 32.26/0.9040 32.22/0.9034 32.42/0.9063 33.04/0.9118 33.03/0.9124 33.14/0.9141 3 27.55/0.7742 29.05/0.8164 29.16/0.8196 29.28/0.8209 29.76/0.8311 29.77/0.8314 30.28/0.8354 4 26.00/0.7027 27.24/0.7451 27.40/0.7518 27.49/0.7503 28.02/0.7670 28.01/0.7674 28.21/0.7715 B100 2 29.56/0.8431 31.16/0.8840 31.18/0.8855 31.36/0.8879 31.85/0.8942 31.90/0.8960 31.90/0.8959 3 27.21/0.7385 28.22/0.7806 28.29/0.7840 28.41/0.7863 28.80/0.7963 28.82/0.7976 28.86/0.7985 4 25.96/0.6675 26.75/0.7054 26.84/0.7106 26.90/0.7101 27.23/0.7233 27.29/0.7251 27.32/0.7264 Urban100 2 26.88/0.8403 29.11/0.8904 29.54/0.8967 29.50/0.8946 30.75/0.9133 30.76/0.9140 30.76/0.9142 3 24.46/0.7349 25.86/0.7900 26.44/0.8088 26.24/0.7989 27.15/0.8276 27.14/0.8279 27.24/0.8308 4 23.14/0.6577 24.19/0.7096 24.79/0.7374 24.52/0.7221 25.14/0.7510 25.18/0.7524 25.30/0.7575 Notice that multi-scale data are used for training our 20-layer model and we compare with state-of-the-art ones based on the learned single model. Bold font indicates the best performance.</figDesc><table><row><cell>Data set</cell><cell>Scale</cell><cell>Bicubic PSNR/</cell><cell>RFL</cell><cell>SelfEx</cell><cell>SRCNN</cell><cell>DRCN</cell><cell>VDSR</cell><cell>DCSR</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACCEPTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China (No. 61872280) and the International S&amp;T Cooperation Program of China (No. 2014DFG12780).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B</head><p>We calculate the fourier transform of I n in mixed convolutions. Denote u(t) as a 1D signal that represents the center pixel in the output. Denote v l1,l2 (t) as the mixed kernels that consist of kernels with dilation factors l 1 and l 2 . Formally, u(t) and v l1,l2 (t) are defined as follows:</p><p>where r is the radius of the kernel and p is its proportion with factor l 1 .</p><p>Then, we transform u(t) and v l1,l2 (t) in the frequency domain as follows:</p><p>According to the convolution theorem, we perform the fourier transform of I n as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C</head><p>When l 1 = 2, l 2 = 1 and r = 1, (17) becomes:</p><p>n is a coefficient of e -jωt in the expansion of the right term. Now, no blind spots within the receptive field is equivalent to I t n &gt; 0 for -2n &lt;= t &lt;= 2n. Because of symmetry, we only consider the case 0 &lt;= t &lt;= 2n. By algebraic simplification, if there exists a solution for the following problem, then I t n &gt; 0:</p><p>where n 1 , n 2 &gt;= 0 are integers. It is easy to check that the solution exists for arbitrary 0 &lt;= t &lt;= 2n. Similar to the case -2n &lt;= t &lt;= 0. Thus, for arbitrary -2n &lt;= t &lt;= 2n, we have I t n &gt; 0. That is, there are no blind spots within the receptive field.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly optimized regressors for image super-resolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2008">2016. 1, 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03">mar 2016. 2, 3</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Restricted weighted integer compositions and extended binomial coefficients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Integer Seq</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">13.1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Eben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D O</forename><surname>Aron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Søren</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.27878</idno>
		<ptr target="http://dx.doi.org/10.5281/zenodo.27878" />
		<title level="m">Lasagne: First release</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image transformation based on learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2016. 1, 2, 3, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2016. 1, 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Filter shaping for convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002">July 2017. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2015. 1, 7, 8</date>
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Theano: A python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T D</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<idno>GT: PSNR/SSIM (b) Bicubic: 22.42/0.5853 (c) SRCNN: 23.78/0.6734 (d) VDSR: 24.02/0.6961 (e) DCSR: 24.61/0.7223</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Super-resolution results of Img099</title>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Urban100&apos;) with scale factor 4</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: a benchmark</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representation (ICLR)</title>
		<meeting>International Conference on Learning Representation (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02778</idno>
		<title level="m">Deep convolution networks for compression artifacts reduction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">His main research interests include data mining, machine learning (including deep learning) and its applications to computer vision tasks. Xinran Wang received the B.S. degree in intelligence science and technology and M</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cheolkon Jung (M&apos;08) received the B.S., M.S., and Ph.D. degrees in electronic engineering from Sungkyunkwan University, Republic of Korea, in 1995, 1997, and 2002, respectively. He was with the Samsung Advanced Institute of Technology (Samsung Electronics)</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2010. 2014. 2014 and 2017. 2002 to 2007. 2007 to 2009</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
		<respStmt>
			<orgName>Xidian University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">respectively</note>
	<note>where he is currently a Full Professor and the Director of the Xidian Media Lab. His main research interests include image and video processing. computer vision, pattern recognition, machine learning, computational photography, video coding, virtual reality, information fusion, multimedia content analysis and management, and 3DTV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
