<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pulsar: Towards Ubiquitous Visible Light Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>MobiCom&apos;17, October 16-20</addrLine>
									<postCode>2017</postCode>
									<settlement>Snowbird</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pulsar: Towards Ubiquitous Visible Light Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">235D1886B6FE75674C1B41A8590C8994</idno>
					<idno type="DOI">10.1145/3117811.3117821</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visible light sensing</term>
					<term>Indoor localization</term>
					<term>Signal processing</term>
					<term>Sensor</term>
					<term>Paper Session V: Location! Location! Location!</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past decade's research in visible light positioning (VLP) has led to technologies with high location precision. However, existing VLP systems either require specialized LEDs which hinder large-scale deployment, or need cameras which preclude continuous localization because of high power consumption and short coverage. In this paper, we propose Pulsar, which uses a compact photodiode sensor, readily fit into a mobile device, to discriminate existing ceiling lights-either fluorescent lights or conventional LEDs-based on their intrinsic optical emission features. To overcome the photodiode's lack of spatial resolution, we design a novel sparse photogrammetry mechanism, which resolves the light source's angle-of-arrival, and triangulates the device's 3D location and even orientation. To facilitate ubiquitous deployment, we further develop a light registration mechanism that automatically registers the ceiling lights' locations as landmarks on a building's floor map. Our experiments demonstrate that Pulsar can reliably achieve decimeter precision in both controlled environment and large-scale buildings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Indoor localization represents the enabling technology behind a huge space of location-based services, including human/robot navigation, targeted advertisement, inventory control, healthcare monitoring, augmented reality, etc. Recent research has led to solutions that deliver decimeter level accuracy under controlled experimental settings <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b24">26]</ref>. Unfortunately, none of such solutions has widely penetrated the real-world. A recent pilot study of state-of-the-art indoor localization systems <ref type="bibr" target="#b31">[33]</ref> concluded that indoor localization persists as a grand challenge, primarily because no existing solution simultaneously satisfies desired properties of high accuracy, reliability and low cost.</p><p>The visible light positioning (VLP) technology has shown potential to fill the gap. VLP employs "smart LEDs" as location landmarks, and photodiodes (PDs) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b64">66]</ref> or cameras <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b62">64]</ref> as sensors. The dense overhead deployment of light fixtures, and multipath-free propagation profile, together enable VLP's high spatial resolution and resilience to environmental dynamics. Existing VLP technologies have achieved meter <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b38">40]</ref> or decimeter precision <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b62">64]</ref>, when using PD and camera, respectively.</p><p>However, two fundamental limitations have been impeding the adoption of VLP: (i) Requirement for specially designed smart LED bulbs, which modulate the light signals to create unique beacon IDs. Such bulbs are substantially more costly and bulkier than commercial LEDs or fluorescent lights (FLs) <ref type="bibr" target="#b46">[48]</ref>, which hinders near-term mass deployment. In fact, even the basic LEDs only make up around 7% of commercial/industrial lighting, and may take 10+ years towards wide penetration <ref type="bibr" target="#b47">[49]</ref>, mainly because FL fixtures have already dominated the market and retrofitting/updating cost is formidable. (ii) Dependence on regular light shape and angular response. VLP systems that employ RSS-based light propagation models commonly assume Lambertian radiation patterns <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">31]</ref>, which are applicable only to round-shaped LED bulbs (Sec. 2.1). A camera offers high spatial resolution and can unleash VLP from RSS models <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b62">64]</ref>. A recent camera-based VLP system, LiTell <ref type="bibr" target="#b62">[64]</ref>, works even for conventional FLs. But the camera's narrow fieldof-view (FoV), high power consumption (∼2 watts <ref type="bibr" target="#b62">[64]</ref>) and high processing latency (∼2 seconds <ref type="bibr" target="#b62">[64]</ref>) prevent it from supporting pervasive, real-time, energy efficient location tracking. The limitation to FLs also makes LiTell a transition solution as LEDs will eventually take over.</p><p>In order to overcome the limitations of conventional VLP systems, we introduce Pulsar in this paper. Pulsar builds on recent measurement observations <ref type="bibr" target="#b62">[64]</ref> which identified the intrinsic highfrequency flickering patterns inside FLs using cameras. Yet Pulsar targets more ambitious design goals that are conducive to ubiquitous location-based services: (i) working under both LEDs and incumbent FLs in existing buildings; (ii) using energy-efficient "single-pixel" PDs, but still able to resolve 3D location with similar Paper Session V: Location! Location! Location! MobiCom <ref type="bibr">'17, October 16-20, 2017</ref>, Snowbird, UT, USA precision as the million-pixel cameras <ref type="bibr" target="#b62">[64]</ref>; (iii) realizing continuous, seamless coverage as users roam around in practical building environment; (iv) allowing easy integration with mobile applications, and easy deployment with minimal bootstrapping overhead. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the main components and workflow of Pulsar. Pulsar uses PD as its light sensor, which reduces the power consumption by several folds compared with camera-based VLP systems. In addition, owing to PDs' much higher dynamic range, Pulsar can capture LEDs' intrinsic frequencies that are much weaker than FLs', and even when the LEDs are more than 9m away (Sec. 3), in contrast to camera-based solutions <ref type="bibr" target="#b62">[64]</ref> that only work within a range of 2.5m even for FLs. This salient property enables Pulsar to capture multiple lights simultaneously even when the lights are sparsely deployed on high ceilings, and obtain location fixes virtually without any blind spots.</p><p>Unfortunately, such benefits come with new challenges: (i) Nearfar interference. The frequency features of faraway lights often mix with the weak side-peak artifacts of close-by lights and get masked, making it hard to distinguish collocated lights. (ii) Lack of spatial resolution. In contrast to the million-pixel camera, the PD is a single-pixel intensity sensor, which provides no spatial resolution and cannot easily identify its geometrical relation with different lights.</p><p>Pulsar's key solution principle lies in a novel mechanism, referred to as sparse photogrammetry, which resolves the light source's angle-of-arrival (AoA) using a compact light sensor. Pulsar's light sensor comprises two PDs with different FoVs 1 , and hence different angular response patterns. The differential response between the two PDs follows a nonlinear function with the AoA, which can be calibrated and known at manufacturing time. At runtime, Pulsar can measure the differential response and map it back to the light source's AoA. By using AoA instead of RSS, Pulsar also circumvents the Lambertian model, enabling localization with lights of any shape. By combining the AoA of adjacent light sources, Pulsar can pinpoint the device's 3D location through a triangulation model. When more than 3 lights fall in its FoV, the Pulsar sensor can even compute its orientation angles. On the other hand, to solve the near-far problem, we observe that all frequency components of the same light have the same AoA, which allows us to single out a light's intrinsic frequency, even when it is immersed in other nearby lights' frequency components.</p><p>Full deployment of an indoor localization system requires knowing the landmarks' locations a priori, a problem often overlooked by existing localization systems. Pulsar uses a light registration mechanism to greatly simplify the issue: it only requires a onetime bootstrapping procedure, where a surveyor walks across the lights inside a building, while the lights' frequency features are captured by Pulsar, and lights' locations automatically registered on a publicly available floor map and stored in a database. Following any subsequent change of light fixtures, the database can be incrementally updated based on user devices' feedback.</p><p>We have conducted a full-fledged implementation of Pulsar. The Pulsar sensor is prototyped as a USB dongle, plugged in an Android phone that runs the localization algorithms. Owing to the use of 1 Defined as the double-sided incident angle where RSS is halved in comparison to RSS at 0 • . Unlike cameras, a PD can still sense light sources outside its FoV.</p><p>lightweight PDs, we expect Pulsar can readily augment new generations of smartphones, robots or low-power IoT devices that need location-based services <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b34">36]</ref>. We have further implemented the light registration mechanism along with a simple user interface.</p><p>Our experiments verify Pulsar's high accuracy, robustness, and usability. In controlled setups similar to many existing VLP systems <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b56">58]</ref>, Pulsar can achieve 10 cm of median location precision. In practical building environments, Pulsar can easily detect 3 or more lights within its FoV, achieving a median localization error of 0.6 m and heading direction error of 4 • while walking. More importantly, Pulsar is robust to typical usage dynamics, such as the device's orientation variation and ambient sunlight interference. Our prototype implementation incurs a response latency of 840 ms, ready for real-time location queries, and a low power consumption of 150 mW even without duty-cycling.</p><p>To summarize, our contributions in Pulsar include: (i) A sparse photogrammetry mechanism that enables single-pixel PDs to sense a light source's AoA, and then pinpoint their own locations and heading direction. (ii) A light identification framework that enables a PD to discriminate LEDs/FLs whose frequency features interfere with each other; (iii) Validation of Pulsar based on a full-fledged implementation including an automatic landmark registration scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AUGMENTING PHOTODIODES WITH AOA SENSING 2.1 Why AoA?</head><p>Prior PD based VLP systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">31]</ref> commonly adopt round LED bulbs whose angular emission pattern follows a Lambertian model, but commercial light fixtures present several key barriers to such RSS-based models: (i) The light fixtures are commonly tube/rectangular shaped, yielding different channel responses along different directions. So the RSS modeling will need the PD's location in the first place. (ii) The light fixture may have complicated reflective structures and covers which reshape the emission pattern <ref type="bibr" target="#b39">[41]</ref>. (iii) Static and dynamic blockages such as ceiling decorators and people passing by may partially block the light, causing unpredictable deviation in RSS. In addition, due to aging and inevitable hardware deviation, different lights' emission power can vary, even among the same type of lights <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b55">57]</ref>. All these factors render RSS propagation modeling unusable in real indoor environments.</p><p>To understand these challenges, we use our light sensor prototype (Sec. 6) to measure the RSS of light sources across different irradiance angles (defined in Fig. <ref type="figure">2</ref>), at 1m distance. We measure two representative tube lights (with and without the diffuser cover, shown in Fig. <ref type="figure">3</ref>), commonly used in commercial buildings. We then measure the light intensity with sensor facing towards the light, and normalize it against intensity at 0 • . From the results in Fig. <ref type="figure">4</ref>, we see that the Lambertian model deviates from the measured RSS by up to 6dB, which translates into a few meters of error when the PD is a few meters away from the light <ref type="bibr" target="#b29">[31]</ref>. Worse still, the angular response of the same light can change with viewing angle, meaning that a precise RSS propagation model requires the location/orientation of the PD a priori.</p><p>In summary, for robustness and ubiquity, the VLP system should work independently from lights' emission characteristics such as angular response and intensity. However, this almost forbids the    use of RSS modeling. An AoA model can naturally avoid such problems, because it approximates the light as a point source, whose irradiation angle is unaffected by its shape, partial occlusion, and intensity variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pulsar's AoA Sensing: Model and Principles</head><p>Harnessing the million-pixel sensors and high spatial resolution, a camera can easily extrapolate the physical geometries of multiple luminaries in an overhead scene image into 2-dimensional AoA information. This so-called photogrammetry technique <ref type="bibr" target="#b24">[26]</ref> can arguably provide the most accurate and reliable geometrical information <ref type="bibr" target="#b2">[3]</ref>, albeit at the cost of higher power consumption, limited FoV, etc. In this section, we describe how Pulsar uses single-pixel PDs to obtain AoA while overcoming camera's inherent limitations (Sec. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1</head><p>AoA Sensing Using Single-Pixel PDs. The key innovation of Pulsar lies in a dual-PD light sensor. Pulsar uses the differential angular responses between the 2 PDs to obtain 1-D AoA (i.e., incident angle between incoming light and the PD's center axis, illustrated in Fig. <ref type="figure">2</ref>), which will in turn feed Pulsar's sparse photogrammetry localization algorithm (to be discussed in Sec. 4).</p><p>More specifically, the RSS between each light source and the PDs follows a generic channel model <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b61">63]</ref>:</p><formula xml:id="formula_0">RSS 1 = P t A t (θ 1 )α(r 1 )A r 1 (ϕ 1 ) RSS 2 = P t A t (θ 2 )α(r 2 )A r 2 (ϕ 2 )<label>(1)</label></formula><p>where P t is the emission power of the light. A t (θ ) is the irradiance angular response at irradiance angle θ ; α(r ) is the propagation loss at distance r , and A r i (ϕ) is the incident angular response of the i-th PD at incident angle ϕ<ref type="foot" target="#foot_0">2</ref> . Since the 2 PDs are most likely co-located on a mobile device, with negligible separation compared with r , we can safely assume θ 1 = θ 2 , r 1 = r 2 and ϕ 1 = ϕ 2 = ϕ. By dividing the linear RSS observed by the 2 PDs, all other factors related to the emission/intensity characteristics of the light will cancel out:</p><formula xml:id="formula_1">RSS 1 RSS 2 = A r 1 (ϕ) A r 2 (ϕ) = A c (ϕ)<label>(2)</label></formula><p>Inverting Eq. ( <ref type="formula" target="#formula_1">2</ref>) and we can obtain the 1-D AoA or incident angle:</p><formula xml:id="formula_2">ϕ = A -1 c RSS 1 RSS 2<label>(3)</label></formula><p>Note that RSS i can be measured in real-time by each PD at each frequency. The composite angular response A c = A r 1 /A r 2 is essentially a function that maps AoA to the differential response between the 2 PDs. Each PD's response function A r i is fixed and is known through data-sheet or a one-time factory calibration, which in turn allows us to derive A c as a static lookup table.</p><p>The above model builds on three approximations: (i) For A -1 c to be a valid function, A c needs to be monotonic, at least within a range of AoA. (ii) The two PDs bear the same AoA relative to a light source. (iii) The light source can be approximated as a point source from the PDs' perspective in terms of area. We now validate these approximations through measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.2</head><p>Accuracy of AoA model. We first verify the monotonic relation between AoA and A c , using different PD combinations. We test 3 PDs with different FoVs: 40 • (Vishay BPV10), 80 • (Everlight PD333) and 120 • (OSRAM BPW34). Fig. <ref type="figure">5</ref> plots the actual angular responses of the PDs, measured by rotating the PD using a programmable motion controller <ref type="bibr" target="#b11">[12]</ref> while illuminated by a LED. Note that the measured FoV often deviates from the nominal rating in the datasheet, which necessitates a one-time factory calibration of the angular response.</p><p>To verify the AoA model, we measure the composite angular response A c of different narrow+wide PD combinations: BPW34 + BPV10 and BPW34 + PD333, respectively. Fig. <ref type="figure">6</ref> shows that A c indeed grows monotonically with AoA up to a certain threshold, beyond which it starts to drop, making A -1 c ambiguous. The threshold AoA equals 28 • and 57 • , translating to usable FoV of 56 • and 114 • for the two combinations, which is much wider than most smartphone cameras (around 70 • <ref type="bibr" target="#b1">[2]</ref>). For both combinations, the monotonic region is slightly larger than the FoV of the PD with narrower FoV. In practice, to ensure a one-to-one mapping between AoA and A c , Pulsar only uses A c when it falls in the monotonic region.</p><p>We also notice a tradeoff between FoV and spatial resolution for the dual-PD sensor. The wider-FoV combination BPW34 + PD333 has lower spatial resolution -the composite angular response A c is flat at near-zero AoA region, which can induce ambiguities. This     is because PDs with larger FoV have flatter angular response at lower AoA values. We expect combining 3 or more PDs can make a better tradeoff and provide even larger FoV. For simplicity, the rest of paper will use the BPW34 + PD333 combination, and leave other possibilities for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Impact of PD Arrangement. Our AoA model assumes the two PDs are arbitrarily close and experience the same optical channel. But in practice, they have to be physically separated, inducing certain anisotropy. To evaluate the impact on AoA sensing, we place Pulsar's dual-PD sensor 3 m below a ceiling light, and rotate it to 500 random but known directions using the motion controller. Since the PDs have a very small form-factor, they are placed approximately 5 mm apart. The scatter plot in Fig. <ref type="figure" target="#fig_4">7</ref> and CDF in Fig. <ref type="figure">8</ref> indicate that the 90-percentile errors are less than 5 • within the ambiguity-free region for all cases. This implies the dual-PD can be abstracted as a single sensor, receiving lights isotropically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.4</head><p>Point-source Approximation. When using a PD to sense a large light fixture (e.g., tube lights in a rectangular housing) at close distance, the light spans across a large range of incident angles. The equivalent angular response will be a smoothed version of the original one, which may break the point-source approximation and introduce error in AoA.</p><p>To quantify the ultimate impact, we place a CFL lamp at various distances, leading to different levels of response smoothing. Using the same measurement methods as in Fig. <ref type="figure" target="#fig_4">7</ref>, we found that even in the extreme case with 1m distance, the mean AoA error is only 4 • (Figs. <ref type="figure">8</ref> and<ref type="figure">9</ref>). And it reduces to 3 • and 2 • at 2m and 3.75m, respectively.</p><p>Further, we repeat the experiments with tube FLs (T8 type, Fig. <ref type="figure">3A</ref>). The results (Figs. <ref type="bibr">10 and 11)</ref> show that when placed 2 m below the ceiling, Pulsar's mean AoA sensing error is below 6 • , and 90-th percentile below 13 • . Interestingly, the error does not change with direction (Sec. 2.2.3), as the amount of smoothing in the PD's angular response does not vary. Overall, the point-source assumption well approximates reality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DISCRIMINATING LIGHT LANDMARKS USING FREQUENCY FEATURES</head><p>Since 3-D localization requires 1-D AoA information from multiple landmarks, Pulsar's sensor must be able to simultaneously discriminate the lights and compute the AoA of each. This requires adjacent light fixtures to emit uniquely identifiable signals that do not interfere with each other when captured by the PD. Most existing VLP systems use modified LEDs which modulate the optical signals to meet these requirements <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b56">58]</ref>. Meanwhile, LiTell <ref type="bibr" target="#b62">[64]</ref> proposed a way to discriminate unmodified FLs, based on the fact that their driver works by oscillating <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b49">51]</ref>, and manufacturing tolerance <ref type="bibr" target="#b43">[45]</ref> leads to unique interference-free oscillating frequencies, referred to as characteristic frequencies (CFs). As in LiTell, Pulsar aims to discriminate the lights using their CFs as features. When capturing multiple lights simultaneously, LiTell can isolate the lights' signals because their locations differ on the camera image. In contrast, the use of single-pixel PDs brings two unique challenges to Pulsar. (i) Near-far interference. A light's emission spectrum comprises not only the CF, but also various spurious frequencies. The spurious signals from nearby/brighter lights can overwhelm the CF features from further/dimmer ones, making them unidentifiable. (ii) Lack of spatial resolution. Previous work <ref type="bibr" target="#b62">[64]</ref> utilizes relation between consecutively located lights to facilitate error correction, while Pulsar's sensor cannot resolve spatial relation. This limits Pulsar's light identifying accuracy when collision happens. In the following sections, we describe how Pulsar meets these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Light Feature Extraction</head><p>To elucidate the near-far interference problem, we use a PD sensor (Sec. 6) to sample and analyze the frequency-domain spectrum of representative luminaries offline, including tube FL, compact FL (CFL), and LED. From the plots in Fig. <ref type="figure" target="#fig_10">12</ref>(a)-(c), we see that each type of light manifests a peak CF, but accompanied by numerous side peaks <ref type="foot" target="#foot_1">3</ref> . Fig. <ref type="figure" target="#fig_10">12(d</ref>) further shows the mixed light spectrum in an office room, with tube FL fixtures separated 1.2m apart on the ceiling. The PD sensor has 120 • FoV, faces upwards and sits 4m below the ceiling. The ground-truth CF is measured by turning on each light separately. We can see that the PD captures the CFs of 6 lights simultaneously. However, the faraway lights' CFs are immersed in the spurious peaks caused by close-by lights. Thus, unlike camera-based LiTell system <ref type="bibr" target="#b62">[64]</ref> which only needs to extract the single strongest peak in the spectrum, we cannot single out every frequency by picking the strongest frequency components.</p><p>In effect, there is even no easy way to identify the number of lights within the spectrum.</p><p>To overcome this challenge, note that we can use the model in Sec. 2.2 to compute the AoA of the signals corresponding to each individual frequency peak. We observe that if different frequency peaks have the same AoA, it is likely that they originated from the same light source. Therefore, we first sort the peaks in the spectrum by their AoAs, and then pick the strongest peak for each unique AoA, which corresponds to the true CF of one light. We repeat this until the remaining peaks become negligible (likely to be noise).</p><p>To verify the feasibility of this AoA-based light extraction, we set up one tube FL inside a darkroom. We then use our AoA sensor to receive the light signal, while moving the sensor up and down, just like walking. Fig. <ref type="figure" target="#fig_0">13</ref> shows that the RSS ratio is indeed consistent across different frequency components from the same light, as long as RSS's from both PDs are sufficient.</p><p>Alg. 1 summarizes the procedure of CF extraction in Pulsar. Note that since the AoA value is derived from RSS difference from 2 PDs, a slight difference in frequency responses of the 2 amplifiers may cause slightly different AoAs to be observed at each frequency. As a result, we empirically set a tolerance to the RSS ratio (0.5dB in our experiments) when merging spectrum peaks with similar AoAs, according to experimental results for a single light in the dark room. Meanwhile, when 2 lights have similar AoAs, the weaker one will be removed. Such similar AoAs often come from co-located lights, and thus will not reduce the number of effective lights.</p><p>Occasionally, frequency components of 2 different lights may collide (fall within 2 FFT bins), which may distort the AoA. However, it will only result in spurious detection under the following additional conditions: (i) Both lights have sufficient RSS and their perceived RSS ratio is distorted by more than 0.5dB. Either their RSS at the colliding frequency is comparable <ref type="foot" target="#foot_2">4</ref> , or a different light dominates each PD. (ii) RSS at the colliding frequency is higher than any other lights with similar perceived AoA. In practice, we find this situation to be rare throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Multiple Lights without Spatial Knowledge</head><p>Despite the diversity of the CF, occasionally there exist remotely separated luminaries with barely distinguishable CF values. LiTell <ref type="bibr" target="#b62">[64]</ref> mitigated such CF collisions by combining two consecutively sampled lights, which provides a form of spatial error correction and greatly lowers possibility of CF feature collision. But it requires the user to travel across at least 2 lights, thus precluding instantaneous location fix without any user intervention. In contrast, Pulsar's PD-based sensor can resolve a group of lights' CFs simultaneously, which enhances feature diversity without user intervention, but entails a new challenge. Unlike camera <ref type="bibr" target="#b62">[64]</ref>, PD has no spatial resolution and cannot recognize whether two lights are adjacent.</p><p>To overcome this barrier, for each measurement observation of the light signals, Pulsar uses all the CFs it extracts (Sec. 3.1) as a feature collection, denoted as L. It then searches for the set of lights in the database whose CFs are closest to L in terms of Euclidean distance. Given the large number of lights in each building, a bruteforce search among all possible group combinations is infeasible. However, we can prune the search space based on two observations: (i) Frequency error of each light. Existing measurement study <ref type="bibr" target="#b62">[64]</ref> showed that the CF feature is highly diverse even among lights in the same building (99% of the lights have CFs separated more than 20Hz). It is also stable, varying only by tens of Hz across weeks. Therefore, for each element in L, it suffices to put the few database candidates with closest CF into the search space. (ii) Physical distance between each identified light. Since within each observation the sensor should see lights which are near each other, groups that have candidates with large distance (computed with the candidates' registered locations) are also likely to be incorrect matches. We thus combine these two factors into a score that describes how likely a match is valid. These two factors can be weighted and we empirically give them equal weight when values are in kHz and meters. In addition, we use the score as a confidence value for each light, which is later used for optimizing the reliability of localization (Sec. 4). Alg. 2 summarizes the above procedure which identifies the group of lights in one measurement observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sensitivity and Coverage</head><p>Ideally, Pulsar needs to ensure multiple lights' CF features are detectable to realize blind-spot free location sensing. Existing work re-purposed smartphone cameras to extrapolate the lights' CFs <ref type="bibr" target="#b62">[64]</ref>. Due to their narrow FoV <ref type="bibr" target="#b1">[2]</ref>  </p><formula xml:id="formula_3">ϵ d ← 0, ϵ f ← 0 for d ∈ D do ϵ d ← ϵ d +meanDistance(d, {d ′ d ∈ L}) for l ∈ L, d ∈ D do ϵ f ← ϵ f + |l .CF -d.CF| D.score= (ϵ d + ϵ f )/|L|</formula><p>density/height <ref type="bibr" target="#b62">[64]</ref>. In contrast, Pulsar's PD sensor, owing to wider FoV and higher dynamic range (typically 30dB higher than CMOS cameras <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b59">61]</ref>), should easily observe more than 3 lights in the same environment (see measurement results in Sec. 3.3). We now conduct experiments to verify how close Pulsar can be to this ideal. Impact of distance. How far need the light/PD to be separated to ensure detectable CF? We answer the question by experimenting with two PDs of different FoVs (40 • and 120 • ), and a GE CFL with 15W power, which is conservative compared with most commercial lighting fixtures (20W -78W <ref type="bibr" target="#b48">[50]</ref>). We also set up a Philips 8.5W LED bulb, with the same 800-lumen output as the CFL. To isolate the impact of incident angle, we pose the PD and lights so that they face each other directly. Then we measure the signal-to-noise ratio (SNR) between the CF component and the noise floor on the received light spectrum. The results in Fig. <ref type="figure" target="#fig_12">14</ref> show that the CFL's SNR decreases over distance, but remains above 26dB even at a distance of 8m. Considering the much higher emission power and high density of commercial tube FLs <ref type="bibr" target="#b48">[50]</ref>, we expect Pulsar to easily capture multiple lights' CF features simultaneously in real-world buildings. Note that the narrower PD has higher SNR due to higher angular gain.</p><p>The LEDs' CF feature is relatively weaker, but still 16dB above the noise floor at 8m distance. The weaker feature is due to the fact that LEDs are driven by DC and manufacturers usually employ filters on outputs of LED drivers to suppress flicker. As a contrast, we note that the camera-based CF detection system <ref type="bibr" target="#b62">[64]</ref> only works for FL, and its SNR drops close to 0dB even at a short distance of 2.5m. In addition, we have followed the methodology in <ref type="bibr" target="#b62">[64]</ref>, and tested the CF diversity in a college building with 179 LEDs, which is the only large-scale LED deployment nearby. There are 2 different light models. The first covers 120 LEDs whose CFs roughly follow a distribution N (89.092, 0.831) kHz, while the CFs of the other model are scattered between 18 and 77 kHz. We found the LEDs exhibit diverse CFs just as the FLs measured in <ref type="bibr" target="#b62">[64]</ref>, albeit at weaker SNR which is not discernible by cameras.</p><p>Joint impact of ceiling height and light density. The actual number of lights Pulsar can capture in practical indoor environments depends on not only the lights' height/density, but also relative angle to the PD sensor. Since it is infeasible to test all buildings, we synthetically generate different height/density configurations, and use the SNR-to-distance trace in the foregoing experiment to compute the number of CFL bulbs visible to a PD. In addition to distance, the signal intensity is also scaled by the angular gains due to the light's irradiation angle (from the bulb) and incidental angle (into the PD), assuming the PD faces upwards and each light source has roughly 20 W power. For simplicity but without loss of generality, we only simulate round-shaped CFL with known gain patterns. A bulb is considered visible to the PD if the signal corresponding to its CF exceeds an SNR of 3dB.</p><p>From the results (Fig. <ref type="figure" target="#fig_14">16</ref>), we observe that the number of observable lights increases with higher light density (smaller spacing). It also grows with ceiling height despite the increase in propagation loss. By analyzing the data, we observe that as long as the PD sensor has &gt; 80 • FoV (which is quite common for commodity PDs), it can observe 3+ lights in most practical settings (spacing &lt; 4m, height &gt; 2m).</p><p>Impact of ambient light interference. Sunlight may affect VLP when it operates near the window or under skylights. To test whether Pulsar can work under strong ambient light, we place its sensor dongle (Sec. 6) by a west-facing window on a sunny day, 2m below a tube FL. We started measurements at 1:30 PM when the sunlight starts to shine through the window, and stop at 7:00 PM close to sunset. The sensor dongle has its TIA gain configured to a low value of 1kΩ to avoid saturation, and the second stage gain is set to 300×. Fig. <ref type="figure" target="#fig_13">15</ref> shows that the sensor remains at high SNR and does not saturate even under direct sunlight (approx. 35 kLux). Since the sunlight has no flicking frequency, it does not interfere Pulsar as long as saturation does not occur. This is salient property is harnessed in Pulsar's 2-stage amplifier design (Sec. 6) that is only sensitive to flickering light.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOCALIZATION WITH SPARSE PHOTOGRAMMETRY</head><p>Once Pulsar identifies multiple lights within its sensor's FoV (Section 3) and computes the AoA relative to each light (Section 2), it uses simple geometrical models to derive the sensor's location. We now introduce how the models work, under common light density (3+ lights within FoV) and extremely sparse deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D Location Fix with 3+ Lights</head><p>Without loss of generality, we assume Pulsar's dual-PD to be installed on a smartphone's front-face, just like the current light sensor and front-camera. Denote the phone's location as K = (x, y, -h), where (x, y) is its 2D coordinate and h the vertical distance to the ceiling. The light i's location L i = (x i , y i , 0) is known from Pulsar's light registration mechanism (Sec. 5), and stored in an online database. Further, denote the unit normal vector of the phone's surface as N = (a, b, c), which can be measured by standard smartphone motion sensors (compass, gyroscope, and accelerometer) <ref type="bibr" target="#b65">[67]</ref>. The   line vector from phone to light PL i = (x i -x, y i -y, h) is parallel to the direction of incoming light beam. By definition, the AoA ϕ i is the angle between the light beam and the normal vector of the phone's surface (Fig. <ref type="figure">2</ref>), so we have:</p><formula xml:id="formula_4">cos ϕ i = N • PL i |N||PL i | = a(x i -x) + b(y i -y) + ch (x i -x) 2 + (y i -y) 2 + h 2<label>(4)</label></formula><p>since |N| = 1 by design. Eq. ( <ref type="formula" target="#formula_4">4</ref>) is the core equation that enables Pulsar's sparse photogrammetry localization. To resolve the 3 unknowns representing the phone's location, we need AoA measurements from at least 3 lights to form a system of equations. When n &gt; 3 lights are sensed, we can turn the equation solving into a minimization problem:</p><formula xml:id="formula_5">K = argmin h &gt;0 n i=0 w i N • PL i |N||PL i | -cos ϕ i 2<label>(5)</label></formula><p>which essentially finds the location vector that best fits the AoA measurements to all lights. Here w i represents the matching confidence for light i (Sec. 3.2), which acts as a weight to prevent occasionally mismatched lights from disturbing the location computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Circumventing Unreliable Compass Information Using 4+ Lights</head><p>Compasses can measure the phone's azimuth angle, but are notorious for its poor accuracy indoor due to magnetic interferences from metal structures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b58">60]</ref>. To overcome this limitation, Pulsar can treat azimuth as a variable in the system of Eq. ( <ref type="formula" target="#formula_4">4</ref>), and use 4 or more lights to solve for both 3D location and azimuth. To this end, we note that the normal vector N can be rewritten as a function of azimuth α, roll β and pitch γ based on rotation matrix <ref type="foot" target="#foot_3">5</ref> :</p><formula xml:id="formula_6">N = ( cos α sin γ + cos γ sin β sin α, -sin γ sin α + cos γ cos α sin β, cos γ cos β)<label>(6)</label></formula><p>The pitch γ and roll β angles are measured by the accelerometers relative to the direction of gravity. They have proven to be reliable in indoor environment <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b65">67]</ref>. So we only solve for the azimuth variable α along with x, y, h, following the same minimization problem in Eq. ( <ref type="formula" target="#formula_5">5</ref>). More generally, given 6 or more lights, we may even resolve the γ and β without any motion sensors, achieving both 3D location and 3D orientation sensing. But this entails ultra-high density light deployment and will not be covered in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Identifying the Nearest of 2 Lights</head><p>Pulsar can fall back to coarse-grained light-cell level localization when insufficient number (≤ 2) of lights are simultaneously visible to populate the system of Eq. ( <ref type="formula" target="#formula_4">4</ref>). This fallback mode requires Pulsar to identify the light closest to the phone, which can still afford meter-scale location granularity, depending on the light density.</p><p>Far-away lights have larger irradiance angle and hence lower angular gain, as well as higher propagation loss. However, the nearest light does not necessarily yield the highest RSS. Ambiguities can be introduced by variations of lights' output power and PD's angular response, as evident from the channel model in Eq. ( <ref type="formula" target="#formula_0">1</ref>). The former effect is negligible compared to the distance attenuation <ref type="bibr" target="#b55">[57]</ref>, especially because most buildings prefer homogeneous light sources. To combat the latter variation (i.e., RSS uncertainty due to PD's orientation), we leverage the PD's known angular response w.r.t. the measured AoA ϕ i of the light i. More specifically, we can compensate for the angular response to obtain the true RSS: RSS ′ = RSS/A r 1 (ϕ i ). Then the light with the highest RSS ′ always has the shortest distance to the PD.</p><p>In Pulsar, the cell-level location is also used to seed the initial point of the nonlinear minimization problem in Eq. ( <ref type="formula" target="#formula_5">5</ref>) and accelerate its convergence. It also helps identify each overhead lights in the light registration process (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIGHT REGISTRATION ON MAP</head><p>Ubiquitous deployment of VLP requires a scalable way of surveying the landmark locations in the first place. Prior VLP systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40]</ref> assume a priori knowledge of the light fixtures' locations, but actually, even the "smart" LEDs do not know their own locations. The lights' locations and identities have to be associated manually, either by programming the bulbs or via a separate database. Pulsar's light registration scheme solves this problem. Simply put, it requires a one-time survey of the lights inside a building, which automatically registers the lights' locations on the building's floor plan image, and stores the results in Pulsar's database, eliminating the need for physically measuring the location of each light. The lights' locations can be translated into physical locations given the scale of the floor map. More specifically, a surveyor needs to start under a light with known location (on the map), hold a Pulsar receiver and walk across each of the ceiling lights inside the target building. Meanwhile, the surveyor's walking trajectory is tracked using existing inertialtracking methods. When Pulsar detects a light (manifested by an RSS peak), it records the light's CF and marks its 2D location the same as the surveyor's current location (ceiling height is assumed known).</p><p>Our Pulsar prototype employs Google's Tango tablet to track the surveyor's trajectory. Tango combines cameras and inertial sensors to achieve decimeter precision <ref type="bibr" target="#b15">[16]</ref> with its visual-inertial odometry (VIO). Due to inevitable drift of the inertial sensors, Tango's precision degrades over long distances. So we partition the surveying process into sections, each starting from a light with known location on map. The residual errors are small and can be compensated through manual adjustment on the map, considering the ceiling lights have highly regular separations. The compensation can also be automated through geometrical fitting and computer vision, but this is beyond the scope of our current work. We note that Pulsar is not tied to the Tango device -advances in robotics have enabled high-precision VIO using commodity smartphone cameras and motion sensors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>We emphasize that the VIO methods work under several assumptions that are tolerable for controlled one-time surveying, but cannot substitute Pulsar's real-time localization: certain anchor points exist whose locations are known to the user device; the surrounding environment scene is static; the walking speed is low and needs to be regular. In addition, the camera operations incur high power consumption and are unsuitable for continuous localization battery-powered devices.</p><p>Further, note that the CF features depend on the lights' hardware and have shown to be highly stable over multiple weeks <ref type="bibr" target="#b62">[64]</ref>. We have also conducted a year-long measurement of 8 randomly sampled lights in an office building. We found that the CFs are also stable at annual scale, with monthly and year-to-year average drift less than 10 Hz for most of the lights, as shown in Fig. <ref type="figure" target="#fig_15">17</ref>, which is stable enough for reliable long-term identification <ref type="bibr" target="#b62">[64]</ref>. In the longer term, a light's CF feature may change due to failure, aging or replacement. Such changes will manifest as a consistent singlelight mismatch when users run the light identification algorithm (Section 3). Therefore, Pulsar can detect such outage events, and update its light database automatically, keeping the overhead of re-registration to a minimum.</p><p>Finally, the light registration should not be confused with the fingerprinting procedure in signature-based localization systems <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b57">59]</ref>. The fingerprinting entails extensive survey of all locations, whereas Pulsar only needs a one-time walk-through of all light landmarks which are much sparser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head><p>Dual-PD light sensor. We design a custom light sensor that can be plugged into mobile devices with USB ports (Fig. <ref type="figure" target="#fig_0">18</ref>). The sensor contains an STM32F103C8T6 microcontroller, which samples the signals from 2 PDs (BPW34 and PD333). The MCP6024 quad operational amplifier is used to form 2 transimpedance amplifiers and 2 voltage amplifiers, which condition the signals before they enter the microcontroller. The entire prototype costs around $30, while a production version built into the phone (Sec. 8) will be significantly cheaper.</p><p>The firmware running on the microcontroller uses DMA and timer to drive 2 internal ADCs, which sample 2 PDs simultaneously at 300Ksps with 8-bit depth. Since the ADCs are directly timed by the hardware timer, jitter and artifacts in sampling are kept to a minimum, ensuring highly accurate spectrum analysis. The sampled data are buffered in a 16 KB ring buffer before being transferred to the mobile device via 12 Mbps USB 2.0 full speed. We use a rigid USB OTG adapter so Euler angles of the device and the dongle are kept the same.</p><p>Android app and registration software. As illustrated in Fig. <ref type="figure" target="#fig_0">19</ref>, the Pulsar Android app comprises a few modules: UI (map/location display), 3-D position solver, database, light identification, signal processing and dongle communication. The light identification first runs FFT on the samples (streamed from the dongle) to obtain the spectrum, and then simultaneously extracts multiple lights' CFs and match their identities, following Sec. 3. The app then looks up a local database that maps each light's CF to its location. Then, the RSS of each light's CF feature feeds the differential channel response, which is mapped to the light's AoA (Sec. 2). Finally, Pulsar computes the sensor's 3D location and heading direction (Sec. 4), and presents the information in the UI.</p><p>We implement the dongle driver part in native C code with JNI to allow best real-time performance and avoid discontinuities in samples, which can introduce severe artifacts in spectrum analysis. We use a modified version of libusb <ref type="bibr" target="#b13">[14]</ref> together with Android's USB Host API to provide access to the USB dongle in C code without requiring root privilege. Some of the signal processing and the location solver are also implemented in C since they require functions such as nonlinear minimization from a numerical library. We pick GNU Scientific Library (GSL) <ref type="bibr" target="#b16">[17]</ref> as our numerical library and have ported it to Android. The database and PDs' calibration data are stored in pure text, providing the lights' identification number, CF, and physical locations. All other components are implemented in Java with standard Android API.</p><p>The inertial tracking for the light registration process (Sec. 5) is provided by the Tango tablet <ref type="bibr">[20]</ref> since it has a complete and ready-to-use visual-inertial odometry (VIO) implementation. Many other monocular VIO techniques emerged recently in the robotic navigation field <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, which have shown comparable tracking precision as Tango. But the implementations are either kept in private or constrained to a specific smartphone model. We have also implemented a graphical application on PC that allows the surveyor to fit the motion trace to floor plans and save the results as the database (Fig. <ref type="figure" target="#fig_17">20</ref>).      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper Session</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PERFORMANCE EVALUATION</head><p>We evaluate Pulsar's performance both through controlled experiments like existing VLP systems <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31]</ref>, and through field tests in two environments: the aisles of an office building and the atrium of an activity center (Fig. <ref type="figure" target="#fig_18">21</ref>). The aisles have tube FLs arranged linearly, 4.5m apart on 3m ceiling, while the atrium has 2 rings of CFLs spaced 1.5m apart on a 4m ceiling. For the light identification test, we include an additional library setting with densely deployed tube FLs on 2.8m ceiling. All computations are performed on the smartphone, with the self-contained Pulsar App. For the light registration mechanism, its landmark location precision depends on the external inertial tracking mechanism and can be manually adjusted during bootstrapping (Sec. 5). So we omit the tests for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Accuracy of Light Identification</head><p>We run Pulsar's multi-light identification algorithm over databases built for the test venues with light registration. The aisle, atrium, and library involve 64, 110 and 157 lights, each with 2-3 models of lights <ref type="foot" target="#foot_4">6</ref> , respectively. For each venue, we hold the sensor, move to 1000 random locations, and compare the identification results with the database ground-truth. Further, we simulate the identification for smart LEDs with randomly allocated frequencies, with frequency drifts following N (0, 5) Hz and N (0, 10) Hz distribution, since we could not find a building with smart-bulbs deployed in large scale. We observed that the number of lights that Pulsar captures within its FoV varies from 3 to 6 in each measurement.</p><p>In addition, with incumbent lights, in 92.5% of the time, Pulsar's light identification mechanism matches every single light correctly (Fig. <ref type="figure">22</ref>, label 'All'). In 92.6% of the cases, Pulsar can identify at least 3 lights correctly (Fig. <ref type="figure">22</ref>, label '3+'). If smart-bulbs are employed, the accuracy further rises to 93% -100% (Fig. <ref type="figure">23</ref>), depending on the number of lights and allocated bandwidth. The identification accuracy also shows high consistency across measurements, primarily owing to the homogeneous light layout in most buildings. Note that the wrongly matched lights in one capture are usually deemphasized by low weights (Sec. 4.1), which lessens their impact on the final localization result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Accuracy of Sparse Photogrammetry</head><p>To receive from multiple lights, Pulsar's sensor must be able to extract frequency and AoA information from sources with highly diverse RSS simultaneously. Therefore, the sensor must work under a wide range of RSS. Its FoV and ambiguity-free range (Sec. 2) also needs to guarantee that it can "see" enough number of lights under low ceiling, sparse deployment scenarios. In the following experiments, we set up a controlled environment similar to existing VLP systems <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31]</ref> to evaluate Pulsar's performance under different settings. Specifically, we arrange 23W CFLs in a linear array, whose spacing and height (w.r.t. the sensor) can be flexibly adjusted. Unless noted otherwise, we repeat the experiment 500 times under each configuration. 3D localization accuracy. Fig. <ref type="figure">24</ref> shows Pulsar's localization performance across different ceiling heights, with 4 CFLs placed 1.5m apart and sensor placed flat. Pulsar achieves very high precision for both 2D and height localization (label 'H'), thanks to its AoA-based sensing scheme. The median error is 5cm for 2D localization (x, y) and 20cm for height (h), when the sensor is held 2m below the ceiling. The result is highly consistent across experiments, with 90-th percentile error close to the mean, due to the determinism and multipath-free propagation profile of visible lights. Under a high ceiling height of 4m, due to weaker RSS, the median error increases slightly, to 6cm and 31cm, for (x, y) and h, respectively. In an extreme test where the height grows to 6 m, we observe a large systematic error-the estimated locations are relatively tightly packed, but deviate from ground truth for more than 1m. However, note that typical commercial lights <ref type="bibr" target="#b48">[50]</ref> use much higher emission power than our CFLs. Therefore, we expect Pulsar's sparse photogrammetry can provide accurate sub-light level localization under common luminaires and ceiling height settings. Fig. <ref type="figure">25</ref> shows Pulsar's localization accuracy under different separations of the 4 CFLs, while height is maintained at 4m. We notice that Pulsar's localization error in h remains roughly the same, whereas the 2D localization precision degrades as the separation increases (median error 0.07m, 0.72m, and 1.27m, for 1.5m, 2m, and 2.5m separation, respectively). Further analysis shows the major source of error comes from the lateral direction w.r.t. the light array, as the RSS becomes much weaker from lights near the tail.</p><p>To verify that Pulsar behaves consistently for different types of lights, we compare Pulsar's localization accuracy between 4 CFLs and 4 T8 tube FLs. The tube FLs are enclosed in diffusive covers, arranged with 1.2m separation in between. Pulsar's sensor is placed 2m below the ceiling. Fig. <ref type="figure">26</ref> shows that Pulsar's 2D localization performance degrades by only about 10cm when we switch from CFL to tube FL, although the area of the light is larger by at least 10×. This echoes the micro-benchmark in Sec. 2.2.4. It further verifies that Pulsar's point source assumption holds in practical scenarios and does not become the bottleneck in location accuracy.</p><p>Pulsar's 3D localization mechanism requires at least 3 lights. Ideally, adding more lights should improve accuracy. However, since the visible light channel is very deterministic and AoA sensor suffers from very little interference, we observed that increasing the number of lights does not improve the accuracy significantly across our experiments.</p><p>Accuracy of heading direction estimation. When there are 4+ lights available within FoV, Pulsar can derive the sensor's normal vector (same as the smartphone's), which can substitute the compass and be used to derive the user's heading direction (Sec. 4.2). To evaluate the accuracy of heading solving, we hold the sensor in ∼ 0 • (roughly flat), 30 • , 45 • and 60 • inclinations under one row of 0.45m-long tube FLs spaced 0.8m apart, with a sensor-to-ceiling distance of 1.5m. We take 200 samples and compare the error with ground truth obtained relative to the floor-plan. Fig. <ref type="figure" target="#fig_4">27</ref> shows that the 90-th percentile error is only 3 • ∼ 5 • , when the sensor is not laid flat. When the sensor is paralleling to the ceiling, however, orientation of the smartphone no longer affects AoA, which means the orientation can be resolved to arbitrary value, yielding huge errors, while 3D localization results are correct. Since users normally will not hold the smartphone completely parallel to the ceiling, we expect this to be a corner case.</p><p>Accuracy of nearest light identification. To evaluate the effectiveness of Pulsar's nearest light identification, we place 2 CFLs 1.5m apart, and the sensor 2m below. As a stress test, we put the sensor near the center of the 2 lights' (x, y) projection, but slightly biased to one of them by a short distance of 0.3m to 0.6m. We then gradually rotate the sensor to the other light and see how often it is mistaken for the nearest one. To evaluate the reliability in discrimination, we use the margin in the compensated RSS (RSS ′ in Sec. 4.3), i.e., difference between the compensated RSS of the nearest light and that of the other light. Each configuration takes 75 trials. From Figs. 28 and 29, we notice that Pulsar can identify the nearest light with close to 100% reliability when both lights fall roughly within the sensor's ambiguity-free range (Sec. 2.2.2). Once the nearest light is far out of the ambiguity-free range, error in identification is likely to happen, especially when the sensor is close to the center of the 2 lights. Since such extreme case should remain minority, and localization error caused by wrong identification is limited since the receiver is close to the center between 2 lights, we conclude that Pulsar's nearest light identification is sufficient as a fallback mechanism to achieve cell-level location granularity. Impact of human motion. We evaluate impact of user's movement with a compact setup, which includes one row of 0.45m-long tube FLs spaced 0.8m apart, placed 1.5m above the sensor. The sensor is first mounted on a tripod with 30 • , 45 • and 60 • inclinations, and then is held by a user walking in the same spot, with device naturally rotated while walking. Fig. <ref type="figure">30</ref> shows that Pulsar's 2D localization accuracy does not degrade significantly under user's motion, while height estimation does show increased error. In addition, 90-th percentile error of heading estimation only increased slightly, from 5 • to 9 • . We thus conclude Pulsar is robust against normal user motion, as long as during each sampling period the AoA remains relatively constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Large-Scale, Real-time Field Tests</head><p>In this section, we verify Pulsar's localization performance in uncontrolled environment in real-time. For each test venue, we found the light registration process takes only about 10 to 20 minutes per 100 lights, since we just need to sample each light once. We walk across each venue following a marked route, and then compare the location traces generated by Pulsar and by Tango's visual-inertial odometry algorithm (shown to have sub-meter precision <ref type="bibr" target="#b15">[16]</ref>). For a fair comparison, both run on the same Tango tablet simultaneously. Fig. <ref type="figure">32</ref> and Fig. <ref type="figure">33</ref> depict Pulsar's performance in the aisles of the office building and the atrium, respectively, with background showing the floor-plan annotated by Pulsar's light registration application. Pulsar shows overall median and 90-th percentile accuracy of 0.6m and 1.6m, respectively. Its real-world performance is comparable to the VIO method which, despite the high precision, runs costly image processing and can only track relative movement (Sec. 5). In addition, unlike VIO, Pulsar does not suffer from sensor drift, thanks to deterministic light propagation and robust AoA sensing. Note that a few missing spots exist in the location traces, because of occasional shadowing or location solver failure. These problems can be avoided by enlarging the FoV and ambiguity-free range of the sensor (Sec. 2.2.2) and employing better minimization algorithms (Sec. 4.1) in future.</p><p>We have also measured Pulsar's processing latency within its Android app. Fig. <ref type="figure">34</ref> shows that the end-to-end localization latency is around 840ms across most of the location queries, which allows real-time 3D location fix. USB transfer and FFT all contribute to latency substantially, each making up 16% of total latency. Latency in extraction process is dominated by peak finding. Both the light (a)  extraction and location solving run efficiently, costing only up to tens of milliseconds. Integrating the sensor into smartphones and eliminating the USB interface, along with sparse spectrum recovery (Sec. 8) can potentially reduce 1/3 of the latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Power Consumption</head><p>We measure Pulsar's end-to-end energy consumption/battery life through the sysfs interface <ref type="bibr" target="#b37">[39]</ref>, and further break the power consumption down by disabling/disconnecting each component. Fig. <ref type="figure" target="#fig_23">35</ref>(a) shows a system-wide breakdown, where "USB Loss" is caused by converting the battery voltage up to the USB's 5V and then back down to the dongle's 3.3V; and "Idling" means active idling with adb over Wi-Fi running. Fig. <ref type="figure" target="#fig_23">35(b</ref>) further analyzes the sensor dongle, where "Sampling" includes ADC, timing, and DMA, which copies sampled data to the dongle's buffer. The microcontroller CPU of the dongle is responsible for copying data to the USB interface, whereas it busy waits when the sampling is in progress since we have not implemented any power-management. The dongle only consumes about 45mA of current when sampling (less than 150mW since it takes 3.3V power), which is even lower than WiFi's idle listening power <ref type="bibr" target="#b63">[65]</ref>. From an end-to-end perspective, it is clear that Pulsar's PD-based solution avoids energy-hungry camera sensors and image processing, which leads to a power consumption of less than half of camera's power end-to-end (Table <ref type="table" target="#tab_1">1</ref>), despite that there still exist substantial spaces for optimization (Sec. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>Integrating the AoA sensor into COTS phones. With the microlens technology that is widely used in CMOS cameras <ref type="bibr" target="#b14">[15]</ref>, it is possible to integrate multiple PDs onto a compact and low-cost chip, much like the ambient light and proximity sensors used on today's smartphones. These sensors usually contain at least 2 PDs <ref type="bibr" target="#b51">[53]</ref>, with the proximity sensor already operating at tens to hundreds of kHz <ref type="bibr" target="#b21">[23]</ref>. Unfortunately, due to hardware design and sometimes merely firmware/software limitation, fine-grained samples from these sensors are usually not available to applications. We envision future smartphones can easily integrate such AoA sensors much like the ambient light sensor, enabling more accurate localization as well as many other applications.</p><p>Further improving energy efficiency. Although the Pulsar prototype is significantly more energy efficient than camera-based counterparts <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b66">68]</ref>, its high sampling rate (300 Ksps) provides a large space for optimization. To achieve reasonable (3 Hz) spectral resolution, Pulsar runs FFT with a long sample length (100,000 points), which wastes energy. Sparse spectrum recovery techniques such as bandpass sampling and sFFT <ref type="bibr" target="#b19">[21]</ref> can significantly reduce computational loads, resulting in less power consumption. With such optimizations, Pulsar may be able to run entirely on energyefficient microcontrollers.</p><p>Pulsar's hardware prototype also has space for polishing, such as integrating the sensor into smartphones to eliminate the costly USB interfaces, and adopting better hardware components. For example, a newer version of the microcontroller used <ref type="bibr" target="#b44">[46]</ref> consumes less than 1/3 of the power while providing more computational power. With proper hardware implementations, Pulsar's sensor can be extremely low power and support continuously localization for battery-operated devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RELATED WORK</head><p>Existing localization schemes broadly fall into two categories: (i) Fingerprinting approaches that associate each location spot to a set of features provided by images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, acoustics <ref type="bibr" target="#b45">[47]</ref>, magnetic field <ref type="bibr" target="#b10">[11]</ref>, RF signals <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">62]</ref>, or a mix of them <ref type="bibr" target="#b5">[6]</ref>. Real-world tests showed that such features lack stability in the presence of human activities and environmental changes, and are often not discriminative enough to offer fine location granularity <ref type="bibr" target="#b30">[32]</ref>. More crucially, they require blanket fingerprinting of all location spots, which entails huge bootstrapping and maintenance overhead. (ii) Model-driven approaches that calculate the distance between user devices and infrastructure landmarks, through models of received signal strength (RSS) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, phase <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b54">56]</ref> or propagation time <ref type="bibr" target="#b32">[34]</ref>. To achieve high location precision, such models often require specialized hardware like antenna arrays; they tend to be disturbed by multipath reflections from human bodies and ambient objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">32]</ref>.</p><p>Motion sensors (accelerometer, gyroscope, and magnetometer) can track a user's relative movement via step counting <ref type="bibr" target="#b28">[30]</ref>, but need to be calibrated by other approaches that provide absolute location fixes <ref type="bibr" target="#b50">[52]</ref>. State-of-the-art robotic systems often integrate such sensors with a camera to realize visual-inertial odometry, which can simultaneously map the environment while tracking user movement <ref type="bibr" target="#b15">[16]</ref>. But the performance suffers in homogeneous environment (e.g., office hallways) and dynamic environment (e.g., retail stores) <ref type="bibr" target="#b7">[8]</ref>. The use of cameras entails heavy sensing and computational power, hence unsuitable for long-term mobile usage.</p><p>Compared with these approaches, VLP has several inherent advantages, especially in its dense overhead anchoring points and line-of-sight propagation profile that evade interferences from ambient environment. Most existing VLP systems employ modulated LEDs as anchor points to emit beacons. The receiver can either be a PD or a camera. The key challenge for PD based VLP lies in low spatial resolution. Existing work either uses multi-light trilateration or requires controlled user movement to resolve the spatial ambiguities <ref type="bibr" target="#b29">[31]</ref>. Trilateration schemes assume LEDs' Lambertian radiation pattern, which no longer holds when collimating or diffusing optics are used for uniform illumination <ref type="bibr" target="#b52">[54]</ref>. On the other hand, controlled device movement disturbs user experience and precludes continuous, real-time localization operations. Camera-based VLP systems <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27]</ref> can achieve decimeter precision through photogrammetry, which maps the geometrical relation between LED luminaries on a camera image to the camera's physical location/orientation <ref type="bibr" target="#b24">[26]</ref>. Other systems <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b56">58]</ref> use physical rotation or electronic polarization to create spatial diversity and simplify the sensor-side workload. All these modulated VLP systems require customized LEDs with a beaconing circuitry, which adds to the already high retrofitting cost of LEDs compared with FLs.</p><p>Our work was inspired by LiTell <ref type="bibr" target="#b62">[64]</ref>, which obviates the need for specialized LEDs. Yet LiTell only works for FLs whose CF features can be extrapolated from camera images. It is not applicable to LEDs, which have much weaker CFs, or light fixtures with small areas that will not leave enough samples in the image. More critically, cameras' low dynamic range limits LiTell's working range to around 2.5 meters, and their narrow FoV causes many blind spots where the ceiling lights are not directly visible. All these factors, along with other limitations (Table . 1), hinder continuous localization services. IDyLL <ref type="bibr" target="#b55">[57]</ref> harnesses the regular separation of ceiling lights to calibrate motion sensors, thus improving dead-reckoning accuracy. But it needs users to walk in straight lines, and can only track relative movement. Fiatlux <ref type="bibr" target="#b4">[5]</ref> extracts location fingerprints from minor intensity differences among incumbent lights, yet the fingerprints vary over time and are sensitive to ambient light and the sensor's orientation.</p><p>Despite more than a decade of research <ref type="bibr" target="#b20">[22]</ref>, VLP has not reached real-world users. By overcoming the high cost <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b56">58]</ref>, high power consumption <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b62">64]</ref>, and intermittent coverage <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b62">64]</ref> in traditional VLP technologies, Pulsar can significantly accelerate the penetration of VLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>To our knowledge, Pulsar represents the first VLP system that employs incumbent FLs/LEDs and lightweight PDs to achieve continuous 3D localization with sub-meter precision. It marks a new step in enabling the ubiquitous deployment of VLP, by solving three problems: discriminating co-located LEDs/FLs, sensing AoA and hence 3D location using single-pixel PDs, automatically registering light landmarks. We believe Pulsar's salient advantages will enable a wide range of location-based services, including robotic navigation, physical analytics and augmented reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank the anonymous reviewers and our shepherd Xia Zhou for their valuable comments and feedback. This project was partially supported by the NSF under Grant CNS-1343363, CNS-1350039, CNS-1506657, CNS-1518728, and CNS-1617321.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pulsar's architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Channel model from light to PD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :igure 5 :</head><label>45</label><figDesc>Figure 4: Axial/lateral Angular response of 2 tube fixtures, at 1m distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: AoA accuracy for horizontally and vertically placed sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: CDF of AoA sensing error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CDF of AoA error for tube FLs, along axial and lateral direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: AoA sensing accuracy of tube FLs, along axial and lateral direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Characteristic frequency (CF) and side peaks for typical commercial lights: (a) T8 tube FL, (b) CFL bulb, (c) commercial LED, (d) signals from multiple tube FLs added together. Green triangles mark CFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :Algorithm 1 :</head><label>131</label><figDesc>Figure 13: Artifacts in a single light's spectrum and corresponding RSS ratio. Dots show ratio with sufficient RSS.Algorithm 1: Algorithm for light extraction. input : P -spectrum peaks with sufficient RSS; ϵ AoAtolerance (minimum difference in AoA/RSS-ratio) output : L -CFs sortByRSS(P) L ← for p ∈ P do L.add(p), P.remove(p) for q ∈ P do if |q.AoAp.AoA| &lt; ϵ AoA then P.remove(q)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>igure 14 :</head><label>14</label><figDesc>RSS vs distance forPDs with wide and narrow FoV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Impact of ambient light on SNR for PDs with narrow and wide FoV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>16 :</head><label>16</label><figDesc>Number of lamps visible to the PD under different ceiling height and lamp density, with matrix (10 × 10) and array (20 × 1) configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>(a) Monthly mean and std. and (b) drift of CF before and after 1 year for 8 office tube FLs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :Figure 19 :</head><label>1819</label><figDesc>Figure 18: Photo and diagram of the customized AoA sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Light registration application running on a PC, generating the light database.</figDesc><graphic coords="9,66.40,190.87,177.04,127.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Photos of test venues: aisles with tube FLs (upper); an atrium with CFLs (lower).</figDesc><graphic coords="9,267.41,252.38,122.36,61.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :Figure 23 :</head><label>2223</label><figDesc>Figure 22: Light identification accuracy for 3 different types of areas, error bars show std.</figDesc><graphic coords="9,267.41,189.76,122.36,61.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>a</head><label></label><figDesc>From phone to a 15W cabinet tube FL. b By area in an office building aisle. c For single measurement with tube FLs. d Nexus 5, full battery cycle, screen turned off, maximum update rate. Active idling alone last ≈11 hrs. e 7 pictures, ≈ 5 Hz spectral resolution. f 100,000-point FFT, 3 Hz resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 :Figure 25 :Figure 26 :Figure 28 :Figure 29 :Figure 30 :Figure 31 :</head><label>24252628293031</label><figDesc>Figure 24: CDF of localization error vs. ceiling height.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 32 :Figure 34 :</head><label>3234</label><figDesc>Figure 32: Localization accuracy within aisles of an office building.</figDesc><graphic coords="11,79.21,96.12,128.71,71.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 35 :</head><label>35</label><figDesc>Figure 35: Power consumption breakdown for (a) the whole system and (b) the sensor dongle alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and low dynamic range, a camera can only capture a single light in office buildings with typical light Algorithm 2: Algorithm for group light matching. input : L -CFs; ϵ = 500Hz -maximum CF error; N = 2number of candidates; B -CF database output : E -light identities</figDesc><table><row><cell>for l ∈ L do</cell></row><row><cell>l .candidates ← {b ∈ B|ϵ &gt; |l .CF -B.CF|}</cell></row><row><cell>sortByError(l .candidates)</cell></row><row><cell>truncate(l .candidates, N )</cell></row><row><cell>C ← candidateCombinations(L)</cell></row><row><cell>for D ∈ C do</cell></row><row><cell>score(L, D)</cell></row><row><cell>E ← {D ∈ C|D has lowest score}</cell></row><row><cell>Procedure score(features L, candidates D)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A comparison between LiTell and Pulsar. height a Coverage b ID accuracy c Battery life d Latency Light registration</figDesc><table><row><cell>Ceiling</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that A r i (ϕ) = cos ϕ only when the receiver follows the ideal Lambertian model and has 120 • FoV.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Diversity of CFs within each model of light has been validated by<ref type="bibr" target="#b62">[64]</ref>.Paper Session V: Location! Location! Location! MobiCom'17, October 16-20, 2017, Snowbird, UT, USA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>If RSS's of the 2 lights are within 9dB, the summed RSS would be higher than both lights by more than 0.5dB, and the error in RSS ratio may exceed 0.5dB.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The rotation matrix has many forms depending on definition of the axis and order of the rotation. Here we use the version that matches the Android API<ref type="bibr" target="#b18">[19]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>CF range: aisle 88∼93 and 137∼148 kHz; atrium 81∼86, 103∼114 and 129∼134 kHz; library 58∼65 and 88∼90 kHz.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Path Loss and Multipath Effects in a Real World Indoor Localization Scenario</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Workshop on Positioning, Navigation and Communication</title>
		<imprint>
			<publisher>WPNC</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding Camera Optics &amp; Smartphone Camera Trends</title>
		<author>
			<persName><surname>Anandtech</surname></persName>
		</author>
		<ptr target="http://www.anandtech.com/show/6777" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visible light positioning: a roadmap for international standardization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sekercioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="68" to="73" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The internet of things: A survey</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Iera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Morabito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="2787" to="2805" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fiatlux: Fingerprinting Rooms Using Light Intensity</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Azizyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ionut</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Pervasive</title>
		<meeting>of Pervasive</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sur-roundSense: Mobile Phone Localization via Ambience Fingerprinting</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Azizyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ionut</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RADAR: an In-Building RF-Based User Location and Tracking System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Padmanabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE INFOCOM</title>
		<meeting>of IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasir</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<idno>CoRR abs/1606.05830</idno>
		<title level="m">Simultaneous Localization And Mapping: Present, Future, and the Robust-Perception Age</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FM-based Indoor Localization</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indoor Localization Without the Pain</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Chintalapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand Padmanabha</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkata</forename><forename type="middle">N</forename><surname>Padmanabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor Location Sensing Using Geo-magnetism</title>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Donahoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Schmandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedram</forename><surname>Razavai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micaela</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cinetics</surname></persName>
		</author>
		<ptr target="https://cinetics.com/kit/axis360" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human Localization Using Mobile Phones</title>
		<author>
			<persName><forename type="first">Ionut</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Azizyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Did You See Bob?</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhaka</forename></persName>
		</author>
		<ptr target="https://github.com/kuldeepdhaka/libusb/tree/android-open2" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>modified version</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CMOS image sensors</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gamal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Helmy</forename><surname>Eltoukhy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Devices Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-Manifold Preintegration for Real-Time Visual-Inertial Odometry</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<ptr target="https://www.gnu.org/software/gsl/" />
	</analytic>
	<monogr>
		<title level="j">GNU Scientific Library</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Free Software Foundation, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sextant: Towards Ubiquitous Indoor Localization Service by Photo-Taking of the Environment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://developer.android.com/reference/android/hardware/SensorManager.html" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple and practical algorithm for sparse Fourier transform</title>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Hassanieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM-SIAM SODA</title>
		<meeting>of ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1183" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pervasive Visible Light Positioning System using White LED Lighting</title>
		<author>
			<persName><forename type="first">Shuhei</forename><surname>Horikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Komine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinichiro</forename><surname>Haruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Nakagawa</surname></persName>
		</author>
		<idno>of IEICE</idno>
	</analytic>
	<monogr>
		<title level="j">DSP</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">719</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Intersil</forename><surname>Americas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llc</forename></persName>
		</author>
		<ptr target="http://www.intersil.com/content/dam/Intersil/documents/isl2/isl29011.pdf" />
		<title level="m">Digital Ambient Light Sensor and Proximity Sensor with Interrupt Function</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SpotFi: Decimeter Level Localization Using WiFi</title>
		<author>
			<persName><forename type="first">Manikanta</forename><surname>Kotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AR-drone as a platform for robotic research and education</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Krajník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtěch</forename><surname>Vonásek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fišer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research and Education in Robotics</title>
		<imprint>
			<date type="published" when="2011-01">Jan Faigl. 2011</date>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Luxapose: Indoor Positioning with Mobile Phones and Visible Light</title>
		<author>
			<persName><forename type="first">Ye-Sheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Pannuto</surname></persName>
		</author>
		<author>
			<persName><surname>Ko-Jen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabal</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Mobi-Com</title>
		<meeting>of ACM Mobi-Com</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RollingLight: Enabling Line-of-Sight Light-to-Camera Communications</title>
		<author>
			<persName><forename type="first">Hui-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Hao-Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Lin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-I</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Mu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ju</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><surname>Lenovo</surname></persName>
		</author>
		<ptr target="http://www3.lenovo.com/in/en/tablets/android-tablets/tablet-phab-series/Lenovo-Phab-2-Pro/p/WMD00000220" />
		<title level="m">Lenovo Phab 2 Pro</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobile robot localization by tracking geometric beacons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="376" to="382" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Reliable and Accurate Indoor Localization Method Using Phone Inertial Sensors</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunshui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Conference on Ubiquitous Computing (UbiComp)</title>
		<meeting>of ACM Conference on Ubiquitous Computing (UbiComp)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Epsilon: A Visible Light Based Positioning System</title>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guobin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX NSDI</title>
		<meeting>of USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Push the Limit of WiFi Based Localization for Smartphones</title>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Sidhom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Realistic Evaluation and Comparison of Indoor Location Technologies: Experiences and Lessons Learned</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlado</forename><surname>Handziski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souvik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM/IEEE IPSN</title>
		<meeting>of ACM/IEEE IPSN</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filtering Noisy 802.11 Time-of-Flight Ranging Measurements</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Marcaletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Giustiniano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aymen</forename><surname>Fakhreddine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International on Conference on Emerging Networking Experiments and Technologies (CoNEXT)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Why Drive White LEDs with Constant Current?</title>
		<ptr target="https://www.maximintegrated.com/en/app-notes/index.mvp/id/3256" />
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<publisher>Maxim Integrated Products, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Internet of things: Vision, applications and research challenges</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Miorandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Sicari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imrich</forename><surname>Chlamtac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ad Hoc Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1497" to="1516" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Life of LED-based white light sources</title>
		<author>
			<persName><forename type="first">N</forename><surname>Narendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Display Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="171" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><surname>Osi Optoelectronics</surname></persName>
		</author>
		<ptr target="http://www.osioptoelectronics.com/technology-corner/frequently-asked-questions/basic-pin-photodiode-characteristics.aspx#06" />
		<title level="m">Basic PIN Photodiode Characteristics</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Getting the battery current values for the Android Phone</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Parker</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/a/7031032/6520528" />
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual Light Landmarks for Mobile Devices</title>
		<author>
			<persName><forename type="first">Niranjini</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lazik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM/IEEE IPSN</title>
		<meeting>of ACM/IEEE IPSN</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">Julian</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Amft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Bohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LuxTrace: Indoor Positioning Using Building Illumination</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">I Am a Smartphone and I Can Tell My User&apos;s Walking Direction</title>
		<author>
			<persName><forename type="first">Nirupam</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The impact of electronic ballast compact fluorescent lighting on power distribution systems</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Iwao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasaki</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Purdue University School of Electrical Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You Are Facing the Mona Lisa: Spot Localization Using PHY Layer Information</title>
		<author>
			<persName><forename type="first">Souvik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Božidar</forename><surname>Radunovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Tolerance design of electronic circuits</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randeep</forename><surname>Singh Soin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Stmicroelectronics</surname></persName>
		</author>
		<ptr target="http://www.st.com/en/microcontrollers/stm32l433cb.html" />
		<title level="m">STM32L433CB</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Indoor Localization Without Infrastructure Using the Acoustic Background Spectrum</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">P</forename><surname>Tarzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Dinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Memik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">More Companies Are Outfitting Warehouses With Smart Lights</title>
	</analytic>
	<monogr>
		<title level="j">The Wall Street Journal</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Energy Savings Forecast of Solid-State Lighting in General Illumination Applications</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>U.S. Department of Energy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adoption of Light-Emitting Diodes in Common Lighting Applications</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<publisher>U.S. Department of Energy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Power driver topologies and control schemes for LEDs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sauerlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wendt</surname></persName>
		</author>
		<idno>IEEE APEC</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">No Need to War-drive: Unsupervised Indoor Localization</title>
		<author>
			<persName><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souvik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">UbiTouch: ubiquitous smartphone touchpads using built-in proximity and ambient light sensors</title>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename><surname>Seah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM UbiComp</title>
		<meeting>of ACM UbiComp</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Designing Uniform Illumination Systems by Surface-Tailored Lens and Configurations of LED Arrays</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J W</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Display Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SpinLight: A High Accuracy and Robust Light Positioning System for Indoor Applications</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prof. of ACM SenSys</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ArrayTrack: A Fine-grained Indoor Location System</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Jamieson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX NSDI</title>
		<meeting>of USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">IDyLL: Indoor Localization Using Inertial and Light Sensors on Smartphones</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hranilovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM UbiComp</title>
		<meeting>of ACM UbiComp</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wearables Can Afford: Light-weight Indoor Positioning with Visible Light</title>
		<author>
			<persName><forename type="first">Zhice</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Locating in Fingerprint Space: Wireless Indoor Localization with Little Human Intervention</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mobility Increases Localizability: A Survey on Wireless Indoor Localization Using Inertial Sensors</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper Session V: Location! Location! Location! MobiCom&apos;17</title>
		<meeting><address><addrLine>Snowbird, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-16">2015. 2015. October 16-20, 2017</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><surname>York</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Jain</surname></persName>
		</author>
		<ptr target="http://www.cse.wustl.edu/~jain/cse567-11/ftp/imgsens/index.html" />
		<title level="m">Fundamentals of Image Sensor Performance</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The Horus WLAN Location Determination System</title>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Extending Mobile Interaction Through Near-Field Visible Light Sensing</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">LiTell: Robust Indoor Localization Using Unmodified Light Fixtures</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Energy Efficient WiFi Display</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranveer</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys</title>
		<meeting>of ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards a Visible Light Network Architecture for Continuous Communication and Localization</title>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM VLCS</title>
		<meeting>of ACM VLCS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Use it free: Instantly knowing your phone attitude</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guobin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiCom</title>
		<meeting>of ACM MobiCom</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Enabling High-Precision Visible Light Localization in Today&apos;s Buildings</title>
		<author>
			<persName><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MobiSys. Paper Session V: Location! Location! Location! MobiCom&apos;17</title>
		<meeting>of ACM MobiSys. Paper Session V: Location! Location! Location! MobiCom&apos;17<address><addrLine>Snowbird, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-16">2017. October 16-20, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
