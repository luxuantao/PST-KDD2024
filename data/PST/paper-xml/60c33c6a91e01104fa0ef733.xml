<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omegaflow: A High-Performance Dependency-based Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaoyang</forename><surname>Zhou</surname></persName>
							<email>zhouyaoyang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Yu</surname></persName>
							<email>yuzihao@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuanqi</forename><surname>Zhang</surname></persName>
							<email>zhangchuanqi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinan</forename><surname>Xu</surname></persName>
							<email>xuyinan@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huizhe</forename><surname>Wang</surname></persName>
							<email>wanghuizhe@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sa</forename><surname>Wang</surname></persName>
							<email>wangsa@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute Of Computing Technology(Nanjing)</orgName>
								<orgName type="institution">Chinese Academy Of Sciences</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Omegaflow: A High-Performance Dependency-based Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447818.3460367</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Dependency-based architecture, Dataflow architecture</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates how to better track and deliver dependency in dependency-based cores to exploit instruction-level parallelism (ILP) as much as possible. To this end, we first propose an analytical performance model for state-of-the-art dependency-based core, Forwardflow, and figure out three vital factors affecting its upper bound of performance. Then we propose Omegaflow, a dependencybased architecture adopting three new techniques, which respond to the discovered three factors. Experimental results show that Omegaflow improves IPC by 24.6% compared to the state-of-the-art design, approaching the performance of the OoO architecture with an ideal scheduler (94.4%) without increasing the clock cycle and consumes only 8.82% more energy than Forwardflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computer systems organization → Superscalar architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Traditional high-performance architecture adopts broadcasting based issue queues and centralized physical register files to fully exploit ILP of applications. These structures provide good performance, but fall short in power efficiency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, especially in the post-Moore era. Furthermore, these structures also could affect the clock rate and require custom design which lags the tape-out flow of high-performance processors <ref type="bibr" target="#b9">[10]</ref>.</p><p>Since the 2000s, researchers and practitioners attempt many designs to overcome such limitations, such as simplified structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>, dependency-based cores <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, dataflow inspired techniques <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>, and in-order based cores <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. Among these proposed designs, Forwardflow architecture is proved to be a promising alternative due to its simplicity and energy efficiency. Forwardflow is designed with simple standard hardware, which is energy-efficiency and can scale its instruction window up for performance. However, Forwardflow pays a remarkable performance price for energy-efficiency, which prevents it from substituting the traditional OoO architecture. <ref type="foot" target="#foot_0">1</ref>In this paper, we ask that whether we already squeeze out the performance of Forwardflow. How much can we further improve the performance of Forwardflow. To answer these questions, we have to figure <ref type="figure">out: 1</ref>) what throttles down the overall performance, and 2) how to eliminate these obstacles? To better guide the performance analysis of Forwardflow architecture, we derive an analytical model to quantify the relationship between performance and various factors. With this model, we have following key findings.</p><p>The throughput and the total number of tokens (a.k.a wakeup messages) are two key factors that throttle the application performance most in Forwardflow. Tokens in Forwardflow are designed to track instruction dependencies and are transferred among distributed components of the Dataflow Queue (DQ, which is the issue engine of Forwardflow). Because tokens occupy hardware resources that need to be arbitrated by the Dataflow Queue, they are not guaranteed to be transferred and consumed as soon as they are generated. As tokens might be delayed, given a fixed number of tokens, the performance tightly relies on the rate of processing tokens. Taking namd of SPECCPU 2017 as an example, it averagely generates 2.17 tokens per instruction when running on Forwardflow with four Dataflow Queue banks, which is a large number of tokens that need to be processed. However, the overall token throughput in Forwardflow with four DQ banks is moderate, e.g., 3.31 tokens per cycle under the most optimistic estimation. Thus, our model suggests that the performance ceil of namd is 1.52, which is confirmed by the experimental results.</p><p>To eliminate such performance ceil, we must either improve the token throughput or reduce the total number of tokens. But we cannot achieve these goals by brutally increasing RAM ports or employing broadcasting, because this will incur significant overhead (Section 4.5). Fortunately, we have two important observations: 1) many tokens do not issue an instruction, 2) the total number of tokens is related to the length of serialized dependency chain in Forwardflow, which is sometimes unnecessarily long. Based on these two observations, we propose Omegaflow that consists of three new techniques to address the performance issue.</p><p>First, based on observation 1, we devise the parallel DQ bank, to largely improve the token consumption rate. The key idea of the parallel DQ bank is to differentially process tokens and simultaneously process multiple tokens that do not issue an instruction when they are accessing different locations of a DQ bank.</p><p>Second, to improve the bandwidth of token communication and adapt to the routing demand of parallel DQ bank, we adopt new interconnection networks  Omega network and 16x16 crossbar) and design corresponding routing mechanism.</p><p>Third, based on observation 2, to reduce the total number of tokens, we introduce Writeback on Completion mechanism. Writeback on Completion mechanism vastly shortens unnecessary dependency chains (Section 3.3) and reduces the total amount of tokens.</p><p>We implement Omegaflow with GEM5 <ref type="bibr" target="#b4">[5]</ref> and use SPECCPU 2017 for evaluation. Experimental results show that Omegaflow outperforms Forwardflow by 24.6% on average in terms of IPC and can achieve 94.4% of the performance of OoO architecture with an idealized scheduler. The power evaluation demonstrates that compared to Forwardflow, Omegaflow increases the energy consumption by only 8.7%. The RTL synthesis reports show that Omegaflow does not lengthen the critical path of Forwardflow significantly.</p><p>To summarize, the contributions of this paper include:</p><p>• We propose an analytical performance model for Forwardflow which reveals the mismatch between application demand (on overall token throughput) and the architecture capability (of processing tokens). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we first revisit Forwardflow to show how Forwardflow achieve out-of-order execution in a non-traditional way -Serialized Successor Representation (SSR). Then we show how Forwardflow implement the idea of SSR with Dataflow Queue. Then we list the pros and cons of Forwardflow, especially how the performance limitations arise. Finally, we show how severe these limitations lag performance, and discuss how to improve the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Forwardflow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Issue mechanism.</head><p>The following two paragraphs briefly describe the issue mechanism in Forwardflow.</p><p>The conventional OoO core broadcasts the physical register tag of the producer on the writeback stage through the issue window to wake up all its consumers. On another end of the spectrum, Forwardflow avoids broadcasting by letting the producer track the exact position of its consumers. To track dependencies between producers and consumers Forwardflow adopts a method called Serialized Successor Representation (SSR).</p><p>SSR records the position of operands of consumer instructions depending on the same register with a linked list of SSR pointers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> (dashed arrows in Figure <ref type="figure">1</ref>). When this destination registerthe head of the linked list -is computed, the computed value is propagated to all consumer instructions following the pointers in the linked list. The following is a detailed description about how the SSR linked list is constructed and used during different pipeline stages. Before explaining details, we introduce some terminologies in advance:</p><p>• Committed ARF refers to the architectural register file to store the results of instructions that have been committed.</p><p>• Pipeline stage division is shown in Figure <ref type="figure" target="#fig_0">2</ref>. It demonstrates the rename, dispatch, and issue stages of Forwardflow.</p><p>• A token is a wake-up message consisting of a pointer and a register value, which travels from a predecessor to a successor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Construction and usage of the SSR linked list.</head><p>The rename stage generates the pointers of the linked list. Considering an instruction at the rename stage, if a register operand is ready, it will obtain the register value from the committed ARF. If an operand is still busy, it will first fetch the tail of the linked list, and then generate a new pointer starting at the old tail and ending at the operand itself. In Figure <ref type="figure">1</ref>, at the rename stage, pointer I1.dest → I2.src1 and pointer I2.scr1 → I3.src1 are generated for I2 and I3, respectively.</p><p>The dispatch stage constructs the linked list with pointers generated from the last stage. At the dispatch stage, the metadata and ready registers of an instruction are filled into DQ. At the same time, the sink of the pointer is stored in the "ptr" field indicated by the source of the pointer. For example, pointer I1.dest → I2.src1 will be sent to the DQ entry of I1, and I2.src1 will be stored at field I1.dest.ptr.</p><p>The issue stage wakes up consumers through the linked list. When completing an instruction, the SSR mechanism will firstly fetch the head of the linked list, which indicates the first dependant consumer. Then a token of the computed value will be sent to the first consumer. A new token will be sent to the next successor iteratively until the tail of the linked list is reached. In Figure <ref type="figure">1</ref>, I2 will first receive the token containing the value of register t2 from I1, and then forward the register value of t2 to I3.src1.</p><p>The length of an SSR linked list will affect the performance from two aspects. The first is that SSR might directly delay instruction issue, which we name as SSR delay. The second is that the number of tokens to transfer increases as the length of an SSR linked list increases, which will indirectly affect the performance. How the number of tokens affects the performance will be discussed in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Dataflow</head><p>Queue. The SSR linked list and instructions are stored in Dataflow Queue (DQ). Dataflow Queue receives pointers generated from the rename stage to construct the linked list, and forwards tokens across different Dataflow Queue entries to issue instructions. Having described how the issue stage works with the linked list, in the following paraphrases, we will go into details about the structure of the Dataflow Queue and how Dataflow Queue follows the linked list to issue instructions.</p><p>The DQ bank is the smallest unit that can independently process tokens and issue an instruction. As previously stated, a token carries the wake-up message and the register value. On receiving a token from the Receiving Queue (Rx Queue), the Dataflow Queue bank stores the register value to the corresponding register value field and check the metadata to decide whether the instruction can issue. In the meantime, if the corresponding pointer field indicates that this is not the end of the linked list, the DQ bank will produce another token which targets the next successor. The newly produced token will be inserted into the Transmission Queue (Tx Queue). An example of such a process is given in Figure <ref type="figure" target="#fig_1">4</ref>.</p><p>Multiple DQ banks are connected with an interconnection network to form a DQ group (Figure <ref type="figure">3(a)</ref>). Instructions are interleaved across these DQ banks (Figure <ref type="figure">3</ref>(b)(c)). The interconnect network obtains tokens from the Tx Queues of DQ banks, and routing tokens to the Rx Queues according to their destination. If multiple tokens target the same bank, only one of them can be granted. Others will  be deferred in the Tx Queues. We call such delay queuing delay.</p><p>An example of such a process is given in Figure <ref type="figure" target="#fig_2">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Performance limitation</head><p>The previous subsection has shown how Forwardflow achieves out-of-order execution with Serialized Successor Representation and the Dataflow Queue. In this subsection, we will firstly discuss the advantages and limitations of such design choices. Then we try to establish a quantitative relationship between the performance loss and such design choices. Forwardflow yields better energy efficiency than the conventional OoO core for two reasons: (1) SSR tracks dependencies and avoids broadcasting wakeup messages. ( <ref type="formula">2</ref>) Dataflow Queue issues instructions with limited ports, which eliminates port-intensive physical register files.</p><p>As indicated previously, the cost of efficiency is that the SSR mechanism and the Dataflow Queue unavoidably incur extra delays compared which ideal scheduling. Previous works have figured out that delaying instruction issue might cause severe performance loss on some workloads <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. However, for Forwardflow, it remains unclear that 1) what kind of workloads tend to be affected; 2) how much these factors contribute to the performance gap between Forwardflow and the ideal scheduling.</p><p>We answer this question by establishing the relationship between the micro-architecture features of Forwardflow and workload characteristics step by step. Because every instruction must wait for all its dependant tokens before issuing, the instruction issue rate of a DQ group is determined by the rate of receiving and processing tokens. Then the time to issue all instructions depend on the rate of receiving and processing tokens and the total number of tokens:</p><formula xml:id="formula_0">𝑁 𝐶𝑦𝑐𝑙𝑒𝑠 𝑡𝑜 𝑖𝑠𝑠𝑢𝑒 = 𝑁 𝑇𝑜𝑘𝑒𝑛𝑠 /𝑇 𝑅</formula><p>where 𝑇 𝑅 indicates the rate of receiving and processing tokens. By definition, instruction committed per cycle (IPC) is less or equally than instruction issued per cycle:</p><formula xml:id="formula_1">𝐼𝑃𝐶 = 𝑁 𝐼𝑛𝑠𝑡𝑠 𝑁 𝐶𝑦𝑐𝑙𝑒𝑠 𝑡𝑜 𝑐𝑜𝑚𝑚𝑖𝑡 ≤ 𝑁 𝐼𝑛𝑠𝑡𝑠 𝑁 𝐶𝑦𝑐𝑙𝑒𝑠 𝑡𝑜 𝑖𝑠𝑠𝑢𝑒</formula><p>Put them together:</p><formula xml:id="formula_2">𝐼𝑃𝐶 ≤ 𝑁 𝐼𝑛𝑠𝑡𝑠 𝑁 𝑇𝑜𝑘𝑒𝑛𝑠 /𝑇 𝑅 = 𝑇 𝑅 𝑁 𝑇𝑜𝑘𝑒𝑛𝑠 /𝑁 𝐼𝑛𝑠𝑡𝑠 (1)</formula><p>This formula directs us to a new variable, token per instruction:</p><formula xml:id="formula_3">𝑇 𝑃𝐼 = 𝑁 𝑇𝑜𝑘𝑒𝑛𝑠 /𝑁 𝐼𝑛𝑠𝑡𝑠</formula><p>Finally, substitute TPI into Formula (1) we have</p><formula xml:id="formula_4">𝐼𝑃𝐶 ≤ 𝑇 𝑅 𝑇 𝑃𝐼 (2)</formula><p>The implication of Formula (2) answers our questions:</p><p>1) For workloads, higher 𝑇 𝑃𝐼 tends to yield lower IPC.</p><p>2) For Forwardflow, limited 𝑇 𝑅 will limit the performance.</p><p>How much is the TR in Forwardflow? For a DQ group of 4 DQ banks (F-1 configuration), we estimate the upper bound of its TR by assuming an optimistic case: TR is at most 4.0. By assuming that all Tx Queues can provide enough tokens and the destination of tokens are independently evenly distributed, we simulate the behavior of the 4x4 crossbar and obtain communication bandwidth ≈ 3.31. So 𝑇 𝑅 = min{4.0, 3.31} = 3.31.</p><p>How much is the performance of workloads affected? We collect tuples (benchmark, 𝑇 𝑃𝐼, IPC, Ideal IPC) from SPECCPU 2017 benchmarks <ref type="bibr" target="#b0">[1]</ref>. 𝑇 𝑃𝐼 and IPC are collected from Forwardflow with F-1 configuration. Ideal IPC is collected from an OoO core with an ideal scheduler. More detailed configurations of F-1 and the OoO core can be found in Section 4.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> illustrates our collected results. It shows that the ideal IPC of many workloads are far above the IPC limited by the current DQ group design in Forwardflow. For example, our performance limitation model explains 83.1% of performance loss of bwaves_0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This suggests that</head><p>The mismatch between TPI of workloads and TR of the DQ group is the most critical performance problem in Forwardflow.</p><p>To conclude, by derivation and experiments, we reveal the severity of performance loss caused by the mismatch between high TPI workloads and limited TR of the DQ group. In the next section, we try to reconcile the mismatch from both directions: increasing TR of the DQ group and reducing TPI of workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Observation and opportunity</head><p>As explained in the last subsection, SSR and Dataflow Queue are the key features that make Forwardflow energy efficient. So either completely deprecates SSR or brutally increase the ability of a DQ group to process might harm the energy efficiency of Forwardflow. To guide the architecture improvement we propose in the next section, we comprehensively analyze factors in the Forwardflow architecture that limits TR or affect TPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">What limits TR and how to increase it.</head><p>TR is decided by two factors, the token consumption rate of each DQ bank and the token transferring bandwidth of the interconnect network. While increasing the bandwidth of the network is simple, when increasing the token consumption rate of a DQ bank we must be careful.</p><p>Considering that every in-coming token might change an instruction from busy to ready to issue, Forwardflow conservatively assumes every token will issue an instruction and reserves the opportunity to issue an instruction for each in-coming token. This design choice tightly couples the token consumption rate with the issue rate of the DQ group. Because the issue rate is limited by RAM ports provided by DQ banks, which puts the token consumption rate and energy efficiency in a dilemma: Increasing the token consumption rate inevitably increases RAMs ports and then harms energy efficiency.</p><p>To further study whether such conservative assumption is necessary, we collect the number of tokens that actually demand the issue opportunity. For convenience, a token is defined to be critical if its destined instruction becomes ready to issue on its arrival. Otherwise, the token is defined to be non-critical. For example, the token I5.R1 in Figure <ref type="figure" target="#fig_1">4</ref> will cause I5 to issue. Therefore, such a token is a critical token. Conversely, token I1.R1 is a non-critical token.</p><p>We count the number of tokens from SPECCPU 2017 rate benchmarks running on Forwardflow. We found that the proportion of non-critical tokens is large, ranging from 39.4% to 68.7%. Although it is expensive to increase the issue rate, it is much cheaper to boost the consumption rate of non-critical tokens. By simple estimation, if we boost the consumption rate of non-critical tokens to 3.0 per cycle, the overall consumption rate can rise by 36%-84%.</p><p>Observation: a large proportion of tokens is non-critical. Opportunity: we can process tokens with divide-and-conquer, and boost the consumption rate of non-critical tokens. This can potentially yield a much higher consumption rate without incurring high overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">What affects TPI and how to reduce it.</head><p>Clearly, TPI is mainly determined by the program characteristic and the compilation techniques. While it is possible to reduce TPI with compiler efforts <ref type="bibr" target="#b28">[29]</ref>, this paper mainly focuses on hardware techniques. Besides the compiler, TPI is also affected by micro-architecture properties.</p><p>On the one hand, TPI will grow as the instruction window grows. According to our experimental results, TPI averagely increases 5.21% when the total instruction count of a DQ group grows from 128 to 192, and increases another 24.6% when it grows from 192 to 384. However, it is not wise to reduce TPI by reducing instruction window size.</p><p>On the other hand, as mentioned in Section 2.1, the number of tokens is positively correlated to the length of the SSR linked lists, which is affected by the rename mechanism. Forwardflow renames instructions by consulting a committed ARF. We found that although the committed ARF although contains the precious state of the architecture, it fails to deliver the latest computed results to rename stage. It is not hard to imagine that there are some instructions that have been computed but not committed. The value of such instructions can not be seen by newly renamed instructions. Figure <ref type="figure" target="#fig_10">13</ref>(a)(b) illustrates the case when I1 is completed (computed) but not committed, I4 that depends on I1's destination register (R1) cannot see that R1 is ready.</p><p>Observation: Committed ARF fails to deliver the latest register values. Opportunity: We can employ an additional ARF that allows instruction to write back to the ARF as soon as possible.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OMEGAFLOW DESIGN</head><p>Guided by two observations in Section 2.3, we propose Omegaflow, which redesigns the Dataflow Queue to improve the throughput of a DQ group (Figure <ref type="figure" target="#fig_4">7</ref>) and adds an extra Architectural Register File (ARF) to reduce the length of SSR linked list (Figure <ref type="figure" target="#fig_5">8</ref>). Briefly, Omegaflow employs three major improvements over Forwardflow: 1) Parallel DQ bank. We allow a DQ bank to consume at most 4 tokens per cycle by differentiating two types of tokens.</p><p>2) Wider Network. To adapt to the routing demand of the parallel DQ bank and improve the inter-bank communication bandwidth, we replace the 4x4 crossbar with a wider interconnect network.</p><p>3) Writeback on completion mechanism. To reduce the length of SSR linked list, we let instructions write back to the extra ARF right after being completed (computed) rather than after committed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallel DQ bank</head><p>The goal of parallel DQ bank is to improve the token consumption rate. The major challenge is how to keep the energy efficiency of Forwardflow. Because consuming a token requires read/write ports of SRAM arrays in a DQ bank, brutally increasing the consumption rate will introduce multi-ported SRAM arrays. Guided by the observation that there are a large proportion of non-critical tokens that demand much fewer resources than critical tokens, parallel DQ bank 1) discriminates different tokens and 2) lets them access pointer arrays in parallel.</p><p>Parallel DQ bank discriminates tokens by maintaining an N -Busy array, which indicates the number of tokens the corresponding Parallel DQ bank greedily assumes all in-coming tokens are non-critical tokens and let them access pointer arrays in parallel if they destine for different pointer arrays. In the meantime, the discrimination logic checks the N -Busy array. If there exists at least one critical token, the parallel DQ bank issues the corresponding instruction in the next cycle. If there are more than one critical token, parallel DQ bank only grants one of them and defer others in the Tx Queue.</p><p>To distinguish tokens that destine for different pointer arrays, parallel DQ bank split the unified Rx Queue into four Rx Queues with each connecting to one pointer array. The duty of ensuring tokens to be routed to the correct Rx Queue is offloaded to the interconnect network(Section 3.2).</p><p>We then give a detailed comparison between the original DQ bank and a parallel DQ bank in Figure <ref type="figure" target="#fig_6">9</ref>.</p><p>(1) In grey rectangles, the number of Tx Queues and Rx Queues increases from one to four, where four means there are three source operands plus one destination operand. Here we assume that every instruction has no more than three source operands. (Our design assumes RV64-G instructions set with FMA instructions <ref type="bibr" target="#b37">[38]</ref>.) (2) In the purple rectangle, each Tx Queue and Rx Queue is directly bound to the corresponding pointer array. In this way, non-critical tokens in different queues can be consumed in parallel. This also requires that all in-coming tokens are already correctly routed to corresponding Rx Queues. Such a requirement is achieved by the interconnected network, which will be discussed in Subsection 3.2.</p><p>(3) In the red rectangle, the N -Busy array records the number of busy operands for every instruction. Parallel DQ bank consults the N -Busy array to determine whether the in-coming tokens are critical or not. If multiple critical tokens are found, only one out of them can be granted. (4) In the light-blue rectangle, The granted critical token have the right to access register value arrays and metadata array. As a comparison, Forwardflow treats all in-coming tokens as critical tokens and grants them to access these arrays.</p><p>Example of parallel DQ bank. Figure <ref type="figure">10</ref> shows an example of how a parallel DQ bank handles two critical tokens (token 1: I5.R1 and token 3: I1.R3) and a non-critical token (token 2: I9.R2) simultaneously. These tokens access their pointer arrays and generate tokens to their successors in parallel ( 1 , 2 , 3 ). Three tokens access the N -Busy array, and token 1 and token 3 are found to be critical <ref type="bibr" target="#b3">( 4 )</ref>. The parallel DQ bank only grants at most one critical token per cycle, and token 1 is assumed to be granted here. After token 1 is granted, it accesses the destination pointer array and retrieves the successor of I5's destination register, I7.R3 <ref type="bibr" target="#b5">( 6 )</ref>, which aims to speculatively wake up I7.src3 in the next cycle. In parallel, it reads I5's source register value and metadata <ref type="bibr" target="#b4">( 5 )</ref>. Information from 5 and 6 is packed together and issued to Functional Units <ref type="bibr" target="#b6">( 7 )</ref>. In contrast to token 1, token 3 is rejected and has to wait in the Rx Queue. Note that the token to the successor of token 3 can be sent in this cycle, and token 3 will be marked to not generate a successor token in future cycles.</p><p>Will the logic to check N -Busy array affect the clock rate? Yes, it is possible. To avoid this issue, we add more ports for the destination pointer array and let in-coming pointers access it in parallel. According to our synthesis results in Section 4.5, this shortens the timing path. Note that because accessing the register value array and the metadata array can be pipelined to the next cycle, we do not need to optimize it. Pipelining issue stage and register read stage will not break instruction back-to-back issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inter-bank Interconnection</head><p>In this subsection, we introduce the inter-bank network in Omegaflow, as well as its design considerations. The first goal of the inter-bank network is to support routing tokens to the correct Rx Queues of a parallel DQ bank b (mentioned in Section 3.1) 2 . The second goal of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Possible solutions.</head><p>Here we assume that a DQ group has four parallel DQ banks. Since each parallel DQ bank has four Tx Queues and Rx Queues, the inter-bank interconnection network has 16 inputs and 16 outputs in total. To meet the goals above, one choice is to adopt a larger crossbar, such as a 16x16 crossbar, or an 8x8 crossbar with additional routing logic. Another choice is the multi-stage interconnection network (MIN) <ref type="bibr" target="#b38">[39]</ref>, like an Omega network. We will compare the performance and the combinational critical path of both networks in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing and Arbitration.</head><p>Because the routing and arbitration of the crossbar has been shown in Figure <ref type="figure" target="#fig_2">5</ref>, here we take the Omega network as an example. Figure <ref type="figure">11</ref>   Load balance among queues. In reality, the distribution of token destinations is not uniform. To address this issue, we shuffle the mapping from the source operands of the instruction to the source register arrays. The shuffling is performed at the rename stage and dispatch stage. Source operands are shifted according to their index in the DQ bank, as shown in Figure <ref type="figure" target="#fig_9">12</ref>. Note that the pointer and tokens can still track the correct dependencies. When the instruction is issued, the shift should be undone to restore the source operands with the correct semantic. In this way, the distribution of load among queues becomes more uniform <ref type="bibr" target="#b21">[22]</ref>. For implementation simplicity, we do not shuffle the destination operands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Writeback on Completion</head><p>The Writeback on Completion mechanism aims to shorten the SSR linked list by adding a "Completed ARF" to allow instructions to write back to this ARF right after completing execution. Its motivation has been explained in Section 2.3. Here we show how it prevents the SSR linked list from growing too long with an example. Then we talk about maintaining the Completed ARF and the original Committed ARF.</p><p>Without the Completed ARF, in Figure <ref type="figure" target="#fig_10">13</ref>(b), I4 that depends on I1's destination register (R1) cannot see that R1 is already computed. This leads to the unnecessary pointer (in purple) that starting from I3.src1 and ending at I4.src. Through the completed ARF, the upto-date state of registers can be delivered to the rename stage. In Figure <ref type="figure" target="#fig_10">13</ref> (c), when I4 is being renamed, it can obtain the value of R1 directly from the completed ARF. So in this case, completed ARF reduces the length of the SSR linked list from 3 to 2.</p><p>To maintain the Completed ARF, every time an instruction is completed, the corresponding Functional Unit sends a writeback request to the completed ARF (Figure <ref type="figure" target="#fig_5">8</ref>). A writeback request includes the register tag, the value, and the instruction's DQ ID that is similar to ROB ID in OoO. The completed ARF is updated only when this DQ ID matches with the last instruction that marks the register as busy. Ensuring ID matching is to avoid an older instruction to override the busy state of a register set by a younger instruction.</p><p>To maintain a precise architectural state, the old committed ARF cannot be removed. Every time exception or branch misprediction happens, to recover the completed ARF we simply copy from the committed ARF to it (including the scoreboard). <ref type="foot" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we firstly compare the overall performance improvements over Forwardflow, and show the performance breakdown from each technique (Section 4.2). Then we show the effectiveness  3). Then we show the effectiveness of enhancing TR of the DQ group (Section 4.4). What followed is the overhead estimation (Section 4.5), which highlighting that Omegaflow incurs very low power overhead and does not affect the timing. In the final of evaluation, we give some complementary results about scaling-up, branch prediction, and memory disambiguation. Before going to experimental results, we list our methodology and configuration in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>We implement Omegaflow and Forwardflow in GEM5 <ref type="bibr" target="#b4">[5]</ref>. The configurations of three 4-wide cores are listed in Table <ref type="table" target="#tab_5">1</ref>. The simulated clock rate is 2GHz. We use SPECCPU 2017 rate benchmarks <ref type="bibr" target="#b0">[1]</ref>(wrf is excluded because it crashes). We use Simpoint <ref type="bibr" target="#b14">[15]</ref> to select three simulation points for each benchmark, with each point of 50M instructions. We set maxK=30, and then pick simulation points from higher weights to ensure the BBV coverage of every benchmark is higher than 80%. For each simulation point, we run 50M instructions to warm up and another 50M instructions to collect statistics.</p><p>Timing. We obtain the timing report of Omegaflow and Forwardflow by implementing the major components of a DQ group in Chisel <ref type="bibr" target="#b3">[4]</ref>. We synthesize four DQ bank implementations with Synopsys Design Compiler and freePDK 15 <ref type="bibr" target="#b19">[20]</ref>.</p><p>Power and Area. Because OoO, Forwardflow, and Omegaflow share components like caches, branch predictors, and LSU, we skip these components and only evaluate the power and area of the modified part of the core. Specifically, we estimate the power and area of the ROB, physical register file (PRF) and an 80-entry scheduler in a realistic OoO core. We assumes a Circ-style scheduler when estimating its power and area <ref type="bibr" target="#b2">[3]</ref>, because the behavior of a Circstyle scheduler matches the standard CAM provided in CACTI. We estimate the power and area of the Dataflow Queue and the (committed/completed) ARF in Forwardflow and Omegaflow. We use CACTI <ref type="bibr" target="#b22">[23]</ref> to calculate the area and power of these components and collect their access statistics in GEM5 <ref type="bibr" target="#b4">[5]</ref>. We do not include the memory-related units (LSQ, StoreSet, and NoSQ), because that is not the major focus of Omegaflow. Note that NoSQ is applicable for the OoO core and traditional LSQ and StoreSet can also be ported for Forwardflow and Omegaflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">End-to-End Performance Improvements</head><p>We evaluate the performance of Omegaflow with the configuration shown in Table <ref type="table" target="#tab_5">1</ref>. In figure <ref type="figure" target="#fig_11">14</ref>(a), we compare the performance improvements of different architectures over Forwardflow. Overall, Omegaflow with the 16x16 crossbar improves the IPC by 26.3%, and the performance gap between Omegaflow and OoO with a 192entry ideal scheduler is only 4.4%. The performance improvements is the direct results of reducing TPI and enhancing TR. As reducing TPI and improving TR of the DQ group do not suffer from crossgroup delays, Omegaflow gain more improvements than scaling up Forwardflow to two groups (FF+2G). <ref type="foot" target="#foot_3">4</ref>The breakdown of the performance contribution of each part is given in Figure <ref type="figure" target="#fig_11">14(b)</ref>. Because the contribution of each technique is similar either with the 16x16 crossbar or with the Omega network, we only show the latter one here. It suggests that the largest performance gain is from increasing TR with parallel DQ bank and wider networks. The second contribution is from shortening the SSR linked list with Writeback on Completion mechanism. (Because the parallel DQ bank is coupled with wider networks, we cannot evaluate them separately.)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of shortening SSR linked list</head><p>We shorten the SSR linked list with Writeback on Completion mechanism, which improves the performance of Forwardflow by 3.0%. As described before, this will contribute to the performance from two perspectives. First, it reduces TPI by 14.7% on average (Figure <ref type="figure" target="#fig_2">15(a)</ref>). As a result of TPI reduction, the total queuing cycles of tokens decrease by 9.6% (Figure <ref type="figure" target="#fig_2">15(b)</ref>). Second, it drastically reduces the average SSR delay cycles by 53.8% (Figure <ref type="figure" target="#fig_2">15(c)</ref>). This suggests that Writeback on Completion mechanism works well with SSR linked lists.</p><p>Although Writeback on Completion mechanism contributes relatively less than other component techniques, it is important for some SSR-unfriendly workloads. Figure <ref type="figure" target="#fig_8">16</ref> shows 21.6% performance gain of Writeback on Completion mechanism on Perlbench (checkspam input set). Such performance gain is because that the token queuing cycles and the SSR delay cycles are reduced by 43.7% and 80.4%, respectively. The huge reduction of SSR delay suggests that most SSR linked lists incurred by the committed scoreboard is unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of enhancing TR</head><p>To show the effectiveness of increasing TR, we first show the distribution of token transferred in the interconnection network from two simulation points, imagick and lbm (Figure <ref type="figure" target="#fig_13">17</ref>). The flow distribution of tokens suggests that the demand on TR of workloads cannot be satisfied by Forwardflow. Parallel DQ banks and a wider network can meet the demand of workloads by providing a much higher token consumption rate and transferring bandwidth. Another observation is that although the TR demand is high, the TR of Forwardflow in most cycles is 2 or 3 but not 4. This is consistent with our calculation in Section 2.2 that the 4x4 crossbar in Forwardflow suffers from port conflicts.</p><p>Then we show the effect on token queuing delay and end-to-end performance in Figure <ref type="figure" target="#fig_5">18</ref>. This is the most powerful evidence that the limited TR provided by Forwardflow lags the overall performance. After increasing the TR, the total queuing time is reduced by 83.5% (with Omega network) and 90.3% (with 16x16 crossbar), and the end-to-end performance increases by 20.4% and 22.6%, respectively. The total queuing time with a 16x16 crossbar is about only a half of that with the Omega network, while the performance difference is 2.2%. This suggests that the demand for TR is almost satisfied and further improvements might benefit marginally. Operand shuffle contributes 0.5% to the total performance. Although the improvement is small, the technique itself is nearly free of overhead.</p><p>One of the workloads that benefit most from the TR enhancement is namd, which is predicted to be severely limited by TR in Section 2.2. Its total token queuing time is reduced by 75.2%, and end-to-end IPC jumps by 68.4% (Figure <ref type="figure" target="#fig_6">19(a)</ref>). Note that significant token queuing time reduction does not necessarily result in significant IPC gain. For example, the token queuing time cactuBSSN is reduced by 92.8%, but the CPI reduction is only 1.2%(Figure <ref type="figure" target="#fig_6">19(b)</ref>). Similar results can also be observed on mcf and xalancbmk. These benchmarks are either control-intensive or memory-intensive and have low Ideal IPC. Limited performance gain from these benchmarks can also be explained by our model: The performance of low IPC applications is not significantly affected by TR limitation even if they have high TPI, such as bwaves_2 in Figure <ref type="figure" target="#fig_3">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Power, Area, and Timing</head><p>Here give estimated power, area, and timing of Omegaflow and other baseline architectures, highlighting that the performance gain of Omegaflow will not incur significant overhead to Forwardflow. If not mentioned in this subsection, we refer to Omegaflow with Omega network as Omegaflow.</p><p>Table <ref type="table" target="#tab_6">2</ref> shows the average energy consumption and normalized IPC (normalized to OoO core with an 192-entry ideal Scheduler) when executing 200M instructions on SPECCPU 2017 workloads. Compared with Forwardflow, Omegaflow increases the energy cost by 8.7% on average and increases IPC by 24.6%. The performance gain makes the energy overhead cost worthy.</p><p>Before comparing with OoO cores, we firstly introduce the compared schedulers. Shift scheduler strictly maintains instruction ages with expensive structures, which is impractical to implement for a 80-entry scheduler <ref type="bibr" target="#b2">[3]</ref>. Circ scheduler does not strictly maintain age relationship between instructions and can be implemented with CAMs <ref type="bibr" target="#b2">[3]</ref>. Compared with the OoO core with an 80-entry Shift scheduler, Omegaflow achieves 94.8% of its performance. Compared with the OoO core with an 80-entry Circ scheduler, Omegaflow reaches 99.5% of its performance and saves 8.3% energy.</p><p>Table <ref type="table" target="#tab_8">4</ref> lists the estimated area cost of different architectures with CACTI <ref type="bibr" target="#b22">[23]</ref>. The area overhead of Omegaflow is moderate (21.4%) for the performance gain (24.8%). If we brutally employ multi-ported RAM for the DQ bank, the area overhead will be 44.9%. This suggests the importance of leveraging non-critical tokens, which enable us to enhance the DQ bank with much less overhead.  <ref type="table" target="#tab_7">3</ref> shows the timing results of DC synthesis, when setting the clock period to 160 ps. <ref type="foot" target="#foot_4">5</ref> These results show that compared with Forwardflow, Omegaflow with will not hurt the timing. However, if we do not apply the optimization mentioned at the end of Section 3.1, the critical path might be affected. <ref type="foot" target="#foot_5">6</ref> It also suggests that 16x16 crossbar might slightly affect the timing. Thus, which interconnection networks to choose depends on the desired clock rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Complementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Scaling-up comparison.</head><p>Although scaling-up is not the main focus of this paper, we provide related results here as complementary. The performance (IPC) of OF+2G is 21.1% better than FF+2G, and is 3.5% better than OF. The benefit of scaling-up of Omegaflow is less than Forwardflow (8.9%), which is related to the higher token consumption rate of Omegaflow. Because scaling-up can provide more DQ banks for Forwardflow to catch up with the demand on TR and relieve the performance loss. Experimental results show that FF+2G reduces 0.43 queuing cycle per instruction. But for Omegaflow, the parallel DQ bank is sufficient to handle the tokens and can not significantly benefit from such a way (The queuing cycle reduction of OF+2G on OF is only 0.067 per instruction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Impact of speculation techniques.</head><p>Because speculation methods might affect the performance gaps between architectures, in rest two subsection, we compares different speculation methods to show that the performance gap between OoO with ideal scheduler and Omegaflow will not significantly vary.</p><p>For branch predictor, we substitute TAGE-SC-L with an oracle branch predictor. With an oracle branch predictor, the average performance gap between Omegaflow and the OoO core with the ideal OoO scheduler is 4.7%. (In comparison, the performance gap is 4.4% with TAGE-SC-L). This result suggests the performance gap will not grow significantly as the branch predictors continue to advance. Because Omegaflow and the ideal OoO scheduler has the same size of instruction window, a stronger branch predictor will prefer none of them.</p><p>For memory speculation, we compare the performance of Omegaflow with NoSQ against Omegaflow with a traditional LSQ and store set <ref type="bibr" target="#b10">[11]</ref>. The average performance improvement is only 0.6%. Similar performance result on the OoO core is reported on the original paper of NoSQ <ref type="bibr" target="#b32">[33]</ref>. Considering that the performance difference is limited, we do not implement NoSQ for the OoO core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK AND DISCUSSION</head><p>Tracking dependencies. Many previous works observed that most instructions have few consumers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. To handle rest minor instructions that have multiple consumers, some works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref> adopted a hybrid wakeup fashion to handle these instructions. <ref type="bibr" target="#b28">[29]</ref> proposed Exposed Operand Broadcasting (EOB) to handle these cases with software efforts. Writeback on Completion can be a complement to the above mechanisms.</p><p>Dataflow architectures. At the very beginning, dataflow architectures adopt new instruction format to express dataflow dependencies of programs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>. To gain benefit on typical RISC instruction sets, other genres of architectures are developed. One genre incorporates dataflow engines into RISC-based in-order or out-of-order core, uses compilers to find most frequent instruction chunks, and lets the the dataflow engine to execute these chunks to obtain high energy efficiency <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. As another genre, Forwardflow and Omegaflow are trying to dynamically generate dataflow graphs with pure hardware effort.</p><p>Recent OoO architectures. Since 2010, there are also other outof-order techniques. <ref type="bibr" target="#b34">[35]</ref> improves the energy efficiency by incorporating in-order cores. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> focus on exploiting memory level parallelism with simple baseline cores. <ref type="bibr" target="#b2">[3]</ref> reconciles the contradiction between aging and size of the out-of-order scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Previous works that study similar architectures mainly focus on fixing the serialized wakeup issue of SSR and boosting performance by composing multiple execution units (scaling-up) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. This paper opens another way that enhancing performance by squeezing the component execution units.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Forwardflow pipeline stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DQ bank and token consumption. When token I5.R1 (i.e., the first source register of I5) arrives at the Receiving Queue (Rx Queue) of DQ bank 1, the DQ bank will read the I5.R2 (immediate value 2) and the metadata (add) and issue I5 to ALU. In the meantime, the DQ bank will read the pointers at I5.dest (which is null) and I5.R1 (which points to I6.R1). A new token (I6.R1) is generated and inserted into the Transmission Queue (Tx Queue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of bank conflict and token queuing. Conflict occurs between T2 and T3 because they have the same destination. However, T1 and T4 do not conflict because T4 arises from bank 4 locally. Hence T4 is not sent through the crossbar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: IPC limitation in Forwardflow. The solid curve and dashed curve are plotted with Equation 2 when 𝑇 𝑅 is equal to 3.31 and 7.0, respectively. Down triangles and up triangles represent the IPC of a program achieved in OoO core and Forwardflow, respectively. Plotted regions are randomly chosen from SimPoint simulation points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Omegaflow improves the token throughput of the Dataflow Queue by replacing the original DQ bank with the parallel DQ bank and replacing the crossbar with the Omega network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Omegaflow reduces TPI and SSR delay with Writeback on Completion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Parallel DQ bank VS. original DQ bank instruction depends on. If a token arrives when the corresponding N -Busy entry is equal to 1, it is a critical token. Otherwise, it is non-critical.Parallel DQ bank greedily assumes all in-coming tokens are non-critical tokens and let them access pointer arrays in parallel if they destine for different pointer arrays. In the meantime, the discrimination logic checks the N -Busy array. If there exists at least one critical token, the parallel DQ bank issues the corresponding instruction in the next cycle. If there are more than one critical token, parallel DQ bank only grants one of them and defer others in the Tx Queue.To distinguish tokens that destine for different pointer arrays, parallel DQ bank split the unified Rx Queue into four Rx Queues with each connecting to one pointer array. The duty of ensuring tokens to be routed to the correct Rx Queue is offloaded to the interconnect network(Section 3.2).We then give a detailed comparison between the original DQ bank and a parallel DQ bank in Figure9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Example of parallel DQ bank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>shows a 16 -</head><label>16</label><figDesc><ref type="bibr" target="#b15">16</ref> Omega network and demonstrates how tokens are routed to their destinations. Routing: Each cell switch in the Omega network forwards the token according to the binary representation of the destination. For example, the destination of Token 1, B1.I3.2, is the second source operand array of bank 1. Such a destination can be represented with bits 0110, where 01 means bank 1, and 10 means the second source register array. According to this representation, four switches during the routing will forward the token to the corresponding port. Arbitration.Figure 11 also  shows that Token B1.I3.2 conflicts with Token B1.I1.3 at switch Row2.Column3. The switch adopts a round-robin arbiter. For this case, Token B1.I1.3 has to be deferred and wait in the Tx Queue.2 The crossbar in Forwardflow only cares about the destination bank of a token</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Load balance among queues by shuffling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Effect of Writeback on Completion of shortening the SSR chain (Section 4.3). Then we show the effectiveness of enhancing TR of the DQ group (Section 4.4). What followed is the overhead estimation (Section 4.5), which highlighting that Omegaflow incurs very low power overhead and does not affect the timing. In the final of evaluation, we give some complementary results about scaling-up, branch prediction, and memory disambiguation. Before going to experimental results, we list our methodology and configuration in the following subsection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: End-to-end performance. The baseline is Forwardflow. We mark Forwardflow with two DQ groups as FF+2G, mark OoO core with an ideal scheduler as Ideal, and mark Omegaflow with the Omega network and with the 16x16 crossbar as OF+Omega and OF+Xbar16, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: Average benefits from Writeback on Completion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: The distribution of tokens processed per cycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :Figure 19 :</head><label>1819</label><figDesc>Figure 18: Average benefits from Parallel DQ bank and wider interconnection networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Figure 3: Forwardflow Hierarchical Layout</head><label></label><figDesc></figDesc><table><row><cell cols="6">(a) Physical Layout of DQ of four groups, each group with four DQ banks</cell><cell></cell><cell cols="2">(b) Four consecutive instructions</cell></row><row><cell></cell><cell>DQ Group 0</cell><cell></cell><cell></cell><cell cols="2">DQ Group 3</cell><cell></cell><cell></cell></row><row><cell>Bank</cell><cell></cell><cell>Bank</cell><cell>Bank</cell><cell></cell><cell></cell><cell>Bank</cell><cell></cell></row><row><cell></cell><cell>Xbar</cell><cell></cell><cell></cell><cell></cell><cell>Xbar</cell><cell></cell><cell></cell></row><row><cell>Bank</cell><cell></cell><cell>Bank</cell><cell>Bank</cell><cell></cell><cell></cell><cell>Bank</cell><cell cols="2">(c) Instructions Interleaved</cell></row><row><cell>Bank</cell><cell></cell><cell>Bank</cell><cell>Bank</cell><cell></cell><cell></cell><cell>Bank</cell><cell cols="2">in four banks</cell></row><row><cell></cell><cell>Xbar</cell><cell></cell><cell></cell><cell></cell><cell>Xbar</cell><cell></cell><cell>Bank I1</cell><cell>Bank I2</cell></row><row><cell>Bank</cell><cell>DQ Group 1</cell><cell>Bank</cell><cell>Bank</cell><cell cols="2">DQ Group 2</cell><cell>Bank</cell><cell>Bank I4</cell><cell>Bank I3</cell></row><row><cell></cell><cell></cell><cell>I5.R1 I1.R1</cell><cell cols="2">t2 t1</cell><cell></cell><cell>Rx queue</cell><cell cols="2">DQ Bank 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">dest</cell><cell cols="2">src1</cell><cell>src2</cell></row><row><cell></cell><cell></cell><cell>meta</cell><cell cols="2">value</cell><cell>ptr</cell><cell>value</cell><cell>ptr</cell><cell>value</cell><cell>ptr</cell></row><row><cell></cell><cell>I1</cell><cell>sub</cell><cell cols="2">s1'</cell><cell>I3.R2</cell><cell>Busy</cell><cell>I4.R2</cell><cell>Busy</cell><cell>I6.R2</cell></row><row><cell></cell><cell>I5</cell><cell>add</cell><cell></cell><cell></cell><cell>null</cell><cell>Busy</cell><cell>I6.R1</cell><cell>2</cell><cell>null</cell></row><row><cell></cell><cell></cell><cell cols="2">Issue I5 Issue I5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">I6.R1 I6.R1</cell></row><row><cell></cell><cell>I5 I5</cell><cell cols="3">add (send it to ALU) t2 (send it to ALU) add t2</cell><cell>2 2</cell><cell cols="2">Send token to I6 via Tx queue Send token to I6 via Tx queue</cell><cell>Tx queue t2 t2 Tx queue</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 : Experiment Setup</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Component</cell><cell></cell><cell></cell><cell></cell><cell cols="2">OoO Core</cell><cell>Omegaflow</cell><cell>Forwardflow</cell></row><row><cell></cell><cell cols="5">Window Type and Size</cell><cell cols="3">192-entry ROB</cell><cell>192-entry Dataflow Queue</cell></row><row><cell></cell><cell cols="5">Scheduler Type and Size</cell><cell cols="3">192-entry idealized CAM</cell><cell>with 4 banks</cell></row><row><cell></cell><cell cols="3">Load Store Queue</cell><cell></cell><cell></cell><cell cols="3">72-entry load queue, 48-entry store queue NoSQ [33]</cell></row><row><cell></cell><cell cols="3">Functional Units</cell><cell></cell><cell></cell><cell cols="3">4xI-ALU(+AGU), 2xI-MultDivUnit, 2xF-Adder, 2xF-MultDivUnit</cell></row><row><cell></cell><cell>DQ bank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>Parallel DQ bank</cell><cell>Original</cell></row><row><cell></cell><cell cols="6">Interconnection Network -</cell><cell></cell><cell>16-16 Omega network 4x4 Crossbar</cell></row><row><cell></cell><cell cols="6">Writeback on Completion -</cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell cols="2">Selection Style</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Idealized Stacked Selection [26]</cell><cell>Grouped into four FU groups</cell></row><row><cell></cell><cell cols="3">Branch Prediction</cell><cell></cell><cell></cell><cell cols="3">64KB TAGE-SC-L Branch predictor [32]</cell></row><row><cell></cell><cell cols="5">L1-I Cache / L1-D Cache</cell><cell cols="3">32KB, 8-way set associative, 2-cycle latency</cell></row><row><cell></cell><cell>L2 Cache</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">2MB, 8-way set associative, 24-cycle latency, with BOP Prefetcher [21]</cell></row><row><cell></cell><cell>L3 Cache</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">16MB, 16-way set associative, 40-cycle latency</cell></row><row><cell></cell><cell cols="2">Main Memory</cell><cell></cell><cell></cell><cell></cell><cell cols="3">DDR3 1600 8x8</cell></row><row><cell cols="2">Initial state:</cell><cell></cell><cell>I1 I3 I2</cell><cell>ꓕ ꓕ ꓕ</cell><cell cols="2">P P src1 src2 P P</cell><cell></cell></row><row><cell cols="2">Next cycle:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Completed</cell><cell cols="2">Committed</cell><cell cols="2">R1 Busy</cell><cell cols="2">Completed</cell><cell>R1 Ready</cell></row><row><cell></cell><cell>Scoreboard/ARF</cell><cell cols="2">Scoreboard</cell><cell cols="2">R5 Ready</cell><cell cols="2">Scoreboard</cell><cell>R5 Ready</cell></row><row><cell></cell><cell>Instruction</cell><cell></cell><cell></cell><cell cols="2">Pointer</cell><cell></cell><cell cols="2">Instruction</cell></row><row><cell></cell><cell cols="2">Rename</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I4</cell><cell>R7=R1&amp;R5</cell><cell></cell><cell cols="3">I3.src1 I4.src1</cell><cell cols="3">I4 R7=R1&amp;R5</cell></row><row><cell></cell><cell></cell><cell cols="3">dest src1 src2</cell><cell></cell><cell></cell><cell cols="2">dest src1 src2</cell></row><row><cell></cell><cell>I1 I2 I3 I4</cell><cell>ꓕ ꓕ ꓕ</cell><cell>P P</cell><cell>P P P</cell><cell></cell><cell>I1 I2 I3 I4</cell><cell>ꓕ ꓕ ꓕ</cell><cell>P P P</cell><cell>P P P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 : Average energy consumption and performance of four configurations.</head><label>2</label><figDesc></figDesc><table><row><cell>OoO with</cell><cell>OoO with</cell><cell></cell><cell></cell></row><row><cell>80-entry</cell><cell>80-entry</cell><cell>FF</cell><cell>OF</cell></row><row><cell>Shift sched</cell><cell>Circ sched</cell><cell></cell><cell></cell></row><row><cell>Energy/mJ -</cell><cell>2.632</cell><cell cols="2">2.220 2.416</cell></row><row><cell>Normalized 0.997</cell><cell>0.949</cell><cell cols="2">0.757 0.944</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 : The register-to-register delay of four different DQ implementation.</head><label>3</label><figDesc></figDesc><table><row><cell>Forwardflow DQ</cell><cell>Parallel DQ (without optimization)</cell><cell>Parallel DQ</cell><cell>Parallel DQ with Xbar16</cell></row><row><cell>slack/ps 0.09</cell><cell>-18.59</cell><cell>0.0</cell><cell>-6.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 : Area of compared architectures</head><label>4</label><figDesc></figDesc><table><row><cell>Forwardflow (ARF, DQ)</cell><cell>Forwardflow (ARF, multi-ported DQ)</cell><cell>Omegaflow (2*ARF, parallel DQ)</cell><cell>OoO (Circ Sched, PRF, ROB)</cell></row><row><cell>area/𝑚𝑚 2 0.0813</cell><cell>0.1178</cell><cell>0.0987</cell><cell>0.1383</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><ref type="bibr" target="#b11">[12]</ref> compares Forwardflow against a baseline OoO core with 32-entry SLIQ<ref type="bibr" target="#b36">[37]</ref>. Our baseline is an OoO core with an idealized unified issue window which always guarantees age-order and back-to-back wakeup.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">ICS '21, June 14-17, 2021, Virtual Event, USA Zhou et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The rename map is recovered by walking through the Dataflow Queue, which is similar among OoO, Forwardflow, and Omegaflow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The average performance gain of scaling up from FF to FF+2G is 8.4% in our implementation. And it is 11.5% on average in the dissertation paper of Forwardflow. The slight difference might be caused by workload differences and implementation details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"> FreePDK 15  can only be used to compare timing of different designs, but cannot be used to estimate real clock rate.<ref type="bibr" target="#b19">[20]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Either optimized or not, both DQ implementations behave the same in terms of IPC.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>We would like to thank anonymous reviewers for their valuable feedbacks and suggestions. We thank our group members for their help on this work. Especially thank Fanjin Zhang for correcting many grammatical errors. This work was supported in part by Key-Area Research and Development Program of Guangdong Province (NO.2020B010164003), the National Natural Science Foundation of China (Grant No. 62090020), Youth Innovation Promotion Association of Chinese Academy of Sciences (2013073, 2020105), and the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDC05030200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.spec.org/cpu2017" />
		<title level="m">SPEC CPU2017</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clock rate versus IPC: The end of the road for conventional microarchitectures</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international symposium on Computer architecture</title>
				<meeting>the 27th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue</title>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="506" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chisel: constructing hardware in a scala embedded language</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimas</forename><surname>Avižienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC Design Automation Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The gem5 simulator</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling to the End of Silicon with EDGE Architectures</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lizy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Charles R Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Burrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Yoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A low-complexity issue logic</title>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Canal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on Supercomputing</title>
				<meeting>the 14th international conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The load slice core microarchitecture</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Allam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The load slice core microarchitecture</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Allam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Broom: an open-source out-of-order processor with resilient low-voltage operation in 28-nm cmos</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pi-Feng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borivoje</forename><surname>Nikolić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory dependence prediction using store sets</title>
		<author>
			<persName><forename type="first">George Z Chrysos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="142" to="153" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forwardflow: a scalable core for powerconstrained CMPs</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dyser: Unifying functionality and parallelism specialization for energy-efficient computing</title>
		<author>
			<persName><forename type="first">Venkatraman</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Han</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jatin</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bundled execution of recurring traces for energy-efficient general purpose processing</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-08">August. 2011</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simpoint 3.0: Faster and more flexible program phase analysis</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Energy-efficient hybrid wakeup logic</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Renau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on low power electronics and design</title>
				<meeting>the international symposium on low power electronics and design</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Freeway: Maximizing MLP for Slice-Out-of-Order Execution</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Needle: Leveraging program analysis to analyze and extract accelerators from whole programs</title>
		<author>
			<persName><forename type="first">Snehasish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Margerm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrvindh</forename><surname>Shriraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="565" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynaspam: dynamic spatial architecture mapping using out of order instruction schedules</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heejin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewook</forename><surname>Stephen R Beard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>August</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="541" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open cell library in 15nm FreePDK technology</title>
		<author>
			<persName><forename type="first">Mayler</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jody</forename><surname>Maick Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><forename type="middle">P</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Schlinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucio</forename><surname>Rech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Michelsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Symposium on International Symposium on Physical Design</title>
				<meeting>the 2015 Symposium on International Symposium on Physical Design</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Randomized parallel communications on an extension of the omega network</title>
		<author>
			<persName><forename type="first">Debasis</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><forename type="middle">A</forename><surname>Cieslak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="802" to="824" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CACTI 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stream-dataflow acceleration</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Gangadhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newsha</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="416" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the potential of heterogeneous von neumann/dataflow execution models</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Gangadhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
				<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="298" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Complexityeffective superscalar processors</title>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A scalable instruction queue design using dependence chains</title>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">L</forename><surname>Steven E Raasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 29th Annual International Symposium on Computer Architecture</title>
				<meeting>29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Direct instruction wakeup for out-of-order processors</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Marco A Ramírez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Veidenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovative Architecture for Future Generation High-Performance Processors and Systems (IWIA&apos;04)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How to implement effective prediction and forwarding for fusable dynamic multicore architectures</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Behnam Robatmili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibi</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="460" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting ILP, TLP, and DLP with the polymorphous TRIPS architecture</title>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramadass</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="422" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matrix scheduler reloaded</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Peter G Sassone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tage-sc-l branch predictors again</title>
		<author>
			<persName><forename type="first">André</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Championship Branch Prediction (CBP-5)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nosq: Store-load communication without a store queue</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milo</forename><forename type="middle">Mk</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chainsaw: Von-neumann accelerators to leverage fused instruction chains</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Sharifian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snehasish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apala</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrvindh</forename><surname>Shriraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Morphcore: An energy-efficient microarchitecture for high performance ilp and high throughput tlp</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Aater Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Schwerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
		<title level="m">Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 36th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A scalable low power issue queue for large instruction window processors</title>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Vivekanandham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharadwaj</forename><surname>Amrutur</surname></persName>
		</author>
		<author>
			<persName><surname>Govindarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual international conference on Supercomputing</title>
				<meeting>the 20th annual international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Krste Asanovic, and Volume I User level Isa. 2014. The RISC-V instruction set manual</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>User-Level ISA</publisher>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On a class of multistage interconnection networks</title>
		<author>
			<persName><forename type="first">Chuan-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tse-Yun</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="694" to="702" />
			<date type="published" when="1980">1980. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
