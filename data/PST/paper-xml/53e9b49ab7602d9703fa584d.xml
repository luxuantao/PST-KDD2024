<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video based technology for ambient assisted living: A review of the literature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Cardinaux</surname></persName>
							<email>fabien.cardinaux@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Health and Related Research</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deepayan</forename><surname>Bhowmik</surname></persName>
							<email>d.bhowmik@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charith</forename><surname>Abhayaratne</surname></persName>
							<email>c.abhayaratne@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Hawley</surname></persName>
							<email>mark.hawley@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Health and Related Research</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video based technology for ambient assisted living: A review of the literature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9F4258228ADCF751868C0C4B046F767</idno>
					<idno type="DOI">10.3233/AIS-2011-0110</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Assisted living</term>
					<term>computer vision</term>
					<term>video monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ambient assisted living (AAL) has the ambitious goal of improving the quality of life and maintaining independence of older and vulnerable people through the use of technology. Most of the western world will see a very large increase in the number of older people within the next 50 years with limited resources to care for them. AAL is seen as a promising alternative to the current care models and consequently has attracted lots of attention. Recently, a number of researchers have developed solutions based on video cameras and computer vision systems with promising results. However, for the domain to reach maturity, several challenges need to be faced, including the development of systems that are robust in the real-world and are accepted by users, carers and society. In this literature review paper we present a comprehensive survey of the scope of the domain, the existing technical solutions and the challenges to be faced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>According to the European Union (EU) commission's projection, the number of older people will increase threefold between 2008 and 2060. This envisages great challenges towards care for older people with limited available resources. On the other hand, research suggests that as many as 35% of older people who live in a care home could be supported to live at home or in extra care housing schemes through the use of technology <ref type="bibr" target="#b45">[46]</ref>. As an alternative to current care models, enriched assistive living based solutions can offer the possibility for many older and vulnerable people to live independently at home.</p><p>No universal definition of ambient assisted living (AAL) has been adopted, but it can be described as information and communication technology based products, services and systems to provide older and vulnerable people with a secure environment, improve their quality of life and reduce the costs of health and social care. According to the technical road map <ref type="bibr" target="#b0">[1]</ref>, outlined by the EU commission, the stakeholders' needs are categorised as: 1) AAL for persons (home and mobile), 2) AAL in the community and 3) AAL at work. However within the scope of this work we restrict our discussion to AAL for personal living in the home. State-of-theart AAL technologies include the use of home embedded wireless sensor networks and body worn sensors. More recently, the research community, public institutions and industry have shown a growing interest in video and computer vision based solutions for AAL (VAAL) applications. The technology is rapidly reaching a stage of maturity with the first products being commercialised <ref type="bibr" target="#b52">[53]</ref>.</p><p>One of the main aims of AAL based systems is to provide personal safety to older people. Falling is a major risk faced by older people with one-third of people over 65 falling each year. Family carers are also impacted by an increased burden of care, anxiety and costs of residential care. The associated cost of falls is almost Â£1bn annually for the UK alone. With such se-1876-1364/11/$27.50 c 2011 -IOS Press and the authors. All rights reserved vere consequences for the individual and society, it is not surprising that the vast majority of VAAL research focuses on fall detection.</p><p>Due to the multi-disciplinary nature of the topic, research reports tend to be scattered across conferences and journals specialised in diverse domains ranging from health and telecare to engineering and computer vision. As a consequence, the scientific community might experience difficulties in following recent advances in the domain. This survey aims to summarise the state-of-the-art of video technology currently used in AAL, to explore the challenges to be faced and to investigate potential future developments in the domain.</p><p>The rest of this paper is organised as follows. An overview of the application domains and system requirements of VAAL is given in Section 2. Section 3 covers current computer vision approaches and Section 4 discusses issues related to acceptability and protection of privacy. Methodologies for performance evaluation are discussed in Section 5. Finally, conclusions and future areas of research are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Applications and requirements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Application domains and scope</head><p>This section presents a list of AAL areas <ref type="bibr" target="#b0">[1]</ref> along with potential applications for video technology. Note that this survey is concerned only with the use of automatic video analysis, therefore, other applications of video cameras such as tele-communication (videoconferencing) or surveillance of video by an operator are not included in this survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Health, rehabilitation and care</head><p>People with long-term health conditions often experience a reduced quality of life and dependence upon health and social care services. Video technology can be used in this context to recognise and monitor physiological data, as recently by proposed by Poh et al. <ref type="bibr" target="#b47">[48]</ref> who presented a methodology for a contactless measurement of cardiac pulse rate using a video camera.</p><p>Physiological data can, however, not always be measured from embedded sensors and a popular alternative solution is to monitor users' activities with the premise that change in activities of daily living is a good indicator of declining health. Video technology can be used in this context to recognise users' activities. For example Mihailidis et al. <ref type="bibr" target="#b37">[38]</ref> propose a health monitoring scheme which tracks the location and motion of the user at home, builds probabilistic models of patterns of activity and then detects deviations in these patterns. With a similar assumption that activities are a good indicator of health, Nait-Charif and McKenna <ref type="bibr" target="#b40">[41]</ref> introduced a method to summarise activity by tracking the user's position using an overhead camera.</p><p>Diet is also an important factor affecting health and Kim et al. <ref type="bibr" target="#b27">[28]</ref> proposed a design concept of an imagebased dietary assessment tool implemented in a mobile phone. Food images are recorded using a built-in camera on a mobile phone and sent to a server where the portion size of the meal is estimated. This information is subsequently used to keep personal dietary records as a means of monitoring energy and nutrient intakes.</p><p>The information related to activity, physiological data, diet, etc. can be subsequently fed back to users to promote health behaviour changes or sent to carers. As suggested in <ref type="bibr" target="#b41">[42]</ref>, these technologies can be part of a mechanism to support self-management of people suffering from chronic problems such as stroke and provide them with a better insight into how and how often they may perform their activity and rehabilitation exercises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Personal and home safety and security</head><p>Older and vulnerable people are particularly exposed to safety and security threats. Feeling safe and secure is thus essential to extend autonomous daily living. Video cameras can be used, for example to automatically detect intrusion by a burglar <ref type="bibr" target="#b18">[19]</ref>. Such security applications have been the subject of a number of research papers in the domain of automatic video surveillance and will not be discussed in detail in this survey. For a recent overview of automatic video surveillance, see <ref type="bibr" target="#b29">[30]</ref>.</p><p>Currently fall detection is a well researched topic and sensor based solutions are available and commercialised. The majority of such solutions are based on accelerometers, but one of the drawbacks of these systems are that users always need to wear the sensor. The system will not work if the user forgets to wear the device. Alternative solutions are suggested, such as passive fall detection which uses floor vibration sensors, sound <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b71">72]</ref> or video based monitoring <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>. Video based monitoring algorithms are discussed in Section 3.</p><p>People with dementia are particularly vulnerable with most exhibiting behavioural disturbances. Longterm dementia care facilities face the risk of the patients eloping and putting themselves at risk. In <ref type="bibr" target="#b7">[8]</ref>, Chen et al. propose to improve safety of patients in a dementia unit with a video elopement detector. The system is based on multiple cameras covering the exit of the unit and demonstrates that it can accurately differentiate elopements from routine exit of the building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Biorobotic systems</head><p>Biorobotic systems have been considered for a long time to improve the quality of life of older and vulnerable people. In the last few years, a growing number of research studies have investigated the development of companion robots for the care of older people. These robots can provide physical, cognitive and affective support to the users. A systematic literature review of the effectiveness of assistive robots has been conducted by Broekens et al. <ref type="bibr" target="#b4">[5]</ref> in 2009 which shows the growing importance and promising preliminary results of assistive robotics. Broekens et al. differentiate two types of functionalities achieved by assistive robots: (1) Robots which support independent living by supporting basic activities and mobility and (2) Robots which aim to enhance health and wellbeing of elderly users by providing companionship. For both functionalities, robots must have a good understanding of the world around them and video cameras along with other sensors associated with appropriate processing can play this role. Mataric et al. <ref type="bibr" target="#b36">[37]</ref> are developing a socially assistive robot equipped with a video camera that makes communicative actions such as looking at or away from the user. Other robots equipped with a camera include the Aibo <ref type="bibr" target="#b19">[20]</ref> developed and produced by Sony.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video based vs sensor based solutions</head><p>Applications of AAL described in the previous section are typically based on solutions employing sen-sors either embedded in the environment or body worn <ref type="bibr" target="#b46">[47]</ref>. The most widely used sensors for AAL are reported in Table <ref type="table" target="#tab_0">1</ref> along with their domains of application. To reach the level of analysis required for AAL applications (e.g. recognising activity), a large network of embedded sensors is generally required, therefore systems are usually costly to maintain, are relatively obtrusive (e.g. sensors set up on every cupboard door) and are highly sensitive to the performance of the sensors. Another approach is to use body worn sensors which are usually quite effective when the user actually wears them, however, it is recognised that user compliance with wearable systems is poor.</p><p>Recently a number of researchers and companies have been looking at developing solutions based on video cameras and computer vision approaches. A monitoring system based on video cameras has potential advantages. In principle a single camera in a room could pick up most of the activities performed in the room and, consequently, could replace a large number of sensors.</p><p>Remarkably, the last five years have shown a large interest in video based solutions. One of the major reasons for this gain of interest is the cost of video cameras. The price of video cameras has drastically reduced within the last decade and good quality video cameras are now available at very low cost. Another important factor in favour of video technology is the maturity of computer vision technology. Latest research in computer vision, motivated by needs in domains such as security and surveillance, provides new means of interpreting rich information provided by video cameras.</p><p>On the other hand, video based AAL has several obstacles to overcome to reach full maturity. The barriers include users' acceptability and risk of loss of privacy. These are discussed in detail in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-modal approaches</head><p>Although a growing popularity has been noted towards video based monitoring in AAL, other sensor based solutions are currently being used in many real life projects. At the same time video based sensors can not replace many traditional sensors. More recently many multi-modal approaches using a network of sensor arrays have been adopted and used. For example, GER'HOME <ref type="bibr" target="#b48">[49]</ref> uses a multi-sensor based analysis for everyday elderly activity monitoring; audiovideo based solutions are proposed in the HERMES project <ref type="bibr" target="#b49">[50]</ref> to facilitate AAL applications for older people using technologies such as: speech recognition, speaker identification, face detection, person tracking and face identification. The UbiMon project <ref type="bibr" target="#b43">[44]</ref> proposed a framework to collect information from wearable and implantable sensors to a local processing unit such as: PDA or smart phone through a body network, before transmitting to a remote database. Toreyin et al. <ref type="bibr" target="#b61">[62]</ref> proposed to use a similar algorithm based on hidden Markov models to detect fall from video <ref type="bibr" target="#b61">[62]</ref>, audio, passive infra-red and vibration sensors <ref type="bibr" target="#b62">[63]</ref>. A list of sensors used in these multi-modal approaches is shown in Table <ref type="table" target="#tab_1">2</ref>. In conclusion, additional sensors, such as, audio sensors can offer complimentary feature parameters and robustness to existing vision based behaviour monitoring systems while bio-medical sensors provide different types of information to the complete AAL system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">General structure of video based AAL</head><p>A video based monitoring typically has the following system blocks as shown in Fig. <ref type="figure" target="#fig_1">1a</ref>):</p><p>1. video data capture, 2. a local video multiplexer and transmitter which collects video data from different camera hardware (wired or wireless connection) and transmits to a central server, 3. a central server which collects and stores data from many local transmitters at remote locations, 4. a video processing system block which analyses video data for action or activity recognition and offers output alerts or decisions, 5. operator to monitor the video data before or after processing.</p><p>Such a centralised camera network often requires high bandwidth for video data transmission and an alternative solution is suggested in <ref type="bibr" target="#b16">[17]</ref> that uses a smart and distributed camera network which processes the video data locally and sends the alert or decision output to the central server as shown in Fig. <ref type="figure" target="#fig_1">1b</ref>). Distributed smart camera solutions also have advantages in terms of confidentiality (see Section 4). In another approach addressing the bandwidth issue in centralised camera networks, compressed video transmission is suggested in <ref type="bibr" target="#b32">[33]</ref>.</p><p>During the video data capture, traditionally a network of fixed cameras fitted on the walls are used to   cover the target areas. In such a scenario, the viewing angle of the individual's movement causes a significant difference in the action recognition processes. Viewing angles of the individual's movement are categorised as follows <ref type="bibr" target="#b32">[33]</ref>:</p><p>-Object motion towards camera, -Object motion away from camera and -Object movement horizontal to camera. Sideways camera location.</p><p>In order to avoid such view angle problems, view invariant computer vision algorithms <ref type="bibr" target="#b50">[51]</ref> using a single camera or image fusion based multi camera setup <ref type="bibr" target="#b16">[17]</ref> or an omnidirectional camera <ref type="bibr" target="#b65">[66]</ref> are suggested. In the case of an omnidirectional camera, the system can capture a 360 â¢ viewing angle of video data and process accordingly as shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visual processing</head><p>In this section we first give details of the computer vision algorithms that can be used to recognised short terms actions and higher level activities for behaviour monitoring, then we provide an overview of approaches for health measurements from video or images. Then we discuss video processing challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Action recognition algorithms</head><p>The processing block is one of the most important blocks in video monitoring systems (see Fig. <ref type="figure" target="#fig_1">1</ref>) and uses various computer vision approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. This section focuses on visual processing for action recognition. Approaches to link actions into activities are summarised in Section 3.2.</p><p>Figure <ref type="figure">3</ref> represents the taxonomy of computer vision approaches for action recognition in AAL. Actions are usually defined either by the individual postural features (e.g. the person is bending over, lying, squatting, etc.) or by the individual ambulatory information obtained through motion tracking. In turn Table <ref type="table">3</ref> Different approaches for action recognition. The silhouette is normally extracted using background subtraction (BS). Posture can be represented by the bounding box features (BoundBox), a 5 dimensional vector (5D vector) representing region of interest features or projected histograms (Hist) and recognised using k-nearest neighbour (KNN). Motion can be represented by motion history image (MHI) and recognised using motion quantification (MQ), distance between histograms (HistDist), multi-Layer Perceptron (MLP), finite state model <ref type="bibr">(FSM)</ref> postural features can be established using static approaches which analyse the posture of an individual from a single image and dynamic approaches which include the temporal information in movement detection. The postural feature recognition process often includes a preliminary step called background subtraction (Section 3.1.1) followed by posture (Section 3.1.2) or movement recognition (Section 3.1.3). Motion tracking based approaches are discussed in Section 3.1.4. Table <ref type="table">3</ref> summarises the action recognition algorithms used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Background subtraction</head><p>Background subtraction is popularly used to separate moving objects (human shape here) in the scene by segmenting into background and foreground <ref type="bibr" target="#b63">[64]</ref> and many methods are proposed in the literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b53">54]</ref>. In its simplest form the background modelling considers a stationary camera with a stationary or nearly stationary background to detect the relative movement of human activity in the scene.</p><p>A comparison of the intensities within the frames is done over time to estimate the static pixels which is then modelled as background and the pixels can be represented by multivariate Gaussian values in HSV (hue, saturation and value) colour space <ref type="bibr" target="#b56">[57]</ref>.</p><p>However, in a real scenario, many other parameters, such as, lighting condition, long-term scene change, small cluttered movement, ghost and shadow, can affect the background modelling. To address such problems, Stauffer and Grimson <ref type="bibr" target="#b53">[54]</ref> proposed an adaptive backgrounding to reliably deal with lighting changes, repetitive motions from clutter and long-term scene changes; Cucchiara et al. <ref type="bibr" target="#b8">[9]</ref> used a statistical approach using the knowledge of moving objects, shadows and ghost objects from the previous frames to update the background; Elgammal et al. <ref type="bibr" target="#b13">[14]</ref> proposed a nonparametric model to handle clutters and small motions in the background by estimating the probability of observing pixel intensity values.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, Eng et al. propose a Bayesian framework to deal with background subtraction under challenging conditions. This framework could provide more robust background subtraction when the foreground object is matches the background (e.g. the individual having clothes similar to the background or if the scene is very dark) or if two objects in the foreground are not separable due to occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Static approaches</head><p>Posture is thought to be a good indicator of a person's activity and events such as falls. In static posture analysis the video sequence is analysed using framewise image processing. Typically, the first stage consists of extracting the silhouette from the image using background subtraction.</p><p>Aspect ratio measurement is one of the simplest and quickest methods in this category and used in many video based AAL algorithms, particularly in fall detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b57">58]</ref>. Aspect ratios are calculated by detecting the bounding box of the foreground segmented moving object (silhouette) and constructing approximated ellipses, rectangles etc. The bounding box is estimated by finding the furthest (leftmost, rightmost, topmost and bottommost) foreground pixels. However sometimes noises and shadows can introduce error (larger size of human object) in the bounding box calculation. To avoid such error, the foreground pixels are projected onto x and y axes by calculating the number of changed pixels row-wise and column-wise to build horizontal and vertical projection histograms. The bounding box can be identified by applying a threshold on generated histograms.</p><p>Images can be classified into different postures using the bounding box aspect ratios <ref type="bibr" target="#b57">[58]</ref>. Emmanuel <ref type="bibr" target="#b42">[43]</ref> proposed to classify postures using the k-Nearest Neighbour (KNN) algorithm. The KNN algorithm compares histograms of the stored posture images with histograms of the current input frame to find the most similar posture. The authors proposed an additional bounding box angle test to filter out false detection between a bending posture and lying towards camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Dynamic approaches</head><p>Static approaches have strong limitations. For example, static approaches alone cannot differentiate a lying posture due to a fall or intentional lying. The static approaches are often restricted to a range of movement patterns as it does not consider the temporal information during posture detection. To overcome such shortcomings dynamic approaches have been proposed considering movement information within the postural feature extraction stage.</p><p>As a first attempt to include temporal information into posture classification, in <ref type="bibr" target="#b42">[43]</ref>, the time of the last standing position is preserved in order to differentiate between normal lying and a real fall. The algorithm checks the difference between time of lying (t lying ) and last standing (t standing ) and a fall is declared if (t lyingt standing ) is less than a specified threshold.</p><p>Other ways to model the dynamic nature of a movement include 1) methods which combine several frames into one single image representing the silhouette at different points in time <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b51">52]</ref> and 2) methods which use finite state models (FSM) to represent the sequence of posture information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>.</p><p>Rougier et al. <ref type="bibr" target="#b51">[52]</ref> suggested a Motion History Image (MHI) based solution. The MHI is a special representation of a foreground object where the intensity of the pixels are derived according to recency of the motion within the image sequence and gives the most recent movement of a person during action. However, a repeated motion in the same position results in a similar MHI and Forughi et al. <ref type="bibr" target="#b17">[18]</ref> proposed an alternative solution to overcome this issue. A spatio-temporal template is proposed to store occurrence time of motion emphasizing the final motion. Finally, in both cases, the derived dynamic images are converted into a scalar valued image using a motion quantification coefficient <ref type="bibr" target="#b51">[52]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, Foroughi et al. use eigenspace to reduce the dimensionality of the MHI. The resulting vector is classified into a set of postures using a Multi-Layer Perceptron (MLP).</p><p>Tao et al. <ref type="bibr" target="#b57">[58]</ref> proposed to model the sequence of bounding box ratio using a two state finite state machine (FSM). The state transitions indicate changes between seating, standing and lying posture. Similarly, TÃ¶reyin et al. <ref type="bibr" target="#b61">[62]</ref> used combined audio and video fea-tures as inputs to a hidden Markov model to propose a robust fall detection. In a dementia patient elopement monitoring scheme, Chen et al. <ref type="bibr" target="#b7">[8]</ref> outlined the region of interest (ROI) at possible exit points of the unit and extracted 5 dimensional feature vectors corresponding to number of foreground pixels, geometric mean and variance of the ROI. An HMM is used here comprising 5 hidden states to model elopement sequences. Finally, Thome et al. <ref type="bibr" target="#b60">[61]</ref> propose a system based on a Layered HMM (LHMM) structure which allow to recognise actions (described by posture) as well as higher level behaviour patterns (activities) within the same model. The LHMM segments the problem into distinct layers that operate at different temporal granularities. The proposed approach implement a semantic of activities, other algorithms to translate actions into activities are presented in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Motion tracking</head><p>Apart from postural information, other important features that can be used to recognise activity are based on tracking of positional information of the individual.</p><p>In one of the initial approaches tracking based algorithms, Nait-Charif and McKenna <ref type="bibr" target="#b40">[41]</ref> proposed an overhead tracking scheme, where the target person is tracked using an ellipse representation so that the state at time t can be described as e t = (x t , y t , Ï t , s t , e t ) where (x t , y t ) is the ellipse center, Ï t is orientation, s t is scale and e t is the eccentricity. The tracking of this ellipse is then performed by applying an Iterated Likelihood Weighting particle filter <ref type="bibr" target="#b39">[40]</ref> to provide trajectories in 5D ellipse parameter space followed by a temporal smoothing operation. These smoothed trajectories and the person's speed (estimated at every point) provide a compact representation of the person's global motion. The global motions are useful to annotate the activities and make a decision in case of an event. However, this scheme is based on 2D vertical velocity estimation and may not distinguish between similar motion and posture information, such as a real fall and a person sitting down. In addressing these issues Rougier et al. <ref type="bibr" target="#b50">[51]</ref> used a 3D head tracking model which reduces the effect of perspective foreshortening. The scheme used the POSIT algorithm <ref type="bibr" target="#b9">[10]</ref> to estimate the relative position of a 3D head ellipsoid in the camera coordinate system and the 3D tracking is performed by using a conditional density propagation based particle filter <ref type="bibr" target="#b23">[24]</ref>. At every position of the ellipse, a 3D pose of the head is calculated to obtain the 3D trajectory which is analyzed to detect a fall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Combination of algorithms</head><p>The algorithms presented above do not always achieve the degree of sensitivity required for real life applications, therefore, some researchers have proposed to combine some classifiers to achieve better results.</p><p>For example, Emmanuel <ref type="bibr" target="#b42">[43]</ref> observed that their approach based on projected histograms and KNN sometimes classifies other postures wrongly as 'lying toward' and propose a bonding box angle test to remove uncertainty. Similarly, Lin and Ling <ref type="bibr" target="#b32">[33]</ref> proposed to reduce the number of false alarms by using a three stage approach which sequentially apply three different algorithms to detect a fall:</p><p>1. Detection of motion and duration of an event (typical fall time 0.4sâ¼0.8s), 2. Human shape detection, location and change of rate of the centroid of the shape, and 3. Vertical projection histogram calculation and posture classification, i.e., fall, walk and squat.</p><p>Rougier et al. <ref type="bibr" target="#b51">[52]</ref> propose a sequential scheme for fall detection which uses both static and dynamic analysis of the scene. First, the system detects the motion of falling, then a static posture recognition algorithm is used to detect if the individual is lying or standing and finally, the system checks if the individual is on the ground without moving. If the three sequential tests are positive, the fall is detected.</p><p>Note that the three mentioned systems are based on a combination of static and dynamic posture recognition. To our knowledge, no action recognition system for VAAL has been proposed which combines postural features with motion tracking features. These two features are, however, likely to be complementary and could potentially improve general action recognition performance. Furthermore, the three approaches combine the results of multiple classifiers using rather simplistic AND and OR rules to take the final decision. The final decision could also be made by weighting the results of the different classifiers or use a more advanced theoretical framework <ref type="bibr" target="#b28">[29]</ref> for classifier combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Monitoring behaviour</head><p>Most video based AAL applications are concerned with recognising and monitoring human behaviour. Exceptions are the methods which take health measurement from video data discussed in Section 3.3. Currently the majority of the literature focuses on fall detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>, while some efforts have been made to detect elopement of patients from a dementia unit <ref type="bibr" target="#b7">[8]</ref>. However the scope of video based monitoring is much wider and yet to be explored fully. Table <ref type="table" target="#tab_3">4</ref> shows a list of behaviour that can potentially be monitored through video based systems and the corresponding domain of application.</p><p>Although action and activity often refer to a similar concept, action is more related to a simple motion pattern such as opening a door, turning the oven on or falling whereas activity refers to complex sequences such as cleaning a room or cooking. Actions are the primitive components used to define an activity. For some applications such as fall detection, the task consists in detecting particular actions, however, in other cases the application requires to recognise higher level activities. Activities are composed of primitive actions linked in a spatio-temporal structure. Note that activities may be composed of several subactivities which can eventually be broken down into actions. Behavioural monitoring usually involves a hierarchical structure. The lower level consists of recognising simple actions using the computer vision techniques presented in Section 3. The output of the action detectors provides the input stream for a higher level activity recognition component.</p><p>Inspired by research in natural language processing, stochastic context free grammar parsing is a popular approach to recognising semantically defined activities. With this approach, for each activity, a grammar is defined specifying expected orders of sub-activities and actions. Parsing mechanisms are used to translate the action sequence into activities description. Note that apart from the actions ordering, no notion of temporality is defined in the grammar. In <ref type="bibr" target="#b24">[25]</ref>, Ivanov et al. propose such a system which has been demonstrated to recognise car activities in a car park. Such a grammar representation and parsing mechanism is well suited to modelling activities for AAL. More recently re-searchers have addressed this problem by using dynamic Bayesian networks (DBN) to model activities. DBN also encodes expected actions and the ordering constraints for each considered activity, however DBN approaches can enforce temporal constraints that provide powerful contextual cues to recognising activities. Laxton et al. <ref type="bibr" target="#b30">[31]</ref> propose a DBN approach which model the time period between actions. A limitation of both stochastic context free grammars and DBNs is that the semantic must be specified a priori. In the context of AAL, each individual could perform a particular activity in their own way and it is unlikely that the same grammar definition could fit all users. It would thus be beneficial if the system could automatically learn the grammar or topology for each user using recorded data which is not the case for existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Physiological monitoring</head><p>Another very promising application is the contact free measurement of physiological data through the use of video cameras. The ability to monitor a patient's physiological measurements has been demonstrated by several research papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Fei and Pavlidis <ref type="bibr" target="#b15">[16]</ref> proposed a method to recover the breathing waveform of patients using thermal video. First, the nostril region is segmented and tracked over time, then the breathing information is extracted through wavelet analysis. Garbey et al. <ref type="bibr" target="#b20">[21]</ref> also used thermal imaging, developing a method to measure cardiac pulse based on a thermal signal emitted from major superficial vessels of the face. Takano and Ohta <ref type="bibr" target="#b55">[56]</ref> proposed a low cost solution to measure both the respiratory and pulse rate from a standard camera. The brightness of a region of the face is measured for 30s. Series of operations are applied to the generated signal of brightness: the derivative, a low pass filter, and spectral analysis. The peaks in the spectral signal are then used to determine the respiratory and the pulse rate.</p><p>Poh et al. <ref type="bibr" target="#b47">[48]</ref> have taken a step further by proposing a high accuracy cardiac pulse measurement from a basic webcam even in the presence of head movements. Moreover they show that the approach can be extended to simultaneous measurement of multiple people. First an automatic face tracker detects faces in the video, then the extracted face image is separated into the three RGB channels. The three signals are normalised and decomposed into three independent components using independent component analysis (ICA). The power spectrum is then extracted from one of the components and the pulse frequency is designated as the frequency that corresponds to the peak within an operational range of frequency. This approach has proven to be effective even in the presence of head movements with a mean bias of only 0.64 bpm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion and challenges</head><p>Table <ref type="table">3</ref> summarises the action recognition algorithms used in the literature as well as the intended recognition task. Most of the proposed systems are devoted to detecting falls or to differentiate falling from walking. Other papers present systems to classify a larger number of actions such as squatting, bending or eloping or different types of falls (forward, backward or sideways) and Nait-Charif and McKenna <ref type="bibr" target="#b40">[41]</ref> recognise ambulatory information to summarise activities and detect falls. VAAL is a relatively new domain and, supported by the success of achieving these tasks, the rich content of video sensing has the potential to recognise more detailed actions such as 'opening a cupboard' or 'picking up an object' which would be very useful information for AAL applications. Some issues are, however, still to be addressed fully such as dealing with occlusions. Indeed, in home environment filled with furniture and various objects, it is impossible to cover the totality of a room using a single camera. Hardware solutions to this problem include the use of a overhead camera. Alternatively, the solution proposed in <ref type="bibr" target="#b7">[8]</ref> make use of multi-views of the scene by combining features from multiple cameras covering the same area.</p><p>Another challenge to be addressed is the multioccupancy of the accommodation. While most AAL solutions are targeted for people living alone, they are still likely to receive visitors, therefore algorithms for identity recognition (e.g. face recognition) and tracking of multiple people should be included in VAAL systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Acceptability and privacy</head><p>As seen in previous sections, AAL technology can potentially provide a wide range of solutions to support older people in their own homes and improve their quality of life. However, to be applicable to a large scale the technology must be accepted by potential users and their families. Some users show some anxiety toward new technologies or are concerned with anaesthetic and invasive sensors (or cameras) fitted in their home. However, the main concern over the adoption of AAL in general and VAAL in particular is the loss of privacy. Maintaining privacy is thus a very important consideration when developing VAAL technology.</p><p>In the first part of this section the technical challenges to protect privacy are discussed, while the second part is an overview of acceptability studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data and privacy protection</head><p>In the current literature only a handful of papers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> discussed the issue of data protection in AAL. Sensor based telecare information security is discussed in <ref type="bibr" target="#b69">[70]</ref>, whereas video based privacy solutions are proposed in <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b68">[69]</ref>.</p><p>Data protection, including medical data, is crucial in the context of AAL. The data should be stored securely in a centralised server and access is controlled by various authenticity measures such as password protection. The main threats involved are reply attacks (a network attack to send data repeatedly), on-line/offline password guessing attacks, impersonation attacks (providing false data to legitimate user) or stolenverifier attacks (stealing passwords by insider member). Increased security solutions are proposed by Wu et al. <ref type="bibr" target="#b69">[70]</ref> by using a two stage authentication process which involves a cryptographical session key followed by password authentication. In conclusion, preserving video or sensor based AAL data is an information security problem and can be addressed by solutions from database management, cryptography or data mining related fields.</p><p>When dealing with video recordings in users' homes, the privacy protection issues are predominant. In most VAAL systems, some authorised people (referred to as carers) have access to a level of information recorded and processed by the system. The most sensitive level being the live video or recorded footage and the less sensitive level being the automatically interpreted information (e.g. a fall detected). Figure <ref type="figure" target="#fig_3">4</ref> shows a classification of information of VAAL systems. Different carers may have access to different levels of information and access to a level of information could be open to a carer only in specific situations. For example, when a critical situation is detected by the system, clinical professionals may need to access the live or recorded footage, therefore a hierarchical access control can address such issues. In an attempt towards providing security and privacy in video based monitoring for AAL, Fleck and Strasser <ref type="bibr" target="#b16">[17]</ref> discussed various aspects and proposed a solution using a smart camera network. In a traditional centralised monitoring system, a carer can have direct access to the privacy sensitive footage (Fig. <ref type="figure">5</ref>(1)); or a network intruder (during channel transmission or in the server) can fetch the data illegally (Fig. <ref type="figure">5</ref>(2)); or the direct camera hardware can be targeted (Fig. <ref type="figure">5</ref>(3)). Using a smart camera, part of the processing is performed locally and the more sensitive information is not transmitted to the server or a carer, thus reducing the vulnerability of the system. Smart cameras can either process the video locally and transmits decision information as in <ref type="bibr" target="#b70">[71]</ref> or they can filter the video to obscure individual identities or actions as in <ref type="bibr" target="#b68">[69]</ref> and <ref type="bibr" target="#b16">[17]</ref>. Several computer vision approaches have been proposed to remove private information from the video data such as using augmented symbols of the target or silhouette of the region of in-  <ref type="bibr" target="#b68">[69]</ref> Edges of the region of interest Dufaux et al. <ref type="bibr" target="#b11">[12]</ref> Scrambling regions of interest Gross et al. <ref type="bibr" target="#b21">[22]</ref> Model-Based Face De-Identification terest as shown in Fig. <ref type="figure" target="#fig_4">6</ref>. Table <ref type="table" target="#tab_4">5</ref> summarises several privacy filter solutions. A similar approach is taken in <ref type="bibr" target="#b68">[69]</ref> to protect privacy and the proposed solution focuses on four different aspects:</p><p>Integrity: Any manipulation in video transmission must be detected, e.g., using checksum or digital signature. Authenticity: Evidence of source of data, i.e., camera identification etc. Confidentiality: Privacy protection by encrypting the region of interest. Multi-level user control: Different level of protection on confidential data using user access control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">User requirements and acceptability</head><p>The Royal Academy of Engineering discusses the dilemmas of privacy and surveillance in <ref type="bibr" target="#b44">[45]</ref>. The dilemmas arise from the impossibility of weighing up the benefits of surveillance systems and the potential loss of privacy. In AAL technology, only the end-user perspective can help to define the right balance.</p><p>Marquis-Faulkes et al. <ref type="bibr" target="#b35">[36]</ref> present a qualitative user requirement study for the use of video technology for fall detection and activity monitoring. Four focus groups were conducted with a total of 37 participants. Three groups were composed of older people with dif-Fig. <ref type="figure">5</ref>. Centralised &amp; distributed camera network and privacy issues. Figure is adopted from <ref type="bibr" target="#b16">[17]</ref>. ferent levels of dependence and the last group comprised sheltered housing wardens. One outcome of the study was that the participants were generally content to have a video based monitoring device provided that the video footage was analysed only by computer and that no-one would actually watch them. The participants in the user group with the highest level of independence expressed concerns about carers having access to too much information about their activities but were favourable to a system that would alert carers in case of fall or abnormal activity detected.</p><p>Turgeon Londei et al. <ref type="bibr" target="#b34">[35]</ref> explore the receptivity and requirements of older people regarding the introduction of an intelligent video monitoring system in Canada. Interviews were conducted with 25 older people with a history of falls. Both qualitative and quantitative data were reported and analysed. The perception toward video monitoring was generally good with 96% of the participants at least partially favourable to the intelligent video monitoring system and 88% accepted the sending of an image to a designated carer. The participants were presented with different types of images that could be sent to a designated carer. The proposed images were the original image and images processed with blurring, pixelisation or silhouette extraction. Interestingly, nearly all the participants chose to sent the original image rather than modified images. Although 19 participants originally refused to be videotaped in one or more room, 8 of them would accept if the original image was modified. Note that only half of the participants who were favourable to the system did not agree to use it, claiming that the system was too complex, that they were not old or ill enough or that they did not need more protection.</p><p>Both studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> suggest that VAAL would be accepted in principle by users if they feel that the technology would make a real difference to their security and quality of life. However, while all the participants of the study from Marquis-Faulkes et al. <ref type="bibr" target="#b35">[36]</ref> would not allow anyone to see the video footage, most participants from the study from Turgeon Londei et al. <ref type="bibr" target="#b34">[35]</ref> would accept sending original images to a designated carer in the case of a critical situation detected by the system. It could be hypothesised that, in the former study the participants did not discuss the possibility of a still image sent to the carer only in the case of a critical situation but were referring to a constant surveillance by an operator. The second study included only participants with a history of falls, therefore, potentially with a greater need for a fall detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Acceptability and privacy are key elements to consider in the design of new VAAL systems. As seen previously, the technology exists to protect users' privacy against access by unauthorised individuals. The question still to be answered is: What level of information could be accessed by designated carers? To answer this question, the right balance between efficacy and loss of privacy should be found. The requirement studies presented in the previous section provide good indicators, however the privacy issues of video based monitoring have not been fully explored in line with the requirements from the users at both ends of AAL systems, i.e., patients and clinical professionals. It is also believed that by improving the technology, the VAAL systems can become more reliable and consequently depend less on human operations, thus improving confidentiality.</p><p>It is also apparent from the requirement studies that the design of the system might not be fully understood by potential users. It is reported in <ref type="bibr" target="#b34">[35]</ref> that some participants did not make any distinction between the image analysis executed by a computer and the images seen by authorised carers. As a consequence, it can be expected that some users can be reluctant to adopt a system which involves a camera even if the video is never transmitted or stored and only abstracted information is sent to a server as proposed by Yang et al. <ref type="bibr" target="#b70">[71]</ref>.</p><p>Both acceptability studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> were conducted with a relatively small number of participants and it would be interesting to study the user requirements at a larger scale. As mentioned by Broadbent et al. in a review of acceptance of health care robots for older population <ref type="bibr" target="#b3">[4]</ref>, research studies show cultural differences in attitudes towards robots. It is likely that similar cross-cultural differences could be observed in ac-ceptability of VAAL and large scale cross-cultural acceptability studies would benefits the domain.</p><p>Social implications of AAL should also be considered, for example Sun et al. <ref type="bibr" target="#b54">[55]</ref> observe that the transfer of the dependence from human side to technology also reduces the social connections of the assisted people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">System evaluation</head><p>VAAL systems can be evaluated at multiple levels:</p><p>1. Technical performance evaluation consists in measuring the accuracy to which the proposed system achieves the designed tasks. Technical performance evaluation strategies are discussed in Section 5.2. 2. Outcome evaluation aims to determine the effect of the system on users, carers and society. While dependent on the technical performances, outcome evaluation is strongly linked to the way carers, clinicians and emergency services respond to the information provided by the system. A typically used methodology to evaluate the effectiveness of health services is to 1) recruit users fulfilling the eligibility criteria, 2) select randomly a group of users who will be provided the technology and associated services and 3) compare the outcome measures for the group receiving the technology and the group receiving traditional care. To our knowledge, no VAAL has been evaluated on a large scale clinical trial yet, however, for an example of clinical evaluation of AAL based on a network of sensors, see Brownsell et al. <ref type="bibr" target="#b5">[6]</ref>. 3. Privacy protection validation aims to insure that the system fulfils the required level of security. Wu et al. <ref type="bibr" target="#b69">[70]</ref> compare telecare information systems according to their vulnerability to several types of attacks (Replay attacks, Password guessing attacks, etc.). Some international stan-dards for computer security certification also exist such as the Common Criteria for Information Technology Security Evaluation <ref type="bibr" target="#b67">[68]</ref>. 4. User acceptability evaluation aims to analyse how the system is perceived by potential users.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, Demiris et al. developed a questionnaire to assess patients' impressions of the risks and benefits of home telecare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Technical performance evaluation</head><p>Many approaches have been recently proposed to tackle VAAL, however the community is lacking a consistent methodology for evaluating model and system technical performance. Key for the field to progress and to be adopted, is experimental validation and presentation of experimental results.</p><p>Technical performance evaluation aims to characterise how well the recognised activities match the true sequence of activities (the groundtruth). Although, activity and action recognition can be a multi-class problem, the performance evaluation is typically done a class at a time which leads to the performance evaluation of a binary classification.</p><p>For action recognition tasks such as fall detection, the technical evaluation typically estimates the detection rate (TPR = true positive rate) and the false alarm rate (FPR = false positive rate). Most algorithms can be adjusted to reduce the number of missed detections with the effect of increasing the number of false alarms. A popular technique to compare, select and visualise performance of such systems is to plot receiver operating characteristic (ROC) curves which depict the tradeoffs between FPR and TPR <ref type="bibr" target="#b12">[13]</ref>.</p><p>The evaluation of performance of activity recognition is more complex and no common methodology has been adopted yet. Indeed, the characterisation of activity recognition performances using FPR and TPR does not take into account other types of errors such as fragmentation, merging and timing offset. Figure <ref type="figure">7</ref> shows examples of the different types of error generated by activity recognition systems.</p><p>Ward et al. <ref type="bibr" target="#b66">[67]</ref> have recently proposed a comprehensive overview and critique of existing perfor-Fig. <ref type="figure">7</ref>. Examples of different types of error which can be generated from activity recognition. mance metrics for activity recognition. These metrics typically use one of two basic units, namely frames and events. The frame based scoring decomposes the time in frames of fixed-length unit. For each frame the groundtruth and recognised activity are matched making it possible to find a FPR and TPR. The scoring based on events compares groudtruth and recognised sequences of activities defined by the start time and stop time. A recognised activity can then be scored as either correctly detected or falsely inserted or deleted. We note that a definition of correct detection should be provided. For example Tapia et al. <ref type="bibr" target="#b58">[59]</ref> propose to label an activity as correctly detected if it is detected within a time interval around the end of the groundtruth activity.</p><p>As stated in <ref type="bibr" target="#b66">[67]</ref>, both scoring approaches suffer from limitations. The frame based scoring does not distinguish between the types of errors which means that an undetected activity (i.e. deletion) could have less effect on the error rate than a timing offset in an otherwise correctly detected activity. On the other hand, event based scoring can differentiate time offset, deletion and insertion but is unable to account for fragmented and merged activities. Moreover, there is no clear definition of what is a correct activity and each researcher seems to provide their own definition. In light of the problems with existing approaches, Ward et al. <ref type="bibr" target="#b66">[67]</ref> propose a novel set of performance metrics for activity recognition which can characterise insertion, merging, time offset (overfill and underfill), deletion and fragmentation.</p><p>In the literature, results are often presented using video sequences of simulated falls, often recorded by the researchers themselves. Unfortunately, each proposed approach is evaluated on a separate dataset which make comparisons impossible. We would thus recommend that a multidisciplinary group of researchers join efforts to build a large and realistic database of sequences of falls with an associated evaluation protocol that could be put together and used by the research community. For obvious ethical reasons, it might not be possible to collect a dataset with actual falls of older people but the simulated falls should be made as realistic as possible by consulting specialist clinicians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Video and computer vision technologies are developed to aid ambient assisted living with particular em-phasis in the detection of falls. However the scope of the domain is rapidly expanding towards new applications related to rehabilitation and care of older people or personal activity management. In our view, three main challenges need to be addressed for the domain to reach maturity:</p><p>1. The Technical Challenge: The technological solutions proposed for VAAL in the last few years, particularly to tackle fall detection, are very promising but robustness to real-world conditions needs to be addressed. The rich information provided by video solutions could also be used to solve problems currently tackled by conventional sensor based systems. Efforts should be made to explore further new applications. 2. The Acceptability Challenge: To be successful, VAAL systems must be accepted by users, carers and society. The existing acceptability studies suggest that users would accept VAAL in principle with the condition that they feel that it would make a real difference to their security, health and quality of life. However, the studies also show that only a small proportion of the interviewees would actually use the proposed systems. The gap between the acceptability in principle and the consent to adopt the technology can only be reduced by developing the technology together with the study of user needs. 3. The Integration Challenge: An integration of technical solutions to build an AAL based society is still far from reality. Steps need to be taken towards the integration of VAAL including a standardisation performance evaluation, as seen in Section 5, and the integration of VAAL with existing telecare services. One of the great advantages of video technology for AAL is that the same hardware setup can be used for multiple applications to help older people to live independently (see Table <ref type="table" target="#tab_0">1</ref>). The integration of multiple applications in a single system could make the solution more attractive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Centralised camera network (b) Smart and distributed camera network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Video monitoring system blocks within AAL.</figDesc><graphic coords="5,70.27,388.65,212.76,157.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Omnidirectional camera to capture 360 â¢ view [66].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sensitivity of information of VAAL systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of individual privacy: Silhouette of region of interest. Figure is adopted from [17].</figDesc><graphic coords="12,70.27,124.05,212.76,102.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Commonly used sensors and their domains of application</figDesc><table><row><cell>Sensor</cell><cell>Application domains</cell><cell>Embedded/Body-worn</cell></row><row><cell>Motion detector</cell><cell>Fall detection, Activity monitoring, Security</cell><cell>Embedded</cell></row><row><cell>Door open/close</cell><cell>Activity monitoring, Security</cell><cell>Embedded</cell></row><row><cell>Electrical appliance</cell><cell>Activity monitoring, Safety</cell><cell>Embedded</cell></row><row><cell>Microphone</cell><cell>Fall detection</cell><cell>Embedded</cell></row><row><cell>Accelerometer</cell><cell>Fall detection, Activity monitoring</cell><cell>Body worn</cell></row><row><cell>RFID</cell><cell>Activity monitoring</cell><cell>Body worn/Embedded</cell></row><row><cell>Temperature/CO2/Smoke sensor</cell><cell>Safety</cell><cell>Embedded</cell></row><row><cell>Vital sign/physiological</cell><cell>Health care</cell><cell>Body worn/Embedded</cell></row><row><cell>Camera</cell><cell>Fall detection, Activity monitoring, Security, Safety, Health care</cell><cell>Embedded</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Sensors used in multi-modal approaches</cell><cell></cell><cell></cell></row><row><cell>Sensor</cell><cell>Project</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>GER'HOME</cell><cell>HERMES</cell><cell>UbiMon</cell><cell>Toreyin et al. [62,63]</cell></row><row><cell>Light sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Humidity sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temperature sensor</cell><cell>X</cell><cell></cell><cell>X</cell><cell></cell></row><row><cell>Presence sensor</cell><cell>X</cell><cell></cell><cell></cell><cell>X</cell></row><row><cell>Pressure sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Water sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Electrical sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Door and window sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Open / close sensor</cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECG</cell><cell></cell><cell></cell><cell>X</cell><cell></cell></row><row><cell>Accelerometer</cell><cell></cell><cell></cell><cell>X</cell><cell></cell></row><row><cell>Blood pressure monitoring sensor</cell><cell></cell><cell></cell><cell>X</cell><cell></cell></row><row><cell>Audio sensor</cell><cell></cell><cell>X</cell><cell>X</cell><cell>X</cell></row><row><cell>Video sensor</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell>X</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>List of behaviours monitored in AAL</cell></row><row><cell>Activities of Daily Living (ADL):</cell></row><row><cell>Bathing, Dressing, Grooming, Walking, Climbing stairs,</cell></row><row><cell>Eating, Cooking, Managing Medication, Using Phone,</cell></row><row><cell>Housework, Doing laundry, etc.</cell></row><row><cell>Sleep/Awake pattern</cell></row><row><cell>Immobility/Activity pattern</cell></row><row><cell>Elopement</cell></row><row><cell>Fall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>Privacy filter solutions</cell></row><row><cell>Ref</cell><cell>Filter</cell></row><row><cell>Fleck et al. [17]</cell><cell>Augmented symbols of the target</cell></row><row><cell></cell><cell>or silhouette of the region of interest</cell></row><row><cell>Winkler et al.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ambient assisted living roadmap</title>
		<author>
			<persName><surname>Aaliance</surname></persName>
		</author>
		<ptr target="http://www.aaliance.eu/public/documents/aaliance-roadmap" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human motion: Modeling and recognition of actions and interactions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3D Data Processing, Visualization and Transmission 3DPVT</title>
		<meeting>3D Data essing, Visualization and Transmission 3DPVT</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A smart and passive floor-vibration based fall detector for elderly</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Felder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Information and Communication Technologies (ICTTA)</title>
		<meeting>Information and Communication Technologies (ICTTA)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1003" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Acceptance of healthcare robots for the older population: Review and future directions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Broadbent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="319" to="330" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Assistive social robots in elderly care: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Broekens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gerontechnology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="94" to="103" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An evaluation of second and third generation telecare services in older people&apos;s housing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brownsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Telemedicine and Telecare</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic fall detection in real time video based applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cambul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Karshgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Signal Processing and Communications Applications</title>
		<meeting>IEEE Signal essing and Communications Applications</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intelligent video monitoring to improve safety of older persons</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bharucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Wactlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Engineering in Medicine and Biology Society (EMBS)</title>
		<meeting>IEEE International Conference on Engineering in Medicine and Biology Society (EMBS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="3814" to="3817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting moving objects, ghosts, and shadows in video streams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1337" to="1342" />
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-based object pose in 25 lines of code</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A questionnaire for the assessment of patients&apos; impressions of the risks and benefits of home telecare</title>
		<author>
			<persName><forename type="first">G</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speedie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Telemedicine and Telecare</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="278" to="284" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scrambling for video surveillance with privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<meeting>2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">160</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Signal detection theory and ROC analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Egan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series in Cognition and Perception</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-parametric model for background subtraction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th European Conference on Computer Vision-Part II (ECCV)</title>
		<meeting>6th European Conference on Computer Vision-Part II (ECCV)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A bayesian framework for robust human detection and occlusion handling human shape model</title>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Kam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Yau</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2004.1334150</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Conference on Pattern Recognition (ICPR 2004)</title>
		<meeting>of the 17th International Conference on Pattern Recognition (ICPR 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thermistor at a distance: Unobtrusive measurement of breathing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="988" to="998" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Smart camera based monitoring system and its application to assisted living</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Strasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1698" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An eigenspace-based approach for human fall detection using integrated time motion image and neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Foroughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Yazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Signal Processing</title>
		<meeting>International Conference on Signal essing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1499" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic intruder detection incorporating intelligent scene monitoring with video surveillance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Beggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Fernandez-Canque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goryashko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Security and Detection (ECOS)</title>
		<meeting>European Conference on Security and Detection (ECOS)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AIBO: Toward the era of digital creatures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Robotic Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="781" to="794" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contact-free measurement of cardiac pulse based on the analysis of thermal imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nanfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1418" to="1426" />
			<date type="published" when="2007-08">Aug. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-based face de-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D T</forename><surname>Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<meeting>2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-based event recognition: Activity representation and probabilistic recognition methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hongeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="162" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CONDENSATION -conditional density propagation for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognition of visual activities and interactions by stochastic parsing</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="852" to="872" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context aware inactivity recognition for visual fall detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deklerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pervasive Health Conference and Workshops</title>
		<meeting>Pervasive Health Conference and Workshops</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Chalidabhongse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-time foreground-background segmentation using codebook model</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
	<note>Special issue on video object processing</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Development of a mobile user interface for image-based dietary assessment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boushey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on Mobile and Ubiquitous Multimedia</title>
		<meeting>9th International Conference on Mobile and Ubiquitous Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998-03">Mar. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on behavior analysis in video surveillance for homeland security applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Applied Imagery Pattern Recognition Workshop (AIPR)</title>
		<meeting>IEEE Applied Imagery Pattern Recognition Workshop (AIPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Leveraging temporal, contextual and ordering constraints for recognizing complex activities in video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jongwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An intelligent emergency response system: Preliminary development and testing of automated fall detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mihailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Telemedicine and Telecare</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="194" to="198" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic fall incident detection in compressed video for intelligent homecare</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Communications and Networks (ICCCN)</title>
		<meeting>International Conference on Computer Communications and Networks (ICCCN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1172" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detection of falls at home using floor vibrations and sound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zigel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Convention of Electrical and Electronics Engineers in Israel (IEEEI)</title>
		<meeting>IEEE Convention of Electrical and Electronics Engineers in Israel (IEEEI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="514" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An intelligent videomonitoring system for fall detection at home: Perceptions of elderly people</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Londei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>St-Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saint-Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Telemed Telecare</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="383" to="390" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gathering the requirements for a fall monitor using drama and video with older people</title>
		<author>
			<persName><forename type="first">F</forename><surname>Marquis-Faulkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Technology and Disability</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Socially assistive robotics for post-stroke rehabilitation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feil-Seifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of NeuroEngineering and Rehabilitation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An intelligent health monitoring and emergency response system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mihailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Smart Homes and Health Telematics (ICOST)</title>
		<meeting>International Conference on Smart Homes and Health Telematics (ICOST)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based human motion capture and analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>KrÃ¼ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tracking poorly modelled motion using particle filters with iterated likelihood weighting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nait-Charif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian conference on computer vision (ACCV)</title>
		<meeting>Asian conference on computer vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="156" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Activity summarisation and fall detection in a supportive home environment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nait-Charif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Pattern Recognition (ICPR)</title>
		<meeting>IEEE International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="323" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self management of stroke supported by assistive technology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Torsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mountain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Virtual Rehabilitation International Conference</title>
		<meeting>Virtual Rehabilitation International Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">193</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Intelligent video surveillance for monitoring elderly in home environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Nasution</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Emmanuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Multimedia Signal Processing</title>
		<meeting>IEEE Workshop on Multimedia Signal essing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="203" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ubiquitous monitoring environment for wearable and implantable sensors (ubimon)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Toumazou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th International Conference on Ubiquitous Computing (UBI-COMP &apos;04)</title>
		<meeting>6th International Conference on Ubiquitous Computing (UBI-COMP &apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The Royal Academy of Engineering, Dilemmas of Privacy and Surveillance: Challenges of Technological Change</title>
		<ptr target="http://www.raeng.org.uk/societygov/policy/current_issues/privacy_surveillance/pdf/dilemmas_of_privacy_and_surveillance_report.pdf" />
		<imprint>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="http://www.csed.dh.gov.uk/_library/Resources/CSED/CSEDProduct/Telecare_North_Yorks_Case_Study_webversionvs_1.pdf" />
		<title level="m">Efficiencies in Telecare</title>
		<imprint>
			<publisher>Department of Health Publication</publisher>
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sensor networks for ambient intelligence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Multimedia Signal Processing</title>
		<meeting>IEEE Workshop on Multimedia Signal essing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-contact, automated cardiac pulse measurements using video imaging and blind source separation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="10762" to="10774" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<ptr target="http://gerhome.cstb.fr/" />
		<title level="m">GER&apos;HOME Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">HERMES Project</orgName>
		</author>
		<ptr target="http://www.fp7-hermes.eu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monocular 3d head tracking to detect falls of elderly people</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>St-Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Engineering in Medicine and Biology Society (EMBS)</title>
		<meeting>IEEE International Conference on Engineering in Medicine and Biology Society (EMBS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="6384" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fall detection from human shape and motion history using video surveillance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>St-Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Advanced Information Networking and Applications Workshops (AINAW)</title>
		<meeting>International Conference on Advanced Information Networking and Applications Workshops (AINAW)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="875" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="http://www.linkcareservices.com/lcs/LCS\_Presentation\_Guide.pdf" />
		<title level="m">Link Care Services</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Promises and challenges of ambient assisted living systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Florio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blondia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Information Technology: New Generations</title>
		<meeting>International Conference on Information Technology: New Generations</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1201" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Heart rate measurement based on a time-lapse image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Engineering and Physics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="853" to="857" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fall incidents detection for intelligent video surveillance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Information, Communications and Signal Processing</title>
		<meeting>International Conference on Information, Communications and Signal essing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1590" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Quickest change detection for health-care video surveillance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Activity recognition in the home using simple and ubiquitous sensors</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="158" to="175" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A HHMM-based approach for robust fall detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<meeting>International Conference on Control, Automation, Robotics and Vision (ICARCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A real-time, multiview fall detection system: A LHMM-based approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ambellouis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<idno type="ISSN">1051-8215</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1522" to="1532" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">HMM based falling person detection using both audio and video, Computer Vision in Human</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>TÃ¶reyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dedeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ãetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3766</biblScope>
			<biblScope unit="page" from="211" to="220" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Falling person detection using multi-sensor signal processing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>TÃ¶reyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Birey</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Onaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ãetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning and matching of dynamic shape manifolds for human action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1646" to="1661" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An intelligent surveillance system based on an omnidirectional vision sensor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Cybernetics and Intelligent Systems</title>
		<meeting>IEEE Cybernetics and Intelligent Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Performance metrics for activity recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Gellersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Privacy impact assessments: The UK experience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bayley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Charlesworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st International Conference of Data Protection and Privacy Commissioners</title>
		<meeting>31st International Conference of Data Protection and Privacy Commissioners</meeting>
		<imprint>
			<publisher>SSRN</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">TrustCAM: Security and privacyprotection for an embedded smart camera based on trusted computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>IEEE Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A secure authentication scheme for telecare medicine information systems</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">From sensor networks to behaviour profiling: A homecare perspective of intelligent building</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiemjarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Neid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE Seminar for Intelligent Buildings</title>
		<meeting>IEE Seminar for Intelligent Buildings</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A method for automatic fall detection of elderly people using floor vibrations and soundproof of concept on human mimicking doll falls</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2858" to="2867" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
