<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motor Learning and Generalization Using Broad Learning Adaptive Neural Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haohui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Automation Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Autonomous Systems and Networked Control</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Automation Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Autonomous Systems and Networked Control</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Chenguang</forename><surname>Yang</surname></persName>
							<email>cyang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">College of Automation Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Autonomous Systems and Networked Control</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">C</forename><forename type="middle">L Philip</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Automation Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Autonomous Systems and Networked Control</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motor Learning and Generalization Using Broad Learning Adaptive Neural Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7C0819B3EA806BB9D95CFCA1F7F2D581</idno>
					<idno type="DOI">10.1109/TIE.2019.2950853</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2950853, IEEE Transactions on</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive neural control</term>
					<term>broad learning</term>
					<term>deterministic learning</term>
					<term>global stability</term>
					<term>guarantee tracking performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human neural motor system has the intelligence to learn new skills, and then to generalize these skills naturally. But it is not easy for a robot to demonstrate such intelligent behaviors. Inspired by the neural motor behaviors, a framework of broad learning based novel adaptive neural control is proposed in this paper, such that in the presence of dynamic disturbance, robots can learn a set of basic skills and then generalize these skills to the neighboring movements naturally as our human motor system. This is achieved by incorporating the deterministic learning with the broad learning system which can accumulate and reuse the learned knowledge. The broad learning enabled adaptive neural control has been rigorously established in theory and tested in both simulation and experimental studies. Simulation results and performance of the Baxter robot in the experiments have shown the effectiveness and superiority of the proposed method in comparison to the conventional adaptive neural control.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, human-like motor training techniques have been introduced into robotics [1]- <ref type="bibr" target="#b2">[3]</ref>, aiming to enable robots think and act in a human-like manner no matter in biomechanics or intelligence aspect. In our daily life, humans have the distinctive ability to accomplish various complex motor tasks by means of being inspired from the fundamental techniques in the past. Notably, it would be ideal to the engineer robot systems to learn and perform in a similar way as our humans.</p><p>In the past decades, motor learning in humans was studied mostly through adaptation in motor tasks. Force and impedance control have been widely implemented to provide a desired adaptive motor control behavior while the robot interacts with the environment <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Kadiallah <ref type="bibr" target="#b3">[4]</ref> proposed a structure to adapt the multiple movements in the state space based on nonlinear adaptive control using Radial Basis Function Neural Network (RBFNN) <ref type="bibr" target="#b4">[5]</ref>. For adaptive control system, deterministic learning theory <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> is one of the most efficient methods to learn the dynamic system under the satisfaction of persistent excitation (PE) condition <ref type="bibr" target="#b8">[9]</ref>. In a local region, it can approximate the dynamic models accurately along the recurrent-like orbit. Deterministic learning can make the neural weights converge to its optimal values and the system will be exponential stability ultimately via localized RBF neural networks <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>Recently, a novel framework of broad learning system (BLS) based on incremental learning principle has been developed and successfully applied for pattern recognition and off-line classification <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Although both BLS and incremental learning are widely studied by researchers, there is barely adaptive neural control research concerning the incremental learning or BLS to our knowledge, which inspires the research of this paper. Hence, this work makes a step forward to extend broad learning to cope with the task generalization in motor learning problem by integrating with adaptive neural control.</p><p>In this context, an adaptive neural control based on broad learning framework for motor generalization is proposed with the satisfaction of global stability <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and prescribed tracking performance <ref type="bibr" target="#b15">[16]</ref> under the dynamic external disturbance environment. The main contributions of this work can be concluded as follows.</p><p>1) New class of input vector can be accommodated through the proposed method by incorporating with broad learning to cope with the problems of determining the appropriate number and position of neural nodes for NN controller. 2) A novel incremental scheme of neural nodes is designed by considering the input vector and information of past nodes in network, rather than regarding the current input vector as the new neural nodes directly which seems not to be a reasonable choice <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. 3) An adaptive switching scheme is developed through a dynamic radius parameter during the backstepping design in order to ensure closed-loop global stability and make the input signs confine within the dynamic compact set in the process of broad learning. 4) The proposed algorithm, which blends deterministic learning <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> is able to improve and then reuse the learned knowledge to enable the controller acts in human-like manner. Recently, Broad Learning System, which is evolved from random vector functional-link neural networks <ref type="bibr" target="#b19">[20]</ref>, has been demonstrated its efficient approximation and generation ability in recognition and classification fields. In this framework, the input vectors of the network are described as X and the output variables are denoted as Y ∈ R N×C , where N is the number of feature mapping and C is the dimension of the network's output. Following the input vectors, the n feature mappings have been used to extract the feature of input vectors</p><formula xml:id="formula_0">F i = ζ 1 (XW f i + ϑ f i ) i = 1, • • • , n<label>(1)</label></formula><p>where ζ 1 is the transfer function, W f i and ϑ f i are the weight and width which are randomly generated at the initial stage of network and then remained constant during training.</p><p>Then, the m enhancement nodes are obtained by the feature mappings</p><formula xml:id="formula_1">H j = ζ 2 (F n W e j + ϑ e j ) j = 1, • • • , m<label>(2)</label></formula><p>where W ei and ϑ ei are the orthonormal basis vectors, ζ 2 is also the transfer function. Eventually, the output of BLS is represented as</p><formula xml:id="formula_2">Y = [F 1 , • • • , F n |ζ 2 (F n W e1 + ϑ e1 ), • • • , ζ 2 (F n W em + ϑ em )W m+n = [F 1 , • • • , F n |H 1 , • • • , H m ]W m+n = [F n |H m ]W m+n = G m+n W m+n<label>(3)</label></formula><p>For the application of classification and recognition, the testing results are usually given beforehand and the neural weight W m+n in BLS can be obtained by solving the pseudoinverse of</p><formula xml:id="formula_3">[F n |H m ], i.e. W m+n = [F n |H m ] † Y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RBFNN with Broad Learning Framework</head><p>Previous researches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref> have indicated that RBFNN can approximate any continuous function f (Ξ) with an optimal neural weight W * as long as the number of node N is large enough, such that</p><formula xml:id="formula_4">f (Ξ) = W * T S(Ξ) + ε(Ξ), ∀Ξ ∈ Ω Ξ (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where Ξ is the input vector of neural networks, Ω Ξ ⊂ R q is a compact set and |ε(Ξ)| &lt; ε * represents the estimated error for each ε * &gt; 0. Here, S(Ξ) = [s 1 (Ξ), • • • , s n (Ξ)] and the Gaussian function is used as the transform function for the input data as follows:</p><formula xml:id="formula_6">s i (Ξ) = exp[ -(Ξ -ϕ i ) T (Ξ -ϕ i ) η 2 i ] i = 1, • • • , n<label>(5)</label></formula><p>where ϕ and η denote the centers and widths of the RBFNN.</p><p>In this part, a detailed description for the development of RBFNN using broad learning framework will be given. The inspiration of the proposed framework is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. As shown in the top panel of Figs. 1, the research in <ref type="bibr" target="#b21">[22]</ref> reveals that the internal model of central nervous system (CNS) has the capability of generalization for skills transfer. Inspired by the incremental properties of BLS, the novel RBFNN with broad learning is designed to model the motor skills learning and generalization as shown in the bottom panel. The neural network is updated dynamically through the incremental node to accommodate the new input vector, i.e., the position or velocity of the new movement. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>), while the compact set Ω 0 covers the whole input vector, the neural nodes can be well activated and it may result in good approximation performance. Nevertheless, considering the Gaussian function in the basic function, it can be easily concluded that the input vector may bring an extremely low impact on it if the input vector goes beyond the initial compact set Ω 0 as shown in Fig. <ref type="figure" target="#fig_0">1(d</ref>). In such case, a new neuron will be augmented to the network in order to incorporate the new information and the original compact set Ω 0 will be extended to Ω 1 . Human's motor learning has been regarded as an outgrowth of motor control process. A more recent finding, which inspired this work, is that the novel motor skills learning is associated with the previous knowledge and current environment information <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. At this point, a novel generalized scheme allowing for incrementing the neurons of NN control is proposed in this part. Specifically, ϕ t+T is calculated by taking into account other existing neural nodes, where T is the calculation interval, rather than using the position of the input data as the neural center directly <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Assume that the n centers vectors in the existing network centers which are closest to the input vector</p><formula xml:id="formula_7">Ξ(t) are c min = {c 1 , c 2 , • • • , c n } ∈ ϕ,</formula><p>then the parameters of incremental node can be represented as</p><formula xml:id="formula_8">ϕ new = cmin + β (Ξ(t) -cmin )<label>(6)</label></formula><p>where β ∈ R q is a tunable parameter which can determine the position of new node by considering with the input vector and nearest nodes; cmin , which represents the average position to the closest centers c min , is defined as cmin =</p><formula xml:id="formula_9">∑ n i=1 c min i n</formula><p>. Hence, the new neural center vector can be defined as</p><formula xml:id="formula_10">ϕ(t + T ) = [ϕ(t) ϕ new ], i f dis(Ξ(t), c) &gt; Θ ϕ(t), otherwise<label>(7)</label></formula><p>where dis(Ξ(t), c) denotes the norm distance between input vector Ξ(t) and c, Θ is the predetermined threshold.</p><p>Inspired by character of broad learning system and considering the real-time requirement in control aspect, a trigonometric expansion scheme <ref type="bibr" target="#b25">[26]</ref> is employed to generate the corresponding enhancement nodes. Let k be the current number of RBFNN centers, the enhancement vectors can be represented as</p><formula xml:id="formula_11">H(t) = [H 1 , • • • , H i ] i = 1, • • • , k<label>(8)</label></formula><p>where</p><formula xml:id="formula_12">H i = [cos(s i (Ξ(t)), sin(s i (Ξ(t))].</formula><p>Hence, the hidden layer of the neural network is represented as</p><formula xml:id="formula_13">G(t) = [S(Ξ)|H].</formula><p>The incremental basic function and corresponding enhancement vector are written as</p><formula xml:id="formula_14">Sn(t + T ) = [S(Ξ)|S ex (Ξ)]<label>(9)</label></formula><formula xml:id="formula_15">Hn(t + T ) = [H| cos(S ex (Ξ)), sin(S ex (Ξ))] (10) W n(t + T ) = [W |W ex ]<label>(11)</label></formula><p>where S ex (Ξ) is the Gaussian function with the new center ϕ new of the incremental node. Consequently, the broad RBFNN is defined as</p><formula xml:id="formula_16">Y (Ξ) = [Sn(t + T )|Hn(t + T )]W T = Gn * W T<label>(12)</label></formula><p>Remark 1: Although the research of NN incremental learning has been noticeably increased in the past years <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, it is noted that the proposed method also has its unique innovations: 1) In order to fully consider the learned primitive motor knowledge, the parameters of new neuron are calculated related to the existing nodes of the NN; 2) the number of neural nodes is determined by the distance threshold Θ which can be specified by user; 3) enhancement nodes are employed to improve the accuracy of approximation and trigonometric expansion scheme is designed to balance the calculation burden; 4) due to the characteristic of the Gaussian function, existing neuron which is far from the new orbit will be less effected during the network training which tend to prevent the learned knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GLOBAL ADAPTIVE NEURAL NETWORK CONTROLLER DESIGN</head><p>Another focus of this work is to equip robots with the capability of skills learning and generalization as human motor system. In this section, the design of the neural adaptive controller integrated with generalize properties for motor skills learning is elaborated by means of the developed broad RBFNN which is discussed above. Fig. <ref type="figure" target="#fig_1">2</ref> shows the control diagram of the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prescribed Tracking Performance</head><p>The dynamic equations of an n-joint rigid manipulator based on Lagrange-Euler form which can be depicted by <ref type="bibr" target="#b26">[27]</ref> is described as</p><formula xml:id="formula_17">M x (q) ẍ +C x (q, q) ẋ + G x (q) = τ x<label>(13)</label></formula><p>In general, the tracking error vector in the task space is designed as</p><formula xml:id="formula_18">z 1 (t) = x(t) -x d (t) z 2 (t) = q -α<label>(14)</label></formula><p>where x d (t) ∈ Ω d , ∀t ≥ 0 is the desired trajectory defined in task space, q stands for the joint velocity, α is the virtual controller which is defined as <ref type="bibr" target="#b19">(20)</ref>, z 1 denotes the tracking error in task space and z 2 represents the velocity tracking error of the manipulator in joint space.</p><p>In order to guarantee the prescribed tracking performance which can be achieved by evolving the tracking error z 1 (t) within a specified area, an exponential decaying performance function <ref type="bibr" target="#b27">[28]</ref> is first considered as</p><formula xml:id="formula_19">ν i (t) = (ν i0 -ν i∞ ) exp(-m i t) + ν i∞ (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>where ν i0 , ν i∞ and m i , i = 1, • • • , N d are the predefined positive constants. N d is the manipulator's degree of freedom.</p><p>Hence, the prescribed tracking performance can be described by the following inequalities:</p><formula xml:id="formula_21">-β a,i ν i (t) &lt; z 1 (t) &lt; β b,i ν i (t)<label>(16)</label></formula><p>where β a and β b are positive constants which can achieve the transient and steady-state performance integrated with ν i0 , ν i∞ and m i during the controller design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global NN Controller Design with BLF and Backstepping</head><p>In this section, the principle of the controller design is given by the following form which is based on the backstepping method with an asymmetric barrier Lyapunov function (BLF) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_22">V 1 = N d ∑ i=1 ( h i (z 1 ) 2 ln 1 1 -δ 2 b,i + 1 -h i (z 1 ) 2 ln 1 1 -δ 2 a,i )<label>(17)</label></formula><p>where</p><formula xml:id="formula_23">δ a,i = z 1 (i) φ a,i = z 1 (i) -β a,i ν i , δ b,i = z 1 (i) φ b,i = z 1 (i) β b,i ν i , and h i (z 1 ) is defined as h i (z 1 ) = 1 z 1 0 0 otherwise<label>(18)</label></formula><p>Then, the derivative of V 1 with respect to t is given by</p><formula xml:id="formula_24">V1 = P T ż1 + Π = P T (J(q)(z 2 + α) -ẋd ) + Π<label>(19)</label></formula><p>where</p><formula xml:id="formula_25">P = [p(1), p(2), • • • , p(N d )] with p(i) = χ 2 i (1-χ 2 i )z 1 (i) de- notes the transient control vector, Π = ∑ N d i=1 ( (1-h i )δ 2 a,i (1-δ 2 a,i ) φa,i φ a,i + (h i )δ 2 b,i (1-δ 2 b,i ) φb,i φ b,i</formula><p>). The virtual control law can be designed as</p><formula xml:id="formula_26">α = J † (q)( ẋd -K 1 z 1 -L(t)z 1 )<label>(20)</label></formula><p>where</p><formula xml:id="formula_27">J † = J T (JJ T ) -1 denotes the Moore-Penrose inverse of J(q), K 1 = diag (k 11 , k 12 , • • • , k 1N d ) denotes the positive control vector. And L(t) = diag(l 1 (t), l 2 (t), • • • , l N d (t)) with l i (t) = ( φ a,i φ a,i ) 2 + ( φ b,i φ b,i</formula><p>) 2 + c i , where c i is also a positive constant to guarantee the boundedness of αi while φa,i and φb,i tend to be zero. Substituting ( <ref type="formula" target="#formula_26">20</ref>) into ( <ref type="formula" target="#formula_24">19</ref>) yields</p><formula xml:id="formula_28">V1 = P T J(q)z 2 -P T (K 1 z 1 + L(t)z 1 ) + Π<label>(21)</label></formula><p>Considering the inequality described below:</p><formula xml:id="formula_29">l i (t) -h i φa,i φ a,i -(1 -h i ) φb,i φ b,i 0 (<label>22</label></formula><formula xml:id="formula_30">)</formula><p>it can be obtained that V1 -</p><formula xml:id="formula_31">N d ∑ i=1 k 1 (i) χ 2 i (1 -χ 2 i ) + P T J(q)z 2 ; (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>where</p><formula xml:id="formula_33">χ i = h i δ b,i + (1 -h i )δ a,i .</formula><p>Next, we choose a positive definite Lyapunov function:</p><formula xml:id="formula_34">V 2 = V 1 + 1 2 z T 2 M(q)z 2<label>(24)</label></formula><p>In terms of ( <ref type="formula" target="#formula_17">13</ref>) , ( <ref type="formula" target="#formula_18">14</ref>) and property 1, 2, the derivative of V 2 becomes</p><formula xml:id="formula_35">V2 = V1 + 1 2 z T 2 Ṁ(q)z 2 + z T 2 M(q)ż 2 = V1 + z T 2 (τ -G(q) -C(q, q)α -M(q) α) = V1 + z T 2 (τ + F(Ξ))<label>(25)</label></formula><p>where Ξ = [q, q, α, α] T and F(Ξ) is an approximation function because the dynamic model of M(q), C(q, q) and G(q) are unknown. Here, the incremental RBFNN (12) which has been discussed above is used to estimate the unknown function.</p><p>Inspired by the recently published work <ref type="bibr" target="#b30">[31]</ref>, the smooth switching function</p><formula xml:id="formula_36">E(Ξ) = diag(E 1 (Ξ), E 2 (Ξ), • • • , E N d (Ξ)) is developed, where E(Ξ) = ∏ 4N d k=1 e(Ξ(t)) with the format of e(Ξ(t)) =      1 |Ξ(t)| &lt; d a d 2 b -Ξ 2 (t) d 2 b -d 2 a exp( Ξ 2 (t)-d 2 a ω i (d 2 b -d 2 a ) ) 2 otherwise 0 |Ξ(t)| &gt; d b<label>(26)</label></formula><p>where d a = ε a * q f , d b = ε b * q f . Here q f denotes the radius of the farthest neural node from the origin of the coordinates. ε a and ε b denote the distance threshold. Hence, the global adaptive incremental NN controller is designed as</p><formula xml:id="formula_37">τ x = -K 2 z 2 -P T J(q) -E(Ξ)ϒ a -(1 -E(Ξ))ϒ b<label>(27)</label></formula><p>where K 2 is a positive definite vector, ϒ a , ϒ b are neural control term and robust control term respectively which are designed as</p><formula xml:id="formula_38">ϒ a = Ŷ (Ξ)<label>(28)</label></formula><formula xml:id="formula_39">ϒ b = Y U (Ξ)R( Y U (Ξ)z 2 ω ) (<label>29</label></formula><formula xml:id="formula_40">)</formula><p>where Ŷ is the approximation of equation <ref type="bibr" target="#b11">(12)</ref>,</p><formula xml:id="formula_41">Y U (Ξ) = diag{y U 1 (Ξ), y U 2 (Ξ), • • • , y U N i (Ξ)}. It is noted that R( Y U (Ξ)z 2 ω ) = [tanh( y U 1 (Ξ)z 2 (1)</formula><p>ω( <ref type="formula" target="#formula_0">1</ref>) ), tanh(</p><formula xml:id="formula_42">y U 2 (Ξ)z 2 (2) ω(2) ), • • • , tanh( y U N d (Ξ)z 2 (N i ) ω(N i )</formula><p>)] T where ω is a positive parameter.</p><p>Eventually, the learning law of incremental NN is designed as</p><formula xml:id="formula_43">˙ W n = ∆(Ez 2 Sn(Ξ) -γ W n) (<label>30</label></formula><formula xml:id="formula_44">)</formula><p>where ∆ and γ are positive constant matrices.</p><p>Remark 2: ϒ a and ϒ b are two important parts in the incremental global adaptive NN controller <ref type="bibr" target="#b26">(27)</ref>. As shown in Fig. <ref type="figure">3</ref>, the minimum and maximum compact subset Ω a , Ω b are variable in terms of the defined radius d a and d b which dedicate to contain the desired compact set Ω 1 . In such case, it is worth to note that the input of broad RBFNN is always confined within the dynamic compact set such that a global tracking performance in the whole workspace can be achieved. Fig. <ref type="figure">3</ref>: The principle of incremental global NN control: solid cycles denote the initial compact subset and they will be changed to dash cycles when a new neural node is added to the network. The orange curve illustrates the process of global tracking performance. The NN controller ϒ a plays a leading role when the input vector lies in domain Ω a . Instead, it will be displaced by the robust controller ϒ b once the input vector runs out of domain Ω b and it will be forced to run back. If the input vector lies in between Ω a and Ω b , both controllers will work which are aimed to constrain the input run in the desired compact set Ω 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning From Accumulated Knowledge</head><p>Lemma 1: <ref type="bibr" target="#b31">[32]</ref> By taking into account the PE condition <ref type="bibr" target="#b8">[9]</ref> with the closed-loop control system (13), the control torque <ref type="bibr" target="#b26">(27)</ref> and the incremental neural learning law <ref type="bibr" target="#b29">(30)</ref>, the neural estimated weights Ŵ can converge to the true or optimal weights W * for any recurrent orbit Ξ(t) with the satisfaction of PE and the approximation function (4) can be represented by Ŵ T S(Ξ) or W T S(Ξ):</p><formula xml:id="formula_45">Y (Ξ) = Ŵ T S(Ξ) + ε(Ξ) = W T S(Ξ) + ε(Ξ)<label>(31)</label></formula><p>where W = mean t∈[t a ,t b ] Ŵ (t) represents the time segment after transient process with t b &gt; t a &gt; T 0 , T 0 denotes a finite time.</p><p>In terms of Lemma 1, the training knowledge of the recurrent orbits Ξ i (t) can be stored in Wi which can be reused for the similar control tasks even the new recurrent movements. Here, i denotes the training process of the ith action. This allows to achieve the motor generalization via the accumulated knowledge. Specifically, the initial neural weights Ŵi+1 (0) for the next new training process can be set as Wi as long as the manipulator works in the same workspace.</p><p>Remark 3: Compared with the conventional adaptive NN control system and deterministic learning control <ref type="bibr" target="#b5">[6]</ref>, the superiority of the proposed controller is that the accumulated knowledge can not only be reused for the similar periodic orbit, but also be used to train other periodic orbits in the same workspace without retuning the controller parameters, which can enhance the extensibility and convenience in design of the controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. STABILITY ANALYSIS</head><p>In this part, the stability of broad learning neural control system is rigorously established and the principle of how to determine the controller parameters in terms of Lyapunov theory is elaborated.</p><p>Theorem 1: With the consideration of the manipulator system (13), combined with the filtered error model ( <ref type="formula" target="#formula_18">14</ref>), control toque <ref type="bibr" target="#b26">(27)</ref>, neural weight learning law <ref type="bibr" target="#b29">(30)</ref> and the incremental scheme ( <ref type="formula" target="#formula_8">6</ref>), <ref type="bibr" target="#b6">(7)</ref>, for any periodic orbit x d , whose initial condition is x d (0) ∈ Ω 0 , the proposed learning scheme both at training stage or incremental stage can guarantee that all signals in the manipulator control system are uniformly ultimately bounded (UUB) by choosing the suitable design parameters of the system.</p><p>Proof: Choose a Lyapunov function as</p><formula xml:id="formula_46">V = V 2 + 1 2 N i ∑ i=1 W n T i Θ -1 W n i (<label>32</label></formula><formula xml:id="formula_47">)</formula><p>Computing the derivative of ( <ref type="formula" target="#formula_46">32</ref>) along ( <ref type="formula" target="#formula_22">17</ref>), ( <ref type="formula" target="#formula_34">24</ref>) and simplifying yield</p><formula xml:id="formula_48">V = V2 + N i ∑ i=1 W n T i Θ -1 ˙ W n i = V1 + z T 2 (τ +Y (Ξ)) + N i ∑ i=1 W n T i Θ -1 ˙ W n i = V1 -K 2 z T 2 z 2 -P T Jz T 2 + N i ∑ i=1 z 2 (i)[-E i ( W n T i -W n * T i )Sn i (Ξ) +E i W n T i Sn i (Ξ) + ε(i)] + N i ∑ i=1 z 2 (i)(1 -E i )(y i -y U i (Ξ) tanh( y U i (Ξ)z 2 (i) ω(i) )) + N i ∑ i=1 [-γ(i) W n T i (W n * i + W n i )]<label>(33)</label></formula><p>And it is easy to have the following inequalities in terms of the Young's inequality:</p><formula xml:id="formula_49">y i z 2 (i) -y U i z 2 (i) tanh( y U i z 2 (i) ω(i) ) ιω(i)<label>(34)</label></formula><p>V</p><formula xml:id="formula_50">N d ∑ i=1 [-K 1 (i) χ 2 i (1 -χ 2 i ) ] + N i ∑ i=1 [-(K 2 (i) - 1 2 )z 2 2 (i) - 1 2 γ(i) Wi Wexi 2 ] + N i ∑ i=1 [ 1 2 γ(i) W * i W * exi 2 + 1 2 ε 2 (i) + ιω(i)]<label>(35)</label></formula><p>Considering the inequality</p><formula xml:id="formula_51">- χ 2 i (1 -χ 2 i ) -ln 1 (1 -χ 2 i ) ∀|χ i | &lt; 1 (<label>36</label></formula><formula xml:id="formula_52">)</formula><p>it can be obtained that</p><formula xml:id="formula_53">V N d ∑ i=1 [-K 1 (i) ln 1 (1 -χ 2 i ) ] + N i ∑ i=1 [-K 3 (i)z 2 2 (i) - 1 2 γ(i) Wi Wexi 2 ] + N i ∑ i=1 [ 1 2 γ(i) W * i W * exi 2 + 1 2 ε 2 (i) + ιω(i)] (<label>37</label></formula><formula xml:id="formula_54">)</formula><p>where</p><formula xml:id="formula_55">K 3 = K 2 -1 2 .</formula><p>In accordance with ( <ref type="formula" target="#formula_22">17</ref>), ( <ref type="formula" target="#formula_34">24</ref>) and ( <ref type="formula" target="#formula_46">32</ref>), it follows that</p><formula xml:id="formula_56">V = Q + 1 2 z T 2 M(q)z 2 + 1 2 N i ∑ i=1 W T i Θ -1 Wi (<label>38</label></formula><formula xml:id="formula_57">)</formula><p>where</p><formula xml:id="formula_58">Q = ∑ N d i=1 (ln 1 1-χ 2 i</formula><p>). Hence, inequality (37) can be rewritten as</p><formula xml:id="formula_59">V (t) = η s V (t) + µ s (<label>39</label></formula><formula xml:id="formula_60">)</formula><p>where</p><formula xml:id="formula_61">η s = min{λ min (K 1 ), 2λ min (K 3 ) λ (M(q)) , γ λ max (Θ -1) },µ s = ∑ N i i=1 ( 1 2 W * i 2 + 1 2 ε 2 (i) + ιω(i))</formula><p>. Assume that β s = µ s η s and it follows from (38)(39) that</p><formula xml:id="formula_62">V (t) (V (0) -β s )exp(-η s t) + β s V (0) + β s<label>(40)</label></formula><p>It be concluded from ( <ref type="formula" target="#formula_46">32</ref>) and (39) that V is definitely negative. Thus it can be obtained that the filter error z 2 as well as the weight approximation errors Ŵ are bounded. According to the definition ( <ref type="formula" target="#formula_19">15</ref>) and ( <ref type="formula" target="#formula_21">16</ref>), it can be also concluded that φ a and φ b are bounded which implies that the tracking error z 1 is bounded and the predefine tracking performance is satisfied. Hence the state x and J(q) are bounded. Consequently, all the signals in the manipulator control system remain UUB during the training and incremental stage. In this way, the system is stable in the sense of Lyapunov both in the learning stage and incremental stage. This completes the proof.</p><p>V. SIMULATION STUDIES In order to test the validness of the developed broad learning enabled adaptive neural control method with application on modelling motor skills generalization, a set of simulation with the same scenarios studied in <ref type="bibr" target="#b3">[4]</ref> is performed under the external force field by utilizing a two-joint manipulator as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The parameters of manipulator are designed as mass of links m 1 = m 2 = 10.0kg, length of links l 1 = l 2 = 1.0m, inertia of links I 1 = I 2 = 0.83kgm 2 and gravity acceleration g = 9.8m/s 2 .</p><p>During the training process, a set of basic training actions with four straight line movements spanning a range of 360 • in an ellipse is defined. After training the predefined movements, the learning knowledge will be used to control the manipulator to draw the ellipse which is defined as (x-1) 2 0.09 + y 2 0.04 = 1 in the same workspace. In this simulation, the parameters of the predefined tracking performance are Aiming at providing convincing comparison results, RBFNN, FNN <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b34">[35]</ref> and proposed method are used to complete the same fundamental skills training and knowledge reusing with the same parameters set designed above. Furthermore, 2 8 = 256 nodes are specified for both RBFN-N and FNN whose centers are located at the space of</p><formula xml:id="formula_63">Φ 1 = [-1, 1] × [-1, 1] × [-1, 1] × [-1, 1] × [-1, 1] × [-1, 1] × [-1, 1] × [-1, 1]</formula><p>. The switching radius d a and d b will be constant with 5 and 20 respectively in this case. Instead of specifying the neuron centers beforehand which are notoriously hard to design approximately, the proposed method only need to design the center distance threshold Θ which is specified as Θ = 20, 30, 50 in this simulation.</p><p>The simulation results of tracking performance are illustrated in Fig. <ref type="figure">5</ref>. It can be seen that the overall tracking by using the proposed method is superior to the conventional adaptive control. Moreover, the tracking error z 1 corresponding to the RBFNN, FNN and proposed method with different distance threshold is shown in Fig. <ref type="figure">6</ref>. From these simulations, it can be objectively concluded that by applying the broad learning enabled adaptive NN control <ref type="bibr" target="#b26">(27)</ref> with the incremental adaptive law <ref type="bibr" target="#b29">(30)</ref>, the manipulator can draw the ellipse with higher accuracy and fewer oscillations in use of the accumulated knowledge. Notably, the predetermined distance threshold Θ is of the most importance parameter to effect the structure of the neural network since it determines whether the new node is ought to be added or not. It is clear that the suitable distance Θ it uses, the tracking error can become smaller whereas it will lead to more oscillatory at the early stage.</p><p>In addition, the number of used neural nodes in incremental neural network during the convergence to optimal neural weights is shown in Fig. <ref type="figure">7</ref>. In Fig. <ref type="figure">7</ref>, it is clear that the number of neurons grows fast with the decreasing of the distance threshold Θ.</p><p>In the following, Figs. 8-10 show the simulation results for partial weights convergence of the incremental neural network with the corresponding distance threshold Θ. The compared results show that only partial weights converge to relative large values, whereas others are remained a small neighbourhood of zero. This is because the large estimated weights of incremental neural network, whose center points are near to the input orbit, can be activated and will be converged to the true value W * . However, the neuron weights which are far away from the input vector are not activated and will stay at a small range of zero. Obviously, these results are consistent with the requirement of the partial PE condition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT AND DISCUSSION</head><p>In this section, the effectiveness of proposed control method will be evaluated by implementation on a 7-DOF Baxter robot arm which provides a variety of interfaces to get access to the real-time state of manipulator through a standard robot operating system (ROS). Moreover, it is also convenient for researchers to utilize different control strategies, i.e., position, velocity or torque control, to perform specific tasks.</p><p>In the experiment, only a single arm is employed. Moreover, in order to denote the payload and simulate the disturbance, an object with 1.5 kg in weight has been attached but not fixed on the end-effector. As shown in Fig. <ref type="figure" target="#fig_0">11</ref> The tracking error of the 9 trajectories by using proposed method in phase III are shown in Fig. <ref type="figure" target="#fig_6">12</ref>. From the results, it can be concluded that the predefined tracking performance which has used the proposed controller can be achieved, while obvious oscillatory during the whole process is occurred for the RBFNN controller. This is because the training experience in phase II, which is far from the compact set Ω 1 , may bring less effect to the knowledge reuse at phase III. Furthermore, it should be emphasized that the number of neural nodes which are incremented during the broad learning in RBF has been reduced sharply with 58 nodes in phase I and 72 nodes in phase II respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, a novel neural adaptive control using broad learning system based on deterministic learning is proposed. The incremental RBF neural network, which can adjust and augment the neural node dynamically, is studied to estimate the unknown model of manipulator dynamic. During the training and knowledge reusing stage, the tracking error can satisfy the predefine tracking performance. It has been proven that the closed-loop system can be remained global stable by using the incremental adaptive controller. Simulation and experiment results have shown that the proposed framework is valid for motor generalizing learning and can use the accumulated knowledge to fulfil new tasks under the dynamic external disturbance environment. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Top panel: motor behaviors in skills learning and transfer [22]. (a) is 8 reaching movements that human need to adapt with. (b) is cycle-drawing movement which is generalized by the training knowledge from (a). Bottom panel: RBFNN learning principle, from conventional learning (c) to broad learning with incremental nodes (d).</figDesc><graphic coords="2,328.56,420.41,218.40,166.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Control diagram of adaptive neural controller using broad learning system.</figDesc><graphic coords="3,314.64,274.42,246.23,82.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Simulation setup for broad learning enabled adaptive neural control: a 2-DOF manipulator is required to complete the reaching and cycle-drawing movements under the force field disturbance.</figDesc><graphic coords="6,364.41,67.68,146.70,90.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :Fig. 6 :Fig. 7 :</head><label>567</label><figDesc>Fig. 5: Simulation results of trajectory tracking for proposed algorithm and knowledge reusing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a) W 1 Fig. 8 :</head><label>18</label><figDesc>Fig. 8: Partial weights convergence with Θ = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :Fig. 10 :Fig. 11 :</head><label>91011</label><figDesc>Fig. 9: Partial weights convergence with Θ = 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Tracking error of proposed algorithm and RBFNN control during knowledge reusing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2950853, IEEE Transactions on Industrial Electronics damentals problems related to the RBF and BLS framework are first explored in Section II. Next, an elaborate presentation about the principle of designing the global adaptive neural network controller is given in Section III and stability demonstration in Section IV. Numerical simulation and experiments which are aimed at verifying the effectiveness and efficiency of the proposed scheme are performed in Section V and</figDesc><table><row><cell>Section VI, respectively. Finally, a conclusion is given in</cell></row><row><cell>Section VII.</cell></row><row><cell>II. PROBLEM FUNDAMENTALS</cell></row><row><cell>A. Broad Learning System</cell></row></table><note><p>The rest of this paper is organized as follows. Several fun-0278-0046 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China grant under number 61861136009, 61811530281, 61702195, 61751202, U1813203, 61572540 and U1801262</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0278-0046 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stimulation of the human motor cortex alters generalization patterns of motor learning</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D X</forename><surname>Jj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Marko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Pekny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Celnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shadmehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="7102" to="7110" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biomimetic motor behavior for simultaneous adaptation of force, impedance and trajectory in interaction tasks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albu-Schffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2705" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-like adaptation of force and impedance in stable and unstable interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haddadin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albu-Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalization in adaptation to stable and unstable dynamics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kadiallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e45075</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive neural network control of robot manipulators in task space</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Woon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="746" to="752" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from neural control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deterministic learning of nonlinear dynamical systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Bifurcation &amp; Chaos</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="1307" to="1328" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deterministic learning based adaptive network control of robot in task space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="806" to="815" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Persistency of excitation in identification using radial basis function approximants</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kurdila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Narcowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Control &amp; Optimization</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="625" to="642" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural learning control of marine surface vessels with guaranteed transient tracking performance</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1717" to="1727" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic learning from adaptive neural control of robot manipulators with prescribed performance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man &amp; Cybernetics Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2255" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Broad learning system: Feature extraction based on k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information, Cybernetics and Computational Social Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Broad learning system: An effective and efficient incremental learning system without the need for deep architecture</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks &amp; Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="24" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global neural dynamic surface tracking control of strict-feedback systems with application to hypersonic flight vehicle</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks &amp; Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2563" to="2575" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global tracking control of strict-feedback systems using neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks &amp; Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1714" to="1725" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prescribed performance tracking for flexible joint robots with unknown dynamics and variable elasticity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Kostarigka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Doulgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rovithakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1137" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Direct adaptive controller for nonaffine nonlinear systems using selfstructuring neural networks</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-T</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="414" to="422" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self adaptive growing neural network classifier for faults detection and diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Druaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mustapha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3865" to="3876" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Barrier function-based neural adaptive control with locally weighted learning and finite neuron self-growing strategy</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal approximation using radialbasis-function networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The motor system does not learn the dynamics of the arm by rote memorization of past experience</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Conditt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Mussa-Ivaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="554" to="560" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motor sequence learning-induced neural efficiency in functional brain connectivity</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Huppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Wollam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Sparto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sejdić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vanswearingen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural brain research</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neuropsychological theory of motor skill learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Willingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">558</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive representation of dynamics during learning of a motor task</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shadmehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Mussa-Ivaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience the Official Journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3208</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Pt 2</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identification of nonlinear dynamic systems using functional link artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern B Cybern</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="262" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adaptive Neural Network Control of Robotic Manipulators</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>WORLD SCIENTIFIC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust adaptive control of feedback linearizable mimo nonlinear systems with prescribed performance</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Bechlioulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rovithakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2090" to="2099" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Control of nonlinear systems with time-varying output constraints</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Tee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2511" to="2516" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Barrier Lyapunov Functions for the control of output-constrained nonlinear systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Tee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global neural dynamic surface tracking control of strict-feedback systems with application to hypersonic flight vehicle</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2563" to="2575" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identification and learning control of ocean surface ship using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="801" to="810" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fuzzy approximation-based adaptive backstepping optimal control for a class of nonlinear discretetime systems with dead-zone</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design of fuzzy-neural-networkinherited backstepping control for robot manipulator including actuator dynamics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muthusamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="709" to="722" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive fuzzy neural network control for a constrained robot using impedance learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks &amp; Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1174" to="1186" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
