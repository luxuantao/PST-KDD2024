<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inter-layer Scheduling Space Definition and Exploration for Tiled Accelerators</title>
				<funder ref="#_NydEdJX">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_HG88QE5">
					<orgName type="full">Key Research and Development Program of Shaanxi</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Cai</surname></persName>
							<idno type="ORCID">0009-0003-7560-8141</idno>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuotong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kaisheng</forename><forename type="middle">2023</forename><surname>Ma</surname></persName>
							<email>kaisheng@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inter-layer Scheduling Space Definition and Exploration for Tiled Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3579371.3589048</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scheduling</term>
					<term>inter-layer scheduling</term>
					<term>neural networks</term>
					<term>tiled accelerators</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the continuous expansion of the DNN accelerator scale, interlayer scheduling, which studies the allocation of computing resources to each layer and the computing order of all layers in a DNN, plays an increasingly important role in maintaining a high utilization rate and energy efficiency of DNN inference accelerators. However, current inter-layer scheduling is mainly conducted based on some heuristic patterns. The space of inter-layer scheduling has not been clearly defined, resulting in significantly limited optimization opportunities and a lack of understanding on different inter-layer scheduling choices and their consequences.</p><p>To bridge the gaps, we first propose a uniform and systematic notation, the Resource Allocation Tree (RA Tree), to represent different inter-layer scheduling schemes and depict the overall space of inter-layer scheduling. Based on the notation, we then thoroughly analyze how different inter-layer scheduling choices influence the performance and energy efficiency of an accelerator step by step. Moreover, we show how to represent existing patterns in our notation and analyze their features. To thoroughly explore the space of the inter-layer scheduling for diverse tiled accelerators and workloads, we develop an end-to-end and highly-portable scheduling framework, SET. Compared with the state-of-the-art (SOTA) opensource Tangram framework, SET can, on average, achieves 1.78? performance improvement and 13.2% energy cost reduction simultaneously. Moreover, the SET framework will be open-sourced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A wide range of deep neural networks (DNNs) has been created to solve real-world problems in many fields, such as image recognition <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b32">31]</ref>, object detection <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b45">44]</ref>, and natural language processing <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21]</ref>. With the pursuit of higher accuracy and better adaptability to complex scenarios, DNNs are becoming deeper, and their structures are becoming more complex <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Under this trend, many large-scale accelerators, whose scales range from dozens of square millimeters to even a whole wafer, have been developed to accelerate the inference period of such increasingly complex DNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b57">56]</ref>. Considering that a single large hardware-tile (HW-tile) suffers low utilization and energy efficiency, these large-scale accelerators mainly employ the tiled architecture <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b57">56]</ref>, where each HW-tile includes a Processing Element (PE) array and a global buffer and is interconnected by a Network on Chip (NoC).</p><p>However, the tiled architecture itself cannot easily guarantee high utilization and energy efficiency. How to efficiently translate massive computing and storage resources into actual performance is still an open challenge <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b62">61]</ref>. The critical point of solving this challenge depends on scheduling, which can be categorized into intra-layer scheduling and inter-layer scheduling.</p><p>Intra-layer scheduling studies how to map a single layer onto one or several HW-tiles. Its space has been depicted by many notations <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b61">60]</ref> and explored by various methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">28]</ref>.</p><p>Inter-layer scheduling studies how to schedule the computing order and resource allocation of all layers in a DNN on an accelerator, which also significantly influences the energy efficiency and performance of the accelerators. For example, Fused-layer <ref type="bibr" target="#b2">[3]</ref> allocates different computing resources for different layers and orchestrates the layers in a layer-pipeline (LP) manner, achieving a 95% reduction in DRAM access compared with a basic-version layersequential (LS) pattern which computes layers one after another. Moreover, the larger the accelerator scale, the greater the impact of inter-layer scheduling. For example, Tangram, which proposes optimized LP and LS techniques <ref type="bibr" target="#b17">[17]</ref>, achieves 67% energy savings on a 32?32-tile accelerator, compared to less than 20% savings on a 4?4-tile accelerator.</p><p>Although inter-layer scheduling plays an increasingly important role in keeping tiled accelerators highly utilized and energyefficient, there is a significant deficiency in its research: Most works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b63">62]</ref> continue to optimize existing heuristic patterns, LP and LS, but do not propose new patterns, let alone define the space of inter-layer scheduling on tiled accelerators clearly and systematically. The lack of a clear definition of the inter-layer scheduling space significantly limits the opportunities for optimizing the performance and energy efficiency of tiled accelerators. Moreover, the lack of a systematic definition hinders people from understanding how different inter-layer scheduling choices influence different hardware behaviors and how these behaviors further influence the energy efficiency and performance of an accelerator. These challenges motivate us to make the following contributions:</p><p>? We introduce a uniform and systematic Resource Allocation Tree notation to depict the space of inter-layer scheduling for inferencing DNNs with various structures on tiled accelerators with various architectures. The notation includes Temporal Cut, which allocates the same HW-tile group and different computing time intervals to each of its children, and Spatial Cut, which allocates different HW-tile groups to each child of it. Each RA Tree is a hierarchical organization of Cuts and Leaf Nodes (layers of the DNN). Then, we elaborate on how to parse the tree structure into the corresponding resource allocation scheme and the flow of data among the HW-tiles. To the best of our knowledge, we are the first to define the space of inter-layer scheduling clearly and systematically. ? Based on the RA Tree notation, we thoroughly analyze how different inter-layer scheduling choices affect hardware behaviors and how these behaviors affect accelerator performance and energy efficiency. Moreover, we represent existing LS and LP patterns in our notation and analyze their features. ? Combining the above, we develop an end-to-end and highlyportable scheduling framework, SET, to automatically explore the whole DNN scheduling space for tiled accelerators. To explore the vast newly-defined inter-layer scheduling space efficiently, we equip SET with a Simulated-Annealing-based algorithm with 6 specifically designed operators. SET can be ported to various tiled accelerators with minor modifications, featuring good portability.</p><p>We have developed an end-to-end SET deploying flow for our test chip. The framework is available at https://github.com/SET-Scheduling-Project/SET-ISCA2023.   The two spaces for scheduling DNNs on the tiled accelerators, including undefined inter-layer scheduling space and well-defined intra-layer scheduling space (the inclusion relationship between the intra-layer scheduling spaces defined by different notations is drawn according to Tenet <ref type="bibr" target="#b38">[37]</ref>).</p><p>13.2% energy cost reduction simultaneously. In addition, we conduct a number of experiments on different DNNs, batch sizes, and hardware platforms to demonstrate the universality of SET and the effect of exploring the newly-defined space over existing LS and LP scheduling patterns. Moreover, we leverage SET to analyze the characteristics of LS and LP and reveal the features of the inter-layer scheduling space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION 2.1 Tiled Accelerators</head><p>In recent years, "Tiled Accelerators" (also known as "Spatial Accelerators") have become very popular in DNN acceleration. Many tiled DNN accelerators have been developed by industry <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b60">59]</ref> and academia <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b64">63]</ref>. Based on these existing tiled accelerators, we aim to abstract out a basic hardware requirement (or hardware template) to support our inter-layer scheduling research, as has been well done in intra-layer scheduling <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b61">60]</ref>.</p><p>Before delving into the hardware requirements of inter-layer scheduling, we can first abstract out the distinct hardware behaviors under inter-layer scheduling compared to intra-layer scheduling. These behaviors include (1) the parallel execution of different layers on different HW-tiles, which necessitates independent control of each HW-tile and requires efficient support for concurrent access to DRAMs; and (2) the producer-consumer data communication between HW-tiles or HW-tile and DRAM, which requires a flexible NoC and efficient support for concurrent access to buffers. While most existing tiled accelerators have supported the inter-layer scheduling behaviors mentioned above efficiently <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b65">64]</ref>, their architecture details, such as control logic, NoC architecture, etc., vary due to different design goals and considerations. Nevertheless, from the inter-layer scheduling perspective, we can clearly identify the common features of these accelerators. Based on these common features, we give the definition of the tiled accelerator mentioned in this paper, which is also the primary hardware requirement of our notation, as follows:</p><p>? The basic components of a tiled accelerator include an NoC, some HW-tiles, DRAM PHY and controllers, and other IOs (Fig. <ref type="figure" target="#fig_1">1 (b)</ref>).</p><p>The NoC should be able to connect the HW-tiles, DRAMs, and other components. Moreover, each HW-tile can access arbitrary global buffers of other HW-tiles or the DRAMs of the accelerator through NoC. ? The basic components of a HW-tile include a unified buffer, an NoC router, and PEs. A HW-tile can be assigned to different layers at different times but cannot compute workloads for multiple layers simultaneously. What is worth mentioning is that a HWtile is not equivalent to a core in our definition. Any computing unit that meets the above definition, such as a Simba chiplet <ref type="bibr" target="#b47">[46]</ref> in a package, can be viewed as a HW-tile. In summary, our RA Tree notation is applicable to all tiled accelerators satisfying the above basic requirement without specifying intra-tile or NoC architectures, which is where SET portability comes from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scheduling for Tiled Accelerators</head><p>Scheduling plays a crucial role in deploying DNNs on accelerators with high utilization and energy efficiency. We categorize DNN scheduling into inter-layer and intra-layer scheduling, which share a top-down relationship. When we schedule a DNN onto a target tiled accelerator, the inter-layer scheduling first decides the computing order and resource allocation of each layer, and then the intra-layer scheduling decides how to map the layer on the allocated HW-tile group.</p><p>Specifically, intra-layer scheduling studies how to map a layer on parallel computing units spatially, tile workloads into small pieces to fit the capacity of each-level memory, and control the computing order of each small workload. A lot of research has been done in this field <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b59">58]</ref>. If we study the development of this field carefully, we can find that, in the early days, many heuristic intra-layer scheduling patterns are proposed, such as output stationary (OS) pattern <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14]</ref>, weight stationary (WS) pattern <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b42">41]</ref>, input stationary (IS) pattern <ref type="bibr" target="#b44">[43]</ref>, and row stationary (RS) pattern <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12]</ref>. We call this stage the "Heuristic Period". With the deepening of researchers' understanding, intra-layer scheduling moves into the "Automation Period". In this period, various notations, such as Computationcentric notation <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b61">60]</ref>, Data-centric notation <ref type="bibr" target="#b33">[32]</ref>, and relationcentric notation <ref type="bibr" target="#b38">[37]</ref>, are proposed to specify the overall space of intra-layer scheduling and explore it thoroughly (Fig. <ref type="figure" target="#fig_1">1(c)</ref>).</p><p>In inter-layer scheduling, there mainly exist two heuristic patterns, layer-sequential and layer-pipeline. LS employs all computing resources to process each layer one after another, while LP allocates different groups of HW-tiles to different layers and orchestrates these layers in a pipeline manner. Existing works either optimize LS or LP themselves <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b63">62]</ref> or optimize them for various applications <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b56">55]</ref> or hardware <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b64">63]</ref>. However, if we compare the development of inter-layer scheduling with intralayer scheduling, we argue that inter-layer scheduling is still in the "Heuristic Period". The space of inter-layer scheduling has not been clearly defined, explored thoroughly, or fully understood. This situation motivates us to propose a notation to specify the space of inter-layer scheduling and then leverage the notation to explore the space thoroughly and analyze the influences of different inter-layer scheduling choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RA TREE DEFINITION AND ANALYSIS 3.1 RA Tree Notation</head><p>As shown in Fig. <ref type="figure" target="#fig_3">2</ref>(a), we develop a uniform notation, Resource Allocation Tree (RA Tree) to describe inter-layer scheduling schemes. In this section, we provide the definition and properties of the components in the RA Tree to demonstrate how to generate its corresponding inter-layer scheduling scheme from an arbitrary RA Tree. The generation methods of different RA Trees by our SET framework are introduced in Sec. 4.2.</p><p>3.1.1 Overall Introduction to RA Tree Notation. The central theme of RA Tree notation revolves around resource allocation. We identify two resource types: spatial resources, which are the total computing resources, and temporal resource, which is usage duration of the computing resources. Our notation is a structured and recursive notation to describe the allocation of the spatial and temporal resources for each sample within each layer, i.e., determine the number of HW-tiles and the duration of their usage for each sample within every layer.</p><p>It is worth noting that in this paper, the granularity of spatial computing resource allocation is defined at the level of HW-tiles (depicted in the bottom-left corner of Fig. <ref type="figure" target="#fig_1">1</ref> and defined in Sec. 2.1),  including all buffers and the PE array within it. However, in general, the granularity can be coarser or finer, depending on whether the computing resources can be independently controlled to compute different layers. For instance, in this paper, the PE array cannot be independently controlled and must work collectively to compute the partitioned workload of the same layer assigned to each HWtile, which is also the control granularity adopted by numerous tiled accelerators today <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b53">52]</ref>. However, if each PE within a HW-tile can be independently controlled to compute different layers, the allocation granularity of our RA Tree can be adjusted to the PE level.</p><formula xml:id="formula_0">L4 [0, 3] T 2 [2, 3] T 2 [0, 1] L3 [0, 1] L3 [2, 3]</formula><formula xml:id="formula_1">L3 [0, 1] L3 [2, 3] L1 [0, 1] L2 [0, 1] L1 [2, 3] L2 [2, 3]</formula><p>For a better understanding of the scheme represented by an RA Tree, we also introduce the Spatial-Temporal Graph (S-T Graph) to visually display the allocation of the two resources. The S-T Graph is used to explain the RA Tree examples in this work and is not necessary for our SET framework. The bottom graph of Fig. <ref type="figure" target="#fig_3">2(b</ref>) illustrates the S-T Graph corresponding to Fig. <ref type="figure" target="#fig_3">2(a)</ref>. The two axes correspond to the resource types: the X-axis is the timeline, and the Y-axis is the HW-tiles on the accelerator. Each colored rectangle corresponds to the processing of a layer with a specific batch of samples. The height of the rectangle indicates which HW-tiles process the layer, and the width is the time period of the processing. Each rectangle is marked with its corresponding layer and batch samples. For example, "?1[0, 1]" means the processing of layer ?1 with samples 0 to 1, a batch of size 2. 3.1.2 Basic Attributes and Encoded Information of Node. In this part, we will firstly give the symbolic definition for the RA Tree, then explain the terms and rules in the definition.</p><p>Denote ? as the set of layers in the DNN model, ??? as the set of HW-tiles on the accelerator, ? ??? as the number of a sample in a batch of the workload (also denoted as the total batch size). An RA Tree ? is a set of Nodes:</p><formula xml:id="formula_2">? = {? 1 , ? 2 , . . . , ? ? }</formula><p>Where each Node ? ? is in the form of ? ? = (???? ? , ? ? , ? ? ,?? ? , ? ? , ?? ? , ???? ? ,?? ? )</p><formula xml:id="formula_3">Here ???? ? ? {???? , ????,???? },<label>(1)</label></formula><p>? ? ? ? is the children of ? ? in the tree structure,</p><formula xml:id="formula_4">? ? ? ?, ?? ? ? ??? ,<label>(2)</label></formula><formula xml:id="formula_5">? ? , ?? ? ? Z + , ? ? %?? ? = 0,<label>(3)</label></formula><formula xml:id="formula_6">? ??? ? = {? ? | ? ? ?, ?? ? ? ? , ? ? ? ? ? , ? ? ? ? },<label>(4)</label></formula><formula xml:id="formula_7">?? ? = {? ? | ? ? ?, ?? ? ? ? , ? ? ? ? ? , ? ? ? ? }<label>(5)</label></formula><p>(? ? ? ? means ? ? needs the ofmaps of ?)</p><p>When ???? ? = ???? :</p><formula xml:id="formula_9">? ? = ?, |? ? | = 1, ?? ? = 1<label>(7)</label></formula><p>When ???? ? ? ???? :</p><formula xml:id="formula_10">? ? ? ?, ? ? = ? ? ?? ? ? ? , ?? ? ? ? ? , ? ? = ? ? ?? ? ,<label>(8)</label></formula><formula xml:id="formula_11">?? ? = ?? ? (?? ? ? ? ? ) , ???? ? = ???? ? ? ?? ? ?? ? , ???? ? = ????<label>(9)</label></formula><p>When</p><formula xml:id="formula_12">???? ? = ????, ?? ? , ? ? ? ? ? ,?? ? ? ?? ? = ? (10) If ? ? is the root of the tree, ? ? = ?,?? ? = ??? , ? ? = ? ???</formula><p>As encoded in ???? ? , an RA Tree has three types of Nodes: Leaf, S Cut and T Cut, the latter two are collectively referred as Cuts (Formula (1)). A Leaf (non-gray rectangle in Fig. <ref type="figure" target="#fig_3">2(a)</ref>) represents the processing of a single layer (the layer in ? ? in Formula ( <ref type="formula" target="#formula_9">7</ref>)), and the layer is labeled on the Leaf (e.g. ?1 on pink rectangle in Fig. <ref type="figure" target="#fig_3">2(a)</ref>). The number of Leaves is equal to the number of layers in the DNN. A Cut is represented by a gray rectangle in an RA Tree (e.g., ? 1 , ? 1 ,? 2 in Fig. <ref type="figure" target="#fig_3">2(a)</ref>), has two levels of representation. From the perspective of its parent node, it represents the processing of all layers beneath it (? ? in Formula ( <ref type="formula" target="#formula_10">8</ref>)), which are labeled on the second line of the Cut Node. For example, from the perspective of ? 1 , ? 1 represents the processing of ?1,?2, and ?3. From the perspective of its children, it illustrates the allocation of spatial and temporal resources for them. For example, from the perspective of ? 2 and ?3, ? 1 represents the spatial HW-tile allocation scheme to them.</p><p>Each Node possesses a group of HW-tiles, referred to as a HWtile group (?? ? in Formula (3)), which are responsible for computing the workloads of the Node. We refer to the number of HW-tiles in the HW-tile group as HW-tile group size. For example, in Fig. <ref type="figure" target="#fig_3">2</ref>, the HW-tile group of ?1 contains HW-tile {1, 2} and has a size of 2, while the HW-tile group of ? 1 contains HW-tile {1, 2, 3} and has a size of 3. The strategies we use to determine the size and physical position of the HW-tile group are detailed in Sec. 3.3.2.</p><p>Each Node has its own batch size (? ? in Formula ( <ref type="formula" target="#formula_6">4</ref>)), which is the number of samples the Node process each time (represented by one colored rectangle in Fig. <ref type="figure" target="#fig_3">2(b)</ref>). For each Leaf in the figures, its batch size is marked in parenthesis (e.g., the batch size of ?1 and ?4 is 2 and 4, in Fig. <ref type="figure" target="#fig_3">2</ref>). For each Cut, one batch is divided into one or several subbatches, whose size is the batch size for each of its children. (see ? ? in Formula ( <ref type="formula" target="#formula_10">8</ref>)) The batch size ? ? and the number of subbatches ?? ? are marked in the form (? ? /?? ? ) in each Cut in all figures. A Leaf Node does not need to divide subbatches, so its number of subbatches ?? ? = 1. (Formula ( <ref type="formula" target="#formula_9">7</ref>))</p><p>Each Node also contains dependency information (Formula ( <ref type="formula" target="#formula_7">5</ref>), ( <ref type="formula" target="#formula_8">6</ref>)). If there is a dependency ? ? ? ? in the DNN, and ? ? ? ? , ? ? ? ? ? , then we construct a dependency ? ? ? ? ? in the RA Tree. Node ? ? will be recorded in ???? ? and Node ? ? will be recorded in ?? ? . The dependencies can be categorized into original dependencies in the DNN and deduced dependencies (both marked in Fig. <ref type="figure" target="#fig_3">2(a)</ref>). For example, in Fig. <ref type="figure" target="#fig_3">2(a)</ref>, there is no dependency from either ?1 or ?2 to ?3, so there is no dependency from ? 2 to ?3. However, since dependency ?1 ? ?4 exists, the deduced dependency ? 2 ? ?4 also exists. For a valid RA Tree, we require all dependencies between Leaves to be from left to right (i.e., Leaves form a topological order in the DNN) to ensure the consumer of a data processes after the producer. Notice that this requirement guarantees that all dependencies, whether original or deduced, are from left to right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Attributes of S Cut and T Cut.</head><p>In this section, we will introduce how S and T Cut allocates spatial and temporal resources for each child.</p><p>An S Cut allocates different HW-tile groups to each child (Formula ( <ref type="formula" target="#formula_11">9</ref>), <ref type="bibr" target="#b10">(10)</ref>). In an S Cut, all children process in parallel or pipeline, determined by the dependencies among them. In detail, each child processes its subbatches sequentially, and if child ? has a dependency on child ?, child ? must start at least one subbatch later to obtain the output feature maps (ofmaps) of child ?.</p><p>For example, in Fig. <ref type="figure" target="#fig_5">3</ref> LP, the S Cut above ?1 and ?2 has two subbatches with a batch size of 2, so ?1 and ?2 have to sequentially process two batches with size 2 on different HW-tile groups, and since there is a dependency from ?1 to ?2 (shown in Fig. <ref type="figure" target="#fig_3">2(a)</ref>), ?2 starts at the second batch of ?1; while in Fig. <ref type="figure" target="#fig_3">2</ref> there is no dependency from ? 2 to ?3, so the two Nodes can start in parallel (see the third graph in Fig. <ref type="figure" target="#fig_3">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)).</head><p>A T Cut allocates different usage duration of the same HW-tile for each child (Formula (9)). Each child of a T Cut is processed sequentially at the unit of a subbatch. In concrete, the first subbatch is processed by the Cut's children in order from left to right sequentially, then the second subbatch is processed, and then the third, ..., until the last one is processed.</p><p>For example, the bottom graph in Fig. <ref type="figure" target="#fig_3">2</ref>(b) illustrates the processing of the children of Cut ? 2 : the first subbatch of ?1 is processed, followed by the first subbatch of ?2, and then the second subbatch of ?1 and ?2 is processed.</p><p>In a nutshell, S/T Cut allocates the spatial/temporal resources for its children.</p><p>Currently, the RA Tree notation and the following SET framework focus on DNN inference scenarios. Given that the training process can also be viewed as a graph with some specific operators and limitations, RA Tree notation also has the potential to be used in training scenarios. In the future, we will extend our notation and framework to DNN training scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Space Size Analysis</head><p>Our inter-layer scheduling space consists of all RA Trees satisfying the above definitions. The size of the space is enormous: For a ?-layer DNN, if we only consider the tree structure and the two Cut types, there are already ? ((5 + 2 ? 6) ? ) ? ? (9.899 ? ) candidates (calculation procedure is provided at this link <ref type="bibr" target="#b0">[1]</ref>); To calculate the total size of the space, we need to multiply the number of topological orders of the DNN network, and consider different subbatch sizes of each Cut, further increasing the whole space size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RA Tree Analysis</head><p>In this section, we elaborate on how to parse an RA Tree into each layer's resource allocation scheme and the flow of data among the components of an accelerator. It is worth mentioning that the RA Tree notation is not tied to the strategies introduced in this section. RA Tree is a generic notation, allowing for the replacement of buffer management, dataflow management, and core allocation strategies to adapt to various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Data Flow.</head><p>We define data flow here as the flow of input feature maps (ifmaps), weights, and ofmaps of each layer among the components of an accelerator.</p><p>For feature maps (fmaps), if a dependency is between two layers under different children of a root T Cut, the corresponding fmaps will be sent to DRAM. Otherwise, there are two cases: (1) If the ofmaps of the former layer are consumed by the latter layer immediately, they will be sent directly to the consuming HW-tiles through NoC. (2) If the ofmaps of the former layer are not consumed by the latter layer immediately, it will be sent to DRAM for temporal buffering. For the example in Fig. <ref type="figure" target="#fig_3">2</ref>, the ofmaps of ?1 will be immediately consumed by ?2. Thus ?1's ofmaps can be directly sent to the HW-tile group of ?2. Since ? 1 Cut and ?4 are different children of the root T Cut, the ofmaps of the layers under ? 1 Cut will be sent to DRAM first. Then ?4 will load these ofmaps from DRAM.</p><p>For weights, if the root Node is a T Cut, the weights of all layers under each child will be loaded from DRAM and pinned on-chip for each batch of the child. If the root Node is an S Cut, all weights will always be pinned on-chip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">HW-tiles Allocation.</head><p>As introduced above, HW-tiles allocation only occurs at each S Cut, since each child of a T Cut uses all HW-tiles of the T Cut. Specifically, the resource allocation for an S Cut is a process of deciding how many and which computing resources should each child of the S Cut own, which will be introduced below. After mastering the resource allocation strategies for a single S Cut, the resource allocation of each RA Tree can be analyzed recursively.</p><p>To equalize the processing time of each child and reduce bubble overheads, the most straightforward method is to keep the number of HW-tiles allocated for each child proportional to the child's total number of operations (ops), which is the sum of the number of ops in all layers belonging to it. However, we observe that using the total op number to represent processing time is inaccurate because of two issues: (1) Due to the inherent features of HW-tile microarchitecture, layers of different types or shapes may have different utilization on the same HW-tile even if they have the same number of ops. For example, if we schedule the first layer in ResNet-50 <ref type="bibr" target="#b19">[19]</ref> on the Simba tile <ref type="bibr" target="#b47">[46]</ref>, we can only utilize 3 of the 8 MACs in one Vector, and the utilization will be upper-bounded by 37.5%. (2) When layers have dependencies in an S Cut, their corresponding batches of samples cannot start at the same time, creating filling and draining overheads (see ?1 and ?2 in Fig. <ref type="figure" target="#fig_5">3</ref> LP). Ideally, the reduction of utilization is ?/(? + ?), where ? is the number of subbatches, and ? is the difference in starting time between the first and last children. For example, the ?/(? + ?) of the S Cut in Fig. <ref type="figure" target="#fig_5">3</ref> LP is equal to 2/(2 + 1).</p><p>To take care of such two issues, we introduce an attribute, "Normalized Processing Time"(NPT), for each Node. The NPT of a layer (Leaf Node) is calculated by simulating the layer with one sample on a HW-tile. For a T Cut, we only need to sum up the NPT of its children to get its NPT. For an S Cut, the sum is divided by ?/(? +?) defined in issue (2) of the previous paragraph to get the NPT of the Cut. Then we can recursively define the NPT of each Node in a bottom-up manner. The remaining question is: How to divide the HW-tile group of the S Cut into subgroups for its children, so that the sizes of these subgroups are proportional to the NTPs of the children? Since the size of each subgroup must be an integer, in most cases, the division cannot be strictly proportional, resulting in an imbalance between different subgroups, which creates bubbles and reduces utilization. We will refer to this deficiency as "bubble overheads" below. The severity of this problem increases with the number and diversity of layers, and can even cause utilization to drop by tens of percentage points. Therefore, we propose an optimal HW-tile allocation algorithm to alleviate this problem as much as possible, and formally prove the optimality of the algorithm. Due to space constraints, we place the algorithm and its proof at this link <ref type="bibr" target="#b1">[2]</ref>.</p><p>After determining the number of HW-tiles in each HW-tile group, we also need to determine which physical HW-tiles the HW-tile group corresponds to. Firstly, each HW-tile should be attached with an id for allocation. The id-attaching policy can be substituted arbitrarily based on the features of the accelerator, such as the NoC topology. For the default mesh NoC used in the experiments, we employ a modified stripe-based id-attaching strategy like Tangram <ref type="bibr" target="#b17">[17]</ref> to ensure a fair comparison with it. For example, the HW-tiles in blue will be assigned ids from 1 to 8 in a left-right and bottom-up order (first row in Fig. <ref type="figure" target="#fig_6">4</ref>). Then, the children of each S Cut will take physical HW-tiles owned by the Cut in a left-right order. For the example in Fig. <ref type="figure" target="#fig_3">2</ref>, the ? 1 Cut owns the HW-tiles with ids from 1 to 3. The ? 1 Cut and ?3 will take the HW-tiles with ids 1&amp;2 and id 3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representation of LP and LS</head><p>In this section, we first show how to use our RA Tree notation to represent existing LP and LS patterns (Fig. <ref type="figure" target="#fig_5">3</ref>), and then analyze their features.</p><p>Due to the limitations in the capacity of the on-chip buffer and the number of HW-tiles, current LP and LS, which have been optimized by existing works <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b63">62]</ref>, tend to cut a DNN into multiple segments and process them one by one. Thus, in RA Tree notation, the root Cut of their schemes is T Cut, and each child corresponds to a segment. For each segment, LP allocates different HW-tile groups for different layers, so the depth-one Cuts in an LP scheme, if any, should be S Cuts. In contrast, LS processes each layer in a segment with all the HW-tiles and switches the fmaps of   the layers with dependencies through the on-chip buffer without accessing the DRAM. Thus, the depth-one Cuts in an LS scheme, if any, should be T Cuts. From the above analysis, the scheduling space of LP and LS patterns can be calculated as ? (2 ? )<ref type="foot" target="#foot_0">1</ref> , which is only about 1  (2 ? ) 2.22 of the space defined by us. However, most of the existing works touch only a small part of the LS or LP scheduling space, which further shows the significant potential of exploring the broad scheduling space defined by us.</p><formula xml:id="formula_13">L1-L4 T(4/2) L1, L2 L4 [0, 3] L2 [2, 3] L3 [0, 3] L2 [0, 1] L1 [2, 3] L1 [0, 1] Nth Tile 1 2 3 4 Time L2 [2, 3] L2 [0, 1] L1 [2, 3] L1 [0, 1] L4 [0, 3]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Trade-offs Analysis</head><p>In this section, we reveal the complex trade-offs behind different inter-layer scheduling choices with the help of our RA Tree notation. The fundamental trade-offs can be studied by analyzing a simple example, where two identical convolution layers with a workload of batch size two are scheduled to be mapped on a 16-tile accelerator with mesh NoC (most following analysis is not sensitive to NoC topology). The following analysis is applicable to most types of layers, such as fully connected layers, general matrix-to-matrix multiplication, and so on. Other types of layers can be theoretically analyzed similarly. Trade-offs behind more complex tree structures can be seen as a hierarchical combination of these fundamental trade-offs.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">4</ref>, when comparing 1 with 2 and 3 with 4, the trade-offs brought by S and T Cuts can be analyzed. As shown in the second column of under S Cut is less than the counterpart in T Cut, which is a core contributing factor. A smaller HW-tile group for a layer brings the following benefits: (1) less data duplication: Partitioning a layer into multiple HW-tiles in any dimension requires copying some part of the data <ref type="bibr" target="#b16">[16]</ref>. Thus, a smaller HW-tile group means less data duplication. For example, if we partition the output channel dimension for 8 HW-tiles, each HW-tile needs 1  8 weights and entire ifmaps. When the HW-tile group size scale to 16, the system buffers one copy of the weights and 16 copies of the ifmaps for this layer;</p><p>(2) larger intra-tile exploration space: Each layer needs to be partitioned first and then optimized by the intra-tile scheduling. Thus, a smaller HW-tile group means a larger workload for each HW-tile, facilitating intra-tile scheduling to take advantage of parallel computing resources and data locality. From the above analysis, a smaller HW-tile group reduces NoC communication costs due to (1), reduces DRAM access times due to (1), and improves intra-tile utilization and data reuse due to <ref type="bibr" target="#b1">(2)</ref>.</p><p>The S Cut also has some drawbacks: (1) filling and draining overheads: When the layers share a sequential dependency, some resources will be idle in the filling and draining period, which can be observed by comparing the sub-graphs of the first and third row of By comparing 1 with 3, and 2 with 4, the trade-offs brought by subbatch size can be analyzed. A smaller subbatch size brings the following benefits: (1) less filling and draining overheads: a smaller subbatch size can reduce the filling and draining overheads when the layers have a sequential dependency and are cut by S Cut;</p><p>(2) less on-chip buffer usage: A smaller subbatch size means that each HW-tile group needs to buffer fewer ifmaps, which will also bring the same benefits as the second benefits of a smaller HWtile group. A smaller subbatch size will also influence intra-layer scheduling effects: a smaller subbatch size means less exploration space for intra-layer scheduling, which may reduce the utilization and energy efficiency of intra-layer scheduling schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Case Study: Learn From a Practical Example</head><p>In this case study, we employ our cycle-accurate simulator (introduced in Sec. 4.3) to show a practical comparison between the scheme explored within the space defined by our RA Trees (abbreviated as SET scheme) and the scheme explored by SOTA Tangramlike strategy <ref type="bibr" target="#b17">[17]</ref>, which mainly falls in LP pattern (abbreviated as LP scheme) in Fig. <ref type="figure" target="#fig_7">5</ref>. The workloads, batch size, and hardware platform are two Inception-ResNet-v1 blocks, 2, and a 16-tile accelerator, respectively. We utilize this case study to vividly show how an RA Tree scheme incorporating artful combination of S and T cuts can outperform the LP scheme.</p><p>In Fig. <ref type="figure" target="#fig_7">5</ref>, we present the RA Trees of the schemes along with their corresponding execution graphs for all HW-tiles on the accelerator. Each HW-tile can initiate computation as soon as all the required data is available, without the need to synchronize with other HWtiles processing the same layer. Consequently, different HW-tiles computing the same layer may begin or end their computation at distinct times. The blank regions in Fig. <ref type="figure" target="#fig_7">5</ref> represent idle states for the respective HW-tiles.</p><p>Overall, the SET scheme saves 41.1% latency and 31.5% energy than the LP scheme. Especially, the DRAM accesses and NoC communication costs are reduced by 61.1% and 35.3%, which contributes to most energy cost reductions. By comparing Fig. <ref type="figure" target="#fig_7">5</ref>(d) and Fig. <ref type="figure" target="#fig_7">5</ref>(e), it can be intuitively observed that the workload execution of the SET scheme is more compact and closely aligned. In the following,  we will first analyze the reasons behind the relatively sparse workload execution of the LP scheme and then proceed to discuss how the SET scheme effectively mitigates these issues.</p><p>For LP scheme: As introduced in Sec. 3.3.2, ensuring that HWtiles allocated to multiple layers are proportionally distributed to maintain reasonable bubble overhead is a challenging task. Consequently, we observe that the LP scheme struggles to simultaneously put more than four layers under a S Cut, as shown in Fig. <ref type="figure" target="#fig_7">5(b</ref>). Such limitation significantly limits possible producer-consumer reuse opportunities, which leads to more DRAM accesses. Moreover, the dependencies between layers in the pipeline introduces much filling and draining overheads. As a result, filling and draining overheads (e.g., blank regions during intervals of approximately 0-7000 cycles and 45000-65000 cycles) and increased DRAM and NoC bandwidth pressure (e.g., blank regions during intervals of approximately 35000-45000 cycles) contribute to the sparser workload execution in the LP scheme, as shown in Fig. <ref type="figure" target="#fig_7">5(d)</ref>.</p><p>For the SET scheme: As shown in Fig. <ref type="figure" target="#fig_7">5(c</ref>) and (e), layers with shared dependencies tend to be under the same T Cut, consequently utilizing the same computing cluster for computation (e.g., L5?L7, L8?L9, L11?L14, L13?L15). As a result, the ofmaps from the producer layer can be rapidly and locally transmitted to the consumer layer, reducing NoC communication costs. Furthermore, the immediate consumption of these ofmaps enhances buffer usage efficiency. Concurrently, as shown in Fig. <ref type="figure" target="#fig_7">5</ref>(e), layers exhibiting parallel relationships are inclined to engage distinct clusters (e.g., L5 and L6, L10 and L12, L11 and L13, and L14 and L15), capitalizing on the advantages of smaller clusters discussed in Sec. 3.5 without incurring filling and draining overheads. Moreover, the strategic combination of S Cut and T Cut makes it easier to balance the HWtile allocation for the children of S Cut, thus significantly alleviates the bubble costs associated with S Cut. For instance in Fig. <ref type="figure" target="#fig_7">5(c</ref>), if we place L10?L15 under ? 1 Cut (marked in red), the bubble costs would be substantial. However, as shown in Fig. <ref type="figure" target="#fig_7">5(c</ref>) and (e), by combining several S and T Cuts, we can strike a sound balance in the HW-tile allocation among the children of ? 1 Cut without compromising parallel opportunities and producer-consumer reuse opportunities</p><p>In a nutshell, SET scheme can achieve both benefits of S Cut and T Cut by combining them in a artful manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SET FRAMEWORK 4.1 SET Overview</head><p>As shown in Fig. <ref type="figure" target="#fig_10">6</ref>, SET is an end-to-end DNN scheduling framework. SET inputs: (1) an NN model description file produced by high-level frameworks like Pytorch; (2) hardware configuration (NoC, DRAM, HW-tile, and so on); (3) framework settings (optimization goal and constraints, hyperparameters, and so on). After the scheduling, SET outputs: (1) energy cost and performance reports; (2) detailed scheduling scheme (3) instructions (optional).</p><p>The optimization goal of SET can be set as ?????? ? ? ????? ? , where n and m can be set as arbitrary numbers to represent different levels of concern for power and performance. The energy and latency here are the total energy cost and latency of a batch of samples. Thus, for latency-centric and throughput-centric scenarios, we can employ small and large batch sizes to test the effectiveness of SET, respectively.</p><p>The whole scheduling process can be divided into two stages, the exploration stage and the post-processing stage. The exploration stage explores the vast scheduling space for deploying the target DNN on the target tiled accelerator. The results are then sent to the post-processing stage for IR and instructions generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Exploration Stage</head><p>In the exploration stage, the inter-layer scheduling space is explored by a simulated-annealing (SA) algorithm, and the intra-layer scheduling space is explored by an exhaustive-search-based method.</p><p>Specifically, the Model Parser engine first parses the DNN description file, abstracting out the DNN graph topology information and the information of each layer in the DNN for exploration.</p><p>The initial RA Tree in SA is generated by putting all Leafs under a root T Cut in a topological order. When scheduling a ?-layer network, the SA algorithm conducts ? = ?? iterations, where ? is a hyperparameter. And in iteration ?, the temperature is</p><formula xml:id="formula_14">? ? = ? 0 1-? ? 1+? ? ?</formula><p>, where ? 0 is the initial temperature and ? is the cooling speed. The design guarantees that the final temperature ? ? will be 0. In each iteration, the SA Controller will randomly choose an operation (Fig. <ref type="figure" target="#fig_11">7</ref>) and use the operation to change the original RA Tree into a new RA Tree. Then the new RA Tree will be sent to the RA Tree Analyzer for analyzing the flow of data and each layer's HW-tile allocation scheme. The type and dimension information of each layer and its allocated HW-tile group will be sent to the Intra-layer Scheduling Engine. Then, the Intra-layer Scheduling Engine will explore the intra-layer scheduling space by searching all partition, tiling, and loop order candidates (dotted arrow in Fig. <ref type="figure" target="#fig_10">6</ref>). Finally, the optimal solution explored by the Intra-layer Scheduling Engine together with the information analyzed by the RA Tree Analyzer will be sent to the Evaluator (see Sec. 4.3) for an overall evaluation, whose results will be sent to the SA Controller. If the new cost ? ? is higher than the cost ? of the previous RA Tree, the scheme will be accepted with probability ? = ? ? -? ? ??? , where ? ? is the temperature of the current iteration; otherwise the modified RA Tree will always be accepted.</p><p>Given that the RA Tree Analyzer has been introduced in Sec. 3.3, we only introduce the SA operators and Intra-layer Scheduling Engine below. 4.2.1 SA Operators. We develop six operators to change the RA Tree in each iteration. With these operators, any two trees in this space can be transformed into each other in finite steps, which is an important property to ensure that simulated annealing can find a near-optimal scheme.</p><p>As shown in Fig. <ref type="figure" target="#fig_11">7</ref>, the operators will be introduced as follows: OP1: Randomly choose two adjacent Leaves and then exchange them. This operation requires the two layers to have no direct or transitive dependency. Adjacency here means adjacency in the order of all the layers in the in-order traversal. OP2: Randomly choose a Leaf and move it to another Cut that shares the same parent or grandparent Node. OP3: Randomly choose a Cut, then randomly choose a group of consecutive children to constitute a new Cut. The attributes of the Cut are generated randomly. OP4: Randomly choose a non-root Cut and delete it, then place its children into its parent Cut. OP5, 6: Randomly choose a Cut and increase/decrease its subbatch number by random times. As a result, its children's batch size will decrease/increase with the same ratio.</p><p>The six operators ensure that the newly generated RA trees will not violate the dependency restrictions introduced in Sec. 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Intra-layer Scheduling</head><p>Engine. Since intra-layer scheduling has been well studied by existing works, as introduced in Sec. 2.2, we adopt the classical strategies in this part. Firstly, we employ a flexible partition strategy <ref type="bibr" target="#b16">[16]</ref> to explore how to partition each layer into smaller workloads and distribute them to each HW-tile in the corresponding HW-tile group. Secondly, for each partition scheme, we employ a search-based intra-tile dataflow exploration strategy <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b43">42]</ref> to explore the loop order and tiling size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluator</head><p>The default Evaluator is basically based on a scalable architecture template with a mesh NoC and NVDLA-style HW-tiles, which is also the architecture of our test chip mentioned in the following. The Evaluator can estimate the cost of deploying different DNNs on tiled accelerators in different configurations using different generated scheduling schemes.</p><p>Specifically, based on the inter-layer scheduling scheme, the evaluator can analyze the workload distribution of all HW-tiles and the data dependencies between HW-tiles or between HW-tile and DRAM. Then, it can figure out the amount of data transmission on each link of NoC and the access pattern of DRAM. Based on the intra-layer scheduling scheme, it can figure out the access times of buffers at all levels and the operation times of MACs with different precision in each HW-tile. Based on the analysis results, the latency is evaluated by a fast simulator; the total energy cost ? ??? is calculated by</p><formula xml:id="formula_15">? ??? = ? ?=0 1 ?=0 ? ? ? ? ? ? ?,? + ? ?=0 ? ??? ? ? ? + ? ?=0</formula><p>? ? ? ? ? ? means the energy cost per access of reading or writing the DRAM. ? ? ?,? means the reading or writing times of the ?th DRAM. ? ??? means the energy cost per hop of transmitting a flit of data from one router to its neighbor router. ? ? means the number of flits that pass the ?th link. ? ? ? means the energy cost of the ?th HW-tile, which includes the access energy cost of buffers at all levels, MACs with different precision, and intra-tile communication.</p><p>A 4-tile test chip is being designed to verify our scalable architecture, hardware system design, and compiling flow in preparation for designing a larger-scale accelerator. Based on this test chip, we also develop a scalable cycle-accurate simulator. Because the cycle-accurate simulator is much slower than the fast simulator used in the exploration stage, it cannot be employed to explore the vast scheduling space. We employ this cycle-accurate simulator to show a practical scheduling example introduced in Sec. 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Post-processing Stage and SET Portability</head><p>The RA Tree notation can be applied to tiled accelerators with different architectures that satisfy the basic requirements introduced in Sec. 2.1. Therefore, SET is also designed to be highly portable to take full advantage of the notation's universality potential.</p><p>When porting SET to a new tiled accelerator, the Evaluator and Instruction Generator should be substituted. The Evaluator only needs to satisfy the scheduling engine's calling interface and give the scheme's evaluation results. Moreover, in order to reduce the difficulty of replacing the Instruction Generator, we develop an IR Generator Engine. This engine first parses the explored scheme and makes some analysis, then generate a workload list for each HW-tile. Each entry of the list records: (1) workload's attributes, such as data dimension, layer information, buffer requirement, and so on; (2) data sources and destinations; (3) intra-tile computing information. SET intra-tile optimization tool allows the users to explore classic architectures, such as NVDLA-style <ref type="bibr" target="#b42">[41]</ref> HW-tile and Eyeriss-style <ref type="bibr" target="#b9">[9]</ref> HW-tile. If the user wants to customize a new HW-tile architecture, the intra-tile scheduling engine, which is a part of the Intra-layer Scheduling Engine introduced in Sec. 4.2.2, will also need to be substituted. The whole deploying flow based on SET has been developed for our test chip and future larger-scale accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Experiment Setup</head><p>5.1.1 Hardware Configuration. To thoroughly test the effects of SET, we consider two hardware platforms in the evaluation: a 16tile (4?4) edge platform and a 144-tile (12?12) cloud or autonomous driving platform. The optimization goal of the edge and cloud platforms are ? 2 ? and ?? 2 to show their prior concern on power and performance, respectively. Both experimental platforms employ TSMC 12nm process and run at 1GHz. Both platforms share the same NVDLA-style HW-tile with 1024 int8 MACs and 1MB global buffer. The default total DRAM bandwidth is set as 0.5GB/s per 1 TOPs for each platform.</p><p>The unit energy costs of different operations used for energy cost evaluation introduced in Sec. 4.3 are as follows. The mesh NoC hop and 8-bit MAC are estimated at 0.7 pJ/bit and 0.018 pJ/op. The DRAM energy cost is extracted from the datasheet <ref type="bibr" target="#b24">[24]</ref> at 7.5pJ/bit. For the register files and the SRAM buffers at different capacities, we employ Memory compiler <ref type="bibr" target="#b5">[5]</ref> to generate their modules and directly acquire their energy cost per access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Workloads.</head><p>To thoroughly test SET effects and analyze the trade-offs behind different across-layer scheduling schemes, we scale batch size from 1 to 64, covering latency-sensitive scenarios to throughput-centric scenarios as introduced in Sec. 4.1. In our experiments, ResNet-50 <ref type="bibr" target="#b19">[19]</ref>, GoogLeNet <ref type="bibr" target="#b50">[49]</ref>, Inception-ResNet-v1 <ref type="bibr" target="#b49">[48]</ref>, PNASNet <ref type="bibr" target="#b37">[36]</ref>, and Transformer <ref type="bibr" target="#b54">[53]</ref> as workloads.</p><p>5.1.3 Baselines. We choose the SOTA open-source Tangram scheduling framework <ref type="bibr" target="#b17">[17]</ref> as our first baseline. For inter-layer scheduling, Tangram first employs a Dynamic Programming (DP) algorithm to cut DNNs into segments under LP patterns. Throughout the experiments, all optimization options in Tangram are turned on, and all hardware configurations of the two frameworks are kept the same (we implement an Eyeriss-style HW-tile in our evaluator).</p><p>5.1.4 SA Hyperparameters. For the SA hyperparameters mentioned in Sec. 4.2, we set ? 0 = 0.07, ? = 8, ? = 100 in the following experiments. Increasing ? 0 and ? for the SA algorithm will result in better RA Trees, but at a cost of longer searching time. Thus the hyperparameter can be determined according to the time budget of specific scenario.</p><p>To better study the pros and cons of each individual pattern, we use SET to explore LS and LP separately and set them as another baseline. Comparisons with such baselines can better prove the benefits of exploring the whole inter-layer scheduling space and provide vast insights into LS, LP, and the newly defined inter-layer scheduling space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with Tangram</head><p>Since Tangram does not support Inception-ResNet, PNASNet, and Transformer, the comparisons are conducted with the remaining workloads. Moreover, Tangram cannot support ? 2 ? and ?? 2 optimization goals. Thus we choose EDP as the optimization goal in the comparison on both platforms. 5.2.1 Validation. In order to ensure the fairness of the comparison of the inter-layer optimization effects between the two frameworks, we first test the intra-layer scheduling engines and the evaluators of the two frameworks by processing each layer of a DNN separately. Results show that, on average, the error of EDP is 3% for all workloads and batch sizes. Considering the improvement of SET over Tangram, we believe this error is acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>. Fig. <ref type="figure">9</ref> shows the overall comparisons between Tangram and SET. On average, across all platforms, batch sizes, and workloads, SET simultaneously achieves 1.78? performance improvement and 13.2% energy cost reduction compared with Tangram, yielding a reduction of 51.2% in total EDP. The following compares the two frameworks step by step. In terms of intra-layer scheduling, the comparable effects of the intra-layer scheduling engines in these two frameworks have been demonstrated in Sec. 5.2.1. For inter-layer scheduling, SET employs an SA algorithm to explore a space with ? (9.899 ? ) schemes (calculated in Sec. 3.2), while Tangram employs a DP algorithm to exhaustively explore a space that is a subset of the space of LS and LP patterns, and these patterns contain much fewer schemes (? (2 ? ), calculated in Sec. 3.4). The DP algorithm is more likely to yield a better solution than SA in the same space due to its capacity to explore the space exhaustively. The above analysis and comparisons demonstrate that: <ref type="bibr" target="#b0">(1)</ref> The larger scheduling space depicted by our RA Tree notation, which contains more efficient scheduling schemes, is the decisive factor in achieving performance and energy efficiency gains; <ref type="bibr" target="#b1">(2)</ref> Although this extremely large space no longer allows SET to use exhaustive search algorithms such as DP, our specifically-designed  SA algorithm can still ensure an efficient exploration of the space and yield good solutions. Meanwhile, we also compared the end-to-end running time of Tangram and SET. The experiments of both frameworks are performed on an AMD Ryzen-7 5800X CPU with 4 threads for each case. Results show that the average running time of SET is 706.9 seconds over all cases, which is about 68x faster than Tangram. The main reasons for the faster speed are that: (1) we equip SET with a better framework structure and implement its coding more efficiently; (2) The well-designed SA operators enable the exploration to converge to an efficient scheduling scheme quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Variance of SET Improvements.</head><p>To study the variance of the improvements of SET across different mappings, we run SA 10 times with different seeds for each situation in Fig. <ref type="figure">9</ref>, and plot the results in Fig. <ref type="figure" target="#fig_1">10</ref>. On average across these ten situations, the best-case and worst-case reduces 53.9% and 35.2% EDP compared with Tangram, while the average reduces 45.4%. Also, every sample reduces some EDP compared with Tangram. This demonstrate the substantial and stable impact of SET, despite the inherent randomness of the SA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with LS and LP</head><p>5.3.1 Analysis of SET. Fig. <ref type="figure" target="#fig_12">8</ref> shows the overall comparisons among LS, LP, and SET. Across all platforms, batch sizes, and workloads, SET simultaneously achieves an average of 1.90? and 1.68? performance improvement and 21.7% and 21.5% energy reduction over LP and LS, respectively.</p><p>Essentially, exploring our space by SET is exploring how to adjust the hierarchical tree structure to leverage the strengths of each Cut, avoid its weaknesses, and try to achieve excellent balance. As a comparison, the maximum depth of LS and LP patterns is two, and the type of depth-one Cut is limited. Thus, though SET explores them fairly, they struggle to strike a good balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Analysis of LP.</head><p>As introduced in Sec. 3.5, a major drawback of S Cut (also LP) is the overheads of filling and draining, so DNNs with more parallel layers, such as GoogLeNet and Inception-ResNet, allow LP to take advantage of the advantages of S Cut without being affected by its disadvantages. This is also why LP performs better on GoogLeNet and Inception-ResNet than ResNet with fewer parallel layers. Although PNASNet and Transformer also have many parallel layers, LP performs poorly on them due to another main drawback of S Cut, bubble overheads. Their layers are much smaller and more diverse, so even with our optimal allocation algorithm 3.3.2, it is difficult for LP to find a well-balanced HW-tile allocation scheme for a large group of layers. Therefore, the pipeline length is significantly limited, and the on-chip buffer cannot be well utilized, worsening performance and energy efficiency. The severity of this problem can be greatly alleviated with the increase in the number of HWtiles, which is reflected in the fact that LP performs better on the cloud platform that the edge one. The above analysis is also the reason why LP performs worse than LS in scheduling PNASNet and Transformer on edge-side platforms as batch sizes increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Analysis of LS.</head><p>It is observed that LS performs much worse on the cloud platform than the edge platform because the disadvantages of LS, which are also the advantages of LP introduced in Sec. 3.5, are much more serious on the cloud platform. As shown in Fig. <ref type="figure" target="#fig_12">8</ref>, using all HW-tiles in the cloud platform to compute each layer incurs significant NoC communication and DRAM access overheads, worsening performance and energy efficiency. Moreover, the parallel dimension of each layer is also limited, making it difficult for LS to use a large number of parallel resources on the cloud platform, further deteriorating performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION 6.1 Scheduling Space Analysis</head><p>In this case study, we employ SET framework to analyze the structure of the whole scheduling space. As shown in Fig. <ref type="figure" target="#fig_14">11</ref>, we randomly select schemes from the scheduling space and use SET to evaluate several metrics, including tree depth, DRAM access, NoC hops, and DRAM time proportion. For comparison, we also plot the search path of our SA algorithm in the right part of Fig. <ref type="figure" target="#fig_14">11 (a)</ref>.</p><p>From the figure, we make the following observations:</p><p>(1) As shown in Fig. <ref type="figure" target="#fig_14">11</ref> (a), schemes with deeper RA Trees tend to cost less energy, but have a bigger variance and smaller lower bound on its delay. The reason is as follows:</p><p>Since intra-tile energy are very close between different schemes (shown in Fig. <ref type="figure" target="#fig_12">8</ref>), the variation of the total energy is determined by the cost of DRAM access and NoC hops. A deeper RA Tree can capture more on-chip reuse within its structure, significantly reducing DRAM access times and NoC hops (see Fig. <ref type="figure" target="#fig_14">11</ref> (a) and (b)), thus reducing much energy.</p><p>However, the latency of a scheme is not only bounded by DRAM and NoC bandwidth, but also affected by the filling&amp;draining and bubble overheads in S Cuts as introduced in Sec. 4. Although a deeper tree can reduce DRAM and NoC bandwidth pressure, randomly using S Cuts increases the risk of introducing more fill&amp;drain costs and bubbles into the scheme. Thus compared with schemes with shallower RA trees, only deep RA Trees which balance workloads well can have both less DRAM&amp;NoC usage, less filling&amp;draining overheads, and bubbles simultaneously, and enjoy much less delay. As shown in the right part of Fig. <ref type="figure" target="#fig_14">11</ref>, our SA successfully finds such deeper RA Trees with much lower delays, proving the effectiveness of our SA algorithm.</p><p>(2) In both ResNet-50 and Transformer-Large, there exists a "DRAM bound" in the plot, where all dots are above this bound (see the dotted line in Fig. <ref type="figure" target="#fig_14">11</ref>). This marks the bound of DRAM bandwidth. To demonstrate this, we introduce the "DRAM Time" of each scheme, which is calculated by dividing the total DRAM access by the DRAM bandwidth. This is the ideal total access time of DRAM. As shown in Fig. <ref type="figure" target="#fig_14">11 (c</ref>), we study the proportion of "DRAM Time" in the total delay of each scheme. A proportion near 1 means DRAM bandwidth is fully utilized most of the time, and the scheme is close to the DRAM bound. From Fig. <ref type="figure" target="#fig_14">11</ref> (c), we can see that the dots close to the dotted line has the biggest DRAM Time proportion, ranging from 0.8 to 0.98. This marks that schemes right above the dotted line fully utilize DRAM, and its delay cannot go down any further and pass the line, which is the meaning of this "DRAM bound".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Existing works on inter-layer scheduling for tiled accelerators mainly focus on optimizing LS and LP patterns, a small subset of the space defined by us. For LS, Efficient Scheduling <ref type="bibr" target="#b63">[62]</ref> and NASA <ref type="bibr" target="#b39">[38]</ref> study how to cut a DNN graph into segments to fully use on-chip buffer and reduce DRAM access times. Shortcut <ref type="bibr" target="#b6">[6]</ref> proposes a buffer management architecture and strategy to optimize the shortcut structure of DNNs. For LP, Tangram <ref type="bibr" target="#b17">[17]</ref> and Atomic <ref type="bibr" target="#b62">[61]</ref> take different approaches to realizing a finer-grained layer pipeline, optimizing for the filling and draining overheads of LP. In fact, most of these specific optimizations on LS or LP are compatible with SET and can be employed to optimize the attributes of S Cut or T Cut to further improve SET effects. However, these works did not clearly define inter-layer scheduling space, significantly limiting the optimization opportunities and hindering the understanding of inter-layer scheduling choices and corresponding consequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This work proposes a universal notation to define the problem and exploration space of inter-layer scheduling on tiled accelerators. In concrete, this work first proposes a universal notation, RA Tree, and then dives into bridging the notation with hardware behaviors and analyzing the complex trade-offs behind different scheduling choices. Based on the space definition mentioned above, we develop an end-to-end and highly-portable framework, SET, to explore the whole scheduling space for tiled accelerators. We conduct vast experiments on different DNNs, batch sizes, and accelerator scales. Results show SET achieves significant energy-efficiency &amp; performance improvements over the SOTA Tangram scheduling framework and existing patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: The deploying flow from the DNN graph to the accelerator. Right:The two spaces for scheduling DNNs on the tiled accelerators, including undefined inter-layer scheduling space and well-defined intra-layer scheduling space (the inclusion relationship between the intra-layer scheduling spaces defined by different notations is drawn according to Tenet<ref type="bibr" target="#b38">[37]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) An example RA Tree. (b) The top-down hierarchical analyzing flow of the left RA Tree. The bottom one is the S-T Graph corresponding to the left RA Tree. For simplicity, the dependencies of the RA Tree are only drawn in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Representing LP and LS in RA Tree notation. The DNN topology is the same as the one in Fig. 2. A Node is called "depth-n Node" if the depth of the Node (distance to the root Node) is ?. "Maximum Depth" refers to the maximal depth of the Nodes in the RA Tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Trade-offs behind different Cuts and DNN topology. The first row shows an id-attaching example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A practical comparison of two schemes: one explored within the space defined by our RA Trees (abbreviated as SET scheme), and the other explored by Tangram-like strategy [17], which mainly falls in LP pattern (abbreviated as LP scheme). The practical execution graph of both RA Trees are drawn by the cycle-accurate simulator of a 16-tile accelerator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 ;</head><label>4</label><figDesc><ref type="bibr" target="#b1">(2)</ref> bubble overheads: This deficiency has been introduced when introducing NPT in Sec. 3.3.2. These deficiencies may worsen performance. The advantages and disadvantages of T Cut and S Cut are largely complementary. (3) possibly data-fetching overheads: If a smaller HW-tile group can no longer buffer all data of a layer, it has to fetch data from DRAM additionally, resulting in increased DRAM accesses and NoC communication costs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Scheme ParserWorkload List Generator</figDesc><graphic url="image-87.png" coords="8,146.18,101.71,244.47,144.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An overview of SET scheduling framework. Solid arrows represent the SA iteration loop, and dotted arrows represent local calls to the evaluator from each engine.</figDesc><graphic url="image-86.png" coords="8,142.03,83.79,329.25,166.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: SA operators. The gray and colored Nodes represent unchanged and changed Nodes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparisons among LP, LS, and SET with different batch sizes (BS), workloads, and hardware platforms. Each row takes the same platform, and each column takes the same workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Comparison with Tangram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Scheduling space depiction. Each colored dot represents an RA Tree under the cloud platform and batch size 8. (a) The average tree depth of random RA Trees and RA Trees on the search path of SA. Depth is averaged among the depth of all Leaves. (b) The DRAM access and NoC hops of random RA Trees. (c) The proportion of DRAM time (defined in Sec. 6.1 (2)) over total delay of random RA Trees. All "RANDOM" figures show the same set of schemes, but exhibit different features.</figDesc><graphic url="image-104.png" coords="12,260.55,84.87,186.58,169.21" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Since each Depth-one Node corresponds to one or multiple layers, a scheme in LS/LP can be seen as dividing a list of ? items (layers) into one or multiple segments, thus the total number is</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>?-1 = ? (2 ? ).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2022YFB2804103</rs>) and the <rs type="funder">Key Research and Development Program of Shaanxi</rs> (<rs type="grantNumber">2021ZDLGY01-05</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NydEdJX">
					<idno type="grant-number">2022YFB2804103</idno>
				</org>
				<org type="funding" xml:id="_HG88QE5">
					<idno type="grant-number">2021ZDLGY01-05</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ARTIFACT APPENDIX A.1 Abstract</head><p>This artifact appendix section describes how to access the artifacts of the SET framework introduced in Sec. <ref type="bibr" target="#b3">4</ref>. Experiments in Sec. <ref type="bibr" target="#b5">5</ref> and Sec. 6 are based on this framework.</p><p>A.2 Artifact check-list (meta-information) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Installation</head><p>For artifact evaluation, start by downloading the artifact from Zenodo: Our SET scheduling framework is in "SET_framework".</p><p>For the convenience of evaluation, we have also provided our baseline, the open-sourced Tangram framework, in the folder "Tan-gram_baseline". The installation, usage, and modifications for the Tangram framework can be found in "Tangram_baseline/README_ SET-Baseline.md".</p><p>The original data of Fig. <ref type="figure">9</ref> is included in the Excel file "SET_Fig8_ data.xlsx" and can be verified by rerunning the two frameworks.</p><p>To install the SET framework, just use the makefile provided with the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>$ cd SET_framework $ make</head><p>Then the executable will be generated at "./build/stschedule"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment workflow</head><p>Now that our environment is set up, we will run the SET artifact and evaluate the result.</p><p>The first step is to write a config file. The config files for the experiments are provided under the folder "config". For config file customization, see the Experiment customization section.</p><p>Then, we can run the SET framework using the following command:</p><p>$ ./ build / stschedule conf . txt &gt; res . txt Where "conf.txt" is the name of the config file, and "res.txt" is the name of the output file.</p><p>To evaluate the experiment more conveniently, we provide a shell script, "run.sh", which will be introduced in the next part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Evaluation and expected results</head><p>To evaluate the results, simply run $ ./ run . sh</p><p>The script runs all experiments sequentially. Changing the script to execute in parallel can significantly reduce the running time, but it requires more available threads in the running environment. (The program executes multiple SA algorithms in different threads in parallel and chooses the best result among them. By default, 4 threads are needed for each execution).</p><p>The results are generated in the folder "results". After all executions finish, run the Python script "get_res.py" to generate a ".csv" file that contains all results: $ python3 ./ get_res . py $ ls *. csv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>results . csv</head><p>We have provided the expected result files in folder "exam-ple_results". To verify the correctness of our framework, one can compare these files with the results output by the program under folder "results", or compare the data in the csv file with the data in the Excel file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Experiment customization</head><p>To run customized experiments, users can write their own config file and feed it to the program. In a config file, the full list of parameters include: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Methodology</head><p>Submission, reviewing and badging methodology:</p><p>? https://www.acm.org/publications/policies/artifact-review-badging ? http://cTuning.org/ae/submission-20201122.html ? http://cTuning.org/ae/reviewing-20201122.html</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/SET-ISCA2023/Tile-Alloc-Algorithm/blob/master/Space_Size_Calculation.pdf" />
		<title level="m">Calculation of Our Inter-layer Space Size</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://github.com/SET-ISCA2023/Tile-Alloc-Algorithm/blob/master/Optimal_Tile_Allocation_Algorithm.pdf" />
		<title level="m">The Optimal Tile Allocation Algorithm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fusedlayer CNN accelerators</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Milder</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2016.7783725</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2016.7783725" />
	</analytic>
	<monogr>
		<title level="m">49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-15">2016. 2016. October 15-19, 2016</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/Apple_A15" />
		<title level="m">Apple A15 Bionic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/downloads-beta/search?term=artisan" />
		<title level="m">ARM Artisan</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Azizimazreah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2019.00030</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2019.00030" />
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Symposium on High Performance Computer Architecture, HPCA 2019</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-02-16">2019. February 16-20, 2019</date>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cambricon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1145/2541940.2541967</idno>
		<ptr target="https://doi.org/10.1145/2541940.2541967" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<title level="s">ASPLOS &apos;14</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Salt Lake City, Utah, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.40</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.40" />
	</analytic>
	<monogr>
		<title level="m">43rd ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2016</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-18">2016. June 18-22, 2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual Path Networks</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio. f7e0b956540676a129760a3eae309294-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DaDianNao: A Machine-Learning Supercomputer</title>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.58</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.58" />
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>MICRO; Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-12-13">2014. 2014. December 13-17, 2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno type="DOI">10.1109/JETCAS.2019.2910232</idno>
		<ptr target="https://doi.org/10.1109/JETCAS.2019.2910232" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Sel. Topics Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ShiDianNao: shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750389</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750389" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<editor>
			<persName><forename type="first">Deborah</forename><forename type="middle">T</forename><surname>Marr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</editor>
		<meeting>the 42nd Annual International Symposium on Computer Architecture<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-06-13">2015. June 13-17, 2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NeuFlow: A runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berin</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Akselrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2011.5981829</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2011.5981829" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011-06-25">2011. 2011. 20-25 June, 2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3037697.3037702</idno>
		<ptr target="https://doi.org/10.1145/3037697.3037702" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<editor>
			<persName><forename type="first">Olivier</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Temam</surname></persName>
		</editor>
		<editor>
			<persName><surname>Carter</surname></persName>
		</editor>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-04-08">2017. 2017. April 8-12, 2017</date>
			<biblScope unit="page" from="751" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297858.3304014</idno>
		<ptr target="https://doi.org/10.1145/3297858.3304014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2019</title>
		<editor>
			<persName><forename type="first">Iris</forename><surname>Bahar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maurice</forename><surname>Herlihy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alvin</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</editor>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2019<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-04-13">2019. April 13-17, 2019</date>
			<biblScope unit="page" from="807" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.81" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23">2014. June 23-28, 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. 2016. June 27-30, 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mind mappings: enabling efficient algorithmaccelerator mapping space search</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/3445814.3446762</idno>
		<ptr target="https://doi.org/10.1145/3445814.3446762" />
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;21: 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Virtual Event</title>
		<editor>
			<persName><forename type="first">Tim</forename><surname>Sherwood</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emery</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</editor>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="page" from="943" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2012.2205597</idno>
		<ptr target="https://doi.org/10.1109/MSP.2012.2205597" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21">2017. 2017. July 21-26, 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CoSA: Scheduling by Constrained Optimization for Spatial Accelerators</title>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Kalaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yakun Sophia</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00050</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00050" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="554" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Innosilicon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gddr6</forename><surname>Innosilicon</surname></persName>
		</author>
		<ptr target="https://www.innosilicon.com/html/ip-solution/14.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ISPD 2020 Physical Mapping of Neural Networks on a Wafer-Scale Deep Learning Accelerator</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kibardin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372780.3380846</idno>
		<ptr target="https://doi.org/10.1145/3372780.3380846" />
	</analytic>
	<monogr>
		<title level="m">ISPD 2020: International Symposium on Physical Design</title>
		<editor>
			<persName><forename type="first">William</forename><surname>Swartz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jens</forename><surname>Lienig</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-03-29">2020. March 29 -April 1, 2020. September 20-23, 2020</date>
			<biblScope unit="page" from="145" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ten Lessons From Three Generations Shaped Google&apos;s TPUv4i : Industrial Product</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ashcraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushma</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00010</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00010" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080246</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080246" />
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA 2017</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA 2017<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-06-24">2017. June 24-28, 2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Datacenter Performance Analysis of a Tensor Processing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geonhwa</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO50266.2020.00058</idno>
		<ptr target="https://doi.org/10.1109/MICRO50266.2020.00058" />
	</analytic>
	<monogr>
		<title level="m">53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>MICRO 2020, Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-10-17">2020. October 17-21, 2020</date>
			<biblScope unit="page" from="622" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm</title>
		<author>
			<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3400302.3415639</idno>
		<ptr target="https://doi.org/10.1145/3400302.3415639" />
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference On Computer Aided Design, ICCAD 2020</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-02">2020. November 2-5, 2020</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graphcore</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS52781.2021.9567075</idno>
		<ptr target="https://doi.org/10.1109/HCS52781.2021.9567075" />
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 33 Symposium, HCS 2021</title>
		<meeting><address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-08-22">2021. August 22-24, 2021</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2012/hash/c399862" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States, Peter L. Bartlett, Fernando C. N. Pereira, Christopher</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
	<note>d3b9d6b76c8436e924a68c45b-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358252</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>MICRO; Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-10-12">2019. 2019. October 12-16, 2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173176</idno>
		<idno>AS- PLOS 2018</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<editor>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Tuck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</editor>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Williamsburg, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-03-24">2018. March 24-28, 2018</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Full HD 60 fps CNN Super Resolution Processor with Selective Caching based Layer Fusion for Mobile Devices</title>
		<author>
			<persName><forename type="first">Juhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoi-Jun</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="DOI">10.23919/VLSIC.2019.8778104</idno>
		<ptr target="https://doi.org/10.23919/VLSIC.2019.8778104" />
	</analytic>
	<monogr>
		<title level="m">2019 Symposium on VLSI Circuits</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06-09">2019. June 9-14, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Million Core, Multi-Wafer AI Cluster</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Lie</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS52781.2021.9567153</idno>
		<ptr target="https://doi.org/10.1109/HCS52781.2021.9567153" />
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 33 Symposium, HCS 2021</title>
		<meeting><address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-08-22">2021. August 22-24, 2021</date>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Progressive Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-5_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01246-5_2" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08">2018. September 8-14, 2018</date>
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation</title>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiqing</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liancheng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00062</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00062" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="720" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NASA: Accelerating Neural Network Design with a NAS Processor</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00067</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00067" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="790" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 28nm 12.1TOPS/W Dual-Mode CNN Processor Using Effective-Weight-Based Convolution and Error-Compensation-Based Prediction</title>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSCC42613.2021.9365943</idno>
		<ptr target="https://doi.org/10.1109/ISSCC42613.2021.9365943" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference, ISSCC 2021</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-02-13">2021. February 13-22, 2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="146" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Google&apos;s Training Chips Revealed: TPUv2 and TPUv3</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS49909.2020.9220735</idno>
		<ptr target="https://doi.org/10.1109/HCS49909.2020.9220735" />
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 32 Symposium, HCS 2020</title>
		<meeting><address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-08-16">2020. August 16-18, 2020</date>
			<biblScope unit="page" from="1" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="http://nvdla.org." />
		<title level="m">NVDLA Deep Learning Accelerator</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Timeloop: A Systematic Approach to DNN Accelerator Evaluation</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2019.00042</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2019.00042" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<meeting><address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-03-24">2019. 2019. March 24-26, 2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080254</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080254" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA 2017</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA 2017<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-06-24">2017. June 24-28, 2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.91" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. 2016. June 27-30, 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Paul Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Stanley</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.12</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.12" />
	</analytic>
	<monogr>
		<title level="m">43rd ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2016</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-18">2016. June 18-22, 2016</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture</title>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><surname>Keckler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358302</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Columbus, OH, USA) (MICRO &apos;52; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2017.55</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2017.55" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture, HPCA 2017</title>
		<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-8, 2017</date>
			<biblScope unit="page" from="541" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaul</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298594" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07">2015. June 7-12, 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DOJO: The Microarchitecture of Tesla&apos;s Exa-Scale Computer</title>
		<author>
			<persName><forename type="first">Emil</forename><surname>Talpes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debjit</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<idno type="DOI">10.1109/HCS55958.2022.9895534</idno>
		<ptr target="https://doi.org/10.1109/HCS55958.2022.9895534" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Hot Chips 34 Symposium, HCS 2022</title>
		<meeting><address><addrLine>Cupertino, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-08-21">2022. August 21-23, 2022</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">NN-Baton: DNN Workload Orchestration and Chiplet Granularity Exploration for Multichip Accelerators</title>
		<author>
			<persName><forename type="first">Zhanhong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00083</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00083" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="1013" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compute Substrate for Software 2.0</title>
		<author>
			<persName><forename type="first">Jasmina</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ljubisa</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davor</forename><surname>Capalija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Sokorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragoljub</forename><surname>Ignjatovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lejla</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Trajkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Hamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Matosevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Cejkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Aydonat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohaib</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armond</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Maksimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahi</forename><surname>Alexander Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhmed</forename><surname>Moudallal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Rakhmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Almeet</forename><surname>Nijjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Bhullar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Drazic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei-Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu Ting Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keivan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dabiri</surname></persName>
		</author>
		<author>
			<persName><surname>Mabee</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2021.3061912</idno>
		<ptr target="https://doi.org/10.1109/MM.2021.3061912" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<editor>
			<persName><forename type="first">Namal</forename><surname>Rakesh Shaji Lal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Renjith</forename><surname>Rajatheva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shripad</forename><surname>Retnamma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Karodi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emilio</forename><surname>Rosen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Munoz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aleksandar</forename><surname>Lewycky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raymond</forename><surname>Knezevic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Allan</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Rui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Drouillard</surname></persName>
		</editor>
		<editor>
			<persName><surname>Thompson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<title level="s">NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MAGNet: A Modular Accelerator Generator for Neural Networks</title>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaorong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">Ross</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Khailany</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCAD45719.2019.8942127</idno>
		<ptr target="https://doi.org/10.1109/ICCAD45719.2019.8942127" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</editor>
		<meeting>the International Conference on Computer-Aided Design<address><addrLine>Westminster, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-04">2019. 2019. November 4-7, 2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">NASGuard: A Novel Accelerator Architecture for Robust Neural Architecture Search (NAS) Networks</title>
		<author>
			<persName><forename type="first">Xingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amro</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00066</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00066" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="776" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spring Hill (NNP-I 1000) Intel&apos;s Data Center Inference Chip</title>
		<author>
			<persName><forename type="first">Ofri</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Behar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Daga</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTCHIPS.2019.8875671</idno>
		<ptr target="https://doi.org/10.1109/HOTCHIPS.2019.8875671" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Hot Chips 31 Symposium (HCS)</title>
		<meeting><address><addrLine>Cupertino, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-08-18">2019. August 18-20, 2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling</title>
		<author>
			<persName><forename type="first">Nellie</forename><surname>Yannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-An</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO56248.2022.00096</idno>
		<ptr target="https://doi.org/10.1109/MICRO56248.2022.00096" />
	</analytic>
	<monogr>
		<title level="m">55th IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>MICRO 2022, Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-10-01">2022. October 1-5, 2022</date>
			<biblScope unit="page" from="1377" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation</title>
		<author>
			<persName><forename type="first">Qingcheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00086</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00086" />
	</analytic>
	<monogr>
		<title level="m">48th ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2021</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-06-14">2021. June 14-18, 2021</date>
			<biblScope unit="page" from="1055" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Learning Training At Scale Spring Crest Deep Learning Accelerator (Intel? Nervana? NNP-T)</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTCHIPS.2019.8875643</idno>
		<ptr target="https://doi.org/10.1109/HOTCHIPS.2019.8875643" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Hot Chips 31 Symposium (HCS)</title>
		<meeting><address><addrLine>Cupertino, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-08-18">2019. August 18-20, 2019</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Interstellar: Using Halide&apos;s Scheduling Language to Analyze DNN Accelerators</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heonjae</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378514</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378514" />
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;20: Architectural Support for Programming Languages and Operating Systems</title>
		<editor>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karin</forename><surname>Strauss</surname></persName>
		</editor>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-03-16">2020. March 16-20, 2020</date>
			<biblScope unit="page" from="369" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Atomic Dataflow based Graph-Level Workload Orchestration for Scalable DNN Accelerators</title>
		<author>
			<persName><forename type="first">Shixuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA53966.2022.00042</idno>
		<ptr target="https://doi.org/10.1109/HPCA53966.2022.00042" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High-Performance Computer Architecture, HPCA 2022</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-04-02">2022. April 2-6, 2022</date>
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Efficient Scheduling of Irregular Network Structures on CNN Accelerators</title>
		<author>
			<persName><forename type="first">Shixuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoli</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2020.3012215</idno>
		<ptr target="https://doi.org/10.1109/TCAD.2020.3012215" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3408" to="3419" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">COMB-MCM: Computing-on-Memory-Boundary NN Processor with Bipolar Bitwise Sparsity Optimization for Scalable Multi-Chiplet-Module Edge Machine Learning</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhengmao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSCC42614.2022.9731657</idno>
		<ptr target="https://doi.org/10.1109/ISSCC42614.2022.9731657" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-02-20">2022. February 20-26, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Scalable Multi-Chip-Module-Based Deep Neural Network Inference Accelerator With Ground-Referenced Signaling in 16 nm</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">Ross</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Thomas</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Khailany</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2019.2960488</idno>
		<idno>2020. A 0.32-128 TOPS</idno>
		<ptr target="https://doi.org/10.1109/JSSC.2019.2960488" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="920" to="932" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
