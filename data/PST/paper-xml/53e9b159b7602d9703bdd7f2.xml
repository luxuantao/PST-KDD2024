<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparison of human and computer performance across face recognition experiments ☆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">Jonathon</forename><surname>Phillips</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology</orgName>
								<address>
									<addrLine>100 Bureau Drive MS 8490</addrLine>
									<postCode>20899</postCode>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alice</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
							<email>otoole@utdallas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Behavioral and Brain Sciences</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
								<address>
									<addrLine>GR4.1</addrLine>
									<postCode>75083-0688</postCode>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparison of human and computer performance across face recognition experiments ☆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C720E364BA2DBEF642C3C381F1BA006D</idno>
					<idno type="DOI">10.1016/j.imavis.2013.12.002</idno>
					<note type="submission">Received 15 December 2012 Received in revised form 3 September 2013 Accepted 4 December 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face recognition Algorithm performance Human performance Challenge problem</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since 2005, human and computer performance has been systematically compared as part of face recognition competitions, with results being reported for both still and video imagery. The key results from these competitions are reviewed. To analyze performance across studies, the cross-modal performance analysis (CMPA) framework is introduced. The CMPA framework is applied to experiments that were part of face a recognition competition. The analysis shows that for matching frontal faces in still images, algorithms are consistently superior to humans. For video and difficult still face pairs, humans are superior. Finally, based on the CMPA framework and a face performance index, we outline a challenge problem for developing algorithms that are superior to humans for the general face recognition problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Overall, humans are the most accurate face recognition systems. People recognize faces as part of social interactions, at a distance, in still and video imagery, and under a wide variety of poses, expressions, and illuminations. A holy grail in automatic face recognition is developing an algorithm that has performance equivalent to humans-this is equivalent to solving the general face recognition problem. While it is easy to state the problem, accuracy equivalent to humans, it is not obvious how to determine if an algorithm's recognition accuracy is better than a human. One of the key challenges is establishing a measurable goal line and knowing when the goal line is crossed.</p><p>Since 2005, human and computer performance has been systematically compared as part of face recognition competitions conducted by the National Institute of Standards and Technology (NIST) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. The comparisons provided an assessment of accuracy for both humans and machines for each competition. However, there has not been a systematic analysis of these results across the competitions.</p><p>To analyze the results across experiments, we introduce the crossmodal performance analysis (CMPA) framework, which is demonstrated on the NIST competitions. CMPA was adapted from techniques in neuroscience that were developed to compare output from different sensing modalities of brain activity; e.g., functional magnetic resonance imaging (fMRI) and human perceptual judgments <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. These techniques can measure concordance between experimental data and computational models. In our study, the modalities compared are human and algorithm performance. In the psychology and neuroscience literature, face recognition algorithms can be referred to as computational models. The computational model can be designed to optimize performance or to model the human face recognition processes. The framework is sufficiently general that it provides a goal line for determining when machine performance reaches human levels.</p><p>On frontal faces in high quality still images, our analysis shows that machine performance is superior to humans. For these images, machines represent a person's identity primarily by encoding information extracted from the face; information from the body, hair, and head is generally ignored. For video and extremely difficult-to-recognize face pairs, experiments show that humans take advantage of all available identity cues when recognizing people <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. CMPA quantifies the potential for improving machine performance if all possible identity information is encoded by algorithms.</p><p>Comparing machine and human performance started with independent experiments in NIST competitions. The synthesis of the results across experiments gives a greater understanding of the relative strengths of machines and humans. The CMPA framework provides a goal line for determining if algorithm and human performance is comparable on the general face recognition problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of human and machine comparisons</head><p>We examine the relative performance of humans and machines for both still and video imagery. This review section presents the key details and conclusion for each study. The key details and conclusions were selected to lay the groundwork for the cross-experiment analysis in Section 3. The summary includes an overview of the images in the experiment, how the images were selected for measuring human performance, the key receiver operating characteristics (ROCs) comparing machine and humans, and the headline conclusions for each experiment. References are provided for full details on each experiment and the associated competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Methods</head><p>To encourage the development of face recognition technology and to provide an independent assessment of algorithm performance, since 1993 the U.S. Government has sponsored a series of competitions <ref type="bibr" target="#b8">[9]</ref>. The competitions came in two varieties: challenge problems and evaluations. A challenge problem can be considered a homework assignment meant to assist developers in improving algorithm performance. An evaluation is considered a final exam that takes the form of an objective test of face recognition technology with sequestered images (i.e., images not available in the challenge problem).</p><p>The goal of challenge problems is to encourage and facilitate the development of new face recognition technology. In a challenge problem, participants were provided with a large set of face images, a protocol for performing a set of experiments, and the code for scoring algorithm performance. The experiments were designed so that multiple algorithms could be compared on exactly the same images. In addition, participants were given the answers, also known as ground truth. By providing the answers, it allowed participants in a challenge problem to improve their algorithms or develop new algorithms. Participants could submit their raw results from matching images in an experiment to NIST for analysis. From the raw results, NIST performed analysis across participants on a common set of images. The analyses were included in summaries, presentations, and papers on a challenge problem.  The goal of an evaluation is to provide an independent assessment of algorithm performance. For evaluations, participants submitted their algorithms to NIST, and the algorithms were tested in the NIST Biometric Testing Laboratory. Performance of algorithms was measured on sequestered data. In the statistical learning domains, this is the standard methodology to measure the ability of an algorithm to generalize to novel data and mitigate the effects of over-tuning to the development data.</p><p>Although the NIST competitions measured algorithm performance on a number of recognition tasks, the analysis in this paper is limited to a verification task. In the perception community, this is known as face-identity matching of unfamiliar faces. In our verification task, humans and machines were given a pair of images or videos, with each image or video containing one face. The humans and machines had to respond how likely the two faces were of the same person. For machines, the response is a number called a similarity score. Each algorithm has its own similarity score distribution. From the similarity scores, receiver operating characteristics (ROCs) can be computed.</p><p>Human performance was measured on normal (i.e., untrained) people who had no professional experience with face recognition. Performance was measured by presenting two face images or videos on a computer screen. They were asked to judge the similarity between a face pair on the following scale: From the human generated ratings, ROCs were computed. For consistency, in our analysis of still image experiments, all face pairs were presented on the computer screen for 2 s. The presentation time of 2 s was chosen based on experiments in O'Toole et al. <ref type="bibr" target="#b0">[1]</ref> showing that human accuracy was stable between 2 s and unlimited time. However, subsequent experiments showed that a slight improvement in performance is possible when a subject has unlimited time to make a decision <ref type="bibr" target="#b7">[8]</ref>. The number of subjects judging the similarity between face pairs varied by experiment. Because this is a review article, we provide citations to the original papers that have the full experimental details including the number of subjects.</p><p>Because one of the main goals of an evaluation was to test the ability of algorithms to generalize to novel faces, faces in evaluations were sequestered; e.g., images of the faces in an evaluation were not released to the face recognition community. Thus, the faces in evaluations were "unfamiliar" to the algorithms. This kind of sequestering is likely to be comparable to the humans tested, who have general experience with faces, but no experience with the faces used as test stimuli in the experiments. Moreover, the unfamiliar face matching task is comparable for machines and humans operating in situations typical of security applications, where face recognition for previously unfamiliar people is required.</p><p>The main difference between measuring performance of humans and machines is the number of face pairs that can be compared. In the NIST competitions, machines compare millions of face pairs. Because it was impossible for human subjects to rate millions of face pairs, the human-machine comparison focused on a subset of the face pairs compared in machine experiments. The maximum number of face pairs that a subject can rate in an experiment is about 250. One of the factors differentiating the experiments is the method for selecting the face pairs in an experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Still frontal face images</head><p>Over the last twenty years the most active research area in automatic face recognition has been developing algorithms to recognize faces from frontal still images. In the last ten years, one emphasis of the NIST competitions has been recognition from frontal face images acquired with a digital single lens reflex camera. The majority of these images are considered high quality to humans. The images were collected under two illumination conditions. One was in a studio environment with controlled lighting. The other was under ambient lighting indoors and outdoors.</p><p>Progress on recognizing images under these conditions has been measured through a series of US Government sponsored competitions <ref type="bibr" target="#b8">[9]</ref>. Three recent competitions included comparing of human and machine performance: the Face Recognition Grand Challenge (FRGC) <ref type="bibr" target="#b9">[10]</ref>, the Face Recognition Vendor Test (FRVT) 2006 <ref type="bibr" target="#b2">[3]</ref>, and the Good, Bad, &amp; Ugly face challenge problem (GBU) <ref type="bibr" target="#b10">[11]</ref>. The FRGC study reports algorithm results from 2005. The GBU algorithm challenge has been ongoing since 2011; however, the best reported results are from the FRVT 2006.</p><p>Human and machines were compared for two categories of experiments. In the first category, one image in a face pair was taken in studio lighting and the other was taken in ambient lighting. In the second category, both images were images taken under ambient illumination.</p><p>The demographics and size of the face varied by experiment. The face size is measured by the number of pixels between the centers of 0.0 0.  the eyes. Table <ref type="table" target="#tab_0">1</ref> specifies face size by experiment, data set, and imaging condition. The experiment is labeled by the associated competition.</p><p>The data for the still face experiments came from two sources: University of Notre Dame and Sandia National Laboratory. For the data collected at Notre Dame, the overall demographic composition was 59% male and 41% female; 71% Caucasian and 10% East Asian; and 92% were 18 to 29 years old. The demographics varied slightly by experiment with precise numbers provided in the references. For data collected at Sandia the demographic composition was 55% female and 45% male; 64% Caucasian and 21% Hispanic; and 35% were 50 to 59 years old, the age range for the remaining 65% was roughly spread evenly over the 18 to 59 and 60 to 65 age ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Benchmark on matching studio against ambient illumination images</head><p>In the first two studies comparing human and machine performance, one image was acquired in a studio environment and the second was captured in ambient indoor lighting, see Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>In the first study we review, results were reported for two categories of face pairs: easy and difficult. The two categories allowed for the comparison of human and machine results at both ends of the performance range, see O'Toole et al. <ref type="bibr" target="#b0">[1]</ref> for details. This baseline is an implementation of an algorithm based on principal components analysis (PCA) <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. This algorithm achieved good recognition performance for the easy category and poor recognition performance for the difficult face pairs. The seven algorithms in the experiment were participants in the FRGC. Human and algorithm performance is reported for the same set of image pairs. The human and machine ROC curves for the difficult face pairs (Fig. <ref type="figure" target="#fig_2">2(b)</ref>) show that three algorithms were more accurate than humans <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and four algorithms were less accurate. For the easy face pairs (Fig. <ref type="figure" target="#fig_2">2(a)</ref>), the algorithms and machines were highly accurate, with all but one algorithm performing more accurately than humans.</p><p>The previous experiment examined performance for easy and difficult face pairs; the second experiment measures performance on face pairs of average difficulty. In this study, humans were compared to algorithms submitted to the FRVT 2006 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Average difficulty is defined relative to algorithms in the FRVT 2006 competition. A face pair had average difficulty if approximately half of the algorithms performed correctly (i.e., in a face pair with images of the same person, then approximately half of the algorithms reported that the images were of the same person). Experiments were performed on images from two data sets, one collected at the U. of Notre Dame and the second at the Sandia National Laboratory.</p><p>ROCs for both data sets are presented in Fig. <ref type="figure">3</ref>. There are two key conclusions. First, the results on both data sets are consistent with the difficult portion of the FRGC (previous experiment) comparison; machine performance is in the range of human performance, with the best algorithms surpassing humans. Second, human performance is stable across the two data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Benchmark on matching ambient illumination images only</head><p>The next experiment relaxed the photometric constraints. Both faces in a pair were acquired in ambient lighting conditions. The images were taken outdoors or indoors in atriums and hallways. To better understand the range of performance under general illumination conditions, three partitions were created based on difficulty of matching. 1 To arrive at the performance-based partitions, three top-performing face recognition algorithms from the FRVT 2006 test were fused to produce a single algorithm. Based on performance of the fusion algorithm, images were divided into three partitions with high (the Good), challenging (the Bad), and very challenging (the Ugly) accuracy, hence the name Good, Bad, and Ugly (GBU) Face Challenge Problem. On the Good partition, the base verification rate (VR) was set to 0.98 at a false accept rate (FAR) of 0.001. For the challenging partition, the VR was set to 0.80 at a FAR of 0.001, and on the very challenging partition the VR was set to 0.15 at a FAR of 0.001.</p><p>In the GBU, the effects of natural variations in a person's day-to-day appearance (hair, facial expression, etc.) and variations in illumination across both indoor and outdoor settings were considered. All of these images were nominally frontal. Because all images were collected between August 2004 and May 2005, aging cannot be a factor. There is the same number of images of each person in all three partitions. Thus, only the images, not the individual identities, changed across the three partitions. This provides an assurance that the accuracy differences were due to factors other than the particular set of face identities tested. Human performance on the GBU is reported in O'Toole et al. <ref type="bibr" target="#b3">[4]</ref>. Fig. <ref type="figure">4</ref> shows three face pairs of the same person, sampled from the good (left column), challenging (middle column), and very challenging (right column) performance conditions. This figure illustrates the wide variation in the appearance of a person across frontal images. It also highlights the difficulties that may occur in matching identity in faces that are taken in different settings and which include variations in expression and appearance-based features such as hairstyle. These factors become even more salient in combination (cf., Fig. <ref type="figure">4</ref> right column). 1 An overview of the creation of the GBU partitions is presented in this Section, details are given in Phillips et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High resolution</head><p>Fig. <ref type="figure">3</ref>. Human and machine performance on the Notre Dame and Sandia data sets.</p><p>ROCs comparing human and machine performance are presented in Fig. <ref type="figure" target="#fig_3">5</ref>. For humans, performance on the Good partition is superior to the challenging and very challenging partitions, see Fig. <ref type="figure" target="#fig_3">5</ref>(a). The difference in human performance on the challenging and very challenging partitions is not statistically significant (cf. O'Toole et al. <ref type="bibr" target="#b3">[4]</ref>). For all three partitions, performance on the fusion algorithm is superior to humans, see Fig. <ref type="figure" target="#fig_3">5(b,</ref><ref type="figure">c,</ref><ref type="figure">d</ref>).</p><p>To gain better understanding of the relative strengths of human performance, Rice et al. <ref type="bibr" target="#b7">[8]</ref> examined human performance when algorithms completely fail. From the very-challenging partition in the GBU, 50 same-identity face pairs and 50 different-identity face pairs were selected so that the similarity score for all same-identity pairs was lower than all different-identity pairs. A higher similarity score implies a greater likelihood that the face pairs consist of two images of the same face. Thus, performance of the FRVT 2006 fusion algorithm was 100% incorrect. Thus, we refer to these as extremely-difficult face pairs.</p><p>To understand the reason for algorithm failure, Rice et al. <ref type="bibr" target="#b7">[8]</ref> measured the contribution of face and body, face only, and body only to recognition by humans. To measure the contribution of these three conditions, three versions of the face images were created, see Fig. <ref type="figure" target="#fig_4">6</ref>. In the first experiment, human observers were presented with the original images, see Fig. <ref type="figure" target="#fig_4">6(a</ref>). In the second experiment, humans were presented with images where the face was masked, see Fig. <ref type="figure" target="#fig_4">6(b</ref>). In the third experiment, the images consisted of only the face, see Fig. <ref type="figure" target="#fig_4">6(c</ref>). The ROCs for all three human viewing conditions and the fusion algorithm are shown in Fig. <ref type="figure">7</ref>.</p><p>Performance between the body only and original images was indistinguishable. Performance on the face only images was remarkably inaccurate, but greater than chance. The results indicate that the body, rather than the face, accounts for human accuracy at identifying people in the original unedited images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video challenge</head><p>In our daily lives, faces are recognized as we interact with people. This allows the incorporation of motion and non-face identity cues into the recognition process. The equivalent model for algorithms is recognition from video.</p><p>O'Toole et al. <ref type="bibr" target="#b6">[7]</ref> extensively studied human performance on video sequences. The data set in the study consisted of two categories of video sequence <ref type="bibr" target="#b17">[18]</ref>. The videos were captured in standard-definition progressive-scan format by a digital video camera. In the first, a person walked towards the camera; in the second, a person was engaged in a conversation, see Fig. <ref type="figure" target="#fig_5">8</ref>. O'Toole et al. <ref type="bibr" target="#b6">[7]</ref> measured effect of face, body, and motion on performance. The analysis in this paper is restricted to two key cases. The first was recognition from the entire video sequence. The second was recognition when only the head and face were visible in the video sequence; the background and the person's body were masked, see Fig. <ref type="figure" target="#fig_5">8(c</ref>). The original video sequence case reports performance when information about the head, face, body, and motion is available. The videos with only the head and face were designed to measure the performance when information about the body was not present.</p><p>The video sequences in the above study were included in the Video Challenge of the Face and Ocular Challenge Series (FOCS). 2,3 The current paradigm in automated video recognition is to first detect frontal faces and then feed the frontal faces into a recognition algorithm. In video algorithms, a key challenge is recognizing people in sequences that do not contain frontal faces. In the Video Challenge, this challenge is represented by the conversation sequences. The video dictionary algorithm of Chen et al. <ref type="bibr" target="#b19">[20]</ref> reports performance on the conversation video sequences in Video Challenge. The video dictionary algorithm extracts the face from each frame. The extracted faces are then grouped by pose. From each group a dictionary is learned. Non-frontal faces are recognized by comparing similar pose groups. Since features are only extracted from the face, the video dictionary algorithm does not incorporate body information in the recognition process.</p><p>Human and machine performance is shown in Fig. <ref type="figure" target="#fig_6">9</ref>. Results are reported for comparing walking-vs-walking videos, conversation-vsconversation videos, and conversation-vs-walking videos. For humans, performance is reported for both original and face only sequences. For humans, comparing walking-vs-walking videos is superior to the other two cases, Fig. <ref type="figure" target="#fig_6">9</ref>(a). On the original video sequences, humans are superior to the algorithm in all three cases, Fig. <ref type="figure" target="#fig_6">9(b,</ref><ref type="figure">c,</ref><ref type="figure">d</ref>). On the face only sequences, the algorithm is roughly equivalent to humans on the walk-vs-walking and conversation-vs-conversation cases, Fig. <ref type="figure" target="#fig_6">9(b,</ref><ref type="figure">d</ref>). Since the algorithm only encodes identity from the face, comparing human performance on the face only sequences is meaningful. In the cross pose case, conversation-vs-walking, humans are better, Fig. <ref type="figure" target="#fig_6">9(c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross experiment comparison</head><p>The next step is to take the experiments reviewed in the previous section and to analyze them as a group. The analysis is performed by using the cross-modal performance analysis (CMPA) framework, which we introduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis</head><p>Traditionally, the performance of humans and machines is compared by plotting their ROCs on a single plot. ROCs are a standard method for reporting performance on a small number of experiments. However, they do not allow for a concise summary across a large number of experiments. Comparing performance across experiments is accomplished by placing ROCs side-by-side; e.g., Figs. 2, 3, 5, and 9. Ideally, to compare results across experiments, each ROC needs to be summarized by a single number. For our analysis, we summarize a ROC by the area under the curve (AUC) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The range of values for AUC is [0,1]. An AUC value of 1 is perfect performance and a value of 0.5 is random performance. An AUC value of 0 corresponds to no correct answers; e.g., algorithm performance on the extremely hard face pairs.</p><p>In the CMPA framework, the relative performance of humans and algorithms is characterized by their AUC statistics on the same experiment. In the experiments reviewed in this paper, the AUCs are computed from responses to stimuli from the same data set. This technique is extensible to analysis were the two underlying ROCs do not need to be responses to stimuli from the same data set. We graphically show this relationship on a scatter plot, see Fig. <ref type="figure" target="#fig_7">10</ref>. The x-axis is AUC for human performance and the y-axis is AUC for algorithm performance. If the AUC for both human and machine performance are approximately equal, then performance for humans and machines is comparable. In Fig. <ref type="figure" target="#fig_7">10</ref> this is the diagonal line. Points in the region above the diagonal line correspond to experiments where machines perform better than humans (as measured by AUCs). Likewise, points in the region below the diagonal line correspond to experiments where humans are better. For algorithm developers, in the ideal case, all points would be on horizontal line with machine AUC equal to 1.0. For those modeling the human face recognition system, in the ideal case, all points will lie along the diagonal line.</p><p>Our CMPA analysis builds on the work on DiCarlo <ref type="bibr" target="#b4">[5]</ref>, where human and machine performance on object recognition is characterized by the statistic d′ <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. A related technique is representational similarity analysis for comparing different brain imaging modalities with human perceptual similarity judgments <ref type="bibr" target="#b5">[6]</ref>.</p><p>To assess the relative capabilities of humans and machines, we apply our CMPA analysis to all the experiments reviewed in Section 2. Fig. <ref type="figure" target="#fig_1">11</ref> shows the results of this analysis. In Fig. <ref type="figure" target="#fig_1">11</ref>, experiments are grouped into three categories.</p><p>The first category consists of the still image experiments with the exception of the difficult-face pairs. For the first two experiments reviewed, see Section 2.2, performance is reported for multiple algorithms. For the analysis in this section, machine performance is represented by one of the top performers. This is because of multiple experiments in each competition, there is not a clear top performer. For the two FRGC experiments, machine performance was reported for the New Jersey Institute of Technology (NJIT) algorithm <ref type="bibr" target="#b14">[15]</ref>. Likewise, for the FRVT 2006 results, performance is reported for the Viisage-norm submission to the FRVT 2006. For the GBU experiments results are for the FRVT 2006 fusion algorithm in Phillips et al. <ref type="bibr" target="#b10">[11]</ref>. For these seven experiments, a regression line has been plotted in Fig. <ref type="figure" target="#fig_1">11</ref>.</p><p>In this category, the images were acquired with digital single lens reflex cameras and the majority of the images are considered "high quality" by the face recognition community. The face-image pairs are selected by different criteria and cover a range of imaging conditions. For these experiments, performance of machines is superior to humans and the regression line suggests that there is a linear relationship between the difficulty of the experiments for humans and algorithms.</p><p>On the video challenge, performance is compared on six experiments. The experiments are organized into three experimental conditions that characterize the pairs of video that are compared. For each condition, there are two viewing conditions: the original sequences and face only video sequences. The machine results on the video experiments is for a video-dictionary based algorithm <ref type="bibr" target="#b19">[20]</ref>.</p><p>For the extremely difficult face pairs, AUCs for three experiments, face only, face masked, and original image, are presented. For all three experiments, machine performance is the FRVT 2006 fusion algorithm. By the design of the selection process, the AUC for the fusion algorithm is 0. False accept rate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verification rate</head><p>Humans -Original Humans -Face Only Humans -Face Masked Fusion Algorithm Fig. <ref type="figure">7</ref>. Performance on the extremely hard face pairs. Because algorithm performance is 100% incorrect, its ROC, green line, hugs the bottom and right axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conclusions</head><p>The most studied area of face recognition is recognition from high quality still frontal face images. In our studies these are represented by images taken with a digital single lens reflex camera. For the seven experiments described in Section 2.2, our analysis shows that machines are superior to normal humans. For these experiments, the faces contain significant identity cues.</p><p>The results on the video and extremely-difficult face pairs show conditions where human performance is superior to machines. In the video experiment, human and machines are at near parity when pose is the same in both video sequences and only face identity cues are considered. When there was a change in pose, human performance was superior to machine performance. On both the video and extremely-difficult face pair experiments, human performance is superior when non-face identity cues are dominant. These results suggest that humans effectively integrate non-face identity cues into the recognition process and that humans take advantage of the head and body in identifying someone. Because the automatic face recognition community has developed algorithms that compensate for changes in pose, future experiments should directly compare human and machine performance on changes in pose.</p><p>The CMPA provides a high level summary across multiple experiments. One direction for future analysis is developing statistical models of both human and machine performance. One example is generalized linear models that allow for analysis that explicitly models the effect of covariates <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. This class of analysis has been restricted to algorithm performance on a single data set. To extend this technique to the problem described in this paper, the models need to be able to analyze results on multiple data sets and incorporate human matching results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Insights from structural comparisons</head><p>The analysis in Section 3 directly compared human and machine performance. There is more to learn from the interplay of machines and humans than what can be learned from relative performance comparisons. We will examine this interplay in the context of three topics. The first is the other-race effect, where algorithms have contributed to understanding the human face processing systems and human face processing has contributed to understanding machine performance. Second, it has been possible to improve techniques for the analysis of machine performance based on the design of human experiments. Finally, the effect of fusing machine and humans is reviewed and can reveal strategy differences in the way humans and machines perform the tasks.</p><p>The other-race effect is a classic property of human face recognition. Our ability to recognize the identity of faces from our own race is better than our ability to recognize the identity of faces of other-races. The other-race effect for face recognition has been established in numerous human memory studies <ref type="bibr" target="#b24">[25]</ref> and in meta-analyses of these studies <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p><p>Phillips et al. <ref type="bibr" target="#b28">[29]</ref> looked for an other-race effect in algorithms submitted to the FRVT 2006. They compared the performance of a fusion of East Asian algorithms and a fusion of Western algorithms matching identity in pairs of Caucasian and East Asian faces. The East Asian algorithm was a fusion of five algorithms submitted from East Asian countries, and the Western algorithm was a fusion of eight algorithms from Western countries.</p><p>The study showed an other-race effect for the algorithms. Specifically, performance of face recognition algorithms varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. The mechanisms underlying the other-race effect for humans are reasonably well understood and are based in early experience with faces of different races. Because the algorithms tested were black boxes, conclusions about the mechanisms underlying the algorithm effects are tentative, but the effects reported were not. The results point to an important performance variable combination that has not received much attention. The results also suggest a need to understand how the ethnic composition of a training set impacts the robustness of algorithm performance. From a practical perspective, algorithms need to be evaluated on face sets whose demographics match those at the location(s) where they will be used.</p><p>Furl et al. <ref type="bibr" target="#b29">[30]</ref> used computational models to investigate two competing hypotheses to account for the other-race effect in humans. First, the generic contact hypothesis links the magnitude of the other-race effect in individuals to the relative amount of contact they have with own versus other race faces. Thus, people who see many individuals of another race on a daily basis should have a smaller other-race effect than those who rarely see other-race individuals. This effect is modeled using a principal components analysis (PAC)-based face recognition algorithm, where the proportion of faces in the training set from two races was varied <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. Second, the developmental contact hypothesis assumes also that experience is the cause of the other-race effect, but that experience early in life (up to about 5 years of age) with other-race faces is the critical factor. The rationale for this hypothesis is that the neural system early in life tunes itself to the statistical structure of the environment as it selects a feature set that will optimally represent the stimuli encountered most frequently. Consistent with the predictions of a developmental contact hypothesis, experience-based models demonstrated an other-race effect only when the representational system was developed through experience that warped the perceptual space in a way that was sensitive to the overall structure of the model's experience with faces of different races. These models were based on combinations of PCA and Fisher discriminant analysis (FDA) applied as the system acquired features for representing faces <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. The results from this study supported a developmental contact hypothesis for the formation of the other-race effect in humans.</p><p>Traditionally, attempts to improve algorithm performance have emphasized methods that increase the degree of similarity between two images of the same person; i.e., by modeling the effects of changes in illumination between two images of the same face. Less consideration has been given to the effects of the composition of the differentidentity distributions in producing stable estimates of algorithm performance. In human experiments, the faces in different-identity pairs always have the same sex, race, and approximate age. The reason for this condition is that humans rarely confuse faces of different sexes, races, or age groups. However, in the majority of face recognition algorithm competitions, different-identity pairs are cross demographic. In fact, in many cases, the majority of different-identity pairs are crossdemographic. Inspired by the design of human experiments, O'Toole et al. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> looked at the effect of algorithm performance as crossdemographic face pairs were limited. Experiments were performed on the GBU face challenge. Performance was measured in four cases. First, performance was measured when there was no pruning of the different-identity face pairs. This is the control case. Second, the faces in all different-identity pairs were of the same sex. Third, the faces in all different-identity pairs were of the same race. Fourth, the faces in all different-identity pairs were of both the same race and sex. On the Bad partition, at a false accept rate of 1 in 1000, the verification rate was 0.79 for the control case, 0.74 and 0.74 for demographic matching on sex or race, and 0.69 for different-identity pairs with the same race and sex. In these experiments, performance was measured for the fusion algorithm <ref type="bibr" target="#b10">[11]</ref>. A similar reduction in performance, measured as verification rate, was observed for face pairs in the Ugly partition.</p><p>The simulations over the four cases showed that differences in the demographic composition of the different-identity distribution can significantly alter the estimates of algorithm performance. These estimates are important for predicting how algorithms will perform in real-world environments. Furthermore, these results pose a new and pressing challenge for the biometric community to find a method for tuning algorithm performance to the constantly changing demographic environments in which systems must operate reliably.</p><p>We end this review with two related questions. "Is an algorithm a reasonable model for human face recognition?" and "Does fusing human and machines improve performance?" Fusion is an effective tool for improving performance when the models encode complementary features. In other words, there is qualitative diversity in the way the models (human and machine) encode and recognize faces. If an algorithm is a good model for human face recognition, then there will be similar approaches to recognizing people. Thus, fusing them will not significantly improve performance. O'Toole et al. <ref type="bibr" target="#b36">[37]</ref> looked at fusing human and machine results. Experiments were performed on the difficulty set of images in the FRGC experiments, see Section 2.2.1 and Fig. <ref type="figure" target="#fig_2">2(b</ref>). The fusion algorithm was based on least partial squares regression <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. Fig. <ref type="figure" target="#fig_2">12</ref> shows the key results from the study and the total error rate that was reported. The first two cases are controls, performance on humans and the FRGC submission from NJIT algorithm. The third case is fusing all seven algorithms from the FRGC. The best results were achieved by fusing humans and all seven algorithms in the study, with an error rate of 0.003.</p><p>That study demonstrated that fusing algorithms and humans can substantially reduce the error rate. For the data set and algorithms in the study, the fused error rate was almost zero. Because of the large reduction in the error rate, the results support two significant conclusions. First, designing systems to effectively fuse humans and machines can significantly improve overall performance. Second, the mechanisms underlying the recognition process in machines and humans are qualitatively different.</p><p>The three structural studies reviewed in this section demonstrate the potential for a broad interaction between human and machine studies. This has led to improvement in experiment design, deeper understanding of the principles of face processing, and the ability to effectively combine humans and machines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future directions</head><p>The cross-modal performance analysis framework was designed to compare human and machine performance across a series of experiments. Although this framework is useful in its own right, we apply this technique to establish goals for advancing face recognition technology.</p><p>Over the last two decades, phenomenal progress has been made in automated face recognition from frontal images taken in mobile studio or mugshot environments. Results from the MBE 2010 report a false reject rates of 27 in 10,000 at a false accept rate of 1 in a 1000; and an identification rate of 0.93 from a gallery of 1.6 million faces <ref type="bibr" target="#b39">[40]</ref>. Between 1993 and 2010, the false reject rate at a false accept rate of 1 in a 1000 has decreased by a factor of two every two years <ref type="bibr" target="#b8">[9]</ref>.</p><p>Clearly, this level of performance cannot be achieved for faces acquired under all conditions. The question then becomes: What is a reasonable performance bound or goal? We propose a goal based on human performance. Establishing a goal based on one set of images will not adequately characterize a problem. To provide the needed benchmarking data, performance should be characterized by a set of experiments, where each experiment focuses on a different aspect of the challenge. For example, the analysis in Section 3 is conducted on a set of 16 experiments. We formalize this concept as a Face Performance Index. The goal in designing a face performance index is to select a set of experiments that adequately characterize performance under the range of conditions that are relevant to a problem.</p><p>How does the CMPA framework and a face performance index establish a performance benchmark relative to humans? The first step is to create a face performance index that consists of a set of experiments that adequately characterizes human performance. Initially, this face performance index could be a first order approximation. Later iterations of this index would provide better approximations.</p><p>For each experiment, human and machine performance is measured, the appropriate performance statistic is computed, and plotted on a CMPA scatterplot. In our analysis the performance statistic was the AUC. The goal of combining CMPA and a face performance index is to spur progress. This is illustrated in Fig. <ref type="figure" target="#fig_8">13</ref>, which is adapted from Fig. <ref type="figure" target="#fig_1">11</ref>. The green region in the upper left region of the figure is the 'goal box.' The goal box is the region of the plot where algorithms perform better than humans and algorithm performance is better than random. When all of the experiments are in the goal box, then within the CMPA framework, the performance of an algorithm on a face performance index is better than humans. The fraction of experiments that are in the goal box is a measure of the success of an algorithm.</p><p>To illustrate the process, we create a notational challenge problem. Here a face performance index is created by augmenting the 16 experiments from Section 3 with additional notational experiments, which are annotated with blue +s. The position of the +s represents performance at the start of a challenge problem, which assumes that performance of humans is superior. The goal of the challenge is to improve algorithm performance so the experiments represented by the + s are in the goal box, which is illustrated by the yellow arrows.</p><p>In the analysis in this paper, performance is measured for averagerecognizers who have not received training. One common assumption is that trained law enforcement officers and forensic examiners are better face recognizers. Russell et al. <ref type="bibr" target="#b40">[41]</ref> reported the existence of four super-recognizers. However, there are few published papers that report the performance of law enforcement officers or forensic examiners and their results are mixed <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>.</p><p>The goal could be updated to have machines match the abilities of super-recognizers, trained law enforcement professionals, or forensic face examiners. The goal of the notational challenge is absolute, better than humans. By modifying the goal-box region, the goal can be relative, with machine performance better than humans by a fixed factor. These two modifications illustrate the flexibility of the CMPA and face performance index for formulating challenge problems.</p><p>The discussions in this paper have focused on the recognition accuracy of machines and humans on a verification task. This ignores many other aspects of performance of a face recognition system. Face recognition systems can search millions of mugshots, adjust to changing watch lists on demand, and process face imagery work 24 h a day. Humans are substantially better at recognizing familiar faces than unfamiliar faces. The algorithm development community has focused on recognition of unfamiliar faces. A challenge for the algorithm community is developing techniques that achieve human-level performance for familiar faces. This will most likely include developing an understanding of when this accuracy can be achieved.</p><p>Accepted conventional wisdom in the face recognition community is that humans are the most robust face recognition system. Humans perform face recognition across numerous imaging conditions; i.e., changes in natural illumination, pose, expression, imaging artifacts, etc. Like algorithms, human performance varies greatly under natural imaging conditions <ref type="bibr" target="#b44">[45]</ref>. Also, human recognition of a person improves as a face transition from unfamiliar to familiar, and humans intuitively integrate all identify cues during recognition. The CMPA and face performance index has the potential to assist in developing algorithms that have these human capabilities. In turn, algorithms have the potential to serve as computational models that assist in explaining human face processing. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a pair of images used in experiments comparing identities in images captured in a studio environment (a) and an ambient environment (b).</figDesc><graphic coords="2,133.80,520.55,336.66,207.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>• 1 .</head><label>1</label><figDesc>) You are sure they are the same person; • 2.) You think they are the same person; • 3.) You don't know; • 4.) You think they are different people; • 5.) You are sure they are different people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Human and machine performance on the FRGC data set. (a) Performance on the easy face pairs and (b) performance on the difficult face pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Human and machine performance on the GBU partitions. (a) Human performance on all three partitions. Comparison of human and machine performance by partition: (b) Good, (c) challenging, and (d) very challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of one set of images from the extremely hard face pair study. (a) Original image. (b) Image with the face masked. (c) Image with only the face visible, where the background, body, and hair have been masked.</figDesc><graphic coords="7,55.05,53.18,208.59,470.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example imagery from the video experiments. (a) Frames from a sequence of a person walking towards the camera. (b) Frames from a sequence of two people talking. The goal is to recognize the person facing the camera. (c) One frame from a floating head sequence that only contains the head and face of a person.</figDesc><graphic coords="8,105.28,53.18,393.51,240.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Human and machine performance on the video challenge. (a) Human performance on all three conditions on the original video sequences. Comparison of human and machine performance by condition: (b) walking vs walking, (c) conversation vs walking, and (d) conversation vs conversation. Human performance is reported for both the original and face only video sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Properties of CMPA scatterplot for comparing human and machine performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. The CMPA framework and a face performance index for setting goals for machine performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,107.60,53.18,371.23,281.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Camera size measured in megapixels and average face-size measured in pixels between centers of the eyes broken out by competition and illumination condition.</figDesc><table><row><cell>Illumination</cell><cell>Competition</cell><cell>Camera size</cell><cell>Average face size</cell></row><row><cell></cell><cell></cell><cell>(megapixels)</cell><cell>(pixels)</cell></row><row><cell>Studio</cell><cell>FRGC</cell><cell>4</cell><cell>261</cell></row><row><cell>Ambient</cell><cell>FRGC</cell><cell>4</cell><cell>144</cell></row><row><cell>Studio</cell><cell>FRVT-Notre Dame</cell><cell>6</cell><cell>400</cell></row><row><cell>Ambient</cell><cell>FRVT-Notre Dame</cell><cell>6</cell><cell>190</cell></row><row><cell>Studio</cell><cell>FRVT-Sandia</cell><cell>4</cell><cell>350</cell></row><row><cell>Ambient</cell><cell>FRVT-Sandia</cell><cell>4</cell><cell>110</cell></row><row><cell>Ambient</cell><cell>GBU</cell><cell>6</cell><cell>175</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>11].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Very-high resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Verification rate</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Verification rate</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Viisage-norm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Viisage-norm</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tsinghua2-norm SAIT-norm</cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tsinghua2-norm SAIT-norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NevenVision1-norm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NevenVision1-norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Identix1-norm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Identix1-norm</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cognitec1-norm Sagem2-norm</cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell>Cognitec1-1to1 Sagem1-1to1</cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False accept rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">False accept rate</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Performance on fusion experiments. Total error rate is reported for humans, NJIT, all algorithms fused, and all algorithms and humans fused.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fusing Humans and Machines</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Human &amp; All Algorithms</cell><cell>0.003</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>All Algorithms</cell><cell>0.059</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NJIT</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0.02 0.04 0.06 0.08</cell><cell>0.1</cell><cell>0.12 0.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error Rate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human &amp; All Algorithms</cell><cell>Algorithms</cell><cell>Human Alone</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig. 12.</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRVT2006-Notre Dame</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRVT2006-Sandia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC-difficult</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC-easy</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GBU-Good GBU-Bad</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GBU-Ugly</cell></row><row><cell>Algorithm AUC</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video: Walking vs Walking Video Face Only: Walking vs Walking Video: Activity vs Walking Video Face Only: Activity vs Walking Video: Activity vs Activity Video Face Only: Activity vs Activity</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Extremely-difficult Body Only</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Extremely-difficult Face Only</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Extremely-difficult Face &amp; Body</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Human AUC</cell><cell></cell></row></table><note><p>Fig. 11. CMPA analysis for experiments reviewed in Section 2.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>P.J. Phillips, A.J. O'Toole / Image and Vision Computing 32 (2014) 74-85</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Information on obtaining the FOCS can be found at http://face.nist.gov.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The video challenge was originally included in the Multiple Biometrics Grand Challenge (MBGC)<ref type="bibr" target="#b18">[19]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Fig. 4. Examples of face pairs of the same person from each of the GBU partitions: (a) good, (b) challenging, and (c) very challenging.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>PJP was supported by the Federal Bureau of Investigation and AJO was supported by the Department of Defense. The identification of any commercial product or trade name does not imply endorsement or recommendation by NIST or U of Texas at Dallas.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition algorithms surpass humans matching faces across changes in illumination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pénard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1642" to="1646" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Humans versus algorithms: comparisons from the FRVT</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narvekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FRVT 2006 and ICE 2006 large-scale results</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="831" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing face recognition algorithms to humans on challenging tasks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Appl. Percept</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Untangling object recognition:which neuronal population codes can ex-7Q1125 plain human object recognition performance?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation: Population 713 Coding of High-Level Representations</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representational similarity analysisconnecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Syst. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing people from dynamic and stable faces and bodies: dissecting identity with a fusion approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unaware person recognition from the body when face identification fails</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2235" to="2243" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving face recognition technology</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput</title>
		<imprint>
			<biblScope unit="page" from="96" to="98" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to the good, the bad, and the ugly face recognition challenge problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahibzada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>Ninth IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The CSU face identification evaluation system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teixera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="128" to="138" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computational and performance aspects of PCA-based face-recognition algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="303" to="321" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Capitalize on dimensionality increasing techniques for improving face recognition performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="725" to="737" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Husken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brauckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
		<title level="m">Strategies and benefits of fusion of 2D and 3D face recognition, in: IEEE Workshop on Face Recognition Grand Challenge Experiments</title>
		<imprint>
			<publisher>Computer Society Digital Library</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel correlation filter based redundant class-dependence feature analysis (KCFA) on FRGC2.0 data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop Analysis and Modeling Faces and Gestures</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A video database of moving faces and people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="812" to="816" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zahibsada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Scallan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weimer</surname></persName>
		</author>
		<title level="m">Proceedings Third IAPR International Conference on Biometrics</title>
		<meeting>Third IAPR International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Overview of the multiple biometrics grand challenge</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dictionary-based face recognition from video</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Egan</surname></persName>
		</author>
		<title level="m">Signal Detection Theory and ROC Analysis</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Creelman</surname></persName>
		</author>
		<title level="m">Detection Theory: A User&apos;s Guide</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to face recognition and evaluation of algorithm performance</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="236" to="247" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factors that influence algorithm performance in the Face Recognition Grand Challenge</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="750" to="762" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognition for faces of own and other race faces</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Malpass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="330" to="334" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Thirty years of investigating the own-race bias in memory for faces: a meta-analytic review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Brigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Public Policy Law</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3" to="35" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-racial identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Bothwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Brigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Malpass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personal. Soc. Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="19" to="25" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-analysis of face identification studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Penrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An other-race effect for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Appl. Percept</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face recognition algorithms and the other-race effect: computational mechanisms for a developmental contact hypothesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Furl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="797" to="815" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Application of the karhunen-loeve procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminant analysis of principal components for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition: From Theory to Applications</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Fogelman Soulie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminant analysis for recognition of human face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1724" to="1733" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eigenfaces vs fisher faces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Demographic effects on estimates of automatic face recognition performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Ninth International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>Ninth International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Demographic effects on estimates of automatic face recognition performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunlop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fusing face recognition algorithms and humans</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1149" to="1155" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partial least squares regression and statistical models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Helland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scand. J. Stat</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Subspace, Latent Structure and Feature Selection Techniques</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Report on the evaluation of 2D still-image face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mbe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NISTIR</title>
		<imprint>
			<biblScope unit="volume">7709</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Super-recognizers: people with extraordinary face recognition ability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Duchaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="252" to="257" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face recognition in poor-quality video: evidence from security surveillance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Matching unfamiliar faces from poor quality closed-circuit television (CCTV) footage</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Houston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Axis Online J. CAHId</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are facial image analysis experts any better than the general public at identifying individuals from CCTV images?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Justice</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="191" to="196" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variability in photos of the same face</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Van Montfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
