<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weihua</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">Zhuyun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical &amp; Automotive Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<addrLine>Celestijnenlaan 300</addrLine>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7CA335D239CEDFC64690CDECCD5E833</idno>
					<idno type="DOI">10.1109/TII.2019.2917233</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2917233, IEEE Transactions on Industrial Informatics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fault diagnosis</term>
					<term>Transfer learning</term>
					<term>Convolutional Neural Network</term>
					<term>Rotary machinery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks present very competitive results in mechanical fault diagnosis. However, training deep models require high computing power while the performance of deep architectures in extracting discriminative features for decision making often suffers from the lack of sufficient training data. In this paper, a Transferable Convolutional Neural Network (TCNN) is proposed to improve the learning of target tasks. Firstly, a one-dimension CNN is constructed and pre-trained based on large source task datasets. Then a transfer learning strategy is adopted to train a deep model on target tasks by reusing the pre-trained network. Thus, the proposed method not only utilizes the learning power of deep network but also leverages the prior knowledge from the source task. Four case studies are considered and the effects of transfer layers and training sample size on classification effectiveness are investigated. Results show that the proposed method exhibits better performance compared with other algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE effective fault detection and diagnosis techniques are of great importance in ensuring the safe and reliable operation of complex mechanical systems. Gears and rolling-element bearings, being vital components, are often the main sources of failure in rotating machines. In order to detect early, accurately and on-time the generation of faults, a number of diagnosis methods have been proposed including signal processing techniques and data driven methods.</p><p>The former techniques e.g. time-domain, frequency-domain and time-frequency analysis provide clear physical interpretations, but require high level diagnostic expertise and may fail when incipient or compound faults are developed in machinery operating under varying conditions <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref>. The latter methods such as artificial neural network, support vector machines and manifold learning <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> may be more suitable for complex diagnosis problems, but their performance relies strongly on the quality of the hand-crafted features <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>.</p><p>Since 2006, deep learning has emerged as a new branch of artificial intelligence. Deep learning methods, such as Deep Belief Network (DBN), Auto-Encoder (AE) and Convolutional Neural Network (CNN) present significant advantages in solving varieties of classification problems. Compared to other intelligent fault diagnosis methods, deep networks containing multiple hidden layers are able to learn useful discriminative features from the raw data itself. Furthermore, hierarchical distributed features learned layer-by-layer from large amounts of mechanical data turn out to be more effective and robust than manually selected or hand-crafted features <ref type="bibr" target="#b8">[9]</ref>. Therefore, deep learning presents the potential to overcome the aforementioned deficiencies in the current intelligent diagnosis methods.</p><p>In recent years, deep learning methods have been also proposed for mechanical fault diagnosis and prediction <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Shao <ref type="bibr" target="#b12">[13]</ref> used an AE method for electrical locomotive roller bearing fault diagnosis based on raw vibration signals. Sun <ref type="bibr" target="#b13">[14]</ref> presented a sparse Deep Stacking Network (DSN) to model the sparsity of output labels achieving improved motor diagnosis accuracy. An improved Local Connect Network (LCN) with Normalized Sparse AE (NSAE) was constructed in <ref type="bibr" target="#b14">[15]</ref> to extract dissimilar and meaningful features for bearing and gear diagnosis. Moreover, multiple sensor signals have been fused in <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> for improving the mechanical fault classification performance.</p><p>In addition to the fully-connected networks like DBN and AE, CNN has been also proposed for fault detection due to its good local representation and invariance. <ref type="bibr">Sun [19]</ref> presented a novel convolutional discriminative feature learning method for induction motor fault diagnosis. Guo <ref type="bibr" target="#b19">[20]</ref> adopted the 2DCNN hierarchical framework with an adaptive learning rate to recognize bearing fault categories and sizes. Liu <ref type="bibr" target="#b20">[21]</ref> developed a dislocated time series CNN (DTS-CNN) to capture the relationship between different fault signals, presenting improved fault classification capability of an induction motor. Furthermore, in <ref type="bibr" target="#b21">[22]</ref>, a deep residual network with wavelet coefficients has been proposed for fault diagnosis of planetary gearboxes.</p><p>In addition, CNN with a 1D convolutional kernel (1DCNN) has been used in <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>  appears to be more suitable for 1D time series data. More specifically, a deep CNN with wide first-layer kernels (WDCNN) has been proposed in <ref type="bibr" target="#b24">[25]</ref>, achieving a 100% testing accuracy on the publically available CWRU bearing data set. As mentioned above, different networks and architectures have been explored to conduct machinery fault detection and diagnosis, achieving good performance. However, the advantages of the evolution in deep learning techniques have not been fully exploited yet. Usually, there is the assumption that the training data and the test data have the same or similar distribution when using machine learning based methods for fault diagnosis. For a given fault diagnosis task, if there are sufficient training samples, an effective model for fault classification can be trained. However, in real industry applications, it is hard to collect sufficient fault samples for training models. Additionally, each time the model is applied to a new diagnosis task, it needs to be re-trained. The working conditions and the working environment might diverse significantly, leading to obvious differences between the training data (used for learning the model, usually obtained in experiments) and the real data (which should be monitored and diagnosed). As a result, the learned model might not be as effective at the testing phase as in the training phase.</p><p>Fortunately, transfer learning provides a way to deal with such problems. Massive data can be obtained in laboratory experiments by fault simulation, and thus the model can be trained sufficiently. Through the transfer of the knowledge learned from the experimental data (source domain), the learned model can be used for another similar task (target domain).</p><p>Moreover, deep learning diagnosis methods, such as CNN and DBN, may suffer a significant loss in performance when applied in a new diagnosis task having available only a small amount of data, even if the new task is similar to the original one. This problem usually occurs as the deep network easily overfits on the small training data and leads to poor performance on the testing data.</p><p>In an effort to deal with such diagnosis problems and motivated by the transfer learning, TCNN method is proposed to leverage source domain diagnosis knowledge, trying to save time and to improve the performance of processing new diagnosis issues in the target domain.</p><p>Transfer learning aims to transfer knowledge learned from related domains to help improve the learning performance of a target task with a small training data. It practically relaxes the assumption that the source and the target datasets must be in the same feature space and have the same distribution, which provides a useful scheme to reduce the need to re-collect training data of enough size <ref type="bibr" target="#b25">[26]</ref>.</p><p>Transfer learning, as an effective method, has achieved remarkable success based on deep model in a number of vision recognition tasks <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. In the field of fault diagnosis, transfer learning with deep neural networks has been less explored due to the limitation of domain-specific dataset of sufficient size and common deep network model. Lu <ref type="bibr" target="#b29">[30]</ref> proposed a deep neural network model combined with Maximum Mean Discrepancy (MMD) for fault diagnosis.</p><p>Zhang <ref type="bibr" target="#b30">[31]</ref> constructed an artificial neural network and used the first-layer weights trained in one operating condition for target task. Sun <ref type="bibr" target="#b31">[32]</ref> proposed a Sparse Auto-encoder (SAE), which was trained by historical failure data and then transferred to a new tool for remaining useful life (RUL) prediction. Shao <ref type="bibr" target="#b32">[33]</ref> utilized directly a pre-trained CNN model for bearing fault diagnosis. However, expertise knowledge is required to generate time-frequency images and the features obtained from the natural image task are different from the diagnosis information, which could lead to a reduction of performance.</p><p>In this paper a transfer learning approach for fault detection in rotating machinery is proposed, inspired by the success of transfer learning in image processing <ref type="bibr" target="#b26">[27]</ref>, web page categorization <ref type="bibr" target="#b27">[28]</ref> and medical disease recognition <ref type="bibr" target="#b28">[29]</ref>. Specifically, a transfer learning framework based on TCNN is developed to explore a strong intelligent fault diagnosis scheme, which provides to the model an ability to learn general representations. Those representations can enable a wide variety of tasks and can adapt quickly to new fault diagnosis issues with less human intervention. In the proposed method, raw time-series signals, collected from source domain datasets, are directly used to train the designed CNN without any hand-crafted feature extraction. Then TCNN obtained from the pre-trained model can be adopted to transfer source domain knowledge to new target domain tasks improving the diagnosis performance.</p><p>The contributions of the proposed TCNN can be summarized as follows:</p><p>(1) The proposed method can be used to deal with fault diagnosis problems lacking training data, for equipment working under different conditions, and even more at different facilities (under the term that the tasks are similar). Moreover, it is promising to be applied in practical industry applications.</p><p>(2) The proposed method gives to the target model reasonable parameter initializations by a pre-training strategy.</p><p>Therefore, it provides a potential tool to train a deep network-based diagnosis system fast and efficiently with less overfitting risk. It can improve the model performance as well as save time.</p><p>(3) From the perspective of model transfer, the proposed scheme can be used not only for CNN model, but also can be extended to other deep learning algorithms like DBN, SAE, LSTM, etc.</p><p>The rest of the paper is organized as follows. In Section II, the CNN basic theoretical background is given. In Section III, the proposed TCNN intelligent diagnosis method is introduced. The details of the experimental datasets, including gearbox datasets and bearing datasets are described in Section IV. In Section V, the results of the application of the proposed method on the measurements are presented and analyzed. Finally, some key conclusions are made in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CONVOLUTIONAL NEURAL NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Neural Network (CNN)</head><p>The CNN, usually consisted of an input layer, multiple hidden layers and an output layer, is well known for its shared-weight architecture and some degree of translation-invariant characteristics. It can extract local features at lower layers and then combine them into more abstract features at higher layers. The basic CNN architecture, including a convolutional layer, a pooling layer and a fully-connected layer is further introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Convolutional layer</head><p>The convolutional layer usually is made of a set of learnable kernels and one trainable bias per feature map. The kernel size corresponds to the length of the convolution window and the kernel depth or kernel filter corresponds to the number of the feature map outputs. The output of neurons, which are connected to the input volume can be obtained by computing the dot product between their weights and the small region. Considering a L-layers CNN architecture, the l-layer convolved feature maps can be expressed as:</p><formula xml:id="formula_0">, , 1, y k conv l pool l l j i j l i j i y w b - = * + ∑<label>(1) Re , , 1, ( ) [0,</label></formula><p>]</p><formula xml:id="formula_1">LU conv conv l j l j l j y f y max y - = =<label>(2)</label></formula><p>where , conv l j y is the output of the l-th layer, l w is the convolutional kernel of the l-th layer, k is the number of kernels, and l b is the bias. ( ) f ⋅ is the activation function which transforms the input to the output map in order to increase the nonlinear property. An activation function ReLU (Rectified Linear Unit) expressed as (0, x) max is usually used instead of a sigmoid. ReLU is proved to work far better in most of the classification tasks for its capacity to accelerate the convergence and alleviate the vanishing gradient problem <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Pooling layer</head><p>After the convolutional layer, an additive pooling layer is applied to each feature map from the previous layer. Pooling is a form of non-linear down-sampling, which can reduce each map size and the network parameters, achieving spatial invariance. Two common pooling strategy choices are the average and the max pooling. Max pooling is generally favorable as it can lead to faster convergence, select superior invariant features and improve generalization. The max pooling function is given by: ( ) ( , ) p s s is the window function applied to the input patch, s1 and s2 correspond to the size of the window, which can be of arbitrary size and overlapping and M×N corresponds to the size of the l-layer feature map output.</p><formula xml:id="formula_2">Re , 1<label>2 1, y ( , )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Fully-connected layer</head><p>The feature map outputs of the last max pooling layer are reshaped into a vector. The neurons in fully-connected layers have fully connection to all activations in the previous layer. Finally, a Softmax classifier is attached to discriminate the different classes. Similar to the fully-connected network learning procedure, CNN can be effectively trained by minimizing the supervised loss function:</p><formula xml:id="formula_3">1 ( ) (y , (x); ) c j j j L L f = θ = θ ∑<label>(4)</label></formula><p>where c corresponds to the number of classes and yj is the</p><formula xml:id="formula_4">1-of-c code of the training labels. (x) j f</formula><p>is the network output of the j-th label of the sample. The cross entropy is used instead of the squared-error loss function, improving the update speed of model parameters. Then Eq. ( <ref type="formula" target="#formula_3">4</ref>) can be rewritten as:</p><formula xml:id="formula_5">1 1 1 1 1 1 1 ( ) 1{y }log(p(y | x ; )) 1 = 1{y }log T L j i T L l i m c L i i i i j m c i c i j l L j j m e j m e - - - = θ θ = = θ = - = = θ - = ∑∑ ∑∑ ∑ x x (5) 1 1 ( ) (1{y } p(y | ; )) o j m i i i i w i J j j m = ∇ θ = - = - = θ ∑ x x (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where m is the number of training samples and the output layer parameters o j w can be updated using Eq. ( <ref type="formula" target="#formula_5">6</ref>). Other layer parameters can be successively adjusted according to the rule of back propagation algorithm and stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED TRANSFER SCHEME</head><p>CNN has demonstrated powerful modeling capabilities in extracting local features and in obtaining hierarchically discriminative representations at different layers. 1D CNN appears as an efficient tool for processing vibration signals for mechanical fault diagnosis. In this paper a novel TCNN architecture is proposed in order to improve the learning of efficient discriminative features from raw data. TCNN is a modified version of WDCNN <ref type="bibr" target="#b24">[25]</ref>, where dropout techniques <ref type="bibr" target="#b34">[35]</ref>, kernel numbers and fully-connected layers are added.</p><p>As presented in Fig. <ref type="figure" target="#fig_1">1</ref>, the 1D raw vibration signals are firstly input into the first convolutional layer to achieve signal local features. Then Batch Normalization (BN in Fig. <ref type="figure" target="#fig_1">1</ref>) is implemented to reduce the distribution of each layer's input by performing normalization for each training mini-batch. Max Pooling is used to down-sample the input and to create position invariance over larger local regions. Dropout is added as a regularization constraint to reduce node interactions and to learn robust features. Then a non-linear map is conducted layer-by-layer by forward propagation. The last Softmax is used to convert the probability output of the categories. The parameters of each layer are updated by using a back propagation algorithm and minimizing the cross entropy error. network layers is explored. Grid-search method in a relatively small ranges of layers is conducted to find a sub-optimal architecture on the source domain dataset. A threshold of testing accuracy (99%) is set as stop criterion, and the optimal architecture of TCNN is listed in Table <ref type="table">I</ref>. The layers and the parameters of TCNN, which are subdivided into six building blocks (B1-B6), are presented in Table <ref type="table">I</ref>. Each building block contains specific layers. The first five blocks consist of Convolutional, Max Pooling, BN and Dropout layers. Following the last dropout layer, the output layer, represented by B6, is designed. It includes the fully-connected layers FC1, FC2 and a Softmax classifier which have embedded multiply non-linear layers of BN and dropout. In the case of one-dimensional (1-D) vibration signal, the first convolutional layer extracts the features from the raw input, where a large convolution kernel (specifically, 64*1 with stride 16) is applied. The number of the filter is set to 32. After that, the rest of the four convolutional layers have 32, 64, 64 and 64 kernels, respectively. Each of them have small kernel sizes (specifically, 3*1 with stride 2). The large kernel is adopted with the purpose to suppress high frequency noise while small kernels in the following layers help to enhance the feature learning capability and improve the network performance. In addition, zero-padding is used to keep the same size before and after the convolution operation. For the pooling layer, the number of kernels in each building block is the same as that in the convolutional layers (e.g. 32 kernels in the first pooling layer), but, compared with the previous output, the feature map size is halved by performing down sampling with a pooling size of 2*1 and stride 2. Finally, the network ends with two fully-connected layers (specifically, 1000 and 100 nodes, respectively) and the Softmax for classification.</p><p>As CNN is a supervised forward network, fully labeled source domain datasets are used. In this scheme, as it will be analytically presented later, large source task datasets are firstly collected Then the CNN is pre-trained based on the source domain datasets to obtain a Transferable CNN (TCNN). During the transfer stage, as the labels of the source domain dataset are usually not equal to those of the target task, the Softmax output of B6 in TCNN model is replaced with a new one, corresponding to the categories of the different target tasks, as shown in Fig. <ref type="figure">2</ref>. Finally, the parameters of the different layers are fine-tuned on the target data with a small number of training samples.</p><p>The transfer scheme of TCNN for fault diagnosis involves seven steps:</p><p>1) Source domain datasets and target domain datasets are collected from different experimental platforms.</p><p>2) The source domain and target datasets, are respectively divided into training data and testing data. All feature vectors are normalized into the range [-1, 1].</p><p>3) CNN model is constructed with multiple convolutional, max pooling, BN, dropout and fully-connected layers. TCNN model is constructed by pre-training CNN model with the source domain samples.</p><p>4) The Softmax output of B6 in TCNN is replaced by an adaptive one, which corresponds to the categories of the target case. The training instances of target domain datasets are used for fine-tuning of the B6 of TCNN, while the other layer parameters are frozen.</p><p>5) TCNN is fine-tuned by fixing the last two blocks B5 and B6, while the rest of the layer's parameters are frozen.</p><p>6) The learning process in step 5 is repeated until the classification rate reaches the given value or the given iterations are completed, which means that the optimized transferable layers are determined.</p><p>7) Testing samples of target cases are fed into the optimized TCNN and the discriminative classes are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DESCRIPTION OF EXPERIMENTAL DATASETS</head><p>Transfer learning strategies depend on various factors. Among them, the two most important ones are its similarity to      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Source domain datasets (D s )</head><p>The source domain datasets include a bearing dataset (D s 1 ) and a gearbox dataset (D s 2 ), which are used to train the proposed transferable CNN (TCNN).</p><p>The bearing dataset (D s 1 ) is obtained from the Case Western Reserve University (CWRU) Bearing Data Center <ref type="bibr">[36]</ref>. The test rig is shown in Fig. <ref type="figure">3</ref>(a) and consists of a 2 HP Reliance Electric motor, a torque transducer and a dynamometer. During the test, the vibration signals were collected under three different working conditions ('1', '2' and '3' HP at speeds ranging from 1772 rpm to 1730 rpm), and the sampling rate is 12 kHz. Three kinds of defects (inner race defect, outer race defect and ball defect) were artificially introduced into the deep-groove ball bearings (Type: 6205-2RS JEM SKF) in different severity levels (0.007, 0.014, and 0.021 inch in diameter, and 0.011 inch in depth for each case). Ten bearings are involved in the experiment, one healthy bearing and nine faulty ones with different defects or different fault severity levels, and each of them is taken as one class. The length of each sample is set as 2000 data points. For every bearing, there are 800 samples for each running condition, thus in total 2400 samples for three working conditions. Among them, 1500 samples were used for training, and the rest for testing. The details are listed in Table <ref type="table" target="#tab_3">II</ref>.</p><p>The gearbox dataset (D s 2 ) was collected on a five-speed automobile transmission. The transmission test rig is shown in Fig. <ref type="figure">3(b)</ref>. The main components of the experimental apparatus include a driven motor, a torque transducer, a gearbox, and a loading motor. The gearbox has five forward gear pairs and one backward gear pair. In the experiment, the fifth speed gear is used to conduct the fault test, and the test was performed under two running speeds (500 rpm and 750 rpm), while the sampling rate was 24 kHz. The output shaft load torque was 50 N•m. In order to simulate different fault types, gear faults (minor chipped tooth, half chipped tooth and missing tooth) and bearing faults (inner race defect) with different fault diameters (0.2 mm and 2 mm) have been introduced by electro-discharge machining. 900 samples have been collected under every working condition, and thus 1800 samples for every class. Among them, 1000 samples are used for training and the remaining for testing. Considering different combinations of bearing faults and gear faults, there are in total ten classes of different faults. More details can be found in Table <ref type="table" target="#tab_3">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Target domain datasets (D t )</head><p>The target domain datasets containing a gearbox dataset (D t 1 ) and a bearing dataset (D t</p><p>2 ), are used to evaluate the proposed method.</p><p>The gearbox dataset (D t 1 ) has been acquired on the same platform of the source domain gearbox dataset with the same fault configuration but operating under different speeds (1000 rpm and 1250 rpm). 300 samples have been collected under each running condition. This dataset is used to evaluate the generalization of the proposed TCNN under different working conditions. More details are listed in Table <ref type="table" target="#tab_5">IV</ref>.</p><p>The bearing dataset (D t 2 ) has been obtained from the experiments conducted on a test bench, shown in Fig. <ref type="figure">3(c</ref>). The shaft is driven by an induction motor through a belt connection. The shaft output end is supported by the testing bearing.</p><p>A healthy bearing and two sets of faulty bearings (inner race defects and outer race defects) have been used in the experiments. For each fault, two different defect severity levels (0.5 mm and 2 mm in diameter) have been introduced and totally 5 health conditions have been generated to simulate different fault types and defect severities. An accelerometer has been attached vertically to the bearing housing to acquire the vibration signals under running speeds of 800 rpm and 1100 rpm, respectively. The sampling frequency is 12 kHz and 300 samples have been collected under each running condition. This dataset is used to evaluate the generalization performance of TCNN on different equipment. More details are listed in Table <ref type="table">V</ref>.</p><p>Additionally, in order to assess the performance of the proposed method, the datasets (D t ) are grouped into four cases (C1-C4) according to the running operating conditions. The dataset (D t 1 ) is divided into C1 and C2, containing vibration data captured under the constant rotating speed of 1250 rpm and the combined speeds of 1000 rpm and 1250 rpm, respectively. Dataset (D t</p><p>2 ) was divided into C3 and C4 with constant rotating speed of 1100 rpm and combined speeds of 800 rpm and 1100 rpm respectively. For each case, 200 samples are used for training and the rest 100 samples for testing. For the gear and the bearing dataset, usually, the data combined under variable speed conditions are more difficult to be classified due to the change of the characteristic fault frequencies. Therefore, C2 and C4 were exactly designed to emulate the fault condition with varying speeds, introducing an extra complexity compared to C1 and C3. The details of target cases are illustrated in Table <ref type="table" target="#tab_5">IV</ref> and<ref type="table">Table V.</ref> V. EXPERIMENTAL RESULTS ANALYSIS In this section, the proposed method is applied and a comparison with four state-of-the-art methods is performed based on the source domain datasets with twenty categories of gear and bearing faults where large labeled training data are available. After proving its superiority, transfer learning is applied on the target datasets. The feature learning ability and the transfer effectiveness are analyzed. Finally, a number of comprehensive comparisons with other algorithms are conducted to evaluate the effectiveness of the modified TCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on source domain datasets</head><p>In order to examine the superiority of the modified TCNN, a comparison of TCNN with CNN-Wen <ref type="bibr" target="#b11">[12]</ref>, 2DCNN <ref type="bibr" target="#b19">[20]</ref>, 1DCNN <ref type="bibr" target="#b22">[23]</ref> and WDCNN <ref type="bibr" target="#b24">[25]</ref>, all trained on the source domain training set and evaluated on the corresponding testing set, is made. For the CNN-Wen and the 2DCNN, all the samples are converted into 40×50 images from the original 2000 data points, which are suitable for network input. Adam optimization algorithm is employed to update the parameters due to its computational effectiveness. The max training epoch is set as 200 with a batch size of 100. All computations have been performed on a computer with Intel Xeon E5-262v3 (2.4GHz), 64GB RAM, 4 TITAN X graphics cards and Google TensorFlow framework <ref type="bibr" target="#b35">[37]</ref>.</p><p>The classification process is repeated 10 times and the final results are averaged throughout all the experiments. The average accuracy and the standard deviation (STD) of the training and the testing stage are presented in Table <ref type="table" target="#tab_7">VI</ref>. From the results, it can be seen that all the other four methods obtain high training accuracies, but the testing accuracies are lower than the proposed method. 1DCNN and WDCNN obtain a testing accuracy of 94.32% and 96.54% with a standard deviation of 2.92% and 1.61%, performing better than CNN-Wen and 2DCNN. It is possible that a CNN with one-dimensional (1D) architecture is more effective than the 2DCNN in capturing discriminative features from raw vibration signal inputs. Which contribute to high classification performance. In contrast, TCNN presents very competitive results in testing accuracy and standard deviation compared with the other four methods. The proposed method achieves a testing accuracy of 99.03% with a standard deviation of 0.21%, showing the superiority of the modified TCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model-based transfer learning in TCNN</head><p>In order to study the influence of the training sample size on the classification rate, initially a reduced dataset and the full training dataset are used for fine-tuning of TCNN in the training stage. In the cases of gear data (C1 &amp; C2), the reduced dataset consists of 30% of the training samples, whereas in the case of bearing data (C3 &amp; C4) of 50% of the training. For TCNN fine-tuning, the Stochastic Gradient Descent (SGD) is utilized to adjust the network weights with a batch size equal to 50. Grid search is used to find good learning rate and momentum. Finally, the learning rate is chosen as 0.01 and the momentum is set as 0.97 with 100 epochs, so that a minor change is allowed for each parameter update during the fine-tuning procedure. The classification accuracies with respect to the different fine-tuning layers for all four cases are displayed in Fig. <ref type="figure" target="#fig_4">4</ref>. The x-axis (S1-S6) represents the number of the fine-tuned layers, while the y-axis gives the corresponding classification results. It can be noticed that by just fine-tuning the last output layers B6, keeping the parameters of the rest layers (B1-B5) unadjusted, achieves a relative high accuracy in C1 and C2 (corresponding to the results shown in axis S1) but not enough high accuracy in C3 and C4. Intuitively it can be explained that the learnt representations from the high layers of TCNN could be more specific to the task it's trained for. As C3 and C4 are less similar to the source domain dataset, many of the high layer features cannot be directly reused. Therefore, only fine-tuning the output layer B6 results in poor performance.</p><p>Furthermore, there is a progressively increase in the classification performance when more layers are fine-tuned. The accuracy reaches at least 95% in C1 and C2 and 90% in C3 and C4 by just fine-tuning the last two blocks using the total of training samples. When the fine-tuned layers increase to four blocks (corresponding to the result of S4), the improvements for the testing accuracies are not so obvious.</p><p>Additionally, for the reduced training samples, a significantly decrease in the classification performance occurs when all the layers are readjusted. It is noted that fine-tuning all layer parameters does not perform as well as with the bottom layer weights fixed. This is mostly due to the fact that in B1, there is a rapid increase in the number of the convolutional kernel size and filters which bring in a high number of parameters to be fine-tuned. When few training instances are used to fine-tune TCNN, although TCNN have enough capacity to process much information, the information contained in the small training data is not enough to train all the neurons of the hidden layers. As a result, overfitting problems occur during the training process, which reduce the ability to predict input data and lead to poor performance in testing instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with CNN (without transfer)</head><p>In order to verify the reliability and extensibility, an evaluation is conducted to investigate if TCNN model can improve the classification accuracy in the target cases compared with the baseline CNN. TCNN and CNN share the same network structure and TCNN is actually the pre-trained CNN on the source domain datasets. The difference between the two models is that, the weights of the networks are different. For target domain tasks, the CNN was initialized randomly with random weights, whereas TCNN has already been pre-trained to have relatively 'good' weights. The comparison is used to evaluate the effectiveness of transfer learning. For a fair comparison, the Adam algorithm is used to train the CNN which presents better classification accuracy and convergence speed than the SGD algorithm in the experiments. As TCNN can nearly achieve the best result in 'S5', the parameters of the last five blocks are fine-tuned in the target cases for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Loss and accuracy comparison</head><p>The learning procedures of TCNN and baseline CNN without transfer are compared. Both models are trained on four cases with a batch size of 50 and 100 epochs. The training and testing losses obtained during the training stage of TCNN and CNN on C1, C2, C3 and C4 are respectively presented in Fig. <ref type="figure" target="#fig_6">5</ref>. In the case of CNN, the training loss is smooth in all cases but the testing loss presents a large fluctuation even after 80 iterations. On the contrary, TCNN achieves better performance on all four cases. The loss curve converges more quickly. After 20 epochs of training, the training loss and the testing loss of TCNN are gradually close to a fixed value and remain stable. By contrast, the testing loss of CNN diverges until it reaches a certain number of iterations. This is mostly because the pre-training procedure of TCNN establishes an optimization starting point of the fine-tuning procedure inside a region of parameter space, which helps to achieve fast and accurate convergence to a good generalizing minimum. On the other hand, CNN is easy to get stuck in poor local solutions in the training procedure with random initialized weights.  Moreover, the classification accuracy and the computational loads are evaluated and compared between TCNN and baseline CNN. TCNN with just 20 epochs (TCNN-20) is also used for comparison. Table <ref type="table" target="#tab_7">VII</ref>  It can also be observed that as the size of training samples increased, the accuracy of TCNN and CNN increased. More especially CNN gives a major improvement. It is possible that CNN with a large number of parameters trained on the reduced training data suffers from the overfitting problem. By adding more training samples, CNN is prone to be resistant to the overfitting and achieves further improvements but still the performance is lower than TCNN's.</p><p>Through all results, TCNN even with only 20 epochs still significantly outperforms CNN in terms of classification accuracy (mean and standard deviation) and computational cost.</p><p>The training time of TCNN-20 is at least 3 times reduced in all cases compared to that of CNN. It should be highlighted that the training time of CNN at C1 and C2 cases (Table <ref type="table" target="#tab_7">VII</ref>) is much longer compared to other cases. This is due to the fact that the testing loss is still divergent after 100 epochs, so a tiny batch size of 10 has been used that significantly accelerates the convergence but incurs the cost of extended training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) High-level feature visualization</head><p>In order to gain some insight into what the network has learned in high-level layers, the activation outputs of layer 20 are used. It would be expected that the features closer to the output layer are more linearly separable. Those features are obtained from TCNN and CNN with testing samples of C1 and C3, respectively. In addition, the Principal Component Analysis (PCA) technique is used for visualization by reducing the data dimension from 192 to 2. The clustering results are presented in Fig. <ref type="figure">6</ref>. It can be seen that the different categories are heavily overlapped in the CNN case. Especially for C1, most of the points are mixed with each other and only points in class ten <ref type="bibr" target="#b9">(10)</ref> are well distinguished. Therefore, it can be expected that the classification performance will be not good enough. This conclusion is consistent with the results obtained in Table VIII. In the cases of C1 and C3, the high-level features of different classes obtained in TCNN are more discriminative in a lower-dimensional space than in CNN. The features learned from TCNN are relatively well clustered and most of the categories are separable presenting less overlap. The results show that TCNN is better to lean meaningful discriminative features from lower layers to high-level layers than CNN, which may help to yield high classification performance in testing stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with other methods</head><p>Furthermore, a comparison of the proposed method with four different methods is carried out for the reduced and the full training dataset: (1) 2DCNN. All the samples are resized to 40×50 pixels from the original 2000 data points. The architecture used in <ref type="bibr" target="#b19">[20]</ref> is considered here. Additionally, a further improvement is made for a fair comparison by adopting   learning strategy is used. WDCNN is pre-trained and transferred (Named TWDCNN) for classification in four target cases as it was presented for TCNN before. The identification accuracy of the proposed method and the other algorithms are shown in Fig. <ref type="figure">7</ref> and Fig. <ref type="figure">8</ref>. The test accuracy of each method is listed in Table <ref type="table" target="#tab_11">IX</ref> and<ref type="table">Table X</ref>. In Fig. <ref type="figure">7</ref> and Table <ref type="table" target="#tab_11">IX</ref>, compared to 2DCNN and DNN, it can be observed that WDCNN achieves similar performance in C3 and C4, obtaining accuracy of 95.5% and 88.7%, respectively. However, in C1 and C2, which contain more fault types, they get worse performance with an accuracy of 40% and 42.5%. It is possible that WDCNN has a larger number of trainable parameters which could lead to more overfitting in the case of the reduced dataset. By adding more training samples as shown in Fig. <ref type="figure">8</ref> and Table <ref type="table">X</ref>, WDCNN and the other traditional deep learning networks all improve the classification performances.</p><p>In C3 and C4, though five classes exist, the highest accuracy yielded in TCNN is 95.5% with 98.7% obtained in C1 and C2 with ten categories. This can be contributed to its similarity to the source datasets. It is in accordance with <ref type="bibr" target="#b26">[27]</ref> that the effectiveness of feature transfer will gradually decline as the source dataset and target dataset become less similar.</p><p>TWDCNN, sharing the same network architecture with WDCNN, makes a large improvement using the proposed transfer learning methods. On the other hand, TCNN achieves higher accuracy on all testing cases compared to other methods The improved performance of TWDCNN and TCNN is more obvious in the case of small number of training samples. This is due to the fact that DNN, WDCNN and 2D CNN are based on deep architectures, which need massive amount of samples to enhance the classification, while TWDCNN and TCNN could utilize the prior knowledge from source domain datasets that reduce the dependence on the number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a transfer learning framework based on TCNN has been proposed for fault diagnosis of mechanical systems. The key idea of this method is to exploit the knowledge gained from fault diagnosis issues and different machines (historical data) to improve the performance of target task problems. TCNN is a modified version of WDCNN, where dropout techniques, kernel numbers and fully-connected layers are added in order to improve the learning of efficient discriminative features from raw data. Different diagnosis cases as well as different datasets have been used in order to test and validate the performance of the proposed method, presenting good stability and robustness and achieving better results compared to state-of-the-art architectures. The proposed method can be used not only for complex diagnosis cases but also for other data-driven tasks, including condition monitoring, anomaly detection, bearing life prediction, prognostics etc. As a next step, the authors will extend the proposed methodology towards unsupervised or semi-supervised settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of TCNN In our experiment, Taking the diagnosis accuracy and computational cost into consideration, TCNN with different</figDesc><graphic coords="3,304.92,579.96,240.84,121.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) CWRU bearing test (b) Gearbox test (c) Bearing test Fig. 3. The experiment test rig</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The relationship between accuracy and fine-tuned layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Loss curve of CNN and TCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) CNN for C1 (b) TCNN for C1 (c) CNN for C3 (d) TCNN for C3 Fig. 6. The testing samples visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Accuracy comparison of five methods on the reduced training samples (Accuracy comparison of five methods on the full training samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the fully-connected layer architecture of TCNN, which helps to learn better discriminate features. (2) WDCNN is employed which achieves state-of-art results in the public CWRU bearing data set in<ref type="bibr" target="#b24">[25]</ref>. (3) DNN [10]. The raw vibration signals are firstly transformed into Fourier spectra and then are fed into the DNN for faulty decision. (4) TWDCNN. The proposed transfer 1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2917233, IEEE Transactions on Industrial Informatics TII-19-1286 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>for different diagnosis tasks, and This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2917233, IEEE Transactions on Industrial Informatics</figDesc><table><row><cell>TII-19-1286</cell><cell>2</cell></row><row><cell>Intelligent Fault Diagnosis for Rotary Machinery</cell><cell></cell></row><row><cell>Using Transferable Convolutional Neural</cell><cell></cell></row><row><cell>Network</cell><cell></cell></row><row><cell>Zhuyun Chen, Student Member, IEEE, Konstantinos Gryllias, Member, IEEE, and Weihua Li, Senior</cell><cell></cell></row><row><cell>Member, IEEE</cell><cell></cell></row></table><note><p>T 1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2917233, IEEE Transactions on Industrial Informatics</figDesc><table><row><cell>TII-19-1286</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II DESCRIPTIONS</head><label>II</label><figDesc>OF THE SOURCE DOMAIN BEARING DATASET</figDesc><table><row><cell>Dataset</cell><cell>Speed</cell><cell>Fault Types</cell><cell>Fault diameters</cell><cell>Number</cell><cell>Class</cell></row><row><cell></cell><cell>(rpm)</cell><cell></cell><cell>(inch)</cell><cell>of Samples</cell><cell>Label</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>Health</cell><cell>0</cell><cell>800 &amp; 800 &amp; 800</cell><cell>1</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>BF</cell><cell>0.007</cell><cell>800 &amp; 800 &amp; 800</cell><cell>2</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>IF</cell><cell>0.007</cell><cell>800 &amp; 800 &amp; 800</cell><cell>3</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>OF</cell><cell>0.007</cell><cell>800 &amp; 800 &amp; 800</cell><cell>4</cell></row><row><cell>D s 1</cell><cell>1772 &amp; 1750 &amp; 1730 1772 &amp; 1750 &amp; 1730</cell><cell>BF IF</cell><cell>0.014 0.014</cell><cell>800 &amp; 800 &amp; 800 800 &amp; 800 &amp; 800</cell><cell>5 6</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>OF</cell><cell>0.014</cell><cell>800 &amp; 800 &amp; 800</cell><cell>7</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>BF</cell><cell>0.021</cell><cell>800 &amp; 800 &amp; 800</cell><cell>8</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>IF</cell><cell>0.021</cell><cell>800 &amp; 800 &amp; 800</cell><cell>9</cell></row><row><cell></cell><cell>1772 &amp; 1750 &amp; 1730</cell><cell>OF</cell><cell>0.021</cell><cell>800 &amp; 800 &amp; 800</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV DESCRIPTION OF TARGET DOMAIN GEARBOX DATASET Dataset D t 1 Speed (rpm) Fault Types Number of Samples Class Label The Fifth shift gear Output bearing C1</head><label>IV</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2917233, IEEE Transactions on Industrial Informatics</figDesc><table><row><cell></cell><cell></cell><cell>TII-19-1286</cell><cell></cell><cell></cell><cell>6</cell></row><row><cell></cell><cell>1250</cell><cell>Normal</cell><cell>Normal</cell><cell>300</cell><cell>1</cell></row><row><cell></cell><cell>1250</cell><cell>Minor chipped tooth</cell><cell>Normal</cell><cell>300</cell><cell>2</cell></row><row><cell></cell><cell>1250</cell><cell>Half chipped tooth</cell><cell>Normal</cell><cell>300</cell><cell>3</cell></row><row><cell></cell><cell>1250</cell><cell>Missing tooth</cell><cell>Normal</cell><cell>300</cell><cell>4</cell></row><row><cell></cell><cell>1250</cell><cell>Normal</cell><cell>0.2 mm inner race fault</cell><cell>300</cell><cell>5</cell></row><row><cell></cell><cell>1250</cell><cell>Minor chipped tooth</cell><cell>0.2 mm inner race fault</cell><cell>300</cell><cell>6</cell></row><row><cell></cell><cell>1250</cell><cell>Half chipped tooth</cell><cell>0.2 mm inner race fault</cell><cell>300</cell><cell>7</cell></row><row><cell></cell><cell>1250</cell><cell>Missing tooth</cell><cell>0.2 mm inner race fault</cell><cell>300</cell><cell>8</cell></row><row><cell></cell><cell>1250</cell><cell>Half chipped tooth</cell><cell>2 mm inner race fault</cell><cell>300</cell><cell>9</cell></row><row><cell></cell><cell>1250</cell><cell>Missing tooth</cell><cell>2 mm inner race fault</cell><cell>300</cell><cell>10</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Normal</cell><cell>Normal</cell><cell>300 &amp; 300</cell><cell>1</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Minor chipped tooth</cell><cell>Normal</cell><cell>300 &amp; 300</cell><cell>2</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Half chipped tooth</cell><cell>Normal</cell><cell>300 &amp; 300</cell><cell>3</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Missing tooth</cell><cell>Normal</cell><cell>300 &amp; 300</cell><cell>4</cell></row><row><cell>C2</cell><cell>1000 &amp; 1250 1000 &amp; 1250</cell><cell>Normal Minor chipped tooth</cell><cell>0.2 mm inner race fault 0.2 mm inner race fault</cell><cell>300 &amp; 300 300 &amp; 300</cell><cell>5 6</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Half chipped tooth</cell><cell>0.2 mm inner race fault</cell><cell>300 &amp; 300</cell><cell>7</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Missing tooth</cell><cell>0.2 mm inner race fault</cell><cell>300 &amp; 300</cell><cell>8</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Half chipped tooth</cell><cell>2 mm inner race fault</cell><cell>300 &amp; 300</cell><cell>9</cell></row><row><cell></cell><cell>1000 &amp; 1250</cell><cell>Missing tooth</cell><cell>2 mm inner race fault</cell><cell>300 &amp; 300</cell><cell>10</cell></row></table><note><p>1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI RESULT</head><label>VI</label><figDesc>COMPARISON WITH DIFFERENT METHODS</figDesc><table><row><cell>Methods</cell><cell>Training accuracy (%)</cell><cell>Testing accuracy (%)</cell></row><row><cell>CNN-Wen</cell><cell>99.99±0.04</cell><cell>91.26±2.63</cell></row><row><cell>2DCNN</cell><cell>99.99±0.02</cell><cell>87.15±1.25</cell></row><row><cell>1DCNN</cell><cell>98.58±1.85</cell><cell>94.32±2.92</cell></row><row><cell>WDCNN</cell><cell>99.85±0.30</cell><cell>96.54±1.61</cell></row><row><cell>TCNN</cell><cell>99.99±0.01</cell><cell>99.03±0.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>RESULTS WITH FULL TRAINING SAMPLES (%)</figDesc><table><row><cell>Cases</cell><cell></cell><cell>Testing</cell><cell></cell><cell></cell><cell>Training</cell></row><row><cell></cell><cell cols="3">accuracy±STD (%)</cell><cell></cell><cell>time (s)</cell></row><row><cell></cell><cell>CNN</cell><cell>TCNN</cell><cell>TCNN</cell><cell cols="2">CNN TCNN</cell><cell>TCNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-20</cell><cell></cell><cell></cell><cell>-20</cell></row><row><cell>C1</cell><cell>94.9±3.2</cell><cell cols="2">99.9±0.1 99.7±0.3</cell><cell>45</cell><cell>38</cell><cell>15</cell></row><row><cell>C2</cell><cell>93.2±4.6</cell><cell cols="2">99.3±0.1 98.6±0.6</cell><cell>98</cell><cell>73</cell><cell>22</cell></row><row><cell>C3</cell><cell>93.6±4.8</cell><cell cols="2">97.9±0.5 96.2±1.2</cell><cell>53</cell><cell>41</cell><cell>14</cell></row><row><cell>C4</cell><cell>93.8±6.3</cell><cell cols="2">96.5±0.5 96.2±3.5</cell><cell>81</cell><cell>68</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and Table VIII list the results obtained using the reduced and the full training samples in the four cases. Compared to TCNN, the classification accuracies of CNN presented in Table VI are respectively only 63.60% and 67.49% using C1 and C2 with reduced training samples and they reach 94.94% and 93.22% when full training samples are used, as presented in Table VIII. In C3 and C4, CNN achieved relatively high accuracies but presented high standard deviations. On the other hand, TCNN presents some advantages over CNN in accuracy and computational cost in all cases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">RESULTS WITH REDUCED TRAINING SAMPLES (%)</cell></row><row><cell>Cases</cell><cell></cell><cell cols="3">Testing accuracy±STD (%)</cell><cell></cell></row><row><cell></cell><cell>2DCNN</cell><cell>WDCNN</cell><cell>DNN</cell><cell>TWDCNN</cell><cell>TCNN</cell></row><row><cell>C1</cell><cell>66.2±6.0</cell><cell>40.0±8.0</cell><cell>43.4±3.8</cell><cell>92.7±2.1</cell><cell>98.7±0.6</cell></row><row><cell>C2</cell><cell>59.7±1.0</cell><cell>42.4±5.9</cell><cell>55.2±4.8</cell><cell>93.8±4.8</cell><cell>96.4±4.8</cell></row><row><cell>C3</cell><cell>90.7±7.2</cell><cell>95.5±0.8</cell><cell>93.2±4.6</cell><cell>93.2±4.6</cell><cell>93.2±4.6</cell></row><row><cell>C4</cell><cell>87.7±2.5</cell><cell>88.7±6.1</cell><cell>59.7±5.8</cell><cell>94.2±1.8</cell><cell>95.5±0.8</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE X</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">COMPARISON RESULTS WITH FULL TRAINING SAMPLES (%)</cell></row><row><cell>Cases</cell><cell></cell><cell cols="3">Testing accuracy±STD (%)</cell><cell></cell></row><row><cell></cell><cell>2DCNN</cell><cell>WDCNN</cell><cell>DNN</cell><cell>TWDCNN</cell><cell>TCNN</cell></row><row><cell>C1</cell><cell>82.8±5.6</cell><cell>88.6±3.7</cell><cell>84.4±5.4</cell><cell>96.4±1.1</cell><cell>99.9±0.1</cell></row><row><cell>C2</cell><cell>87.2±3.8</cell><cell>84.1±4.9</cell><cell>90.6±1.5</cell><cell>96.4±2.1</cell><cell>99.3±0.1</cell></row><row><cell>C3</cell><cell>91.5±2.7</cell><cell>92.0±5.2</cell><cell>88.4±5.8</cell><cell>95.7±1.4</cell><cell>97.9±0.5</cell></row><row><cell>C4</cell><cell>93.0±5.1</cell><cell>94.2±2.8</cell><cell>64.6±4.8</cell><cell>95.6±1.6</cell><cell>96.5±0.5</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 51875208 and National Key R&amp;D Program of China under Grant 2018YFB1702402.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underdetermined Source Separation of Bearing Faults Based on Optimized Intrinsic Characteristic-Scale Decomposition and Local Non-Negative Matrix Factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11427" to="11435" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Novel Feature Enhancement Method based on Improved Constraint Model of Online Dictionary Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2895776</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regrouping particle swarm optimization based variable neural network for gearbox fault diagnosis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3671" to="3680" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diagnosis of stator faults severity in induction motors using two intelligent approaches</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H C</forename><surname>Palácios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goedtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vibration-Based Intelligent Fault Diagnosis for Roller Bearings in Low-Speed Rotating Machinery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1887" to="1899" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Application of pattern recognition in gear faults based on the matching pursuit of a characteristic waveform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="212" to="222" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature denoising and nearest-farthest distance preserving projection for machine fault diagnosis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rakheja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="393" to="404" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compressive sensing and sparse decomposition in precision machining process monitoring: From theory to applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechatronics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning and Its Applications to Machine Health Monitoring: A Survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07640</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks: A promising tool for fault characteristic mining and intelligent diagnosis of rotating machinery with massive data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">73</biblScope>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2016-05">May. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bearing performance degradation assessment using long short-term memory recurrent network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in industry</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2019-04">April. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A New Convolutional Neural Network-Based Data-Driven Fault Diagnosis Method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5990" to="5998" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel deep autoencoder feature learning method for rotating machinery fault diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="187" to="204" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse Deep Stacking Network for Fault Diagnosis of Motor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018-07">July. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural network constructed by deep learning technique and its application to intelligent fault diagnosis of machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page" from="619" to="628" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Coupling Autoencoder for Fault Diagnosis With Multimodal Sensory Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1137" to="1145" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multisensor feature fusion for bearing fault diagnosis using sparse autoencoder and deep belief network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1693" to="1702" />
			<date type="published" when="2017-07">Jul.2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel convolutional neural network based fault recognition method via image fusion of multi-vibration-signals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Industry</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="182" to="190" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional Discriminative Feature Learning for Induction Motor Fault Diagnosis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1359" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical adaptive deep convolution neural network and its application to bearing fault diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="490" to="502" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dislocated time series convolutional neural architecture: An intelligent fault diagnosis approach for electric machine</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1320" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Residual Networks with Dynamically Weighted Wavelet Coefficients for Fault Diagnosis of Planetary Gearboxes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4290" to="4300" />
			<date type="published" when="2018-05">May. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time motor fault detection by 1-d convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiranyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Askar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7067" to="7075" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Decoupling Convolutional Neural Network for Intelligent Compound Fault Diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1848" to="1858" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A New Deep Learning Model for Fault Diagnosis with Good Anti-Noise and Domain Adaptation Ability on Raw Vibration Signals</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the impact of data set size in transfer learning using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soekhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Putten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks and transfer learning applied to multimedia web mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>López-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Corchado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Distributed Computing and Artificial Intelligence</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Valentim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep model based domain adaptation for fault diagnosis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2296" to="2305" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer Learning With Neural Networks for Bearing Fault Diagnosis in Changing Working Conditions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="14347" to="14357" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning Based on Sparse Auto-encoder for Remaining Useful Life Prediction of Tool in Manufacturing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2018.2881543</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Highly-Accurate Machine Fault Diagnosis Using Deep Transfer Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2018.2864759</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">…</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
