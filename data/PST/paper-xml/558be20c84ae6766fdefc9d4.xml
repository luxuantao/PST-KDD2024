<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REAL-LIFE VOICE ACTIVITY DETECTION WITH LSTM RECURRENT NEURAL NETWORKS AND AN APPLICATION TO HOLLYWOOD MOVIES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Florian</forename><surname>Eyben</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence &amp; Signal Processing Group</orgName>
								<orgName type="institution" key="instit1">MMK</orgName>
								<orgName type="institution" key="instit2">Techische Universität München</orgName>
								<address>
									<country key="DE">GERMANY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Weninger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence &amp; Signal Processing Group</orgName>
								<orgName type="institution" key="instit1">MMK</orgName>
								<orgName type="institution" key="instit2">Techische Universität München</orgName>
								<address>
									<country key="DE">GERMANY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Squartini</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<settlement>Ancona</settlement>
									<country key="IT">ITALY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Björn</forename><surname>Schuller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Intelligence &amp; Signal Processing Group</orgName>
								<orgName type="institution" key="instit1">MMK</orgName>
								<orgName type="institution" key="instit2">Techische Universität München</orgName>
								<address>
									<country key="DE">GERMANY</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REAL-LIFE VOICE ACTIVITY DETECTION WITH LSTM RECURRENT NEURAL NETWORKS AND AN APPLICATION TO HOLLYWOOD MOVIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A86E9EEE6D6BB43EBE5FD98DA89B09F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Voice Activity Detection</term>
					<term>Speech Detection</term>
					<term>Neural Networks</term>
					<term>Long Short-Term Memory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel, data-driven approach to voice activity detection is presented. The approach is based on Long Short-Term Memory Recurrent Neural Networks trained on standard RASTA-PLP frontend features. To approximate real-life scenarios, large amounts of noisy speech instances are mixed by using both read and spontaneous speech from the TIMIT and Buckeye corpora, and adding real long term recordings of diverse noise types. The approach is evaluated on unseen synthetically mixed test data as well as a real-life test set consisting of four full-length Hollywood movies. A frame-wise Equal Error Rate (EER) of 33.2% is obtained for the four movies and an EER of 9.6% is obtained for the synthetic test data at a peak SNR of 0 dB, clearly outperforming three state-of-the-art reference algorithms under the same conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Voice activity detection (VAD), also referred to as speech activity detection, is an important first step in many speech-based systems. It is important for Automatic Speech Recognition (ASR), to avoid word insertions due to noise and background speech; it is also used in audio coding to save bandwidth, and in multi-party conference systems, for example, to reduce the amount of background noise.</p><p>Early approaches to VAD were based on simple energy thresholds or pitch and zero-crossing rate rules (cf. <ref type="bibr" target="#b1">[1]</ref>). These approaches perform well in settings where there is little or no background noise. More recent approaches consider more advanced parameters like autoregressive (AR) model parameters <ref type="bibr" target="#b2">[2]</ref> and line spectral frequencies (LSPs). The most promising approaches in highly corrupted conditions seem to be data-driven methods, where a classifier is trained to predict speech vs. non-speech from acoustic features (cf., e. g., <ref type="bibr" target="#b3">[3]</ref>). Still, the performance of such approaches degrades when background noise with spectral characteristics similar to speech is present. Very recent studies suggest that the use of long-span features clearly improves the robustness in real-life and noisy settings because the decision for each frame can be performed in the context of the previous frames <ref type="bibr" target="#b4">[4]</ref>.</p><p>In this light, we propose a novel approach, which uses traditional frame-wise features, but where the classifier is capable of learning the dynamics of the inputs and adaptively using previous inputs for the decision of the current frame. In Section 2 we present three state-of-the-art statistical VAD algorithms, some of which use context information in a rule-based fashion, which we will use as baselines in our evaluation. Next, in Section 3 we introduce our proposed approach. The data-sets used for evaluations are described in Section 4. We use both synthetic data of spontaneous and read speech in controlled noise conditions, and audio tracks of Hollywood movies containing highly non-stationary noise. Results are presented in Section 5; we conclude our findings in Section 6 and discuss our work in the context of prior work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STATE OF THE ART STATISTICAL VAD ALGORITHMS</head><p>In this Section the three baseline, state-of-the-art VAD algorithms <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6]</ref> designed for the use in noisy conditions are briefly presented. They all belong to the category of statistical methods, where a Likelihood Ratio (LR) test is applied to the hypotheses of speech presence (denoted as H1) and speech absence (H0), on a certain frame of the observed noisy signal xt = st + nt, where st and nt denote the clean speech and the noise signal, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sohn's approach (SOHN )</head><p>The VAD proposed in <ref type="bibr" target="#b5">[5]</ref> is based on a statistical model in the timefrequency domain for the derivation of the LR test. Given S, N, and X the DFT coefficient vectors of dimension L at the current frame m, for the speech, noise and noisy speech signals, respectively, the probability density functions conditioned on H0 and H1 are:</p><formula xml:id="formula_0">p(X|H0) = L-1 k=0 1 πλN (k) exp - |X k | 2 λN (k)<label>(1)</label></formula><formula xml:id="formula_1">p(X|H1) = L-1 k=0 1 π [λN (k) + λS(k)] exp - |X k | 2 λN (k) + λS(k)</formula><p>where λN (k) and λS(k) are the variances of N k and S k , respectively, i. e., the k-th terms of vectors N and S. The LR for the k-th frequency bin is given by:</p><formula xml:id="formula_2">Λ k p(X k |H1) p(X k |H0) = 1 1 + ξ k exp γ k ξ k 1 + ξ k<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">ξ k λS(k) λN (k) and γ k |X k | 2 λN (k)</formula><p>are the a-priori and aposteriori signal-to-noise ratio (SNR) <ref type="bibr" target="#b7">[7]</ref>. The decision rule is based on the log-LR function at the current frame m, which is obtained by averaging the log-likelihood ratios for each frequency bin, as follows:</p><formula xml:id="formula_4">L(m) = log Λ(m) = 1 L L-1 k=0 log Λ k H 1 ≷ H 0 η (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where η is the decision threshold. A hangover mechanism, based on relationship between consecutive speech frames, is also implemented to reduce the false negative occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ramirez' approach (RAM 05)</head><p>The algorithm proposed in <ref type="bibr" target="#b6">[6]</ref>, as the one in <ref type="bibr">[8]</ref>, is based on the concept that more consecutive speech frames concur into the definition of the LR function. Given the M noisy observation DFT coefficient vectors X1, X2, ..., XM involved in the two-class classification problem, the Multiple Observation -LR (MO-LR) Test over a window of 2M + 1 frames centered on frame m is the following (assuming that the vectors Xj are independent and taking the logarithm):</p><formula xml:id="formula_6">L(m) = log Λ M (m) = m+M j=m-M log p(Xj|H1) p(Xj|H0) H 1 ≷ H 0 η<label>(4)</label></formula><p>where m denotes the frame on which the decision is performed. The MO-LR function can be recursively calculated. The same statistical model explicited in (1) has been considered here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">AR-GARCH based approach (ARG)</head><p>The algorithm proposed in <ref type="bibr" target="#b2">[2]</ref> is based on the idea of modelling the speech signal by means of an AR-GARCH (autoregressivegeneralized autoregressive conditional heteroskedasticity) model in the time domain. These are the main steps of the algorithm:</p><p>• Estimation in the time domain of the noise variance σt by means of the IMCRA algorithm <ref type="bibr" target="#b9">[9]</ref> and noisy signal normalization by this value; • For each time instant t, estimation of, first, the AR-GARCH parameter vector θ, by means of a Recursive Maximum Likelihood (RML) updating rule, and then of the clean signal in the Minimum Mean Square sense, by exploiting the knowledge of the θ vector; • VAD decision on a frame-by-frame basis, by using the estimated clean speech over all observation windows.</p><p>Focusing on this last step, the likelihood ratio Λ(m) is obtained by averaging the LRs calculated for each time step. In order to take the correlation between adjacent signal samples into account, and to derive the final decision rule, the vocal activity is then modelled as a first-order Markov model, so that:</p><formula xml:id="formula_7">L(m) = P m|m = ΛmP m|m-1 ΛmP m|m-1 + (1 -P m|m-1 ) H 1 ≷ H 0 η (5)</formula><p>where P m|m and P m|m-1 are the rules obtained by using and notusing the vocal activity information of the m-th frame, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED LSTM-RNN VAD</head><p>In this paper we present a novel data-driven method for voice activity detection based on (unidirectional) Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) <ref type="bibr" target="#b10">[10]</ref>. The motivation behind the use of LSTM-RNN is their ability to model long range dependencies between the inputs. Other common data-driven VAD approaches, such as those based on GMM (cf. section 7) or Feed-Forward Neural Networks do not consider any temporal dependencies in the model. Delta features, modulation or long-span features <ref type="bibr" target="#b4">[4]</ref> are used to overcome these issues. Standard Recurrent Neural Networks (as used in <ref type="bibr" target="#b11">[11]</ref> for VAD), are able to model a limited amount of temporal dependency. However, LSTM-RNN go beyond simply using context information by introducing the concept of a memory cell that can be read, written and reset depending on feature context and previous outputs, by means of multiplicative input, output and forget units whose multiplicative weights are learned automatically during training. Thereby, they learn when to access which parts of past context, solving the vanishing gradient problem of traditional RNNs <ref type="bibr" target="#b12">[12]</ref>.</p><p>The networks we use for VAD have an input layer which matches the size of the low-level acoustic feature vectors, one or more hidden layers, and an output layer with a single linear unit. The networks are trained as regressors to output a voicing score for every frame in the range [-1; +1]; +1 indicating voicing, -1 indicating silence or noise. Two neural network topologies are investigated:</p><p>• N 1: 1 recurrent hidden layer (4 blocks with 50 LSTM cells each)</p><p>• N 2: 3 recurrent hidden layers (50 LSTM cells in one block; 10 sigmoid neurons; 20 LSTM cells)</p><p>On the input side of the networks we use a standard RASTA-PLP <ref type="bibr" target="#b13">[13]</ref> frontend with cepstral coefficients 1-18 and their first order delta coefficients. The frame size is 25 ms and the frame step is 10 ms. It is important to highlight that the 36 dimensional feature vector does not contain an energy coefficient (e. g., 0-th cepstral coefficient). We decided for this to make the networks input level invariant. Features were extracted with our openSMILE toolkit <ref type="bibr" target="#b14">[14]</ref> and z-normalization was applied to all features (mean zero, variance 1). The means and variances for the z-normalization are computed from the training set only. The LSTM-RNNs were trained and evaluated with the rnnlib by Alex Graves <ref type="bibr" target="#b15">[15]</ref>.</p><p>Training is performed with the backpropagation through time (BPTT) algorithm; the weights are updated using the gradient descent algorithm with a learning rate of 10 -5 and momentum 0.9. This requires weights to be initialized with non-zero values, thus we initialize the weights with uniform random values sampled from ]0; 0.1].</p><p>To increase robustness against convergence into a suboptimal local minimum of the weight space, we train three networks with different random weight initialisations. Network predictions for the test and validation set are then computed by averaging the predictions for all three networks. To further enhance generalisation, we added Gaussian noise with zero mean and standard deviation of 0.3 to all inputs. To avoid over-adaptation, a maximum of 40 training epochs was run. Further, training was stopped early if there was no error improvement over 10 epochs. The frame-wise root mean quadratic error between the targets and the network predictions is used as evaluation criterion during network training.</p><p>The computational complexity for evaluating the networks is linear with respect to the number of input frames. For every frame a constant number of operations needs to be performed. Many computations can be run in parallel, which is ideal for implementation on embedded hardware such as Digital Signal Processors (DSPs) or Field Programmable Gate Arrays (FPGAs). The asymptotically quadratic complexity wrt. the network size can be drastically reduced in practice by the chosen block structure of the hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DATA SETS</head><p>To obtain a large amount of labelled and diverse speech data for training and validating the networks, we synthesise data by building random utterance sequences overlaid with additive noise. The speech data is taken from the Buckeye <ref type="bibr" target="#b16">[16]</ref> and the TIMIT corpus <ref type="bibr" target="#b17">[17]</ref>. The Buckeye corpus consists of 26 h of spontaneous speech from 40 speakers (20 male, 20 female) recorded in informal interview situations. Only the subjects' speech is used, and speaker turns corresponding to utterances between silence segments of at least 0.5 s are extracted according to the automatic alignment delivered with the Buckeye corpus. The corpus is split subject-independently into a training, validation and test set, respecting stratification by age and gender. The segmentation and subdivision is exactly equal to the one used in <ref type="bibr" target="#b18">[18]</ref>. The original TIMIT training set is split speakerindependently into a training and validation set. Speech for the VAD test set is taken from the original TIMIT and Buckeye test sets. Four types of noise are used: babble, city, white and pink noise, and music. The babble noise recordings are taken from the freesound.org website. Samples from the categories pub-noise, restaurant chatter, and crowd noise are joint. The music recordings are instrumental and classical music pieces from the last.fm website. The city recordings were recorded at TUM in Munich, Germany with smartphones while people where cycling and walking through the city. White and pink noise samples were generated with pseudo random number generators and a bandpass filter.</p><p>The noise samples used for synthesising the VAD training, validation, and test samples are fully disjunctive (i.e., different pieces of music, different babble samples, etc.). Noise samples for the test and validation sets are 30 minutes each, the remaining noise audio is used for the training set. The lengths of these samples varies from 94 minutes (babble) to 176 minutes (music).</p><p>Each synthetic utterances in the VAD training set is composed of N ∈ {1, . . . , 5} original speech utterances, which are randomly selected either from TIMIT or Buckeye. A pause before the first utterance, pauses between all utterances, and a pause after the last utterance are inserted with a uniformly random length of 0.5 to 5 seconds. Each of the original utterances is normalised to have a peak amplitude of 0 dB and then all N normalised utterances are multiplied with a uniformly random gain factor g s,lin = 10 where gs ∈ [+3 dB; -20 dB]. For 80 % of the synthetic utterances, a random noise sample, which matches the total length of the N speech utterances and the N + 2 pauses, is selected from the training noise pool and normalised to a peak amplitude of 0 dB. A multiplicative gain g n,lin according to equation ( <ref type="formula">6</ref>) is applied to the noise segment:</p><formula xml:id="formula_8">g n,lin = 10 (log(g s,lin )-SN R 20.0 ) (6)</formula><p>The SNR is randomly chosen for each mixed instance as SN R ∈ [-6dB; +25dB]. The remaining 20% of all synthetic utterances are not overlaid with noise. 1 948 instances are created with speech from Buckeye. This corresponds to 15 h of total audio, where 6:43 h are non-speech and 8:17 h are speech. From TIMIT speech, 3 493 instances are generated. This corresponds to 19:45 h of total audio, where 12:54 h are non-speech and 6:51 h are speech. In total there is 34:54 h of audio in the VAD training set.</p><p>The validation set is built in a similar way, however one single mixed instance each with a total length of 22.5 minutes is generated from Buckeye speech and TIMIT speech. The gain of each of the original utterances is varied randomly over the same range as for the training set, and pauses are added using the same parameters. This same sequence of speech utterances and pauses is overlaid with four continuous 30 minute segment of babble, music, city, and white+pink noise (all normalised to 0 dB peak amplitude). A fixed gain g n,lin is chosen for this noise segment as g n,lin = 0.5(g µ s,lin + g min s,lin ), where g µ s,lin and g min s,lin are the average and minimum multiplicative gain factors of the speech utterances. In total, the VAD validation set has 3 h of audio, where 1:22 h are speech and 1:38 h are non-speech.</p><p>For the VAD test set, 15 minute long mixed instances are created each from TIMIT and Buckeye speech. Thus, the total length of each test instance is 30 minutes. The clean version of the 30 minute test audio contains 12 minutes of speech and 18 minutes of silence. A single fixed gain of -6 dB for the clean speech is applied and noise is added with a peak SNR (noise gain relative to speech gain) of 0 dB. To test the VAD in challenging real-life conditions, we use a second test set consisting of the full English audio tracks of four Hollywood movie DVDs. The choice of these movies was inspired by the official development set of the 2012 MediaEval campaign's violence detection task <ref type="bibr" target="#b19">[19]</ref>. Speech and non-speech segments in those four movies were manually annotated. The list of movies and statistics on speech / non-speech segments are found in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>Results for the synthetic test and validation sets are given in Tab. 1. We use two evaluation metrics: the area under receiver operating characteristic (ROC) curves (AUC) and the combined error rate (false positive rate (FPR) + false negative rate (FNR)). Fixed thresholds which correspond to the thresholds at the Equal Error Rate (EER) on the validation set are used for all test set evaluations to ensure the test data is fully unknown to the system. For nets N 1 and N 2 the thresholds are -0.268 and -0.071, respectively. The same thresholds are used for the DVD movie test set. For FPR and FNR computation, the thresholded predictions (both for reference and LSTM) are smoothed with a silence hysteresis of 5 frames (i. e., non-speech segments shorter than 5 frames are joined with the adjacent speech segments). We observe that both the N 1 and N 2 network topologies outperform all baseline algorithms in terms of both AUC and FNR + FPR, and notably also for clean speech. The largest margin of improvement is found for music, babble, white and pink noise. On city noise, the baselines are relatively robust, which can be attributed to the fact that the average energy of these noise samples is much lower than the peak amplitude (e. g., loud cars passing by). The ROC curves for the proposed and the baseline algorithms are shown in Figure <ref type="figure">1</ref>. The 'smoothness' of the curves for the proposed approach compared Fig. <ref type="figure">1</ref>: Receiver operating characteristic (ROC) curves for VAD on synthetic test set: True-positive-ratio (TPR) vs. false-positive-ratio (FPR) and area under curve (AUC) for AR-GARCH <ref type="bibr" target="#b2">[2]</ref> (i), Ramirez' approach <ref type="bibr" target="#b6">[6]</ref> (ii), Sohn's approach <ref type="bibr" target="#b5">[5]</ref> (iii) and the proposed LSTM-RNN approach (iv) using network topologies N1 and N2. to the baselines is due to the modeling as a regression task in training, which delivers a 'continuum' of scores in testing. As to ROC, the behaviour of the two network topologies is nearly identical. The EER across validation and test sets is around 10 % for both network topologies as opposed to 25 % and above for the baseline algorithms.</p><p>The results for the movie test set are given in Tab. 3. Compared to the synthetic test set, the performance of our method and SOHN on the movie test set is much lower. However, the networks still clearly outperfom SOHN . Note, that the results for RAM 05 and ARG could not be obtained due to the high computational complexity of these algorithms, but given the test set results we estimate their performance to be similar to SOHN . One main reason for the reduced performance on the movie set might be that many noise types occur that have not been seen in training, such as gunshots, fighting, etc., and noises that are easy to confuse with speech, such as animal sounds. Another reason is the coarse annotation style of speech segments; for the sake of efficiency, longer conversations were labelled as continuous speech segments, even though they included small pauses. In the evaluations this results in a higher miss rate than is actually given. Compared to <ref type="bibr" target="#b3">[3]</ref> (25.3% EER on YouTube videos) our EERs are very competitive, considering that their system was trained on in-domain data. Next, a comparison of both approaches on YouTube videos and Hollywood movies would be highly interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND OUTLOOK</head><p>In this paper we have presented a novel VAD approach based on LSTM-RNN. We further presented a method for synthesising training data for the LSTM-RNN to approximate real-life settings without the need for in-domain data. We demonstrated the feasibility of this approach on real-life noisy speech data from Hollywood movies, and we showed that LSTM-RNN outperforms all three statistical VAD baselines. This is all the more notable since our method does not require future context, unlike the RAM05 and SOHN methods.</p><p>Future work will investigate the performance of LSTM-RNNs in more detail, analysing the context learning behaviour in comparison to GMMs, MLPs and RNNs using time-frequency or modulation spectrum features. Furthermore, using semi-supervised and active learning to efficiently adapt the generic models presented in this paper to specific use cases such as movies or web videos will be attempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATION TO PRIOR WORK</head><p>Many previous approaches to VAD rely on Gaussian mixture modeling and adaptation as typical for ASR, to adapt the VAD models to speakers <ref type="bibr" target="#b20">[20]</ref> and background noise <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>, in contrast to the proposed discriminative approach. <ref type="bibr" target="#b24">[24]</ref> adapts GMMs to channel and noise conditions. <ref type="bibr" target="#b25">[25]</ref> proposes to couple VAD with the acoustic models in the recogniser, whereas the proposed approach does not rely on phonetic modeling. Use of temporal context in data-based approaches has been proposed, e. g., by <ref type="bibr" target="#b26">[26]</ref> who use PLP based and similar, more advanced temporal features combined with GMMs. <ref type="bibr" target="#b4">[4]</ref> compares a standard GMM system using 14 PLP cepstral coefficients with a Multi-Layer Perceptron (MLP) based system using Long-Span acoustic features computed over .5 seconds windows. MLP based speech/non-speech posteriors are then decoded with two ergodic Hidden Markov Models (HMMs). However, these systems do not use adaptive context learning as by LSTM-RNN. <ref type="bibr" target="#b3">[3]</ref> compares GMM with a discriminative classifier and proposes novel features instead of standard MFCC/PLP frontends. Real noisy, manually labelled YouTube videos are used for evaluation, but only in-domain training is considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Frame-level results for validation and test set of nets N 1 and N 2 and the RAM 05, ARG, and SOHN algorithms. Area under ROC curve (AUC), Equal Error Rate (EER), and combined error rate (false negative rate (FNR) + false positive rate (FPR)) computed with a threshold estimated from the validation set. Test set: -6 dB gain applied to original speech signal, average SNR is 0 dB.</figDesc><table><row><cell>AUC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Movie test set. Movie length and percentage of parts with speech; min., mean, max. duration of continuous speech segments.</figDesc><table><row><cell>Title</cell><cell cols="3">[hh:mm] % sp. min/mean/max [s]</cell></row><row><cell>I Am Legend</cell><cell>1:36</cell><cell>39.2</cell><cell>0.5/21.4/174.9</cell></row><row><cell>Kill Bill Vol. 1</cell><cell>1:46</cell><cell>33.9</cell><cell>0.4/39.3/321.2</cell></row><row><cell>Saving Private Ryan</cell><cell>2:42</cell><cell>48.6</cell><cell>0.5/25.2/230.4</cell></row><row><cell>The Bourne Identity</cell><cell>1:53</cell><cell>40.7</cell><cell>0.6/32.6/185.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Frame-wise results for the movie test set of nets N 1 and N 2 and the SOHN algorithm. Area under ROC curve (AUC), Equal Error Rate (EER), and combined error rate (false negative rate (FNR) + false positive rate (FPR)) computed with a threshold estimated from the validation set. Results for RAM05 and ARG are not included due to their heavy computational load on the large DVD test set.</figDesc><table><row><cell></cell><cell></cell><cell>AUC</cell><cell></cell></row><row><cell>movie</cell><cell>N 1</cell><cell>N 2</cell><cell>SOHN</cell></row><row><cell>I Am Legend</cell><cell>.704</cell><cell>.676</cell><cell>.567</cell></row><row><cell>Kill Bill 1</cell><cell>.627</cell><cell>.601</cell><cell>.554</cell></row><row><cell>Saving P.</cell><cell>.743</cell><cell>.680</cell><cell>.577</cell></row><row><cell>Bourne Id.</cell><cell>.685</cell><cell>.647</cell><cell>.603</cell></row><row><cell>ALL</cell><cell>.722</cell><cell>.676</cell><cell>.556</cell></row><row><cell>[%]</cell><cell></cell><cell cols="2">FNR + FPR</cell></row><row><cell cols="3">I Am Legend 76.65 75.57</cell><cell>94.90</cell></row><row><cell>Kill Bill 1</cell><cell cols="2">94.14 94.41</cell><cell>102.88</cell></row><row><cell>Saving P.</cell><cell cols="2">67.03 81.70</cell><cell>92.46</cell></row><row><cell>Bourne Id.</cell><cell cols="2">70.83 80.10</cell><cell>90.95</cell></row><row><cell>ALL</cell><cell cols="2">69.87 78.03</cell><cell>95.52</cell></row><row><cell>[%]</cell><cell></cell><cell>EER</cell><cell></cell></row><row><cell>ALL</cell><cell cols="2">33.18 36.76</cell><cell>45.73</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust voice activity detection algorithm for estimating noise spectrum</title>
		<author>
			<persName><forename type="first">K</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Electronics Letters</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AR-GARCH in Presence of Noise: Parameter Estimation and Its Application to Voice Activity Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mousazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="916" to="926" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech/nonspeech segmentation in web videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH 2012</title>
		<meeting>of INTERSPEECH 2012<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Developing a speech activity detection system for the darpa rats program</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matjka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IN-TERSPEECH 2012</title>
		<meeting>of IN-TERSPEECH 2012<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A statistical model-based voice activity detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical voice activity detection using a multiple observation likelihood ratio test</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="689" to="692" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the decision-directed estimation approach of Ephraim and Malah</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="1" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient voice activity detection algorithms using long-term speech information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="287" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Noise spectrum estimation in adverse environment: Improved minima controlled recursive averaging</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="466" to="475" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-linear esimation of voice activity to improve automatic recognition of noisy speech</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gemello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH 2005</title>
		<meeting>of INTERSPEECH 2005<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
			<biblScope unit="page" from="2617" to="2620" />
		</imprint>
		<respStmt>
			<orgName>ISCA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Field Guide to Dynamical Recurrent Neural Networks</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">openSMILE -the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia (MM)</title>
		<meeting>ACM Multimedia (MM)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multidimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 International Conference on Artificial Neural Networks</title>
		<meeting>of the 2007 International Conference on Artificial Neural Networks<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dilley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<title level="m">Buckeye Corpus of Conversational Speech</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Department of Psychology, Ohio State University (Distributor)</orgName>
		</respStmt>
	</monogr>
	<note>nd release. www.buckeyecorpus.osu.edu</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TIMIT acoustic-phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Dahlgrena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localization of non-linguistic events in spontaneous speech by non-negative matrix factorization and Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5840" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The MediaEval 2012 Affect Task: Violent scenes detection in Hollywood Movies</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Penet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MediaEval 2012 Workshop</title>
		<meeting>of MediaEval 2012 Workshop<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speaker-dependent voice activity detection robust to background speech noise</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsujino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH 2012</title>
		<meeting>of INTERSPEECH 2012<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Portland</publisher>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A modified MAP criterion based on hidden Markov model for voice activity detecion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-05">may 2011</date>
			<biblScope unit="page" from="5220" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple acoustic model-based discriminative likelihood ratio weighting for voice activity detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="507" to="510" />
			<date type="published" when="2012-08">aug 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frame-wise model re-estimation method based on gaussian pruning with weight normalization for noise robust voice activity detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="244" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speech activity detection for noisy data using adaptation techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Omar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH 2012, Portland</title>
		<meeting>of INTERSPEECH 2012, Portland<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voice activity detection using speech recognizer feedback</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thambiratnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH 2012</title>
		<meeting>of INTER-SPEECH 2012<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acoustic and data-driven features for robust speech activity detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Janu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH 2012</title>
		<meeting>of INTERSPEECH 2012<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
