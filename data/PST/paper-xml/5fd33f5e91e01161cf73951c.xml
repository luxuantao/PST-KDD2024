<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMPIRICAL ANALYSIS OF UNLABELED ENTITY PROB-LEM IN NAMED ENTITY RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">EMPIRICAL ANALYSIS OF UNLABELED ENTITY PROB-LEM IN NAMED ENTITY RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of the performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach that is capable of eliminating the misguidance brought by unlabeled entities. The core idea is using negative sampling to keep the probability of training with unlabeled entities at a very low level. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with state-of-the-art method 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Named entity recognition (NER) is an important task in information extraction. Previous methods typically cast it into a sequence labeling problem by adopting IOB tagging scheme <ref type="bibr" target="#b4">(Huang et al., 2015;</ref><ref type="bibr" target="#b13">Ma &amp; Hovy, 2016;</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref>. A representative model is Bi-LSTM CRF <ref type="bibr" target="#b7">(Lample et al., 2016)</ref>. The great success achieved by these methods benefits from massive correctly labeled data. However, in some real scenarios, not all the entities in the training corpus are annotated. For example, in some NER tasks <ref type="bibr" target="#b11">(Ling &amp; Weld, 2012)</ref>, the datasets contain too many entity types or a mention may be associated with multiple labels. Since manual annotation is too hard, some entities are neglected by human annotators. Situations in distantly supervised NER <ref type="bibr" target="#b19">(Ren et al., 2015;</ref><ref type="bibr" target="#b2">Fries et al., 2017)</ref> are even more serious. To reduce handcraft annotation, distant supervision <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref> is applied to automatically produce labeled data. As a result, large amounts of entities in the corpus are missed due to the limited coverage of knowledge resources. We refer this to unlabeled entity problem, which largely degrades the performance of models.</p><p>There are several approaches used in prior works to alleviating this problem. Fuzzy CRF and Au-toNER <ref type="bibr" target="#b22">(Shang et al., 2018b)</ref> allow models to learn from the phrases that may be potential entities. However, since these phrases are obtained by using a distantly supervised phrase mining method <ref type="bibr" target="#b21">(Shang et al., 2018a)</ref>, many unlabeled entities in the training data may still not be recalled. In the context of only resorting to unlabeled corpora and an entity ontology, <ref type="bibr" target="#b16">Peng et al. (2019)</ref> employ positive-unlabeled (PU) learning <ref type="bibr" target="#b9">(Li &amp; Liu, 2005)</ref> to unbiasedly and consistently estimate the task loss. In implementations, they build a distinct binary classifiers for each label. Nevertheless, the unlabeled entities still impact on the classifiers of the corresponding entity types and, importantly, the model can't disambiguate neighboring entities. Partial CRF <ref type="bibr" target="#b23">(Tsuboi et al., 2008)</ref> is an extension of commonly used CRF <ref type="bibr" target="#b6">(Lafferty et al., 2001)</ref> that supports learning from incomplete annotations. <ref type="bibr" target="#b24">Yang et al. (2018)</ref>; <ref type="bibr" target="#b15">Nooralahzadeh et al. (2019)</ref> use it to circumvent the unlabeled entities. However, as fully annotated corpus is still required to get true negative instances, this approach is not applicable to the cases where no high-quality data is available.</p><p>In this work, our goal is to study what are the impacts of unlabeled entity problem on the models and how to effectively eliminate them. Initially, we construct some synthetic datasets and introduce degradation rates. The datasets are constructed by randomly removing the annotated named entities in well-annotated datasets, e.g., <ref type="bibr">CoNLL-2003</ref><ref type="bibr" target="#b20">(Sang &amp; De Meulder, 2003)</ref>, with different probabilities. The degradation rates measure how severe an impact of unlabeled entity problem degrades the results of models. Extensive studies are investigated on synthetic datasets. We find two causes: the reduction of annotated entities and treating unlabeled entities as negative instances. The first cause is obvious but has far less impact than the second one. Besides, it can be mitigated well by using a pretraining language model, like BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, as the sentence encoder. The second cause seriously misleads the models in training and exerts a great negative impact on their performances. Even in less severe cases, it can sharply reduce the F1 score by about 20%. Based on the above observations, we propose a novel method that can almost completely eliminate the misguidance of unlabeled entities in training. The core idea is applying negative sampling to keep the probability of training with unlabeled entities at a very low level. In Section 5, we provide a proof to confirm its theoretical correctness.</p><p>We have conducted extensive experiments to verify the effectiveness of our approach. Studies on synthetic datasets and real-world datasets (e.g., EC) show that our model handles unlabeled entities well and significantly outperforms previous baselines. On well-annotated datasets (e.g., <ref type="bibr">CoNLL-2003)</ref>, our model is competitive with current state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we formally define the unlabeled entity problem and briefly describe a strong baseline, BERT Tagging <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, used in empirical studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">UNLABELED ENTITY PROBLEM</head><p>We denote an input sentence as x = [x 1 , x 2 , • • • , x n ] and the annotated named entity set as y = {y 1 , y 2 , • • • , y m }. n is the sentence length and m is the amount of entities. Each member y k of set y is a tuple (i k , j k , l k ). (i k , j k ) is the span of an entity which corresponds to the phrase</p><formula xml:id="formula_0">x i k ,j k = [x i k , x i k +1 , • • • , x j k ]</formula><p>and l k is its label. The unlabeled entity problem is defined as, due to the limited coverage of machine annotator or the negligence of human annotator, some ground truth entities y of the sentence x are not covered by annotated entity set y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BERT TAGGING</head><p>BERT Tagging adopts IOB tagging scheme, where each token x i in a sentence x is labeled as Btag if it's the beginning of an entity, I-tag if it's inside but not the first one within the entity, or O otherwise. Hence, its output is a n-length label sequence z</p><formula xml:id="formula_1">= [z 1 , z 2 , • • • , z n ].</formula><p>Formally, BERT tagging firstly uses BERT to get the representation h i for every token x i :</p><formula xml:id="formula_2">[h 1 , h 2 , • • • , h n ] = BERT(x).</formula><p>(1)</p><p>Then, the label distribution q i is computed as Softmax(Wh i ). In training, the loss is induced as</p><formula xml:id="formula_3">1≤i≤n − log q i [z i ].</formula><p>At test time, it obtains the label for each token x i by arg max q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EMPIRICAL STUDIES</head><p>To understand the impacts of unlabeled entity problem, we conduct empirical studies over multiple synthetic datasets, different methods, and various metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PREPARATIONS</head><p>Synthetic Datasets. To simulate poorly annotated datasets, we randomly remove the labeled entities of well-annotated datasets with different masking probabilities. The datasets are <ref type="bibr">CoNLL-2003</ref><ref type="bibr" target="#b20">(Sang &amp; De Meulder, 2003)</ref> and OntoNotes 5.0 <ref type="bibr" target="#b18">(Pradhan et al., 2013)</ref>. Both datasets are usually used to test the performances of NER models. The probabilities p are respectively set as 0.0, 0.1, 0.2, • • • , 0.9. In this way, 2 × 10 synthetic datasets are constructed.  Methods. We adopt two models. One of them is BERT Tagging, which has long been regarded as a strong baseline. The other is LSTM Tagging that replaces the original encoder (i.e., BERT) of BERT Tagging with LSTM <ref type="bibr" target="#b3">(Hochreiter &amp; Schmidhuber, 1997)</ref>. We use it to study the effect of using pretraining language model. To explore the negative impact brought by unlabeled entities in training, we present an adjusted training loss for above two models:</p><formula xml:id="formula_4">1≤i≤n − log q i [z i ] − (i ,j ,l )∈ y i ≤k≤j − log q k [z k ] .<label>(2)</label></formula><p>The idea here is to remove the incorrect loss incurred by unlabeled entities. Note that missed entity set y is reachable in synthetic datasets but unknown in real-world datasets.</p><p>Metrics. Following prior works, the F1 scores of models are tested by using conlleval script<ref type="foot" target="#foot_1">2</ref> . We also design two degradation rates to measure the different impacts of unlabeled entity problem. One is erosion rate α p and the other is misguidance rate β p :</p><formula xml:id="formula_5">α p = f a 0 − f a p f a 0 , β p = f a p − f p f a p .<label>(3)</label></formula><p>For a synthetic dataset with the masking probability being p, f p and f a p are the performances of a model and its adjusted version, respectively. Erosion rate α p measures how severe the reduction of annotated entities degrades the F1 scores of a model. Misguidance rate β p measures how serious unlabeled entities misguide the model during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OVERALL ANALYSIS</head><p>The left parts of Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref> show the results of empirical studies. From them, we can draw the following observations. Firstly, the significant downward trends of solid lines confirm the fact that NER models severely suffer from unlabeled entity problem. For example, by setting the masking probability as 0.4, the performance of LSTM Tagging decreases by 33.01% on CoNLL-2003 and 19.58% on OntoNotes 5.0. Secondly, in contrast, the dashed lines change very slowly, indicating that the models with adjusted training loss (see Equation <ref type="formula" target="#formula_4">2</ref>) are much less influenced by the issue. For instance, when masking probability is 0.7, adopting adjusted loss preserves the F1 scores of BERT Tagging by 41.04% on CoNLL-2003 and 57.38% on OntoNotes 5.0. Lastly, for high masking probabilities, even though the negative impact of unlabeled entities is eliminated by adjusting the training loss, the performance still declines to a certain extent. For example, when masking probability is set as 0.8, the F1 scores of adjusted LSTM Tagging decrease by 31.64% on CoNLL-2003 and 17.22% on OntoNotes 5.0.</p><formula xml:id="formula_6">𝑥 ! Sequence Encoder (BERT or LSTM) 𝑥 " 𝑥 # 𝑥 $ 𝒉 ! 𝒉 " 𝒉 # 𝒉 $ 𝒔 !,# ⊕ MLP 𝒐 !,#</formula><p>Reduction of Annotated Entities. From the last observation, we can infer that a cause of performance degradation is the reduction of annotated entities. In the middle parts of Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>, we plot the change of erosion rates α p (see Equation <ref type="formula" target="#formula_4">2</ref>) with respect to masking probabilities. We can see that its impact is not very serious when in low masking probabilities but can't be neglected when in high ones. Besides, using pre-training language models greatly mitigates the issue. As an example, when the probability is 0.8, on both CoNLL-2003 and OntoNotes 5.0, the erosion rates of adjusted BERT Tagging are only about half of those of adjusted LSTM Tagging.</p><p>Misguidance of Unlabeled Entities. From the last two observations, we can conclude that the primary cause is treating unlabeled entities as negative instances, which severely misleads the models during training. To better understand it, in the right parts of Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>, we plot the change of misguidance rates β p (see Equation <ref type="formula" target="#formula_5">3</ref>) with masking probabilities. These rates are essentially the percentage decreases of F1 scores. From them, we can see that the impact of misguidance is very much serious even when in low masking probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>Motivated by Section 3.2, we present a model that is robust to the unlabeled entity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SCORING MODEL WITH BERT</head><p>Based on the findings in Section 3.2, we use BERT as the default encoder to mitigate the reduction of annotated entities. Specifically, given a sentence x, we firstly obtain the token representations h i with Equation <ref type="formula">1</ref>. Then, we get the representation for every phrase x i,j as</p><formula xml:id="formula_7">s i,j = h i ⊕ h j ⊕ (h i − h j ) ⊕ (h i h j ), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ⊕ is vector concatenation and is element-wise product.</p><p>Finally, multi-layer perceptron (MLP) computes the label distribution o i,j for a span (i, j):</p><formula xml:id="formula_9">o i,j = Softmax(U tanh(Vs i,j )).<label>(5)</label></formula><p>The term o i,j [l] is the predicted score for an entity (i, j, l).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRAINING VIA NEGATIVE SAMPLING</head><p>From Section 3.2, we know that regarding all the unlabeled spans as negative instances certainly degrades the performances of models, since some of them may be missed entities. Our solution to this issue is negative sampling. Specifically, we randomly sample a small subset of unlabeled spans as the negative instances to induce the training loss.</p><p>Given the annotated entity set y, we firstly get all the negative instance candidates as</p><formula xml:id="formula_10">{(i, j, O) | (i, j, l) / ∈ y, 1 ≤ i ≤ j ≤ n, l ∈ L}, (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where L is the label space and O is the label for non-entity spans.</p><p>Then, we randomly sample a subset y from the whole candidate set. The size of sample set y is λ * n , 0 &lt; λ &lt; 1, where is the ceiling function.</p><p>Ultimately, a span-level cross entropy loss used for training is incurred as</p><formula xml:id="formula_12">(i,j,l)∈y − log(o i,j [l]) + (i ,j ,l )∈ y − log(o i ,j [l ]) .<label>(7)</label></formula><p>In Section 5, we prove that, through negative sampling, the probability of training with unlabeled entities is kept at a very low level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">INFERENCE</head><p>At test time, firstly, the label for every span (i, j) is obtained by arg max l o i,j <ref type="bibr">[l]</ref>. Then, we select the ones whose label l is not O as predicted entities. When the spans of inferred entities intersects, we preserve the one with the highest predicted score and discard the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We prove that, through negative sampling, the probability of not treating a specific missed entity in a n-length sentence as the negative instance is larger than 1 − 2 n−5 :</p><formula xml:id="formula_13">0≤i&lt; λn 1 − 1 n(n−1) 2 − m − i &gt; 1 − 1 n(n−1) 2 − m − λn λn &gt; 1 − 1 n(n−1) 2 − n − n n ≥ 1 − n * 1 n(n−1) 2 − n − n = 1 − 2 n − 5 .<label>(8)</label></formula><p>Here n(n−1) 2 − m is the amount of negative instance candidates. Note that we use the facts λ &lt; 1, m ≤ n, and (1 − z) n ≥ 1 − nz, 0 ≤ z ≤ 1 during the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We have conducted extensive experiments on multiple datasets to verify the effectiveness of our method. Studies on synthetic datasets show that our model can almost eliminate the misguidance brought by unlabeled entities in training. On real-world datasets, our model has notably outperformed prior baselines and achieved the state-of-the-art performances. On well-annotated datasets, our model is competitive to current state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SETTINGS</head><p>The setup of synthetic datasets and well-annotated datasets are the same as what we describe in Section 3.1. For real-world datasets, we use EC and NEWS, both of which are collected by <ref type="bibr" target="#b24">(Yang et al., 2018)</ref>. EC is in e-commerce domain, which has 5 entity types: Brand, Product, Model, Material, and Specification. The data contains 2400 sentences tagged by human annotators and are divided into three parts: 1200 for training, 400 for dev, and 800 for testing. <ref type="bibr" target="#b24">Yang et al. (2018)</ref> also construct an entity dictionary of size 927 and apply distant supervision on a raw corpus to obtain additional 2500 sentences for training. NEWS is from MSRA dataset <ref type="bibr" target="#b8">(Levow, 2006)</ref>. <ref type="bibr" target="#b24">Yang et al. (2018)</ref>   We adopt the same neural network configurations for all the datasets. L2 regularization and dropout ratio are respectively set as 1 × 10 −5 and 0.4 for reducing overfit. The dimension of scoring layers is 256. Ratio λ is set as 0.35. When the sentence encoder is LSTM, we set the hidden dimension as 512 and use cased, 300d GloVe <ref type="bibr" target="#b17">(Pennington et al., 2014)</ref> to initialize the word embedding. We utilize Adam (Kingma &amp; Ba, 2014) as the optimization algorithm and adopt the suggested hyperparameters. At test time, we convert the predictions of our models into IOB format and use conlleval script to compute the F1 score. In all the experiments, the improvements of our models over the baselines are statistically significant with p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RESULTS ON SYNTHETIC DATASETS</head><p>In this section, our model is compared with BERT Tagging on the synthetic datasets of the masking probabilities being 0.1, 0.2, • • • , 0.6. From Table <ref type="table" target="#tab_0">1</ref>, we can get two conclusions. Firstly, our model significantly outperforms BERT Tagging, especially in high masking probabilities. For example, on CoNLL-2003, our F1 scores outnumber those of BERT Tagging by 1.88% when the probability is 0.2 and 27.16% when the probability is 0.6. Secondly, our model is very robust to the unlabeled entity problem. When increasing the masking probability from 0.1 to 0.5, the results of our model only decrease by 2.35% on CoNLL-2003 and 1.91% on OntoNotes 5.0.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> demonstrates the misguidance rate comparisons between BERT Tagging and our models. The way to adjust our model is reformulating Equation ( <ref type="formula" target="#formula_12">7</ref>) by defining the negative term via {(i, j, O) | ∀l : (i, j, l) / ∈ y ∪ y} rather than the negatively sampled ŷ. The idea here is to avoid the unlabeled entities being sampled. From Figure <ref type="figure" target="#fig_3">4</ref>, we can discover that, in all masking probabilities, the misguidance rates of our model are far smaller than those of BERT Tagging and are consistently lower than 2.50%. These indicate that, in training, our model indeed eliminates the misguidance brought by unlabeled entities to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CoNLL-2003 OntoNotes 5.0 Flair Embedding <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref> 93.09 89.3 BERT-MRC <ref type="bibr" target="#b10">(Li et al., 2019)</ref> 93.04 91.11 HCR w/ BERT <ref type="bibr" target="#b12">(Luo et al., 2020)</ref> 93.37 90.30 BERT-Biaffine Model <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> 93 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">RESULTS ON FULLY ANNOTATED DATASETS</head><p>We additionally apply our model with negative sampling on the well-annotated datasets where the issue of incomplete entity annotation is not serious. To train our model with negative sampling, λ is set to be 0.35 as in experiments of real-world datasets. As shown in Table <ref type="table" target="#tab_1">2</ref>, the F1 scores of our model are very close to current best results. Our model slightly underperforms BERT-Biaffine Model by only 0.09% on CoNLL-2003 and 0.78% on OntoNotes 5.0. Besides, our model surpasses many other strong baselines. On OntoNotes 5.0, our model outperforms HCR w/ BERT by 0.32% and Flair Embedding by 1.44%. On CoNLL-2003, the improvements of F1 scores are 0.41% over BERT-MRC and 0.35% over Flair Embedding. All these results indicate that our model is still very effective when applied to high-quality data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">RESULTS ON REAL-WORLD DATASETS</head><p>For two real-world datasets, a large portion of training data is obtained via distant supervision. As stated in <ref type="bibr" target="#b24">Yang et al. (2018)</ref>, the F1 scores of string matching through an entity dictionary are notably declined in terms of the low recall scores, although its precision scores are higher than those of other methods. Therefore, unlabeled entity problem is serious in the datasets.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, the baselines come from two works <ref type="bibr" target="#b24">(Yang et al., 2018;</ref><ref type="bibr" target="#b15">Nooralahzadeh et al., 2019)</ref>. <ref type="bibr" target="#b24">Yang et al. (2018)</ref> use Partial CRF to circumvent all possible unlabeled entities and leverage reinforcement learning (RL) to skip noisy annotation in an adaptive manner, and <ref type="bibr" target="#b15">Nooralahzadeh et al. (2019)</ref> further improve their work by optimizing the policy of RL. A disadvantage of Partial CRF is that it can't be trained with the true negative instances in poorly labeled corpora. Our model has significantly outperformed prior baselines and obtained new state-of-the-art results. Compared with prior best model <ref type="bibr" target="#b15">(Nooralahzadeh et al., 2019)</ref>, we achieve the improvements of 3.94% on EC and 6.27% on NEWS. Compared with strong baseline, BiLSTM + Partial CRF, the increases of F1 scores are 9.20% and 8.21%. To make fair comparisons, we replace BERT with LSTM. Even so, we still outperform <ref type="bibr" target="#b15">(Nooralahzadeh et al., 2019</ref>) by 1.76% on EC and 2.52% on NEWS. All these strongly confirm the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">RATIO λ IN NEGATIVE SAMPLING</head><p>Intuitively, setting ratio λ (see Section 4.2) as too large values or too small values both are inappropriate. Large ratios amplify the contribution of negative entities, leading to underfitting. Small ratios reduce the amount of negative instances used for training, leading to overfitting. Figure <ref type="figure">5</ref> shows the experiments on some synthetic datasets with the ratio λ of our method being 0.1, 0.2, • • • , 0.9. From Figure <ref type="figure">5</ref>: The results of our models with different ratio λ on synthetic datasets. it, we can see that all the score curves are roughly arched, which verifies our intuition. Besides, we find that 0.3 &lt; λ &lt; 0.4 performs well in all the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>NER is a classical task in information extraction. Previous works generally regard it as a sequence labeling problem by using IOB tagging scheme <ref type="bibr" target="#b4">(Huang et al., 2015;</ref><ref type="bibr" target="#b7">Lample et al., 2016;</ref><ref type="bibr" target="#b0">Akbik et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2020)</ref>. Each word in the sentence is labeled as B-tag if it is the beginning of an entity, I-tag if it's inside but not the first one within the entity, or O otherwise. This approach is extensively studied in prior works. For example, <ref type="bibr" target="#b0">Akbik et al. (2018)</ref> propose Flair Embedding that pretrains character embedding in large corpora and uses it rather than token representations to represent a sentence. Recently, there is a growing interest in span-based models <ref type="bibr" target="#b10">(Li et al., 2019;</ref><ref type="bibr" target="#b25">Yu et al., 2020)</ref>. They treat the spans, instead of single words, as the basic units for labeling. For example, <ref type="bibr" target="#b10">Li et al. (2019)</ref> present BERT-MRC that regards NER as a MRC task, where named entities are extracted as retrieving answer spans.</p><p>In some practical applications (e.g., data annotation in fine-grained NER), NER models are faced with unlabeled entity problem, where the unlabeled entities seriously degrade the performances of models. Several approaches to this issue have been proposed. Fuzzy CRF and AutoNER <ref type="bibr" target="#b22">(Shang et al., 2018b)</ref> allow learning from high-quality phrases. However, since these phrases are obtained through distant supervision, the unlabeled entities in the corpora may still be missed. PU learning <ref type="bibr" target="#b9">(Li &amp; Liu, 2005)</ref> unbiasedly and consistently estimates the training loss. Nevertheless, the unlabeled entities still impact on the classifiers of the corresponding entity types and, importantly, the model can't disambiguate neighboring entities. Partial CRF <ref type="bibr" target="#b24">(Yang et al., 2018;</ref><ref type="bibr" target="#b15">Nooralahzadeh et al., 2019)</ref> supports learning from incomplete annotations. However, because fully annotated corpus is still in need to training models with true negative instances, this type of approach is not applicable to the situations where no high-quality data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we study what are the impacts of unlabeled entities on NER models and how to effectively eliminate them. Through empirical studies performed on synthetic datasets, we find two causes: the reduction of annotated entities and treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misleads the models in training and greatly affects their performances. Based on the above observations, we propose a novel method that is capable of eliminating the misguidance of unlabeled entities in training. The core idea is applying negative sampling to keep the probability of training with unlabeled entities at a very low level. Experiments on synthetic datasets and real-world datasets show that our model handles unlabeled entities well and significantly outperforms previous baselines. On well-annotated datasets, our model is competitive with existing state-of-the-art approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The empirical studies conducted on CoNLL-2003 dataset.</figDesc><graphic url="image-2.png" coords="3,117.90,223.07,376.20,105.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The empirical studies investigated on OntoNotes 5.0 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: This demonstrates how our model scores possible entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The misguidance rates of BERT Tagging and our model.</figDesc><graphic url="image-3.png" coords="6,163.44,216.44,285.12,126.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>only adopt the PERSON entity type. Training data of size 3000, dev data of size 3328, and testing data of size 3186 are all sampled from MSRA. They collect an entity dictionary of size 71664 and perform distant supervision on the rest data to obtain extra 3722 training cases by using The experiment results on two synthetic datasets.</figDesc><table><row><cell>Masking Prob.</cell><cell cols="4">CoNLL-2003 BERT Tagging Our Model BERT Tagging Our Model OntoNotes 5.0</cell></row><row><cell>0.1</cell><cell>90.71</cell><cell>91.37</cell><cell>87.69</cell><cell>89.20</cell></row><row><cell>0.2</cell><cell>89.57</cell><cell>91.25</cell><cell>86.86</cell><cell>89.15</cell></row><row><cell>0.3</cell><cell>88.95</cell><cell>90.53</cell><cell>84.75</cell><cell>88.73</cell></row><row><cell>0.4</cell><cell>82.94</cell><cell>89.73</cell><cell>82.55</cell><cell>88.20</cell></row><row><cell>0.5</cell><cell>78.99</cell><cell>89.22</cell><cell>71.07</cell><cell>88.17</cell></row><row><cell>0.6</cell><cell>63.84</cell><cell>87.65</cell><cell>58.17</cell><cell>87.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The experiment results on two well-annotated datasets.</figDesc><table><row><cell>.5</cell><cell>91.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The experiment results on two real-world datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The source code of this work is publicly available at https://XXX.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Swellshark: A generative model for biomedical named entity recognition without labeled data</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06360</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from positive and unlabeled examples with different data distributions</title>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="218" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A unified mrc framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11476</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical contextualized representation for named entity recognition</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengshun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8441" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement-based denoising of distantly supervised ner with partial annotation</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>Lønning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>Øvrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
				<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distantly supervised named entity recognition using positive-unlabeled learning</title>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01378</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustype: Effective entity recognition and typing by relation phrase-based clustering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="995" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><surname>De Meulder</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03599</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training conditional random fields using incomplete annotations</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
				<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distantly supervised ner with partial annotation learning and reinforcement learning</title>
		<author>
			<persName><forename type="first">Yaosheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2159" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
