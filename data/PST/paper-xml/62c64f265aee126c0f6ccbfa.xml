<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Key Lab of Intelligent Telecomm. Software and Multimedia</orgName>
								<orgName type="institution">Beijing Univ. of Posts and Telecomm</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junping</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Key Lab of Intelligent Telecomm. Software and Multimedia</orgName>
								<orgName type="institution">Beijing Univ. of Posts and Telecomm</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Key Lab of Intelligent Telecomm. Software and Multimedia</orgName>
								<orgName type="institution">Beijing Univ. of Posts and Telecomm</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Key Lab of Intelligent Telecomm. Software and Multimedia</orgName>
								<orgName type="institution">Beijing Univ. of Posts and Telecomm</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeli</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Key Lab of Intelligent Telecomm. Software and Multimedia</orgName>
								<orgName type="institution">Beijing Univ. of Posts and Telecomm</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aspect-Based Sentiment Analysis</term>
					<term>DeBERTa</term>
					<term>Local Context Focus</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text sentiment analysis, also known as opinion mining, is research on the calculation of people's views, evaluations, attitude and emotions expressed by entities. Text sentiment analysis can be divided into text-level sentiment analysis, sentencelevel sentiment analysis and aspect-level sentiment analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the field of sentiment analysis, which aims to predict the polarity of aspects. The research of pre-training neural model has significantly improved the performance of many natural language processing tasks. In recent years, pre training model (PTM) has been applied in ABSA. Therefore, there has been a question, which is whether PTMs contain sufficient syntactic information for ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis problem. DeBERTa is a kind of neural language model based on transformer, which uses self-supervised learning to pre-train on a large number of original text corpora. Based on the Local Context Focus (LCF) mechanism, by integrating DeBERTa model, we purpose a multi-task learning model for aspect-based sentiment analysis. The experiments result on the most commonly used the laptop and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that LCF mechanism with DeBERTa has significant improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment Analysis (SA) is an active field in Natural Language Processing, which involves the viewpoint of text <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. With the development of science and Technology Society, more and more people like to express opinions and comments on products after consumption, which has formed a large number of comment texts <ref type="bibr">[3][4]</ref>. There are lots of entity attributes in the comment texts. Aspect-Based Sentiment Analysis (ABSA) <ref type="bibr" target="#b4">[5]</ref>[6] aims to do the fine-grained sentiment analysis towards aspects, which is the sub-task of Sentiment Analysis. Traditional methods explore the general emotions of texts, but ABSA is a more challenging task because opinions can contain several aspects. Take the sentence "Its size is ideal and the weight is acceptable" for example, the customer had a positive sentiment for both size and weight. The task is to predict the emotions towards the underlined aspects. Generally, the main research line of ABSA involves the identification of various aspectlevel sentiment elements, namely, aspect terms, aspect categories, opinion terms and sentiment polarities <ref type="bibr" target="#b6">[7]</ref>. ABSA tasks include four tasks, which are called aspect term extraction (ATE), aspect category detection (ACD), opinion term extraction (OTE), and aspect sentimental classification (ASC).ASC task is designed to predict the sentiment polarity for a particular aspect within a sentence. In this paper, we only focus on ASC tasks.</p><p>Early ASC tasks were usually based on manually designed functions, such as term frequency <ref type="bibr" target="#b7">[8]</ref>. In recent years, ASC tasks based on deep learning has attracted extensive attention. BERT-based deep neural language models are widely used for ABSA <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b9">[10]</ref>. Bert explores the recent deep bidirectional encoder representation from transformers, which processes text considering the context on the left and right sides of the word in all layers <ref type="bibr" target="#b10">[11]</ref>. Bert can generate more text semantic representations, in which each word is mapped to an embedded vector, which depends on the context in the sentence. Pre-training methods (PTMs) such as BERT have brought significant performance improvements of the ASC task. Next, by training the model longer, with bigger batches, over more data, RoBERTa (A Robustly Optimized BERT Pre-training Approach) can exceed the performance of the traditional BERT method <ref type="bibr" target="#b11">[12]</ref>. In 2020, He et al. <ref type="bibr" target="#b12">[13]</ref> introduced a new architecture for BERT-based language model training called Decoding-enhanced BERT with Disentangled Attention (DeBERTa). Two new technologies are used to improve the previous state-ofthe-art PLMs: a disentangled attention mechanism, and an enhanced mask decoder.</p><p>In this paper, we present a method for Aspect-Based Sentiment Analysis using Local Context Focus (LCF) Mechanism with DeBERTa. LCF mechanism combines local context features and global context features to predict sentiment polarity of targeted aspect <ref type="bibr" target="#b13">[14]</ref>. In this way, the model can discover the unknown aspects and pay more attention to local context words of specific aspect. It is very important for sentient analysis based on domain specific aspects. Because the words representing aspects and sentiments are position dependent in the viewpoint text, they are usually close to each other. LCF can compute local context features. By further studying the application of PTMs in ABSA tasks, we made fine-tuning and mechanism enhancement on it.</p><p>Therefore, the main contributions of this paper are as follows:</p><p>1. This paper adds the latest PTM to the LCF design model for the first time. DeBERTa improves previous state-of-the-art PLMs using two novel techniques: a disentangled attention mechanism, and an enhanced mask decoder. This significantly improves the performance of LCF design. 2. On the basis of preprocessing, a new mechanism is added to enhance the relationship between local context features and global context features. Contextual features make the model better predict the polarity in aspects of the target.</p><p>3. Through the improvement and adjustment of the model, the performance of the ASC task is significantly improved. We conducted experiments with three datasets, which is the laptop and restaurant datasets of SemEval-2014 and the ACL twitter dataset.</p><p>The experimental results show that the model is enhanced to some extent on three datasets, especially the Restaurant dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>ABSA tasks include four tasks. ASC task is designed to predict the sentiment polarity for a particular aspect within a sentence. Because ASC task is the focus of this paper. And in the method, we use the PTMs. Accordingly, we separate our discussion of related work into two areas: Firstly, methods and related research on ASC tasks in recent years. Secondly, development and application of PTM in Aspect-Based Sentiment Analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aspect Sentimental Classification</head><p>Aspect Sentimental Classification is another important subtask of ABSA. Generally, the aspects can be instantiated as aspect terms or aspect categories, resulting in two ASC problems: aspect term-based sentiment classification and aspect category-based sentiment classification. In fact, the main research issues of these two subtasks are the same. What they explore is how to use aspects and context to classify sentients at aspect level.</p><p>In recent years, ASC based on deep learning has aroused widespread interest. Many ASC models based on neural networks <ref type="bibr" target="#b14">[15]</ref>[16] <ref type="bibr" target="#b16">[17]</ref> have been proposed and brought great performance improvements <ref type="bibr" target="#b17">[18]</ref>. Neural models such as TC-LSTM <ref type="bibr" target="#b19">[20]</ref> were created in order to model the interaction between the aspect and sentence context. Different parts of a sentence have different functions in specific aspects, so attention mechanism is widely used to obtain the representation of specific aspects <ref type="bibr" target="#b20">[21]</ref>. As a representative work, Wang et al. <ref type="bibr" target="#b21">[22]</ref> purposed Attention-based LSTM with Aspect Embedding (ATAE-LSTM) model. The paper attached aspect embeddedness to each word vector of the input sentence to calculate the attention weight, and can calculate aspect specific sentence embeddedness accordingly to classify sentient. And then came the design of more complex attention mechanisms, which purposed to learn better aspect-specific representations. IAN interactively generate the representations for aspect and sentence attention separately, which was purposed by Ma et al. <ref type="bibr" target="#b22">[23]</ref>. Besides, other network structures like the gated network <ref type="bibr" target="#b23">[24]</ref> also had a good effect. Recently, the development of preprocessing models has greatly improved the performance of tasks. For instance, Sun et al. <ref type="bibr" target="#b24">[25]</ref> utilize the ability of BERT by transforming the task as a pair classification problem. There is another method of the ASC research modeling the syntactic structure of the sentence to infer the polarity of the sentient aspects. As the improvement of dependency analysis based on neural network <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b27">[28]</ref> in recent years, better parse tree brought significant improvements to the dependent ASC models. Sun et al. <ref type="bibr" target="#b28">[29]</ref> and Zhang et al. <ref type="bibr" target="#b29">[30]</ref> utilized word dependencies and the syntactical information to model the dependency tree. What they employ is the graph neural network (GNN) <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b31">[32]</ref>.</p><p>Compared with text-level and sentence-level sentient analysis, aspect-level sentient analysis faces new problems. Aspect-level sentiment analysis technology not only analyzes the explicit language expression structure, but also deeply understands the implicit semantic expression. Besides, Aspect-level sentiment analysis needs to determine the context range in which sentiment is expressed for each evaluation aspect. Aspect-level sentiment analysis technology needs to correctly understand the semantic information of the text word-level and sentence-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-Training Models</head><p>The pre-trained word embeddings, such as Word2Vec <ref type="bibr" target="#b32">[33]</ref> and GloVe <ref type="bibr" target="#b33">[34]</ref> were used in the conventional neural ABSA models, coupling with the task-specific neural architecture. Compared with early feature-based models, they have a certain effectiveness. But they can't capture complex sentiment dependencies in the sentence because of the context-independent word embeddings. In recent years, pre-trained language models such as BERT <ref type="bibr" target="#b34">[35]</ref> and RoBERTa <ref type="bibr" target="#b35">[36]</ref> have brought significant improvements on NLP tasks. Given the wealth of knowledge learned during the pre-training phase, simply leveraging such contextualized embeddings has yielded large performance gains. For example, Li et al. <ref type="bibr" target="#b36">[37]</ref> tried using several stacked standard prediction layers on PTM for the E2E-ABSA tasks. On the basis of the original Bert model, RoBERTa has proved the following points through experiments: further increasing the number of pre-training data can improve the model effect; extending the pre-training time or increasing the number of pre-training steps can improve the model effect.</p><p>For PTMs, most of the current mainstream models use Transformer as a feature extractor. At this stage, the potential of Transformer has not been fully tapped, and there is still a lot of potential to tap. Besides being the backbone of the ABSA model, PTMs can also benefit from handling ABSA tasks in other ways. For example, language modeling tasks used in the pre-training phase of PTM often bring the ability to perform generative data augmentation. Li et al. <ref type="bibr" target="#b37">[38]</ref> leveraged PTM to achieve semantic-preserving augmentation in a generative manner, obtaining clear improvements over baseline methods on a range of ABSA tasks.</p><p>At present, it is generally believed in the NLP community that PTMs can accurately reflect the semantics of input words <ref type="bibr" target="#b38">[39]</ref>. However, the contextual embeddings obtained through the self-attention mechanism, which capture the full word dependencies in the sentence, may be somewhat redundant for the ABSA task. Because ABSA often doesn't need to capture as many dependencies, doing so is wasteful. How to consolidate meaningful sparse relational structures with PLM, or perfect the intrinsic fully connected self-attention, and obtain ABSA-related representations in a more efficient way, deserves more attention and research work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeBERTa-LCF</head><p>In this section, we will introduce that our approach the Aspect-Target Sentiment Classification task using a two-step procedure. In the first step, we use the pre-trained model DeBERTa as a basis. We first briefly introduce the model structure of BERT and DeBERTa.</p><p>In the second step, BERT shared layer is adopted as the embedding layer and feature extractor layer. And integrate it into the local context focusing mechanism. The main framework of DeBERTa-LCF model is shown in Fig. <ref type="figure">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT and DeBERTa</head><p>BERT and DeBERTa both take Transformers as backbone architecture. The disentangled attention introduced by DeBERTa proposes to separate the content and text position components. Learning attention weights for each component is the main idea of DeBERTa. So it is different from other proposals that sum the position vector to the content vector <ref type="bibr" target="#b39">[40]</ref>. This explicit separation allows the model to better segment the positional and content components of the data, where positional embeddings are responsible for generating syntactic features and content embeddings are responsible for semantic features.</p><p>Ai j = {Hi, Pi |j} ? {H j, P j |i} T = Hi H j T + Hi P j| i T + Pi| j H j T + Pi| j P j| i T (1)</p><p>Equation 1 defines the cross-attention matrix used in DeBERTa. In BERT-based models, tokens are represented by content vectors and position vectors. The existing relative position coding methods use a separate embedding matrix to calculate the relative position deviation when calculating the attention weight <ref type="bibr" target="#b40">[41]</ref>. It is similar to calculating the attention weight using only the content to content and content to position terms in equation 1.</p><p>In ABSA tasks, the goal is to determine the sentiment of each aspect of the entity. Our model utilized DeBERTa pre-trained model parameters to initialize the model for downstream tasks and we apply fine-tuning based on ABSA labeled data to update model parameters. He et al. <ref type="bibr" target="#b41">[42]</ref> purposed that without major changes to other parts of the neural network structure, the disentangled attention mechanism can be incorporated into BERT model. We followed the original fine-tuning strategy because these features have been naturally captured by the disentangled attention mechanism.</p><p>The disentangled mechanism already took into account the content and relative positions of the context words. So we do not do much fine-tuning on the DeBERTa model. DeBERTa only needs half the data and is better than BERT and RoBERTa. We use the standard softmax layer with categorical cross-entropy loss function, which is the output of the language model, to provides downstream sentiment classification tasks. BERT-Shared layer is regarded as embedded layers, and the fine-tuning process is performed independently according to the joint loss function learned by multi-task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Context Focus</head><p>To determine whether the context word belongs to the local context of the targeted aspect, semantic-relative distance (SRD) is purposed for helping models capture local context. SRD is a concept based on token-aspect pairs,which describes the distance between token and aspect. After calculating all the tagged outputs from the attention layer, the output features at each output location above the SRD threshold will be hidden or weakened, while the output features of the local context words will be fully preserved. We purposed two architectures to focus on local contexts CDM and CDW. Our model focuses on local context by adopting local context focus layer. The input sequence of LCF design is mainly based on the global context. The DeBERTa layer is powerful enough to capture context features. Based on its self-attention mechanism, Multi-Head Self-Attention (MHSA) performs multiple attention functions to calculate the attention score of each contextual word. By MHSA, the features of each code are more closely related to itself. The multiple self-attentions work at the same time, and the obtained results are processed, so that the obtained information is more comprehensive and can be used to extract deep semantic features in the context. MHSA can avoid the negative effects of long-distance context dependence when learning features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate our model, we used the SemEval-2014 and the ACL twitter dataset. There are three datasets for this task: the laptop, restaurants, and twitter reviews. Rest14 and Laptop14 are from SemEval 2014 task 4 <ref type="bibr" target="#b0">[1]</ref> containing sentiment reviews from restaurant and laptop domains. Twitter is from Dong et al. <ref type="bibr" target="#b42">[43]</ref> which is processed from tweets. In these datasets, users rate their experiences with laptops and restaurants and comments made on Twitter in various aspects. The polarity of each aspect on these datasets may be positive, neutral, and negative. Therefore, the datasets we used contain 12184 total samples and three sentiments (positive, negative, and neutral). The statistics of these datasets are presented in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Models</head><p>In order to prove the excellent reliability of the DeBERTa-LCF model described in this paper, it was necessary to show that this model is superior to other models. We compared DeBERTa-LCF with the following state-of-the-art models: BERT-BASE <ref type="bibr" target="#b43">[44]</ref> is the baseline of BERT-based models. ATAE-LSTM <ref type="bibr" target="#b44">[45]</ref> is a classic LSTM based model, which uses attention-based LSTM to explore the relationship between an aspect and sentence content. GCAE <ref type="bibr" target="#b45">[46]</ref> is a CNN based model which based on convolutional neural networks and gating mechanisms. BERT-ADA <ref type="bibr" target="#b46">[47]</ref> is a domain-adapted BERT-based model proposed, which fine-tuned the BERT-BASE model on task-related corpus. IAN <ref type="bibr" target="#b47">[48]</ref> generates the representation of target aspect and context through two LSTM networks respectively, which learns the representation of target aspects and contexts interactively.</p><p>RAM <ref type="bibr" target="#b48">[49]</ref> is a novel framework based on neural networks to identify the sentiment of opinion targets with a RNN for sentence representation. The results with "*" are derived from our model. We highlight the best results on bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Overall Performance</head><p>Table <ref type="table" target="#tab_1">2</ref> shows an overview of the experimental results using ACC and F1 metrics for the Laptop, Restaurant and Twitter datasets. The experiments result on the most commonly used the laptop and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that LCF mechanism with DeBERTa has been improved in different degrees, especially in restaurant datasets.</p><p>In Restaurant dataset, our proposed DeBERTa-LCF model achieves the best results in terms of both macro-F1 scores and accuracy scores. It gets an improvement of 1.34, 0.87.</p><p>In Laptop dataset, our model has also been improved to some extent on macro-F1 scores. And its accuracy scores are only slightly worse than the base-lines.</p><p>In Twitter dataset, our model is second only to the best one at present, and the difference is very slight. The comparisons with BERT-BASE models suggest that the local context focus mechanism competent to discover unknown aspects and predict sentiment polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a DeBERTa model with LCF mechanism for aspect-based sentiment classification tasks. We introduce LCF mechanism, which is of great significance for aspect item extraction. LCF designs focus on the local context and learn global context representations in parallel. At the same time, we introduce the DeBERTa model, which is the latest pre-training model. This greatly increased the performance of the model. We applied the DeBERTa model and integrate it and LCF mechanisms for the first time. We conduct a set of experiments on three datasets. The results prove that our model achieves certain performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The main framework of DeBERTa-LCF model.</figDesc><graphic url="image-1.png" coords="5,206.25,258.40,199.34,269.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detail of benchmark datasets</figDesc><table><row><cell></cell><cell>Positive</cell><cell>Negative</cell><cell>Neutral</cell></row><row><cell>Laptops</cell><cell>994</cell><cell>870</cell><cell>464</cell></row><row><cell>Restaurants</cell><cell>2164</cell><cell>807</cell><cell>637</cell></row><row><cell>Twitter</cell><cell>1561</cell><cell>1560</cell><cell>3127</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results of performance.</figDesc><table><row><cell></cell><cell>Laptop</cell><cell></cell><cell>Restaurant</cell><cell></cell><cell>Twitter</cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>BERT-BASE</cell><cell>78.52</cell><cell>75.5</cell><cell>82.12</cell><cell>74.53</cell><cell>73.4</cell><cell>70.22</cell></row><row><cell>ATAE-LSTM</cell><cell>68.7</cell><cell>67.23</cell><cell>77.1</cell><cell>66.84</cell><cell>69.22</cell><cell>67.4</cell></row><row><cell>GCAE</cell><cell>78.05</cell><cell>69.93</cell><cell>78.31</cell><cell>68.74</cell><cell>71.42</cell><cell>69.3</cell></row><row><cell>BERT-ADA</cell><cell>79.7</cell><cell>75.21</cell><cell>81.23</cell><cell>72.5</cell><cell>74.36</cell><cell>71.53</cell></row><row><cell>IAN</cell><cell>72.14</cell><cell>70.6</cell><cell>78.64</cell><cell>70.12</cell><cell>70.3</cell><cell>67.6</cell></row><row><cell>RAM</cell><cell>74.45</cell><cell>71.32</cell><cell>80.27</cell><cell>71.02</cell><cell>69.72</cell><cell>67.2</cell></row><row><cell>DeBERTa-LCF*</cell><cell>79.54</cell><cell>75.64</cell><cell>83.46</cell><cell>75.4</cell><cell>70.83</cell><cell>68.64</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Opinion leader mining of social network combined with hierarchical sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Intelligent Automation Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="639" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Web opinion mining based on sentiment phrase classification vector</title>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 2nd IEEE InternationalConference on Network Infrastructure and Digital Content</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="308" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08615</idno>
		<title level="m">Scientific and Technological Information Oriented Semantics-adversarial and Media-adversarial Cross-media Retrieval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: SemEval-2014 Task 4: Aspect Based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">SemEval 2014. 2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Semeval-2016 task 5: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aspect sentiment quad prediction as paraphrase generation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2021 Main Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9209" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">XRCE: Hybrid Classification for Aspect-based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caroline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Claude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">SemEval 2014. 2014</date>
			<biblScope unit="page" from="838" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Targeted Sentiment Classification with Attentional Encoder Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2019: Text and Time Series</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Tetko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>K?rkov?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Karpov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Theis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11730</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
	<note>ICANN 2019</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4933" to="4941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Myle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Naman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Veselin</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoding-enhanced BERT with Disentangled Attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Deberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LCF: A local context focus mechanism for aspect-based sentiment classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3389</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image fusion based on nonsubsampled contourlet transform and saliencymotivated pulse coupled neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Average consensus for networks of continuous-time agents with delayed information and jointly-connected topologies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 American Control Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3884" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consensus seeking via iterative learning for multi-agent systems with switching topologies and communication time-delays</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robust and Nonlinear Control</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3772" to="3790" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Buenos Aires Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1347" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed extended Kalman filter with nonlinear consensus estimate</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="7983" to="7995" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective LSTMs for Target-Dependent Sentiment Classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5956" to="5963" />
		</imprint>
		<respStmt>
			<orgName>Association for the Advancement of Artificial Intelligence</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for Aspect-level Sentiment Classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association of Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variance-constrained state estimation for nonlinear complex networks with uncertain coupling strength</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed consensus extended Kalman filter: a variance-constrained approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theory &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="389" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Region-based multi-focus image fusion using the local spatial frequency</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 25th Chinese control and decision conference (CCDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3792" to="3796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong; China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5678" to="5687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment classification with aspect-specific graph convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong; China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4568" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep collaborative filtering with multi-aspect information in heterogeneous networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1413" to="1425" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging Graph to Improve Abstractive Multi-Document Summarization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6232" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach. CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting BERT for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</title>
		<meeting>the 5th Workshop on Noisy User-generated Text (W-NUT 2019)<address><addrLine>Hong Kong; China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantics-preserved data augmentation for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4417" to="4422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Technol. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Red Hook, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decoding-enhanced BERT with Disentangled Attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Deberta</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.03654" />
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adapt or get left behind: Domain adaptation through BERT language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4933" to="4941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
