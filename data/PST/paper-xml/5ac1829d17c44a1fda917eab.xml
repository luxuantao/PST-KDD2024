<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CREPE: A CONVOLUTIONAL REPRESENTATION FOR PITCH ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Laboratory</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Laboratory</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Urban Science and Progress</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Laboratory</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music and Audio Research Laboratory</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CREPE: A CONVOLUTIONAL REPRESENTATION FOR PITCH ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>pitch estimation, convolutional neural network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pretrained version of CREPE is made freely available as an open-source Python module for easy application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Estimating the fundamental frequency (f0) of a monophonic audio signal, also known as pitch tracking or pitch estimation, is a longstanding topic of research in audio signal processing. Pitch estimation plays an important role in music signal processing, where monophonic pitch tracking is used as a method to generate pitch annotations for multi-track datasets <ref type="bibr" target="#b1">[1]</ref> or as a core component of melody extraction systems <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Pitch estimation is also important for speech analysis, where prosodic aspects such as intonations may reflect various features of speech <ref type="bibr" target="#b4">[4]</ref>.</p><p>Pitch is defined as a subjective quality of perceived sounds and does not precisely correspond to the physical property of the fundamental frequency <ref type="bibr">[5]</ref>. However, apart from a few rare exceptions, pitch can be quantified using fundamental frequency, and thus they are often used interchangeably outside psychoacoustical studies. For convenience, we will also use the two terms interchangeably throughout this paper.</p><p>Computational methods for monotonic pitch estimation have been studied for more than a half-century <ref type="bibr" target="#b6">[6]</ref>, and many reliable methods have been proposed since. Earlier methods commonly employ a certain candidate-generating function, accompanied by preand post-processing stages to produce the pitch curve. Those functions include the cepstrum <ref type="bibr" target="#b6">[6]</ref>, the autocorrelation function (ACF) <ref type="bibr" target="#b7">[7]</ref>, the average magnitude difference function (AMDF) <ref type="bibr" target="#b8">[8]</ref>, the normalized cross-correlation function (NCCF) as proposed by RAPT <ref type="bibr" target="#b9">[9]</ref> and PRAAT <ref type="bibr" target="#b10">[10]</ref>, and the cumulative mean normalized difference function as proposed by YIN <ref type="bibr" target="#b11">[11]</ref>. More recent approaches include SWIPE <ref type="bibr" target="#b12">[12]</ref>, which performs template matching with the spectrum of a sawtooth waveform, and pYIN <ref type="bibr" target="#b13">[13]</ref>, a probabilistic variant of YIN that uses a Hidden Markov Model (HMM) to decode the most probable sequence of pitch values. According to a few comparative studies, the state of the art is achieved by YIN-based methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>, with pYIN being the best performing method to date <ref type="bibr" target="#b13">[13]</ref>.</p><p>A notable trend in the above methods is that the derivation of a better pitch detection system solely depends on cleverly devising a robust candidate-generating function and/or sophisticated post-processing steps, i.e. heuristics, and none of them are directly learned from data, except for manual hyperparameter tuning. This contrasts with many other problems in music information retrieval like chord ID <ref type="bibr" target="#b16">[16]</ref> and beat detection <ref type="bibr" target="#b17">[17]</ref>, where data-driven methods have been shown to consistently outperform heuristic approaches. One possible explanation for this is that since fundamental frequency is a low-level physical attribute of an audio signal which is directly related to its periodicity, in many cases heuristics for estimating this periodicity perform extremely well with accuracies (measured in raw pitch accuracy, defined later on) close to 100%, leading some to consider the task a solved problem. This, however, is not always the case, and even top performing algorithms like pYIN can still produce noisy results for challenging audio recordings such as a sound of uncommon instruments or a pitch curve that fluctuates very fast. This is particularly problematic for tasks that require a flawless f0 estimation, such as using the output of a pitch tracker to generate reference annotations for melody and multi-f0 estimation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>In this paper, we present a novel, data-driven method for monophonic pitch tracking based on a deep convolutional neural network operating on the time-domain signal. We show that our approach, CREPE (Convolutional Representation for Pitch Estimation), obtains state-of-the-art results, outperforming heuristic approaches such as pYIN and SWIPE while being more robust to noise too. We further show that CREPE is highly precise, maintaining over 90% raw pitch accuracy even for a strict evaluation threshold of just 10 cents. The Python implementation of our proposed approach, along with a pre-trained model of CREPE are made available online <ref type="foot" target="#foot_0">1</ref> for easy utilization and reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ARCHITECTURE</head><p>CREPE consists of a deep convolutional neural network which operates directly on the time-domain audio signal to produce a pitch estimate. A block diagram of the proposed architecture is provided in Figure <ref type="figure" target="#fig_0">1</ref>. The input is a 1024-sample excerpt from the time-domain audio signal, using a 16 kHz sampling rate. There are six convolutional layers that result in a 2048-dimensional latent representation, which is then connected densely to the output layer with sigmoid activations corresponding to a 360-dimensional output vector ?. From this, the resulting pitch estimate is calculated deterministically. The architecture of the CREPE pitch tracker. The six convolutional layers operate directly on the time-domain audio signal, producing an output vector that approximates a Gaussian curve as in 3, which is then used to derive the exact pitch estimate as in Equation <ref type="formula" target="#formula_1">2</ref>.</p><p>Each of the 360 nodes in the output layer corresponds to a specific pitch value, defined in cents. Cent is a unit representing musical intervals relative to a reference pitch f ref in Hz, defined as a function of frequency f in Hz:</p><formula xml:id="formula_0">?(f ) = 1200 ? log 2 f f ref ,<label>(1)</label></formula><p>where we use f ref = 10 Hz throughout our experiments. This unit provides a logarithmic pitch scale where 100 cents equal one semitone. The 360 pitch values are denoted as ? 1 , ? 2 , ? ? ? , ? 360 and are selected so that they cover six octaves with 20-cent intervals between C1 and B7, corresponding to 32.70 Hz and 1975.5 Hz. The resulting pitch estimate ? is the weighted average of the associated pitches ? i according to the output ?, which gives the frequency estimate in Hz:</p><formula xml:id="formula_1">? = 360 i=1 ?i? i 360 i=1 ?i , f = f ref ? 2 ?/1200 .<label>(2)</label></formula><p>The target outputs we use to train the model are 360-dimensional vectors, where each dimension represents a frequency bin covering 20 cents (the same as the model's output). The bin corresponding to the ground truth fundamental frequency is given a magnitude of one.</p><p>As in <ref type="bibr" target="#b19">[19]</ref>, in order to soften the penalty for near-correct predictions, the target is Gaussian-blurred in frequency such that the energy surrounding a ground truth frequency decays with a standard deviation of 25 cents:</p><formula xml:id="formula_2">yi = exp - (? i -? true ) 2 2 ? 25 2 ,<label>(3)</label></formula><p>This way, high activations in the last layer indicate that the input signal is likely to have a pitch that is close to the associated pitches of the nodes with high activations.</p><p>The network is trained to minimize the binary cross entropy between the target vector y and the predicted vector ?:</p><formula xml:id="formula_3">L(y, ?) = 360 i=1 (-yi log ?i -(1 -yi) log(1 -?i)) ,<label>(4)</label></formula><p>where both yi and ?i are real numbers between 0 and 1. This loss function is optimized using the ADAM optimizer <ref type="bibr" target="#b20">[20]</ref>, with the learning rate 0.0002. The best performing model is selected after training until the validation accuracy no longer improves for 32 epochs, where one epoch consists of 500 batches of 32 examples randomly selected from the training set. Each convolutional layer is preceded with batch normalization <ref type="bibr" target="#b21">[21]</ref> and followed by a dropout layer <ref type="bibr" target="#b22">[22]</ref> with the dropout probability 0.25. This architecture and the training procedures are implemented using Keras <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>In order to objectively evaluate CREPE and compare its performance to alternative algorithms, we require audio data with perfect ground truth annotations. This is especially important since the performance of the compared algorithms is already very high. In light of this, we cannot use a dataset such as MedleyDB <ref type="bibr" target="#b1">[1]</ref>, since its annotation process includes manual corrections which do not guarantee a 100% perfect match between the annotation and the audio, and it can be affected, to a degree, by human subjectivity. To guarantee a perfectly objective evaluation, we must use datasets of synthesized audio in which we have perfect control over the f0 of the resulting signal. We use two such datasets: the first, RWC-synth, contains 6.16 hours of audio synthesized from the RWC Music Database <ref type="bibr" target="#b24">[24]</ref> and is used to evaluate pYIN in <ref type="bibr" target="#b13">[13]</ref>. It is important to note that the signals in this dataset were synthesized using a fixed sum of a small number of sinusoids, meaning that the dataset is highly homogenous in timbre and represents an over-simplified scenario. To evaluate the algorithms under more realistic (but still controlled) conditions, the second dataset we use is a collection of 230 monophonic stems taken from MedleyDB and re-synthesized using the methodology presented in <ref type="bibr" target="#b18">[18]</ref>, which uses an analysis/synthesis approach to generate a synthesized track with a perfect f0 annotation that maintains the timbre and dynamics of the original track. This dataset consists of 230 tracks with 25 instruments, totaling 15.56 hours of audio, and henceforth referred to as MDB-stem-synth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methodology</head><p>We train the model using 5-fold cross-validation, using a 60/20/20 train, validation, and test split. For MDB-stem-synth, we use artistconditional folds, in order to avoid training and testing on the same artist which can result in artificially high performance due to artist or album effects <ref type="bibr" target="#b25">[25]</ref>. The evaluation of an algorithm's pitch estimation is measured in raw pitch accuracy (RPA) and raw chroma accuracy (RCA) with 50 cent thresholds <ref type="bibr" target="#b26">[26]</ref>. These metrics measure the proportion of frames in the output for which the output of the algorithm is within 50 cents (a quarter-tone) of the ground truth. We use the reference implementation provided in mir eval <ref type="bibr" target="#b27">[27]</ref> to compute the evaluation metrics. We compare CREPE against the current state of the art in monophonic pitch tracking, represented by the pYIN <ref type="bibr" target="#b13">[13]</ref> and SWIPE <ref type="bibr" target="#b12">[12]</ref> algorithms. To examine the noise robustness of each algorithm, we also evaluate their pitch tracking performance on degraded versions of MDB-stem-synth, using the Audio Degradation Toolbox (ADT) <ref type="bibr" target="#b28">[28]</ref>. We use four different noise sources provided by the ADT: pub, white, pink, and brown. The pub noise is an actual recording of the sound in a crowded pub, and the white noise is a random signal with a constant power spectral density over all frequencies. The pink and brown noise have the highest power spectral density in low frequencies, and the densities fall off at 10 dB and 20 dB per decade respectively. We used seven different signal-to-noise ratio (SNR) values: ?, 40, 30, 20, 10, 5, and 0 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Pitch Accuracy</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the pitch estimation performance tested on the two datasets. On the RWC-synth dataset, CREPE yields a close-toperfect performance where the error rate is lower than the baselines by more than an order of magnitude. While these high accuracy numbers are encouraging, those are achievable thanks to the highly homogeneous timbre of the dataset. In order to test the generalizability of the algorithms on a more timbrally diverse dataset, we evaluated the performance on the MDB-stem-synth dataset as well. It is notable that the degradation of performance from RWC-synth is more significant for the baseline algorithms, implying that CREPE is more robust to complex timbres compared to pYIN and SWIPE.</p><p>Finally, to see how the algorithms compare under scenarios where any deviation in the estimated pitch from the true value could be detrimental, in Table <ref type="table" target="#tab_1">2</ref> we report the RPA at lower evaluation tolerance thresholds of 10 and 25 cents as well as the RPA at the standard 50 cents threshold for reference. We see that as the threshold is decreased, the difference in performance becomes more accentuated, with CREPE outperforming by over 8 percentage points when the evaluation tolerance is lowered to 10 cents. This suggests that CREPE is especially preferable when even minor deviations from the true pitch should be avoided as best as possible. Obtaining highly precise pitch annotations is perceptually meaningful for transcription and analysis/resynthesis applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Noise Robustness</head><p>Noise robustness is key to many applications like speech analysis for mobile phones or smart speakers, or for live music performance. In Figure <ref type="figure">2</ref> we show how the pitch estimation performance is affected when an additive noise is present in the input signal. CREPE maintains the highest accuracy for all SNR levels for pub noise and white noise, and for all SNR levels except for the highest level of pink noise. Brown noise is the exception where pYIN's performance is almost unaffected by the noise. This can be attributed to the fact that brown noise has most of its energy at low frequencies, to which the YIN algorithm (on which pYIN is based) is particularly robust.</p><p>To summarize, we confirmed that CREPE performs better in all cases where the SNR is below 10 dB while the performance varies depending on the spectral properties of the noise when the noise level is higher, which indicates that our approach can be reliable under a reasonable amount of additive noise. CREPE is also more stable, exhibiting consistently lower variance in performance compared to the baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Model Analysis</head><p>To gain some insight into the CREPE model, in Figure <ref type="figure">3</ref> we visualize the spectra of the 1024 convolutional filters in the first layer of the neural network, with histograms of the ground-truth frequencies to the right of each plot. It is noticeable that the filters learned from the RWC-synth dataset have the spectral density concentrated between 600 Hz and 1500 Hz, while the ground-truth frequencies are mostly between 100 Hz and 600 Hz. This indicates that the first con-  volutional layer in our model learns to distinguish the frequencies of the overtones rather than the fundamental frequency. These filters focusing on overtones are also visible for MDB-stem-synth, where peak frequencies of the filters range well above the f0 distribution of the dataset, but in this case, the majority of the filters overlap with the ground-truth distribution, unlike RWC-synth. A possible explanation for this is that since the timbre in RWC-synth is fixed and identical for all tracks, the model is able to obtain a highly accurate estimate of the f0 by modeling its harmonics. Conversely, when the timbre is heterogeneous and more complex, as is the case for MDBstem-synth, the model cannot rely solely on the harmonic structure and requires filters that capture the f0 periodicity directly in addition to the harmonics. In both cases, this suggests that the neural network can adapt to the distribution of timbre and frequency in the dataset of interest, which in turn contributes to the higher performance of CREPE compared to the baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Performance by Instrument</head><p>The MDB-stem-synth dataset contains 230 tracks from 25 different instruments, where electric bass (58 tracks) and male singer (41 tracks) are the most common while there are instruments that occur in only one or two tracks. In Figure <ref type="figure" target="#fig_2">4</ref> we plot the performance of CREPE on each of the 230 tracks, with respect to the instrument of each track. It is notable that the model performs worse for the instruments with higher average frequencies, but the performance is also dependent on the timbre. CREPE performs particularly worse on the tracks with the dizi, a Chinese transverse flute, because the tracks came from the same artist, and they are all placed in the same split. This means that for the fold in which the dizi tracks are in the test set, the training and validation sets do not contain a single dizi track, and the model fails to generalize to this previously unseen timbre. There are 5 instruments (bass clarinet, bamboo flute, and the family of saxophones) that occur only once in the dataset, but their performance is decent, because their timbres do not deviate too far from other instruments in the dataset. For the flute and the violin, although there are many tracks with the same instrument in the training set, the performance is low when the sound in the tested tracks is too low (flute) or too high (violin) compared to other tracks of the same instruments. The low performance on the piccolo tracks is due to an error in the dataset where the annotation is inconsistent with the correct pitch range of the instrument. Unsurprisingly, the model performs well on test tracks whose timbre and frequency range are well-represented in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSIONS AND CONCLUSION</head><p>In this paper, we presented a novel data-driven method for monophonic pitch tracking based on a deep convolutional neural network operating on time-domain input, CREPE. We showed that CREPE obtains state-of-the-art results, outperforming pYIN and SWIPE on two datasets with homogeneous and heterogeneous timbre respectively. Furthermore, we showed that CREPE remains highly accurate even at a very strict evaluation threshold of just 10 cents. We also showed that in most cases CREPE is more robust to added noise.</p><p>Ideally, we want the model to be invariant to all transformations that do not affect pitch, such as changes due to distortion and reverberation. Some invariance can be induced by the architectural design of the model, such as the translation invariance induced by pooling layers in our model as well as in deep image classification models. However, it is not as straightforward to design the model architecture to specifically ignore other pitch-preserving transformations. While it is still an intriguing problem to build an architecture to achieve this, we could use data augmentation to generate transformed and degraded inputs that can effectively make the model learn the invariance. The robustness of the model could also be improved by applying pitch-shifts as data augmentation <ref type="bibr" target="#b29">[29]</ref> to cover a wider pitch range for every instrument. In addition to data augmentation, various sources of audio timbre can be obtained from software instruments; NSynth <ref type="bibr" target="#b30">[30]</ref> is an example where the training dataset is generated from the sound of software instruments.</p><p>Pitch values tend to be continuous over time, but CREPE estimates the pitch of every frame independently without using any temporal tracking, unlike pYIN which exploits this by using an HMM to enforce temporal smoothness. We can potentially improve the performance of CREPE even further by enforcing temporal smoothness. In the future, we plan to do this by means of adding recurrent architecture to our model, which could be trained jointly with the convolutional front-end in the form of a convolutional-recurrent neural network (CRNN).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: The architecture of the CREPE pitch tracker. The six convolutional layers operate directly on the time-domain audio signal, producing an output vector that approximates a Gaussian curve as in 3, which is then used to derive the exact pitch estimate as in Equation2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Pitch tracking performance when additive noise signals are present. The error bars are centered at the average raw pitch accuracies and span the first standard deviations. With brown noise being a notable exception, CREPE shows the highest noise robustness in general.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The raw pitch accuracy (RPA) of CREPE's predictions on each of the 230 tracks in MDB-stem-synth with respect to the instrument, sorted by the average frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average raw pitch/chroma accuracies and their standard deviations, tested with the 50 cents threshold.</figDesc><table><row><cell cols="2">Dataset Metric</cell><cell>CREPE</cell><cell>pYIN</cell><cell>SWIPE</cell></row><row><cell>RWC-</cell><cell>RPA</cell><cell cols="3">0.999?0.002 0.990?0.006 0.963?0.023</cell></row><row><cell>synth</cell><cell cols="4">RCA 0.999?0.002 0.990?0.006 0.966?0.020</cell></row><row><cell>MDB-stem-synth</cell><cell cols="4">RPA RCA 0.970?0.084 0.936?0.092 0.936?0.100 0.967?0.091 0.919?0.129 0.925?0.116</cell></row><row><cell cols="2">Dataset Threshold</cell><cell>CREPE</cell><cell>pYIN</cell><cell>SWIPE</cell></row><row><cell></cell><cell cols="4">50 cents 0.999?0.002 0.990?0.006 0.963?0.023</cell></row><row><cell>RWC-synth</cell><cell cols="4">25 cents 0.999?0.003 0.972?0.012 0.949?0.026</cell></row><row><cell></cell><cell cols="4">10 cents 0.995?0.004 0.908?0.032 0.833?0.055</cell></row><row><cell>MDB-</cell><cell cols="4">50 cents 0.967?0.091 0.919?0.129 0.925?0.116</cell></row><row><cell>stem-</cell><cell cols="4">25 cents 0.953?0.103 0.890?0.134 0.897?0.127</cell></row><row><cell>synth</cell><cell cols="4">10 cents 0.909?0.126 0.826?0.150 0.816?0.165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average raw pitch accuracies and their standard deviations, with different evaluation thresholds.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/marl/crepe</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Medleydb: A multitrack dataset for annotation-intensive mir research</title>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cannam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ISMIR Conference</title>
		<meeting>the 15th ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Melody extraction in symphonic classical music: a comparative study of mutual agreement between humans and algorithms</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>G?mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Interdisciplinary Musicology (CIM14)</title>
		<meeting>the 9th Conference on Interdisciplinary Musicology (CIM14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computer-aided melody note transcription using the tony software: Accuracy and efficiency</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cannam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Technologies for Music Notation and Representation</title>
		<meeting>the First International Conference on Technologies for Music Notation and Representation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Maria</forename><surname>Luisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zubizarreta</forename></persName>
		</author>
		<title level="m">Prosody, focus, and word order</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>William M Hartmann</surname></persName>
		</author>
		<title level="m">Signals, Sound, and Sensation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cepstrum pitch determination</title>
		<author>
			<persName><forename type="first">Noll</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="309" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time digital hardware pitch detector</title>
		<author>
			<persName><forename type="first">John</forename><surname>Dubnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Average magnitude difference function pitch extractor</title>
		<author>
			<persName><forename type="first">Myron</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Freudberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Manley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A robust algorithm for pitch tracking (rapt)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Talkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Coding and Synthesis</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Institute of Phonetic Sciences</title>
		<meeting>Institute of Phonetic Sciences</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yin, a fundamental frequency estimator for speech and music</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheveign?</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sawtooth waveform inspired pitch estimator for speech and music</title>
		<author>
			<persName><forename type="first">Arturo</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1638" to="1652" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">pYIN: A fundamental frequency estimator using probabilistic threshold distributions</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison of pitch trackers for real-time guitar effects</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dem</forename><surname>Knesebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Z?lzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Digital Audio Effects (DAFx)</title>
		<meeting>the International Conference on Digital Audio Effects (DAFx)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study of pitch extraction algorithms on a large variety of singing sounds</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Babacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Nicolas D'alessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="7815" to="7819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking automatic chord recognition with convolutional neural networks,&quot; in Machine Learning and Applications (ICMLA), 2012 11th International Conference on</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><surname>Bello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced beat tracking with context-aware neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Digital Audio Effects (DAFx)</title>
		<meeting>the International Conference on Digital Audio Effects (DAFx)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An analysis/synthesis framework for automatic f0 annotation of multitrack datasets</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Bonada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Jos? Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><forename type="middle">G?mez</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">P</forename><surname>Guti?rrez</surname></persName>
		</author>
		<author>
			<persName><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ISMIR Conference</title>
		<meeting>the 18th ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep salience representations for f0 tracking in polyphonic music</title>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ISMIR Conference</title>
		<meeting>the 18th ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Keras: The python deep learning library</title>
		<author>
			<persName><forename type="first">Chollet</forename><surname>Franc</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rwc music database: Popular, classical and jazz music databases</title>
		<author>
			<persName><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Hashiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuichi</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ISMIR Conference</title>
		<meeting>the 3rd ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="287" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classification accuracy is not enough</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="406" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Melody extraction from polyphonic music signals: Approaches, applications, and challenges</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?l</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="134" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">mir eval: A transparent implementation of common mir metrics</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel Pw</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ISMIR Conference</title>
		<meeting>the 15th ISMIR Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The audio degradation toolbox and its application to robustness evaluation</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ISMIR Conference</title>
		<meeting>the 14th ISMIR Conference<address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A software framework for musical data augmentation</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<publisher>ISMIR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cinjon</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01279</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
