<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep learning-based personality recognition from text posts of online social networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Di</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Army Engineering University</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifa</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Army Engineering University</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">Zheng</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Army Engineering University</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shize</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of North Electronic Equipment</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Liang Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of North Electronic Equipment</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Army Engineering University</orgName>
								<address>
									<postCode>210007</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">Xiaofeng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Electronic Engineering Institute</orgName>
								<address>
									<postCode>230037</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianshan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Hong</surname></persName>
						</author>
						<title level="a" type="main">Deep learning-based personality recognition from text posts of online social networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">647DEDC2944A8001ADBA05C2EC3458CA</idno>
					<idno type="DOI">10.1007/s10489-018-1212-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Personality recognition</term>
					<term>Deep learning</term>
					<term>Online social networks</term>
					<term>Big Five personality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personality is an important psychological construct accounting for individual differences in people. Computational personality recognition from online social networks is gaining increased research attention in recent years. However, the majority of existing methodologies mainly focused on human-designed shallow statistical features and didn't make full use of the rich semantic information in user-generated texts, while those texts are exactly the most direct way for people to translate their internal thoughts and emotions into a form that others can understand. This paper proposes a deep learningbased approach for personality recognition from text posts of online social network users. We first utilize a hierarchical deep neural network composed of our newly designed AttRCNN structure and a variant of the Inception structure to learn the deep semantic features of each user's text posts. Then we concatenate the deep semantic features with the statistical linguistic features obtained directly from the text posts, and feed them into traditional regression algorithms to predict the real-valued Big Five personality scores. Experimental results show that the deep semantic feature vectors learned from our proposed neural network are more effective than the other four kinds of non-trivial baseline features; the approach that utilizes the concatenation of our deep semantic features and the statistical linguistic features as the input of the gradient boosting regression algorithm achieves the lowest average prediction error among all the approaches tested by us.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Personality is a psychological construct aimed at explaining various human behaviors in terms of a few stable and measurable individual characteristics <ref type="bibr" target="#b0">[1]</ref>. It not only reflects an individual's consistent patterns of behavior, thought and interpersonal communication <ref type="bibr" target="#b1">[2]</ref>, but also influences important life aspects <ref type="bibr" target="#b2">[3]</ref>, including happiness, motivations, preferences, emotion, mental and physical health <ref type="bibr" target="#b3">[4]</ref>. The dominant paradigm for formal description of personality in psychology <ref type="bibr" target="#b0">[1]</ref> is the Big Five, also known as the Five Factor Model, which consists of five basic traits: openness Hefei University of Technology, Hefei 230009, China to experience (O), conscientiousness (C), extraversion (E), agreeableness (A), and neuroticism (N) <ref type="bibr" target="#b4">[5]</ref>. The study of personality is foundational to psychology, and personality recognition (PR) <ref type="bibr" target="#b5">[6]</ref> can benefit many other applications, such as social network analysis <ref type="bibr" target="#b6">[7]</ref>, recommendation systems <ref type="bibr" target="#b7">[8]</ref>, deception detection <ref type="bibr" target="#b8">[9]</ref>, authorship attribution <ref type="bibr" target="#b9">[10]</ref>, sentiment analysis/opinion mining <ref type="bibr" target="#b10">[11]</ref> and so on <ref type="bibr" target="#b11">[12]</ref>. However, the traditional methods of personality assessment through questionnaire investigation or expert interview are costly and less practical in cyber space <ref type="bibr" target="#b12">[13]</ref>.</p><p>Along with the popularization of social media in recent years, automatic personality recognition from online social network (OSN) is gaining increased research attention because of its potentials in many computational applications <ref type="bibr">[14]</ref>. There are rich self-disclosed personal information and emotional contents on social media, which have been proved to be highly correlated with user's personality traits <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Numerous studies have been done to explore optimal feature space and machine learning algorithms for recognizing individual's personality <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. However, the achievements of existing methodologies are not satisfactory. For one thing, the majority of existing approaches focused on human-designed shallow linguistic features (e.g. dictionary-based statistical features, n-grams, topics) extracted from the user-posted texts, or basic statistical features obtained from self-disclosed personal information on users' profiles. These practices did not seem to make full use of the rich user-generated text information on social networks, while those words and texts are exactly the most direct and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Therefore, it may contribute a lot to PR task if taking the contextual information and word orders into account to capture meaningful syntactic and semantic features when modeling user's text posts. For another, most previous approaches solved the PR problem as a classification task, which simply split subjects into two or three classes. This kind of outputs are not meaningful from a psychological point of view and is not useful enough for practical purposes, because it can hardly provide convincing arguments when emphasizing comparisons among individuals, which is exactly what humans loves to do <ref type="bibr" target="#b3">[4]</ref>. Therefore, PR models that output real-valued scores for personality traits would be more suitable and psychologically meaningful.</p><p>Besides, in recent years, deep learning based neural networks <ref type="bibr" target="#b28">[29]</ref> and distributed representation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> have been demonstrated to be powerful in sentence/document modeling and achieved remarkable performance in natural language processing (NLP) applications, such as text-based sentiment analysis <ref type="bibr" target="#b31">[32]</ref>, opinion mining <ref type="bibr" target="#b32">[33]</ref>, etc. It is worth noting that these NLP applications seem to be similar to our personality recognition task, since they both involve mining user attributes from texts, and feature representation of texts could be their common challenge. Given this, to improve the performance of personality recognition approaches, the most intuitive and straightforward idea is to introduce the powerful text modeling techniques that have been successfully applied in NLP domains into the field of personality recognition.</p><p>Considering the above-mentioned limitations of existing PR approaches and the potentials of deep learning and distributed representation, we propose a hierarchical deep neural network-based method to predict the big five personality scores of OSN users from their text posts. Specifically, we design an AttRCNN structure by introducing the attention mechanism <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> and batch normalization (BN) technique <ref type="bibr" target="#b35">[36]</ref> to the recurrent-conventional neural network (RCNN) <ref type="bibr" target="#b36">[37]</ref> to perform the vectorization of a single text post, and combine the AttRCNN structure with a variant of the convolutional neural network (CNN) based Inception structure <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref> through a hierarchical architecture to learn deep semantic representations of the aggregation of each user's text posts. Then we concatenate these deep semantic representations with pre-extracted statistical linguistic features vectors to construct the final feature space, and adopt the gradient boosting regression (GBR) <ref type="bibr" target="#b37">[38]</ref> algorithm to predict the Big Five personality scores for users. Our study is carried out based on the dataset from MyPersonality Project <ref type="bibr" target="#b11">[12]</ref>, which contains more than 11 million Facebook users' profile data and Big Five personality scores tested via online psychometric tests. Experimental results demonstrate that the deep semantic features learned from our neural network are more effective than the other four kinds of non-trivial baseline features, and our recognition approach surpasses all the others with the lowest prediction errors.</p><p>In summary, the contributions of our work are as follows:</p><p>( The rest of the paper is organized as follows. We discuss related work in Section 2. The details of our proposed neural network architecture and personality recognition approach are described in Section 3. Section 4 presents the experimental evaluation, followed by the conclusion and future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Computational personality recognition</head><p>Along with the explosive popularity of social media, various studies have been carried out for personality recognition from OSNs <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. In the year of 2011, Golbeck et al. <ref type="bibr" target="#b22">[23]</ref> extracted 77 features from 167 Facebook users' egocentric network, personal information, language usage, preferences and activities, and adopted M5' Rules and Gaussian Processes to predict their Big Five scores. A similar approach was also applied over 279 Twitter users by Golbeck et al. <ref type="bibr" target="#b23">[24]</ref> in the same year. Quercia et al. <ref type="bibr" target="#b24">[25]</ref> analyzed the relationship between personality and different types of Twitter users, and applied M5 algorithm to predict 335 users' Big Five traits simply based on three publicly available counts: follower, following and listed counts (i.e. the number of individuals that include the user in their reading list). Alam et al. <ref type="bibr" target="#b26">[27]</ref> followed bag-of-words approach and used tokens (unigrams) as feature input of different classification methods, namely Sequential Minimal Optimization (SMO) for SVM, Bayesian Logistic Regression (BLR) and Multinomial Naïve Bayes (MNB) sparse modeling, to predict Big Five traits based on the MyPersonality<ref type="foot" target="#foot_0">1</ref> corpus collected from Facebook by Celli et al. <ref type="bibr" target="#b11">[12]</ref>. Skowron et al. <ref type="bibr" target="#b27">[28]</ref> carried out PR research based on text, image, and users' meta data collected from two popular social networking sites, i.e., Twitter and Instagram, and found that such joint analysis could improve the prediction accuracy.</p><p>In addition to the utilization of English corpora, personality recognition research has also been carried out in Chinese language environments <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. Bai et al. <ref type="bibr" target="#b38">[39]</ref> analyzed the demographic information, usage statics and emotional states of 209 users on RenRen, a Chinese social networking platform, and applied C4.5 decision trees to classify users into three groups of low, middle or high scores. Li et al. <ref type="bibr" target="#b41">[42]</ref> carried out PR experiments over 547 Chinese active users of Sina Weibo. They extracted not only static features from users' profiles, privacy setting, selfexpression and interpersonal behaviors but also dynamic features consisting of micro-blogging updates, @ mentions, use of apps and recordable browsing. Peng et al. <ref type="bibr" target="#b42">[43]</ref> used a Chinese text segmentation tool named Jieba as the tokenizer to process the texts of 222 Chinese Facebook users, and adopted SVM to classify their personality traits. They reported that text segmentation and utilization of side information such as the number of friends could contribute to the performance improvement <ref type="bibr" target="#b42">[43]</ref>.</p><p>Overall, most existing approaches to personality recognition adopted classification methods to solve the PR problem, and previous efforts on feature space exploration mainly concentrated on statistics of users' online activities or profile information and human-designed shallow features of texts. They did not seem to make full use of the rich user-generated text information on OSNs. In this paper, we mainly focus on text information to predict user's Big Five personality traits.</p><p>As for personality recognition that concentrated on written texts, there have been various studies, too. Argamon et al. <ref type="bibr" target="#b43">[44]</ref> took word categories and relative frequency of function words as the input of Support Vector Machines (SVM) to discriminate between students at the opposite extremes of Extraversion and Neuroticism. Mairesse et al. <ref type="bibr" target="#b44">[45]</ref> studied the effectiveness of different sets of textual features extracted from psychologically oriented text analysis tools (e.g. LIWC<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b45">[46]</ref>) or psycholinguistic dictionary (e.g. MRC <ref type="bibr" target="#b47">[47]</ref>). In <ref type="bibr" target="#b48">[48]</ref> and <ref type="bibr" target="#b49">[49]</ref>, the frequencies of N-grams (i.e. N-long word sequences) were extracted as input features of Naïve Bayes classifiers and SVM to classify high and low scoring bloggers for Big Five traits. Recently, Majumder et al. <ref type="bibr" target="#b50">[50]</ref> adopted the Convolution Neural Networks (CNNs) to model document and extract deep semantic features to recognize personality from texts. The accuracy of this approach outperformed the baselines for all Big Five personality traits, making their work the state of the art.</p><p>Besides, to provide corpora and tools for standard evaluation of PR approaches, Workshop on Computational Personality Recognition (Shared Task) was organized in 2013 <ref type="bibr" target="#b11">[12]</ref> and 2014 <ref type="bibr" target="#b4">[5]</ref>, and another shared task of personality recognition was organized under the umbrella of Author Profiling task at PAN 2015 <ref type="bibr" target="#b51">[51]</ref>. Unfortunately, the corpora they provided are not large enough to carry out deep learning based study, so we didn't use them in our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep neural networks</head><p>In recent years, deep neural networks <ref type="bibr" target="#b28">[29]</ref> have achieved remarkable performance in sentence/document modeling, which is the foundational task in many natural language processing (NLP) applications such as text classification <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">52]</ref>, sentiment analysis <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b53">53]</ref>, etc. Among all these models, convolutional neural network (CNN) <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b55">55]</ref> and recurrent neural network (RNN) <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref> constructed on top of pre-trained word embeddings are two mainstream architectures, which adopt different ways of understanding natural language and both have their own strengths and weaknesses in text modeling. CNNs achieve good performance in extracting n-gram features at different positions of a sequence through convolutional filters, but they are not good at capturing long-term sequential correlations. RNNs can handle sequences of arbitrary input/output lengths and capture long-term dependencies, but RNNs are biased models, in which later words are more dominant than earlier words <ref type="bibr" target="#b58">[58]</ref>.</p><p>To better model sentences/documents, various modified architectures were proposed based on the basic CNN and RNN. The most relevant structures to our work is the Gated Recurrent Units (GRU) <ref type="bibr" target="#b33">[34]</ref>, recurrent convolutional neural network (RCNN) <ref type="bibr" target="#b36">[37]</ref> structure and the CNN-based Inception architectures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref>. The GRU is a variant of RNN, which uses a gating mechanism to track the state of sequences without using separate memory cells <ref type="bibr" target="#b52">[52]</ref>. RCNN is proposed by Lai et al. <ref type="bibr" target="#b36">[37]</ref> to deal with text classification task in 2015. They applied a bi-directional recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce much less noise compared to traditional window-based CNN neural networks. As for the Inception architecture <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref>, it was proposed by Szegedy et al.</p><p>to keep relatively low computational budget while increasing the depth and width of the CNN networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Language is the most common and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Words and language, then, are the very stuff of psychology and communication <ref type="bibr" target="#b14">[15]</ref>. Texts tend to reflect various aspects of the author's personality <ref type="bibr" target="#b50">[50]</ref>, and if we could model the OSN user's text posts better, the performance of PR approaches would improve a lot. Motivated by this intuition, we propose a hierarchical deep neural network based on our newly designed AttRCNN structure and a variant of CNNbased Inception structure, from which we extract the deep semantic vector representations of the aggregation of each user's text posts. Then we concatenate them with preextracted global statistical features to construct the input feature space for traditional regression algorithm to carry out final prediction of each user's real-valued Big Five personality scores.</p><p>Overall, our methodology includes four phases: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text posts preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Text tokenization</head><p>The target of this step is to tokenize each text-only status update into a sequence of tokens, which are separated by a space and roughly correspond to "words".</p><p>Considering that people with different personalities may have different habits of using punctuations, symbols, emoticons and capital letters, we choose to keep the original elements of each text post as much as possible and do not remove any words, letters or symbols in this step, so that we could extract relatively complete features (i.e. special linguistic statistics features) from the users' status updates. Specifically, we only add necessary spaces between different text elements (words, punctuations, emoticons, URLs, numbers, etc.) and delete unnecessary spaces within a single text element, e.g., emoticons like ∧ ∧ , ( * ∼ * ), to ensure that each text element could be treated as a single complete token, e.g. ∧ ∧ , (*∼*), etc., rather than a sequence of meaningless separated symbols or punctuations.</p><p>The outputs of this step are named as the tokenized text posts, which would not only be the processing objects of the following text unification step, but also the input of the special linguistic statistical feature extraction phase in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text unification</head><p>Users of online social networks tend to use informal language which may contain casually defined terms and punctuations, such as 'busyyyy', 'busyyyyyyyyy', '!!!', '!!!!!', etc. The number of this kind of usage may contribute to personality recognition, since people may emphasize their emotions by repeating letters or symbols. However, these kinds of raw texts may also directly affect the quality of the word embeddings trained based on them and further influence the performance of prediction models, because the same term with different number of tandem duplicated letters or punctuations would be considered as different "words" in the following training process of word embeddings. Thus, in this text unification step, we reduce the length of such tandem duplicated elements to make sure the length of such elements in a certain token is no more than 3, and further convert the text into lower case.</p><p>The outputs of this processing step are named as the unified text posts, which would be the input of dictionarybased linguistic feature extraction process (Section 3.2.2) and the word embeddings learning process (Section 3.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Statistical feature extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Extracting special linguistic statistics features</head><p>As mentioned above, people with different personalities may have different habits of using punctuations, symbols, emoticons and capital letters. Users of online social networks tend to use informal language which may contain casually defined terms and punctuations, and they may emphasize their emotions by using capital letters and emoticons, repeating letters or symbols in one term, and so on. Considering that the statistics of these special tokens in user-generated texts may contribute to personality recognition, we extract the following 5 special linguistic statistical features from the tokenized text posts: (1) rate of emoticons; (2) rate of tokens which have no less than 3 tandem duplicated letters or symbols; (3) rate of capital letters; (4) rate of capitalized words; (5) total number of text posts of each user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Extracting dictionary-based linguistic features</head><p>Correlations between lexical categories of user generated texts and user's personality have been widely proved by previous psychological studies <ref type="bibr">[14,</ref><ref type="bibr" target="#b44">45]</ref>. In this paper, we adopt the Linguistic Inquiry and Word Count (LIWC) tool, a popular text analysis software widely used in psychology studies <ref type="bibr" target="#b61">[61]</ref>, to extract the psychology dictionary-based linguistic features. For each user, we first aggregate all his/her unified text posts, preprocessed by text tokenization and unification in Section 3.1, to construct a unified document for him/her. Then, with the aid of LIWC tool, we extract 64 features from each Facebook user's unified document, which includes features related to standard counts (e.g., word count), psychological processes (e.g., the number of anger words such as hate and annoyed in the document), relativity (e.g., the number of verbs in the future tense), personal concerns (e.g., the number of words that refer to occupation such as job and majors), and linguistic dimensions (e.g., the number of swear words). For a complete overview of the features, please refer to literature <ref type="bibr" target="#b62">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep learning based text posts modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Unsupervised learning of word embeddings</head><p>Word embeddings are distributed representations of words that can capture meaningful syntactic and semantic regularities <ref type="bibr" target="#b36">[37]</ref>. The underlying idea of word embedding is the "distributional hypothesis" proposed by Zellig Harris <ref type="bibr" target="#b63">[63]</ref>, which can be summarized as "a word is characterized by the company it keeps" and "words that occur in the same contexts tend to purport similar meanings". Word embeddings are dense, low-dimensional, real-valued vectors, which can be generated using large unlabeled text data and are suitable as input for neural network models to alleviate the data sparsity problem. Previous studies have shown that word embeddings can boost the performance of deep learning methods in numerous natural language processing (NLP) tasks <ref type="bibr" target="#b64">[64]</ref> such as text classification and sentiment analysis.</p><p>In this work, to model OSN user's text posts better, we try to extract the deep semantic features from the texts based on deep neural network, where pre-training word embeddings (i.e., word-level semantic features) should be the first step. We adopt the CBOW model <ref type="bibr" target="#b65">[65]</ref>, state-ofthe-art in many NLP tasks, to pre-train word embeddings by the aid of word2vec, 3 a popular word embedding toolkit developed by Mikolov et al. which is used on corpus of 11 million Facebook users' text posts. Finally, each word in the 3 https://code.google.com/p/word2vec/ vocabulary is represented as a real-valued vector of fixed length (i.e., E dimensions).</p><p>After obtaining the word embeddings, we construct the embedding matrix M e , which would be used in the embedding layer of the neural network. For the unknown words that do not appear in the pre-trained word list, we assign all its E coordinates randomly with a uniform distribution in [-0.25, 0.25].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Supervised learning of deep semantic features</head><p>To learn the deep semantic features of each user's text posts, we propose a two-level hierarchical deep neural network model, code-named AttRCNN-CNNs, whose schematic is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. For the sake of clarity, we refer to each unified text post of a Facebook user as a sentence, and the aggregation of each user's unified text posts as a document.</p><p>The input of our neural network is a 3-dimensional realvalued array from R N×S×W , where N is the total number of documents, S is the number of sentences in each document and W is the number of words in each sentence.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, we first utilize the AttRCNN-based sentence encoder to learn semantic vector representations of sentences, and then we apply the CNN-based document encoder to extract document vectors from the aggregation of previously-learned sentence vectors. The detailed structures and components of these two encoders are presented in Section 3.3.2-(1) and - <ref type="bibr" target="#b1">(2)</ref>.</p><p>As for the output of our neural network, we apply a fully connected layer of size 1 (denoted as FC <ref type="bibr" target="#b0">(1)</ref> in Fig. <ref type="figure" target="#fig_1">1</ref>) on top of the document encoder, and further apply the function in (3.1) as the activation function (i.e., Custom Activation in Fig. <ref type="figure" target="#fig_1">1</ref>) to constrain the output in the <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> range, so that its output could be compared with the real personality score to compute the loss value and further help tune the parameters of the model.</p><formula xml:id="formula_0">Out (x) = 1 + 4 (1 + e -x ) (3.1)<label>(1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Sentence Vectorization with AttRCNN</head><p>Inspired by the popular RCNN model <ref type="bibr" target="#b36">[37]</ref> applied for text classification task, we design a new structure named AttRCNN for sentence vectorization by introducing the attention mechanism <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> and batch normalization technique <ref type="bibr" target="#b35">[36]</ref> into RCNN model to modify the way it captures the semantics of context. The intuition underlying our modification is that not all words that occur around a certain word w it contribute equally to the semantics of w it 's context, thus, we introduce the attention mechanism to help find the informative contextual words and learn the leftand right-side context vectors of w it better. Details are as follows. The fully-connected layer is donated as "FC (number of neurons)". The shape of each object is shown within angle brackets, and so is the output shape of each layer S1. Given a sentence s i with words w it (i ∈ [1, S] , t ∈ [1, W ]), we first use the embedding layer to convert each word index in the sentence into a pre-trained word vector v it of length E through the embedding matrix M e . S2. Then, as shown in Fig. <ref type="figure">2</ref>, we apply the GRU-based block G1 and G2 to the sentence (sequence of pretrained word vectors) to obtain all left-and rightside context vectors for each word, respectively. The operation processes in G1 and G2 are similar to each other, except the scan directions of their GRU <ref type="bibr" target="#b33">[34]</ref> layer: G1 executes a forward scan of sentence (i.e., reads the sentence s i from words w i1 to words w iW ), while G2 carries out a backward scan (i.e., reads s i from w iW to w i1 ). Given this, we only take G1 as an example to describe the detailed processes of these two blocks in the following part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2-1.</head><p>In block G1, we first apply a forward GRU of size 50 that read the sentence s i from words w i1 to words w iW to get annotations of words by summarizing previous contextual information. On top of the GRU, we apply batch normalization to help achieve stable distribution of activation values throughout training <ref type="bibr" target="#b35">[36]</ref>, and apply ReLU <ref type="bibr" target="#b66">[66]</ref> function as activation function to introduce nonlinearity. At this point, we get a 50-dimensional vector named u it as the annotation of word w it . S2-2. Considering that not all words that occur around a certain word w it contribute equally to the semantics of w it 's context, we introduce the attention mechanism to block G1 by applying an attention layer on top of G1's ReLU activation layer as shown in Fig. <ref type="figure">2</ref>. Note that the detailed components of this attention layer are not shown in Fig. <ref type="figure">2</ref>, and it actually includes a fully connected sublayer and a softmax function sublayer, whose detailed working process are as follows: Given a word w il that occurs to the left of the target word w it , we first feed its annotation u il (obtained through the lower three layers of G1) into a fully connected sublayer to get a hidden representation h il of u il . Then we calculate the similarity between h il and a word level contribution vector c w to measure the importance of word w il and Fig. <ref type="figure">2</ref> Structure of the AttRCNN-based Sentence Encoder. The GRU layer and dropout layer are donated as "GRU (number of neurons)-scan direction" and "Dropout (dropout rate)", respectively. The output shape of each layer is shown within angle brackets obtain a normalized contribution weight alpha α il through a softmax function. Then, we calculate the left-side context vector p l-it of word w it as a weighted sum of the word annotations u il based on all the contribution weights of w it 's left neighbors. Note that, the contribution vector c w is randomly initialized and jointly learned during the training process.</p><p>In sum, the attention layer is used to extract informative words that are important to the meaning of w it 's left context and aggregate the representation of those informative words to form the left-context vectors of w it . S2-3. Besides, to avoid overfitting in the training process, we further apply a dropout layer <ref type="bibr" target="#b67">[67]</ref> on top of the attention layer.</p><p>S3. Through the GRU-based block G1 and G2, we respectively get the 50-dimensional left-and right-side context vectors for all words in each sentence. After that, we concatenate those context vectors with the pre-trained word embeddings to represent each word as p l-it , v it , p r-it , where v it is the pre-trained word vectors, p l-it and p r-it are the left-and rightcontext vectors of word w it , respectively. At this point, each word is represented as a real-valued vector of length (50 + E + 50). S4. We further apply a fully connected layer with ReLU activation function on top of the concatenation layer to convert each word vector of length (50 + E + 50) to 100 dimensions, which is exactly the final distributed semantic vector of the word. Through the processing process from S1 To S4, each word w it of sentence s i has been transformed by the AttRCNN encoder from the original pretrained word embedding v it to the final distributed word representation v it that contains rich semantic information of w it 's left-and right-side neighbor words. Detailed transformation process is shown in Fig. <ref type="figure" target="#fig_2">3</ref>, where v il and v ir are the pre-trained word embeddings of w it 's left-and right-side neighbor words. Till now, each sentence is represented as a sequence of its words' distributed semantic vectors of length 100. S5. Then we continue to apply a max-pooling layer, which uses an element-wise max function to get the max element in each dimension of word representations across all the words in one sentence, so as to capture the most important latent semantic factors. Specifically, suppose v s i as the final representation of sentence s i , v it as the annotation of word w it that fed into the max-pooling layer, then the k-th element of s i 's vector v s i could be calculated following equation <ref type="bibr">(3.2)</ref>, where e k (v) means the k-th element of vector v.</p><formula xml:id="formula_1">e k (v s i ) = W max t=1 (e k (v it )) (3.2)</formula><p>Eventually, through the AttRCNN-based sentence encoder, all sentences in a document would be represented as real-valued vectors of length 100.</p><p>(2) Document Vectorization with CNNs Users of online social networks tend to publish text posts on social networks to express their feelings and share their daily life experiences. It's common and sometimes inevitable that there would be some emotional continuities and semantic correlations between each user's text posts that have been posted within a relatively short period of time (e.g., one day or a few days), while the semantic dependencies would be not so strong if the time intervals between the "Sent Time" of text posts are large (e.g., several weeks or months). Given this, we think the biased RNN-based architecture that are good at capturing longterm semantic dependencies may not be optimal to be used for document vectorization in our case, while the CNNs, that can efficiently capture local features in a parallel way and then assemble global representations through layer stacking, seem to be more appropriate. Besides, it's well known that increasing the depth and width of network can improve the performance of deep learning model, but the computing budget would increase at the same time.</p><p>To balance the contradiction between the performance and efficiency, Christian et al. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref> proposed the Inception architecture and achieved good performance in computer vision tasks.</p><p>Given the above consideration, we take a variant of the popular CNN-based Inception architecture as our document encoder to learn vector representation from the aggregation of each user's text post vectors extracted through the AttRCNN-based sentence encoder. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the document encoder of our model consists of four CNN-based blocks (i.e., C1 ∼ C4), one concatenation layer and two fully connected layers.</p><p>C1 and C4 each contain one convolutional layer that comprises 50 independent filters of size 3 and 5, respectively. C2 and C3 mainly contain two convolutional layers: The first layer comprises 100 convolutional filters of size 1 and size 5, respectively, on top of which we further introduce batch normalization mechanism and ReLU function; the second layer comprises 50 convolutional filters of size 3 and 5, respectively. The stride value of all the convolutional filters in each block is set to be 1, and in the convolution, the input is padded so that the output would be as long as the original input. These four blocks are applied in parallel to the 3-dimensional input of document from R N×S×100 , whose elements corresponding to the sentence vectors obtained from the AttRCNN sentence encoder, where N is the number of documents, S is the number of sentences in each document, and 100 is the length of the learned sentence vectors. Through each of these four blocks, the 100-dimensional sentence vector would be converted into 50 dimensions.</p><p>Then we concatenate these four kinds of 50-dimensional sentence vectors through a concatenation layer and flatten them to get the preliminary document annotations of length (S × 200). We further apply two stacked fully connected layers of size 100 and 50 on top of the flatten layer to convert the (S × 200)-dimensional document feature vectors to 50 dimensions. Besides, batch normalization mechanism, ReLU activation function and the dropout technique are still applied here as shown in Fig. <ref type="figure">4</ref>.</p><p>Fig. <ref type="figure">4</ref> The structure of the CNN-based Document Encoder. The convolutional layer, fully-connected layer and dropout layer are donated as "conv (number of filters, kernel size)", "FC (number of neurons)" and "Dropout (dropout rate)", respectively. The output shape of each layer is shown within angle brackets Note that the 50-dimensional output obtained through all the above-mentioned layers from the well-trained model is exactly the target document vectors that we aim to use as the input of the final personality prediction algorithm.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Model Training</head><p>We adopt mini-batch training approach with batch size B to train five different neural networks with the same architecture for five personality traits. Mean Square Error (MSE) is used as the objective function for training, which can be calculated using <ref type="bibr">(3.3)</ref>, where n denotes the number of unseen instances, s y j x i * the predicted value for trait y j ,, and s y j x i the observed one. The Adam <ref type="bibr" target="#b71">[71]</ref> optimizer is adopted to tune the network parameters to minimize the MSE. The training process runs for 30 iterations. We monitored the MSE value of the validation set after each epoch and employ early stopping mechanism (stop training when the monitored MSE has stopped reducing for 4 epochs) to avoid overfitting when training the model. Besides, if the monitored MSE value reduces after a certain epoch, we save current model and overwrite the previouslysaved one to guarantee that saved model would be the best model we ever obtained.</p><formula xml:id="formula_2">MSE = 1 n n i=1 s y j x i * -s y j x i 2 (3.3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction</head><p>After obtaining the neural network model, we extract the 50-dimensional output of the document encoder as the final deep semantic vector of each document. Then we concatenate it with the dictionary-based features (64 dimensions) and special linguistic features (5 dimensions) which have been pre-extracted directly from user's text posts to construct the ultimate feature space.</p><p>In order to get real-valued scores for Big Five personality traits, we adopt regression algorithm as the final prediction algorithm in our approach. Finally, the 119-dimensional features are fed into the popular gradient boosting regression algorithm to predict the Big Five personality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the effectiveness of the feature vectors extracted from our proposed neural network, we carried out experiments based on the dataset collected as part of the MyPersonality project <ref type="bibr" target="#b11">[12]</ref>, a popular Facebook application allowing users to test their personality via online psychometric tests and donate their scores and Facebook profile data to research. We concentrated on the users whose default language is English and the Big Five personality trait scores are available. The final dataset we utilized involves 115,864 Facebook users, 11,494,862 text posts and 3,055,272 unique word tokens. The average number of text posts per user is 142; the average of the maximum text post length per user is 70. The standard deviation of the text posts number per user is 162.51; the standard deviation of the maximum text posts length per user is 31.22.</p><p>According to the statistics of the dataset, we found that the posting habits of different users are quite different. Thus, when constructing the 3-dimensional input array for our neural network, we set the length of each document S as the average number of sentences across all documents, and the length of each sentence W as the average number of words across all sentences.</p><p>To guarantee that all documents/sentences contain the same number of sentences/words, we padded the shorter documents/sentences with dummy sentences/words and truncate the excess part of the longer documents/sentences. The number of documents N equals to the total number of users. Eventually, we got a 3-dimensional input array of shape(115864 × 140 × 70). Note that, we didn't set S to be 142 but rounded the number down by changing the ones digit to zero and set S to be 140 for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline feature sets</head><p>To evaluate the effectiveness of the 50-dimensional feature vectors obtained from our proposed deep neural network, which are named as ARCC, we extracted other four kinds of non-trivial feature set to construct the baseline feature spaces in our experiments.</p><p>The first one, code-named Cnn, is the semantic feature set extracted from a CNN-based deep neural network as presented in literature <ref type="bibr" target="#b50">[50]</ref>. This is the only work that introduced deep learning technique into personality recognition from texts before ours, and its outperformance over traditional approaches making it the state of the art. Utilizing the 3-dimensional input array with shape (115864 × 140 × 70), we built five neural networks with the same structure of that proposed in literature <ref type="bibr" target="#b50">[50]</ref> and extracted the document-level features from the model for the prediction of Big Five personality traits.</p><p>The second one, code-named RCC, is the deep semantic features extracted from the RCNN-CNNs architecture of hierarchical neural network, which adopts the original RCNN structure as the sentence encoder but the same CNNbased document encoder with our proposed neural network. This baseline feature set is mainly used to evaluate the effectiveness of our modification to RCNN.</p><p>The third one, code-named D2V, is another kind of document-level semantic feature vectors extracted through the unsupervised Doc2Vec algorithm <ref type="bibr" target="#b68">[68]</ref>, an extension of the popular word2vec algorithm that could learn continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents. Taking the aggregation of all the unified text posts in the Facebook dataset as input, we extracted this kind of doc2vec feature vectors of length 50 using the Doc2Vec module provided by Gensim. <ref type="foot" target="#foot_2">4</ref>The fourth one, code-named SL, is the 69-dimension statistical linguistic feature set extracted following the methods presented in Section 3.2. It consists of the dictionary-based features (64 dimensions) and the special linguistic features (5 dimensions) extracted from the unified text posts and the tokenized text posts, respectively.</p><p>Overall, the feature sets can be divided into two categories: ARCC, RCC and Cnn are all extracted from deep neural networks through supervised learning, while D2V and SL are both obtained from unlabeled corpus. To comprehensively evaluate these feature sets and find the optimal feature space for personality recognition, we adopted not only the above-mentioned single feature set but also their combinations as the input of the prediction algorithms. In total, 15 kinds of feature space were evaluated in our experiments, and the detailed list could be found in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Regression algorithms</head><p>We experimented with four regression settings in total, including multi-layer perceptron (MLP) with one hidden layer, which is trained together with the hierarchical neural network as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, and other three commonly-used regression algorithms: support vector regression (SVR) <ref type="bibr" target="#b69">[69]</ref>, gradient boosting regression (GBR) and random forest (RF) <ref type="bibr" target="#b70">[70]</ref>, which are trained separately using the preextracted feature sets or the combination of them as their input. These regression algorithms were adopted not only to test the predictive ability of different algorithms, but also to figure out whether the performance of each feature set are consistent or not when fed into different prediction algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation metrics</head><p>In our experiments, the predictive ability of the personality recognition approaches was evaluated by MAE (Mean Absolute Error), a frequently used measure of differences between the predicted score and the observed score tested by Big Five Inventory in APR research. It can be calculated using (4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methodology</head><p>Following the methods presented in Section 3, we built five different neural networks with the same architecture of our proposed model for five personality traits. From each well-trained neural network model, we extracted the 50dimensional outputs of the document encoder as the final deep semantic vectors of documents. The baseline feature vectors were also extracted following the above presented methods. Then, we separately fed the obtained feature sets or the combinations of them to the regression algorithms to build different prediction models for Big Five traits using scikit-learn,<ref type="foot" target="#foot_3">5</ref> a powerful Python module for machine learning.</p><p>We carried out model training and testing experiments with 5-fold cross-validation, and the parameter selection process was nested into the 5-fold cross-validation. In details, we split the whole dataset into 5 equal chunks randomly. Each time three chunks were used as training set, one was used as validation set and the rest one was used as test set. Each model was trained with different parameter settings on the training set, validated on the validation set, and tested on the testing set. The average MAE of each model over a 5-fold cross-validation was recorded, and the parameter setting with the best average performance was selected.</p><p>In the case of the neural network models' hyperparameters, we tried different vector size (namely 50, 100, 150, 200) of the pre-trained word embeddings, dropout rate (varied from 0.1 to 0.9 with step size of 0.1), and batch size (namely 8, 16, 32, 64). The final selected hyper-parameters are as follows: the vector size of the pre-trained word embeddings E was 100; the dropout rate of the dropout layer dr was set to be 0.4; the batch size B was 16. The parameters of the Adam optimizer were set following the original paper <ref type="bibr" target="#b71">[71]</ref>. The other hyper-parameters, such as the layer size, are shown in Figs. <ref type="figure" target="#fig_1">1, 2</ref> and<ref type="figure" target="#fig_2">3</ref>.</p><p>As for the parameters of the regression algorithms, we tried different kernels (namely radial, linear and polynomial) for SVR, different number of estimators for GBR and RF. For each kind of these regressors, the best results were respectively achieved by the SVR learner with the radial kernel, the GBR learner with 100 estimators and the RF learner with 100 trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and analysis</head><p>The average testing results over 5-fold cross-validation achieved by different approaches with their best parameter settings are presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Without regard for the MAE differences between approaches with different prediction algorithms, we may find that any personality recognition approach that took the ARCC feature vectors into its input feature set achieves lower prediction errors than the other approaches that didn't involve the ARCC features. In other words, the ARCC feature vectors extracted from our proposed AttRCNN-CNNs neural networks are the most effective ones compared to the other four kinds of baseline features, the features extracted from RCNN-CNNs neural networks that code-named RCC, the features extracted from the CNN baseline neural networks code-named Cnn, the features learned from the Doc2Vec algorithm codenamed D2V and the statistical linguistic features codenamed SL. Ranking after the above mentioned approaches that utilized the ARCC features, the RCC feature-involved approaches come off the second most effective features with overall average MAEs no higher than 0.45888. The outperformance of both the ARCC and the RCC feature set over the Cnn ones (which were also extracted from deep neural network models) demonstrates the advantages of both the overall hierarchical architecture and the document encoder structure of our proposed neural network for text posts modeling in PR. Furthermore, since the ARCC features surpass the RCC ones, we could conclude that our modifications, including introductions of the attention mechanism and batch normalization technique, to RCNN structure laid the foundation for the effectiveness of our methodology.</p><p>The bottom three rows in Table <ref type="table" target="#tab_1">1</ref> show the performance of the end-to-end approaches that utilized a fully-connected layer on top of the document encoder to directly output the predicted personality scores. With average MAEs no lower than 0.53162, these three approaches underperformed all the 45 two-phase approaches whose performance are shown in the top portion of Table <ref type="table" target="#tab_1">1</ref>. It implies that deep neural networks are good at feature extraction; applying separate prediction algorithm rather than fully-connected layers trained together with neural networks could improve the performance of PR approaches. Despite the overall underperformance of the end-to-end approaches, the ARCC feature set learned from our AttRCNN-CNNs model surpass both the RCC and the Cnn ones obtained from the baseline models in all dimensions of the Big Five personality traits, demonstrating the advantages of our proposed text modeling neural network over other two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>Computational personality recognition is an emerging research field that consists of the automatic inference of users' personality traits from publicly available information on online social platforms. In this paper, we present a two-level hierarchical neural network based on the newly designed AttRCNN structure and a variant of the CNNbased Inception structure to learn the deep semantic representations of online social network users' text posts. Experimental evaluation shows that taking these kinds of deep semantic features as input of traditional regression algorithms contribute a lot to the performance improvement of Big Five personality recognition approaches. In future work, we will utilize these kind of deep semantic features as the input of some special designed regression algorithms so as to further improve the prediction accuracy of the personality recognition approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(1) Text post preprocessing phase is to tokenize and unify users' text posts; (2) Statistical feature extraction phase is to extract global statistical features by directly counting the frequency of target text elements in each user's text posts; (3) In deep learning-based text posts modeling phase, word embeddings are firstly trained through unsupervised learning. Then our newly designed deep neural network for text posts modeling are built utilizing the Facebook corpus; (4) Prediction phase is to predict real-valued Big Five personality scores with traditional regression algorithm based on the deep semantic features extracted from the neural network and the preextracted global statistical features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Schematic of our proposed hierarchical neural network for text posts modeling.The fully-connected layer is donated as "FC (number of neurons)". The shape of each object is shown within angle brackets, and so is the output shape of each layer</figDesc><graphic coords="6,178.23,61.07,365.80,340.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig.<ref type="bibr" target="#b2">3</ref> The process of obtaining the distributed semantic representation of word w it by AttRCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,178.23,61.67,365.80,331.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,178.23,61.43,365.80,388.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1) We design a new AttRCNN structure of neural network to learn the distributed semantic representation of OSN user's single text post. (2) By combining the AttRCNN structure with a variant of the CNN-based Inception structure we propose a new hierarchical deep neural network named AttRCNN-CNNs to learn deep semantic representations of the aggregation of each OSN user's text posts.</figDesc><table /><note><p><p><ref type="bibr" target="#b2">(3)</ref> </p>Based on the distributed semantic representations of user's text posts learned from the deep neural networks, we propose a personality recognition methodology, successfully applying the deep learning techniques on text corpora of OSN users for personality tasks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>(continued) N refer to the five dimensions of Big Five traits: Openness to experience, Conscientiousness, Extraversion, Agreeableness, Neuroticism, respectively. In each column, the lowest MAE among all the approaches is typeset in bold, and the lowest MAEs among approaches with the same prediction algorithm are marked by *</figDesc><table><row><cell>Algorithm</cell><cell>Feature set</cell><cell>O</cell><cell>C</cell><cell>E</cell><cell>A</cell><cell>N</cell><cell>Average</cell></row><row><cell>MLP</cell><cell>ARCC</cell><cell>0.4445*</cell><cell>0.5387*</cell><cell>0.5851*</cell><cell>0.5006*</cell><cell>0.4892*</cell><cell>0.53162*</cell></row><row><cell></cell><cell>RCC</cell><cell>0.5021</cell><cell>0.5810</cell><cell>0.6437</cell><cell>0.5312</cell><cell>0.6329</cell><cell>0.57818</cell></row><row><cell></cell><cell>Cnn</cell><cell>0.5439</cell><cell>0.5970</cell><cell>0.6503</cell><cell>0.5556</cell><cell>0.6363</cell><cell>0.59662</cell></row><row><cell>O, C, E, A,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://mypersonality.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.liwc.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://radimrehurek.com/gensim/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://scikit-learn.org/stable/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was support by the scientific research funds of PLA (Grant No. AWS13J003). The authors would like to thank the anonymous reviewers for their careful review and constructive comments. Thanks also to David Stillwell, Michal Kosinski and the myPersonality project for their efforts on collecting the Facebook dataset.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of personality computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mohammadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2014.2330816</idno>
		<ptr target="https://doi.org/10.1109/taffc.2014.2330816" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="291" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personality</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Funder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Psychol</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="197" to="221" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Personality: a psychological interpretation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Allport</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1937">1937</date>
			<publisher>Henry Holt</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Personality recognition on social media with label distribution learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="13478" to="13488" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The workshop on computational personality recognition 2014</title>
		<author>
			<persName><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Biel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM conference on multimedia</title>
		<imprint>
			<biblScope unit="page" from="1245" to="1246" />
			<date type="published" when="2014-11-03">2014. November 3-7, 2014</date>
			<publisher>ACM</publisher>
			<pubPlace>Orlando</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using linguistic features to estimate suicide probability of Chinese microblog users</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human centered computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The role of emotional stability in Twitter conversations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on semantic analysis in social media</title>
		<meeting><address><addrLine>Avignon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04-27">2012. 23-27 April 2012</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparative evaluation of personality estimation algorithms for the twin recommender system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roshchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cardiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3th international workshop on search and mining usergenerated contents</title>
		<meeting><address><addrLine>Glasgow</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-10-28">2011. October 28 2011</date>
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personality factors in human deception detection: comparing human to machine performance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Enos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Cautin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2006 and 9th international conference on spoken language processing</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09">2006. September</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personae: a corpus for author and personality prediction from text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luyckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th international conference on language resources and evaluation</title>
		<meeting><address><addrLine>Marrakech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05-30">2008. 28-30 May 2008</date>
			<biblScope unit="page" from="2981" to="2987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computing political preference among twitter followers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1105" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Workshop on computational personality recognition: shared task</title>
		<author>
			<persName><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th international AAAI conference on weblogs and social media</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07-08">2013. Jul 8-11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting personality on social media with semi-supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM international joint conferences on web intelligence and intelligent agent technologies</title>
		<meeting><address><addrLine>Warsaw</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-08-11">2014. August 11-14, 2014</date>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="158" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational personality recognition in social media</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sitaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sushmita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1007/s11257-016-9171-0</idno>
		<ptr target="https://doi.org/10.1007/s11257-016-9171-0" />
	</analytic>
	<monogr>
		<title level="j">User Model User-Adap Int</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="109" to="142" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Personality, gender, and age in the language of social media: the open-vocabulary approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dziurzynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ramones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Seligman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">73791</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online social networks and insights into marketing communications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Polonsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Internet Commer</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="55" to="72" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The impact of the Big Five personality traits on the acceptance of social networking website</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kluemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Americas Conference on Information Systems</title>
		<meeting><address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personality traits, usage patterns and information disclosure in online communities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrammel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tscheligi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd annual conference on human computer interaction</title>
		<meeting><address><addrLine>Cambridge, UK; Swindon</addrLine></address></meeting>
		<imprint>
			<publisher>BCS Learning &amp; Development Ltd</publisher>
			<date type="published" when="2009-05">2009. 2009. September 01-05, 2009</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emerging late adolescent friendship networks and Big Five personality traits: a social network approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Selfhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meeus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="509" to="538" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-report versus web-log: which one is better to predict personality of website users?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Cyber Behav Psychol Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Private traits and attributes are predictable from digital records of human behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="5802" to="5805" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manifestations of personality in online social networks: self-reported Facebook-related behaviors and observable profile information</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaddis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cyberpsychol Behav Soc Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="483" to="488" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting personality with social media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;11 extended abstracts on human factors in computing systems</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-05-07">2011. May 7-12, 2011</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting personality from Twitter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Edmondson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-10">2011. Oct. 2011</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Our Twitter profiles, our selves: predicting personality with twitter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on privacy, security, risk and trust and 2011 IEEE international conference on social computing</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-10-09">2011. October 9-11, 2011</date>
			<biblScope unit="page" from="180" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognising personality traits using Facebook status updates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zoghbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on computational personality recognition at the 7th international AAAI conference on weblogs and social media</title>
		<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2013-07-08">2013. July 8-11 2013</date>
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personality traits recognition on social network-Facebook</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Stepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on weblogs and social media</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>AI Access Foundation</publisher>
			<date type="published" when="2013-07-11">2013. July 11 2013</date>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fusing social media cues: personality prediction from Twitter and Instagram</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tkalčič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th international conference companion on world Wide Web</title>
		<imprint>
			<publisher>Montreal. ACM</publisher>
			<date type="published" when="2016-04-11">2016. April 11-15 2016</date>
			<biblScope unit="page" from="107" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised text normalization using distributed representations of words and phrases</title>
		<author>
			<persName><forename type="first">Vkr</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS@ HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="8" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<idno>arXiv:161008815</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:14090473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Big-five personality prediction based on user behaviors at social network sites</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<idno>arXiv:12044809</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting big five personality traits of Microblog users</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/wi-iat.2013.70</idno>
		<ptr target="https://doi.org/10.1109/1109/wi-iat.2013.70" />
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conscientiousness measurement from Weibo&apos;s public information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40705-5_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40705-56" />
	</analytic>
	<monogr>
		<title level="m">Partially supervised learning: second IAPR international workshop, PSL 2013</title>
		<editor>
			<persName><forename type="first">Z-H</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</editor>
		<meeting><address><addrLine>Nanjing, China; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-05-13">2013. May 13-14, 2013</date>
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting active users&apos; personality based on micro-blogging behaviors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">84997</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predicting personality traits of Chinese users based on Facebook wall posts</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/WOCC.2015.7346106</idno>
		<ptr target="https://doi.org/10.1109/WOCC.2015.7346106" />
	</analytic>
	<monogr>
		<title level="m">24th wireless and optical communication conference</title>
		<meeting><address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-10">2015. Oct. 2015</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pennebaker JW Lexical predictors of personality type</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shlomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhawle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint annual meeting of the interface and the classification society of North America</title>
		<meeting><address><addrLine>St. Louis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">2005. June</date>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using linguistic cues for the automatic recognition of personality in conversation and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="457" to="500" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<title level="m">Linguistic inquiry and word count: LIWC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Austin</forename><surname>Net</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The MRC psycholinguistic database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Coltheart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q J Exp Psychol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Oberlander</surname></persName>
		</author>
		<title level="m">Nowson S Whose thumb is it anyway? Classifying author personality from weblog text</title>
		<meeting><address><addrLine>Sydney, Australia; Stroudsburg</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07-17">2006. July 17-18, 2006</date>
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
	<note>COLING/ACL on main conference poster sessions</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Identifying more bloggers: towards large scale personality classification of personal weblogs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on weblogs and social media</title>
		<meeting><address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007-03-26">2007. March 26-28 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poria</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Overview of the 3rd author profiling task at PAN</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Conference and labs of the evaluation forum</title>
		<meeting><address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-08">2015. 2015. September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: a deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lau</surname></persName>
		</author>
		<idno>arXiv:151108630</idno>
		<title level="m">A C-LSTM neural network for text classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Inceptionv4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Linguistic styles: language use as an individual difference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Pers Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1296</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The psychological meaning of words: LIWC and computerized text analysis methods</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Lang Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="54" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast semantic extraction using a novel neural network architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual meeting-association for computational linguistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>arXiv:13013781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on international conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">1188</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
