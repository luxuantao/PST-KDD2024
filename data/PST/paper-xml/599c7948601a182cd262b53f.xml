<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving (most) of a set of quadratic equalities: composite optimization for robust phase retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-14">14 September 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Feng</forename><surname>Ruan</surname></persName>
							<email>fengruan@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Solving (most) of a set of quadratic equalities: composite optimization for robust phase retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-14">14 September 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">C0BC5CF803F9B674E00D20B546A545D4</idno>
					<idno type="DOI">10.1093/imaiai/iay015</idno>
					<note type="submission">Received on 16 August 2017; revised on 17 April 2018; accepted on 4 July 2018]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Composite optimization</term>
					<term>phase retrieval</term>
					<term>sharp growth</term>
					<term>proximal methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop procedures, based on minimization of the composition f (x) = h(c(x)) of a convex function h and smooth function c, for solving random collections of quadratic equalities, applying our methodology to phase retrieval problems. We show that the prox-linear algorithm we develop can solve phase retrieval problems-even with adversarially faulty measurements-with high probability as soon as the number of measurements m is a constant factor larger than the dimension n of the signal to be recovered. The algorithm requires essentially no tuning-it consists of solving a sequence of convex problemsand it is implementable without any particular assumptions on the measurements taken. We provide substantial experiments investigating our methods, indicating the practical effectiveness of the procedures and showing that they succeed with high probability as soon as m/n ≥ 2 when the signal is real-valued.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We wish to solve the following problem: we have a set of m vectors a i ∈ C n and non-negative scalars b i ∈ R + , i = 1, . . . , m, and wish to find a vector x ∈ C n such that b i = | a i , x | 2 for most i ∈ {1, . . . , m}.</p><p>(</p><p>As stated, this is a combinatorial problem that is, in the worst case, NP-hard <ref type="bibr" target="#b19">[20]</ref>. Yet it naturally arises in a number of real-world situations, including phase retrieval <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, in which one receives measurements of the form</p><formula xml:id="formula_1">b i = | a i , x | 2</formula><p>for known measurement vectors a i ∈ C n , while x ∈ C n is unknown. The problem in phase retrieval arises due to limitations of optical sensors, where one illuminates an object x , which yields diffraction pattern Ax , but sensors may measure only the amplitudes b = |Ax | 2 , where | • | 2 denotes the elementwise squared magnitude <ref type="bibr" target="#b37">[38]</ref>. In the case in which some measurements may be corrupted, the problem is even more challenging. An alternative objective for the problem (1) is an exact penalty formulation <ref type="bibr" target="#b25">[26]</ref>, which replaces the equality constraint b i = | a i , x | 2 with a non-differentiable cost measuring the error b i -| a i , x | 2 , yielding the formulation minimize</p><formula xml:id="formula_2">x f (x) := 1 m m i=1 | a i , x | 2 -b i = 1 m |Ax| 2 -b 1 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>This objective is a natural replacement of the equality constrained problem <ref type="bibr" target="#b0">(1)</ref>, and the 1 -loss handles gross errors in the measurements b i in a relatively benign way (as is well known in the statistics and optimization literature on 1 -based losses and median estimators). Moreover, in the case when b i = | a i , x | 2 for all i, it is clear that taking ι = √ -1 to be the imaginary unit, then the set {e ιθ x | θ ∈ [0, 2π )} globally minimizes f (x), though it is only possible to recover x up to its phase (or sign flip in the real case). Candès et al. <ref type="bibr" target="#b8">[9]</ref> and Eldar and Mendelson <ref type="bibr" target="#b18">[19]</ref>, as well as results we discuss later in the paper, show (roughly) that f (x) stably identifies x , in that f (x) grows very quickly as dist(x, X ) = inf{ xx 2 | x ∈ X }, where X denotes the global minimum of f , grows. The objective is, unfortunately, non-smooth, non-convex-not even locally convex near x , as is clear in the special case when x ∈ R and f (x) = |x 2 -1|, so that a local analysis based on convexity is impossible-and at least f (x) a priori seems difficult to minimize.</p><p>Nonetheless, the objective (2) enjoys a number of structural properties that, as we explore below, make solving problem (1) tractable as long as the measurement vectors a i are sufficiently random. In particular, we can write f as the composition f (x) = h(c(x)) of a convex function h and smooth function c, a structure known in the optimization literature to be amenable to efficient algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. This compositional structure lends itself nicely to the prox-linear algorithm, a variant of the Gauss-Newton procedure, which we describe briefly here for the real case. The composite optimization problem, which Fletcher &amp; Watson <ref type="bibr" target="#b22">(23)</ref> originally develop (see also <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>) and a number of researchers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> have studied further, is to minimize</p><formula xml:id="formula_4">minimize x f (x) := h(c(x)) subject to x ∈ X,<label>( 3 )</label></formula><p>where the function h : R m → R is convex, c : R n → R m is smooth and X is a convex set. Extended to the complex case, this general form encompasses our objective <ref type="bibr" target="#b1">(2)</ref>, where we take h(z) = 1 m z 1 and</p><formula xml:id="formula_5">c(x) = [| a i , x | 2 -b i ] m i=1 .</formula><p>Using the common idea of most optimization schemes-trust region, gradient descent, Newton's method-to build a simpler to optimize local model of the objective and repeatedly minimize this model, we can replace h(c(x)) in problem (3) by linearizing only c. This immediately gives a convex surrogate and leads to the prox-linear algorithm developed by <ref type="bibr">Burke &amp; Ferris [7,</ref><ref type="bibr" target="#b7">8]</ref>, among others <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Fixing x ∈ R n , for any y ∈ R n we define the local 'linearization' of f at x by f x (y) := h c(x) + ∇c(x) T (yx) , <ref type="bibr" target="#b3">( 4 )</ref> where ∇c(x) ∈ R n×m denotes the Jacobian transpose of c at x. This function is evidently convex in y and the prox-linear algorithm proceeds iteratively x 1 , x 2 , . . . by minimizing the regularized models</p><formula xml:id="formula_6">x k+1 = argmin x∈X f x k (x) + 1 2α k x -x k 2 2 ,<label>( 5 )</label></formula><p>where α k &gt; 0 is a stepsize. If h is L-Lipschitz and ∇c is β-Lipschitz, then choosing any stepsize α ≤ 1 βL guarantees that the method ( <ref type="formula" target="#formula_6">5</ref>) is a descent method and finds approximate stationary points of the problem (3) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. The case in which c : C n → R m requires a bit more elaboration based on the Wirtinger calculus, which we address later.</p><p>We briefly summarize our main contribution as follows. Broadly, this work provides a general method for robust non-convex modeling, focusing carefully on the problem (2); our work carries on a line of work identifying statistical scenarios that nominally yield non-convex optimization problems, yet admit computationally efficient estimation and optimization procedures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. In this direction, our work develops analytic and statistical tools to analyze a collection of optimization and modeling approaches beyond gradient-based (and Riemannian gradient-based) procedures, using both non-smooth and non-convex models while leveraging statistical structure. More precisely, we show how to apply prox-linear method <ref type="bibr" target="#b4">(5)</ref> to any measurement matrix A with no tuning parameters, except that the stepsize satisfies α ≤ 1 m A H A op -1 . Each iteration requires solving a QP in n variables, which is efficiently solvable using standard convex programming approaches. We show that-with extremely high probability under appropriate random measurement models-our prox-linear method exhibits local quadratic convergence to the signal as soon as the number of measurements m/n is greater than some numerical constant, meaning we must solve only log 2 log 2 1 such convex problems to find an estimate x of x such that dist(x, X ) ≤ . In practice, this is five convex quadratic programs. Our procedure applies both in the noiseless setting and when a (constant but random) fraction of the measurements are even adversarially corrupted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work and approaches to phase retrieval</head><p>Our work should be viewed in the context of the recent and successful collection of work on phase retrieval. A natural strategy for problem <ref type="bibr" target="#b0">(1)</ref>, when we wish to find x satisfying | a i , x |<ref type="foot" target="#foot_2">2</ref> = b i for all i ∈ [m], is to lift the problem into a semidefinite program (SDP) by setting X = xx H , relaxing the rank 1 constraint, and solving minimize X tr(X) subject to X 0, tr(Xa i a H i ) = b i . This is the approach that a number of convex approaches to phase retrieval take <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45]</ref>. The resulting SDP is computationally challenging for large n, as it requires storing and manipulating an n × n matrix variable. Moreover, computation times to achieve -accurate solutions to this problem generally scale on the order of n 3 /poly( ), where poly( ) denotes a polynomial in .</p><p>These difficulties have led a number of researchers to consider non-convex approaches to the phase retrieval problem that-as we do-maintain only a vector x ∈ C n , rather than forming a full matrix X ∈ C n×n . We necessarily give only a partial overview, focusing on recent work on provably convergent schemes. Early work in computational approaches to phase retrieval is based on (non-convex) alternating projection approaches, notably those by Gerchberg &amp; Saxton <ref type="bibr" target="#b23">[24]</ref> and Fineup <ref type="bibr" target="#b21">[22]</ref>. Motivated by the challenges of convex approaches and the success of alternating minimization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, Netrapalli et al. <ref type="bibr" target="#b32">[33]</ref> develop an algorithm (AltMinPhase) that alternates between minimizing Ax -Cb 2 in x and in C over diagonal matrices of phases (signs) with modulus 1. Their algorithm is elegant, but the analysis requires resampling a new measurement matrix A and measurements b in each iteration. More recently, Candès et al. <ref type="bibr" target="#b11">[12]</ref> develop Wirtinger flow, a gradient-based method that performs a careful modification of gradient descent on the objective</p><formula xml:id="formula_7">F(x) := 1 2m m i=1 a i , x 2 -b i 2 ,</formula><p>where x ∈ C n may be complex. Wang et al. <ref type="bibr" target="#b45">[46]</ref> build on this work by attacking a modification of this objective, showing how to perform a generalized descent method on</p><formula xml:id="formula_8">F(x) := 1 2m m i=1 a i , x -b i</formula><p>and providing arguments for the convergence of their method. Wang et al. achieve striking empirical results when the measurements and signals are real-valued, achieving better than 50% perfect signal recovery when the measurement ratio m/n = 2, which is essentially at the threshold for injectivity of the real-valued measurements b = (Ax ) 2 . Zhang et al. <ref type="bibr" target="#b46">[47]</ref> also study a variant of Wirtinger flow based on median estimates that handles some outliers. Unfortunately, these procedures rely fairly strongly on Gaussianity assumptions, and their gradient descent approaches require subsampling schemes (to select 'good' terms in the sum); these procedures have parameters chosen carefully to reflect Gaussianity in the measurement matrices A, and it is not always clear how to extend them to non-Gaussian measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our contributions and outline</head><p>In this paper, we focus on prox-linear methods, the iterations <ref type="bibr" target="#b4">(5)</ref> for the non-smooth, non-convex problem <ref type="bibr" target="#b1">(2)</ref>. In addition to being (to us at least) aesthetically pleasing, as we minimize the natural objective <ref type="bibr" target="#b1">(2)</ref>, our approach yields a number of theoretical and practical benefits.</p><p>In the literature on signal recovery from phaseless measurements, stability of the reconstruction of a signal is of paramount importance. To solve the phase retrieval problem at all, one requires injectivity of the measurements b = |Ax| 2 , which for real A ∈ R m×n in general position necessitates m ≥ 2n -1 and for complex A ∈ C m×n in general position necessitates m ≥ 4n -2 (cf. <ref type="bibr" target="#b1">[2]</ref>). Stability makes this injectivity more robust. Consider the real-valued case first. Eldar &amp; Mendelson <ref type="bibr" target="#b18">[19]</ref> say that a measurement matrix A ∈ R m×n is λ ≥ 0 stable if (Ax) 2 -(Ay) 2  1 ≥ λ xy 2 x + y 2 for all x, y ∈ R n .</p><p>Such conditions, which hold with high probability for suitable designs A, are also common in semidefinite relaxation approaches to phase retrieval; cf. Candès et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Lemma 3.2]</ref>. (See also the paper <ref type="bibr" target="#b3">[4]</ref>.) This condition means that distant signals x, x ∈ R n cannot be confused in the measurement domain {(Ay) 2 | y ∈ R n } ⊂ R m + because A does a good job of separating them; the more stable a measurement matrix, the 'easier' the recovery problem should be. In the case in which x is complex, the stability condition (6a) becomes</p><formula xml:id="formula_10">|Ax| 2 -|Ay| 2 1 ≥ λ inf θ</formula><p>xe ιθ y 2 • sup θ</p><p>xe ιθ y 2 for all x, y ∈ C n , (6b) a slightly stronger condition. We provide stability guarantees for both situations for general classes of random matrices by adapting Mendelson's 'small ball' techniques <ref type="bibr" target="#b31">[32]</ref> (Section 3.1). Most literature on non-convex approaches to phase retrieval requires such a stability condition-and usually more because of the quadratic objectives often used-to guarantee signal recovery. In contrast, our procedure requires essentially only the stability condition ( <ref type="formula" target="#formula_9">6</ref>), a mild bound on the operator norm 1 m |||A||| 2 op , and an initialization within some constant factor of x 2 to guarantee both fast convergence and exact signal recovery.</p><p>With this in mind, in Section 2 we develop purely optimization-based deterministic results, which build off of classical results on composite optimization, that rely on the stability condition <ref type="bibr" target="#b5">(6)</ref>. By identifying the conditions required for fast convergence and recovery, we can then spend the remainder of the paper showing how various measurement models guarantee sufficient conditions for our convergence results. In particular, in Section 3, we show how a number of sensing matrices A suffice to guarantee convergence and signal recovery in the noiseless setting, that is, when b = |Ax | 2 . In Section 4, we extend these results to the case when a constant fraction of the measurements b i may be arbitrarily corrupted, showing that stability and a somewhat stronger condition on |||A||| op are still sufficient to guarantee signal recovery; again, these results hold for our basic algorithm with no tuning parameters.</p><p>In the final sections of the paper, we provide a substantial empirical evaluation of our proposed algorithms. While our method in principle requires no tuning-it solves a sequence of explicit convex problems-there is some art in developing efficient methods for the solution of the sequence of convex optimization problems we solve. In Section 5, we describe these implementation details, and in Section 6 we provide experimental evidence of the success of our proposed approach. In reasonably high-dimensional settings (n ≥ 1000), with real-valued random Gaussian measurements our method achieves perfect signal recovery in about 80-90% of cases, even when m/n = 2. The method also handles outlying measurements well, substantially improving state-of-the-art performance, and we give applications with measurement matrices that demonstrably fail all of our conditions, but for which the method is still straightforward to implement and empirically successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>We collect our common notation here. We let • and • 2 denote the usual vector 2 -norm, and for a matrix A ∈ C m×n , |||A||| op denotes its 2 -operator norm. The notation A H means the Hermitian conjugate (conjugate transpose) of A ∈ C m×n . For a ∈ C, Re(a) denotes its real part and Im(a) its imaginary part. We take •, • to be the standard inner product on whatever space it applies; for u</p><formula xml:id="formula_11">, v ∈ C n , this is u, v = u H v, while for A, B ∈ C m×n , this is A, B = tr(A H B). Let quant α ({c i }) denote the α-quantile of a vector c ∈ R m , that is, if c (1) ≤ c (2) ≤ • • • ≤ c (m)</formula><p>, the αth quantile linearly interpolates c ( mα ) and c ( mα ) . For a random variable X, quant α (X) denotes its αth quantile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Composite optimization, algorithm and convergence analysis</head><p>We begin our development by providing convergence guarantees-under appropriate conditions-for the prox-linear algorithm (the iteration ( <ref type="formula" target="#formula_6">5</ref>)) applied to the composite optimization problem (3), which we recall is to minimize</p><formula xml:id="formula_12">x f (x) := h(c(x)) subject to x ∈ X,</formula><p>where h : R m → R is convex and c : C n → R m is appropriately smooth. In our context, as c is a real-valued complex function, it cannot have an ordinary complex derivative, so some care is required; we use the Wirtinger Calculus, also known as the CR-calculus, referring the interested reader to the survey of Kreutz-Delgado <ref type="bibr" target="#b28">[29]</ref> for more (see especially <ref type="bibr" target="#b28">[29,</ref><ref type="bibr">Eq. (31)</ref>]). In brief, however, because c is real-valued, we let ∇c(x) denote the Hermitian conjugate of the Jacobian of c. This allows us to treat c as a mapping from R 2n to R, so that ∇c(x) ∈ C n×m satisfies</p><formula xml:id="formula_13">c(y) = c(x) + Re ∇c(x) H (y -x) + O( y -x 2 )</formula><p>as y → x. We summarize the procedure in Algorithm 1 for further reference. In our application to quadratic constraints and phase retrieval, h(z) = 1 m z 1 and c(x) = |Ax| 2b, so that the iteration ( <ref type="formula" target="#formula_6">5</ref>) is the solution of a quadratic problem.</p><p>A number of researchers have studied convergence and stopping conditions for Algorithm 1, showing that it converges to stationary points <ref type="bibr" target="#b6">[7]</ref>, as well as demonstrating that the stopping condition</p><formula xml:id="formula_14">x k -x k-1 2 ≤ α holds after O( -2</formula><p>) iterations and guarantees approximate stationarity <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Algorithm 1 and the iteration (5) sometimes enjoy fast (local) convergence rates as well. To describe this phenomenon, define the distance function dist(x, S) := inf y { xy 2 | y ∈ S}. We say that h has weak sharp minima if it grows linearly away from its minima, meaning h(z) ≥ inf z h(z) + λ dist(z, argmin h) for some λ &gt; 0. Under this condition (with an additional transversality condition between c and argmin h), Burke &amp; Ferris <ref type="bibr" target="#b7">[8]</ref> show that convergence of the prox-linear algorithm near points in X := {x : c(x) ∈ argmin h} is quadratic, because the model (4) of f is quadratically good, but h(c(x)) grows linearly away from X . These assumptions can be weakened to growth of h • c along its minimizing set <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">Thm. 7.2]</ref>. We build from this elegant development-though our problems do not precisely satisfy the weak sharp minima conditions because of outliers-to show how the prox-linear algorithm provides an effective, easy-to-implement and elegant approach to problems involving solution of quadratic equalities, specifically focusing on phase retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quadratic convergence and the prox-linear method for phase retrieval</head><p>We turn now to an analysis of the prox-linear algorithm for phase retrieval problems, providing conditions on the function f sufficient for quadratic convergence. We introduce two conditions on the function f (x) and its linearized form f x (y) that suffice for this desideratum; as we show in the sequel, these conditions hold with extremely high probability for a number of random measurement models. These conditions are the keystones of our analysis of the (robust) phase retrieval problem.</p><p>As motivation for our first condition, consider the phase retrieval problem. If the measurement matrix A satisfies conditions <ref type="bibr" target="#b5">(6)</ref> and the measurements b i are noiseless with b</p><formula xml:id="formula_15">= |Ax | 2 , then for f (x) = 1 m |Ax| 2 -b 1 we have f (x) -f (x ) ≥ λ dist(x, X ) x 2 for the set of signals X = {e ιθ x | θ ∈ R}.</formula><p>When the measurements have noise or outliers, this may still hold, prompting us to define the following.</p><p>Condition C1 There exists λ &gt; 0 such that for all x ∈ R n (or x ∈ C n in the complex case)</p><formula xml:id="formula_16">f (x) -f (x ) ≥ λ dist(0, X ) dist(x, X ),</formula><p>where X denotes the set of global minima of f . This condition is a close cousin of Burke &amp; Ferris's sharp minima condition <ref type="bibr" target="#b7">[8]</ref>, though it does not require that c(x ) ∈ argmin z h(z); based on their work, it is intuitive that it should prove useful in establishing fast convergence of the prox-linear algorithm. The second condition, which is essentially automatically satisfied for the linear approximation (4), is a requirement that the linearized function f x (y) is quadratically close to f (y).</p><p>Condition C2 There exists L &lt; ∞ such that for all x, y ∈ R n (or x, y ∈ C n in the complex case)</p><formula xml:id="formula_17">| f (y) -f x (y)| ≤ L 2 x -y 2 2 .</formula><p>Locally, Condition C2 holds for any composition f (x) = h(c(x)) of a convex h with smooth c, but the phase retrieval objective (2) satisfies the bound globally. Indeed, for a, x, y ∈ C n we have</p><formula xml:id="formula_18">| a, y | 2 = | a, x | 2 + 2Re( x, a a, y -x ) + | a, y -x | 2 , so the linearization (4) of f around x ∈ C n is f x (y) = 1 m m i=1 | a i , x | 2 -b i + 2Re( x, a i a i , y -x ) . (<label>7</label></formula><formula xml:id="formula_19">)</formula><p>Letting A = [a  </p><formula xml:id="formula_20">-(x -y) H 1 m A H A (x -y) + f x (y) ≤ 1 m m i=1 || a i , y | 2 -b i | = f (y) ≤ f x (y) + (x -y) H 1 m A H A (x -y).</formula><p>That is, Condition C2 holds with L = 2||| 1 m A H A||| op :</p><formula xml:id="formula_21">f (y) -f x (y) ≤ 1 m A H A op x -y 2 2 . (<label>8</label></formula><formula xml:id="formula_22">)</formula><p>Given Conditions C1 and C2, we turn to convergence guarantees for the prox-linear Algorithm 1, which in our case requires solving a sequence of convex quadratic programs. An implementation of Algorithm 1 that solves iteration (5) exactly may be computationally challenging. Thus, we allow inaccuracy in the solutions, assuming there exists a sequence of additive accuracy parameters k ≥ 0 such that the iterates x k satisfy</p><formula xml:id="formula_23">f x k (x k+1 ) + L 2 x k+1 -x k 2 2 ≤ inf x f x k (x) + L 2 x -x k 2 2 + k . (<label>9</label></formula><formula xml:id="formula_24">)</formula><p>We have the following theorem, whose proof we provide in Section 2.2.</p><p>Theorem 1 Let Conditions C1 and C2 hold. Assume that in each step of Algorithm 1, we solve the intermediate optimization problem to accuracy k , and define the relative error measures</p><formula xml:id="formula_25">β k = 2 k λ dist(0,X ) 2 . Then dist(x k , X ) dist(0, X ) ≤ λ 2L max 2L λ • dist(x 0 , X ) dist(0, X ) 2 k , max 0≤ j&lt;k 4L λ • β j If k = 0 in the solution quality inequality (9), then dist(x k , X ) dist(0, X ) ≤ λ L L λ • dist(x 0 , X ) dist(0, X ) 2 k</formula><p>. Theorem 1 motivates our approach for the remainder of the paper; we can guarantee exact, accurate and fast solutions to the phase retrieval problem under the following three conditions:</p><p>1. Stability (Condition C1), 2. Quadratic approximation (Condition C2), via an upper bound on A H A op and application of inequality (8) and</p><p>3. An initializer x 0 of the iterations that is good enough, meaning that it satisfies the constant relative error guarantee dist(x 0 , X ) ≤ dist(0, X ) λ L . In the coming sections, we show that each of these three conditions holds in both noiseless measurement models (Section 3) and with adversarially perturbed measurements b i (Section 4).</p><p>Before continuing, we provide a few additional remarks regarding Theorem 1. First, if k are very small because we solve the intermediate steps ( <ref type="formula" target="#formula_6">5</ref>) to (near) machine precision, then for all intents and purposes about five iterations suffice for machine precision accurate solutions. Quadratic convergence is also achievable with errors in inequality <ref type="bibr" target="#b8">(9)</ref>; if the minimization accuracies decrease quickly enough that k ≤ 2 -2 k , then we certainly still have quadratic convergence. More broadly, Theorem 1 shows that the accuracy of solution in iteration j need not be very high to guarantee high accuracy reconstruction of the signal x ; only in the last few iterations is moderate to high accuracy necessary. If it is computationally cheap, it is thus advantageous-as we explore in our experimental work-to solve early iterations of the prox-linear method inaccurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proof of Theorem 1</head><p>We prove the result in two steps as follows: we first provide a per-iteration progress guarantee, and then we use this guarantee to show quadratic convergence.</p><p>Let x ∈ X be any global optimum of the objective f (x). The function</p><formula xml:id="formula_26">x → f x k (x) + L 2 x -x k 2 2</formula><p>is L-strongly convex in x. If we define x ,k+1 to be the exact minimizer of</p><formula xml:id="formula_27">f x k (x) + L 2 x -x k 2 2</formula><p>, the standard optimality conditions for strongly convex minimization imply</p><formula xml:id="formula_28">f x k (x k+1 ) + L 2 x k+1 -x k 2 2 ≤ f x k (x ,k+1 ) + L 2 x ,k+1 -x k 2 2 + k ≤ f x k (x ) + L 2 x -x k 2 2 - L 2 x -x ,k+1<label>2</label></formula><formula xml:id="formula_29">2 + k .</formula><p>Using the approximation Condition C2, so that L 2 x kx k+1</p><formula xml:id="formula_30">2 2 + f x k (x k+1 ) ≥ f (x k+1 ) and f x k (x ) ≤ f (x ) + L 2 x k -x 2 2</formula><p>, we have by substituting in the preceding inequality that </p><formula xml:id="formula_31">f (x k+1 ) ≤ f x k (x ) + L 2 x -x k 2 2 - L 2 x -x ,k+1 2 2 + k ≤ f (x ) + L x k -x 2 2 - L 2 x -x ,k+1<label>2</label></formula><formula xml:id="formula_32">f (x k+1 ) -f (x ) + L 2 x -x ,k+1 2 2 ≤ L x k -x 2 2 + k . (<label>10</label></formula><formula xml:id="formula_33">)</formula><p>By applying the stability Condition C1, we immediately obtain the progress guarantee</p><formula xml:id="formula_34">λ x 2 dist x k+1 , X + L 2 x ,k+1 -x 2 2 ≤ L x k -x 2 2 + k . (<label>11</label></formula><formula xml:id="formula_35">)</formula><p>We now transform the guarantee (11) into one involving only x k , x k+1 and x , rather than x ,k+1 , by bounding the difference between x ,k+1 and x k+1 . The L-strong convexity of</p><formula xml:id="formula_36">f x k (•) + L 2 • -x k 2 2</formula><p>implies that</p><formula xml:id="formula_37">k + f x k (x ,k+1 ) + L 2 x ,k+1 -x k 2 2 ≥ f x k (x k+1 ) + L 2 x k+1 -x k 2 2 ≥ f x k (x ,k+1 ) + L 2 x ,k+1 -x k 2 2 + L 2 x k+1 -x ,k+1<label>2 2 ,</label></formula><p>whence we obtain</p><formula xml:id="formula_38">L 2 x ,k+1 -x k+1 2 2 ≤ k . Using the standard quadratic inequality x k+1 -x 2 2 ≤ 2 x k+1 -x ,k+1 2 2 + 2 x ,k+1 -x 2 2</formula><p>, we thus have by expression <ref type="bibr" target="#b10">(11)</ref> that, for all x ∈ X ,</p><formula xml:id="formula_39">λ x 2 dist x k+1 , X + L 4 x k+1 -x 2 2 ≤ L x k -x 2 2 + 2 k . (<label>12</label></formula><formula xml:id="formula_40">)</formula><p>Now, taking infimum over x ∈ X over both sides for Equation <ref type="bibr" target="#b11">(12)</ref>, we find that,</p><formula xml:id="formula_41">λ dist(0, X ) dist(x k+1 , X ) ≤ L dist(x k , X ) 2 + 2 k .</formula><p>Dividing each side by λ dist(0, X )</p><formula xml:id="formula_42">2 yields dist(x k+1 , X ) dist(0, X ) ≤ L λ dist(x k , X ) 2 dist(0, X ) 2 + 2 k λ dist(0, X ) 2 . (<label>13</label></formula><formula xml:id="formula_43">)</formula><p>Inductively applying inequality <ref type="bibr" target="#b12">(13)</ref> when k = 0 yields the second statement of the theorem. When k &gt; 0, a brief technical lemma shows the convergence rate.</p><formula xml:id="formula_44">Lemma 2.1 Let the sequences a k ≥ 0, k ≥ 0 satisfy a k ≤ κa 2 k-1 + k-1 .</formula><p>Then</p><formula xml:id="formula_45">a k ≤ max (2κ) 2 k -1 a 2 k 0 , max 0≤j&lt;k (2κ) 2 k-j-1 -1 (2 j ) 2 k-j-1 .</formula><p>Proof. The proof is by induction. For a 1 , we certainly have a 1 ≤ 2κ ∨ 2 0 because both sequences are non-negative. For the general case, assume the result holds for a k , where k is arbitrary. Then</p><formula xml:id="formula_46">a k+1 ≤ κa 2 k + k ≤ max 2κa 2 k , 2 k ≤ 2κ(2κ) 2 k+1 -2 a 2 k+1 0 ∨ max 0≤j&lt;k 2κ(2κ) 2 k-j -2 (2 j ) 2 k-j ∨ 2 k = max (2κ) 2 k+1 -1 a 2 k+1 0 , max 0≤j&lt;k+1 (2κ) 2 k-j -1 (2 j ) 2 k-j</formula><p>as desired.</p><p>Applying Lemma 2.1 in inequality <ref type="bibr" target="#b12">(13)</ref> with κ = L λ and a k = dist(x k ,X ) dist(0,X ) yields the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Noiseless phase retrieval problem</head><p>We begin our discussion of the phase retrieval problem by considering the noiseless case, that is, when the observations b i = | a i , x | 2 . Throughout this section and the remainder of the paper, the vectors a i are assumed to be independent and identically distributed copies of a random vector a ∈ C n . Based on our discussion after Theorem 1, we show that (i) the objective (2) is stable, C1, (ii) is quadratically approximable, C2 and (iii) that we have a good initializer, x 0 . In the coming three sections, we address each of these in turn, providing progressively stronger assumptions that are sufficient for each condition to hold with high probability as soon as the number of measurements m/n &gt; c, where c is a numerical constant. In Section 3.4 we provide a summary theorem that encapsulates our results. For readability, we defer proofs to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stability</head><p>Our first step is to provide conditions under which stability holds. We divide our discussion of stability conditions into the real and complex cases, as the real case is considerably easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Stability for real-valued vectors</head><p>With the stability condition in mind, we make the following assumption on the random measurement vectors a ∈ R n .</p><p>Assumption 1 There exist constants κ st &gt; 0 and p 0 &gt; 0 such that for all u, v ∈ R n , u 2 = v 2 = 1, we have</p><formula xml:id="formula_47">P | a, u | ∧ | a, v | ≥ κ st ≥ p 0 .</formula><p>Intuitively, Assumption 1 says that the measurement vectors a ∈ R n have sufficient support in all directions u, v ∈ R n . One simple sufficient condition for this is a type of small-ball condition <ref type="bibr" target="#b31">[32]</ref>, as follows, which makes it clear that Assumption 1 requires no light tails: just that the probability of one of | a, u | and | a, v | being small is small.</p><p>Example 1 (Stability by the small-ball method) Assume that we may choose positive but small enough ε &gt; 0 that sup</p><formula xml:id="formula_48">u 2 =1 P (| a, u | &lt; ε) &lt; 1 2 .</formula><p>Then by the union bound, the choice p 0 = 1 -2 sup u 2 P(| a, u | &lt; ε) &gt; 0 and κ st = ε immediately yields</p><formula xml:id="formula_49">P | a, u | ∧ | a, v | ≥ κ st = 1 -P (| a, u | &lt; ε or ∧ | a, v | &lt; ε) ≥ 1 -P (| a, u | &lt; ε) -P (| a, u | &lt; ε) ≥ p 0 .</formula><p>As a further specialization, if a ∼ N(0, I n ) is an isotropic Gaussian, then the choice κ st = 0.31 yields</p><formula xml:id="formula_50">P(| a, u | ≤ κ st ) ≤ 1 4</formula><p>, so we may take κ st = 0.31 and p 0 = 1 2 . As we note in the discussion preceding Condition C1, for the objective (2) it is immediate that in the noiseless case that,</p><formula xml:id="formula_51">f (x) -f (±x ) = f (x) = 1 m (Ax) 2 -(Ax ) 2 1 = 1 m m i=1 | a i , x -x a i , x + x |.</formula><p>Thus, if we can show the stability condition (6a)-equivalently, that m i=1 | a i , u a i , v | ≥ λm for u, v ∈ S n-1 -then Condition C1 holds for X = {±x }. To that end, we provide the following guarantee, which we prove in Appendix A.1.</p><p>Proposition 1 Let Assumption 1 hold. There exists a numerical constant c &lt; ∞ such that for any t ≥ 0,</p><formula xml:id="formula_52">P 1 m m i=1 | a i , u a i , v | ≥ κ 2 st • p 0 -c n m - √ 2t for all u, v ∈ S n-1 ≥ 1 -2e -mt .</formula><p>Proposition 1 immediately yields the following corollary, which shows that the stability condition C1 holds with high probability for m/n 1.</p><p>Corollary 3.1 Let Assumption 1 hold. Then there exists a numerical constant c &lt; ∞ such that if mp 2 0 ≥ cn, then</p><formula xml:id="formula_53">P f (x) -f (x ) ≥ 1 2 κ 2 st p 0 x -x 2 x + x 2 for all x ∈ R n ≥ 1 -2 exp - mp 2 0 32 .</formula><p>Eldar &amp; Mendelson <ref type="bibr" target="#b18">[19]</ref> establish the stability Condition C1 under more restrictive assumptions on the distribution of the measurement vectors {a i } m i=1 . Concretely, they require that the distribution of a is sub-gaussian (see Definition 3.1 to come) and isotropic (meaning that E[aa T ] = I n ). As Example 1 makes clear, our result only requires weaker small ball assumptions without any restrictions on tails or the covariance structure of the random vector a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Complex Case</head><p>We now investigate conditions sufficient for stability of A in the measurement vectors a i are complex-valued. In this case, the argument is not quite so simple, as stability for complex vectors requires more uniform notions of function growth. Accordingly, we make the following two assumptions on the random vectors a i ∈ C n . The first is a small-ball type assumption, while the second requires that the vectors a i are appropriately uniform in direction. To fully state our assumptions, we require an additional definition on the sub-Gaussianity of random vectors. We define this in terms of Orlicz norms (following <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr">Ch. 2.2]</ref>).</p><formula xml:id="formula_54">Definition 3.1 The random vector a ∈ C n is σ 2 -sub-Gaussian if for all v ∈ C n , v 2 = 1, E exp | a, v | 2 σ 2 ≤ e.</formula><p>With this definition, we now provide assumptions on the random measurement vector a ∈ C n .</p><p>Assumption 2 There exists a non-increasing function h : R + → R + with h(0) = 0 such that for ≥ 0,</p><formula xml:id="formula_55">P( a 2 ≤ √ n) ≤ h( ).</formula><p>We also require that the distribution of a is sufficiently uniform in direction.</p><formula xml:id="formula_56">Assumption 3 Let w = √ na/ a 2 .</formula><p>The random vector w is σ 2 -sub-Gaussian (Definition 3.1). In addition, for any matrix X ∈ C n×n with rank at most 2,</p><formula xml:id="formula_57">E w H Xw ≥ τ 2 X Fr .</formula><p>Assumption 3 may seem somewhat challenging to verify, but it holds for any rotationally symmetric distribution, and moreover, in this case we have that τ ≥ cσ for a numerical constant c &gt; 0.</p><p>Example 2 (Rotationally symmetric measurements) Let the measurement vectors a i be rotationally symmetric, so that for unitary U ∈ C n×n , the distribution of Ua is identical to a. We show that Assumption 3 holds. In this case, w = √ na/ a 2 is uniform on the radius-√ n sphere in C n , and standard results in convex geometry <ref type="bibr" target="#b2">[3]</ref> show w is O(1)-sub-Gaussian (Definition 3.1). As a is rotationally symmetric, w is also equal in distribution to √ nz/ z 2 , where z is standard complex normal; thus, we have for any rank 2 or less Hermitian X that</p><formula xml:id="formula_58">E[|w H Xw|] = nE[|z H Xz|]E[1/ z 2 2 ] (i) ≥ E[|z H Xz|] (ii) ≥ 2 √ 2 π X Fr ,</formula><p>where inequality</p><formula xml:id="formula_59">(i) is a consequence of E[1/ z 2 2 ] ≥ 1 n and inequality (ii) is a calculation (see Lemma C.2 in Appendix C.3) as X is rank 2.</formula><p>With these assumptions in place, we have the following stability guarantee for the random matrix A. We defer the proof to Appendix A.2.</p><p>Proposition 2 Let Assumptions 2 and 3 hold. Let c &gt; 0 be chosen such that h(c) ≤ 1 2(1+e) τ 4 σ 4 . There exist numerical constants c 0 &gt; 0 and c 1 &lt; ∞ such that with probability at least</p><formula xml:id="formula_60">1 -exp -c 0 m τ 4 σ 4 + c 1 n log σ 2 τ 2 ,</formula><p>we have</p><formula xml:id="formula_61">1 m |Ax| 2 -|Ay| 2 1 ≥ c 2 τ 2 4 • inf θ x -e ιθ y 2 • sup θ x -e ιθ y 2</formula><p>simultaneously for all x, y ∈ C n .</p><p>The proposition shows that if the ratio between the growth constant τ and the sub-Gaussian constant σ in Assumption 3 is bounded, as it is in the case of rotationally invariant vectors a (Example 2), then we have the complex stability guarantee (6b) as soon as m n with extremely high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quadratic approximation</head><p>With the stability condition C1 in place, we turn to a discussion of the approximation condition C2.</p><p>As implied by the estimate in inequality ( <ref type="formula" target="#formula_21">8</ref>), the quadratic approximation condition is satisfied with parameter L = 2||| 1 m A H A||| op . To control this quantity, we require that the rows of the matrix A ∈ C m×n be sufficiently light-tailed.</p><formula xml:id="formula_62">Assumption 4 The random vector a ∈ C n is σ 2 -sub-Gaussian.</formula><p>It is of course possible to bound |||A||| op when the rows have heavier tails using appropriate symmetrization techniques and matrix Khintchine inequalities (cf. [44, Section 2.6)]; the extension is clear, so we do not address such issues.</p><p>Certainly, not all measurement vectors a i satisfy Assumption 4. Using that</p><formula xml:id="formula_63">E[e λZ 2 ] = [1 -2λ] -1 2 +</formula><p>for Z ∼ N(0, 1), it holds for standard real normal vectors a i iid ∼ N(0, I n ) or complex normal vectors</p><formula xml:id="formula_64">a i iid ∼ 1 √ 2 N(0, I n ) + ιN(0, I n ) with σ 2 = 2e 2 e 2 -1 ≈ 2.</formula><p>313. Similarly, it also holds for a i uniform on</p><formula xml:id="formula_65">S n-1 with σ 2 = O(1) • 1</formula><p>n . In practice it may be useful to apply our algorithm to the transformed data</p><formula xml:id="formula_66">{a i / a i 2 } m i=1 and {b i / a i 2 2 } m i=1</formula><p>, which (in the noiseless case or case with infrequent but arbitrary corruptions of b i ) is likely to make the problem better conditioned. There are two heuristic motivations for this: first, those measurement vectors with larger magnitudes a i tend to place a higher weight in the optimization problem (2), and thus normalization can make the observations 'comparable' to each other; second, normalization guarantees the measurement vectors {a i } m i=1 satisfy Assumption 4, yielding easier verification of Condition C2. (It may be more challenging to verify Assumption 1, but if the a i are sufficiently isotropic this presents no special difficulties.)</p><p>Standard results guarantee that the random matrices A have well-behaved singular vectors whenever Assumption 4 holds; we provide one such result due to Vershynin [44, Thm. 39, Eq. ( <ref type="formula">25</ref>)] with constants that are achievable by tracing his proof. Lemma 3.1 Let Assumption 4 hold and Σ = E[aa H ]. Then for all t ≥ 0,</p><formula xml:id="formula_67">P 1 m A H A -Σ op ≥ 11σ 2 max 4n m + t, 4n m + t ≤ exp(-mt). Moreover, |||Σ||| op ≤ σ 2 and E[| a, v | k ] ≤ k 2 + 1 eσ k</formula><p>for all k ≥ 0. Thus, we have the following corollary of Lemma 3.1, which guarantees that Condition C2 holds with high probability for m/n 1.</p><p>Corollary 3.2 Let Assumption 4 hold. Then there exists a numerical constant c &lt; ∞ such that whenever m ≥ cn</p><formula xml:id="formula_68">P f x (y) -f (y) ≤ 2σ 2 x -y 2 2 holds uniformly for x, y ∈ R n ≥ 1 -exp - m c .</formula><p>Proof. Assume m ≥ cn for c large enough, and choose t small enough in Lemma 3.</p><formula xml:id="formula_69">1 that 11σ 2 √ 4n/m + t ≤ σ 2 . Applying the triangle inequality to ||| 1 m A H A -Σ||| op ≤ ||| 1 m A H A||| op + σ 2</formula><p>yields the result by equation (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initialization</head><p>The last ingredient in achieving strong convergence guarantees for our prox-linear procedure for phase retrieval is to provide a good initialization. There are a number of initialization strategies in the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> based on spectral techniques, which work as follows. We decompose the initialization into the following two steps: we (i) find an estimate of the direction direction d := x / x 2 and (ii) estimate the magnitude r := x 2 . The latter is easy; assuming that E[aa H ] = I n , one simply uses r 2 = m -1 m i=1 b 2 i , which is unbiased and tightly concentrated. The former, the direction estimate, is somewhat trickier.</p><p>Wang et al. <ref type="bibr" target="#b45">[46]</ref> provide an empirically excellent initialization whose heuristic justification is as follows. First, for random vectors a i in high dimensions, we expect a i to usually be orthogonal to the direction d . Thus, by extracting the smallest magnitude b i = | a i , x | 2 , we have the vectors a i that are 'most' orthogonal to the direction d ; letting I sel be these small indices, the eigenvector corresponding to the smallest eigenvalue (for simplicity, we simply call this the smallest eigenvector) of i∈I sel a i a H i should be close to the direction d . A variant of this procedure is to note that 1 m m i=1 a i a H i ≈ I n when the a i are isotropic, so that the largest eigenvector of i ∈I sel a i a H i should also be close to d . This initalization strategy has the added benefit that-unlike the original spectral initialization schemes developed by Candès et al. <ref type="bibr" target="#b11">[12]</ref>, which rely on eigenvectors of m i=1 b i a i a H i that may not concentrate at sub-Gaussian rates (as the sum involves fourth moments of random vectors)-the sums i a i a H i are tightly concentrated.</p><p>Unfortunately, we believe Wang et al.'s proof that this initialization works contain a mistake (note that they consider only the case where {a i } m i=1 are real); letting U ∈ R n×(n-1) be an orthogonal matrix whose columns are all orthogonal to d , in the proof of Lemma 2 (Eqs. (68-70) in <ref type="bibr" target="#b45">[46]</ref>) they assert that</p><formula xml:id="formula_70">|I c sel | -1 i∈I c sel U H a i a i U → I n-1</formula><p>when the a i are uniform on √ nS n-1 . This is not true (nor does appropriate normalization by n or n -1 make it true), as it ignores the subtle effects of conditioning in the construction of I sel . In spite of this issue, the initialization they propose works remarkably well, and as we show presently, it provably provides a good estimate d of d . We include the initialization procedure in Algorithm 2.</p><p>With the previous discussion in mind, we provide a general assumption that is sufficient for Algorithm 2 to return a direction and magnitude estimate sufficiently accurate for phase retrieval.</p><p>Assumption 5 For some 0 ∈ (0, 1] and p 0 (d ) &gt; 0, the following hold:</p><p>(i) For all ∈ 0, 0 , the following continuity and directional likelihood conditions hold:</p><formula xml:id="formula_71">P | a, d | 2 ∈ 1 - 2 , 1 + 2 ≤ κ and P | a, d | 2 ≤ 1 - 2 ≥ p 0 (d ) &gt; 0.</formula><p>(ii) There exist functions φ :</p><formula xml:id="formula_72">[0, 0 ] → R + and Δ : [0, 0 ] → C n×n such that E aa H | | a, d | 2 ≤ 1 - 2 = I n -φ( )d d H + Δ( ) for ∈ [0, 0 ]. (iii) E[aa H ] = I n . Assumption 5 on its face seems complex, but each of its components is not too stringent. Part (i) says that | a, d | 2 has no point mass at | a, d | 2 = 1</formula><p>2 and that | a, d | 2 has reasonable probability of being smaller than 1  2 . Part (iii) simply states that in expectation, a is isotropic (and a rescaling of a can guarantee this). Part (ii) is the most subtle and essential for our derivation; it says that a ∈ R n /C n is reasonably isotropic, even if we condition on | a, d | being near zero for some direction d , so that most mass of aa H is distributed uniformly in the orthogonal directions I nd d H . The error terms φ and Δ allow non-trivial latitude in this condition, so that Assumption 5 holds for more than just Gaussian vectors. That said, for concreteness we provide the following example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 3 (Gaussian vectors and conditional directions) We consider the standard cases that a</head><formula xml:id="formula_73">i iid ∼ N(0, I n ), or the complex counterpart a i iid ∼ 1 √ 2 N(0, I n ) + ιN(0, I n ) ,</formula><p>showing that such a i satisfy Assumption 5 for any 0 ∈ (0, 1) with residual error Δ ≡ 0. Clearly Part (iii) holds. For Part</p><formula xml:id="formula_74">(i), note that | a, d | 2 is χ 2 1 -distributed with density f (t) = e -t/2 (2π t) -1 2 .</formula><p>Integrating the density using its upper bound, we may set</p><formula xml:id="formula_75">κ ≤ f 1-0 2 = exp 0 -1 4 / π(1 -0 ) and p 0 (d ) = P χ 2 1 ≤ 1-0 2 ≥ 1 2 1 -0 . Part (ii) is all that remains. By the rotational invariance of a ∼ N(0, I n ), we see for any t ∈ R + that E[aa H | | a, d | 2 ≤ t] = I n -d d H + E[| a, d | 2 | | a, d | 2 ≤ t]d d H .</formula><p>We claim the following lemma, whose proof we provide in Appendix C.1. Lemma 3.2 Let Z be a continuous random variable with a density symmetric about zero and decreasing on R + . Then for any c ∈ R we have</p><formula xml:id="formula_76">E[Z 2 | Z 2 ≤ c 2 ] ≤ c 2 3 . Thus, setting φ( ) = 1 -E[| a, d | 2 | | a, d | 2 ≤ 1- 2 ] ≥ 1 -(1-) 2 6</formula><p>≥ 5  6 , we see that part (ii) of Assumption 5 holds with φ( ) ≥ 5  6 and Δ( ) ≡ 0. We now state our main proposition of this section.</p><p>Proposition 3 Let Assumptions 4 and 5 hold and let 0 and p 0 (d ) be as in Assumption 5. Define the error measure</p><formula xml:id="formula_77">ν( ) := 1 + 4e 1 + log 1 1∧κ σ 2 κ p 0 (d ) + 2κ(1 + ) p 0 (d ) 2 .</formula><p>There exists a numerical constant c &gt; 0 such that the following holds. Let ( r, d) be the estimated magnitude and direction of Algorithm 2 and define</p><formula xml:id="formula_78">x 0 = r d. For any ∈ [0, 0 ], if c • m n ≥ σ 4 log 2 p 0 (d ) p 0 (d ) 2 ∨ 1 (κ ) 2 then dist(x 0 , X ) x 2 ≤ 2(1 + ) |||Δ( )||| op + ν( ) φ( ) -|||Δ( )||| op -ν( ) + +<label>(14)</label></formula><p>with probability at least</p><formula xml:id="formula_79">1 -exp - cm 2 σ 4 -2 exp -cm 2 κ 2 -exp - mp 0 (d ) 2 2 -exp - cmκ 2 σ 4 log 2 p 0 (d )</formula><p>.</p><p>We prove Proposition 3 in two parts. In the first part (Appendix A.3), we define a number of events and proceed conditionally, showing that if each of the events occurs then the conclusion ( <ref type="formula" target="#formula_78">14</ref>) holds. In the second part (Appendix A.4) we show that the events occur with high probability.</p><p>We provide a few remarks to make the result clearer. Let us make the simplifying assumptions that the constants in Assumption 5 are absolute (which is true for Gaussian measurements), that is, that σ 2 = O(1) and p 0 (d ) = (1) (it is no loss of generality to assume κ ≥ 1). Then for numerical constants c &gt; 0, C &lt; ∞ we have for any ∈ [0, 0 ] that the error measure ν( ) ≤ C log 1 , and with probability at least 1 -5 exp(-cm 2 ) that</p><formula xml:id="formula_80">m n ≥ C 2 implies dist(x 0 , X ) x 2 ≤ C |||Δ( )||| op + ν( ) φ( ) -|||Δ( )||| op -ν( ) + + .</formula><p>Here, we see three competing terms. The first two, the separation φ( ) and error Δ( ), arise from the conditional expectation of Assumption 5 (ii), where</p><formula xml:id="formula_81">E[aa H | a, d 2 ≤ 1- 2 ] = I n -φ( )d d H + Δ(</formula><p>). This is intuitive; the larger the separation φ( ) from uniformity in the conditional expectation of aa H , the easier it is for spectral initialization to succeed; larger error Δ( ) will hide the directional signal d . The last term is the error ν( ) log 1 , which approaches 0 nearly as quickly as → 0. In the case that the error Δ( ) = 0 and gap φ( ) is bounded away from zero, which holds for elliptical distributions with identity covariance-the Gaussian distribution and uniform distribution on the sphere being the primary examples-we thus see that as soon as m n -2 we have relative error dist(x 0 , X )</p><formula xml:id="formula_82">x 2 log 1 with probability ≥ 1 -5 exp(-cm 2 ). (<label>15</label></formula><formula xml:id="formula_83">)</formula><p>That is, we can construct an arbitrarily good initialization with large enough sample size. (This proves that the initialization scheme of Wang et al. <ref type="bibr" target="#b45">[46]</ref> also succeeds with high probability.) On the other hand, when Δ( ) = 0 for all ∈ [0, 0 ], then Proposition 3 cannot guarantee arbitrarily good initialization; the error term |||Δ( )||| op is never zero. However, if it is small enough, we still achieve initializers that are within constant relative distance of x , which is good enough for Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary and success guarantees</head><p>We now have guarantees of stability, quadratic approximation and good initialization for appropriate measurement matrices A ∈ R m×n when the observations b = |Ax | 2 . We provide a summary theorem showing that our composite optimization procedure works as soon as the sample size is large enough.</p><p>In stating the theorem, we assume that each of Assumptions 1, 4 and 5 holds with all of their constants actually numerical constants. The one somewhat technical assumption we require relates the sub-Gaussian parameter σ 2 , the stability parameters κ st and p 0 of Assumption 1 (alternatively, in the complex case the growth constant τ 2 and small-ball function h(•) of Assumptions 2 and 3), and the error Δ( ) and directional separation constants φ( ) of Assumption 5. In the real case, define ν := κ 2 st p 0 and in the complex, recall Proposition 2 and define ν</p><formula xml:id="formula_84">:= h -1 τ 4 2(1+e)σ 4 τ 2 .</formula><p>Then if we assume that for a suitably small numerical constant c &gt; 0, we have</p><formula xml:id="formula_85">|||Δ( )||| op ≤ c ν σ 2 , φ( ) ≥ 1 2 |||Δ( )||| op and φ( ) ≥ φ &gt; 0</formula><p>for all ∈ [0, 0 ]. We then have the following theorem, which follows by combining our convergence Theorem 1 with Proposition 1, Corollary 3.2 and Proposition 3.</p><p>Theorem 2 Let the conditions of the preceding paragraph hold. There exists a numerical constant c &gt; 0 such that if n m &lt; c, then the initializer x 0 returned by Algorithm 2 satisfies dist(x 0 , X ) ≤ ν 8σ 2 x 2 , and assuming no error in the minimization steps of the prox-linear method (Algorithm 1), dist(x k , X ) ≤ x 2 2 -2 k for all k ∈ N with probability at least 1e -cm . We make two brief summarizing remarks. First, it is necessary to have m n to achieve exact recovery of the signal, as the parameter x ∈ C n has 2n unknowns and we have m equations (indeed, m ≥ 4n -2 is necessary for inectivity of the measurements in the complex case <ref type="bibr" target="#b1">[2]</ref>). Thus, the sample complexity of Theorem 2 is optimal to within numerical constants. Secondly, Theorem 2 shows that the prox-linear algorithm exhibits local quadratic convergence to the signal x , which is in contrast to the local linear convergence of other non-convex methods based on gradients and generalized gradients <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46]</ref>. In contrast, however, each iteration of our algorithm requires solving a structured convex quadratic program, which is somewhat more expensive than the typical gradient iterations; as we demonstrate in our experiments (Section 6), this means our methods are about four times slower in overall run time than the best gradient-based methods, though their recovery properties are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Phase retrieval with outliers</head><p>The objective (2) is the analogue of the least-absolute deviation estimator-the median in R-so in analogy with our understanding of robustness <ref type="bibr" target="#b26">[27]</ref>, it is natural to expect it should be robust to outliers. We show that this is indeed the case, and the prox-linear method we develop is effective. For simplicity in our development, we assume for this section that all measurements and signals are real-valued, and we consider the following corruption model; let {ξ i } ⊂ R be an arbitrary sequence, and given the m measurement vectors a i , we observe</p><formula xml:id="formula_86">b i = a i , x 2 if i ∈ I in ξ i if i ∈ I out ,</formula><p>where I out ⊂ [m] and I in ⊂ [m] denote the outliers and inliers, respectively. We assume there is a prespecified measurement failure probability p fail ∈ 0, 1  2 , and |I out | = p fail m, and the indices i ∈ I out are chosen randomly. That is, measurement failures are random, though the noise sequence ξ i may depend on a i (even adversarially), as we specify presently. We assume no prior knowledge of which indices i ∈ [m] actually satisfy i ∈ I out , or even of p fail .</p><p>We consider the following two models for errors:</p><p>Model M1. The measurement vectors {a i } m i=1 are independent of the all the values {ξ i } m i=1 . Model M2. The inlying measurement vectors {a i } i∈I in are independent of the values {ξ i } i∈I out of the corrupted observations. Model M1 requires independence between the noise and measurements; the adversary may only corrupt ξ i without observing a i . Model M2 relaxes this, allowing arbitrary dependence between the corrupted data and the measurement vectors a i for i ∈ I out . This is natural in scenarios in which the corruption may depend on the measurement a i .</p><p>The arbitrary corruption causes some technical challenges, but we may still follow the outline in our analysis of phase retrieval without noise in Section 3. As we show in Section 4.1, the objective f (x) is still stable (Condition C1) as long as the measurement vectors are light-tailed, though Gaussianity is unnecessary. The quadratic approximation conditions (Condition C2) are completely identical to those in Section 3.2, so we ignore them. Thus, as long as |||A||| op is not too large (meaning f x (y) ≈ f (y)) and we can find a good initializer, the prox-linear iterations (5) will converge quadratically to x . Finding a good initializer x 0 is somewhat trickier, but in Section 4.2 we provide a spectral method, inspired by Wang et al. <ref type="bibr" target="#b45">[46]</ref>, that works with high probability as soon as m/n ≥ C for some numerical constant C. We defer our arguments to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stability</head><p>The outlying indices, even when corruptions are chosen adversarially, have limited effect on the growth and identification behavior of</p><formula xml:id="formula_87">f (x) = 1 m (Ax) 2 -b 1 .</formula><p>In particular, for p fail smaller than a numerical constant, which we can often specify, the stability condition C1 holds with high probability whenever m/n is large. More precisely, we have the following proposition, which applies to independent σ 2 -sub-Gaussian measurements. (See Appendix B.1 for a proof.)</p><formula xml:id="formula_88">Proposition 4 Let Assumption 4 hold and κ st = inf u,v∈S n-1 E[| a, u a, v |].</formula><p>Then under either of the models M1 or M2, there are numerical constants c &gt; 0 and C &lt; ∞ such that Example 4 (Gaussian vectors) We claim that for a ∼ N(0, I n ) we have</p><formula xml:id="formula_89">f (x) -f (x ) ≥ κ st -2p fail -Cσ 2 3 n m -Cσ 2 t x -x 2 x + x 2 for all x ∈ R n with probability at least 1 -2e -cm -2e -mt 2 .</formula><formula xml:id="formula_90">κ st := inf u,v∈S n-1 E[| a, u a, v |] = 2 π .</formula><p>Let Z u = a, u , Z v = a, v , and let X, Y be independent N(0, 1). Then inf</p><formula xml:id="formula_91">u,v∈S n-1 E[|Z u Z v |] = inf ρ∈[0,1] f (ρ) := E ρX 2 -(1 -ρ)Y 2 .</formula><p>The function f (•) is convex and symmetric around 1 2 . Thus, f (ρ) ≥ f (1/2) = 2/π . In the Gaussian measurement case, whenever p fail &lt; 1 π ≈ 0.318, there is a numerical constant λ &gt; 0 such that we have the stability f (x)f (x ) ≥ λ xx</p><p>x + x for as long as m/n is larger than some numerical constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Initialization</head><p>The last ingredient for achieving strong convergence guarantees for the prox-linear algorithm for phase retrieval is to provide a good initialization x 0 ≈ x . The strategies in the noiseless setting in Section 3 will fail because of corruptions. With this in mind, we present Algorithm 3, which provides an initializer in corrupted problems.</p><p>Before turning to the analysis, we provide some intuition for the algorithm. We must construct two estimates; an estimate d of the direction d = x / x 2 and an estimate r of the radius, or magnitude, of the signal r = x 2 . For the former, a variant of the spectral initialization (Algorithm 2) suffices. If we take the I sel ⊂ [m] to be the set of indices I sel corresponding to the smallest (say) b i , in either model M1 or M2 the indices i ∈ I out are independent of the measurements a i , so we expect as in Section 3.3 that</p><formula xml:id="formula_92">X init = |I sel | -1 i∈I sel a i a T i = zI n -z d d T + Δ,</formula><p>where z, z are random positive constants and Δ is an error matrix coming from both randomness in the a i and the corruptions. As long as the error Δ is small, the minimum eigenvector of X init should be approximately d . Once we have a good initializer d ≈ d , a natural idea to estimate r is to pretend that d is the direction of the signal, substitute the variable x = √ r d into the objective (2), and solve for r to get a robust estimate of the signal strength x . As we show presently, this procedure succeeds with high probability (and the estimate r is good even when the data are non-Gaussian).</p><p>Let us make these ideas precise. First, we show that our estimate r of x 2 is accurate (see Appendix B.2 for a proof).</p><p>Proposition 5 Let Assumption 4 hold and E[aa T ] = I n . Let δ ∈ [0, 1] and p fail ∈ [0, 1  2 ]. There exist numerical constants 0 &lt; c, C &lt; ∞ such that if d is an estimate of d for which</p><formula xml:id="formula_93">δ := Cσ 2 1 -2p fail dist( d, {±d }) ≤ 1,<label>(16)</label></formula><p>then with probability at least 1</p><formula xml:id="formula_94">-2e -cm(1-2p fail ) 2 /σ 4 all minimizers r 2 of G(r) = 1 m m i=1 |b i -r a i , d 2 |, defined in Algorithm 3, satisfy r 2 ∈ [1 ± δ] x 2 2 .</formula><p>Given Proposition 5, finding a good initialization of x reduces to finding a good estimate d of the direction d = x / x 2 . To make this precise, let r := x and assume that δ = Cσ 2 1-2p fail dist( d, {±d }) ≤ 1 as in Proposition 5; assume also the relative error guarantee | r-r | ≤ δr . Using the triangle inequality and Proposition 5, for</p><formula xml:id="formula_95">x 0 = r d we have dist(x 0 , X ) ≤ r dist( d, {±d }) + |r -r| ≤ r (1 + δ) dist( d, {±d }) + δ ≤ 2δr = Cσ 2 1 -2p fail dist( d, {±d })r ,</formula><p>as claimed. We turn to the directional estimate; to make the analysis cleaner we make the normality Assumption 6 The measurement vectors a i iid ∼ N(0, I n ).</p><p>To state our guarantee on d, we require additional notation for quantiles of Gaussian and χ 2 -random variables. Let W ∼ N(0, 1) and define the constant q fail and its associated χ 2 -quantile w 2 q by</p><formula xml:id="formula_96">P(W 2 ≤ w 2 q ) = q fail := 1 2(1 -p fail ) + 1 -2p fail 4(1 -p fail ) = 3 -2p fail 4(1 -p fail ) &lt; 1. (<label>17</label></formula><formula xml:id="formula_97">)</formula><p>Secondly, define the constant</p><formula xml:id="formula_98">δ q = 1 -E[W 2 | W 2 ≤ w 2 q ]</formula><p>. We have the following guarantee.</p><p>Proposition 6 Let d be the smallest eigenvector of </p><formula xml:id="formula_99">X init := 1 m m i=1 a i a T i 1 i ∈ I sel ,</formula><formula xml:id="formula_100">C n m + t (1 -2p fail )δ q -C n m + t -Mp fail + .</formula><p>For intuition, we provide a few simplifications of Proposition 6 by bounding the quantities w q and δ q defined in Equation <ref type="bibr" target="#b16">(17)</ref>. Using the conditional expectation bound in Lemma 3.2 and a more careful calculation for Gaussian random variables (see Lemma C.1 in the Appendices) we have</p><formula xml:id="formula_101">E a 2 i,1 | a 2 i,1 ≤ w 2 q ≤ min w 2 q 3 , 1 - 1 2 w 2 q P(a 2 i,1 ≥ w 2 q ) &lt; 1, so δ q ≥ max 1 - w 2 q 3 , 1 2 w 2 q P(W 2 ≥ w 2 q )</formula><p>. For p fail ≤ 3 8 , we may take w 2 q ≤ 2.71 and δ q &gt; 1 11 . More generally, a standard Gaussian calculation that Φ -1 (p) ≥ | log(1p)| as p → 1 shows that w 2 q ≥ log 8(1-p fail ) 1-2p fail as p fail → 1 2 , so δ q ≥ 1-2p fail 8(1-p fail ) log 8(1-p fail ) 1-2p fail . Under Model M1, then, as long as the sample is large enough and p fail &lt; 1  2 , we can achieve constant accuracy in the directional estimate dist( d, {±d }) with probability of failure e -cm . Under the more adversarial noise model M2, we require a bit more; more precisely, we must have (1 -2p fail )δ qp fail &gt; 0 to achieve accurate estimates. A numerical calculation shows that if p fail &lt; 1  4 , then this condition holds, so that under Model M2 we can achieve constant accuracy in the directional estimate dist( d, {±d }) with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary and success guarantees</head><p>With the guarantees of stability, quadratic approximation and good initialization for suitably random matrices A ∈ R m×n , we can provide a theorem showing that the prox-linear approach to the composite optimization phase retrieval objective succeeds with high probability. Roughly, once m/n is larger than a numerical constant, the prox-linear method with noisy initialization succeeds with exponentially high probability, even with outliers. Combining the convergence Theorem 1 with Propositions 4, 5, 6 and Corollary 3.2, we have the following theorem.</p><p>Theorem 3 Let Assumptions 6 hold. There exist numerical constants c &gt; 0 and C &lt; ∞ such that the following hold for any t ≥ 0. Let the independent outliers Model M1 hold and p fail &lt; 1 π or the adversarial outliers Model M2 hold and p fail &lt; 1  4 . Let x 0 be the initializer returned by Algorithm 3, and assume the iterates x k of Algorithm 1 are generated without error. Then</p><formula xml:id="formula_102">dist(x 0 , X ) ≤ C (1 -2p fail ) 2 n m + t • x 2 and dist(x k , X ) ≤ x 2 2 -2 k for all k ∈ N with probability at least 1 -4e -mt -e -c(1-2p fail ) 2 m .</formula><p>Thus, we see that the method succeeds with high probability as long as the sample size is large enough, though there is non-trivial degradation when p fail is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Optimization methods</head><p>In practice, we require some care to solve the sub-problems (5), to minimize f x (y) + L 2 xy 2 2 , at large scale. In this section, we describe the three schemes we use to solve the sub-problems (one is simply using the industrial Mosek solver). We evaluate these more carefully in the experimental section to come.</p><p>To fix notation, let φ(•) be the element-wise square operator. Recalling that f (x) = 1 m φ(Ax)b 1 , we perform a few simplifications to more easily describe the schemes we use to solve the proxlinear sub-problems. Assuming we begin an iteration at point x 0 , defining the diagonal matrix D = 2 diag(Ax 0 ) ∈ R m×m and setting c = b + φ(Ax 0 ), we recall inequality <ref type="bibr" target="#b7">(8)</ref> and have</p><formula xml:id="formula_103">f (y) ≤ f x 0 (y) + 1 m (y -x 0 ) T A T A(y -x 0 ) ≤ 1 m DAy -c 1 + m -1 A T A op y -x 0 2 2 .</formula><p>Rewriting this with appropriately rescaled diagonal matrices D and vector c, implementing Algorithm 1 becomes equivalent to solving a sequence of optimization problems of the form</p><formula xml:id="formula_104">minimize x DAx -c 1 + 1 2 x 2 2 . (<label>18</label></formula><formula xml:id="formula_105">)</formula><p>In small scale scenarios, the problem ( <ref type="formula" target="#formula_104">18</ref>) is straightforward to solve via standard interior point method software; we use Mosek via the Convex.jl toolbox <ref type="bibr" target="#b41">[42]</ref>. We do not describe this further. In larger-scale scenarios, we use more specialized methods, which we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph splitting methods for the prox-linear sub-problem</head><p>When the matrices are large, we use a variant of the Alternating Directions Method of Multipliers procedure known as the proximal operator graph splitting (POGS) method of Parikh and Boyd <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, which minimizes objectives of the form f (x) + g(y) subject to a linear constraint Bx = y. We experimented with a number of specialized first-order and interior-point methods for solving problem <ref type="bibr" target="#b17">(18)</ref>, but in our experience, POGS offers the empirically best performance. Let us describe the POGS method. Let the matrix B = DA for shorthand; evidently, problem <ref type="bibr" target="#b17">(18)</ref> has precisely this form and is equivalent to</p><formula xml:id="formula_106">minimize x∈R n ,y∈R m y -c 1 + 1 2</formula><p>x 2 2 subject to Bx = y.</p><p>The POGS method iterates to solve problem (18) as follows. Introduce dual variables λ k ∈ R n and ν k ∈ R m associated to x and y, and consider the iterations + ν k ) to the set {x, y : Bx = y}. The first two updates amount to</p><formula xml:id="formula_107">x k+ 1 2 = argmin x 1 2 x 2 2 + ρ 2 x -(x k -λ k ) 2 2 y k+ 1 2 = argmin y y -c 1 + ρ 2 y -(y k -ν k ) 2 2 x k+1 y k+1 = I n B T B -I m -1 I n B T 0 0 x k+ 1 2 + λ k y k+ 1 2 + ν k λ k+1 = λ k + (x k+ 1 2 -x k+1 ), ν k+1 = ν k + (y k+ 1 2 -y k+1 ). (<label>19</label></formula><formula xml:id="formula_108">)</formula><formula xml:id="formula_109">x k+ 1 2 = ρ 1 + ρ (x k -λ k ) and y k+ 1 2 = c + sgn(y k -ν k -c) |y k -ν k -c| -1/ρ + ,</formula><p>where denotes element-wise multiplication and each operation is element-wise. The matrix B is tall, so setting</p><formula xml:id="formula_110">v k = x k+ 1 2 + λ k + B T (y k+ 1 2 + ν k ) ∈ R n</formula><p>, then the solution of the system</p><formula xml:id="formula_111">I n B T B -I n x y = v k 0 n is x k+1 = (I n + B T B) -1 v k and y k+1 = Bx k+1 . (<label>20</label></formula><formula xml:id="formula_112">)</formula><p>In this iteration, it is straightforward to cache the matrix (I n + B T B) -1 , or a Cholesky factorization of the matrix, so that we can repeatedly compute the multiplication (20) in time n 2 + nm. Following Parikh and Boyd <ref type="bibr" target="#b34">[35]</ref>, we define the primal and dual residuals</p><formula xml:id="formula_113">r pri k+1 := x k+1 -x k+ 1 2 y k+1 -y k+ 1 2 and r dual k+1 := ρ x k -x k+1 y k -y k+1 .</formula><p>These residuals define a natural stopping criterion <ref type="bibr" target="#b34">[35]</ref>, where one terminates the iteration <ref type="bibr" target="#b18">(19)</ref> once the residuals satisfy</p><formula xml:id="formula_114">r pri k 2 &lt; √ n + max{ x k 2 , y k 2 } and r dual k 2 &lt; √ n + max{ λ k 2 , ν k 2 }<label>(21)</label></formula><p>for some &gt;0, which must be specified. In our case, the quadratic convergence guarantees of Theorem 1 suggest a strategy of decreasing across iterative solutions of the sub-problem <ref type="bibr" target="#b17">(18)</ref>. That is, we begin with some = 0 . We perform iterations of the prox-linear Algorithm 1 using the POGS iteration <ref type="bibr" target="#b18">(19)</ref> (until the residual criterion (21) holds) to solve the inner problem <ref type="bibr" target="#b17">(18)</ref>. Periodically, in the outer prox-linear iterations of Algorithm 1, we decrease by some large multiple.</p><p>In our experiments with the POGS method for sub-problem solutions, we perform two phases. In the first phase, we iterate the prox-linear method (Algorithm 1) until either k = 25 or x kx k+1 2 ≤ δ/ (1/m)A T A op , where δ = 10 -3 , using accuracy parameter = 10 -5 for POGS (usually, the iterations terminate well-before k = 25). We then decrease this parameter to = 10 -8 , and begin the iterations again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Conjugate gradient methods for sub-problems</head><p>In a number of scenarios, it is possible to multiply by the matrix A quickly, though the direct computation <ref type="bibr" target="#b19">(20)</ref> (recall B = DA) may be difficult. For example, if A is structured (say, a Fourier or Hadamard transform matrix or or sparse), computing multiplication by I n + A T D 2 A quickly is possible. This suggests <ref type="bibr">[40,</ref> Part VI] using conjugate gradient methods.</p><formula xml:id="formula_115">(I n + A T D 2 A) -1 in expression</formula><p>We make this more explicit here to mesh with our experiments to come. Let H n be an n×n orthogonal matrix for which computing the multiplication H n v is efficient (e.g. a Hadamard or discrete cosine transform matrix). We assume that the measurement matrix A takes the form of repeated randomized measurements under H n , that is,</p><formula xml:id="formula_116">A = H n S 1 H n S 2 • • • H n S k T ,<label>(22)</label></formula><p>where S l ∈ R n×n are (random) diagonal matrices, so that A ∈ R m×n with m = kn. In this case, the expensive part of the system <ref type="bibr" target="#b19">(20)</ref> is the solution of (I + A T D 2 A)x = v. This is a positive definite system, and it is possible to compute the multiplication (I + A T D 2 A)x in time O(kn + kT mult ), where T mult is the time to muliply an n-vector by the matrix H n and H T n . When S is a random sign matrix and H n is a Hadamard or FFT matrix, the matrix A T D 2 A is well-conditioned, as the analysis of random (subsampled) Hadamard and Fourier transforms shows <ref type="bibr" target="#b40">[41]</ref>. Thus, in our experiments with structured matrices we use the conjugate gradient method without preconditioning <ref type="bibr" target="#b39">[40]</ref>, iterating until we have relative error (I + A T DA)xv ≤ cg / v where cg = 10 -6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We perform a number of simulations as well as experiments on real images to evaluate our method and compare with other state-of-the-art (non-convex) methods for real-valued phase retrieval problems. To standardize notation and remind the reader, in each experiment, we generate data via (variants) of the following process. We take a measurement matrix A ∈ R m×n , from one of a few distributions and generate a signal x ∈ R n either by drawing x ∼ N(0, I n ) or by taking x ∈ {-1, 1} n uniformly at random. We receive observations of the form b i = a i , x 2 , and with probability p fail ∈ [0, 1  2 ], we corrupt the measurements b i . In our experiments, to more carefully isolate the relative performance of the iterative algorithms, rather than initialization used, we compare three initializations. The first two rely on Gaussianity of the measurement matrix A.</p><p>(i) Big: The initialization of Wang et al. <ref type="bibr" target="#b45">[46]</ref>. Defining</p><formula xml:id="formula_117">I 0 := i ∈ [m] : b i / a i 2 2 ≥ quant 5/6 ({b i / a i 2 2 }) and X init := i∈I 0 1 a i 2 2 a i a T i , we set the direction d = argmax d 2 =1 d T X init d and x 0 = 1 m m i=1 b i 1 2 d.</formula><p>(ii) Median: The initialization of Zhang et al. <ref type="bibr" target="#b46">[47]</ref>. We set r 2 = quant 1 2 ({b i })/0.455 and define</p><formula xml:id="formula_118">I 0 := i ∈ [m] : |b i | ≤ 9λ 0 and X init := i∈I 0 b i a i a T i .</formula><p>We then set the direction d = argmax d 2 =1 d T X init d and x 0 = r d.</p><p>(iii) Small: The outlier-aware initialization we describe in Section 4.2.</p><p>Our convergence results in Section 2 rely on the following four quantities: the quality of the initialization, dist(x 0 , X ); the stability parameter λ such that f (x)f (x ) ≥ λ xx 2 x + x 2 ; the quadratic upper bound guaranteed by our linearized models f x (y), so that | f x (y)-f (y)| ≤ L 2 xy 2 2 and the accuracy to which we solve the optimization problems. The random matrix A governs the middle two quantities-stability λ and closeness L-and we take L = 2 m |||A||| 2 op . Thus, in our experiments we directly vary the initialization scheme to generate x 0 and the accuracy to which we solve the sub-problems <ref type="bibr" target="#b4">(5)</ref>, that is,</p><formula xml:id="formula_119">x k+1 = argmin x 1 m m i=1 a i , x k 2 + 2 a i , x k a i , x -x k -b i + L 2 x -x k 2 2 .</formula><p>We perform each of our experiments on a server with a 16 core 2.6 GhZ Intel Xeon processor with 128 GB of RAM using julia as our language, with OpenBLAS as the BLAS library. We restrict each optimization method to use four of the cores (OpenBLAS is multi-threaded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Simulations with zero noise and Gaussian matrices</head><p>For our first collection of experiments, we evaluate the performance of our method for recovering random signals x ∈ R n using Gaussian measurement matrices A ∈ R m×n , varying the number of measurements m over the ten values m ∈ {1.8n, 1.9n, . . . , 2.7n}. (Taking m ≥ 2.7n yielded 100% exact recovery in all the methods we experiment with.) We assume a noiseless measurement model, so that for</p><formula xml:id="formula_120">A = [a 1 • • • a m ] T ∈ R m×n , we have b i = a i , x 2 for each i ∈ [m].</formula><p>We perform experiments with dimensions n ∈ {400, 600, 800, 1000, 1500, 2000, 3000}, and within each experiment we vary a number of problem parameters, including initialization scheme and the algorithm we use to solve the sub-problems <ref type="bibr" target="#b4">(5)</ref>.</p><p>To serve as our baseline for comparison, we use Truncated Amplitude Flow (TAF) <ref type="bibr" target="#b45">[46]</ref>, given that it outperforms other non-convex iterative methods for phase retrieval, including Wirtinger Flow <ref type="bibr" target="#b11">[12]</ref>, Truncated Wirtinger Flow <ref type="bibr" target="#b14">[15]</ref> and Amplitude Flow <ref type="bibr" target="#b45">[46]</ref>. Setting ψ i = b i , TAF tries to minimize the loss 1 2m m i=1 (ψ i -| a i , x |) 2 via a carefully designed (generalized) gradient method. For convenience, we replicate the method (as described by Wang et al. <ref type="bibr" target="#b45">[46]</ref>) in Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Low-dimensional experiments with accurate prox-linear steps</head><p>We begin by describing our experiments with dimensions n ∈ {400, 600, 800}. For each of these, we solve the iterative sub-problems to machine precision, using Mosek and the Julia package Convex.jl <ref type="bibr" target="#b41">[42]</ref>. We plot representative results for n = 400 and n = 800 in Fig. <ref type="figure" target="#fig_1">1</ref>, which summarize 400 independent experiments, and we generate the true x by taking x ∈ {-1, 1} n uniformly at random. (In separate experiments, we drew x ∼ N(0, I n ), and the results were essentially identical.) In these figures, we use the 'big' initialization (i) (the initialization of Wang et al., as it yields the best empirical performance). Following Wang et al. <ref type="bibr" target="#b45">[46]</ref>, we perform 1000 iterations of TAF (Algorithm 4), and within each experiment, both TAF and the prox-linear method use identical initializer and data matrix A. We run the proxlinear method until sequential updates x k and x k+1 satisfy x kx k+1 2 ≤ = 10 -5 , which in every successful experiment we perform (with these data settings) requires six or fewer iterations. We declare an experiment successful if the output x of the algorithm satisfies dist( x, X ) = min xx 2 , xx 2 ≤ acc x 2 , where acc = 10 -5 .</p><p>(</p><formula xml:id="formula_121">)<label>23</label></formula><p>In Fig. <ref type="figure" target="#fig_1">1</ref>(a), we plot the number of times that one method succeeds (out of the 400 experiments) while the other does not as a function of the ratio m/n. We see that for these relatively small dimensional problems, the prox-linear method has mildly better performance for m/n small than does TAF. In Fig. <ref type="figure" target="#fig_1">1</ref>(b), we plot the fraction of successful runs of the algorithms, again against m/n for n = 400, and in Fig. <ref type="figure" target="#fig_1">1</ref>(c) we plot the fraction of successful runs for n = 800. Even when m/n = 2, the prox-linear method has success rate of around 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Medium dimensional experiments with inaccurate prox-linear steps</head><p>We now shift to a description of our experiments in dimensions n ∈ {1000, 1500, 2000, 3000}, again without noise in the measurements; we perform between 100 and 400 experiments for each dimension in this regime, and we use the 'big' initialization (i) <ref type="bibr" target="#b45">[46]</ref>. In this case, we use Parikh and Boyd's POGS method <ref type="bibr" target="#b33">[34]</ref> for the prox-linear steps, as we describe in Section 5.1. We perform two phases of the prox-linear method; the first performing POGS until the residual errors <ref type="bibr" target="#b20">(21)</ref> are less than = 10 -5 within the prox-linear steps, the second to accuracy = 10 -8 . We apply the prox-linear method to the matrix A/ √ m with data b/m, which is equivalent but numerically more stable because A/ √ m and I n are comparable (see the recommendations <ref type="bibr" target="#b33">[34]</ref>). In these accuracy regimes, the time for solution of the prox-linear method and that required for 1000 iterations of TAF are comparable; TAF requires about 1.5 s while the two-phase prox-linear method requires around 4 s in dimension n = 1000, and TAF requires about 5 s while the two-phase prox-linear method requires around 20 s in dimension n = 2000, each with m = 2n.</p><p>We provide two sets of plots for these results, where again we measure success by the criterion (23), x ± x 2 ≤ acc = 10 -5 . In the first, Fig. <ref type="figure" target="#fig_0">2</ref>, we show performance of prox-linear against TAF specifically for dimensions n = 1000 and 3000. In Fig. <ref type="figure" target="#fig_0">2</ref>(a), we plot the number of trials in which one method succeeds (out of 400 experiments) while the other does not as a function of the ratio m/n. In Fig. <ref type="figure" target="#fig_17">2(b</ref>) we plot the number of trials in which the prox-linear or TAF method succeeds, while the other does not, for n = 3000 with x chosen either N(0, I n ) or uniform in {±1} n ( and + markers, respectively). We ignore ratios m/n ≥ 2.2 as both methods succeed in all of our trials. Out of 100 trials, there is only one (with m/n = 2) in which TAF succeeds, but the prox-linear method does not. In Fig. <ref type="figure" target="#fig_0">2</ref>(c), we plot the fraction of successful runs of the algorithms, again against m/n for n = 3000. For these larger problems, there is a substantial gap in recovery probability between the prox-linear method and TAF, where with m/n = 2 the prox-linear method achieves recovery more than 78% (±4), 88% (±6) and 91% (±6) of the time, with 95% confidence intervals, for n = 1000, 2000 and 3000, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Phase retrieval with outlying measurements</head><p>One of the advantages we claim for the objective (2) is that it is robust to outliers. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> develop a method, which they term median-truncated Wirtinger flow, for handling outlying measurements, which (roughly) sets the index set I k used for updates in Algorithm 4 to be a set of measurements near the median values of the b i . Their algorithm requires a number of parameters that strongly rely on the assumptions that a i iid ∼ N(0, I n ); in contrast, the objective (2) and prox-linear method we investigate are straightforward to implement without any particular assumptions on A (and require no parameter tuning, relying on an explicit sequence of convex optimizations).</p><p>Nonetheless, to make comparisons between the algorithms as fair as possible, we implement their procedure and perform experiments in dimension n ∈ {100, 200} with i.i.d. standard normal data matrices A. We perform 100 experiments as follows. Within each experiment, we evaluate each m ∈ {1.8n, 2n, 3n, 4n, 6n, 8n} and failure probability p fail ∈ {0, 0.01, 0.02, . . . , 0.29, 0.3}. For fixed m, n, we draw a data matrix A ∈ R m×n , then choose x ∈ R n either by drawing x ∼ Uni({-1, 1} n ) or x ∼ N(0, I n ). We then generate b i = a i , x 2 for i ∈ [m]. For our experiments with n = 100, we simply set b i = 0, which is more difficult for our initialization strategy, as it corrupts a large fraction of the vectors a i used to initialize x 0 . For our experiments with n = 200, we draw b i from a Cauchy distribution. Each problem setting (b i Cauchy vs. zeroing and x discrete or normal) yields qualitatively similar results.   accuracy dist( x, X ) ≤ 10 -5 x 2 , while black squares indicate 0% success rates. Within each row of the figure, we present results for the two methods using the same initialization scheme. Fig. <ref type="figure" target="#fig_3">3</ref> gives results with n = 100, using precise (Mosek-based) solutions of the prox-linear updates, while Fig. <ref type="figure">4</ref> gives results with n = 200 using the POGS-based updates (recall step <ref type="bibr" target="#b18">(19)</ref>), with identical parameters as in the previous section. It is clear from the figures that the composite objective yields better recovery.</p><p>In Fig. <ref type="figure" target="#fig_5">5</ref>, we present a different view into the behavior of the prox-linear and median truncated Wirtinger flow (MTWF) methods. In the left two plots, we show the recovery probability (over 100 trials) for our composite optimization method (Fig. <ref type="figure" target="#fig_5">5(a)</ref>) and MTWF (Fig. <ref type="figure" target="#fig_5">5(c)</ref>). The success probability for the composite method is higher. In Fig. <ref type="figure" target="#fig_5">5</ref>(b), we plot the average number of iterations (along with standard error bars) the prox-linear method performs when the dimension n = 100 and we use accurate sub-problem solves. We give iteration counts only for those trials that result in successful recovery; the iteration counts on unsuccessful trials are larger (indeed, if the method is not converging rapidly, this serves as a proxy for failure). In the high measurement regime, m/n ≥ 2.5 or so, we see that if p fail = 0 no more than seven iterations are required: this is the quadratic convergence of the method. (Indeed, for m/n = 8, for p fail ≤ 0.15 each execution of the prox-linear method uses precisely five iterations, never more, and never fewer.) In Fig. <ref type="figure" target="#fig_5">5(d)</ref>, we show the number of matrix multiplications by the inverse matrix (I n + A T DA) -1 the method uses (recall the update <ref type="bibr" target="#b18">(19)</ref> in the proximal graph operator splitting method). We see that for well-conditioned problems and those with little noise, the methods require relatively few matrix multiplications, while for more outliers and when m/n shrinks, there is a non-trivial increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Recovery of real images</head><p>Our final collection of experiments investigates recovery of real-world images using more specialized measurement matrices. In this case, we let H n ∈ {-1, 1} n×n / √ n denote a normalized Hadamard matrix, where the multiplication H n v requires time n log n, and H n satisfies H n = H T n and H 2 n = I n . For some k ∈ N, we then take k i.i.d. diagonal sign matrices S 1 , . . . , S k ∈ diag({-1, 1} n ), uniformly at random, and</p><formula xml:id="formula_122">define A = [H n S 1 H n S 2 • • • H n S k ] T ∈ R kn×n ,</formula><p>as in expression <ref type="bibr" target="#b21">(22)</ref>. We note that this matrix explicitly does not satisfy the stability conditions that we use for our theoretical guarantees. Indeed, letting e 1 and e 2 be the first standard basis vectors, we have H n S(e 1 + e 2 ) ⊥ H n S(e 1e 2 ) no matter the sign  matrix S; there are similarly pathological vectors for FFT, discrete cosine and other structured matrices. Nonetheless, we perform experiments with this structured A matrix as follows. <ref type="foot" target="#foot_5">1</ref>Given an image X represented as a matrix, we define x = Vec(X), the vectorized representation of X. (In the case of colored images, where X ∈ R n 1 ×n 2 ×3 because of the 3 RGB channels, we vectorize the channels as well.) We then set b = φ(Ax), where φ(•) denotes element-wise squaring, and corrupt a fraction p fail ∈ [0, 0.2] of the measurements b i by zeroing them. We then follow our standard experimental protocol, initializing x 0 by the 'small' initialization scheme (iii), with the slight twist that now we use the POGS method with conjugate gradient methods (Section 5.2) to solve the graph projection step <ref type="bibr" target="#b19">(20)</ref>.</p><p>We first give results on a collection of 500 images of handwritten digits (using k = 3), available on the website for the book <ref type="bibr" target="#b24">[25]</ref>. We provide example results of the execution of our procedure in Fig. <ref type="figure">6</ref>, which shows that while there is signal in the initialization, there is substantial work that the prox-linear method must perform to recover the images. For each of the 500 images, we vary p fail ∈ {0, 0.025, 0.05, . . . , 0.175, 0.2}, then execute the prox-linear method. We plot summary results in Fig. <ref type="figure">7</ref>, which in the blue curve with square markers gives the probability of successful recovery of the digit (left axis) vs. p fail (horizontal axis). The right axis indexes the number of matrix multiplications the method executes until completion (black line with circular marks). We see that in spite of the demonstrated failure of the matrix A to satisfy our stability assumptions, we have frequent recovery of the images to accuracy 10 -3 or better.</p><p>Finally, we perform experiments with eight real color images with sizes up to 1024 × 1024 (yielding n = 2 22 -dimensional problems), where we use k = 3 random Hadamard sensing matrices. The proxlinear method successfuly recovers each of the eight images to relative accuracy at least 10 -4 , and performs an average of 15100 matrix-vector multiplications (i.e. fast Hadamard transforms) over the eight experiments, with a standard deviation of 2600 multiplications. To give a sense of the importance of different parts of our procedure, and the relative accuracies to which we solve sub-problems ( <ref type="formula" target="#formula_6">5</ref>), we display one example in Fig. <ref type="figure" target="#fig_7">8</ref>. In this example, we perform phase retrieval on an image of RNA nanoparticles in cancer cells <ref type="bibr" target="#b35">[36]</ref>. In Fig. <ref type="figure" target="#fig_7">8</ref>(a), we show the result of initialization, which displays nontrivial structure, though is clearly noisy. In Fig. <ref type="figure" target="#fig_7">8(b)</ref>, we show the result of 10 steps of the prox-linear method, solving each step using POGS until the residual errors <ref type="bibr" target="#b20">(21)</ref> are below = 10 -3 . There are clear artifacts in this image. We then perform one refinement step with higher accuracy ( = 10 -7 ), which results in Fig. <ref type="figure" target="#fig_7">8(c</ref>). This is indistinguishable, at least to our eyes, from the original image (Fig. <ref type="figure" target="#fig_7">8(d))</ref>.</p><p>respectively. Then</p><formula xml:id="formula_123">1 -| u 1 , v 1 | 2 = |sin θ(u 1 , v 1 )| ≤ |||Δ||| op gap(A) -|||Δ||| op + .</formula><p>We will also have occasion to use the following one-sided variant of Bernstein's inequality.</p><p>Lemma A.2 (One-sided Bernstein inequality) Let X i be non-negative random variables with Var(X i ) ≤ σ 2 . Then</p><formula xml:id="formula_124">P 1 m m i=1 X i ≤ 1 m m i=1 E[X i ] -t ≤ exp - mt 2 2σ 2 .</formula><p>A.1 Proof of Proposition 1</p><p>Our proof uses Mendelson's 'small-ball' techniques for concentration <ref type="bibr" target="#b31">[32]</ref> along with control over a particular VC-dimension condition. If we define</p><formula xml:id="formula_125">h i (u, v) := κ 2 st 1 | a i , u | ∧ | a i , v | ≥ κ st , then we certainly have 1 m m i=1 | a i , u a i , v | ≥ 1 m m i=1 h i (u, v) for all u, v ∈ R n .</formula><p>We now control the class</p><formula xml:id="formula_126">F := {a → f (a) = | a, u | ∧ | a, v | | u, v ∈ R n } by its VC-dimension.</formula><p>Lemma A.3 There exists some numerical constant C &gt; 0, such that, for any c ≥ 0, the VC-dimension of the collection of sets</p><formula xml:id="formula_127">G := x ∈ R n s.t. | x, u | ∧ | x, v | ≥ c | u ∈ R n , v ∈ R n</formula><p>is upper bounded by Cn.</p><p>Proof. The collection of half planes <ref type="bibr">Lemma 2.6.15]</ref>. We have the containment</p><formula xml:id="formula_128">G plane := x ∈ R n s.t. u, x + b ≥ 0 | u ∈ R n , b ∈ R has VC-dimension at most n + 2 [43,</formula><formula xml:id="formula_129">G ⊂ (G 1 ∪ G 2 ) ∩ (G 3 ∪ G 4 ) | G 1 , G 2 , G 3 , G 4 ∈ G ,</formula><p>and so standard results on preservation of VC-dimension under set operations [43, Lemma 2.6.17], imply that the VC-dimension of G is at most C times the VC-dimension of G for some numerical constant C &gt; 0.</p><p>The associated thresholds a </p><formula xml:id="formula_130">→ 1 | a, u | ∧ | a, v | ≥</formula><formula xml:id="formula_131">P sup u,v∈S n-1 1 κ st 1 m m i=1 h i (u, v) -E[h i (u, v)] ≥ c n m + t ≤ 2 exp - mt 2 2</formula><p>for all t ≥ 0, where c &lt; ∞ is a numerical constant. Substitute t 2 → 2t to achieve the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 2</head><p>We begin with a small lemma relating the Frobenius norm of certain rank 2 matrices to the distances between vectors.</p><p>Lemma A.4 Let X = xx Hyy H ∈ C n×n . Then X Fr ≥ inf θ xe ιθ y 2 sup θ xe ιθ y 2 .</p><p>We defer the proof of the lemma to Appendix A.2.1.</p><p>To prove the proposition, we define the function F :</p><formula xml:id="formula_132">C m×n × C n×n , for Z = [z 1 • • • z m ] H , by F(Z, X) = 1 m m i=1 z i , X z i . ( A . 1 )</formula><p>Now, for a constant c &gt; 0 to be chosen, define the truncated variables</p><formula xml:id="formula_133">z i := √ na i / a i 2 1 a i 2 ≥ c √ n . ( A . 2 )</formula><p>Then we have that for any A ∈ C m×n that</p><formula xml:id="formula_134">F(A, xx H -yy H ) ≥ c 2 F(Z, xx H -yy H ).</formula><p>As the function F is homogenous in its second argument, based on Lemma A.4, the result of the theorem will hold if we can show that (with suitably high probability)</p><formula xml:id="formula_135">F(Z, X) ≥ τ 2 4</formula><p>for all rank 2 or less X ∈ C n×n with X Fr = 1.</p><formula xml:id="formula_136">(A.3)</formula><p>It is, of course, no loss of generality to assume that X is Hermitian in inequality (A.3). With our desired inequality (A.3) in mind, we present a covering number-based argument to prove the theorem. The first step in this direction is to lower bound the expectation of F(Z, X). Let X ∈ S r , X = r j=1 λ j u j u H j . Then we have that for w i = √ na i / a i 2 and our choice of</p><formula xml:id="formula_137">z i that E [|z H i X z i |] = E [|w H i X w i |] -E [|w H i X w i |1 { a i 2 ≤ c √ n}].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, we have</head><formula xml:id="formula_138">E |w H i X w i |1 a i 2 ≤ √ n (i) ≤ E[|w H i X w i | 2 ] h(c) (ii) ≤ r i=1 λ 2 j 1 2 E r i=1 | u i , w i | 4 1 2 h(c),</formula><p>where the inequalities follow (i) by Cauchy-Schwartz and Assumption 3 (for the small-ball probability bound h(c)) and (ii) by Cauchy-Schwartz, respectively. As w i is a σ 2 -sub-Gaussian vector, standard results on sub-Gaussians imply that E[| u, w i | 4 ] ≤ (1 + e)σ 4 for any unit vector u, and thus the final expectation has bound r i=1 E[ u i , w i 4 ] ≤ (1 + e)rσ 4 , and thus we obtain for any choice of c in our construction (A.2) of z i and any rank r matrix X that In particular, as we consider only rank r = 2 matrices, Assumption 3 allows us to choose c in the definition (A.2) small enough that</p><formula xml:id="formula_139">E |z H i X z i | ≥ E |w H i X w i | - √ 1 + e X Fr σ 2 rh(c).</formula><formula xml:id="formula_140">h(c) ≤ 1 2(1 + e) • τ 4 σ 4 ,</formula><p>and then</p><formula xml:id="formula_141">E |z H i X z i | ≥ E |w H i X w i | -τ 2 X Fr /2 ≥ τ 2 2 X Fr . ( A . 4 )</formula><p>The remainder of our argument now proceeds by a covering argument, which we begin with a lemma on the continuity properties of F.</p><p>Lemma A.5 Let Hermitian matrices X, Y ∈ C n×n have rank at most r and eigen-decompositions</p><formula xml:id="formula_142">X = U U H and Y = VΣV H , and let Z = [z 1 • • • z m ] H ∈ C m×n . Then |F(Z, X) -F(Z, Y)| ≤ 2 √ r m |||Z||| 2 op U -V 1,2 + Λ -Σ Fr .</formula><p>We provide the proof of Lemma A.5 in Appendix A.2.2. Based on Lemma A.5, we develop a particular covering set of the n × n Hermitian matrices of rank r, following a construction due to Candès and Plan [13, Thm. Lemma A.6 For any &gt; 0, there exists a set S r ⊂ S r of cardinality at most card(S r )</p><formula xml:id="formula_143">≤ (3/ ) (2n+1)r such that inf X∈S r F(Z, X) ≥ min X∈S r F(Z, X) - 4 √ r m |||Z||| 2 op .</formula><p>For our final step, we control the variance of F(Z, Y), so that we can apply the one-sided Bernstein inequality (Lemma A.2) and then a covering argument. For X ∈ S r , then, we have</p><formula xml:id="formula_144">Var z H i X z i ≤ E z H i X z i 2 = E ⎡ ⎣ r j=1 λ j | z i , u j | 2 2 ⎤ ⎦ ≤ X 2 Fr r j=1 E[| z i , u j | 4 ] ≤ r(1 + e) X 2</formula><p>Fr σ 2 by Cauchy-Schwartz and that E[| z i , u j | 4 ] ≤ (1 + e)σ 4 by sub-Gaussian moment bounds. Thus, Lemma A.2 implies that for any fixed X ∈ S r we have</p><formula xml:id="formula_145">P F(Z, X) ≤ τ 2 2 -2(1 + e)r • σ 2 t ≤ e -mt 2</formula><p>for all t ≥ 0. Now, let E t be the event that 1 m |||Z||| 2 op ≤ ε n,m (t) := σ 2 + 11σ 2 max{ √ 4n/m + t, 4n/m + t}. Then by Lemmas A.6 and 3.1 with r = 2, we have for any K and that</p><formula xml:id="formula_146">P inf X∈S r F(Z, X) ≤ K ≤ P min X∈S r F(Z, X) - 4 m |||Z||| 2 op ≤ K ≤ P min X∈S r F(Z, X) - 4 m |||Z||| 2 op ≤ K, E t + P(E t ) ≤ P min X∈S r F(Z, X) -4ε n,m (t) ≤ K + e -mt ≤ 3 (2n+1)r sup X∈S r P F(Z, X) -4ε n,m (t) ≤ K + e -mt . Letting K = τ 2 2 -2 √ (1 + e)σ 2 t -4ε n,m (t)</formula><p>, we thus see that</p><formula xml:id="formula_147">P inf X∈S 2 F(Z, X) ≤ τ 2 2 -2 (1 + e)σ 2 t -4ε n,m (t) ≤ 3 2(2n+1) e -mt 2 + e -mt .</formula><p>Summarizing, we have the following lemma.</p><p>Lemma A.7 There exist numerical constants</p><formula xml:id="formula_148">C 1 ≤ 2 √ (1 + e) and C 2 ≤ 44 such that P inf X∈S 2 F(Z, X) ≤ τ 2 2 -C 1 σ 2 t -C 2 σ 2 max n m + t, n m + t + ≤ exp -mt 2 + 2(2n + 1) log 3 + e -mt .</formula><p>Rewriting this slightly by taking , t τ 2 σ 2 , we see that there exist numerical constants c 0 &gt; 0 and c 1 &lt; ∞ such that</p><formula xml:id="formula_149">P inf X∈S 2 F(Z, X) ≤ τ 2 4 ≤ exp -c 0 m τ 4 σ 4 + c 1 n log σ 2 τ 2 .</formula><p>Now, returning to inequality (A.3), we see we have that for</p><formula xml:id="formula_150">z i = √ na i / a i 2 1{ a i 2 ≥ c √ n} chosen as in (A.</formula><p>2) and c &gt; 0 chosen small enough so that inequality (A.4) holds, we have inf</p><formula xml:id="formula_151">X∈S 2 F(Z, X) ≥ τ 2 4</formula><p>with probability at least </p><formula xml:id="formula_152">X 2 Fr = x 4 2 + y 4 2 -2| x, y | 2 = x 4 2 + y 4 2 -2 Re( x, y ) 2 + Im( x, y ) 2 = 2 x 4 + 2 y 4 -2 Re( x, y ) 2 + Im( x, y ) 2 2 x 4 + 2 y 4 + 2 Re( x, y ) 2 + Im( x, y ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By using that</head><p>x -</p><formula xml:id="formula_153">e ιθ y 2 = x 2 + y 2 -2 (cos θ • Re( x, y ) -sin θ • Im( x, y )),</formula><p>and that the concavity of √ • implies √ 2a 4 + 2b 4 ≥ a 2 + b 2 , the final expression above has lower bound</p><formula xml:id="formula_154">x 2 + y 2 -2 Re( x, y ) 2 + Im( x, y ) 2 x 2 + y 2 + 2 Re( x, y ) 2 + Im( x, y ) 2 = inf θ x -e ιθ y 2 • sup θ x -e ιθ y 2 . A.2.2 Proof of Lemma A.5 We have m|F(Z, X) -F(Z, Y)| ≤ m i=1 r j=1 λ j | u j , z i | 2 -σ j | v j , z i | 2 ≤ m i=1 r j=1 λ j (| u j , z i | 2 -| v j , z i | 2 ) + m i=1 r j=1 (λ j -σ j )| v j , z i | 2 . (A.5)</formula><p>We bound each of the terms in expression (A.5) in term. For the first, we note that for any complex u, v, z ∈ C n , we have</p><formula xml:id="formula_155">| u, z | 2 -| v, z | 2 ≤ z H uu H z -z H vv H z + u H zz H v -v H zz H u = | u -v, z z, u + v |, as u H zz H v -v H zz H u is purely imaginary. As a consequence, we have m i=1 r j=1 λ j (| u j , z i | 2 -| v j , z i | 2 ) = m i=1 r j=1 λ j u j u H j -v j v H j , z i z H i ≤ r j=1 |λ j | m i=1 | z i , u j -v j || z i , u j + v j | ≤ r j=1 |λ j | m i=1 | z i , u j -v j | 2 1 2 m i=1 | z i , u j + v j | 2 ≤ 2 r j=1 |λ j | Z T Z op u j -v j 2 ≤ 2 λ 1 Z T Z op U -V 1,2 ,</formula><p>where we have used that u j 2 = v j 2 = 1 so u j + v j 2 ≤ 2. For the second term in inequality (A.5),</p><p>we have</p><formula xml:id="formula_156">m i=1 r j=1 (λ j -σ j )| v j , z i | 2 ≤ r j=1 |λ j -σ j | m i=1 | v j , z i | 2 ≤ λ -σ 1 Z T Z op .</formula><p>Noting that for vectors λ, σ ∈ R r we have λ 1 ≤ √ r λ 2 , we obtain the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Proposition 3: deterministic part</head><p>Our proof of Proposition 3 eventually reduces to applying eigenvector perturbation results to the random matrices X init . To motivate our approach, note that</p><formula xml:id="formula_157">X init := 1 |I sel | i∈I sel a i a H i = I n -φ( ) • d d H + Δ, ( A . 6 )</formula><p>where Δ is a random error term that we will show has small norm. From the quantity (A.6) we can apply eigenvector perturbation arguments to derive that the directional estimate d = argmin d∈S n-1 d H X init d satisfies d ≈ d . This will hold so long as |||Δ||| op is small because there is substantial separation in the eigenvalues of I n and</p><formula xml:id="formula_158">I n -φ( )d d H .</formula><p>With this goal in mind, we define two index sets that with high probability surround I sel . Let</p><formula xml:id="formula_159">I -:= i ∈ [m] | | a i , d | 2 &lt; 1 - 2 and I + := i ∈ [m] | | a i , d | 2 ≤ 1 + 2 .</formula><p>We now define events E 1 through E 5 , showing that conditional on these five, the result of the proposition holds. These events roughly guarantee (E 1 ) that m i=1 a i a H i is well-behaved, (E 2 and E 3 ) that I + \ I - is small, and (E 4 and E 5 ) that most of the vectors a i for indices i ∈ I sel are close enough to uniform on the subspace perpendicular to d that we have a good directional estimate. Now, let q ∈ [1, ∞] and let 1/p + 1/q = 1, so that p is its conjugate. Recalling the definition of the error Δ( ) in Assumption 5(ii), we define</p><formula xml:id="formula_160">E 1 := 1 m A H A op ∈ [1 -, 1 + ] , E 2 := |I + | ≤ |I -| + 2 κm , E 3 := |I -| ≥ 1 2 mp 0 (d ) , E 4 := ⎧ ⎨ ⎩ 1 m i∈I + \I - a i a H i op ≤ 4qσ 2 (κ ) 1 p ⎫ ⎬ ⎭ E 5 := ⎧ ⎨ ⎩ 1 |I -| i∈I - a i a H i -I n -φ( )d d H op ≤ |||Δ( )||| op + ⎫ ⎬ ⎭ . (A.7)</formula><p>We prove the result of the proposition when each E i occurs. Decompose the matrix X init into</p><formula xml:id="formula_161">X init = I n -φ( )d d H :=Z 0 + ⎡ ⎣ 1 |I -| i∈I - a i a H i -(I n -φ( )d d H ) ⎤ ⎦ :=Z 1 + 1 |I -| i∈I sel \I - a i a H i :=Z 2 - 1 |I -| - 1 |I sel | i∈I sel a i a H i :=Z 3 .</formula><p>We bound the operator norms of Z 1 , Z 2 , Z 3 in turn. On the event E 5 , we have</p><formula xml:id="formula_162">Z 1 op ≤ |||Δ( )||| op + . ( A . 8 )</formula><p>We turn to the error matrix Z 2 . On the event E 1 , we evidently have r ∈ x 2 (1 ± ) by definition of</p><formula xml:id="formula_163">r 2 = 1 m Ax 2 2</formula><p>, so that</p><formula xml:id="formula_164">I -⊂ I sel ⊂ I + .</formula><p>Using that the summands a i a H i are all positive semidefinite, we thus obtain the upper bound</p><formula xml:id="formula_165">Z 2 op = 1 |I -| i∈I sel \I - a i a H i op ≤ 1 |I -| i∈I + \I - a i a H i op (i) ≤ 1 p 0 (d ) • 4qσ 2 (κ ) 1 p , (A.9)</formula><p>where in inequality (i) we use that 2|I -| ≥ p 0 (d )m on E 3 and</p><formula xml:id="formula_166">||| i∈I + \I -a i a H i ||| op ≤ 4qmσ 2 (κ ) 1 p</formula><p>on E 4 . Lastly, we provide an upper bound on Z 3 op . Again using that I -⊂ I sel ⊂ I + on event E 1 , we have 1</p><formula xml:id="formula_167">|I -| - 1 |I sel | ≤ 1 |I -| - 1 |I + | = |I + | -|I -| |I -||I + | ≤ 2 κ p 0 (d ) 2 m</formula><p>, where in the last inequality, we use that</p><formula xml:id="formula_168">|I + |-|I -| ≤ 2 κm on E 2 and that |I + | ≥ |I -| ≥ p 0 (d )m/2 on E 3 .</formula><p>Thus, by the definition of E 1 , we have</p><formula xml:id="formula_169">Z 3 op ≤ 2 κ p 0 (d ) 2 1 m m a i a H i op ≤ 2 (1 + )κ p 0 (d ) 2 . (A.10)</formula><p>Combining inequalities (A.8), (A.9) and (A.10) on the error matrices Z i , the triangle inequality gives</p><formula xml:id="formula_170">Z 1 + Z 2 + Z 3 op ≤ |||Δ( )||| op + ⎡ ⎣ + 4qσ 2 (κ ) 1 p p 0 (d ) + 2 (1 + )κ p 0 (d ) 2 ⎤ ⎦ :=ν( ) .</formula><p>This implies equality (A.6) with error bound</p><formula xml:id="formula_171">|||Δ||| op ≤ |||Δ( )||| op + ν( ). Recall the definition of Z 0 = I n -φ( )d d H</formula><p>, which has smallest eigenvector d and eigengap φ( ). Lastly, we simplify ν( ) by a specific choice of p and q in the definition (A.7) of E 4 . Without loss of generality, we assume κ &lt; 1 (recall Assumption 5 on κ), and define p = 1 + 1 log 1 κ and q = 1 + log 1 κ . Using that for any z &lt; 0 we have exp( z 1-1/z ) ≤ e z+1 and (κ )</p><formula xml:id="formula_172">1</formula><p>p ≤ eκ , allowing us to bound q(κ )</p><formula xml:id="formula_173">1</formula><p>p ≤ e 1 + log 1 κ (κ ) in the error term ν( ).</p><p>We now apply the eigenvector perturbation inequality of Lemma A.1. Using that, for</p><formula xml:id="formula_174">θ ∈ R, u -e ιθ v 2 2 ≥ 2-2| u, v | ≥ 2-2| u, v | 2 for u 2 = v 2 = 1, a minor rearrangement of Lemma A.1 applied to X init = Z 0 + Δ for Δ = Z 1 + Z 2 + Z 3 yields dist d, 1 r X = inf θ∈[0,2π ] d -e ιθ d 2 ≤ √ 2(|||Δ( )||| op + ν( )) φ( ) -(|||Δ( )||| op + ν( )) + .</formula><p>Finally, using that x 0 = r d and defining r = x , we have by the triangle inequality that dist</p><formula xml:id="formula_175">(x 0 , X ) ≤ r dist d, 1 r X + |r -r| (i) ≤ √ 1 + dist( d, d ) + r ,</formula><p>where inequality (i) uses E 1 , which we recall implies</p><formula xml:id="formula_176">(1 -) x 2 2 ≤ r 2 ≤ (1 + ) x 2 2</formula><p>, where ∈ [0, 1]. This is the claimed consequence (14) in Proposition 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Proposition 3: high probability events</head><p>It remains to demonstrate that each of the events E 1 , . . . , E 5 (recall definition (A.7)) holds with high probability, to which we dedicate the remainder of this argument in the next series of lemmas, each of which argues that one of the five events occurs with high probability. Before the statement of each lemma, we recall the corresponding event whose high probability we wish to demonstrate. Throughout, c &gt; 0 and C &lt; ∞ denote numerical constants whose values may change. <ref type="bibr" target="#b7">8</ref> We have P(E 1 ) ≥ 1exp(-cm 2 /σ 4 ) for m large enough that m/n ≥ σ 4 /(c 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We begin with E</head><formula xml:id="formula_177">1 := { 1 m |||A H A||| op ∈ [1 ± ]}. Lemma A.</formula><p>Proof. Set t = c Lemma A.9 We have P(E 2 ) ≥ 1exp(-2 2 κ 2 m).</p><p>Proof. We always have that</p><formula xml:id="formula_178">I -⊂ I + and I + \ I -= i ∈ [m] : 1 2 (1 -) ≤ | a i , d | 2 ≤ 1 2 (1 + ) .</formula><p>Therefore, the difference in cardinalities of |I -| and |I + is</p><formula xml:id="formula_179">1 m |I + | -|I -| = 1 m m i=1 1 1 -≤ 2| a i , d | 2 ≤ 1 + .</formula><p>The right-hand side is an average of i. Showing that events E 4 and E 5 in Equation (A.7) each happen with high probability requires a little more work. We begin with E 4 , defined in terms of a conjugate pair p, q ≥ 1 with 1/p + 1/q = 1, as</p><formula xml:id="formula_180">E 4 := {||| i∈I + \I -a i a H i ||| op ≤ 4qσ 2 (κ ) 1 p }. Lemma A.11 If m/n &gt; c -1 (κ ) -2 p , then P(E 4 ) ≥ 1 -exp(-cm(κ ) 2 p ).</formula><p>It is no loss of generality to assume that κ ≤ 1 by Assumption 5, so (κ )</p><formula xml:id="formula_181">2 p ≥ (κ ) 2 .</formula><p>Proof. The {a i } m i=1 are independent σ 2 -sub-Gaussian random vectors by Assumption 4, and for any random variable B i with |B i | ≤ 1, which may depend on a i , it is clear that the collection {B i a i } m i=1 are mutually independent and still satisfy Definition 3.1. To that end, define the Bernoulli variables</p><formula xml:id="formula_182">B(a) = 1 | a, d | 2 ∈ 1- 2 , 1+ 2 (letting B i = B(a i ) = 1 i ∈ I + \ I -for shorthand). Then Lemma 3.1 implies for a numerical constant C &lt; ∞ that P 1 m m i=1 a i a H i B i -E[aa H B(a)] op ≥ Cσ 2 max n m + t, n m + t ≤ e -mt . (A.11)</formula><p>Now, note by Hölder's inequality that</p><formula xml:id="formula_183">E[ v, a 2 B(a)] ≤ E[ v, a 2q ] 1 q P | a, d | 2 ∈ 1 - 2 , 1 + 2 1 p</formula><p>≤ (σ 2q (q + 1)e) 1 q (κ )  <ref type="bibr">.11)</ref>, we find that for any q ∈ (1, ∞) and</p><formula xml:id="formula_184">1 p ≤ qe 1/q σ 2 (κ )</formula><formula xml:id="formula_185">1/p + 1/q = 1, if n m ≤ 1 4C (κ ) 2 p we have P 1 m m i=1 a i a H i B i -E[aa H B(a)] op ≥ qσ 2 (κ ) 1 p ≤ exp ⎛ ⎝ - m(κ ) 2 p 4C ⎞ ⎠ .</formula><p>Applying the triangle inequality and that 1 + e 1/q &lt; 4 gives the result.</p><p>The final high probability guarantee is the most complex, and applies to the event</p><formula xml:id="formula_186">E 5 := 1 |I -| i∈I -a i a H i -(I n -φ( )d d H ) op ≤ |||Δ( )||| op + . Lemma A.12 Let E 3 = {|I -| ≥ 1 2 mp 0 (d )} as in Equation (A.7). Then c m n ≥ σ 4 log 2 p 0 (d ) p 0 (d ) 2 implies P(E 5 | E 3 ) ≥ 1 -exp -c mp 0 (d ) 2 σ 4 log 2 p 0 (d )</formula><p>.</p><p>Proof. For notational simplicity, define the following shorthand:</p><formula xml:id="formula_187">E d := E aa H | | a, d | 2 ≤ 1 - 2 = I n -φ( )d d H + Δ( ),</formula><p>where the equality uses Assumption 5 (ii). The main idea of the proof is to show the following crucial fact: define the new sub-Gaussian parameter τ 2 = σ 2 log e p 0 (d ) ≥ 1. Then there exists a numerical constant 1 ≤ C &lt; ∞ such that for all t ≥ 0,</p><formula xml:id="formula_188">P ⎛ ⎝ 1 |I -| i∈I - a i a H i -E d op ≥ Cτ 2 max n |I -| + t, n |I -| + t | I - ⎞ ⎠ ≤ exp(-|I -|t).</formula><p>(A.12)</p><p>Suppose that the bound (A.12) holds. On the event E 3 , we have that |I -| ≥ </p><formula xml:id="formula_189">P ⎛ ⎝ 1 |I -| i∈I - a i a H i -E d op ≥ | I - ⎞ ⎠ ≤ exp - mp 0 (d ) 2 8C 2 τ 4 .</formula><p>By the definition of E d and the triangle inequality we have</p><formula xml:id="formula_190">P ⎛ ⎝ 1 |I -| i∈I - a i a H i -(I n -φ ( ) d d H ) op ≥ |||Δ( )||| op + | I - ⎞ ⎠ ≤ P ⎛ ⎝ 1 |I -| i∈I - a i a H i -E d op ≥ | I - ⎞ ⎠ .</formula><p>The lemma follows from the fact that E 3 is measurable with respect to the indices I -. Now, we show the key inequality (A.12). The main idea is to show that, conditioning on the set I -, the distribution {a i } i∈I -is still conditionally independent and sub-Gaussian. To do so, we introduce a bit of (more or less standard) notation. For a random variable X, let L(X) denote the law of distribution of X. Using the independence of the vectors a i , we have the fact that for any fixed subset I ⊂ [m], the collection {a i } i∈I is independent of {a i } i ∈I . Therefore, using the definition that</p><formula xml:id="formula_191">I -= i ∈ [m] : | a i , d | 2 ≤ 1-</formula><p>2 , we have the key identity</p><formula xml:id="formula_192">L 1 |I -| i∈I - a i a H i | I -= I dist = L 1 |I| i∈I a i a H i | max i∈I | a i , d | 2 ≤ 1 - 2 .</formula><p>This implies that, conditioning on I -= I, the vectors {a i } i∈I are still conditionally independent, and their conditional distribution is identical to the law</p><formula xml:id="formula_193">L a | | a, d | 2 ≤ 1- 2 .</formula><p>The claimed inequality (A.12) will thus follow by the matrix concentration inequality in Lemma 3.1, so long as we can demonstrate appropriate sub-Gaussianity of the conditional law</p><formula xml:id="formula_194">L a | | a, d | 2 ≤ 1- 2 . Indeed, let us temporarily assume that a | | a, d | 2 ≤ 1-</formula><p>2 is τ 2 -sub-Gaussian, let J denote all subsets I ⊂ [m] such that |I| ≥ 1  2 mp 0 (d ), and define the shorthand</p><formula xml:id="formula_195">E d = E aa H | | a, d | 2 ≤ 1- 2 .</formula><p>Then by summing over J , we have on the event E 3 that for a numerical constant C &lt; ∞,</p><formula xml:id="formula_196">P ⎛ ⎝ 1 |I -| i∈I - a i a H i -E d op ≥ Cτ 2 max n |I -| + t, n |I -| + t | E 3 ⎞ ⎠ = I∈J P 1 |I| i∈I a i a H i -E d op ≥ Cτ 2 max n |I| + t, n |I| + t | I -= I P(I = I -| E 3 ) = I∈J P 1 |I| i∈I a i a H i -E d op ≥ Cτ 2 max n |I| + t, n |I| + t | max i∈I | a i , d | 2 ≤ 1 -<label>2</label></formula><formula xml:id="formula_197">P(I = I -| E 3 ) (i) ≤ I∈J e -|I|t • P(I = I -| E 3 ) ≤ e -1 2 mp 0 (d )t ,</formula><p>where inequality (i) is an application of Lemma 3.1. This is evidently inequality (A.12) with appropriate choice of τ 2 . We thus show that</p><formula xml:id="formula_198">L a | | a, d | 2 ≤ 1- 2</formula><p>is sub-gaussian with parameter τ 2 = σ 2 log e p 0 (d ) by bounding the conditional moment generating function. Let λ ∈ [1, ∞] and λ be conjugate, so that 1/λ + 1/λ = 1. Then by Hölder's inequality, for any v ∈ S n-1 we have</p><formula xml:id="formula_199">E exp | a, v | 2 λσ 2 | | a, v | 2 ≤ 1 - 2 = E exp | a,v | 2 λσ 2 1 | a, v | 2 ≤ 1- 2 P | a, v | 2 ≤ 1- 2 ≤ E exp | a,v | 2 σ 2 1 λ P | a, v | 2 ≤ 1- 2 1 λ P | a, v | 2 ≤ 1- 2 = E exp | a, v | 2 σ 2 1 λ P | a, v | 2 ≤ 1 - 2 -1 λ ≤ e p 0 (d ) 1 λ</formula><p>, where the final inequality uses the σ 2 -sub-Gaussianity of a. Set λ = log e p 0 (d ) to see that conditional on | a, d | 2 ≤ 1- 2 , the vector a is σ 2 log e p 0 (d ) -sub-Gaussian, as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs for phase retrieval with outliers</head><p>In this section, we collect the proofs of the various results in Section 4. Before providing the proofs, we state one inequality that we use frequently that will be quite useful. Let W i ∈ {0, 1} satisfy W i = 1 if i ∈ I out and W i = 0 otherwise, so that W indexes the outlying measurements. Because W i are independent of the a i vectors and i W i = p fail m, Lemma 3.1 implies for all t ≥ 0 that</p><formula xml:id="formula_200">P 1 m m i=1 W i a i a T i -p fail E[a i a T i ] op ≥ Cσ 2 max n m + t, n m + t ≤ e -mt . (B.1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Proposition 4</head><p>Recalling the set I out of outlying indices, we evidently have</p><formula xml:id="formula_201">f (x) -f (x ) = 1 m m i=1 | a i , x 2 -a i , x 2 | + 1 m i∈I out | a i , x 2 -ξ i | -| a i , x 2 -a i , x 2 | -f (x ) = (Ax) 2 -(Ax ) 2 1 m + 1 m i∈I out | a i , x 2 -ξ i | -| a i , x 2 -a i , x 2 | -| a i , x 2 -ξ i | ≥ (Ax) 2 -(Ax ) 2 1 m - 2 m i∈I out | a i , x 2 -a i , x 2 |. ( B . 2 )</formula><p>Now, we note the trivial fact that if we define W i = 1 for i ∈ I out and W i = 0 for i ∈ I in , then</p><formula xml:id="formula_202">i∈I out | a i , x 2 -a i , x 2 | = m i=1 W i |(x -x ) T a i a T i (x + x )| ≤ A T diag(W)A op x -x 2 x + x 2 .</formula><p>Inequality (B.1) shows that the matrix 1 m m i=1 W i a i a T i is well concentrated. </p><formula xml:id="formula_203">f (x) -f (x ) ≥ 1 m (Ax) 2 -(Ax ) 2 1 -2(p fail + Cσ 2 n/m + t) x -x 2 x + x 2</formula><p>for all x ∈ R n with probability at least 1e -mt by inequality (B.1). We finish with the following lemma, which is a minor sharpening of Theorem 2.4 of Eldar and Mendelson <ref type="bibr" target="#b18">[19]</ref> so that we have sharp concentration in all dimensions n. We provide a proof for completeness in Appendix C.2.</p><p>Lemma B.1 Let a i be independent σ 2 -sub-Gaussian vectors, and define κ st (u, v)</p><formula xml:id="formula_204">:= E[| a, u a, v |] for u, v ∈ R n . Then there exist a numerical constants c &gt; 0 and C &lt; ∞ such that 1 m m i=1 | a i , u a i , v | ≥ κ st (u, v) -Cσ 2 3 n m -σ 2 t for all u, v ∈ S n-1</formula><p>with probability at least 1e -cmt 2e -cm when m/n ≥ C.</p><p>Noting that | a i , x 2a i , x 2 | = | a i , xx a i , x + x | and substituting the result of the lemma into the preceding display, we have</p><formula xml:id="formula_205">f (x) -f (x ) ≥ κ st -2p fail -Cσ 2 3 n m -Cσ 2 t x -x 2 x + x 2</formula><p>uniformly in x with probability at least 1 -2e -cmt 2e -cm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 5</head><p>We first state a lemma providing a deterministic bound on the errors of the minimizing radius.</p><formula xml:id="formula_206">Lemma B.2 Let δ = 6 x 2 2 A T A op i∈I in a i , x 2 -i∈I out a i , x 2 dist( d, {±d }). ( B . 3 ) If δ ≤ 1, then all minimizers of G(•) belong to the set [1 ± δ] x 2 2 .</formula><p>Temporarily assuming the conclusions of the lemma, let us show that the random quantities in the bound (B.3) are small with high probability. We apply the matrix concentration inequality (B.1) to see that for a numerical constant C &lt; ∞ and all t ∈ [0, 1</p><formula xml:id="formula_207">-n m ], we have 1 m i∈I in a i , v 2 ≥ (1 -p fail ) -Cσ 2 n m + t and 1 m i∈I out a i , v 2 ≤ p fail + Cσ 2 n m + t</formula><p>with probability at least 1e -mt for all vectors v ∈ S n-1 , and with probability at least 1 -2e -cm(1-2p fail )<ref type="foot" target="#foot_8">2</ref> /σ 4 , where C is a numerical constant. This is our desired result.</p><p>Proof. We define a few pieces of notation for shorthand. Let</p><formula xml:id="formula_208">σ 2 := 1 m A T A op and σ 2 I out := 1 m i∈I in a i , x 2 - 1 m i∈I out a i , x 2 ,</formula><p>and define the functions</p><formula xml:id="formula_209">g(δ) = G((1 + δ) x 2 2 ), equivalently g(δ) := 1 m m i=1 |b i -x 2 2 a i , d (1 + δ)|,</formula><p>and a slightly more accurate counterpart</p><formula xml:id="formula_210">g(δ) := 1 m m i=1 b i -(1 + δ) a i , x 2 = 1 m i ∈I in a i , x 2 -(1 + δ) a i , x 2 + 1 m i∈I out b i -(1 + δ) a i , x 2 .</formula><p>Note that if δ minimizes g(δ), then (1 + δ) x 2 2 minimizes G(r). By inspection we find that the subgradients of g with respect to δ are</p><formula xml:id="formula_211">∂ δ g(δ) = 1 m i∈I in sgn(δ) a i , x 2 - 1 m i∈I out sgn((1 + δ) a i , x 2 -b i ) a i , x 2 ,</formula><p>where sgn(t) = 1 if t &gt; 0, \1 if t &lt; 0, and sgn(0) = [\1, 1]. Evidently, for δ &gt; 0 we have g (δ) ≥ σ 2</p><formula xml:id="formula_212">I out and g (-δ) ≤ -σ 2 I out , so that g(δ) ≥ σ 2 I out |δ| + g(0). ( B . 4 )</formula><p>Now, we consider the gaps between g and g; for δ ∈ [\1, 1], we have the gap</p><formula xml:id="formula_213">|g(δ) -g(δ)| ≤ (1 + δ) x 2 2 m m i=1 | a i , d 2 -a i , d 2 | ≤ (1 + δ) x 2 2 m A T A op d -d 2 d + d 2 ≤ 4 x 2 2 σ 2 dist(d , {± d}),</formula><p>where we have used the triangle inequality and Cauchy-Schwarz. Thus, we obtain</p><formula xml:id="formula_214">g(δ) -g(0) ≥ g(δ) -g(0) + g(δ) -g(δ) + g(0) -g(0) ≥ σ 2 I out |δ| -6 x</formula><p>where we have applied inequality (B.4). Rearranging, we have that if g(δ) ≤ g(0) we must have</p><formula xml:id="formula_215">|δ| ≤ 6 x 2 2 σ 2 σ 2 I out dist( d, {±d }).</formula><p>By convexity, any minimizer of g must thus lie in the above region, which gives the result when we recall that minimizers δ of g are equivalent to minimizers (1 + δ) x We decompose the matrix X init into four matrices, each of which we control to guarantee that X init ≈ I ncd d T for some constant c, thus guaranteeing d ≈ d . Let P = d d T and P ⊥ = I nd d T be the projection operator onto the span of d and its orthogonal complement. Then we may decompose the matrix X init into the four parts</p><formula xml:id="formula_216">X init = 1 m m i=1 Pa i a T i P1 i ∈ I in sel :=Z 0 + 1 m m i=1 Pa i a T i P ⊥ + P ⊥ a i a T i P 1 i ∈ I in sel :=Z 1 + 1 m n i=1 P ⊥ a i a T i P ⊥ 1 i ∈ I in sel :=Z 2 + 1 m m i=1 a i a T i 1 i ∈ I out sel :=Z 3 . (B.5)</formula><p>Let us briefly motivate this decomposition. We expect that Z 0 should be small because we choose indices I sel by taking the smallest values of b i , which should be least correlated with d (recall the P = d d T ). We expect Z 1 to be small because of the independence of the vectors Pa i and P ⊥ a i for Gaussian measurement vectors, and Z 3 to be small because I out sel should be not too large. This leaves Z 2 , which (by Gaussianity) we expect to be some multiple of I nd d T , which will then allow us to apply eigenvector perturbation guarantees using the eigengap of the matrix X init .</p><p>The rotational invariance of the Gaussian means that it is no loss of generality to assume that d = e 1 , the first standard basis vector, so for the remainder of the argument we assume this without further comment. This means that we may decompose a i as</p><formula xml:id="formula_217">a i = [a i,1 a i,2 • • • a i,n ] T = [a i,1 a T i,\1</formula><p>] T , which we will do without further comment for the remainder of the proof.</p><p>We now present four lemmas, each controlling one of the terms Z l in the expansion (B.5). We defer proofs of each of the lemmas to the end of this argument. We begin by considering the Z 0 term, which (because P is rank 1) satisfies</p><formula xml:id="formula_218">Z 0 = 1 m m i=1 a i , d 2 1 i ∈ I in sel :=z 0 d d T . constants z 0 = 1 m i∈I in sel a i , d 2 , z 2 = 1 m |I in sel | and z 3 = 1 m |I out sel |, guarantees that X init = Z 0 + Z 1 + Z 2 + Z 3 = z 0 d d T + z 2 (I n -d d T ) + z 3 I n + Δ = (z 2 + z 3 )I n -(z 2 -z 0 )d d T + Δ,</formula><p>where the perturbation Δ ∈ R n×n satisfies</p><formula xml:id="formula_219">|||Δ||| op ≤ Z 1 op + Z 2 -z 2 (I n -d d T ) op + Z 3 -z 3 I n op under model M1 Z 3 op under model M2.</formula><p>On the event that z 2 &gt; z 0 , the minimal eigenvector of (z</p><formula xml:id="formula_220">2 + z 3 )I n -(z 2 -z 0 )d d T is d with eigengap z 2 -z 0 . Applying Lemma A.1 gives that d = argmin d∈S n-1 d T X init d satisfies 2 -1 2 dist( d, {±d }) ≤ 1 -d, d 2 ≤ ||| ||| op z 2 -z 0 -||| ||| op + . ( B . 6 )</formula><p>Applying Lemmas B.4 and B.5, we have for some numerical constant C &lt; ∞ that</p><formula xml:id="formula_221">Z 1 op + Z 2 -z 2 (I n -d d T ) op ≤ C n m + t with probability ≥ 1 -e -mt -e -m/2</formula><p>for any t ≥ 0. We now consider the two noise models in turn. Under Model M1, Lemma B.6 then implies that |||Δ||| op ≤ C n m + t with probability at least 1e -mte -m/2 . Recalling Lemma B.3, we have for the constants w 2 q and δ q &gt; 0 defined in the lemma that z 2 ≥ z 0 + 1-2p fail 2 δ q with probability at least 1exp(-c(1 -2p fail )m)exp(-cmδ 2 q /w 2 q ), where c &gt; 0 is a numerical constant. That is, the perturbatoin inequality (B.6) implies that under Model M1, we have with probability at least 1exp(-mt)exp(-c(1</p><formula xml:id="formula_222">-2p fail )m) -exp(-cmδ 2 q /w 2 q ) that 2 -1 2 dist( d, {±d }) ≤ C √ n/m + t (1 -2p fail )δ q -C √ n/m + t + ,</formula><p>where 0 &lt; c, C &lt; ∞ are numerical constants. Under Model M2, we can bound Z 3 op by p fail + C √ n/m + t with probability 1e -mte -m/2 (recall Lemma B.6), so that with the same probability as above, we have</p><formula xml:id="formula_223">2 -1 2 dist( d, {±d }) ≤ p fail + C √ n/m + t (1 -2p fail )δ q -C √ n/m + t -p fail + under Model M2.</formula><p>This is the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 Proof of Lemma B.3</head><p>As noted earlier, it is no loss of generality to assume that d = e 1 , the first standard basis vector, so that using our definitions of z</p><formula xml:id="formula_224">0 = 1 m i∈I in sel a i , d 2 and z 2 = 1 m |I in sel |, we have z 2 -z 0 = 1 m m i=1 (1 -a 2 i,1 )1 i ∈ I in sel . ( B . 7 )</formula><p>Given that we choose in indices I sel by a median, it is helpful to have the following median concentration result.</p><p>Lemma B.7 Let {W i } m i=1 iid ∼ N(0, 1). Fix p ∈ (0, 1) and choose w q ≥ 0 so that q := P(W 2 ≤ w 2 q ) = 2(1 -Φ(w q )) &gt; p. Then</p><formula xml:id="formula_225">P quant p {W 2 i } m i=1 ≥ w 2 q ≤ exp - m(q -p) 2 2(q -p)/3 + 2q(1 -q) . Proof. Note that quant p ({W 2 i } m i=1 ) ≥ w 2 q if and only if 1 m m i=1 1 W 2 i ≤ w 2 q ≤ p. Using that Var 1 W 2 i ≤ w 2 q</formula><p>= q(1-q), Bernstein's inequality applied to the i.i.d. sum m i=1 (1{W 2 i ≤ w 2 q }-q) gives the result.</p><p>We now control the median of the perturbed vector b ∈ R m . Since we have |I out | ≤ p fail m, we have deterministic result</p><formula xml:id="formula_226">med {b i } m i=1 ≤ quant 1 2(1-p fail ) { a i , x 2 } i∈I in ,</formula><p>so by upper bounding the right-hand quantity we can upper bound med({b i }), which in turn allows us to control I sel . By the definition of w q and q fail in Equation ( <ref type="formula" target="#formula_96">17</ref>), which satisfies δ = q fail -1 2(1-p fail ) = 1-2p fail 4(1-p fail ) , Lemma B.7 with q = q fail and the fact that |I in | = (1p fail )m then implies</p><formula xml:id="formula_227">P med({b i } m i=1 ) ≥ w 2 q x 2 2 ≤ P quant (2(1-p fail )) -1 ({ a i , x 2 } i∈I in ) ≥ w 2 q x 2 2 ≤ exp - m(1 -p fail )δ 2 2δ/3 + 2δ(1 -δ) = exp - 3(1 -2p fail )m 4(2 + 6(1 -δ)) ≤ exp - 3 32 (1 -2p fail )m . (B.8)</formula><p>We now consider the indices i that are inliers for which a i , d 2 is small; again letting w q ≥ 0 be defined as in the quantile (17), we define</p><formula xml:id="formula_228">I in q := i ∈ I in | a i , d 2 ≤ w 2 q = i ∈ I in | a i , x 2 ≤ w 2 q x 2 2 .</formula><p>Lemma B.8 Let the set of inliers I in q be defined as above, and let</p><formula xml:id="formula_229">δ q = 1 -E[W 2 | W 2 ≤ w 2 q ]</formula><p>for W ∼ N(0, 1). Then for all t ≥ 0 we have</p><formula xml:id="formula_230">P ⎛ ⎜ ⎝ 1 |I in q | i∈I in q (1 -a 2 i,1 ) ≤ δ q -t | I in q ⎞ ⎟ ⎠ ≤ exp - 2|I in q |t 2 w 2 q .</formula><p>We defer the proof of Lemma B. </p><formula xml:id="formula_231">P ⎛ ⎜ ⎝ 1 |I in q | i∈I in q (1 -a 2 i,1 ) ≤ δ q -t ⎞ ⎟ ⎠ ≤ exp - mt 2 4w 2 q P(|I in q | ≥ m/8) + P(|I in q | ≤ m/8) ≤ exp - mt 2 4w 2 q + exp - m 4 .</formula><p>Using the notation δ q in Lemma B.8, for t ∈ [0, 1] we define the event</p><formula xml:id="formula_232">E := ⎧ ⎪ ⎨ ⎪ ⎩ med({b i } m i=1 ) ≤ w 2 q x 2 2 , |I in q | ≥ m 8 , 1 |I in q | i∈I in q (1 -a 2 i,1 ) ≥ (1 -t)δ q ⎫ ⎪ ⎬ ⎪ ⎭ .</formula><p>We immediately find that</p><formula xml:id="formula_233">P(E) ≥ 1 -exp - 1 10 (1 -2p fail )m -2 exp - m 4 -exp - mt 2 δ 2 q 4w 2 q (B.9)</formula><p>by the preceding display and inequality (B.8). Recalling the set I in q = {i ∈ I in : a i , d 2 ≤ w 2 q }, we have on the event E that the selected inliers satisfy I in sel ⊂ I in q (because med(b) ≤ w 2 q x 2 2 ). Because of our selection mechanism with I in sel as the smallest b i in the sample, we have that for t ≥ 0. Recalling the definition δ q := 1 -E[W 2 | W 2 ≤ w 2 q ] for W ∼ N(0, 1), we thus find that for any index set I 0 ⊂ [m], we have</p><formula xml:id="formula_234">1 |I in sel | i∈I in sel (1 -a 2 i,1 ) ≥ 1 |I in q | i∈I in q | (1 -a 2 i,1</formula><formula xml:id="formula_235">P ⎛ ⎜ ⎝ 1 |I in q | i∈I in q (1 -a 2 i,1 ) ≤ δ q -t | I in q = I 0 ⎞ ⎟ ⎠ = P ⎛ ⎝ 1 |I 0 | i∈I 0 (1 -a 2 i,1 ) ≤ δ q -t | a 2 i,1 ≤ w 2 q for i ∈ I 0 ⎞ ⎠ ≤ exp - 2|I 0 |t 2 w 2 q .</formula><p>Noticing that the random vectors {a i } i∈I 0 are independent of {a i } i ∈I 0 , we have for any measurable set C ⊂ R that</p><formula xml:id="formula_236">P ⎛ ⎜ ⎝ i∈I in q (1 -a 2 i,1 ) ∈ C | I in q = I 0 ⎞ ⎟ ⎠ = P ⎛ ⎝ i∈I 0 (1 -a 2 i,1 ) ∈ C | a 2 i,1 ≤ w 2 q for i ∈ I 0 ⎞ ⎠ .</formula><p>Combining the preceding two displays yields Lemma B.8. ∼ N(0, I n ) be an independent collection of vectors, the collections {a i } i∈I in and {ξ i } i∈I out are independent, as are a i,1 and a i,\1 for each i. Thus, because I in sel ⊂ I in , that for any measurable set C ⊂ R n-1 we have  The proof is an essentially standard concentration and covering number argument, with a few minor wrinkles. </p><formula xml:id="formula_237">|Z u,v -Z u ,v | ≤ 1 m m i=1 | a i , u a i , v -a i , u a i , v | ≤ 1 m m i=1 | a i , u -u a i , v | + | a i , u a i , v -v | ≤ 1 m A(u -u ) 2 Av 2 + 1 m A(v -v ) 2 Au 2 ≤ |||A||| 2 op m u -u 2 + v -v 2 .</formula><p>Now, let N be an -cover of the sphere S n-1 , which we use to control inf valid for all ≤ 0 . Now, if we set = 3 √ n/m and define t = σ 2 t + (C + 1)σ 2 , then we find that if m/n ≥ -3 0 , we have</p><formula xml:id="formula_238">P inf u,v∈S n-1 {Z u,v -κ st (u, v)} ≤ -σ 2 t -(C + 1)σ 2 3 n m ≤ exp cmt 2 + e -m .</formula><p>This is the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Properties of Gaussian random variable</head><p>Lemma C.1 Let W ∼ N(0, 1). Then for all c ∈ R +</p><formula xml:id="formula_239">E W 2 | W 2 ≤ c ≤ 1 - 1 2 cP(W 2 &gt; c).</formula><p>Proof. By the law of total probability, we have</p><formula xml:id="formula_240">E[W 2 ] = E[W 2 | W 2 ≤ c]P(W 2 ≤ c) + E[W 2 | W 2 &gt; c]P(W 2 &gt; c).</formula><p>Using that E[W 2 ] = 1 yields  Proof. Without loss of generality, we assume that X Fr = 1, so that X = λ 1 u 1 u H 1 + λ 2 u 2 u H 2 for λ 2  1 + λ 2 2 = 1 and u i orthonormal. Thus, if Z 1 , Z 2 are standard normal, we have</p><formula xml:id="formula_241">1 -E[W 2 | W 2 ≤ c] = P(W 2 &gt; c) E[W 2 | W 2 &gt; c] -E[W 2 | W 2 ≤ c] .</formula><formula xml:id="formula_242">z H Xz = λ 1 | z, u 1 | 2 + λ 2 | z, u 2 | 2 dist = λ 1 Z 2 1 + λ 2 Z 2 2 .</formula><p>Without loss of generality, we assume λ 1 ≥ 0, so that by inspection we have for λ</p><formula xml:id="formula_243">= λ 1 that E[|λ 1 Z 2 1 + λ 2 Z 2 2 |] ≥ E[|λZ 2 1 - √ 1 -λ 2 Z 2 2 |]. Letting f (λ) = E[|λZ 2 1 - √ 1 -λ 2 Z 2 2 |],</formula><p>we have that</p><formula xml:id="formula_244">f (λ) = P λ √ 1 -λ 2 Z 2 1 ≥ Z 2 2 -P λ √ 1 -λ 2 Z 2 1 ≤ Z 2 2 = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ &gt; 0 if λ &gt; 1 √ 2 = 0 if λ = 1 √ 2 &lt; 0 if λ &lt; 1 √ 2 .</formula><p>In particular, f is quasi-convex and minimized at λ = 1/ √ 2, and thus</p><formula xml:id="formula_245">1 X Fr E[|z H Xz|] ≥ 1 √ 2 E[|Z 2 1 -Z 2 2 |] = 2 √ 2 π ,</formula><p>where the final equality is a calculation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>q w 2 q</head><label>2</label><figDesc>as in Algorithm 3. Let the constant M = 0 if Model M1 holds and M = 1 if Model M2 holds. There are numerical constants 0 &lt; c, C &lt; ∞ such that for t ≥ 0, with probability at least 1exp(-mt)exp(-c(1 -2p fail )m)exp -c mδ 2 Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES 491 we have dist( d, {±d }) ≤</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Zero-noise experiments in dimensions n = 400 and n = 800. (a) Fraction of times (in 400 experiments) that one method succeeded while the other failed for n = 400. (b) Fraction of successful recoveries for n = 400. (c) Fraction of successful recoveries for n = 800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Zero-noise experiments. (a) Fraction of trials (of 400) in which one method succeeds while other fails for n = 1000. (b) Fraction of trials (of 100) in which one method succeeds while other fails for n = 3000. (c) Fraction of successful recoveries for n = 3000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Probability of success for median-truncated Wirtinger flow (left) and the prox-linear method (right) for different initializations, where dimension n = 100. Horizontal axis indexes p fail while vertical axis indexes the measurement ratio m/n. Each pixel represents the fraction of successful recoveries (defined as dist( x, X )/ x 2 ≤ 10 -5 ) over 100 experiments. (a) and (b): 'Big' initialization, with r estimated by Algorithm 3. (c) and (d): 'Median' initialization. (e) and (f): 'Small' initialization.</figDesc><graphic coords="28,72.33,106.15,387.35,178.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figs 3 -</head><label>3</label><figDesc>Figs 3-5 summarize our results. In Figs 3 and 4, we display success rates of the median truncated Wirtinger flow (the left column in each figure) and our composite optimization-based procedure (right column) for a number of initializations; each plot represents results of 100 experiments. Within each plot, a white square indicates that 100% of trials were successful, meaning the signal is recovered to</figDesc><graphic coords="28,72.24,452.62,387.50,110.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of median-truncated Wirtinger flow and composite minimization with 'Median' initialization. Horizontal axis of each plot indexes p fail ∈ [0, 0.3]. (a) Proportion of successful solves for prox-linear method. (b) Number of iterations of prox-linear step (5) over successful solves (with error bars). (c) Proportion of successful solves for median-truncated Wirtinger flow method. (d) Number of matrix multiplications of the form x → (I + A T D 2 A) -1 Ax for the prox-linear method with proximal graph solves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Example recoveries of digits. Left digit is true digit, middle is initialization and right is recovered image.</figDesc><graphic coords="30,65.19,329.89,63.87,63.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Reconstruction of RNA image (n = 2 22 ) from m = 3n measurements. (a) Initialization x 0 of prox-linear method. (b) Result of 10 inaccurate ( = 10 -3 ) solutions of prox-linear step (5). (c) Result of one additional accurate ( = 10 -7 ) solution of prox-linear step (5). (d) Original image.</figDesc><graphic coords="31,75.81,78.67,396.14,412.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>2.3]. Let S r ⊂ {X ∈ C n×n } be the set of Hermitian rank r matrices with X Fr = 1, and let O n,r = {U ∈ C n×r | U H U = I r } denote the set of n × r unitary matrices; for each &gt; 0 there exists an -cover O n,r of O n,r in • 1,2 -norm of cardinality at most (3/ ) 2nr . Similarly, there exists an -cover D ⊂ R r of all vectors v ∈ R r in 2 -norm of cardinality at most (3/ ) r . Now, let S r = {U diag(d)U H | U ∈ O n,r , d ∈ D} be a subset of the rank-r Hermitian matrices. Then card(S r ) ≤ (3/ ) (2n+1)r , and we have the following immediate consequence of Lemma A.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 σ 4 in≤ c 2 σ 4</head><label>2424</label><figDesc>Lemma 3.1, noting that we must have σ 2 ≥ 1 because E[aa H ] = I n and E[aa H ] op ≤ σ 2 (recall the final part of Lemma 3.1). Moreover, ∈ [0, 1] by assumption. Then taking c small enough, once we have n m we obtain the result. The event E 2 := {|I -| ≥ |I + | -2κ m} likewise holds with high probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>i.d. Bernoulli random variables with means bounded by κ by Assumption 5(i). Hoeffding's inequality gives the result that P(|I + | &gt; |I -| + 2κm ) ≤ e -2κ 2 2 m . Lemma A.10 The event E 3 := {|I -| ≥ 1 2 mp 0 (d )} satisfies P(E 3 ) ≥ 1exp(-1 2 mp 0 (d ) 2 ). Proof. As in Lemma A.9, this result is immediate from Hoeffding's inequality. We have |I -| = m i=1 1{| a i , d | 2 ≤ (1 -)/2}, an i.i.d. sum of Bernoulli's with P(| a i , d | 2 ≤ (1 -)/2) ≥ p 0 (d ) by Assumption 5(i). Hoeffding's inequality gives the result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 4C (κ ) 2 p</head><label>12</label><figDesc>applied Assumption 5(i) and Lemma 3.1 to bound E[ a, d 2q ]. Using the triangle inequality and substituting t = into inequality (A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>B. 3</head><label>3</label><figDesc>Proof of Proposition 6 We introduce a bit of notation before giving the proof proper. Recall that I out ⊂ [m] denotes the outliers, or failed measurements, and I in = [m] \ I out the set of i such that b i = a i , x 2 (the inliers). Recalling the selected set of indices I sel , we define the shorthand I in sel := I in ∩ I sel and I out sel := I out ∩ I sel for the chosen inliers and outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>B. 3 . 2</head><label>32</label><figDesc>Proof of Lemma B.4 By our assumption (w.l.o.g.) that d = e 1 , we have Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Z m, 2 -⎠B. 3 . 4 C. 1 C. 2</head><label>23412</label><figDesc>z 2 (I nd d T ) proof of Lemma B.4, we let {a i } m i=1 iid∼ N(0, I n ) be an independent copy of the a i . The independence of {a i } i∈I in and {ξ i } i∈I out imply that for any measurable set C we haveP ⎛ ⎜ ⎝ i∈I in sel (a i,\1 a T i,\1z 2 I n-1 ) ∈ C | {a i,1 } i∈I in , {ξ i } i∈I out \1 a i,\1 T -I n-1 ∈ C | I in sel ⎞ ⎟ ⎠ .As {a i,\1 } m i=1 are i.i.d. Gaussian vectors, Lemma 3.1 implies that for t ∈ [0, 1], ≤ exp(-mt)for some constant C. The claim of the lemma follows by integration of the above inequality. Proof of Lemma B.6 For notational convenience, let us denote the selected outlying indices byI out sel = I out ∩ I sel .We begin our proof by considering Model M1, in which case the proof is essentially identical to Lemma B.5 (see Appendix B.3.3). Indeed, we have the identityZ 3z 3 I n op = 1 m m i=1 a i a T i -I n 1 i ∈ I out sel op, and so following the completely parallel route of introducing the independent sample {a i } m i=1 iid ∼ N(0, I n ), we have for any t ≥ 0 thatP Z 3z 3 I n op ≥ t = E P Z 3z 3 I n op ≥ t | I outsel the vectors a i 1 i ∈ I out sel gives the result. In the case in which we assume Model M2, we have the deterministic upper boundZ 1 i ∈ I out is an O(1)-sub-Gaussian vector, Lemma 3.1 implies i -I n 1 i ∈ I out op ≥ C n m + t | I out sel ⎞ ⎠ ≤ exp(-mt).Since |I out |/m = p fail , the desired claim follows via the triangle inequality. Proof of Lemma 3.2 Let p(z) denote the density of z and p c (z ) = p(z )/ c -c p(z) dz for z ∈ [-c, c]. Let F c (z) = z -c p c (z ) dz be the CDF of p c , and define the mapping T : [-c, c] → [-c, c] by T(u) = F -1 c u + c 2c = z for the z s.t. F c (z) = u + c 2c . Evidently, for Z ∼ P c we have T(Z) ∼ Uni[-c, c], and by the symmetry and monotonicity properties of p and p c we have |T(z)| ≥ |z| for z ∈ [-c, c]. In particular, letting U ∼ Uni[-c, c], we have Var(U) = E[U 2 ] = E[T(Z) 2 | |Z| ≤ c] ≥ E[Z 2 | |Z| ≤ c]. Using that Var(U) = 1 2c c -c u 2 du = c 2 3 gives the result. Proof of Lemma B.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Cmσ 2 ≤ 2 + σ 4 ⎞⎠</head><label>224</label><figDesc>u,v∈S n-1 {Z u,v -E[Z u,v ]}.Using the previous display, we obtain by the definition of an -cover argument thatP inf u,v∈S n-1 {Z u,vκ st (u, v)} ≤ -t ≤ P min u,v∈N {Z u,vκ st (u, v)} ≤ -t + 2 A T A op m .From Lemma 3.1, we know that |||A T A||| op is well concentrated, and thus by considering the event that |||A T A||| op mσ 2 that for some numerical constant C &lt; ∞ we haveP inf u,v∈S n-1 {Z u,vκ st (u, v)} ≤ -t ≤ P min u,v∈N {Z u,vκ st (u, v)} ≤ -t + 2Cσ 2 + P A T A op ≥ termcomes from Bernstein's inequality (C.1) and the second from Lemma 3.1. Now, let us assume that N is an -cover of S n-1 with minimal cardinality, which by standard volume arguments [44, Lemma 2] satisfies N( ) := card(N ) ≤ (1 + 2/ ) n for &gt; 0. Noting that (1 + 2/ ) 2n ≤ exp(n/ ) for ≤ 0 := 0.21398, we may replace inequality (C.2) with P inf u,v∈S n-1 {Z u,vκ st (u, v)} ≤ -t ≤ exp ⎛ ⎝ ncm t -Cσ 2 + e -m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>For 2 .</head><label>2</label><figDesc>t ∈ [0, c], we have P(W 2 ∈ [ct, c]) ≤ P(W 2 ∈ [0, t]), so that for such t we haveP(W 2 ≥ ct | W 2 ≤ c) ≤ P(W 2 ≤ t | W 2 ≤ c), or P(W 2 ≥ t | W 2 ≤ c) + P(W 2 ≥ ct | W 2 ≤ c) ≤ 1. Performing the standard change of variables to compute E[W 2 | W 2 ≤ c], we thus obtain E[W 2 | W 2 ≤ c] = c 0 P(W 2 ≥ t | W 2 ≤ c) dt = 2 ≥ t | W 2 ≤ c) + P(W 2 ≥ ct | W 2 ≤ c) dt ≤ Using that E[W 2 | W 2 &gt; c] ≥ c gives the lemma.Lemma C.2 Let X ∈ C n×n be rank 2 and Hermitian and z = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>√ 2 (</head><label>2</label><figDesc>N(0, I n ) + ιN(0, I n )) be standard complex normal. Then E[|z H Xz|]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</figDesc><table /><note><p>SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES 489 We continue our running example of Gaussian random variables to motivate the proposition.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES 493 Each of the steps of the method (19) is trivial except for the matrix inversion, or the 'graph projection' step, which projects the pair (x k+ 1</figDesc><table /><note><p><p>2</p>+ λ k , y k+ 1 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>κ st likewise have VC-dimension at most Cn as u, v vary. Applying standard VC-concentration inequalities [5, 43, Ch. 2.6] we immediately obtain that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</figDesc><table><row><cell>SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</cell><cell>507</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Proof of Lemma A.4 For X = xx Hyy H that</figDesc><table><row><cell>SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</cell><cell>509</cell></row><row><cell>A.2.1</cell><cell>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</cell></row></table><note><p>1e -c m τ 4 σ 4 +c 1 n log σ 2 τ 2 . Comparing with inequality (A.3) gives the theorem.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>E denote the event that 1 m |||A T diag(W)A||| op ≤ p fail + Cσ 2 √n/m + t, where t is chosen so that n/m + t ≤ 1. Returning to inequality (B.2), we have</figDesc><table><row><cell>SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</cell><cell>517</cell></row><row><cell>Now, let</cell><cell>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>If we assume that n m ≤ c(1 -2p fail ) 2 /σ 4 and replace t with c(1 -2p fail ) 2 /σ 4 for small enough constant c, we find that</figDesc><table><row><cell>1 m |||A T A||| op ≤ σ 2 1 + C n m + t with m with probability at least 1 -2e -mt we have that δ in the same probability. That is, for t ∈ 0, 1 -n expression (B.3) satisfies δ ≤ 6σ 2 1 -2p δ ≤ Cσ 2 1 -2p fail dist( d, {±d })</cell></row></table><note><p>fail -Cσ 2 n m + t dist( d, {±d }).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc><ref type="bibr" target="#b7">8</ref>, continuing on to give the proof of Lemma B.3.We now integrate out the conditioning in Lemma B.8. Recalling the definition (17) of w q in terms of p fail , we have that q fail = 3-2p fail 4(1-p fail ) &gt; 1 2 for p fail ∈ [0, 1/2). By Hoeffding's inequality we have P(|I in q | ≤ m/8) ≤ e -m/4 because |I in | ≥ (1p fail )m ≥ m/2, whence we obtain</figDesc><table><row><cell>SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</cell><cell>523</cell></row><row><cell></cell><cell>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Moreover, on the event E the rightmost sum is positive, and using that|I in sel | ≥ |I in | + |I sel |m ≥ 1 2p fail m,we obtain that on E we have Fix any set of indices I 0 ⊂ [m], and note that by Hoeffding's inequality for bounded random variables we have Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</figDesc><table><row><cell>1 m</cell><cell>i∈I in sel</cell><cell>(1 -a 2 i,1 ) ≥</cell><cell>1 -2p fail 2|I in sel |</cell><cell>i∈I in sel</cell><cell>(1 -a 2 i,1 ) ≥</cell><cell>1 -2p fail 2|I in q |</cell><cell>q i∈I in</cell><cell>(1 -a 2 i,1 ) ≥</cell><cell cols="2">1 -2p fail 2</cell><cell>δ q .</cell></row><row><cell cols="6">Recalling expression (B.7) thus gives Lemma B.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">⎛ Proof of Lemma B.8 P ⎝ 1 |I 0 | i∈I 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">⎞ ⎠ ≤ exp -</cell><cell>2|I 0 |t 2 w 2 q</cell></row></table><note><p><p>).</p>a 2 i,1 -E[a 2 i,1 | a 2 i,1 ≤ w 2 q ] ≥ t | a 2 i,1 ≤ w 2 q for i ∈ I 0</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>\1 ∈ C | {a i,1 } i∈I in , {ξ i } i∈I out \1 ∈ C | {a i,1 } i∈I in , I inNow, we use the standard result<ref type="bibr" target="#b29">[30]</ref> that if f is an L-Lipschitz function with respect to the 2 -norm, then for any standard Gaussian vector Z we haveP(f (Z) -E[f (Z)] ≥ t) ≤ exp -t 22L 2 for all t ≥ 0. Thus, defining the random Lipschitz constant L 2 = 1 Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES 525 Moreover, L is {a i,1 } i∈I in -measurable, and P( L ≥ 2) ≤ exp(-m/2), again by the Lipschitz concentration of Gaussian random variables. We thus apply Equation (B.11) and obtain Recalling the equality (B.11) on Z 1 op shows that the previous display gives the lemma. B.3.3 Proof of Lemma B.5 Our first observation is simply the definition of the projection operator P ⊥ , which gives</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">⎛</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎞</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P</cell><cell cols="2">⎜ ⎝</cell><cell>1 m</cell><cell cols="2">sel i∈I in</cell><cell cols="2">a i,1 a i,\1</cell><cell>2</cell><cell>≥ 2</cell><cell>n -1 m</cell><cell>+ t</cell><cell>⎟ ⎠</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎛</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎞</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">≤ P</cell><cell>⎜ ⎝</cell><cell>1 m</cell><cell cols="2">sel i∈I in</cell><cell cols="2">a i,1 a i,\1</cell><cell>2</cell><cell>≥ 2</cell><cell>n -1 m</cell><cell>+ t | L ≤ 2 ⎟ ⎠ P( L ≤ 2) + P( L ≥ 2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">≤ exp -</cell><cell cols="2">mt 2 8</cell><cell cols="2">+ exp -</cell><cell>m 2</cell><cell>.</cell></row><row><cell>P</cell><cell>⎛ ⎜ ⎝</cell><cell cols="10">a i,1 a i,⎞ ⎟ ⎠ = P</cell><cell>⎛ ⎜ ⎝</cell><cell>a i,1 a i,sel</cell><cell>⎞ ⎟ ⎠ .</cell><cell>(B.11)</cell></row><row><cell></cell><cell>i∈I in sel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>i∈I in sel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m</cell><cell>i∈I in a 2 i,1 we obtain</cell></row><row><cell></cell><cell>E</cell><cell>1 m</cell><cell cols="2">m i=1</cell><cell cols="6">a i,1 a i,\1 1 i ∈ I in sel</cell><cell>2 2</cell><cell>| I in sel , {a i,1 } i∈I in =</cell><cell>1 m 2</cell><cell>sel i∈I in</cell><cell>a 2 i,1 (n -1) ≤</cell><cell>n -1 m</cell><cell>L 2 ,</cell></row><row><cell cols="4">and thus for t ≥ 0,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>1 m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mt 2 2 L 2 .</cell></row></table><note><p>m i=1 a i,1 a i,\1 1 i ∈ I in sel 2 ≥ L n -1 m + t | {a i,1 } i∈I in , I in sel ≤ exp -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES 527 where inequality (i) is Cauchy-Schwarz and the final inequality follows Lemma 3.1. Thus, the lower tail Bernstein's inequality (Lemma A.2) applied to the positive random variables | u, a i v, a i | ≥ 0 with variance bounded by 2eσ 4 implies that , a i v, a i | for shorthand. Using that for vectors w, x ∈ R n we have m i=1 | a i , w a i , x | ≤ Aw 2 Ax 2 by Cauchy-Schwarz, we see that for any u, v, u , v ∈ S n-1 ,</figDesc><table><row><cell>P</cell><cell>1 m</cell><cell>m i=1</cell><cell cols="3">| u, a i v, a i | -κ st (u, v) ≤ -t ≤ exp -</cell><cell>mt 2 4eσ 4 .</cell><cell>( C . 1 )</cell></row><row><cell>Define Z u,v := 1 m</cell><cell cols="2">m i=1 | u</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(i) ≤ sup u∈S n-1</cell><cell>E[ u, a i</cell><cell cols="2">4 ] ≤ 2eσ 4 ,</cell></row></table><note><p><p>First, we note that</p>Var(| u, a i v, a i |) ≤ E[| u, a i v, a i | 2 ]</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>© The Author(s) 2018. Published by Oxford University Press on behalf of the Institute of Mathematics and its Applications. All rights reserved. Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>, Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>k-j-1 . Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_5"><p>We do not compare to other methods designed for outliers because we could not set the constants the methods require in such a way as to yield good performance. Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_6"><p>August 2019   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>J. C. DUCHI AND F. RUAN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_8"><p>2 σ 2 dist( d, {±d }), Downloaded from https://academic.oup.com/imaiai/article-abstract/8/3/471/5092740 by Chalmers University of Technology user on 26 August 2019 SOLVING (MOST) OF A SET OF QUADRATIC EQUALITIES</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Dima Drusvyatskiy and Courtney Paquette for a number of inspirational and motivating conversations. F. R. was additionally supported by a Stanford Graduate Fellowship. Funding NSF-CAREER Award (1553086 to J.C.D. and F.R.); Toyota Research Institute (to J.C.D. and F.R.).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs for noiseless phase retrieval</head><p>In this Appendix, we collect the proofs of the propositions and results in Section 3.</p><p>Because we use it multiple times in what follows, we state a standard eigenvector perturbation result, a variant of the Davis-Kahane sin-Θ theorem.</p><p>Lemma A.1 <ref type="bibr">(Stewart &amp; Sun [39]</ref>, Theorem 3.6) For vectors u, v ∈ S n-1 , define the angle θ(u, v) = cos -1 | u, v |. Let X ∈ C n×n be symmetric, Δ a symmetric perturbation and Z = X + Δ, and define gap(X) = λ 1 (X)λ 2 (X) to be the eigengap of X. Let v 1 and u 1 be the first eigenvectors of X and Z, Recalling the definition (17) of the constants δ q and w q in the statement of the proposition, we have the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.3 Define the random quantities z</head><p>See Appendix B.3.1 for a proof of the lemma. We thus see that it is likely that z 0 is substantially smaller than the rough fraction of inlying indices selected.</p><p>We now argue that Z 1 is likely to be small because it is the sum of products of independent vectors.</p><p>Lemma B.4 For t ≥ 0 we have</p><p>See Appendix B.3.2 for a proof. We can also show that Z 2 is well behaved in the sense that it is approximately a scaled multiple of (Id d T ).</p><p>Lemma B.5 Let z 2 be the random quantity z 2 := 1 m |I in sel |. There exists a numerical constant C such that for t ∈ [0, 1] we have</p><p>See Appendix B.3.3 for a proof of the lemma.</p><p>Finally, we control the size of the error matrix Z 3 in the expansion (B.5), which corresponds to the contribution of the outlying measurements a i that are included in the initialization matrix X init . We provide two slightly different guarantees, depending on the model (strength) of adversarial noise ξ assumed.</p><p>Lemma B.6 Define the random quantity z 3 := 1 m |I out ∩ I sel | ≤ p fail . There is a numerical constant C such that the following hold:</p><p>1. Under the independent noise model M1, for all t ∈ [0, 1],</p><p>2. Under the adversarial noise model M2, for all t ∈ [0, 1],</p><p>See Appendix B.3.4 for a proof. With these four lemmas in hand, we can prove the result of the proposition by applying the eigenvector perturbation Lemma A.1. The expansion (B.5), coupled with the four lemmas defining the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical guarantees for the EM algorithm: from population to sample-based analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="77" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On signal reconstruction without phase</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Casazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edidin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="345" to="356" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An elementary introduction to modern convex geometry</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Flavors of Geometry</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Levy</surname></persName>
		</editor>
		<meeting><address><addrLine>Berkeley, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MSRI Publications</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Saving phase: injectivity and stability for phase retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="106" to="125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation procedures based on the method of multipliers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="487" to="510" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Descent methods for composite nondifferentiable optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="260" to="279" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Gauss-Newton method for convex composite optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1241" to="1274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phase retrieval via matrix completion</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sc</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="199" to="225" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving quadratic equations via PhaseLift when there are about as many equations as unknowns</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phase retrieval via Wirtinger flow: theory and algorithms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<date type="published" when="1985">2015. 1985-2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2342" to="2359" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Array imaging using intensity-only measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moscoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papanicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">15005</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving random quadratic systems of equations is nearly as easy as solving linear systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="822" to="0883" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03446</idno>
		<ptr target="https://arxiv.org/abs/1610.03446" />
		<title level="m">Nonsmooth optimization using Taylor-like models: error bounds, convergence, and termination criteria</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Error bounds, quadratic growth, and linear convergence of proximal methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficiency of minimizing compositions of convex functions and smooth maps</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00125</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phase retrieval: stability and recovery guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="473" to="494" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phase retrieval from very few measurements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fickus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Mixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="475" to="499" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reconstruction of an object from the modulus of its Fourier transform</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Fienup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phase retrieval algorithms: a comparison</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Fienup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2758" to="2769" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">First and second order conditions for a class of nondifferentiable optimization problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A practical algorithm for the determination of phase from image and diffraction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Gerchberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Saxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lemaréchal</surname></persName>
		</author>
		<title level="m">Convex Analysis and Minimization Algorithms I</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust Statistics, 2nd edn</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley</publisher>
			<pubPlace>San Francisco, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matrix completion from a few entries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2980" to="2998" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
		<idno>arXiv:09064835</idno>
		<title level="m">The complex gradient operator and the CR-calculus</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Concentration of Measure Phenomenon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Providence, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized M-estimators with nonconvexity: statistical and algorithmic theory for local optima</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="559" to="616" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning without concentration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Seventh Annual Conference on Computational Learning Theory</title>
		<meeting>the Twenty Seventh Annual Conference on Computational Learning Theory<address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phase retrieval using alternating minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2796" to="2804" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="123" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Block splitting for distributed optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="77" to="102" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">RNA nanoparticles in cancer cells</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://visualsonline.cancer.gov/details.cfm?imageid=11167" />
		<imprint>
			<date type="published" when="2016-04-01">2016. April 1, 2017</date>
		</imprint>
		<respStmt>
			<orgName>National Cancer Institute Visuals Online</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the Bertsekas&apos; method for minimization of composite functions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Systems Optimization and Analysis</title>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bensoussan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</editor>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Lions eds)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phase retrieval with application to optical imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Schechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="page" from="87" to="109" />
			<date type="published" when="2015-05">2015. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Matrix Perturbation Theory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Numerical Linear Algebra</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>SIAM</publisher>
			<pubPlace>Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved analysis of the subsampled randomized Hadamard transform</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Adapt. Data Anal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Special issue on Sparse Representation of Data and Images</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Convex optimization in Julia. In First Workshop on High Performance Technical Computing in Dynamic Languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wellner</surname></persName>
		</author>
		<title level="m">Weak Convergence and Empirical Processes: With Applications to Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing: Theory and Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="210" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phase recovery, maxcut and complex semidefinite programming</title>
		<author>
			<persName><forename type="first">I</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. A</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="47" to="81" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Solving systems of random quadratic equations via truncated amplitude flow</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08285</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Provable non-convex phase retrieval with outliers: median truncated Wirtinger flow</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
