<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
							<email>zhoushiyu2013@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
							<email>shuang.xu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ASR</term>
					<term>speech recognition</term>
					<term>multilingual</term>
					<term>lowresource</term>
					<term>sequence-to-sequence</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence-to-sequence attention-based models integrate an acoustic, pronunciation and language model into a single neural network, which make them very suitable for multilingual automatic speech recognition (ASR). In this paper, we are concerned with multilingual speech recognition on low-resource languages by a single Transformer, one of sequence-to-sequence attentionbased models. Sub-words are employed as the multilingual modeling unit without using any pronunciation lexicon. First, we show that a single multilingual ASR Transformer performs well on low-resource languages despite of some language confusion. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence under the condition of language information being known during training. Experiments on CALLHOME datasets demonstrate that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5% average word error rate (WER) reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multilingual speech recognition has been investigated for many years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Conventional studies concentrate on the area of multilingual acoustic modeling by the contextdependent deep neural network hidden Markov models (CD-DNN-HMM) <ref type="bibr" target="#b5">[6]</ref>. The hidden layers of DNN in CD-DNN-HMM can be thought of complicated feature transformation through multiple layers of nonlinearity, which can be used to extract universal feature transformation from multilingual datasets <ref type="bibr" target="#b0">[1]</ref>. Among the CD-DNN-HMM based approaches, the architecture of SHL-MDNN <ref type="bibr" target="#b0">[1]</ref>, in which the hidden layers are shared across multiple languages while the softmax layers are language dependent, is a significant progress in the area of multilingual ASR. These shared hidden layers and language dependent softmax layers of SHL-MDNN are optimized jointly by multilingual datasets. SHL-MLSTM <ref type="bibr" target="#b4">[5]</ref> further explores long short-term memory (LSTM) <ref type="bibr" target="#b6">[7]</ref> with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN.</p><p>Although these models achieve encouraging results on multilingual ASR tasks, a hand-designed language-specific pronunciation lexicon must be employed. This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="bibr">Chiu et al.</ref> shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) <ref type="bibr" target="#b9">[10]</ref>. Our previous work <ref type="bibr" target="#b8">[9]</ref> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets.</p><p>Since the acoustic, pronunciation and language model are integrated into a single neural network by sequence-to-sequence attention-based models, it makes them very suitable for multilingual ASR. In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <ref type="bibr" target="#b8">[9]</ref>, we employ sub-words generated by byte pair encoding (BPE) <ref type="bibr" target="#b10">[11]</ref> as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence <ref type="bibr" target="#b12">[13]</ref> under the condition of language information being known during training. A comparison with SHL-MLSTM <ref type="bibr" target="#b4">[5]</ref> with residual learning is investigated on CALL-HOME datasets with 6 languages. Experimental results reveal that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5% average WER reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token.</p><p>The rest of the paper is organized as follows. After an overview of the related work in Section 2, Section 3 describes the proposed method in detail. We then show experimental results in Section 4 and conclude this work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Although multilingual speech recognition has been studied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> for a long time, these researches are commonly limited to making acoustic model (AM) multilingual, which require language-specific pronunciation model (PM) and language model (LM). Recently, sequence-to-sequence attentionbased models, integrating the AM, PM and LM into a single network, have attracted a lot of attention on multilingual ASR <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> have presented a single sequence-tosequence attention-based model can be capable of recognizing any of the languages seen in training. <ref type="bibr" target="#b12">[13]</ref> explored the possibility of training a single model serve different English dialects and compared different methods incorporating dialect-specific information into the model. However, multilingual ASR on low-resource languages are few investigated by sequence-tosequence attention-based models. Furthermore, we argue that the modeling unit of sub-words allows for a much stronger decoder LM compared to graphemes <ref type="bibr" target="#b9">[10]</ref>, so sub-words encoded by BPE are employed as the multilingual modeling unit rather than graphemes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System overview 3.1. ASR Transformer model architecture</head><p>The ASR Transformer architecture used in this work is the same as our work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> which is shown in Figure <ref type="figure" target="#fig_0">1</ref>. It stacks multihead attention (MHA) <ref type="bibr" target="#b16">[17]</ref> and position-wise, fully connected layers for both the encode and decoder. The encoder is composed of a stack of N identical layers. Each layer has two sublayers. The first is a MHA, and the second is a position-wise fully connected feed-forward network. Residual connections are employed around each of the two sub-layers, followed by a layer normalization. The decoder is similar to the encoder except inserting a third sub-layer to perform a MHA over the output of the encoder stack. To prevent leftward information flow and preserve the auto-regressive property in the decoder, the self-attention sub-layers in the decoder mask out all values corresponding to illegal connections. In addition, positional encodings <ref type="bibr" target="#b16">[17]</ref> are added to the input at the bottoms of these encoder and decoder stacks, which inject some information about the relative or absolute position of the tokens in the sequence.</p><p>The difference between the neural machine translation (NMT) Transformer <ref type="bibr" target="#b16">[17]</ref> and the ASR Transformer is the input of the encoder. We add a linear transformation with a layer normalization to convert the log-Mel filterbank feature to the model dimension d model for dimension matching, which is marked out by a dotted line in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multilingual modeling unit</head><p>Sub-words are employed as the multilingual modeling unit, which are generated by BPE 1 <ref type="bibr" target="#b10">[11]</ref>. Firstly, the symbol vocabulary with the character vocabulary is initialized, and each word is represented as a sequence of characters plus a special end-of-word symbol '@@', which allows to restore the original tokenization. Then, all symbol pairs are counted iteratively and each occurrence of the most frequent pair ('A', 'B') are replaced with a new symbol 'AB'. Each merge operation produces a new symbol which represents a character n-gram. Frequent character n-grams (or whole words) are eventually merged into a single symbol. Then the final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations α , which is the only hyper-parameter of this algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>In our multilingual experiments, training transcripts in all languages are combined together to generate the multilingual symbol vocabulary, instead of directly merging each language symbol vocabulary together. So same sub-words are shared among different languages automatically, which is very beneficial for languages belonging to the same language family. For example, for a German word of "universitätsgebäu", it is encoded into "univer@@ sit@@ ä@@ ts@@ ge@@ b@@ ä@@ u"; for an English word of "university", it is encoded into "univer@@ sit@@ y". Two sub-words "univer@@" and "sit@@" are shared in these two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Language information as output targets</head><p>Similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, we expand the symbol vocabulary of the multilingual ASR Transformer to include a list of special symbols, each corresponding to a language. For example, we add the symbol &lt;S EN&gt; into the symbol vocabulary when including English. If the language information of training data can only be known beforehand, two methods of adding the language symbol are explored, i.e. inserting at the beginning (Transformer-B) or at the end (Transformer-E) of the original sub-words sequence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>. What's more, if the language information of both training and testing data can be known beforehand, we directly take the language symbol &lt;S Lang&gt; as the sentence start token (Transformer-B2) rather than original sentence start token &lt;S&gt;. It can force the multilingual ASR Transformer to decode a speech utterance into the pointed language, which is able to alleviate the language confusion greatly during testing.</p><p>The difference between Transformer-B and Transformer-B2 is whether to utilize the language information during testing.</p><p>The sentence start token is &lt;S&gt; in Transformer-B. It first predicts a language symbol by itself and then the following tokens are predicted as usual. Therefore, Transformer-B do not need to know the language information beforehand during testing. In contrast, Transformer-B2 employs &lt;S Lang&gt; as its sentence start token and predicts the following tokens as usual, which need to know the language information beforehand during testing. An example of adding the language symbol is shown in Table <ref type="table">1</ref>. Table <ref type="table">1</ref>: An example of adding the language symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Example Source amazing Transformer</head><p>&lt;S&gt; ama@@ z@@ ing &lt;\S&gt; Transformer-B &lt;S&gt; &lt;S EN&gt; ama@@ z@@ ing &lt;\S&gt; Transformer-E &lt;S&gt; ama@@ z@@ ing &lt;S EN&gt; &lt;\S&gt; Transformer-B2</p><p>&lt;S EN&gt; ama@@ z@@ ing &lt;\S&gt; 4. Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>The datasets in the paper come from CALLHOME corpora collected by Linguistic Data Consortium (LDC). The following six languages are used: Mandarin (MA), English (EN), Japanese (JA), Arabic (AR), German (GE) and Spanish (SP). We follow the Kaldi <ref type="bibr" target="#b18">[19]</ref> recipe to process CALLHOME datasets <ref type="foot" target="#foot_0">2</ref> . The detailed information is listed below in Table <ref type="table" target="#tab_0">2</ref>. We train the ASR Transformer with a given number of epochs, so validation sets are not employed in this paper. All experiments are conducted using 80-dimensional log-Mel filterbank features, computed with a 25ms window and shifted every 10ms. The features are normalized via mean subtraction and variance normalization on the speaker basis. Similar to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, at the current frame t, these features are stacked with 3 frames to the left and downsampled to a 30ms frame rate. We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 <ref type="bibr" target="#b21">[22]</ref>, since it is always beneficial for training the ASR Transformer <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model and training details</head><p>We perform our experiments on the big model (D1024-H16) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> of the ASR Transformer. Table <ref type="table" target="#tab_1">3</ref> lists our experimental parameters. The Adam algorithm <ref type="bibr" target="#b22">[23]</ref> with gradient clipping and warmup is used for optimization. During training, label smoothing of value ls = 0.1 is employed <ref type="bibr" target="#b23">[24]</ref>. After trained, the last 20 checkpoints are averaged to make the performance more stable <ref type="bibr" target="#b16">[17]</ref>.</p><p>At the beginning we train the ASR Transformer on English data with a random initialization, but the result is poor although the CE loss looks good. We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work. To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work <ref type="bibr" target="#b8">[9]</ref>. Its softmax layer is replaced by the language-specific softmax layer which is initialized randomly. Through this initialization method, the ASR Transformer can converge very well. All experiments in this paper are conducted by this initialization method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Number of merge operations</head><p>First, we evaluate how the number of merge operations α in BPE affects the performance of the ASR Transformer. When α is tiny, the number of sub-words is small. Otherwise the number of sub-words is large. Since the training data is quite few on low-resource languages, it means that the number of sub-words cannot be too large in order to make sure each sub-word has enough training samples.</p><p>For each monolingual ASR Transformer, we first experiment on English dataset for choosing an appropriate α. As shown in Table <ref type="table" target="#tab_2">4</ref>, the performance reaches the best when α = 500 and the number of sub-words is 548 on English dataset. Appended with 4 extra tokens, (i.e. an unknown token (&lt;UNK&gt;), a padding token (&lt;PAD&gt;), and sentence start and end tokens (&lt;S&gt;/&lt;\S&gt;)), the total number of sub-words is 552. In this paper, we choose α = 500 in monolingual ASR Transformer experiments. For the multilingual ASR Transformer, all languages training transcripts are combined together to generate the multilingual symbol vocabulary by BPE. Table <ref type="table" target="#tab_4">6</ref> shows that α do not affect the performance too much on average. We choose α = 3000 in all multilingual ASR Transformer experiments and the total number of sub-words is 8062.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>The baseline systems come from our previous work <ref type="bibr" target="#b4">[5]</ref> and all results are summarized in Table <ref type="table" target="#tab_3">5</ref>. First, we train six monolingual ASR Transformers (Mono-Transformer) independently on each language data. As can be seen from Table <ref type="table" target="#tab_3">5</ref>, the monolingual ASR Transformer performs very well on each low-resource language and can obtain about relatively 15.7% WER reduction on average compared to monolingual LSTM (Mono-LSTM).</p><p>Furthermore, we build a single multilingual ASR Transformer (Multi-Transformer) on all training data together without using any language information during training and testing. We note that the Multi-Transformer can achieve slightly better performance than Mono-Transformer on average, which represents simply pooling the data together can give an acceptable recognition performance by a single multilingual ASR Transformer.</p><p>After analyzing recognition results from Multi-Transformer, we find that some recognition results are completely wrong because of language confusion, especially when the speech utterance is short. For example, sometimes an English word "um" is decoded into a German word "ja", because they have similar pronunciation.</p><p>Since the language information of training data usually can be known beforehand, we go on to build two multilingual ASR Transformers integrating language information as depicted in Section 3.3 to alleviate the problem of language confusion. Here, the language information is just used during training and the model itself predicts the language symbol during testing.</p><p>From Table <ref type="table" target="#tab_3">5</ref>, we can observe that inserting the language symbol at the end (Multi-Transformer-E) is better than inserting it at the beginning (Multi-Transformer-B). Compared to SHL-MLSTM-RESIDUAL, Multi-Transformer-B can obtain about relatively 10.5% average WER reduction.</p><p>If the language information of both training and testing data can be known beforehand, we directly take the language symbol &lt;S Lang&gt; as the sentence start token rather than original sentence start token &lt;S&gt;. It forces the multilingual ASR Transformer to decode a speech utterance into the pointed language, which greatly alleviate the language confusion during testing. As can be seen from Table <ref type="table" target="#tab_3">5</ref>, Multi-Transformer-B2 performs best and obtain relative 12.4% average WER reduction compared to SHL-MLSTM-RESIDUAL although the improvement on Spanish is very little. What's more, an interesting observation is that if we give a wrong language symbol &lt;S Lang&gt; as the sentence start token, Multi-Transformer-B2 is able to transliterate speech into the pointed language. An English example of predictions from Multi-Transformer-B2 with different &lt;S Lang&gt; is shown in Table <ref type="table" target="#tab_5">7</ref>. We can find that the prediction from wrong &lt;S Lang&gt; is an approximate pronunciation of the correct target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we investigated multilingual speech recognition on low-resource languages by a single multilingual ASR Transformer. Sub-words are chosen as the multilingual modeling unit to remove the dependency on the pronunciation lexicon. A comparison with SHL-MLSTM with residual learning is investigated on CALLHOME datasets with 6 languages. Experimental results reveal that a single multilingual ASR Transformer by inserting the language symbol at the end can obtain relatively 10.5% average WER reduction compared to SHL-MLSTM with residual learning if the language information of training data can be employed during training. We go on to show that about relatively 12.4% average WER reduction can be observed compared to SHL-MLSTM with residual learning by giving the language symbol as the sentence start token assuming the language information being known during training and testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the ASR Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Multilingual dataset statistics.</figDesc><table><row><cell>Language</cell><cell cols="2"># training utts. # test utts.</cell></row><row><cell>Mandarin (MA)</cell><cell>23915</cell><cell>3021</cell></row><row><cell>English (EN)</cell><cell>21194</cell><cell>2840</cell></row><row><cell>Japanese (JA)</cell><cell>27165</cell><cell>3381</cell></row><row><cell>Arabic (AR)</cell><cell>20828</cell><cell>2978</cell></row><row><cell>German (GE)</cell><cell>20027</cell><cell>5236 3</cell></row><row><cell>Spanish (SP)</cell><cell>17840</cell><cell>1982</cell></row><row><cell>Total</cell><cell>130969</cell><cell>19438</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Experimental parameters configuration.</figDesc><table><row><cell>model</cell><cell cols="2">N d model</cell><cell>h</cell><cell>d k dv</cell><cell>warmup</cell></row><row><cell>D1024-H16</cell><cell>6</cell><cell>1024</cell><cell cols="3">16 64 64 12000 steps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>WERs(%) of different α on English dataset.</figDesc><table><row><cell>α</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>1000</cell><cell>2000</cell></row><row><cell># output.</cell><cell>106</cell><cell>156</cell><cell>552</cell><cell>1047</cell><cell>1997</cell></row><row><cell>WER</cell><cell cols="5">45.28 44.64 42.77 43.88 43.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison of baseline systems and ASR Transformer on CALLHOME datasets in WER/CER (%). Relative WER/CER reduction is also shown between Multi-Transformer-B2 and SHL-MLSTM-RESIDUAL.</figDesc><table><row><cell></cell><cell>Model</cell><cell># params.</cell><cell>MA</cell><cell>EN</cell><cell>JA</cell><cell>AR</cell><cell>GE</cell><cell>SP</cell><cell>Average</cell></row><row><cell></cell><cell>Mono-DNN [5]</cell><cell>≈21.0M</cell><cell cols="3">53.05 50.45 57.52</cell><cell>61.52</cell><cell cols="2">59.11 59.77</cell><cell>56.90</cell></row><row><cell></cell><cell>Mono-LSTM [5]</cell><cell>≈17.8M</cell><cell cols="3">50.53 48.16 55.14</cell><cell>59.21</cell><cell cols="2">56.61 57.71</cell><cell>54.56</cell></row><row><cell></cell><cell>SHL-MDNN [5]</cell><cell>38.0M</cell><cell cols="3">50.67 46.77 54.15</cell><cell>58.91</cell><cell cols="2">55.94 57.88</cell><cell>54.05</cell></row><row><cell cols="2">SHL-MLSTM-RESIDUAL [5]</cell><cell>22.0M</cell><cell>45.85</cell><cell>43.93</cell><cell>50.13</cell><cell>56.47</cell><cell>51.75</cell><cell>53.38</cell><cell>50.25</cell></row><row><cell></cell><cell>Mono-Transformer</cell><cell>≈231M</cell><cell>39.62</cell><cell>42.77</cell><cell>39.55</cell><cell>50.78</cell><cell>48.94</cell><cell>54.42</cell><cell>46.01</cell></row><row><cell></cell><cell>Transformer</cell><cell>235M</cell><cell>40.28</cell><cell>42.35</cell><cell>39.29</cell><cell>50.87</cell><cell>47.82</cell><cell>53.26</cell><cell>45.65</cell></row><row><cell>Multi</cell><cell>Transformer-B Transformer-E</cell><cell>235M 235M</cell><cell>40.56 40.49</cell><cell>41.61 40.63</cell><cell>38.86 38.67</cell><cell>50.96 50.16</cell><cell>47.59 47.24</cell><cell>53.85 52.58</cell><cell>45.57 44.96</cell></row><row><cell></cell><cell>Transformer-B2</cell><cell>235M</cell><cell>37.62</cell><cell>40.36</cell><cell>38.13</cell><cell>48.82</cell><cell>46.22</cell><cell>53.07</cell><cell>44.03</cell></row><row><cell cols="2">Relative WER/CER Reduction</cell><cell>−</cell><cell>17.9%</cell><cell>8.1%</cell><cell cols="3">23.9% 13.5% 10.7%</cell><cell>0.6%</cell><cell>12.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Multilingual results with different α in WER/CER (%).</figDesc><table><row><cell>α</cell><cell>1000</cell><cell>3000</cell><cell>5000</cell><cell>7000</cell><cell>9000</cell></row><row><cell cols="2"># output. 6084</cell><cell cols="4">8062 10025 11959 13883</cell></row><row><cell>MA</cell><cell cols="2">41.14 40.28</cell><cell>40.66</cell><cell>40.14</cell><cell>40.72</cell></row><row><cell>EN</cell><cell cols="2">42.76 42.35</cell><cell>42.49</cell><cell>42.73</cell><cell>42.76</cell></row><row><cell>JA</cell><cell cols="2">40.04 39.29</cell><cell>38.63</cell><cell>38.68</cell><cell>39.76</cell></row><row><cell>AR</cell><cell cols="2">51.04 50.87</cell><cell>51.32</cell><cell>51.15</cell><cell>51.80</cell></row><row><cell>GE</cell><cell cols="2">48.92 47.82</cell><cell>48.85</cell><cell>48.21</cell><cell>48.11</cell></row><row><cell>SP</cell><cell cols="2">54.34 53.26</cell><cell>53.07</cell><cell>53.37</cell><cell>53.73</cell></row><row><cell>Average</cell><cell cols="2">46.37 45.65</cell><cell>45.84</cell><cell>45.71</cell><cell>46.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>An English example of predictions from Multi-Transformer-B2 with different &lt;S Lang&gt;.</figDesc><table><row><cell>Correct Target</cell><cell>by any means</cell></row><row><cell>&lt;S MA&gt;</cell><cell>八月零零</cell></row><row><cell>&lt;S EN&gt;</cell><cell>by any means</cell></row><row><cell>&lt;S JA&gt;</cell><cell>バアエリミン</cell></row><row><cell>&lt;S AR&gt;</cell><cell>tayyib yacni min</cell></row><row><cell>&lt;S GE&gt;</cell><cell>war er nicht mit</cell></row><row><cell>&lt;S SP&gt;</cell><cell>vaya a dime mil</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">the scripts of fisher callhome spanish in Kaldi are used to process all CALLHOME datasets with some tiny modifications.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We employ devtest as evaltest in German since there is no evaltest from CALLHOME corpora.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="7639" to="7643" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-lingual speech recognition with low-rank multi-task deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4994" to="4998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of rankconstrained multilingual dnns for low-resource asr</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sahraeian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Compernolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="5420" to="5424" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual recurrent neural networks with residual learning for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="704" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">No need for a lexicon? evaluating the value of the pronunciation lexica in end-to-end models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schogol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06239</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stateof-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01769</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Syllable-based sequencespeech recognition with the transformer in mandarin chinese</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10752</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-dialect speech recognition with a single sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01541</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01694</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards language-universal end-toend speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02207</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language independent end-to-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zeroshot translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
				<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>EPFL-CONF-192584</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06947</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01996</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02737</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
