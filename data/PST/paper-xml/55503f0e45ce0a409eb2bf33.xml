<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A robust least squares support vector machine for regression and classification with noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>86 20 87110446.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xiaowei</forename><surname>Yang</surname></persName>
							<email>xwyang@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">School of Sciences</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510641</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangjun</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">School of Sciences</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510641</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510641</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A robust least squares support vector machine for regression and classification with noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">86 20 87110446.</date>
						</imprint>
					</monogr>
					<idno type="MD5">E432B15A7FA380124AEE309CDF18331B</idno>
					<idno type="DOI">10.1016/j.neucom.2014.03.037</idno>
					<note type="submission">Received 18 September 2013 Received in revised form 12 March 2014 Accepted 15 March 2014 Communicated by X. Gao</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Least squares support vector machines Weighted least squares support vector machines Robust least squares support vector machine Regression Classification Noise</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Least squares support vector machines (LS-SVMs) are sensitive to outliers or noise in the training dataset. Weighted least squares support vector machines (WLS-SVMs) can partly overcome this shortcoming by assigning different weights to different training samples. However, it is a difficult task for WLS-SVMs to set the weights of the training samples, which greatly influences the robustness of WLS-SVMs. In order to avoid setting weights, in this paper, a novel robust LS-SVM (RLS-SVM) is presented based on the truncated least squares loss function for regression and classification with noise.</p><p>Based on its equivalent model, we theoretically analyze the reason why the robustness of RLS-SVM is higher than that of LS-SVMs and WLS-SVMs. In order to solve the proposed RLS-SVM, we propose an iterative algorithm based on the concave-convex procedure (CCCP) and the Newton algorithm. The statistical tests of the experimental results conducted on fourteen benchmark regression datasets and ten benchmark classification datasets show that compared with LS-SVMs, WLS-SVMs and iteratively reweighted LS-SVM (IRLS-SVM), the proposed RLS-SVM significantly reduces the effect of the noise in the training dataset and provides superior robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Support vector machines (SVMs) are very important methodologies for classification <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> and regression <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> in the fields of pattern recognition and machine learning. It has been widely applied to many real world pattern recognition problems, such as text classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, feature extraction <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, web mining <ref type="bibr" target="#b14">[15]</ref> and function estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Based on equality constraints instead of inequality ones, two least squares support vector machines (LS-SVMs) are proposed for classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and regression <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, respectively. Recently, a matrix pattern based LS-SVM is also presented <ref type="bibr" target="#b21">[22]</ref>. The solutions of LS-SVMs are obtained by solving a set of linear equations instead of solving a quadratic programming (QP) problem as in SVM. Several effective numerical algorithms have been suggested, such as the conjugate gradient based iterative algorithm <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, the reduced set of linear equations based algorithm <ref type="bibr" target="#b24">[25]</ref>, the sequential minimal optimization algorithm (SMO) <ref type="bibr" target="#b25">[26]</ref>, and the Sherman-Morrison-Woodbury (SMW) identity based algorithm <ref type="bibr" target="#b26">[27]</ref>.</p><p>At present, LS-SVMs have been widely applied to text classification <ref type="bibr" target="#b27">[28]</ref>, image processing <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, time series forecasting <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, and control <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. Unfortunately, in real-world applications, there exist two main drawbacks in LS-SVMs. The first one is their solutions are non-sparse <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> and the second one is their training processes are sensitive to noise in the training dataset due to over-fitting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. In order to deal with the first problem, some pruning algorithms have been proposed <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. In order to deal with the second problem, two weighted LS-SVMs (WLS-SVMs) have been presented for regression <ref type="bibr" target="#b44">[45]</ref> and classification <ref type="bibr" target="#b45">[46]</ref>, respectively. A key issue for WLS-SVMs is how to assign suitable weights to training samples. In the previous studies, the weights are assigned to the training samples by a two-stage method <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> and a multi-stage method <ref type="bibr" target="#b47">[48]</ref>. Theoretical analyses and the related experiments show that WLS-SVMs are robust to some noise.</p><p>In the field of machine learning, robust loss function is usually one of the key issues in designing a robust algorithm. At present, various margin-based loss functions, such as squared loss, logistic loss, hinge loss, exponential loss, 0-1 loss, and brownboost loss, have been used to search for the optimal classification and regression functions. From their function curves <ref type="bibr" target="#b48">[49]</ref>, we know that squared loss, logistic loss, hinge loss, and exponential loss are the upper boundary on the generalization error of a 0-1 loss. When the training sample has a large negative margin, squared loss, hinge loss and exponential loss are larger than brownboost loss. Therefore, the brownboost loss is usually more robust than the other loss functions. Recently, motivated by the link between Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom the pinball loss and quantile regression, Huang et al. introduced the pinball loss to classification problems and proposed the pinball loss SVM (pin-SVM) <ref type="bibr" target="#b49">[50]</ref>. The theoretical analysis and the experimental results show that compared to the hinge loss SVM, the pin-SVM is less sensitive to the feature noise around the decision boundary and more stable for re-sampling.</p><p>In order to avoid setting the weights of the training samples, which greatly influence the robustness of WLS-SVMs, in this study, inspired by the ideas in <ref type="bibr" target="#b50">[51]</ref>, we propose a novel robust LS-SVM (RLS-SVM) based on the truncated least squares loss function for regression and classification with noise. Based on the definition of influence function <ref type="bibr" target="#b51">[52]</ref>, we show that the proposed loss function is insensitive to noise. Considering that the proposed loss function is neither differentiable nor convex, inspired by <ref type="bibr" target="#b52">[53]</ref>, we firstly give a smoothing procedure to make the proposed loss function smooth. Secondly, using the concave-convex procedure (CCCP) <ref type="bibr" target="#b53">[54]</ref>, we transform solving a concave-convex optimization problem into solving iteratively a series of the convex optimization problems. Finally, we apply the Newton algorithm <ref type="bibr" target="#b52">[53]</ref> to solve these convex optimization problems. In order to test the robustness of RLS-SVM, we conduct a set of experiments on four synthetic regression datasets, fourteen benchmark regression datasets, two synthetic classification datasets and ten benchmark classification datasets. In the analysis of the experimental results, the Wilcoxon signedranks test and the Friedman test <ref type="bibr" target="#b54">[55]</ref> are used to check the significant of RLS-SVM.</p><p>This paper is organized as follows. In Section 2, we briefly review LS-SVMs and WLS-SVMs. In Section 3, we propose RLS-SVM. In Section 4, we theoretically analyze the reason why the robustness of RLS-SVM is higher than that of LS-SVMs and WLS-SVMs. An algorithm for RLS-SVM is given based on the CCCP and the Newton algorithm in Section 5. The experimental results and analyses are presented in Section 6. Finally, conclusions are given in Section 7.</p><p>2. Least squares support vector machines and weighted least squares support vector machines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Least squares support vector machine for regression</head><p>Considering a training set of l pairs of samples fx i ; y i g l i ¼ 1 for regression problem, where x i A R n are the input data and y i A R are the corresponding prediction values, LS-SVM for the regression problem is a QP problem based on the equality constraints and can be described in the following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>:</p><formula xml:id="formula_0">min w;b;ξ Jðw; b; ξÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 ξ 2 i ;<label>ð1Þ</label></formula><formula xml:id="formula_1">s. t. y i À½w T φðx i Þþb ¼ ξ i ; i ¼ 1; ⋯; l;<label>ð2Þ</label></formula><p>where w is the normal of the hyperplane, ξ i is the error of the ith training sample, φðx i Þ is a nonlinear function that maps x i to a high-dimensional feature space, C is a regularized parameter balancing the tradeoff between the margin and the error, and b is a bias. The Lagrangian function of the optimization problem (1) and ( <ref type="formula" target="#formula_1">2</ref>) is</p><formula xml:id="formula_2">Lðw; b; ξ; αÞ ¼ Jðw; b; ξÞÀ ∑ l i ¼ 1 α i fw T φðx i Þþb þ ξ i Ày i g;<label>ð3Þ</label></formula><p>where α i are the Lagrangian multipliers.</p><p>The optimal conditions can be written as the following system of linear equations:</p><formula xml:id="formula_3">0 e T e Ω þ I C ! b α ¼ 0 Y ;<label>ð4Þ</label></formula><p>where I A R lÂl is an identity matrix,</p><formula xml:id="formula_4">Y ¼ ðy 1 ; y 2 ; …; y l Þ T ;<label>ð5Þ</label></formula><formula xml:id="formula_5">α ¼ ðα 1 ; α 2 ; …; α l Þ T ;<label>ð6Þ</label></formula><formula xml:id="formula_6">e ¼ ð1; 1; …; 1Þ T ;<label>ð7Þ</label></formula><formula xml:id="formula_7">Ω ¼ ðΩ ij Þ ¼ ðkðx i ; x j ÞÞ;<label>ð8Þ</label></formula><formula xml:id="formula_8">kðx i ; x j Þ ¼ 〈φðx i Þ; φðx j Þ〉:<label>ð9Þ</label></formula><p>2.2. Weighted least squares support vector machine for regression WLS-SVM for the regression problem is described in the following <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_9">min w;b;ξ Jðw; b; ξÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i ξ 2 i ;<label>ð10Þ</label></formula><formula xml:id="formula_10">s. t. y i Àðw T φðx i ÞþbÞ ¼ ξ i ; i ¼ 1; ⋯; l;<label>ð11Þ</label></formula><p>where s ¼ ðs 1 ; s 2 ; …; s l Þ is a vector of weights associated with the training samples. If s j ¼0, one can delete the corresponding training sample from the model. The optimal dual variables can be given by the solution of the following system of linear equations:</p><formula xml:id="formula_11">0 e T e Ω þ 1 C diag 1 s 1 ; 1 s 2 ; …; 1 s l 0 @ 1 A b α ¼ 0 Y ;<label>ð12Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Least squares support vector machine for binary classification</head><p>Considering a training set of l pairs of samples fx i ; y i g l i ¼ 1 for binary classification, where x i A R n are the input data and y i A fÀ1; þ 1g are the corresponding class labels, LS-SVM for classification problem is also a QP problem based on the equality constraints and quadratic loss function, and can be described in the following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>:</p><formula xml:id="formula_12">min w;b;ξ Jðw; b; ξÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 ξ 2 i ;<label>ð13Þ</label></formula><formula xml:id="formula_13">s. t. y i ðw T φðx i ÞþbÞ ¼ 1 À ξ i ; i ¼ 1; ⋯; l;<label>ð14Þ</label></formula><p>2.4. Weighted least squares support vector machine for binary classification WLS-SVM for binary classification is described in the following <ref type="bibr" target="#b45">[46]</ref>:</p><formula xml:id="formula_14">min w;b;ξ Jðw; b; ξÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i ξ 2 i ;<label>ð15Þ</label></formula><formula xml:id="formula_15">s. t. y i ðw T φðx i ÞþbÞ ¼ 1 À ξ i ; i ¼ 1; ⋯; l:<label>ð16Þ</label></formula><p>Multiplying y i in both sides of (16) yields</p><formula xml:id="formula_16">y i Àðw T φðx i ÞþbÞ ¼ y i ξ i ; i ¼ 1; ⋯; l:<label>ð17Þ</label></formula><formula xml:id="formula_17">Let η i ¼ y i ξ i ; i ¼ 1; ⋯; l:<label>ð18Þ</label></formula><p>Then the optimization problem ( <ref type="formula" target="#formula_14">15</ref>) and ( <ref type="formula" target="#formula_15">16</ref>) can be rewritten as</p><formula xml:id="formula_18">min w;b;η Jðw; b; ηÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i η 2 i ;<label>ð19Þ</label></formula><formula xml:id="formula_19">s. t. y i Àðw T φðx i ÞþbÞ ¼ η i ; i ¼ 1; ⋯; l:<label>ð20Þ</label></formula><p>From the optimization problems (1) and ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_9">10</ref>) and ( <ref type="formula" target="#formula_10">11</ref>), we know that if all of the weights s i are equal to 1, then WLS-SVM for regression becomes LS-SVM for regression. Therefore, LS-SVM for regression is a special case of WLS-SVM for regression. Obviously, this conclusion is also true for classification. Comparing the optimization problem <ref type="bibr" target="#b18">(19)</ref> and <ref type="bibr" target="#b19">(20)</ref> with the optimization problem <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref>, we find that two WLS-SVMs for classification and regression can be uniformed. Therefore, we will only discuss the optimization problem <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref> in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The novel robust least squares support vector machine</head><p>The optimization problem <ref type="bibr" target="#b9">(10)</ref> and ( <ref type="formula" target="#formula_10">11</ref>) is equivalent to the following unconstrained optimization problem:</p><formula xml:id="formula_20">min w;b;s Jðw; b; sÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i ðy i Àðw T φðx i ÞþbÞÞ 2 :<label>ð21Þ</label></formula><p>In order to avoid setting the weights of the training samples and build a more robust learning machine, inspired by the ideas in <ref type="bibr" target="#b50">[51]</ref>, we consider the following two optimization problems: </p><formula xml:id="formula_21">min w;b min 0 r s r 1 1 2 w T w þ C 2 ∑ l i ¼ 1 L s ðw; b; s i ; x i ; y i Þ;<label>ð22Þ</label></formula><formula xml:id="formula_22">min w;b 1 2 w T w þ C 2 ∑ l i ¼ 1 robust 2 ðw; b; x i ; y i Þ;<label>ð23Þ</label></formula><p>Using ( <ref type="formula">27</ref>) and (28), we can prove the following theorem holds.</p><p>Theorem. The optimization problem <ref type="bibr" target="#b21">(22)</ref> is equivalent to the optimization problem <ref type="bibr" target="#b22">(23)</ref>, i.e.</p><formula xml:id="formula_24">min w;b min 0 r s r 1 1 2 w T w þ C 2 ∑ l i ¼ 1 L s ðw; b; s i ; x i ; y i Þ ¼ min w;b 1 2 w T w þ C 2 ∑ l i ¼ 1 robust 2 ðw; b; x i ; y i Þ:<label>ð29Þ</label></formula><p>Proof. Define </p><formula xml:id="formula_25">f rob ðw; bÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 robust 2 ðw; b; x i ; y i Þ;<label>ð30Þ</label></formula><formula xml:id="formula_26">f L ðw; b; sÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 L s ðw; b; s i ; x i ; y i Þ;<label>ð31Þ</label></formula><formula xml:id="formula_27">min 0 r s r 1 1 2 w T w þ C 2 ∑ l i ¼ 1 L s ðw; b; s i ; x i ; y i Þ ¼ min w;b 1 2 w T w þ C 2 ∑ l i ¼ 1 robust 2 ðw; b; x i ; y i Þ:<label>ð37Þ</label></formula><p>This shows that the theorem holds. □</p><p>From the following inequality</p><formula xml:id="formula_28">f rob ðw r ; b r Þ ¼ f L ðw r ; b r ; s r Þ Z f L ðw L ; b L ; s L Þ ¼ f rob ðw L ; b L Þ Zf rob ðw r ; b r Þ;<label>ð38Þ</label></formula><p>we know that the optimal solutions ðw r ; b r Þ and ðw L ; b L Þ of two optimization problems with respect to ðw; bÞ are interchangeable.</p><p>Let the residual of the training sample r ¼ yÀðw T φðxÞþbÞ, then the loss function robust 2 ðp; rÞ in the optimization problem <ref type="bibr" target="#b22">(23)</ref>   <ref type="formula" target="#formula_22">23</ref>) is a truncated least squares loss function and the losses of the noise and outliers are bounded, we call the optimization model ( <ref type="formula" target="#formula_22">23</ref>) RLS-SVM. An important role of the truncated parameter p in RLS-SVM is to control the errors of the noise and outliers and reduce their effects on the robustness of RLS-SVM. It is very obvious that when p is large enough, the solution of RLS-SVM is the same as LS-SVM. Based on this observation, we set 0 rp r1 and 0r pr 3 for regression and classification, respectively.</p><p>4. Relationship between the solutions of weighted least squares support vector machine and the optimization problem <ref type="bibr" target="#b21">(22)</ref> In this section, we discuss the relationship between the solutions of WLS-SVM and the optimization problem <ref type="bibr" target="#b21">(22)</ref>, and explain why the robustness of RLS-SVM is higher than that of LS-SVM and WLS-SVM.</p><p>For the fixed weight vector s, the optimization problem ( <ref type="formula" target="#formula_21">22</ref>) is equivalent to the following QP problem:</p><formula xml:id="formula_29">min w;b;ξ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i ξ 2 i þ pC 2 ∑ l i ¼ 1 ð1 À s i Þ;<label>ð39Þ</label></formula><formula xml:id="formula_30">s. t. y i Àðw T x i þbÞ ¼ ξ i ; i ¼ 1; 2; …; l:<label>ð40Þ</label></formula><p>Dropping the constant item ðpC=2ÞΣ l i ¼ 1 ð1 À s i Þ of the objective function in <ref type="bibr" target="#b38">(39)</ref> yields the following optimization problem.</p><formula xml:id="formula_31">min w;b;ξ 1 2 w T w þ C 2 ∑ l i ¼ 1 s i ξ 2 i ;<label>ð41Þ</label></formula><formula xml:id="formula_32">s. t. y i Àðw T x i þbÞ ¼ ξ i ; i ¼ 1; 2; …; l:<label>ð42Þ</label></formula><formula xml:id="formula_33">This is the standard WLS-SVM. Let ðw LS À SV M ; b LS À SV M Þ and ðw WLS À SV M ; b WLS À SV M</formula><p>Þ be the optimal solutions of LS-SVM and WLS-SVM, respectively, then</p><formula xml:id="formula_34">ðw LS À SV M ; b LS À SV M ; s LS À SVM Þ and ðw WLS À SV M ; b WLS À SV M ; s WLS À SV M</formula><p>Þ are two feasible solutions but not necessarily optimal solutions for the optimization problem <ref type="bibr" target="#b21">(22)</ref>, where s LS À SV M ¼ ð1; 1; …; 1Þ, s WLS À SV M ¼ ðs 1 ; s 2 ; …; s l Þ, and 0 o s i r 1 ði ¼ 1; 2; …; lÞ. Therefore, the robustness of the optimization problem ( <ref type="formula" target="#formula_21">22</ref>) is usually higher than that of LS-SVM and WLS-SVM.</p><p>Based on the equivalence of the optimization problems ( <ref type="formula" target="#formula_21">22</ref>) and ( <ref type="formula" target="#formula_22">23</ref>), we know that the robustness of the optimization problem ( <ref type="formula" target="#formula_22">23</ref>) is also usually higher than that of LS-SVM and WLS-SVM. In order to use some detailed experimental results to support our theoretical analyses, we give an algorithm for solving the optimization problem <ref type="bibr" target="#b22">(23)</ref> in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Solution to RLS-SVM</head><p>The loss function robust 2 ðw; b; x; yÞ in the optimization problem ( <ref type="formula" target="#formula_22">23</ref>) is neither differentiable nor convex so that most convex optimization methods cannot be employed to solve it. In order to overcome this difficulty, inspired by <ref type="bibr" target="#b52">[53]</ref>, we firstly give a smoothing procedure to make the loss function robust 2 ðw; b; x; yÞ smooth. Secondly, using the CCCP <ref type="bibr" target="#b53">[54]</ref>, we transform solving a concaveconvex optimization problem into solving iteratively a series of the convex optimization problems. Finally, we apply the Newton algorithm <ref type="bibr" target="#b52">[53]</ref> to solve these convex optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The smoothing procedure of the loss function</head><formula xml:id="formula_35">Let z ¼ w T φðxÞþb; then robust 2 ðw; b; x; yÞ ¼ min fp; ðy ÀzÞ 2 g ¼ ðy À zÞ 2 þ hðzÞ;<label>ð43Þ</label></formula><formula xml:id="formula_36">where hðzÞ ¼ 0; y À ffiffiffi p p r z ry þ ffiffiffi p p</formula><p>p Àðy À zÞ 2 ; otherwise</p><formula xml:id="formula_37">( :<label>ð44Þ</label></formula><p>It is very obvious that hðzÞ is a non-smooth function. In order to solve the optimization problem <ref type="bibr" target="#b22">(23)</ref> via classical convex optimization algorithms, we use the following smoothing function h n ðzÞ to replace hðzÞ.</p><formula xml:id="formula_38">h n ðzÞ ¼ p ÀðyÀ zÞ 2 ; z o y À ffiffiffi p p À h or z 4 y þ ffiffiffi p p þh À ðh þ 2 ffiffi p p Þðy þ h À ffiffi p p À zÞ 2 4h ; z À yþ ffiffiffi p p r h 0; y þ h À ffiffiffi p p o z oy À h þ ffiffiffi p p À ðh þ 2 ffiffi p p Þðy À h þ ffiffi p p À zÞ 2 4h ; z À yÀ ffiffiffi p p r h ; 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; :<label>ð45Þ</label></formula><p>where h is the smoothing parameter, typically taking its values between 0.001 and 0.5. Obviously, the function h n ðzÞ is continuous and twice-differentiable. When h-0, the function h(z) can be recovered immediately.</p><p>Based on ( <ref type="formula" target="#formula_35">43</ref>)-( <ref type="formula" target="#formula_38">45</ref>), the optimization problem ( <ref type="formula" target="#formula_22">23</ref>) can be rewritten as</p><formula xml:id="formula_39">min w;b J Rob ðw; bÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 ðy i À z i Þ 2 þ C 2 ∑ l i ¼ 1 h n ðz i Þ ¼ J LS vex ðw; bÞþJ LS cav ðw; bÞ:<label>ð46Þ</label></formula><p>where</p><formula xml:id="formula_40">J LS vex ðw; bÞ ¼ 1 2 w T w þ C 2 ∑ l i ¼ 1 ðy i À z i Þ 2 ;<label>ð47Þ</label></formula><formula xml:id="formula_41">J LS cav ðw; bÞ ¼ C 2 ∑ l i ¼ 1 h n ðz i Þ:<label>ð48Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CCCP for the optimization problem (46)</head><p>It is difficult to solve <ref type="bibr" target="#b45">(46)</ref> by classical convex optimization algorithms because the second term J LS cav ðw; bÞ in the objective function J Rob ðw; bÞ is non-convex. Fortunately, utilizing the CCCP, we can transform this non-convex optimization problem into a series of the convex optimization problems. According to the basic principle of CCCP, the optimal solution ðw; bÞ of the optimization problem ( <ref type="formula" target="#formula_39">46</ref>) can be achieved by iteratively solving the following optimization problem until it converges:</p><formula xml:id="formula_42">ðw t þ 1 ; b t þ 1 Þ ¼ argmin w;b J LS vex ðw; bÞþ ∂J LS cav ðw t ; b t Þ ∂w U w þ ∂J LS cav ðw t ; b t Þ ∂b U b ! :<label>ð49Þ</label></formula><p>where</p><formula xml:id="formula_43">∂J LS cav ðw t ; b t Þ ∂w ¼ ∑ l i ¼ 1 ∂J LS cav ðw t ; b t Þ ∂z i Uφðx i Þ;<label>ð50Þ</label></formula><formula xml:id="formula_44">∂J LS cav ðw t ; b t Þ ∂b ¼ ∑ l i ¼ 1 ∂J LS cav ðw t ; b t Þ ∂z i<label>ð51Þ</label></formula><p>Let</p><formula xml:id="formula_45">λ t i ¼ ∂J LS cav ðw t Þ ∂z i ¼ Cðy i À z i Þ; z i o y i À ffiffiffi p p À h or z i 4 y i þ ffiffiffi p p þ h Cðh þ 2 ffiffi p p Þðy i þ h À ffiffi p p À z i Þ 4h ; z i À y i þ ffiffiffi p p r h 0; y i þ h À ffiffiffi p p oz i oy i À hþ ffiffiffi p p Cðh þ 2 ffiffi p p Þðy i À h þ ffiffi p p À z i Þ 4h ; z i À y i À ffiffiffi p p r h 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; : :<label>ð52Þ</label></formula><p>Then (49) can be rewritten as</p><formula xml:id="formula_46">ðw t þ 1 ; b t þ 1 Þ ¼ argmin w;b J LS vex ðw; bÞþ ∂J LS cav ðw t ; b t Þ ∂w U w þ ∂J LS cav ðw t ; b t Þ ∂b U b ! ¼ argmin w;b J LS vex ðw; bÞþ ∑ l i ¼ 1 λ t i ϕðx i Þ Uw þ b ∑ l i ¼ 1 λ t i !<label>ð53Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Newton algorithm for solving the optimization problem (53)</head><p>Using the representation theorem in the reproducing kernel Hilbert space, w can be written as follows:</p><formula xml:id="formula_47">w ¼ ∑ l i ¼ 1 α i φðx i Þ:<label>ð54Þ</label></formula><p>Substituting ( <ref type="formula" target="#formula_47">54</ref>) into (53) yields</p><formula xml:id="formula_48">ðα t þ 1 ; b t þ 1 Þ ¼ argmin α 1 2 α T Kα þ C 2 α T ∑ l i ¼ 1 K i K T i ! α þ ∑ l i ¼ 1 ðλ t i À Cy i ÞK T i α þ C 2 ∑ l i ¼ 1 y 2 i þ b ∑ l i ¼ 1 ðλ t i À Cy i ÞþCb ∑ l i ¼ 1 K T i α þ Cl 2 b 2 !<label>ð55Þ</label></formula><formula xml:id="formula_49">X. Yang et al. / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</formula><p>The Hessian matrix and the gradient of the optimization problem <ref type="bibr" target="#b54">(55)</ref> are</p><formula xml:id="formula_50">H ¼ Ce T e Ce T K CKe K þ CKK !<label>ð56Þ</label></formula><p>and</p><formula xml:id="formula_51">∇ ¼ Ce T e Ce T K CKe K þ CKK ! b α þ e T ðλ À CYÞ Kðλ À CYÞ ! ;<label>ð57Þ</label></formula><p>respectively. For any nonzero vector</p><formula xml:id="formula_52">ðb α T Þ A R l þ 1 , ðb α T Þ Ce T e Ce T K CKe K þ CKK ! b α ¼ α T Kα þCðbe þ KαÞ T ðbe þ KαÞ 4 0:<label>ð58Þ</label></formula><p>Therefore, the optimization problem ( <ref type="formula" target="#formula_48">55</ref>) is a strictly convex quadratic programming. We can solve it using the Newton algorithm. From</p><formula xml:id="formula_53">Ce T e Ce T K CKe K þCKK ! b α þ e T ðλ À CYÞ Kðλ À CYÞ ! ¼ 0;</formula><p>we can obtain that the iterative formula for updating</p><formula xml:id="formula_54">b t þ 1 α t þ 1 ! is in the following: b t þ 1 α t þ 1 ! ¼ À Ce T e Ce T K CKe K þ CKK ! À 1 e T ðλÀ CYÞ Kðλ ÀCYÞ ! ¼ À 0 e T Ce I þCK ! À 1 0 λ ÀCY ;<label>ð59Þ</label></formula><p>where λ ¼ ðλ t 1 ; λ t 2 ; …; λ t l Þ T . Based on the analyses above, we can give the detailed algorithm as follows:</p><p>Step 1: Given the tolerance parameter ε, the SVM hyperparameters ðs; CÞ, and the iterative variable t ¼ 0.</p><p>Step 2: Obtain the initial values ðb 0 ; α 0 Þ via solving the standard LS-SVM.</p><p>Step 3: From <ref type="bibr" target="#b53">[54]</ref>, we know that the CCCP is globally or locally convergent. In Step 3, we obtain a globally optimal solution. Therefore the proposed algorithm is globally or locally convergent. Let T r and T i be the total iterative number of the CCCP and iteratively reweighted LS-SVM (IRLS-SVM) <ref type="bibr" target="#b47">[48]</ref> respectively, then the computational complexity of LS-SVM, WLS-SVM, IRLS-SVM, and RLS-SVM are</p><formula xml:id="formula_55">Calculate ðb t þ 1 ; α t þ 1 Þ</formula><formula xml:id="formula_56">Oððl þ 1Þ 3 Þ, Oð2ðl þ 1Þ 3 Þ, OðT i ðl þ 1Þ 3 Þ</formula><p>, and OðT r ðl þ1Þ 3 Þ, respectively. From the analyses of the computational complexity, we know that the training time of IRLS-SVM and RLS-SVM are usually longer than those of LS-SVM and WLS-SVM. When T i 4T r , the training time of IRLS-SVM is longer than that of RLS-SVM. Otherwise, the training time of IRLS-SVM is shorter than that of RLS-SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Numerical experiments and discussions</head><p>In this section, we will conduct experiments on four synthetic regression datasets, fourteen benchmark regression datasets, two synthetic classification datasets, and ten benchmark classification datasets to test the robustness of RLS-SVM. In order to achieve this goal, we compare the robustness of RLS-SVM with that of LS-SVM, WLS-SVM, and IRLS-SVM in regression experiments. In classification experiments, we compare the robustness of RLS-SVM with that of LS-SVM and WLS-SVM. All the programs are written in Cþ þ and compiled using Microsoft Visual Cþ þ6.0 compiler. All computations are conducted on a computer with 2.8 GHz Intel(R) Pentium(R) 4 processor, a maximum of 1.96 GB memory and running Microsoft Windows XP.</p><p>In order to evaluate the robustness of WLS-SVMs, inspired by the ideas in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b55">[56]</ref>, we give the following seven weightsetting formulas: For IRLS-SVM, we use the Myriad weight function, in which the parameter δ A f0:5; 1; 1:5; 2; 2:5; …; 20g. In RLS-SVM, the tolerance parameter ε ¼ 0:001 and the smooth parameter h A f0:025; 0:05; 0:075; ⋯; 0:5g.</p><formula xml:id="formula_57">s Suyken i ¼ 1 i f ξ i =ŝ r c 1 c 2 À ξ i =ŝ j j c 2 À c 1 if c 1 r ξ i =ŝ r c 2 10 À 4 otherwise 8 &gt; &gt; &lt; &gt; &gt; : ;<label>ð60Þ</label></formula><formula xml:id="formula_58">s hyp À lin i ¼ 1 À jξ i j maxðjξ i jÞ þ Δ ;<label>ð61Þ</label></formula><formula xml:id="formula_59">s hyp À exp i ¼ 2 1 þ expðβjξ i jÞ ;<label>ð62Þ</label></formula><formula xml:id="formula_60">s cen À lin i ¼ 1 À d cen i maxðd cen i ÞþΔ ;<label>ð63Þ</label></formula><formula xml:id="formula_61">s cen À exp i ¼ 2 1 þ expðβd cen i Þ ;<label>ð64Þ</label></formula><formula xml:id="formula_62">s sph À lin i ¼ 1 À d sph i maxðd sph i ÞþΔ ;<label>ð65Þ</label></formula><formula xml:id="formula_63">s sph À exp i ¼ 2 1 þ expðβd sph i Þ ;<label>ð66Þ</label></formula><p>In all of the experiments, the Gaussian kernel function is adopted, the hyperparameters sAf2 À 4 ; 2 À 3 ; 2 À 2 ; ⋯; 2 5 g and C A f2 0 ; 2 1 ; 2 2 ; ⋯; 2 9 g. In order to obtain the unbiased statistical results, we use the ten-fold cross validation strategy to search the optimal parameters and weight-setting strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Regression experiments</head><p>In regression experiments, we firstly generate four synthetic datasets to show the robustness of RLS-SVM. Training datasets are generated by the sine function with four different kinds of noise: Gaussian noise, multiplicative noise, heterogeneous variant Gaussian noise, and transformed κ 2 noise. Testing dataset is generated by the sine function. These datasets are illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. Secondly, we conduct the experiments on fourteen benchmark regression datasets to test the robustness of RLS-SVM, where the datasets Chwirut, Nelson, Gauss3 and Enso are downloaded from http://www.itl.nist.gov/div898/strd/nls/nls_main.shtml, the datasets Boston Housing, Heart Disease and Servo are downloaded from http://archive.ics.uci.edu/ml/datasets.html, the datasets Auto MPG and Bodyfat are downloaded from http://www.csie.ntu.edu. tw/ $ cjlin/libsvmtools/datasets/, the datasets Pollution Scale and Crabs are downloaded from http://stat.cmu.edu/datasets/, and the datasets Compass and Bolts are downloaded from http://www.sci. usq.edu.au/staff/dunn/Datasets/index.html. The detailed information about these datasets is listed in Table <ref type="table" target="#tab_3">1</ref>. Each attribute of the sample including the output is normalized into [ À 1,1] and the truncated parameter p A f0:001; 0:01; 0:1g.</p><p>In regression analysis, the mean absolute error (MAE) is usually used to evaluate the generalization error of the algorithm. However, statisticians have noticed that MAE is sensitive to noise and outliers in the testing set <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>. To solve this problem, statisticians suggested using R 2 statistics to evaluate the regression models in noisy circumstances. In this section, we use the R 2 statistics to evaluate the robustness of LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM, which is defined as follows <ref type="bibr" target="#b58">[59]</ref>:</p><formula xml:id="formula_64">R 2 ¼ 1 À medðjy i À f ðx i ÞjÞ madðy i Þ 2 ;<label>ð67Þ</label></formula><p>where medðjy i À f ðx i ÞjÞ is the median of the absolute errors between the objective values and the forecasting values of the testing samples and</p><formula xml:id="formula_65">madðy i Þ ¼ medðjy i À medðy j ÞjÞ<label>ð68Þ</label></formula><p>is the median of the absolute errors between the objective values and the median of the objective values of the testing samples. Generally speaking, for a reasonable model, 0 r R 2 r 1. From Eq. (67), we know that the smaller medðjy i À f ðx i ÞjÞ is, the larger R 2 is. It is very obvious that R 2 ¼1 corresponds to a perfect fit and R 2 o0 corresponds to a bad fit.</p><p>The optimal statistics R 2 obtained by LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM on four synthetic regression datasets are reported in Table <ref type="table" target="#tab_4">2</ref>. The optimal statistics R 2 obtained by LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM on fourteen benchmark regression datasets, the optimal parameters, the optimal weight-setting strategy, the corresponding training time and testing time are reported in Table <ref type="table" target="#tab_5">3</ref>. The best statistics R 2 are presented in bold type.</p><p>From Table <ref type="table" target="#tab_4">2</ref>, we can observe that RLS-SVM is the most robust to Gaussian noise, heterogeneous variant Gaussian noise, and transformed κ 2 noise. For multiplicative noise, the robustness of RLS-SVM is comparable with that of IRLS-SVM.</p><p>In the field of machine learning, the Friedman test is usually used to compare the performances of multiple learning machines <ref type="bibr" target="#b54">[55]</ref>. It ranks the learning machines for each dataset separately, the learning machine with the best performance gets the rank of 1, the second best gets the rank of 2. In case of equality (like in Gauss3), average ranks are assigned. Based on this ranking criterion, we first rank LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM for each dataset and the results are reported in Table <ref type="table" target="#tab_5">3</ref>. Then we use the Friedman test to conduct a statistical comparison of these four learning machines and demonstrate the robustness of RLS-SVM, where the significance level α is set to 0.05.</p><p>From Table <ref type="table" target="#tab_5">3</ref>, we can obtain that the average ranks of LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM are R 1 ¼3.9286, R 2 ¼ 2.4643, R 3 ¼2.3214 and R 4 ¼1.2857, respectively. In the Friedman test, the test statistics for comparing the i À th and j À th learning machines</p><formula xml:id="formula_66">z ¼ ðR i À R j Þ= ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ðkðk þ 1Þ=6NÞ p ,</formula><p>where k is the number of the compared learning machines and N is the number of the experimental datasets. After obtaining z, we find the corresponding probability prob from the table of normal distribution and calculate p ¼ 2nð1 À probÞ. In order to run the Hochberg's step-up procedure <ref type="bibr" target="#b59">[60]</ref>, we sort the hypotheses in descending order according to their significance. The sorted results and the corresponding p are reported in Table <ref type="table">4</ref>. From Table <ref type="table">4</ref>, we know that for the last hypothesis, 0.034 o0.05. Therefore, we should reject this nullhypothesis. It indicates that RLS-SVM is significantly more robust than LS-SVM, WLS-SVM and IRLS-SVM for regression with noise. However, from Table <ref type="table" target="#tab_4">2</ref>, we know that the training time of RLS-SVM is longer than that of LS-SVM and WLS-SVM while the training time of IRLS-SVM is longer than that of RLS-SVM. As for the testing time, in most cases, there are not significant differences among four learning machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Classification experiments</head><p>In the field of machine learning, the noise in classification problems is usually divided into two categories <ref type="bibr" target="#b60">[61]</ref>: contradictory examples, which appear more than once and are labeled with different classes, and misclassification examples, which are labeled with wrong classes. In classification experiments, we firstly give two synthetic datasets with class noise, which are illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>, to show that RLS-SVM is insensitive to class noise. Secondly, we conduct the experiments on ten benchmark datasets to test the robustness of RLS-SVM, where the dataset Ripley is from <ref type="bibr" target="#b61">[62]</ref>; the datasets Banana, Cleveland Heart, Glass, Heartstatlog, Liver Disorder, Monk, PIMA, Transfusion and Vehicle are downloaded from http://archive.ics.uci. edu/ml/datasets.html. The detailed information about benchmark datasets is listed in Table <ref type="table" target="#tab_7">5</ref>. Each attribute of the sample is normalized into [À 1,1] and the truncated parameter p A f0:2; 0:5; 0:8; 1:0; 1:5; 2:0; 3:0g. In order to compare the robustness of RLS-SVM with that of LS-SVM and WLS-SVM, The experimental results on two synthetic datasets are reported in Table <ref type="table" target="#tab_8">6</ref>. The testing accuracy of three learning machines on ten benchmark classification datasets, the optimal parameters, the optimal weight-setting strategy, the training time and the testing time are reported in Table <ref type="table" target="#tab_9">7</ref>. The best testing accuracy are presented in bold type.</p><p>From Table <ref type="table" target="#tab_8">6</ref>, we can observe that RLS-SVM is the most robust to class noise among three learning machines.</p><p>Next, we analyze the experimental results obtained by three learning machines on ten benchmark datasets. In order to demonstrate the robustness of RLS-SVM on these datasets, similarly to the regression analysis above, we first rank LS-SVM, WLS-SVM and RLS-SVM for each classification dataset and the results are listed in Table <ref type="table" target="#tab_9">7</ref>. Then we use the Friedman test to conduct a statistical comparison of these three learning machines, where the significance level α is set to 0.05. From Table <ref type="table" target="#tab_9">7</ref>, we can obtain that the average ranks of LS-SVM, WLS-SVM and RLS-SVM are R 1 ¼2.75, R 2 ¼ 1.95 and R 3 ¼1.30, respectively. In order to run the Holm's step-down procedure <ref type="bibr" target="#b62">[63]</ref>, we sort the hypotheses in descending order according to their significance.</p><p>The sorted results and the corresponding p are reported in Table <ref type="table" target="#tab_10">8</ref>. From Table <ref type="table" target="#tab_10">8</ref>, we know that for the first hypothesis, 0.001o0.025. Therefore, RLS-SVM is significantly more robust than LS-SVM. However, for the last hypothesis, 0.14640.05. It indicates that the Friedman test cannot detect the significance between RLS-SVM and WLS-SVM. Considering that the Wilcoxon signed-ranks test is used to compare the performances of two learning machines in the field of machine learning <ref type="bibr" target="#b54">[55]</ref>, in the following, we use it to detect the significance between RLS-SVM and WLS-SVM. The Wilcoxon signed-ranks test ranks the differences in performances of two learning machines for each dataset. The differences are ranked according to their absolute values. The smallest absolute value gets the rank of 1, the second smallest gets the rank of 2. In case of equality, average ranks are assigned. The statistics of the Wilcoxon signed-ranks test is <ref type="bibr" target="#b54">[55]</ref>:</p><formula xml:id="formula_67">zða; bÞ ¼ Tða; bÞÀðNðN þ 1Þ=4Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ð1=24ÞNðN þ 1Þð2N þ 1Þ p ;<label>ð69Þ</label></formula><p>where Tða; bÞ ¼ min fR þ ða; bÞ; R À ða; bÞg, N is the number of the experimental datasets, R þ (a,b) is the sum of ranks for the experimental datasets on which learning machine b outperforms learning machine a and R À (a,b) is the sum of ranks for the opposite, which are defined as follows:</p><p>R þ ða; bÞ ¼ ∑</p><formula xml:id="formula_68">d i 4 0 rankðd i Þþ 1 2 ∑ d i ¼ 0 rankðd i Þ;<label>ð70Þ</label></formula><p>Table <ref type="table">4</ref> The sorted results of significance of RLS-SVM VS. LS-SVM, WLS-SVM and IRLS-SVM for regression.   </p><formula xml:id="formula_69">i Learning machines z ¼ ðR i À R 4 Þ= ffiffiffiffiffiffiffiffiffiffiffi ffi kðk þ 1Þ 6N q p α k À i 1 LS-SVM<label>(</label></formula><formula xml:id="formula_70">rankðd i Þþ 1 2 ∑ d i ¼ 0 rankðd i Þ:<label>ð71Þ</label></formula><p>where d i is the difference between the performance scores of two learning machines on the ith experimental dataset, rank(d i ) is the The difference between the optimal testing accuracy of WLS-SVM and RLS-SVM and their rank values on ten benchmark classification datasets.  <ref type="table" target="#tab_11">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>From Table <ref type="table" target="#tab_11">9</ref>, based on formulas (69)-(71), we can obtain zðWLS À SVM; RLS À SVMÞ ¼ À2:29 o À 1:96. It shows that for the significance level of 0.05, RLS-SVM is significantly more robust than WLS-SVM for classification with noise. However, from Table <ref type="table" target="#tab_9">7</ref>, we can find that the training time of RLS-SVM is longer than LS-SVM and WLS-SVM for classification with noise. In most cases, the testing time of RLS-SVM is the shortest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and future work</head><p>The contributions of this paper are as follows. Firstly, a novel RLS-SVM model is presented based on the truncated least squares loss for regression and classification with noise. Secondly, the relationship between the solutions of WLS-SVM and the equivalent model of RLS-SVM is discussed. Thirdly, an algorithm for solving RLS-SVM is presented based on CCCP and the Newton algorithm. The experiments have been conducted on four synthetic regression datasets, fourteen benchmark regression datasets, two synthetic classification datasets, and ten benchmark classification datasets to test the robustness of RLS-SVM. The results show that RLS-SVM is significantly more robust than LS-SVM, WLS-SVM and IRLS-SVM for regression with noise. As for classification with noise, RLS-SVM is also the most robust among the compared three learning machines. The main shortcoming of RLS-SVM is its training time is usually longer than that of LS-SVM and WLS-SVM.</p><p>In future work, we will investigate the techniques of data sampling and data compressing so that RLS-SVM can be applied to large-scale regression and classification with noise. Another interesting topic would be to design some pruning algorithms for the proposed RLS-SVM to obtain sparse solutions. Further study on this topic will also include many applications of RLS-SVM in realworld regression and classification with noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>;b min 0 r s r 1 fs r ¼ arg min 0 r s r 1 fFromr s r 1 fr s r 1 fr s r 1 f</head><label>11111</label><figDesc>ðw r ; b r Þ ¼ argmin w;b f rob ðw; bÞ; ð32Þ ðw L ; b L ; s L Þ ¼ argmin wL ðw; b; sÞ; ð33Þ L ðw r ; b r ; sÞ: ð34Þ L ðw; b; sÞ ¼ min 0 L ðw L ; b L ; sÞ Z f rob ðw L ; b L Þ Z min ðw; bÞ ¼ f rob ðw r ; b r Þ ¼ min 0 L ðw r ; b r ; sÞ Z min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>where c 1</head><label>1</label><figDesc>¼2.5, c 2 ¼3.0, ξ i are the sample errors obtained by LS-SVMs, ŝ ¼ 1:483 Â madðξ i Þ, β¼0.3, Δ¼0.001, d cen i is the Euclidean distance between the training sample x i and its own class center, d sph i is the Euclidean distance between the training sample x i and the center of the minimum enclosed ball covering the two classes. In regression experiments, we use s Suyken i , s hyp À lin i and s hyp À exp i to set the weight of the training sample x i , respectively. In classification experiments, we use s Suyken i , s hyp À lin i , s hyp À exp i , s cen À lin i , s cen À exp i , s sph À lin i and s sph À exp i to set the weight of the training sample x i , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four synthetic regression datasets (a) synthetic training dataset one with Gaussian noise ( the mean is zero and the variance is 0.1); (b) synthetic training dataset two with multiplicative noise (y ¼ ð1 þ vÞ Â y, where v is random noise whose mean is zero and variance is 0.05); (c) synthetic training dataset three with heterogeneous variant Gaussian noise (the mean of noise is zero and the variance of noise changes as the input coordinate changes); (d) synthetic training dataset four with transformed κ 2 noise, whose degree of the freedom is 199; and (e) synthetic testing dataset. The number of training samples and testing samples are 200 and 100, respectively.</figDesc><graphic coords="6,77.59,58.64,430.32,604.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two synthetic classification datasets: (a) Training dataset one with misclassification examples and the number of training examples is 172; (b) Training dataset two with contradictory examples and misclassification examples. The number of training examples is 174; and (c) Testing dataset. The number of testing examples is 205.</figDesc><graphic coords="9,85.24,329.96,434.64,393.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>:</head><label></label><figDesc>Therefore, the proposed loss function is insensitive to noise and outliers in the training samples, which usually tend to cause large residuals. Considering that the loss function robust 2 ðw; b; x; yÞ in (</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>can</cell></row><row><cell cols="2">be rewritten as</cell><cell></cell><cell></cell></row><row><cell cols="4">robust 2 ðp; rÞ ¼ minðp; r 2 Þ ¼</cell><cell>(</cell><cell>r 2 ; r j jr p; r j j4 p p ffiffiffi ffiffiffi p p</cell><cell>:</cell></row><row><cell cols="5">According to the definition of influence function [52], we can</cell></row><row><cell cols="5">obtain its influence function</cell></row><row><cell>drobust 2 ðp; rÞ dr</cell><cell>¼</cell><cell>(</cell><cell cols="2">2r; r j jo 0; r j j4 p p ffiffiffi ffiffiffi p p</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Detailed information of fourteen benchmark regression datasets.</figDesc><table><row><cell>Datasets</cell><cell>Number of examples</cell><cell>Number of attributes</cell></row><row><cell>Chwirut</cell><cell>214</cell><cell>1</cell></row><row><cell>Nelson</cell><cell>128</cell><cell>2</cell></row><row><cell>Boston Housing</cell><cell>506</cell><cell>13</cell></row><row><cell>Pollution Scale</cell><cell>60</cell><cell>14</cell></row><row><cell>Gauss3</cell><cell>250</cell><cell>1</cell></row><row><cell>Heart Disease</cell><cell>400</cell><cell>4</cell></row><row><cell>Crabs</cell><cell>200</cell><cell>6</cell></row><row><cell>Compass</cell><cell>108</cell><cell>2</cell></row><row><cell>Bolts</cell><cell>40</cell><cell>7</cell></row><row><cell>Motocycle</cell><cell>133</cell><cell>1</cell></row><row><cell>Servo</cell><cell>167</cell><cell>4</cell></row><row><cell>Auto MPG</cell><cell>392</cell><cell>7</cell></row><row><cell>Bodyfat</cell><cell>252</cell><cell>14</cell></row><row><cell>Enso</cell><cell>168</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Comparison of the results of LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM on four synthetic regression datasets.</figDesc><table><row><cell>Datasets</cell><cell>LS-SVM</cell><cell>WLS-SVM</cell><cell>IRLS-SVM</cell><cell>RLS-SVM</cell></row><row><cell>Synthetic dataset one</cell><cell>0.9920</cell><cell>0.9968</cell><cell>0.9986</cell><cell>0.9995</cell></row><row><cell>Synthetic dataset two</cell><cell>0.9986</cell><cell>0.9989</cell><cell>0.9999</cell><cell>0.9998</cell></row><row><cell>Synthetic dataset three</cell><cell>0.9959</cell><cell>0.9977</cell><cell>0.9991</cell><cell>0.9994</cell></row><row><cell>Synthetic dataset four</cell><cell>0.9905</cell><cell>0.9929</cell><cell>0.9907</cell><cell>0.9934</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Comparison of the results of LS-SVM, WLS-SVM, IRLS-SVM and RLS-SVM on fourteen benchmark regression datasets.</figDesc><table><row><cell>Datasets</cell><cell>Algorithms</cell><cell>p</cell><cell>h</cell><cell>δ</cell><cell>s</cell><cell>C</cell><cell>The optimal weight</cell><cell>R 2</cell><cell>Ranks</cell><cell>Training time (s)</cell><cell>Testing time (s)</cell></row><row><cell>Chwirut</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>-</cell><cell>0.9769</cell><cell>4</cell><cell>0.0858</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>s Suyken i</cell><cell>0.9793</cell><cell>3</cell><cell>0.1609</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0</cell><cell>1.0000</cell><cell>32.0000</cell><cell></cell><cell>0.9800</cell><cell>2</cell><cell>1.8548</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.075</cell><cell>-</cell><cell>1.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.9815</cell><cell>1</cell><cell>0.3811</cell><cell>0.0000</cell></row><row><cell>Nelson</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>-</cell><cell>0.6517</cell><cell>4</cell><cell>0.0204</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>64.0000</cell><cell>s hyp À lin i</cell><cell>0.6713</cell><cell>3</cell><cell>0.0234</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.5</cell><cell>1.0000</cell><cell>32.0000</cell><cell></cell><cell>0.6719</cell><cell>2</cell><cell>0.1908</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.050</cell><cell>-</cell><cell>1.0000</cell><cell>128.0000</cell><cell>-</cell><cell>0.7128</cell><cell>1</cell><cell>0.1125</cell><cell>0.0000</cell></row><row><cell>Boston Housing</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.8708</cell><cell>4</cell><cell>1.4753</cell><cell>0.0016</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>256.0000</cell><cell>s Suyken i</cell><cell>0.8818</cell><cell>2</cell><cell>2.6921</cell><cell>0.0016</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>1.0000</cell><cell>64.0000</cell><cell></cell><cell>0.8735</cell><cell>3</cell><cell>147.0519</cell><cell>0.0062</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.025</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>-</cell><cell>0.8839</cell><cell>1</cell><cell>5.9359</cell><cell>0.0078</cell></row><row><cell>Pollution Scale</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.0000</cell><cell>16.0000</cell><cell>-</cell><cell>0.6413</cell><cell>4</cell><cell>0.0047</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.0000</cell><cell>16.0000</cell><cell>s hyp À lin i</cell><cell>0.6938</cell><cell>1</cell><cell>0.0221</cell><cell>0.0016</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.5</cell><cell>4.0000</cell><cell>16.0000</cell><cell></cell><cell>0.6845</cell><cell>3</cell><cell>0.0358</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.05</cell><cell>-</cell><cell>4.0000</cell><cell>16.0000</cell><cell>-</cell><cell>0.6928</cell><cell>2</cell><cell>0.0235</cell><cell>0.0000</cell></row><row><cell>Gauss3</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>32.0000</cell><cell>-</cell><cell>0.9972</cell><cell>4</cell><cell>0.1266</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>128.0000</cell><cell>s Suyken i</cell><cell>0.9973</cell><cell>2.5</cell><cell>0.2594</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>0.1250</cell><cell>32.0000</cell><cell></cell><cell>0.9973</cell><cell>2.5</cell><cell>0.9078</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.075</cell><cell>-</cell><cell>0.1250</cell><cell>64.0000</cell><cell>-</cell><cell>0.9974</cell><cell>1</cell><cell>0.5702</cell><cell>0.0000</cell></row><row><cell>Heart Disease</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>4.0000</cell><cell>-</cell><cell>-0.4059</cell><cell>4</cell><cell>0.7656</cell><cell>0.0016</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.5000</cell><cell>8.0000</cell><cell>s Suyken i</cell><cell>0.5298</cell><cell>3</cell><cell>2.6112</cell><cell>0.0031</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>1.0000</cell><cell>4.0000</cell><cell></cell><cell>0.7240</cell><cell>1</cell><cell>18.2749</cell><cell>0.0015</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.001</cell><cell>0.025</cell><cell>-</cell><cell>1.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.5988</cell><cell>2</cell><cell>3.6968</cell><cell>0.0000</cell></row><row><cell>Crabs</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>128.0000</cell><cell>-</cell><cell>0.9876</cell><cell>3</cell><cell>0.0812</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>128.0000</cell><cell>s hyp À exp i</cell><cell>0.9876</cell><cell>3</cell><cell>0.1656</cell><cell>0.0016</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>15.0</cell><cell>1.0000</cell><cell>128.0000</cell><cell></cell><cell>0.9876</cell><cell>3</cell><cell>0.3608</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.100</cell><cell>-</cell><cell>1.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.9877</cell><cell>1</cell><cell>0.3689</cell><cell>0.0000</cell></row><row><cell>Compass</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>2.0000</cell><cell>-</cell><cell>0.3941</cell><cell>4</cell><cell>0.0144</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.0000</cell><cell>256.0000</cell><cell>s Suyken i</cell><cell>0.6086</cell><cell>3</cell><cell>0.0250</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>1.0000</cell><cell>2.0000</cell><cell></cell><cell>0.6130</cell><cell>2</cell><cell>0.2347</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.125</cell><cell>-</cell><cell>4.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.8760</cell><cell>1</cell><cell>0.0685</cell><cell>0.0000</cell></row><row><cell>Bolts</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.5020</cell><cell>4</cell><cell>0.0032</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.0000</cell><cell>512.0000</cell><cell>s hyp À lin i</cell><cell>0.6010</cell><cell>3</cell><cell>0.0032</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>8.0000</cell><cell>512.0000</cell><cell></cell><cell>0.8768</cell><cell>1</cell><cell>0.0359</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.075</cell><cell>-</cell><cell>8.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.7293</cell><cell>2</cell><cell>0.0094</cell><cell>0.0000</cell></row><row><cell>Motocycle</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>512.0000</cell><cell>-</cell><cell>0.4818</cell><cell>4</cell><cell>0.0219</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>128.0000</cell><cell>s hyp À lin i</cell><cell>0.5176</cell><cell>3</cell><cell>0.0235</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>0.1250</cell><cell>512.0000</cell><cell></cell><cell>0.6854</cell><cell>2</cell><cell>0.8609</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.01</cell><cell>0.050</cell><cell>-</cell><cell>0.1250</cell><cell>4.0000</cell><cell>-</cell><cell>0.6911</cell><cell>1</cell><cell>0.1156</cell><cell>0.0000</cell></row><row><cell>Servo</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>-</cell><cell>0.7466</cell><cell>4</cell><cell>0.0468</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>256.0000</cell><cell>s Suyken i</cell><cell>0.8763</cell><cell>2</cell><cell>0.0781</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0</cell><cell>1.0000</cell><cell>32.0000</cell><cell></cell><cell>0.8561</cell><cell>3</cell><cell>1.8952</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.001</cell><cell>0.025</cell><cell>-</cell><cell>1.0000</cell><cell>256.0000</cell><cell>-</cell><cell>0.8936</cell><cell>1</cell><cell>0.3249</cell><cell>0.0015</cell></row><row><cell>Auto MPG</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>32.0000</cell><cell>-</cell><cell>0.9333</cell><cell>4</cell><cell>0.8671</cell><cell>0.0032</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>256.0000</cell><cell>s Suyken i</cell><cell>0.9413</cell><cell>2</cell><cell>1.4564</cell><cell>0.0032</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>10.0</cell><cell>1.0000</cell><cell>32.0000</cell><cell></cell><cell>0.9349</cell><cell>3</cell><cell>5.7719</cell><cell>0.0031</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.001</cell><cell>0.025</cell><cell>-</cell><cell>1.0000</cell><cell>256.0000</cell><cell>-</cell><cell>0.9416</cell><cell>1</cell><cell>2.7312</cell><cell>0.0016</cell></row><row><cell>Bodyfat</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.0000</cell><cell>256.0000</cell><cell>-</cell><cell>0.9979</cell><cell>4</cell><cell>0.1997</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.0000</cell><cell>512.0000</cell><cell>s Suyken i</cell><cell>0.9997</cell><cell>1</cell><cell>0.3279</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>10.0</cell><cell>8.0000</cell><cell>256.0000</cell><cell></cell><cell>0.9993</cell><cell>3</cell><cell>1.3656</cell><cell>0.0015</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.001</cell><cell>0.025</cell><cell>-</cell><cell>8.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.9994</cell><cell>2</cell><cell>0.8172</cell><cell>0.0000</cell></row><row><cell>Enso</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.0625</cell><cell>8.0000</cell><cell>-</cell><cell>0.6891</cell><cell>4</cell><cell>0.0439</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.0625</cell><cell>8.0000</cell><cell>s hyp À exp i</cell><cell>0.6905</cell><cell>3</cell><cell>0.0876</cell><cell>0.0000</cell></row><row><cell></cell><cell>IRLS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>0.0625</cell><cell>8.0000</cell><cell></cell><cell>0.6967</cell><cell>2</cell><cell>0.3437</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.1</cell><cell>0.300</cell><cell>-</cell><cell>0.0625</cell><cell>16.0000</cell><cell>-</cell><cell>0.7059</cell><cell>1</cell><cell>0.2015</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Detailed information of ten benchmark datasets.</figDesc><table><row><cell>X. Yang et al. / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>Number of</cell><cell>Number of</cell><cell>Number of</cell></row><row><cell></cell><cell>Examples</cell><cell>Classes</cell><cell>Attributes</cell></row><row><cell>Banana</cell><cell>400</cell><cell>2</cell><cell>2</cell></row><row><cell>Cleveland heart</cell><cell>303</cell><cell>5</cell><cell>13</cell></row><row><cell>Glass</cell><cell>211</cell><cell>6</cell><cell>9</cell></row><row><cell>Heartstatlog</cell><cell>270</cell><cell>2</cell><cell>13</cell></row><row><cell>Liver disorder</cell><cell>345</cell><cell>2</cell><cell>6</cell></row><row><cell>Monk</cell><cell>122</cell><cell>2</cell><cell>6</cell></row><row><cell>PIMA</cell><cell>768</cell><cell>2</cell><cell>8</cell></row><row><cell>Ripley</cell><cell>250</cell><cell>2</cell><cell>2</cell></row><row><cell>Transfusion</cell><cell>748</cell><cell>2</cell><cell>4</cell></row><row><cell>Vehicle</cell><cell>846</cell><cell>4</cell><cell>18</cell></row></table><note><p>R À ða; bÞ ¼ ∑ d i o 0</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Comparison of the results of LS-SVM, WLS-SVM and RLS-SVM on two synthetic classification datasets.</figDesc><table><row><cell>Datasets</cell><cell>Algorithms</cell><cell>p</cell><cell>h</cell><cell>s</cell><cell>C</cell><cell>The optimal weight</cell><cell>Testing accuracy</cell></row><row><cell>Synthetic dataset one</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.0625</cell><cell>1.0000</cell><cell>-</cell><cell>0.9561</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.0625</cell><cell>128.0000</cell><cell>s Suyken i</cell><cell>0.9561</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.2</cell><cell>0.025</cell><cell>32</cell><cell>256.0000</cell><cell>-</cell><cell>1.0000</cell></row><row><cell>Synthetic dataset two</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.2500</cell><cell>128.000</cell><cell>-</cell><cell>0.9512</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.5000</cell><cell>512.0000</cell><cell>s hyp À exp i</cell><cell>0.9805</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.2</cell><cell>0.025</cell><cell>32.0000</cell><cell>256.0000</cell><cell>-</cell><cell>1.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Comparison of the results of LS-SVM, WLS-SVM and RLS-SVM on ten benchmark classification datasets.</figDesc><table><row><cell>Datasets</cell><cell>Algorithms</cell><cell>p</cell><cell>h</cell><cell>s</cell><cell>C</cell><cell>The optimal weight</cell><cell>Testing accuracy</cell><cell>Ranks</cell><cell>Training time (s)</cell><cell>Testing time (s)</cell></row><row><cell>Banana</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>1.0000</cell><cell>-</cell><cell>0.8850</cell><cell>3</cell><cell>0.7717</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>2.0000</cell><cell>s hyp À exp i</cell><cell>0.8875</cell><cell>1.5</cell><cell>0.9094</cell><cell>0.0016</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.2</cell><cell>0.050</cell><cell>0.1250</cell><cell>8.0000</cell><cell>-</cell><cell>0.8875</cell><cell>1.5</cell><cell>1.7048</cell><cell>0.0000</cell></row><row><cell>Cleveland Heart</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>16.000</cell><cell>16.000</cell><cell>-</cell><cell>0.5433</cell><cell>3</cell><cell>0.6420</cell><cell>0.0047</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>16.0000</cell><cell>8.0000</cell><cell>s Suyken i</cell><cell>0.5567</cell><cell>2</cell><cell>1.0487</cell><cell>0.0062</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.2</cell><cell>0.025</cell><cell>8.0000</cell><cell>256.0000</cell><cell>-</cell><cell>0.5633</cell><cell>1</cell><cell>1.6405</cell><cell>0.0077</cell></row><row><cell>Glass</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>8.0000</cell><cell>-</cell><cell>0.6952</cell><cell>2.5</cell><cell>0.2153</cell><cell>0.0062</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>64.0000</cell><cell>s hyp À exp i</cell><cell>0.6952</cell><cell>2.5</cell><cell>0.3216</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.2</cell><cell>0.450</cell><cell>2.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.7381</cell><cell>1</cell><cell>0.5845</cell><cell>0.0000</cell></row><row><cell>Heartstalog</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>32.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.8407</cell><cell>2</cell><cell>0.2780</cell><cell>0.0015</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>32.0000</cell><cell>256.0000</cell><cell>s cen À exp i</cell><cell>0.8407</cell><cell>2</cell><cell>0.3609</cell><cell>0.0030</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>2.0</cell><cell>0.150</cell><cell>2.0000</cell><cell>1.0000</cell><cell>-</cell><cell>0.8407</cell><cell>2</cell><cell>0.6797</cell><cell>0.0000</cell></row><row><cell>Liver disorder</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0000</cell><cell>128.0000</cell><cell>-</cell><cell>0.7735</cell><cell>3</cell><cell>0.5280</cell><cell>0.0015</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0000</cell><cell>128.0000</cell><cell>s cen À lin i</cell><cell>0.7824</cell><cell>1.5</cell><cell>0.7437</cell><cell>0.0016</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>3.0</cell><cell>0.025</cell><cell>4.0000</cell><cell>128.0000</cell><cell>-</cell><cell>0.7824</cell><cell>1.5</cell><cell>1.1063</cell><cell>0.0000</cell></row><row><cell>Monk</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>4.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.9167</cell><cell>2</cell><cell>0.0314</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>64.0000</cell><cell>s sph À exp i</cell><cell>0.9167</cell><cell>2</cell><cell>0.0470</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>3.0</cell><cell>0.025</cell><cell>4.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.9167</cell><cell>2</cell><cell>0.0683</cell><cell>0.0000</cell></row><row><cell>PIMA</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>8.0000</cell><cell>256.0000</cell><cell>-</cell><cell>0.7750</cell><cell>3</cell><cell>5.5578</cell><cell>0.0046</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>4.0000</cell><cell>s hyp À lin i</cell><cell>0.7789</cell><cell>2</cell><cell>12.2968</cell><cell>0.0109</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>2.0</cell><cell>0.350</cell><cell>4.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.7816</cell><cell>1</cell><cell>11.1577</cell><cell>0.0046</cell></row><row><cell>Ripley</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.5000</cell><cell>8.0000</cell><cell>-</cell><cell>0.8800</cell><cell>3</cell><cell>0.1717</cell><cell>0.0000</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.1250</cell><cell>16.0000</cell><cell>s Suyken i</cell><cell>0.8960</cell><cell>2</cell><cell>0.3733</cell><cell>0.0000</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.8</cell><cell>0.125</cell><cell>0.2500</cell><cell>2.0000</cell><cell>-</cell><cell>0.9000</cell><cell>1</cell><cell>0.4031</cell><cell>0.0000</cell></row><row><cell>Transfusion</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>1.0000</cell><cell>64.0000</cell><cell>-</cell><cell>0.8243</cell><cell>3</cell><cell>5.0513</cell><cell>0.0047</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>0.2500</cell><cell>1.0000</cell><cell>s Suyken i</cell><cell>0.8284</cell><cell>2</cell><cell>11.5060</cell><cell>0.0109</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>0.8</cell><cell>0.100</cell><cell>0.1250</cell><cell>2.0000</cell><cell>-</cell><cell>0.8311</cell><cell>1</cell><cell>10.2362</cell><cell>0.0047</cell></row><row><cell>Vehicle</cell><cell>LS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>512.0000</cell><cell>-</cell><cell>0.8571</cell><cell>3</cell><cell>7.3796</cell><cell>0.0331</cell></row><row><cell></cell><cell>WLS-SVM</cell><cell>-</cell><cell>-</cell><cell>2.0000</cell><cell>512.0000</cell><cell>s hyp À exp i</cell><cell>0.8583</cell><cell>2</cell><cell>14.1562</cell><cell>0.0329</cell></row><row><cell></cell><cell>RLS-SVM</cell><cell>2.0</cell><cell>0.225</cell><cell>2.0000</cell><cell>128.0000</cell><cell>-</cell><cell>0.8726</cell><cell>1</cell><cell>16.0751</cell><cell>0.0328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>The sorted results of significances of RLS-SVM VS. LS-SVM and WLS-SVM for classification.</figDesc><table><row><cell>i</cell><cell>Learning machines</cell><cell>z ¼ ðR i À R 3 Þ=</cell><cell>q</cell><cell>ffiffiffiffiffiffiffiffiffiffiffi ffi kðk þ 1Þ 6N</cell><cell>p</cell><cell>α k À i</cell></row><row><cell>1</cell><cell>LS-SVM</cell><cell cols="3">(2.75 À 1.30)/0.447¼3.244</cell><cell>0.001</cell><cell>0.025</cell></row><row><cell>2</cell><cell>WLS-SVM</cell><cell cols="3">(1.95 À 1.30)/0.0477 ¼1.454</cell><cell>0.146</cell><cell>0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>value of |d i |. d i and rank(d i ) on ten classification datasets are reported in Table</figDesc><table><row><cell></cell><cell>WLS-SVM</cell><cell>RLS-SVM</cell><cell>d i</cell><cell>rank(d i )</cell></row><row><cell>Banana</cell><cell>0.8875</cell><cell>0.8875</cell><cell>0.0000</cell><cell>2.5</cell></row><row><cell>Cleveland heart</cell><cell>0.5567</cell><cell>0.5633</cell><cell>0.0066</cell><cell>8</cell></row><row><cell>Glass</cell><cell>0.6952</cell><cell>0.7381</cell><cell>0.0429</cell><cell>10</cell></row><row><cell>Heartstalog</cell><cell>0.8407</cell><cell>0.8407</cell><cell>0.0000</cell><cell>2.5</cell></row><row><cell>Liver disorder</cell><cell>0.7824</cell><cell>0.7824</cell><cell>0.0000</cell><cell>2.5</cell></row><row><cell>Monk</cell><cell>0.9167</cell><cell>0.9167</cell><cell>0.0000</cell><cell>2.5</cell></row><row><cell>PIMA</cell><cell>0.7789</cell><cell>0.7816</cell><cell>0.0027</cell><cell>5.5</cell></row><row><cell>Ripley</cell><cell>0.8960</cell><cell>0.9000</cell><cell>0.004</cell><cell>7</cell></row><row><cell>Transfusion</cell><cell>0.8284</cell><cell>0.8311</cell><cell>0.0027</cell><cell>5.5</cell></row><row><cell>Vehicle</cell><cell>0.8583</cell><cell>0.8726</cell><cell>0.0143</cell><cell>9</cell></row></table><note><p>rank</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Yang et al. / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article as: X. Yang, et al., A robust least squares support vector machine for regression and classification with noise, Neurocomputing (2014), http://dx.doi.org/10.1016/j.neucom.2014.03.037i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work presented in this paper is supported by the National Science Foundation of China (61273295), the Major Project of the National Social Science Foundation of China (11&amp;ZD156), and the Open Project of Key Laboratory of Symbolic Computation and Knowledge Engineering of the Chinese Ministry of Education (93K-17-2009-K04).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improvement to Platt&apos;s SMO algorithm for SVM classifier design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R K</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Core vector machines: fast SVM training on very large data sets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A study on SMO-type decomposition methods for support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="908" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tree decomposition for large-scale SVM problems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2935" to="2972" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improvements to SMO algorithm for SVM regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R K</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1188" to="1193" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient SVM regression training with SMO</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Flake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="271" to="290" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text document preprocessing with the Bayes formula for classification using the support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Isa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Kallimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. and Eng</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1264" to="1272" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study of spam filtering using support vector machines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Amayri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="108" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-dependent kernels for object classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="699" to="708" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A learning-based similarity fusion and filtering approach for biomedical image retrieval using SVM classification and relevance feedback</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="646" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint feature re-extraction and classification using an iterative semi-supervised support vector machine algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="53" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature selection via sensitivity analysis of SVM probabilistic outputs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient sparse kernel feature extraction based on partial least squares</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dhanjal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1347" to="1361" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A web search engine-based approach to measure semantic similarity between words</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="977" to="990" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector machine with adaptive parameters in financial time series forecasting</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1518" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Objective image quality estimation based on support vector regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="519" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Benchmarking least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dedene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ridge regression learning algorithm in dual variables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Machine Learning ICML-98</title>
		<meeting>the 15th International Conference on Machine Learning ICML-98</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="515" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Financial time series prediction using least squares support vector machines within the evidence framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Baestaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vandaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="809" to="821" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New least squares support vector machines based on matrix patterns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers: a large scale algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Dooren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Europe Conference on Circuit Theory and Design (ECCTD&apos;99)</title>
		<meeting>Europe Conference on Circuit Theory and Design (ECCTD&apos;99)<address><addrLine>Stresa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="839" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparison of iterative methods for least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hamers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Esat-Sista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internal Report</title>
		<imprint>
			<biblScope unit="page" from="1" to="110" />
			<date type="published" when="2001">2001</date>
			<pubPlace>Leuven, Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An improved conjugate gradient scheme to the solution of least squares SVM</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="501" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SMO algorithm for least squares SVM formulations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="487" to="507" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient computations for large least square support vector machine classifiers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="75" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text classification: a least square support vector machine approach</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="908" to="914" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multisource image fusion method using support value transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1831" to="1839" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A combined MRI and MRSI based multiclass system for brain tumour recognition using LS-SVMs with class probabilities and feature selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heerschap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="102" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Model selection for the LS-SVM application to handwriting recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Adankon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3264" to="3270" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complicated financial data time series forecasting analysis based on least square support vector machine</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Tsinghua Univ. (Sci. Technol.)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1147" to="1149" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variance minimization least squares support vector machines for time series analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ormandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8th IEEE International Conference on Data Mining</title>
		<meeting>8th IEEE International Conference on Data Mining<address><addrLine>Pisa, ITALY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="965" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimal control by least squares support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Implementation of speed controller for rotary hydraulic motor based on LS-SVM</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Bardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14249" to="14256" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A heuristic training-based least squares support vector machines for power system stabilization by SMES</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pahasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ngamroo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13987" to="13993" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse approximation using least squares support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Circuits and Systems</title>
		<meeting>IEEE International Symposium on Circuits and Systems<address><addrLine>Genvea, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="757" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A weighted L q adaptive least squares support vector machine classifiers-robust and sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2253" to="2259" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A heuristic weight-setting and iteratively updating algorithm for weighted least squares support vector regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="3096" to="3103" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weighted least squares support vector machine local region method for nonlinear time series prediction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="562" to="566" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pruning error minimization in least squares support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Kruif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="696" to="702" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comparison of pruning algorithms for sparse least squares support vector machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoegaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICONIP 2004</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>ICONIP 2004<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3316</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SMO based pruning methods for sparse least squares support vector machines</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1541" to="1546" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive pruning algorithm for least squares support vector machine classifier</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="667" to="680" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weighted least squares support vector machines: robustness and sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Cawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pattern classification with mixtures of weighted least-squares support vector machine experts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A M</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L V</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Von Zuben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="843" to="860" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">De</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pelckmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debruyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<title level="m">Proceedings of the 19th International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the 19th International Conference on Artificial Neural Networks (ICANN)<address><addrLine>Limassol, Cyprus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
	<note>Robustness of kernel based regression: a comparison of iterative weighting schemes</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust feature selection based on regularized brownboost loss</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="180" to="198" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Support vector machine classifier with pinball loss</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.178</idno>
		<ptr target="http://doi.ieeecomputersociety.org/10.1109/TPAMI.2013.178" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<meeting>IEEE Transactions on Pattern Analysis and Machine Intelligence</meeting>
		<imprint>
			<publisher>IEEE computer Society Digital Library. IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust support vector machine training via convex outlier ablation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;06: Proceedings of the 21 st National Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="536" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust error measure for supervised neural network learning with outliers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="250" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training a support vector machine in the primal</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1155" to="1178" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">FSVM-CIL: fuzzy support vector machines for class imbalance learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Batuwita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="571" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cautionary note about R2</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Kvalseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Stat</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="285" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leroy</surname></persName>
		</author>
		<title level="m">Robust Regression and Outlier Detection</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust least squares support vector machine based on recursive outlier elimination</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1241" to="1251" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A sharper Bonferroni procedure for multiple tests of significance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="800" to="802" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Knowledge Acquisition from Databases</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Ablex Publishing Corporation</publisher>
			<pubPlace>Norwood, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Neural Networks</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scand. J. Stat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He is currently a full time professor in the Department of Mathematics, South China University of Technology. His current research interests include designs and analyses of algorithms for large-scale pattern recognitions, imbalanced learning, semi-supervised learning, support vector machines, tensor learning, and evolutionary computation</title>
	</analytic>
	<monogr>
		<title level="m">Liangjun Tan received the B.S. degree in information management and information system, the M.Sc. degree in computational mathematics from South China University of Technology</title>
		<title level="s">Xiaowei Yang received the B.S. degree in theoretical and applied mechanics, the M.Sc. degree in computational mechanics, and the Ph.D. degree in solid mechanics from Jilin University</title>
		<meeting><address><addrLine>Changchun, China; Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991. 1996, and 2000. 2010 and 2013</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, South China University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include robust learning, support vector machines, and tensor learning. Lifang He received the B.S. degree in information and computing science from the Northwest Normal University in 2009. She is currently a Ph.D. candidate in the. Her current research interests include manifold learning, machine learning, tensor learning, and pattern recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
