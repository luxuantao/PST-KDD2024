<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LieTransformer: Equivariant self-attention for Lie Groups</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-20">20 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Hutchinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charline</forename><surname>Le Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheheryar</forename><surname>Zaidi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yee</forename><surname>Whye Teh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DeepMind</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Work in progress</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LieTransformer: Equivariant self-attention for Lie Groups</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-20">20 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2012.10885v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to non-linear neural network modules, namely selfattention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.</p><p>* Equal contribution, with alphabetical ordering. See Appendix A for detailed contributions. Please cite as: [Hutchinson,  Le Lan, Zaidi et al. 2020].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Group equivariant neural networks are useful architectures for problems with certain symmetries, that can be described in terms of a group (in the mathematical sense). Convolutional neural networks (CNNs) are a special case that deal with translational symmetry, in that when the input to a convolutional layer is translated, the output is also translated. This property is known as translation equivariance, and offers a useful inductive bias for perception tasks that usually have translational symmetry. Despite the convolutional layer having far fewer parameters than a fully-connected linear layer of the same input and output dimensionality, it is sufficiently expressive to be useful for such perception tasks. This has led to the success of CNNs in multiple domains such as computer vision <ref type="bibr" target="#b32">(Krizhevsky et al., 2012)</ref> and audio <ref type="bibr" target="#b16">(Graves &amp; Jaitly, 2014)</ref>.</p><p>Following on from this success, there has been a growing literature on the study of group equivariant CNNs (G-CNNs) that generalise CNNs to deal with other types of symmetries beyond translations, such as rotations and reflections. The benefit of the G-CNN is that if it learns to detect edges or patterns at a particular orientation, then it will also have learned to detect that edge or pattern at any orientation. Instead of the data augmentation approach used for vanilla CNNs, to 'train' the symmetry into the model, G-CNNs instead have a built in symmetry that leads to improvements in performance and data efficiency <ref type="bibr" target="#b5">(Cohen &amp; Welling, 2016a;</ref><ref type="bibr">b)</ref>.</p><p>All works on G-CNNs deal with CNNs i.e. linear maps with shared weights composed with pointwise non-linearities, building on the result that group equivariant linear maps (with mild assumptions) are necessarily convolutions <ref type="bibr">(Kondor &amp; Trivedi, 2018;</ref><ref type="bibr" target="#b8">Cohen et al., 2019;</ref><ref type="bibr" target="#b2">Bekkers, 2020)</ref>. However there has been little work on non-linear group equivariant building blocks. In this paper we extend group equivariance to self-attention <ref type="bibr" target="#b48">(Vaswani et al., 2017)</ref>, a non-trivial non-linear map, with the following motivations:</p><p>(1) Self-attention has become a prominent building block of deep learning models in various data modalities, such as natural-language processing <ref type="bibr" target="#b48">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Brown et al., 2020</ref><ref type="bibr">), computer vision (Zhang et al., 2019;</ref><ref type="bibr" target="#b40">Parmar et al., 2019b)</ref>, reinforcement learning <ref type="bibr" target="#b38">(Parisotto et al., 2020)</ref>, and audio generation <ref type="bibr" target="#b21">(Huang et al., 2019)</ref>. (2) While convolutions are inherently translation equivariant, self-attention is permutation equivariant, hence it is a natural starting point for generalising permutation equivariance to other groups.</p><p>We thus propose LieTransformer, a group invariant Transformer built from equivariant LieSelfAttention layers. It uses a lifting based approach, that relaxes constraints on the attention module compared to approaches without lifting. Our method is applicable to Lie groups and their discrete subgroups (e.g. cyclic groups C n and dihedral groups D n ) acting on homogeneous spaces. We consider a wide range of tasks to demonstrate the generality of our approach, namely shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We begin by introducing several concepts that are necessary for the rest of the paper, namely group equivariance, equivariant maps on homogeneous input spaces, group equivariant convolutions and self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Group Equivariance</head><p>Central to our work are the concepts of group theory and representation theory. This section lays down some of the necessary definitions and notations. For a more formal overview of the topic, we refer the reader to <ref type="bibr" target="#b10">Esteves (2020)</ref>.</p><p>Loosely speaking, a group is a set of symmetries. Definition 1. A group G is a set endowed with a single operator</p><formula xml:id="formula_0">• : G × G → G such that 1. Associativity: ∀g, g , g ∈ G, (g • g ) • g = g • (g • g ) 2. Identity: ∃e ∈ G, ∀g ∈ G g • e = e • g = g 3. Invertibility: ∀g ∈ G, ∃g −1 ∈ G, g •g −1 = g −1 •g = e</formula><p>An example of a discrete (finite) group is C n , the set of rotational symmetries of a regular n-gon.</p><p>Note that there are n such rotations, including the identity. An example of a continuous (infinite) group is SO(2), the set of all 2D rotations. C n is a subset of SO(2), hence we call C n a subgroup of SO(2). Note that SO(2) can be parameterised by the angle of rotation SO(2) = {g θ : θ ∈ [0, 2π)}. Such groups that can be continuously parameterised by real values are called Lie groups. Formally, a Lie group is a finite-dimensional real smooth manifold, in which group multiplication and inversion are both smooth maps. The general linear group GL(n, R) of invertible n×n matrices is an example of a Lie group.</p><p>We can define how a group acts on an object using symmetry transformations as follows: Definition 2. Let S be a set, and let Sym(S) denote the set of invertible functions from S to itself. We say that a group G acts on S via an action ρ :</p><formula xml:id="formula_1">G → Sym(S) when ρ is a group homomorphism: ρ(g 1 g 2 )(s) = (ρ(g 1 ) • ρ(g 2 ))(s) ∀s ∈ S.</formula><p>If S is a vector space V and this action is, in addition, a linear function, i.e. ρ : G → GL(V ), where GL(V ) is the set of linear invertible functions from V to itself, then we say that ρ is a representation of G.</p><p>For SO(2), the standard rotation matrix is an example of a representation that acts on V = R 2 :</p><formula xml:id="formula_2">ρ(g θ ) = cos θ − sin θ sin θ cos θ (1)</formula><p>Note that this is only one of many possible representations of SO(2) acting on R 2 (e.g. replacing θ with nθ yields another valid representation), and SO(2) can act on spaces other than R 2 e.g. R d for arbitrary d ≥ 2.</p><p>In the context of group equivariant neural networks, V is commonly defined to be the space of scalar-valued functions on some set S, so that V = {f | f : S → R}. This set could be a Euclidean input space e.g. a grey-scale image can be expressed as a feature map from pixel coordinates to pixel intensities f : R 2 → R supported on the grid of pixel coordinates. We may express the rotation of the image as a representation of SO(2) by extending the action ρ on the pixel coordinates to a representation π that acts on the space of feature maps:</p><formula xml:id="formula_3">[π(g θ )(f )](x) f (ρ(g −1 θ )x).<label>(2)</label></formula><p>As a special case, we can define V = {f |f : G → R} to be the space of scalar-valued functions on the group G, for which we can define a representation π acting on V via the regular representation:</p><formula xml:id="formula_4">[π(g θ )(f )](g φ ) f (g −1 θ g φ ).</formula><p>(3)</p><p>Here the action ρ is replaced by the action of the group on itself. If we wish to handle multiple channels of data, e.g. RGB images, we can stack these feature maps together, transforming in a similar manner. Now let us define the notion of equivariance with respect to a group. Definition 3. We say that a map</p><formula xml:id="formula_5">Φ : V 1 → V 2 is G- equivariant with respect to representations ρ 1 , ρ 2 of G act- ing on V 1 , V 2 respectively if: Φ[ρ 1 (g)f ] = ρ 2 (g)Φ[f ] for any g ∈ G, f ∈ V 1 .</formula><p>In the above example of rotating RGB images, we have G = SO(2) and ρ 1 = ρ 2 = π. Hence the equivariance of Φ means that rotating an input image and then applying Φ yields the same result as first applying Φ to the original input image and then rotating the output, i.e. Φ commutes with the representation π.</p><p>The end goal for group equivariant neural networks is to design a neural network that obeys certain symmetries in the data. For example, we may want an image classifier to output the same classification when the input image is rotated. So in fact we want a G-invariant neural network, where the output is invariant to group actions on the input space. Note that G-invariance is a special case of G-equivariance, where ρ 2 is the trivial representation i.e. ρ 2 (g) is the identity map for any g ∈ G. Invariant maps are easy to design, by discarding information, e.g. pooling over spatial dimensions is invariant to rotations and translations. However, such maps are not expressive as they fail to extract high-level features from the data. This is where equivariant neural networks become relevant; the standard recipe for constructing an expressive invariant neural network is to compose multiple equivariant layers with a final invariant layer. It is a standard result that such maps are invariant (e.g. Bloem-Reddy &amp; Teh (2019)) and a proof is given in Appendix B for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Equivariant Maps on Homogeneous Input Spaces</head><p>In this section we introduce the framework for G-equivariant maps, and provide group equivariant convolutions as an example. Suppose we have data in the form of a set of input pairs (x i , f i ) n i=1 where x i ∈ X are spatial coordinates and</p><formula xml:id="formula_6">f i ∈ F are feature values. In practice X = R dx , F = R d f .</formula><p>We assume that a group G acts on the x-space X via action ρ, and that the action is transitive (also referred to as X is homogeneous). This means that all elements of X are connected by the action: ∀x, x ∈ X , ∃g ∈ G : ρ(g)x = x . Let us write gx instead of ρ(g)x to reduce clutter. Now say we choose some origin x 0 ∈ X . Then each x ∈ X corresponds to the set of group elements that map x 0 to x: {g ∈ G | gx 0 = x}. This set can be defined in terms of the origin's stabiliser H {g ∈ G | gx 0 = x 0 }, as a (left) coset gH {gh | h ∈ H} where g is any element of {g ∈ G | gx 0 = x} (the left coset does not depend on the choice of the element). This correspondence between x and gH is described mathematically as an isomorphism between X and G/H {gH | g ∈ G}, the set of cosets of H. So for every point x ∈ X , we can choose a representative element s(x) ∈ G among the elements in the coset that corresponds to x under the isomorphism, i.e. x corresponds to the coset s(x)H. Note that the coset s(x i )H does not depend on the choice of s, but only on the isomorphism between X and G/H. For example, when the group of 2D translations T (2) acts on R 2 , the stabiliser of the origin x 0 is H = {e}, a singleton identity element. Hence the only possible choice for s(x) is t x , the group element describing the translation from the origin x 0 to x. On the other hand SO(2), the group of 2D rotations centered at the origin, does not act transitively on R 2 because points that have different distances to the origin cannot be mapped onto each other via such a rotation. However the group of 2D translations and rotations SE(2) T (2) SO(2) acts transitively on R 2 , with stabiliser H = SO(2), and indeed we have R 2 G/H = SE(2)/SO(2) = T (2). A canonical choice for s(x) is t x , but other elements of the coset t x H are also valid.</p><p>The set of data pairs (x i , f i ) n i=1 can be described as a feature map f X : x i → f i , a function from X G/H to R d f , that can be mapped to a function from G to R d f via a lifting layer described in Section 3.1. Let I U denote the space of such unconstrained functions from G to R d f . Subsequently, we may define the group equivariant maps as functions from I U to itself.</p><p>The group equivariant convolution <ref type="bibr" target="#b5">(Cohen &amp; Welling, 2016a;</ref><ref type="bibr" target="#b7">Cohen et al., 2018;</ref><ref type="bibr" target="#b13">Finzi et al., 2020;</ref><ref type="bibr">Romero et al., 2020)</ref> is an example of such a group equivariant map that has been studied extensively. Convolutions are by definition translation equivariant: translating the input image then passing it through a convolution yields the same output as passing the original image through a convolution then translating it. Hence they are a good starting point for constructing group equivariant maps for groups that contain translations. Specifically, the group equivariant convolution Ψ : I U → I U is defined as:</p><formula xml:id="formula_7">[Ψf ](g) G ψ(g −1 g)f (g )dg<label>(4)</label></formula><p>where ψ : G → R is the convolutional filter and the integral is defined with respect to the left Haar measure of G.</p><p>Note that for discrete groups the integral amounts to a sum over the group. Hence the integral can be computed exactly for discrete groups <ref type="bibr" target="#b5">(Cohen &amp; Welling, 2016a)</ref>, and for Lie groups it can be approximated using Fast Fourier Transforms <ref type="bibr" target="#b7">(Cohen et al., 2018)</ref> or Monte Carlo (MC) estimation <ref type="bibr" target="#b13">(Finzi et al., 2020)</ref>.</p><p>Given the regular representation π of G acting on I U as</p><formula xml:id="formula_8">[π(u)f ](g) f (u −1 g),<label>(5)</label></formula><p>we can easily verify that Ψ is equivariant with respect to π (c.f. Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-attention</head><p>Self-attention <ref type="bibr" target="#b48">(Vaswani et al., 2017)</ref> is a mapping from an input set of N vectors {x 1 , . . . , x N }, where x i ∈ R D , to an output set of N vectors in R D . Let us represent the inputs as a matrix X ∈ R N ×D such that the ith row X i: is x i . Multihead self-attention (MSA) consists of M heads where M is chosen to divide D. The output of each head is a set of N vectors of dimension D/M , where each vector is obtained by taking a weighted average of the input vectors {x 1 , . . . , x N } with weights given by a weight matrix W , followed by a linear map W V ∈ R D×D/M . Using m to index the head (m = 1, . . . , M ), the output of the mth head can be written as:</p><formula xml:id="formula_9">W softmax(XW Q,m (XW K,m ) ) ∈ R N ×N f m (X) W XW V,m ∈ R N ×D/M</formula><p>where W Q,m , W K,m , W V,m ∈ R D×D/M are learnable parameters, and the softmax normalisation is performed on each row of the matrix XW Q,m (XW K,m ) ∈ R N ×N . Finally, the outputs of all heads are concatenated into a N × D matrix and then right multiplied by W O ∈ R D×D . Hence MSA is defined by: Note XW Q (XW K ) is the Gram matrix for the dotproduct kernel, and softmax normalisation is a particular choice of normalisation. Hence MSA can be generalised to other choices of kernels and normalisation that are equally valid <ref type="bibr" target="#b51">(Wang et al., 2018;</ref><ref type="bibr" target="#b46">Tsai et al., 2019)</ref>.</p><formula xml:id="formula_10">M SA(X) [f 1 (X), . . . , f M (X)]W O ∈ R N ×D . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LieTransformer</head><p>We first outline the problem setting before describing our model, the LieTransformer. We tackle the problem of regression/classification for predicting a scalar/vectorvalued target y given a set of input pairs (x i , f i ) n i=1 where x i ∈ R dx are spatial locations and f i ∈ R d f are feature values at the spatial location. Hence the training data of size N is a set of tuples ((x i , f i ) nj i=1 , y j ) N j=1 . In some tasks such as point cloud classification, the feature values f i may not be given. In this case the f i can set to be a fixed constant or a function of (x i ) n i=1 . LieTransformer is composed of a lifting layer followed by residual blocks of LieSelfAttention layers, LayerNorm and pointwise MLPs, all of which are equivariant with respect to the regular representation, followed by a final invariant G-pooling layer (c.f. Appendix E for more details on these layers). We summarise the architecture in Figure <ref type="figure" target="#fig_0">1</ref> and provide details of its key components below. We provide all proofs of equivariance in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lifting</head><p>Recall the following observations from Section 2.2:</p><formula xml:id="formula_11">• A set of input pairs (x i , f i ) n</formula><p>i=1 can be described as a feature map f X : x i → f i supported on the set {x 1 , . . . , x n }.</p><p>• For G acting on a homogeneous space X , there exists an isomorphism X G/H = {gH | g ∈ G} where each x i corresponds to the coset s(x i )H where s(x i ) is a representative element of the coset.</p><p>• The coset s(x i )H does not depend on the choice of s, but only on the isomorphism between X and G/H.</p><p>The lifting L maps f X (supported on</p><formula xml:id="formula_12">n i=1 {x i } ⊂ X ) to L[f X ] (supported on n i=1 s(x i )H ⊂ G) such that: L[f X ](g) f i for g ∈ s(x i )H.<label>(7)</label></formula><p>This can be thought of as extending the domain of f X from X to G, while preserving the feature values f i . See Figure <ref type="figure">2</ref> for a visualisation.</p><p>As in Equation <ref type="formula" target="#formula_3">2</ref>, we define the representation π on f X as:</p><formula xml:id="formula_13">[π(u)f X ](x) = f X (u −1 x)<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">u ∈ G. Note f X (u −1 x) = f i for x = ux i , hence the action simply corresponds to mapping (x i , f i ) to (ux i , f i ).</formula><p>As in Equation 3, we may define the representation π on L[f X ] as:</p><formula xml:id="formula_15">[π(u)L[f X ]](g) = L[f X ](u −1 g)<label>(9)</label></formula><p>The role of this lifting layer is to move our feature maps f X , lying in the space of functions on X , to the space of functions on G. We need to ensure that this is done while preserving equivariance, which is why we need the space to be homogeneous with respect to the action of G on X .</p><p>Proposition 1. The lifting layer L is equivariant with respect to the representation π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LieSelfAttention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><formula xml:id="formula_16">f L[f X ], hence f is defined on the set G f = ∪ n i=1 s(x i )H.</formula><p>We define the LieSelfAttention layer in Algorithm 1, where self-attention is defined across the Algorithm 1 LieSelfAttention</p><formula xml:id="formula_17">Input: {f (g), g} g∈G f for g ∈ G f for g ∈ G f (or nbhd η (g)) Compute content/location attention k c (f (g), f (g )), k l (g −1 g ) Compute unnormalised weights α f (g, g ) = F (k c (f (g), f (g )), k l (g −1 g ))</formula><p>Compute normalised weights and output</p><formula xml:id="formula_18">{w f (g, g )} g ∈G f = norm{α f (g, g )} g ∈G f f out (g) = G f w f (g, g )W V f (g )dg Output: {f out (g)} g∈G f</formula><p>elements of G f . We first describe the method for finite G f , and then generalise to infinite G f at the end of the section.</p><p>There are various choices for functions k c that represents content-based attention, k l that represents location-based attention, F that determines how to combine the two to form unnormalised weights, and the choice of normalisation of weights. We explore the following non-exhaustive list of choices:</p><formula xml:id="formula_19">Content-based attention k c (f (g), f (g )): 1. Dot-product: 1 √ dv W Q f (g) W K f (g ) ∈ R for W Q , W K ∈ R dv×dv 2. Concat: Concat[W Q f (g), W K f (g )] ∈ R 2dv 3. Linear-Concat-linear: W Concat[W Q f (g), W K f (g )] ∈ R ds for W ∈ R ds×2dv .</formula><p>Location-based attention k l (g −1 g ) for Lie groups G:</p><formula xml:id="formula_20">1. Plain: ν[log(g −1 g )] 2. MLP: MLP(ν[log(g −1 g )])</formula><p>where log : G → g is the log map from G to its Lie algebra g, and ν : g → R d is the isomorphism that extracts the free parameters from the output of the log map <ref type="bibr" target="#b13">(Finzi et al., 2020)</ref>. We can use the same log map for discrete subgroups of Lie groups (e.g.</p><formula xml:id="formula_21">C n ≤ SO(2), D n ≤ O(2)</formula><p>). See Appendix C for an introduction to the Lie algebra and the exact form of ν • log(g) for common Lie groups.</p><p>Combining content and location attention α f (g, g ):</p><formula xml:id="formula_22">1. Additive: k c (f (g), f (g )) + k l (g −1 g ) 2. MLP: MLP[Concat[k c (f (g), f (g )), k l (g −1 g )]] 3. Multiplicative: k c (f (g), f (g )) • k l (g −1 g )</formula><p>Note that the MLP case is a strict generalisation of the additive combination, and for this option k c and k l need not be scalars.</p><p>Normalisation of weights {w f (g, g )} g ∈G f :</p><p>1. Softmax:</p><formula xml:id="formula_23">softmax {α f (g, g )} g ∈G f 2. Constant: { 1 |G f | α f (g, g )} g ∈G f</formula><p>Any combination of choices leads to equivariant LieSelfAttention: Proposition 2. LieSelfAttention is equivariant with respect to the regular representation π.</p><p>Multihead equivariant self-attention is a simple extension of the above single-head case. Let M be the number of heads, assuming it divides d v , with m indexing the head.</p><p>Then the output of each head is:</p><formula xml:id="formula_24">V m (g) = G f w f (g, g )W V,m f (g )dg ∈ R dv/M (10) The only difference is that W Q,m , W K,m , W V,m ∈ R dv/M ×dv .</formula><p>The multihead self-attention combines the heads using W O ∈ R dv×dv , to output:</p><formula xml:id="formula_25">f out (g) = W O    V 1 (g) . . . V M (g)   <label>(11)</label></formula><p>Generalisation to infinite G f Note that for Lie Groups, G f is usually infinite (it need not be if H is discrete e.g. for T (n) acting on R n , we have H = {e} hence G f is finite).</p><p>To deal with the infinite case we resort to Monte Carlo (MC) estimation to approximate the above integral, following the approach of <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>:</p><formula xml:id="formula_26">1. Replace G f ∪ n i=1 s(x i )H with a finite subset Ĝf ∪ n i=1 s(x i ) Ĥ</formula><p>where Ĥ is a finite subset of H sampled uniformly.</p><p>2. (Optional, for computational efficiency) Further replace Ĝf with uniform samples from the neighbourhood nbhd η (g) {g ∈ Ĝf : d(g, g ) ≤ η} for some threshold η where distance is measured by the log map</p><formula xml:id="formula_27">d(g, g ) = ||ν[log(g −1 g )]||.</formula><p>See Figure <ref type="figure">2</ref> for a visualisation. Due to MC estimation we now have equivariance in expectation as <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>.</p><p>For sampling within the neighbourhood, we can show that the resulting LieSelfAttention is still equivariant in expectation given that the distance is a function of g −1 g (c.f. Appendix B).</p><p>Figure <ref type="figure">2</ref>. Visualisation of lifting, sampling Ĥ, and subsampling in the local neighbourhood for SE(2) acting on R 2 . Self-attention is performed on this subsampled neighbourhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Equivariant maps with/without lifting Equivariant neural networks can be broadly categorised by whether the input spatial data is lifted onto the space of functions on group G or not. Without lifting, the equivariant map is defined between the space of functions/features on the homogeneous input space X, with equivariance imposing a constraint on the parameterisation of the convolutional kernel or attention module <ref type="bibr" target="#b6">(Cohen &amp; Welling, 2016b;</ref><ref type="bibr" target="#b55">Worrall et al., 2017;</ref><ref type="bibr" target="#b46">Thomas et al., 2018;</ref><ref type="bibr">Kondor et al., 2018;</ref><ref type="bibr" target="#b54">Weiler et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b52">Weiler &amp; Cesa, 2019;</ref><ref type="bibr" target="#b12">Esteves et al., 2020;</ref><ref type="bibr" target="#b14">Fuchs et al., 2020)</ref>. In the case of convolutions, the kernel is expressed using a basis of equivariant functions such as circular or spherical harmonics. However with lifting, the equivariant map is defined between the space of functions/features on G, and aforementioned constraints on the convolutional kernel or attention module are relaxed at the cost of an increased dimensionality of the input to the neural network <ref type="bibr" target="#b5">(Cohen &amp; Welling, 2016a;</ref><ref type="bibr" target="#b7">Cohen et al., 2018;</ref><ref type="bibr" target="#b11">Esteves et al., 2018;</ref><ref type="bibr" target="#b13">Finzi et al., 2020;</ref><ref type="bibr" target="#b2">Bekkers, 2020;</ref><ref type="bibr">Romero et al., 2020;</ref><ref type="bibr" target="#b20">Hoogeboom et al., 2018)</ref>. Our method also uses lifting to define equivariant self-attention.</p><p>Equivariant self-attention Most of the above works use equivariant convolutions as the core building block of their equivariant module, drawing from the result that bounded linear operators are group equivariant if and only if they are convolutions <ref type="bibr">(Kondor &amp; Trivedi, 2018;</ref><ref type="bibr" target="#b8">Cohen et al., 2019;</ref><ref type="bibr" target="#b2">Bekkers, 2020)</ref>  <ref type="formula">2020</ref>) describe group equivariant self-attention also using lifting and regular representations.</p><p>Their analogue of location-based attention are group invariant positional encodings. The main difference between the two works is that Romero &amp; Cordonnier (2020) specify methodology for discrete groups applied to image classification only, whereas our method provides a general formula for (unimodular) Lie groups and their discrete subgroups for the aforementioned applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We consider three different tasks that have certain symmetries, highlighting the benefits of the LieTransformer:</p><p>(1) Counting shapes in 2D point cloud of constellations (2) Molecular property regression and (3) Modelling particle trajectories under Hamiltonian dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Counting Shapes in 2D Point Clouds</head><p>We first consider the toy, synthetic task of counting shapes in a 2D point cloud {x 1 , x 2 , ..., x K } of constellations <ref type="bibr" target="#b31">(Kosiorek et al., 2019)</ref>. Each example consists of K points on the plane that form the vertices of multiple patterns. There are four types of patterns: triangles, squares, pentagons and the 'L' shape, with varying sizes, orientation, and number of instances per pattern (see Figure <ref type="figure" target="#fig_3">3a</ref>). The task is to classify the number of instances of each pattern, hence is invariant to rotations and translations in the plane (SE(2)). The f i are set to be a vector of pairwise distances to every other    <ref type="formula" target="#formula_3">2</ref>), hence we need to subsample Ĝf ) whereas LieTransformer-T2 is exactly T (2)-invariant (G f is infinite since H = e). However, we see that for D SE2 test , LieTransformer-SE2 outperforms LieTransformer-T2, even with D SE2 train . This shows the benefit of using an SE(2)-invariant model over a T (2)-invariant model with SE(2) data augmentation.</p><formula xml:id="formula_28">(2)-invariant in ex- pectation (G f is infinite since H = SO(</formula><p>In Figure <ref type="figure" target="#fig_3">3b</ref>, we report the equivariance error of LieTransformer-SE2 when increasing the number of lift samples (| Ĥ|) used in the Monte Carlo approximation of Equation <ref type="formula">10</ref>. As expected, the invariance error decreases monotonically with the number of lift samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">QM9: Molecular Property Regression from Molecular Geometry</head><p>We apply the LieTransformer to the QM9 molecule property prediction task <ref type="bibr" target="#b56">(Wu et al., 2018)</ref>. This dataset consists of 133,885 small inorganic molecules described by the location and charge of each atom in the molecule, along with the bonding structure of the molecule. The dataset includes 19 properties of each molecule, such as various rotational constants, energies and enthalpies, and 12 of these are used as regression tasks. We expect these molecular properties to be invariant to 3D roto-translations, i.e. SE(3)invariant.</p><p>In applying our model to this task, we ignore the bonding structure of the molecule. As noted in <ref type="bibr">(Klicpera et</ref>   <ref type="bibr" target="#b28">(Klicpera et al., 2020)</ref> .047 35 28 20 .029 .025 9 8 .331 7 8 1.29 L1Net <ref type="bibr" target="#b34">(Miller et al., 2020)</ref> .  <ref type="bibr" target="#b14">(Fuchs et al., 2020)</ref> .148 <ref type="bibr" target="#b13">(Finzi et al., 2020)</ref> . 2020) this should not be needed to learn the task, although it may be helpful as auxiliary information. Given most methods compared against do not use such information, we follows this for a fair comparison (an exception is the SE(3)-Transformer <ref type="bibr" target="#b14">(Fuchs et al., 2020)</ref> that uses the bonding information). It would be possible to utilise the bonding structure both in the neighbourhood selection step and as model features by treating only atoms that are connected via a bond to another atom as in the neighbourhood of that atom.</p><formula xml:id="formula_29">53 36 33 .053 .057 − − − − − − LieConv-T3</formula><p>We follow the customary practice of performing hyperparameter search on the HOM O task and use the same hyperparameters for training on the other 11 tasks. Further details of the exact experimental setup can be found in Appendix E.2.</p><p>Table <ref type="table" target="#tab_5">2</ref> shows the results of running our model on the 12 tasks. The results in Table <ref type="table" target="#tab_5">2</ref> are broken into 3 sections. These detail models specifically designed to target the QM9 problem that are not equivariant, those that specifically target QM9 and are equivariant, and those that are general purpose equivariant models, applied to QM9. We show very competitive results, and perform best of generic equivariant methods on 8/12 tasks. In particular when comparing against LieConv, we see better performance on the majority of tasks, suggesting that the attention framework is better suited to this task than convolutions.</p><p>We trained three variants of both LieTransformer and LieConv.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Modelling Particle Trajectories with Hamiltonian Dynamics</head><p>We also apply the LieTransformer to a physics simulation task in the context of Hamiltonian dynamics. Hamiltonian mechanics is a formalism for describing the evolution of a physical system using a single scalar function, called the Hamiltonian. A notable fact about Hamiltonian dynamics is that symmetries of the Hamiltonian function play an important role in the physical properties of the modelled system. Indeed, a famous result known as Noether's theorem <ref type="bibr" target="#b35">(Noether, 1971)</ref> states that if the Hamiltonian function has a symmetry, the resulting physical system modelled by the Hamiltonian will have a conserved quantity. For example, translation invariance of the Hamiltonian implies conservation of momentum and rotation invariance implies conservation of angular momentum. As conservation laws of a physical system are often known a priori, learning Hamiltonians with symmetries corresponding to these conservation laws would be useful for realistic modelling of particle trajectories.</p><p>For clarity, we consider the case of a single particle, although the Hamiltonian approach easily generalizes to an arbitrary number of particles. Consider a particle with position q ∈ R d and momentum p ∈ R d often compactly written as a single state z = (q, p). We define a scalar function H : R 2d → R called the Hamiltonian, which takes as input the state of the particle and returns its total (potential plus kinetic) energy. The time evolution is then given by the following system of ODEs called Hamilton's equations</p><formula xml:id="formula_30">dq dt = ∂H ∂p , dp dt = − ∂H ∂q .<label>(12)</label></formula><p>As a simple example, it is clear to see that translation invariance (in position) of the Hamiltonian implies conservation of momentum. Indeed, if H(q, p) is translation invariant then ∂H ∂q = 0 and so dp dt = 0, i.e. momentum is conserved. Noether's theorem shows this in more generality: for any symmetry of the Hamiltonian there is a corresponding conserved quantity.</p><p>Several recent papers have considered modeling physical systems by learning the corresponding Hamiltonian of the system instead of learning the dynamics directly <ref type="bibr" target="#b17">(Greydanus et al., 2019;</ref><ref type="bibr" target="#b44">Sanchez-Gonzalez et al., 2019;</ref><ref type="bibr">Zhong et al., 2019;</ref><ref type="bibr" target="#b13">Finzi et al., 2020)</ref>. Specifically, we can parameterise the Hamiltonian by a neural network H θ and learn the dynamics of the system by ensuring trajectories from the ground truth and learned system are close to each other. Given a learned H θ , we can simulate the system for T timesteps by solving equation ( <ref type="formula" target="#formula_30">12</ref>) with a numerical ODE solver to obtain a trajectory {ẑ t } T t=1 and minimize the difference between this trajectory and the ground truth {z t } T t=1 :</p><formula xml:id="formula_31">L(θ) = 1 T t=T t=1 ẑt − z t 2 .<label>(13)</label></formula><p>In our experiments, we parameterise the Hamiltonian H θ by a LieTransformer and endow it with the symmetries corresponding to the conservation laws of the physical system we are modeling. We test our model on the spring dynamics task proposed in <ref type="bibr" target="#b44">Sanchez-Gonzalez et al. (2019)</ref>.  Specifically, we consider a system of n particles with mass m 1 , . . . , m n in two dimensions with each particle connected to all others by springs. This system conserves both linear and angular momentum, so the ground truth Hamiltonian will be both translationally and rotationally invariant. We simulate this system for 500 timesteps from various initial conditions and use random subsets of these roll-outs to train the model (see Appendix E.3 for full experimental details).</p><p>As approaches that explicitly take the symmetries of the Hamiltonian into account have been shown to outperform ones that don't <ref type="bibr" target="#b13">(Finzi et al., 2020)</ref>, we only compare our method to LieConv. Figure <ref type="figure" target="#fig_5">4</ref> shows the performance of our model as a function of the number of training points. As can be seen, the model is highly data-efficient: the inductive bias from the symmetries of the Hamiltonian allow us to accurately learn the dynamics even from a small training set. Further, our model consistently performs better than LieConv suggesting that the attention framework is also useful for this task. In addition, the invariance of the LieTransformer model improves generalization: even though we only train on 5-step roll-outs, our model generalizes well to 100-step roll-outs.</p><p>Figure <ref type="figure">5</ref> shows the test error as function of the roll-out time step for a training data size of 10,000 (corresponding plots for other training data sizes are included in the appendix). As expected, the difference between the ground truth and predicted trajectories increases with time although the error remains low (&lt; 10 −3 after 100 time steps). Further, the LieTransformer model consistently outperforms LieConv. We also include example trajectories of our model in Figure <ref type="figure">6</ref> (more examples can be found in the appendix, including ones where LieConv performs better than LieTransformer) illustrating the accuracy of our model on this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Future Work</head><p>From the algorithmic perspective, LieTransformer shares the weakness of LieConv in being memoryexpensive due to: 1. The lifting procedure that increases the number of inputs by | Ĥ|, and 2. Quadratic complexity in the number of inputs from having to compute the kernel value at each pair of inputs. Although the first cause is a weakness shared by all lifting-based equivariant neural networks, the second cause can be addressed by incorporating works that study efficient variants of self-attention <ref type="bibr" target="#b50">(Wang et al., 2020;</ref><ref type="bibr" target="#b27">Kitaev et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020;</ref><ref type="bibr" target="#b25">Katharopoulos et al., 2020)</ref>. An alternative approach is to incorporate information about pairs of inputs (such as bonding information for the QM9 task) as masking in self-attention.</p><p>From the methodological perspective, a key weakness of the LieTransformer that is also shared with LieConv is its approximate equivariance due to MC estimation of the integral in Equation 10 for the case where H is infinite. The aforementioned directions for memory-efficiency can help to reduce the approximation error by allowing to use more samples after lifting (higher | Ĥ|). An alternative would be to use Fast Fourier Transforms to approximate the integral as in <ref type="bibr" target="#b7">Cohen et al. (2018)</ref>. Other further directions include incorporating the notion of steerability <ref type="bibr" target="#b6">(Cohen &amp; Welling, 2016b)</ref> to deal with vector fields in an equivariant manner (given inputs (x i , f i ), the group acts non-trivially on f i as well as x i ), and extending to non-homogeneous input spaces as outlined in <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>.</p><p>Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J Proof.</p><p>•</p><formula xml:id="formula_32">L[π(u)f X ](g) = f i for g ∈ s(ux i )H. • [π(u)L[f X ]](g) = L[f X ](u −1 g) = f i for g ∈ us(x i )H. • s(ux i )H = us(x i )H ∀u ∈ G.</formula><p>Proposition 2. LieSelfAttention is equivariant with respect to the regular representation π.</p><p>Proof. Let I U = L(G, R D ) be the space of unconstrained functions f : G → R D . We can define the regular representation π of G acting on I U as follows:</p><formula xml:id="formula_33">[π(u)f ](g) = f (u −1 g)<label>(14)</label></formula><p>f is defined on the set G f = ∪ n i=1 s(x i )H (i.e. union of cosets corresponding to each x i ). Note G π(u)f = uG f , and G f does not depend on the choice of section s.</p><p>Note that for all above choices of k c and k l , we have:</p><formula xml:id="formula_34">k c ([π(u)f ](g), [π(u)f ](g )) = k c (f (u −1 g), f (u −1 g )) (15) k l (g −1 g ) = k l ((u −1 g) −1 (u −1 g ))<label>(16)</label></formula><p>Hence for all choices of F , we have that</p><formula xml:id="formula_35">α π(u)f (g, g ) = F (k c ([π(u)f ](g), [π(u)f ](g )), k l (g −1 g )) = F (k c (f (u −1 g), f (u −1 g )), k l ((u −1 g) −1 u −1 g )) = α f (u −1 g, u −1 g )<label>(17)</label></formula><p>We thus prove equivariance for the below choice of LieSelfAttention Φ : I U → I U that uses softmax normalisation, but a similar proof holds for constant normalisation. Let A f (g, g ) exp(α f (g, g )), hence Equation ( <ref type="formula" target="#formula_35">17</ref>) also holds for A f .</p><p>[Φf ](g) =</p><formula xml:id="formula_36">G f w f (g, g )f (g )dg (18) = G f A f (g, g ) G f A f (g, g )dg f (g )dg<label>(19)</label></formula><p>Hence:</p><formula xml:id="formula_37">w π(u)f (g, g ) = A π(u)f (g, g ) G π(u)f A π(u)f (g, g )dg = A f (u −1 g, u −1 g ) uG f A f (u −1 g, u −1 g )dg = A f (u −1 g, u −1 g )</formula><p>Then we can show that Φ is equivariant with respect to the representation π as follows:</p><formula xml:id="formula_38">Φ[π(u)f ](g) = G π(u)f w π(u)f (g, g )[π(u)f ](g )dg = uG f w f (u −1 g, u −1 g )f (u −1 g )dg = G f w f (u −1 g, g )f (g )dg = [Φf ](u −1 g) = [π(u)[Φf ]](g)<label>(21)</label></formula><p>Equivariance holds for any α f that satisfies Equation ( <ref type="formula" target="#formula_35">17</ref>). Multiplying α f by an indicator function 1{d(g, g ) &lt; λ} where d(g, g ) is some function of g −1 g , we can show that local self-attention that restricts attention to points in a neighbourhood also satisfies equivariance. When approximating the integral with Monte Carlo samples (equivalent to replacing G f with Ĝf ) we obtain a self-attention layer that is equivariant in expectation for constant normalisation of attention weights (i.e.</p><formula xml:id="formula_39">E[ Φ[π(u)f ](g)] = Φ[π(u)f ](g) = Φ[π(u)f ](g)</formula><p>where Φ is the same as Φ but with Ĝf instead of G f ). However for softmax normalisation we obtain a biased estimate due to the nested MC estimate in the denominator's normalising constant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Memory and Time Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PointConv Trick</head><p>One-line summary: instead of applying a shared linear map then summing across nbhd, first sum across nbhd then apply the linear map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Equivariant Self-Attention</head><p>• Inputs: {g, f (g)} g∈G f where f (g) ∈ R dv -G f defined as in Section 3.1.</p><p>• Outputs: {g, f (g) + g ∈nbhd(g) w f (g, g )W V f (g )} g∈G f where nbhd(g) = {g ∈ G f : ν[log(g)] &lt; r}. Let us assume that |nbhd(g)| ≈ n ∀g.</p><p>-{w f (g, g )} g ∈G f = softmax {α f (g, g )} g ∈G f α f (g, g ) = k f (f (g), f (g )) + k x (g −1 g )</p><formula xml:id="formula_40">-k f (f (g), f (g )) = W Q f (g) W K f (g ) ∈ R -k x (g) = MLP φ (ν[log(g)]) ∈ R -W Q , W K , W V ∈ R dv×dv .</formula><p>• Memory: Store α f (g, g ) and W V f (g ) ∀g ∈ G f , g ∈ nbhd(g). This requires O(|G f |nd v ) memory.</p><p>• Time: Compute k f (f (g), f (g )) and w f (g, g ) ∀g ∈ G f , g ∈ nbhd(g). This requires O(|G f |nd 2 v ) flops.</p><p>With multihead self-attention (M heads), the output is:</p><formula xml:id="formula_41">f (g) + W O     V 1 . . . V M    </formula><p>where W O ∈ R dv×dv , V m = g ∈nbhd(g) w f (g, g )W V,m f (g ) for W K,m , W Q,m , W V,m ∈ R dv/M ×dv .</p><p>• Memory: Store α m f (g, g ) and W V,m f (g ) ∀g ∈ G f , g ∈ nbhd(g), m ∈ {1, . . E. Other Equivariant/Invariant building blocks G-Pooling is simply averaging over the features across the group: Inputs: {f (g)} g∈G f Output: f (g)</p><formula xml:id="formula_42">1 |G f | g∈G f f (g)</formula><p>Note that G-pooling is invariant with respect to the regular representation.</p><p>Pointwise MLPs are MLPs applied independently to each f (g) for g ∈ G f . It is easy to show that any such pointwise operations are equivariant with respect to the regular representation.</p><p>LayerNorm <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> is defined as follows:</p><p>Inputs: {g, f (g)} g∈G f where • f (g) ∈ R dv • G f defined as in Section 3.1.</p><p>Outputs: {g, β f (g)−m(g) √ v(g)+</p><p>+ γ} g∈G f where</p><p>• Division in fraction above is scalar division i.e. v(g) + ∈ R.</p><p>• m(g) = Mean c f c (g ) ∈ R.</p><p>• v(g) = Var c f c (g ) ∈ R.</p><p>• β, γ ∈ R D are learnable parameters.</p><p>BatchNorm We also describe BatchNorm <ref type="bibr" target="#b23">(Ioffe &amp; Szegedy, 2015)</ref> that is used in <ref type="bibr" target="#b13">(Finzi et al., 2020)</ref> for completeness:</p><p>Inputs: {g, f b (g)} g∈G f ,b∈B where</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Architecture of the LieTransformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) An example 2D point cloud from Dtrain. The red, blue, yellow and green labels correspond to different patterns. (b) Invariance error vs. number of samples used in the Monte Carlo approximation during lifting for a single layer LieTransformer-SE2. The model outputs the logits over the shape classes which should be invariant to SE(2) transformations of the input constellation. Plot shows median and interquartile range across 100 runs, randomizing over model seed, input constellation and transformation applied to input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(1) T (3)-equivariant model. (2) T (3)equivariant model trained with SO(3) data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 3 )</head><label>3</label><figDesc>SE(3)-equivariant (in expectation) model trained with SO(3) data augmentation. Surprisingly we see that for both LieTransformer and LieConv the T (3) model with SO(3) augmentation outperforms the SE(3) counterpart on most tasks. This is likely due to the Monte Carlo approximation of Equation 10 that only gives SE(3) equivariance in expectation -the SE(3) models were trained using | Ĥ| = 4, and the variance of this estimate is likely causing imperfect equivariance and optimisation difficulties. Due to the O(| Ĝf ||nbhd η |) memory cost for both LieTransformer and LieConv (Appendix D), we could not use a higher value of | Ĥ|. Finding more memoryefficient ways to approximate Equation 10 or avoiding the sampling approximation would likely lead to improved results. Note however that LieTransformer-SE3 and LieConv-SE3 tend to outperform the irreducible representation (irrep) based SE(3)-Transformer and TFN. This can be seen as further evidence that regular representation approaches tend to outperform irrep approaches, in line with the conclusions of<ref type="bibr" target="#b52">Weiler &amp; Cesa (2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Data efficiency of models on Hamiltonian spring dynamics. Both models are trained using 5-step roll-outs. The left and right plot shows test performance with 5-step and 100-step rollouts respectively. Plots show median MSE and interquartile range across 10 model seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Test error vs. roll-out time step. The training data size is 10,000 for both models. Plots show median MSE and interquartile range across 10 model seeds.LieConv (895K params.)LieTransformer (842K params.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>{g, f (g)} g∈G f where f (g) ∈ R dv -G f defined as in Section 3.1.• Outputs: {g,1 |nbhd(g)| g ∈nbhd(g) k L (g −1 g )f (g )} g∈G f where nbhd(g) = {g ∈ G f : ν[log(g)] &lt; r}. Let us assume that |nbhd(g)| ≈ n ∀g. k L (g −1 g ) = MLP θ (ν[log(g −1 g )]) ∈ R dout×dv .There are (at least) two ways of computing LieConv: 1. Naive and 2. PointConv Trick.1. Naive-Memory: Store k L (g −1 g ) ∈ R dout×dv ∀g ∈ G f , g ∈ nbhd(g). This requires O(|G f |nd out d v ) memory. -Time: Compute k L (g −1 g )f (g )∀g ∈ G f , g ∈ nbhd(g). This requires O(|G f |nd out d v ) flops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Details: k L (g −1 g ) = MLP θ (ν[log(g −1 g )]) = reshape(HM (g −1 g ), [d out , d v ]) where -M (g −1 g ) ∈ R d mid are the final layer activations of MLP θ . -H ∈ R doutdv×d mid is the final lienar layer of MLP θ .The trick assumes d mid d out d v , and reorders the computation as:g ∈nbhd(g) reshape(HM (g −1 g ), [d out , d v ])f (g ) = reshape(H, [d out , d v d mid ]) g ∈nbhd(g) M (g −1 g ) ⊗ f (g )where ⊗ is the Kronecker product: x⊗y = [x 1 y 1 , . . . x 1 y dy , . . . , x dx y 1 , . . . x dx y dy ] ∈ R dxdy . So M (g −1 g )⊗f (g ) ∈ R dvd mid .-Memory: Store M (g −1 g ) ∀g ∈ G f , g ∈ nbhd(g), and store H. This requires O(|Gf |nd mid + d out d v d mid ) −1 g 1 ) . . . M (g −1 g n ) requires O(d v nd mid ) flops.Then multiply by H, requiring O(d v d out d mid ) flops. This is done for each g ∈ G f , so the total number of flops is O(|G f |d v d mid (n + d out )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. , M }. This requires O(M |G f |n + M |G f |nd v /M ) = O(|G f |n(M + d v )) memory. • Time: Compute k m f (f (g), f (g )) and w m f (g, g ) ∀g ∈ G f , g ∈ nbhd(g), m ∈ {1, . . . , M }. This requires O(M |G f |nd v d v /M ) = O(|G f |nd 2 v ) flops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Such convolutions are used with pointwise non-linearities (more precisely non-linearities applied independently to the features at each spatial location/group element) to form expressive equivariant maps (examples of non-linearities are given in<ref type="bibr" target="#b52">Weiler &amp; Cesa (2019)</ref>). Exceptions to this areRomero et al. (2020)  and<ref type="bibr" target="#b14">Fuchs et al. (2020)</ref> that explore equivariant attentive convolutions, reweighing convolutional kernels with attention weights. This gives nonlinear equivariant maps with non-linear interactions across spatial locations/group elements. Instead, our work removes convolutions and investigates the use of equivariant selfattention only, inspired by works that use stand-alone selfattention on images to achieve competitive performance to convolutions<ref type="bibr" target="#b39">(Parmar et al., 2019a;</ref><ref type="bibr" target="#b9">Dosovitskiy et al., 2020)</ref>. Furthermore,Romero et al. (2020)  focus on image applications (hence scalability) and discrete groups (p4, p4m), and<ref type="bibr" target="#b14">Fuchs et al. (2020)</ref> focus on point cloud applications and the SE(3) group with irreducible representations. Instead we use regular representations, and attempt to give a general method for Lie groups acting on homogeneous spaces, with a wide range of applications from dealing with point cloud data to modelling Hamiltonian dynamics of particles. This is very much in the spirit of<ref type="bibr" target="#b13">Finzi et al. (2020)</ref>, except for self-attention instead of convolutions. In concurrent work,Romero &amp; Cordonnier (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Test accuracies on the shape counting task at convergence</figDesc><table><row><cell>Train &amp; test config</cell><cell cols="3">Fixed train &amp; test Fixed train &amp; augmented test Dtrain/Dtest Dtrain/D T 2 test Dtrain/D SE2 test</cell><cell cols="3">Augmented train &amp; test train /D T 2 D T 2 test D T 2 train /D SE2 test D SE2 train /D SE2 test</cell></row><row><cell>SetTransformer (Lee et al., 2019)</cell><cell>0.61</cell><cell>0.43</cell><cell>0.43</cell><cell>0.58</cell><cell>0.50</cell><cell>0.63</cell></row><row><cell>LieTransformer-T2 (Us)</cell><cell>0.87</cell><cell>0.87</cell><cell>0.80</cell><cell>0.87</cell><cell>0.79</cell><cell>0.84</cell></row><row><cell>LieTransformer-SE2 (Us)</cell><cell>0.85</cell><cell>0.85</cell><cell>0.85</cell><cell>0.85</cell><cell>0.85</cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>al.,   </figDesc><table><row><cell>Task</cell><cell cols="4">α ∆ HOMO LUMO</cell><cell>µ</cell><cell>Cν</cell><cell>G</cell><cell>H</cell><cell>R 2</cell><cell>U</cell><cell>U0 ZPVE</cell></row><row><cell>Units</cell><cell cols="4">bohr 3 meV meV meV</cell><cell cols="7">D cal/mol K meV meV bohr 2 meV meV meV</cell></row><row><cell>WaveScatt (Hirn et al., 2017)</cell><cell cols="2">.160 118</cell><cell>85</cell><cell cols="2">76 .340</cell><cell>.049</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>NMP (Gilmer et al., 2017)</cell><cell>.092</cell><cell>69</cell><cell>43</cell><cell cols="2">38 .030</cell><cell>.040</cell><cell>19</cell><cell cols="2">17 .180</cell><cell>20</cell><cell>20 1.50</cell></row><row><cell>SchNet (Schütt et al., 2017)</cell><cell cols="2">.235 63</cell><cell>41</cell><cell cols="2">34 .033</cell><cell cols="6">.033 14 14 .073 19 14 1.70</cell></row><row><cell>Cormorant (Anderson et al., 2019)</cell><cell>.085</cell><cell>61</cell><cell>34</cell><cell cols="2">38 .038</cell><cell>.026</cell><cell>20</cell><cell cols="2">21 .961</cell><cell>21</cell><cell>22 2.03</cell></row><row><cell>DimeNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>QM9 molecular property prediction mean absolute error. Upper section of the table are non-equivariant models designed specifically for molecular property prediction, middle section are equivariant models designed specifically for molecular property prediction, lower section are general purpose equivariant models. Bold indicates best performance in a given section, underlined indicates best overall performance.</figDesc><table><row><cell></cell><cell>125</cell><cell>60</cell><cell>36</cell><cell>32 .057</cell><cell>.046</cell><cell>35</cell><cell>37 1.54</cell><cell>36</cell><cell>35 3.62</cell></row><row><cell>LieConv-T3+SO3 Aug (Finzi et al., 2020)</cell><cell>.084</cell><cell>49</cell><cell>30</cell><cell>25 .032</cell><cell>.038</cell><cell>22</cell><cell>24 .800</cell><cell>19</cell><cell>19 2.28</cell></row><row><cell cols="3">LieConv-SE3+SO3 Aug (Finzi et al., 2020) .088 45</cell><cell>27</cell><cell>25 .038</cell><cell>.043</cell><cell>47</cell><cell>46 2.12</cell><cell>44</cell><cell>45 3.25</cell></row><row><cell>LieTransformer-T3 (Us)</cell><cell>.179</cell><cell>67</cell><cell>47</cell><cell>37 .063</cell><cell>.046</cell><cell>27</cell><cell>29 .717</cell><cell>27</cell><cell>28 2.75</cell></row><row><cell>LieTransformer-T3+SO3 Aug (Us)</cell><cell>.082</cell><cell>51</cell><cell>33</cell><cell>27 .041</cell><cell cols="5">.035 19 17 .448 16 17 2.10</cell></row><row><cell>LieTransformer-SE3+SO3 Aug (Us)</cell><cell>.112</cell><cell>54</cell><cell>35</cell><cell>30 .066</cell><cell>.046</cell><cell>28</cell><cell>28 2.87</cell><cell>30</cell><cell>30 3.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.,Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,  Yang, L., et  al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020.</figDesc><table><row><cell>Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A. Self-</cell></row><row><cell>attention Generative Adversarial Networks. In Proceed-</cell></row><row><cell>ings of the 36th International Conference on Machine</cell></row><row><cell>Learning, pp. 7354-7363, 2019.</cell></row><row><cell>Zhong, Y. D., Dey, B., and Chakraborty, A. Symplectic</cell></row><row><cell>ode-net: Learning hamiltonian dynamics with control.</cell></row><row><cell>arXiv preprint arXiv:1909.12077, 2019.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">G f A f (u −1 g, g )dg = w f (u −1 g, u −1 g )(20)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">This yields better results for the LieConv baseline compared to those reported by<ref type="bibr" target="#b13">Finzi et al. (2020)</ref>, where they use early stopping and fewer total epochs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Adam R.Kosiorek for setting up the initial codebase at the beginning of the project, and David W. Romero &amp; Jean-Baptiste Cordonnier for useful discussions. Michael is supported by the EPSRC Centre for Doctoral Training in Modern Statistics and Statistical Machine Learning (EP/S023151/1). Charline acknowledges funding from the EPSRC grant agreement no. EP/N509711/1. Sheheryar wishes to acknowledge support from Aker Scholarship. Emilien acknowledges support of his PhD funding from Google DeepMind. Yee Whye Teh's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.</p><p>We would also like to thank the Python community <ref type="bibr" target="#b47">(Van Rossum &amp; Drake Jr, 1995;</ref><ref type="bibr" target="#b37">Oliphant, 2007)</ref> for developing the tools that enabled this work, including Pytorch <ref type="bibr" target="#b41">(Paszke et al., 2017)</ref>, NumPy <ref type="bibr" target="#b36">(Oliphant, 2006;</ref><ref type="bibr" target="#b49">Walt et al., 2011;</ref><ref type="bibr" target="#b18">Harris et al., 2020)</ref>, SciPy <ref type="bibr" target="#b24">(Jones et al., 2001)</ref>, and Matplotlib <ref type="bibr" target="#b22">(Hunter, 2007)</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contributions</head><p>• Charline and Yee Whye conceived the project and Yee Whye initially came up with an equivariant form of self-attention.</p><p>• Through discussions between Michael, Charline, Hyunjik and Yee Whye, this was modified to the current LieSelfAttention layer, and Michael derived the equivariance of the LieSelfAttention layer.</p><p>• Michael, Sheheryar and Hyunjik simplified the proof of equivariance and further developed the methodology for the LieTransformer in its current state, and created links between LieTransformer and other related work.</p><p>• Michael wrote the initial framework of the LieTransformer codebase. Charline and Sheheryar wrote the code for the shape counting experiments, Michael wrote the code for the QM9 experiments, Sheheryar wrote the code for the Hamiltonian dynamics experiments, after helpful discussions with Emilien.</p><p>• Charline carried out the experiments for Table <ref type="table">1</ref>, Michael carried out most of the experiments for Table <ref type="table">2</ref> with some help from Hyunjik, Sheheryar carried out the experiments for Figure <ref type="figure">3b</ref> and all the Hamiltonian dynamics experiments.</p><p>• Hyunjik wrote all sections of the paper except the experiment sections: the shape counting section was written by Charline, the QM9 section by Michael and the Hamiltonian dynamics section was written by Emilien and Sheheryar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs</head><p>Lemma 1. The function composition</p><p>, ..., K} followed by an invariant function f , is an invariant function.</p><p>Proof. Consider group representations π 1 , . . . , π K that act on f 1 , . . . f K respectively, and representation π 0 that acts on the input space of</p><p>Lemma 2. The group equivariant convolution Ψ :</p><p>The second equality holds by invariance of the left Haar measure.</p><p>Proposition 1. The lifting layer L is equivariant with respect to the representation π. 2 sin θ</p><p>where cos θ = Tr(R)−1 2 . Note that the Taylor expansion of θ/ sin θ should be used when θ is small.</p><p>where</p><p>+ γ} g∈G f ,b∈B where</p><p>• Division in fraction above denotes pointwise division i.e. v(g)</p><p>A moving average of m(g) and v(g) are tracked during training time for use at test time. It is easy to check that both BatchNorm and LayerNorm are equivariant wrt the action of the regular representation π on f (for BatchNorm, note g ∈ nbhd(g) iff u −1 g ∈ nbhd(u −1 g)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Counting shapes in 2D point clouds</head><p>Each training / test example consists of up to two instances of each of the following shapes: triangles, squares, pentagons and the "L" shape.</p><p>We performed an architecture search on the SetTransformer first and then set the architecture of the LieTransformer such that the models have a similar number of parameters (547k for the SetTransformer and 444k for both LieTransformer-T2 and LieTransformer-SE2) and depth.</p><p>Model architecture. The architecture used for the SetTransformer <ref type="bibr" target="#b33">(Lee et al., 2019)</ref> consists of 4 layers in the encoder, 4 layers in the decoder and 4 attention heads. No inducing points were used.</p><p>The architecture used for both LieTransformer-T2 and LieTransformer-SE2 is made of 4 layers, 8 heads, feature dimension d v = 128 and kernel dim=12. One lift sample was used for each point.</p><p>Training procedure. We use Adam <ref type="bibr" target="#b26">(Kingma &amp; Ba, 2014)</ref> with parameters β 1 = 0.5 and β 2 = 0.9 and a learning rate of 1e − 4. Models are trained with mini-batches of size 32 until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. QM9</head><p>For the QM9 experiment setup we follow the approach of <ref type="bibr" target="#b0">Anderson et al. (2019)</ref> for parameterising the inputs and for the train/validation/test split. The value of the f i are learnable embeddings of each atom type. We split the available data as follows: 100k samples for training, 10% for a test set and the rest used for validation.</p><p>We performed architecture and hyperparameter optimisation on the HOM O task and then trained with the resulting hyperparameters on the other 11 tasks. The model used has 13 layers of attention blocks, using 8 heads (M ) in each layer and feature dimension d v = 848. The attention kernel uses the linear − concat − linear feature embedding, identity embedding of the Lie algebra elements, and an MLP to combine these embeddings into the final attention coefficients.</p><p>The final part of the model used had minor differences to the one in diagram 1. Instead of a global pooling layer followed by a 3 layer MLP, a single linear layer followed by global pooling was used. All models were trained using Adam, with a learning rate of 3e − 4 and a batch size of 75. 4 lifting samples were used for each input point (| Ĥ| = 4), with the radius of the neighbourhood η chosen such that the |nbhd η (g)| = 50 ∀g ∈ G and we uniformly sample 25 points from this neighbourhood.</p><p>Training these models with T (3) and SE(3) equivariance took approximately 2 and 8 days respectively on a single 1080Ti, roughly in line with training the LieConv model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Hamiltonian dynamics</head><p>Spring dynamics simulation. We exactly follow the setup described in Appendix C.4 of <ref type="bibr" target="#b13">Finzi et al. (2020)</ref> for generating the trajectories used in the train and test data.</p><p>Model architecture. We used a LieTransformer-T2 with 5 layers, 8 heads and feature dimension d v = 160. The attention kernel uses dot-product for the content component, a 3-layer MLP with hidden layer width 16 for the location component and addition to combine the content and location attention components. We use constant normalisation of the weights. We observed a significant drop in performance when, instead of constant normalisation, we used softmax normalisation (which caused small gradients at initialization leading to optimization difficulties). The architecture had 842k parameters. Note that the true Hamiltonian H(q, p) for the spring system separates as H(q, p) = K(p) + V (q) where K and V are the kinetic and potential energies of the system respectively. Following <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>, our model parameterises the potential term V .</p><p>Training details. To train the LieTransformer, we used Adam with a learning rate of 0.001 with cosine annealing and a batch size of 100. For a training dataset of size n, we trained the model for 400 3000/n epochs (although we found model training usually converged with fewer epochs). When n ≤ 100, we used the full dataset in each batch. For training the LieConv baseline, we used their default architecture and hyperparameter settings for this task, except for the number of epochs which was 400 3000/n to match the setting used for training LieTransformer. 1 Loss computation. One small difference between our setup and that of <ref type="bibr" target="#b13">Finzi et al. (2020)</ref> is in the way we compute the test loss. Since we compare models' losses not only over 5-step roll-outs but also longer 100-step roll-outs, we average the individual time step losses using a geometric mean rather than an arithmetic mean as in <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>. Since the losses for later time steps are typically orders of magnitude higher than for earlier time steps (see e.g. Figure <ref type="figure">5</ref>), a geometric mean prevents the losses for later time steps from dominating over the losses for the earlier time steps. During training, we use an arithmetic mean across time steps to compute the loss for optimization, exactly as in <ref type="bibr" target="#b13">Finzi et al. (2020)</ref>. This applies for both LieTransformer and LieConv.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14537" to="14546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">B-spline CNNs on Lie groups</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probabilistic symmetry and invariant neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06082</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Steerable</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Cnns</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Cnns</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general theory of equivariant CNNs on homogeneous spaces</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9142" to="9153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05154</idno>
		<title level="m">Theoretical aspects of group equivariant neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning SO(3) equivariant representations with spherical CNNs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10731</idno>
		<title level="m">Spin-weighted spherical CNNs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12880</idno>
		<title level="m">Generalizing convolutional neural networks for equivariance to Lie groups on arbitrary continuous data</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Transformers: 3D roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Se</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01563</idno>
		<title level="m">Hamiltonian neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Array programming with numpy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wavelet scattering regression of quantum chemical energies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Poilvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="827" to="863" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Hexaconv</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Music</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><surname>Transformer</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2d graphics environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in science &amp; engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scipy: Open source scientific tools for python</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03123</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2747" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clebsch-Gordan nets: a fully Fourier space spherical convolutional neural network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10117" to="10126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked capsule autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15512" to="15522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="http://proceediangs.mlr.press/v97/lee19d.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research. PMLR</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Relevance of rotationally equivariant convolutions for predicting molecular properties</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08461</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Invariant variation problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Noether</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Theory and Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="207" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A guide to NumPy</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trelgol Publishing USA</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Python for scientific computing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stabilizing Transformers for reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Group equivariant stand-alone self-attention for vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03830</idno>
		<title level="m">Attentive group equivariant convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hamiltonian graph networks with ode integrators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12790</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Schnet: A continuousfilter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for Transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="page" from="4344" to="4353" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Python reference manual</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Drake</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Centrum voor Wiskunde en Informatica Amsterdam</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Gar</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Nett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The numpy array: a structure for efficient numerical computation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V D</forename><surname>Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in science &amp; engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-attention with linear complexity</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">General E(2)-equivariant steerable CNNs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Cnns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="10381" to="10392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
