<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Location-Aware Deep Collaborative Filtering for Service Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
							<email>zhangyiwen@ahu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chunhui</forename><surname>Yin</surname></persName>
							<email>yinchunhui.ahu@gmail.com</email>
							<idno type="ORCID">0000-0002-6366-0941</idno>
						</author>
						<author>
							<persName><forename type="first">Qilin</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0000-0002-4559-9315</idno>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qiang</forename><surname>He</surname></persName>
							<idno type="ORCID">0000-0002-2607-4556</idno>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Haibin</forename><surname>Zhu</surname></persName>
							<email>haibinz@nipissingu.ca</email>
							<idno type="ORCID">0000-0003-1922-1631</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<postCode>230039</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Chaohu University</orgName>
								<address>
									<postCode>238000</postCode>
									<settlement>Chaohu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Swinburne University of Technology</orgName>
								<address>
									<postCode>3122</postCode>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Control and System Engineering</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science and Mathematics</orgName>
								<orgName type="institution">Nipissing University</orgName>
								<address>
									<postCode>P1B 8L7</postCode>
									<settlement>North Bay</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Location-Aware Deep Collaborative Filtering for Service Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78B0FF2AC31077BBD368653FAF9B9B33</idno>
					<idno type="DOI">10.1109/TSMC.2019.2931723</idno>
					<note type="submission">received February 15, 2019; revised May 21, 2019; accepted July 18, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative filtering (CF)</term>
					<term>deep learning</term>
					<term>service recommendation</term>
					<term>similarity adaptive corrector (AC)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the widespread application of service-oriented architecture (SOA), a flood of similarly functioning services have been deployed online. How to recommend services to users to meet their individual needs becomes the key issue in service recommendation. In recent years, methods based on collaborative filtering (CF) have been widely proposed for service recommendation. However, traditional CF typically exploits only low-dimensional and linear interactions between users and services and is challenged by the problem of data sparsity in the real world. To address these issues, inspired by deep learning, this article proposes a new deep CF model for service recommendation, named location-aware deep CF (LDCF). This model offers the following innovations: 1) the location features are mapped into high-dimensional dense embedding vectors; 2) the multilayer-perceptron (MLP) captures the highdimensional and nonlinear characteristics; and 3) the similarity adaptive corrector (AC) is first embedded in the output layer to correct the predictive quality of service. Equipped with these, LDCF can not only learn the high-dimensional and nonlinear interactions between users and services but also significantly alleviate the data sparsity problem. Through substantial experiments conducted on a real-world Web service dataset, results indicate that LDCF's recommendation performance obviously outperforms nine state-of-the-art service recommendation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the advent of the era that everything is ser- vice [e.g., cloud services, micro-services, Internet of Things (IoT) services, etc.], services deployment is proceeding at a rapid pace. How to recommend services to users that meet their individual needs has become a critical and challenging issue. Growing amounts of data support the idea that users are more inclined to choose services that satisfy their personal preferences, drawing interest from researchers examining the development of service recommendations based on contextual information of users and services.</p><p>Predicting quality of service (QoS) is the primary challenge in service recommendations. Among the existing QoS prediction methods, collaborative filtering (CF) is the most widely used <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Some researches seek to combine time, trust, location, and other contextual information to improve recommendation performance when applying CF technology to QoS prediction. However, traditional CF technologies have the following two shortcomings: 1) the similarity calculation method employed by traditional CF-based methods can only learn the low dimensional and linear characteristics from the past interactions between users and services and 2) the common data sparsity problem in the real world significantly impacts their recommendation performance.</p><p>Some efforts have been devoted to the combination of deep neural networks with CF with the aim to overcome the limitations of CF <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. He et al. <ref type="bibr" target="#b4">[5]</ref> combined matrix factorization (MF) with the multilayer-perceptron (MLP) in deep learning, and proposed the neural CF (NCF) framework to overcome the limitation of MF in low-dimensional latent spaces. The deep MF (DMF) framework proposed by Xue et al. <ref type="bibr" target="#b5">[6]</ref> can extract features directly from the userservice interaction matrix, and consider explicit rating and implicit feedbacks for making Top-K recommendations. The existing research has raised two issues: 1) only the correlation between the user and the service is studied, but the robustness of the method is ignored and 2) using only the identifier information of the user and the service does not reflect the location correlation between the user and the service.</p><p>This article proposed the location-aware deep CF (LDCF) model, which not only has strong robustness but also reflect the location correlation between the user and the service. The main contributions of this article are as follows.</p><p>1) We propose the LDCF model that innovatively integrates MLP with a similarity adaptive corrector (AC), designed to learn the high-dimensional and nonlinear interactions and the location correlation between users and services.</p><p>2168-2216 c 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>2) We first introduce the Huber loss function in this model, which has strong robustness and achieves excellent performance on all evaluation metrics. Thus, LDCF has good adaptability and extensibility in exploiting contextual information such as locations. 3) Experiments have been conducted to evaluate the performance of our approach and compare it with nine other state-of-the-art alternatives. Results indicate that our approach not only achieves better recommendation performance but also greatly alleviates problems caused by data sparsity. The remainder of this article is organized as follows. Related work is described in Section II. Section III supplies the motivation for this article. Section IV discusses the architecture of our proposed model. Section V presents the experimental results and analysis. The conclusions appear in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section reviews related works based on traditional CF and the latest deep learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Collaborative Filtering-Based Methods</head><p>The CF-based methods use historical information to recommend services for potential users. The CF-based method for service recommendation has been widely studied since the first use of CF by Shao et al. <ref type="bibr" target="#b6">[7]</ref> for predicting QoS. CF can be further characterized as memory-based or model-based.</p><p>The memory-based approach includes user-based <ref type="bibr" target="#b7">[8]</ref>, itembased <ref type="bibr" target="#b1">[2]</ref>, and a combination of the two <ref type="bibr" target="#b8">[9]</ref>. One of the main tasks of this CF technique is to predict missing QoS values for target users. The key step is to perform similarity calculations on users or items. In order to more accurately calculate the similarity of users or services, many improved memory-based works were proposed. For example, Wu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a neighborhood-based CF approach called ADF, in which the A-cosine approach, the data smoothing process, and the similarity fusion approach are adopted. Zhang et al. <ref type="bibr" target="#b10">[11]</ref> combined the covering-based clustering algorithm with MF, and proposed a covering-based via Neighborhood-aware MF (CNMF) method to fully utilize neighborhood information in service recommendations. To ensure the correct execution of the resulting composite service, Wang et al. <ref type="bibr" target="#b11">[12]</ref> proposed a solution that included a graph search-based algorithm and two novel preprocessing methods. The concept of generalized component services (GCSs) proposed by Wu et al. <ref type="bibr" target="#b12">[13]</ref> is defined in a semantic manner to expand the scope of service selection. Ding et al. <ref type="bibr" target="#b13">[14]</ref> addressed the issue of selecting and composing Web services via a genetic algorithm (GA) and offered a QoS-aware selection approach. Wu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a ratio-based approach to calculate similarity to recommend services.</p><p>To further improve the accuracy of similarity calculations in memory-based CF methods, many researchers have begun to focus on contextual information, such as reliability, time, locations, and so on. For example, Chen et al. <ref type="bibr" target="#b15">[16]</ref> considered the user's trust value and location for QoS prediction before the similarity calculation. Zheng and Lyu <ref type="bibr" target="#b16">[17]</ref> proposed two personalized reliability predictions, which use past fault data to predict Web service failure probability. Hu et al. <ref type="bibr" target="#b17">[18]</ref> used time information to improve similarity calculation for predicting QoS. Tang et al. <ref type="bibr" target="#b18">[19]</ref> improved the accuracy of QoS prediction by integrating the locations of users and services into traditional similarity calculations. Liu et al. <ref type="bibr" target="#b19">[20]</ref> proposed a location-aware CF method, which uses the locations of users and services to effectively improve recommendation performance. Tang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a network-aware method called NAMF for service recommendation by integrating MF with the network map. However, when facing a large amount of data, memory-based CF methods cannot propose recommendations in real time due to the complexity of calculations involved.</p><p>Fortunately, model-based CF methods effectively solve this problem. For instance, Zhang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a WSPred model with embedded time information for predicting QoS. Yang et al. <ref type="bibr" target="#b22">[23]</ref> introduced location information into the factorization machine (FM) for QoS prediction. Although the contextual information contributes to the similarity calculation of CF, this kind of calculation can only learn the low dimensional and linear features of users and services. When facing the real-world problem of data sparsity, feature learning is insufficient, thereby limiting recommendation performance. To address this issue, our proposed method uses MLP to capture the complex high-dimensional and nonlinear relationships between users and services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning-Based Methods</head><p>To the best of our knowledge, He et al. <ref type="bibr" target="#b4">[5]</ref> first applied deep learning techniques to the field of recommendation systems. They proposed the NCF model, which solves the problem of poor representation of MF in low dimensions. Subsequently, many methods have been proposed, such as the DMF model proposed by Xue et al. <ref type="bibr" target="#b5">[6]</ref>, which extracts features directly from the user-item matrix as neural network inputs. This takes into account explicit and implicit feedback for Top-K recommendation. He et al. <ref type="bibr" target="#b23">[24]</ref> proposed ConvNCF that used the convolutional neural network (CNN) to study the highdimensional correlation between local and global embedding dimensions in a hierarchical manner for Top-K recommendation. Kim et al. <ref type="bibr" target="#b24">[25]</ref> proposed the ConvMF model to combine CNN with probability MF (PMF) for QoS prediction. Wu et al. <ref type="bibr" target="#b25">[26]</ref> developed a novel neural architecture CNSR that jointly incorporates the social network structure and user-item interaction in a unified model for social recommendations. Xiong et al. <ref type="bibr" target="#b26">[27]</ref> proposed an online method based on personalized long and short time memory network (PLSTM), which can capture dynamic implicit feature representations of multiple users and services, and update the prediction model in time to process new data. Most of these studies rely mainly on the user identifier and the item identifier to achieve good performance in the field of film recommendation.</p><p>Recently, Xiong et al. <ref type="bibr" target="#b27">[28]</ref> proposed a deep hybrid service recommendation (DHSR) model that integrates MLP and text similarity to learn the nonlinear relationship between mashups and services. Bai et al. <ref type="bibr" target="#b28">[29]</ref> used denoising  autoencoders (SDAE) to construct a deep learning framework DLTSR to solve the long tail network problem of service recommendation. Yuan et al. <ref type="bibr" target="#b29">[30]</ref> proposed a deep learning model for healthcare service recommendation, which embeds the trust relationship and distrust relationship of the target user. It is worth noting that the above work often uses the user's or service's identifier information, but rarely considers location information that may be closely related to the QoS. In contrast to the existing research, our method addresses the problem by embedding the similarity AC of the user location and service location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION</head><p>In this section, we illustrate the motivation of our research according to Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref>. Specifically, Part A discusses why locations should be introduced and Part B analyzes the necessity of applying deep learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Why Include Locations?</head><p>Developments in cloud and edge computing have given rise to a hybrid platform based on the edge infrastructure. This has become the focus of attention for many researchers. The content distribution network (CDN) is an important part of this platform. It relies on edge servers, deployed in the local area, to enable personalized nearby user services through content distribution, load balancing, and other technologies. This is done to alleviate network congestion, along with improved unified coordination and service capabilities, to enhance the user experience.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows a location-aware service recommendation scenario. The figure includes three users: u 1 , u 2 , and u 3 , one CDN central server c 0 and three edge servers: s 1 , s 2 , and s 3 with coverage areas region 1, region 2, and region 3, respectively. Orange dotted lines represent data packet transmission paths. Black oval dotted lines represent edge server coverage area. Our goal is to examine the impact on QoS of location correlation between target users and services and then recommend suitable maps services to users.</p><p>QoS is largely dependent on bandwidth and the network distance between user and cloud server. Users can experience better QoS by calling services that are geographically close to them. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, u 1 , u 2 , and u 3 send requests to c 0 to call Google Maps service. User u 1 is within the coverage areas of both s 1 and s 2 . Since u 1 is closer to s 2 , c 0 can use the global load balancing strategy to point u 1 's access to s 2 instead of s 1 . Improving user experience can be achieved by considering regional differences between them and services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Why Use Deep Learning?</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> illustrates how the similarity calculation limits the effectiveness of CF. CF-based methods employ similarity calculation for service recommendation based on similarity measurements, such as cosine similarity, Euclidean similarity, Pearson correlation coefficient, etc. This limits the ability of CF-based methods in mining features effectively. Fig. <ref type="figure" target="#fig_1">2</ref> exemplifies this limitation with the cosine similarity.</p><p>From the above user service innovation matrix presented in Fig. <ref type="figure" target="#fig_1">2</ref>, we can obtain user u 1 and u 2 's feature vectors: </p><formula xml:id="formula_0">u 1 = [0.</formula><formula xml:id="formula_1">(u 1 , u 3 ) = 0.44 &lt; Sim (u 1 , u 2 ) = 0.66 &lt; Sim (u 2 , u 3 ) = 0.</formula><p>81. This indicates that u 3 is more similar to u 2 than u 1 . However, if a CF-based method places u 3 as the closest user to u 1 as demonstrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, u 3 will be closer to u 1 than u 2 , i.e., Sim (u 1 , u 3 ) &gt; Sim (u 2 , u 3 ). This will lead to inaccuracy and misjudgment in user similarity evaluation. A similar issue has been raised and resolved in work <ref type="bibr" target="#b4">[5]</ref>. To address this issue, in this article, we leverage the ability of deep learning to extract features effectively <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED MODEL</head><p>In this section, we first introduce the model, then describe its components. After that, we provide an explanation of the loss function and optimizer parameters applied in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Location-Aware Deep Collaborative Filtering</head><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the LDCF architecture is a multilayer feedforward neural network that includes three specific functional layers, i.e., the input layer, the middle layer, and the output layer. In its forward propagation process, the output of each layer is used as the input of the next layer. For example, we use the input layer to generate the input vectors required by the middle layer and the similarity required by the output layer. the middle layer is used for centralized training to obtain high-dimensional and nonlinear features.</p><p>The basic meanings of each component in Fig. <ref type="figure" target="#fig_3">3</ref> are as follows: the orange circle represents the computing node (or calculation unit), which includes all neurons of the deep neural network and an AC that calculates the similarity; the arrows represent data flow; the light-colored rounded oval rectangle represents the merge operation. Each functional layer will be described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input Layer</head><p>The input layer is primarily used to process the original input. For neural networks to learn additional data characteristics, we input the user identifier, user's location information, service identifier, and service's location information into the embedding layer of keras, 1 which can be regarded as a special fully connected layer without bias term. Specifically, embedding performs one-hot encoding on the input to generate a zero vector with a specified dimension and the ith position of the vector will be set to 1 <ref type="bibr" target="#b31">[32]</ref>. Similar to Word2vec, Doc2vec, and GloVe, our embedding method uses dense vectors to represent words or documents, similar to natural language processing <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b34">[35]</ref>. Through this operation, the categorical features are mapped to the high-dimensional dense embedding vectors. The mapping process is shown as</p><formula xml:id="formula_2">I k u = f 1 P T 1 i u + b 1 (1) 1 https://keras.io/ G k u = f 1 P T 1 g u + b 1 (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">I k s = f 1 Q T 1 i s + b 1 (3) G k s = f 1 Q T 1 g s + b 1 (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where i u and i s represent the user's and the service's identifier, respectively; g u and g s represent the original inputs of the user's and service's location; P 1 represents the user's embedding weight matrix; Q 1 represents the service's embedding weight matrix; b 1 represents the bias term initialized to zero; f 1 represents the activation function of this layer; and the standard identity function is selected in this article. I k u and G k u are the k-dimensional user's identifier embedding vector and location embedding vector, respectively. Similarly, U and S are the k-dimensional service's identifier embedding vector and location embedding vector, respectively.</p><p>Finally, we combine identifier feature vector with the corresponding location feature vector to obtain user feature vector and service feature vector, respectively. Then, we concatenate these two feature vectors to get the input vector required for the middle layer. The formula is expressed as follows:</p><formula xml:id="formula_6">U = I k u , G k u = I k u G k u (5) S = I k s , G k s = I k s G k s (6) x = (U, V) = U V (7)</formula><p>where represents the mergence operation, U and S the embedding vector of a user and a service, and x the input vector.</p><p>Here, we propose an AC, which performs similarity calculation between user location embedding and service location embedding. In recent years, many CF-based methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> have integrated user location similarity and service location similarity into MF to improve prediction accuracy. AC shares the similar methodology by integrating location similarity between users and services into the forward propagation process in the neural network. In this way, AC helps bridge the gap between deep learning and CF. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the operation result of the AC is directly transmitted to the output layer without the middle layer. The AC can be adaptive and can be adapted to various similarity calculations, such as cosine similarity and Euclidean similarity. The formula is expressed as either</p><formula xml:id="formula_7">o AC = cosine(G u , G s ) = G u • G s G u G s (8) o AC = euclidean(G u , G s ) = G u • G s (9)</formula><p>where o AC represents the similarity output of AC, here the cosine similarity or the Euclidean similarity is available. If no special statement is presented, we use <ref type="bibr" target="#b7">(8)</ref> to obtain the cosine similarity result of AC in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Middle Layer</head><p>The middle layer is used to process the input vector from the Input layer for capturing the nonlinear features.</p><p>In this article, a fully connected MLP structure is used to learn the high-dimensional nonlinear relationship between users and services. First of all, we should choose the activation function. Through experiments, we found that the activation function of the (ReLu) has many advantages. For example, it can accelerate the convergence of a model and solve the problem of the disappearance of the sigmoid function gradient in the saturation zone. Furthermore, Relu is one-sided compared with the anti-symmetry of tanh, thus it has more biological plausibility. Therefore, ReLu is chosen as the activation function of the middle layer. Second, in order for the neural network to learn more features, the network architecture needs to follow the typical tower structure, i.e., the more the bottom neurons, the lower the top levels <ref type="bibr" target="#b4">[5]</ref>. Finally, we use L2 regularization on weight to prevent overfitting. The forward propagation process of the input vector in the middle layer is defined as follows:</p><formula xml:id="formula_8">o mlp 2 = f 2 W T 2 x + b 2 (<label>10</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">o mlp i = f i W T i o mlp i-1 + b i , i = 3, 4, . . . , n -1 (<label>11</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">o mlp = o mlp n-1 (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where</p><formula xml:id="formula_14">o mlp i</formula><p>denotes the ith layer of the middle layer' output, W i the corresponding weight matrix, b i the bias term corresponding to the middle layer, and o mlp the output of the middle layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Output Layer</head><p>The output layer is primarily used to generate the final prediction result. LDCF models users and services in two pathways. Inspired by <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>, we directly concatenate on the outputs of these two pathways. We combine the similarity output o AC with o mlp to construct a new output vectoro. Finally, LDCF generate final predictions via a single-layer neural network. Since the output is a specific value, it can be regarded as a regression problem, and the identity function is also selected as the activation function. The parameter initialization of this layer uses Gaussian distribution, as shown</p><formula xml:id="formula_15">o = o AC , o mlp = o AC o mlp (13) Qu,s = f n W T n o + b n (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>where Qu,s represents the predictive QoS value of user u invoking service s, and f n is a standard identity function that represents the activation function of the last layer of this layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. LDCF Learning</head><p>In supervised learning, the learning of the neural network model can be considered as a process of comparing the predictive results with the real values and then continuously optimizing the target loss function to achieve a final fit. The selection of the loss function and optimizer has a nonnegligible effect on the performance of the algorithm. In this section, we mainly describe the loss functions and optimizer used in the LDCF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Loss Function Selection:</head><p>The loss functions currently applied in mainstream recommendation systems can be divided into two types: pointwise and pairwise. A pointwise loss function converts the recommendation problem into a multiclassification problem or regression problem, while a pairwise loss function converts the recommendation problem into a binary classification problem. According to applications, the loss functions of pointwise (e.g., root-mean-square loss, log loss, etc.) can be further divided into regression-based, classification-based, and ordinal regression-based, and pairwise loss functions include BPR <ref type="bibr" target="#b35">[36]</ref>, AUC, and so on. The LDCF model predicts the value of QoS and belongs to the regression problem. Thus, the binary cross information entropy <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref> is no longer suitable for our model. The commonly used loss functions of pointwise for regression are square loss, absolute loss, and so on. In statistical theory, an absolute loss function is not differential at a specific point (origin), and may lead to an unbiased estimation of the arithmetic average. The square loss function is extremely sensitive to outliers and easily leads to a median unbiased estimation. In order for the LDCF model to perform well across all evaluation metrics, we have chosen the Huber loss function <ref type="bibr" target="#b36">[37]</ref> that combines the advantages of the former two. In statistics, Huber loss is a loss function used in robust regression, and is less sensitive to outliers in data analysis than the squared error loss. <ref type="foot" target="#foot_0">2</ref> The Huber loss function is defined as follows:</p><formula xml:id="formula_17">L δ Q u,s , Qu,s = ⎧ ⎨ ⎩ 1 2 Q u,s -Qu,s 2 for Q u,s -Qu,s ≤ δ δ Q u,s -Qu,s -1 2 δ 2 otherwise (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>where Q u,s is the original QoS value of user u invoking service s, Qu,s is the predictive QoS value of user u invoking service s, and δ is a threshold for switching and δ is set to 1.0 in this article.</p><p>2) Optimizer Selection: The mini-batch adaptive moment estimation (Adam) <ref type="bibr" target="#b37">[38]</ref> optimizer has the advantages of highcomputational efficiency, smaller memory requirement, and strong interpretability, etc. Adam comprehensively considers the first moment estimation and the second moment estimation of the gradient to update the step size and effectively combines the advantages of the two optimization algorithms AdaGrad <ref type="bibr" target="#b38">[39]</ref> and RMSProp <ref type="bibr" target="#b39">[40]</ref>. Therefore, we choose the Adam optimizer.</p><p>3) Complexity Analysis: We further analyze the time complexity of the proposed approach. The pseudocode of the proposed algorithm is presented in Algorithm 1.</p><p>In the LDCF algorithm, the time complexity of line 10 is O(k), where k represents the dimension of the embedding vectors. The time complexity of lines 11 is O(1) because the concatenation is conducted. The time complexity of line 12 is O(l u × l s × k), where l u and l s represent the length of user location features and service locations feature, respectively, k represents dimension of embedding vectors. Among these parameters, l u and l s are constants. The time complexity of line 13 is O <ref type="bibr" target="#b0">(1)</ref>. Then, the algorithm repeats lines 10-13 until for user and service in R train do 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 LDCF Algorithm</head><p>generate embedding vectors through Eq. ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_4">4</ref>); 11.</p><p>generate input vector through Eq. ( <ref type="formula">5</ref>)-( <ref type="formula">7</ref>); 12.</p><p>generate AC output through Eq. ( <ref type="formula">8</ref>) or (9); 13.</p><p>generate prediction Qu,s through Eq. ( <ref type="formula">13</ref>)-( <ref type="formula" target="#formula_15">14</ref>); 14.</p><p>end for <ref type="bibr">15.</ref> pass l and r to Adam; 16.</p><p>update model parameters by Adam minimizing Eq. ( <ref type="formula" target="#formula_17">15</ref>); 17.</p><p>for user and service in R test do 18.</p><p>evaluate model performance through Eq. ( <ref type="formula">18</ref>)-( <ref type="formula" target="#formula_24">19</ref>); 19.</p><p>end for 20. end for R train is traversed. Therefore, the time complexity of the forward propagation process (lines 9-14</p><formula xml:id="formula_19">) is O(n)×(O(k)+O(1)+ O(l u × l s × k) + O(1)) = O(n × k),</formula><p>where n is the number of entries in R train .</p><p>The time complexity of line 15 is O(1). The time complexity of line 16 is O(n × k) because the time complexity of backpropagation is the same as the forward propagation. The time complexity of line 18 is O(1). Line 18 needs to be repeated until R test is traversed. Thus, the time complexity of lines 17-19 is O(m), where m is the number of entries in R test . The time complexity of lines 15-19 is</p><formula xml:id="formula_20">O(1) + O(n × k) + O(m) = O(n × k).</formula><p>Then, lines 9-19 need to be repeated until all iterations are completed.</p><p>Overall, the time complexity of LDCF is</p><formula xml:id="formula_21">O(i) × (O(n × k) + O(n × k)) = O(i × n × k)</formula><p>, where i is the total number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we conduct extensive experiments aimed at answering the following research questions.</p><p>1) RQ1: Can the LDCF model alleviate the data sparsity problem compared to existing classic recommendation algorithms? Is there a significant improvement in recommendation performance? 2) RQ2: Can the introduction of locations be helpful for the learning of the LDCF model? 3) RQ3: Can deep-learning acquire high-dimensional and nonlinear characteristics of users and services? Can the depth help recommend performance? 4) RQ4: Can the similarity AC help improve performance?</p><p>Can it be adaptable and extensible? 5) RQ5: Can the Huber loss function achieve excellent performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We conducted experiments on the WS-Dream dataset, a large-scale real-world Web services dataset collected and maintained by Zheng et al. <ref type="bibr" target="#b3">[4]</ref>, which contains 1 974 675 QoS values of Web services collected from 339 users on 5825 services. The dataset provides location information about users and services (such as countries, etc.). In this article, the QoS dataset is represented in the form of a user-service matrix, where the row index represents the user identifier, the column index represents the service identifier, and each value in the matrix is described by the response time (RT) and throughput (TP). In the experiment, we used RT and TP as the input to LDCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>The network unit with the same autonomous system number (ASN) commonly has similar network environments <ref type="bibr" target="#b19">[20]</ref>. This article uses two geographically related attributes provided by the datasets: Country Name (CN) and ASN. Through dataset statistics, users are distributed among 30 countries and 136 autonomous systems, while services are distributed among 990 autonomous systems in 73 countries. For CN, we use the categorical encoding of the Sklearn 3 to transform the classified features into integer encodings, so that each classification feature is represented as the national code. For ASN, we use its numeric coding directly. After preprocessing, the data can be represented as</p><formula xml:id="formula_22">U = (UID, UASN, UCN) (16) S = (SID, SASN, SCN) (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>where U and S indicate the input embedding vector of a user and a service, respectively. The UID indicates the identifier of the user, the UASN indicates the autonomous system numerical code of the user, and the UCN represents the national code of the user. SID, SASN, and SCN are similar to the above. In the real world, the user-service matrix is usually very sparse, and users only invoke a very small number of services. In order to make the experiment more realistic, we randomly delete entries from the user-service matrix to make the matrix sparse at six different densities. For example, a matrix density (i.e., the ratio of nonzero entries) of 0.30 means that we randomly select 30% of the QoS entries as the training set for the model, and the remaining 70% are used as the test set to evaluate the accuracy of the model predictions. The matrix density is in steps of 0.05 and ranges from 0.05 to 0.30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Setting</head><p>For methods of CF [e.g., user-based CF method using PCC (UPCC), item-based CF method using (IPCC), location-aware CF (LACF), RegionKNN, etc.], Top-K is set to 10, the learning rate is initialized to 0.001, the number of implicit feature factors (dimensions) is set to 10, the maximum number of iterations is set to 300, the regularization parameters are set to 0.1 and the random factor for rarefy matrix is set to 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I EXPERIMENTAL RESULTS OF RT</head><p>For methods based on deep learning (e.g., NCF, LDCF <ref type="foot" target="#foot_1">4</ref> ), we implement them on Keras (TensorFlow as the backend), where we use Gaussian distribution (avg = 0, stdev = 0.01) to initialize model parameters, use <ref type="bibr" target="#b14">(15)</ref> to update parameter of the model. And we set the batchsize to 256, the learning rate to 0.0001, the number of MLP to 4, and use Adam for optimizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>Two basic statistical accuracy metrics: 1) mean absolute error (MAE) and 2) root mean squared error (RMSE) metrics are used to measure the recommendation performance of the selected methods. MAE and RMSE can be defined as</p><formula xml:id="formula_24">MAE = u,s Q u,s -Qu,s N (18) RMSE = 1 N u,s Q u,s -Qu,s 2<label>(19)</label></formula><p>where Q u,s is the QoS original value of the user u invoking the service s, Qu,s the predictive QoS value of the user u invoking the service s, and N the total number of QoS.</p><p>The MAE calculates the average difference based on the number of values that need to be predicted, and measures all absolute differences between the labels and predicted values. RMSE gives a relatively high weight to outliers, and is sensitive to larger or smaller values within a set of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison Methods</head><p>To verify the performance of the LDCF model in this article, we compared nine of the most typical methods, including the traditional CF methods and the latest deep learning methods.</p><p>1) User Mean (UMEAN): This method employs a user's average QoS value on the used Web services for Web service recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Item Mean (IMEAN):</head><p>This method employs the average QoS value of the Web service observed by other service users for service recommendation. 3) UPCC <ref type="bibr" target="#b0">[1]</ref>: This method employs similar user behavior information for service recommendation. 4) IPCC <ref type="bibr" target="#b1">[2]</ref>: This method employs similar item attribute information for service recommendation. 5) User-Based and Item-Based CF (UIPCC) <ref type="bibr" target="#b3">[4]</ref>: This method combines the similar users and similar Web services adopted in UPCC and IPCC for service recommendation. 6) RegionKNN <ref type="bibr" target="#b39">[40]</ref>: This method incorporates region factors into CF method for service recommendation, which is benchmark location-aware method. 7) LACF <ref type="bibr" target="#b18">[19]</ref>: This method uses both locations of users and services for service recommendation, which is stateof-art location-aware method. 8) PMF <ref type="bibr" target="#b41">[42]</ref>: This method incorporates probabilistic factors into MF for service recommendation. 9) NCF <ref type="bibr" target="#b4">[5]</ref>: This is an advanced neural network method that combines MLP and MF for recommendation. Among the above, 1)-5) are traditional benchmark methods, 6) and 7) are geolocation-based methods, and 8) and 9) are model-based and deep learning-based methods, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental Results and Analysis</head><p>We measured two evaluation metrics: RT and TP at six different matrix densities. Each experiment was run 20 times and the average was taken as the final result. The experimental results are shown in Tables I and II. As can be seen from Tables I and II, our LDCF model has the smallest MAE and RMSE at any density, i.e., our LDCF is significantly better than the other existing methods. Especially when the sparsity is very large, our LDCF model has obvious advantages in recommending services.  Furthermore, the performance of NCF and LDCF based on deep learning is significantly better than that of neighbor-based CF methods when the sparsity is large, indicating that deep learning is effective for high-dimensional and nonlinear feature learning of relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Performance Comparison (RQ1):</head><p>As shown in Fig. <ref type="figure" target="#fig_4">4(a)-(d)</ref>, in addition to UMEAN, IMEAN, the performance of most of the chosen methods increases with the increase of density. Such a phenomenon is consistent with our intuitive experience that "the more data provided, the more accurate the CF similarity calculation, the more features the neural network can learn, and the higher the recommendation performance." Although the performance of the methods varies from different densities, the trends of these methods are similar, and the relationships under the same conditions are similar. To simplify the statement, we compare them at the same matrix density of 0.05. By observation, we can get the following relationships from the MAE performance comparison of RT and TP: LDCF &lt; NCF &lt; PMF &lt; RegionKNN &lt; UIPCC &lt; IPCC &lt; UIPCC &lt; LACF &lt; IMEAN &lt; UMEAN. This indicates that the traditional CF method has poor accuracy of recommendation when the sparsity is large, but the deep learning-based method still performs well. Through analysis, we believe that traditional CF methods can only learn low dimensional and linear features, hence their feature learning ability is greatly restricted, while deep learning can capture high-dimensional nonlinear features, effectively compensate for data sparsity, and effectively make up for the limitation of CF feature learning by data sparsity.</p><p>In terms of RMSE performance, our LDCF model achieves the smallest RMSE at various densities, indicating that the LDCF model is very stable. Similarly, we can get the RMSE performance relationships of all the methods from Fig. <ref type="figure" target="#fig_4">4</ref>: LDCF &lt; NCF &lt; PMF &lt; UIPCC &lt; UPCC &lt; IPCC &lt; LACF &lt; IMEAN &lt; RegionKNN &lt; UMEAN. UPCC, IPCC, and UIPCC performed similarly. The performance of NCF was similar to PMF after the density became 2.5%. At a density of 0.2 and 0.25, NCF's RT was not as good as PMF's. Traditional CF methods can achieve good performance when the data is not sparse. NCF performs well when the data is sparse, but it only uses the identifier information. Fortunately, our method LDCF can not only learn the additional nonlinear characteristics of users and services but also incorporate location information to make up for the defects of CF method and NCF, so that at any density, the RMSE of our method LDCF is superior to that of other methods. Therefore, compared with the existing recommendation methods including  traditional CF methods and the state-of-the-art deep learning methods, LDCF can effectively deal with data sparsity and achieve better recommendation performance.</p><p>2) Impact of Location Information (RQ2): To examine the impact of location information, we compare the LDCF model with the NCF model, because LDCF incorporates locations, but NCF does not. In the experiment, both NCF and LDCF are trained under the same loss function and the same network parameters. To the best of our knowledge, the performance of neural network methods largely depends on the initialization of the machine, and the model gradually converges to the optimal situation as the number of iterations increases. To verify the impact of location information on deep CF recommendations, we iterate 50 times with matrix density of 0.05, predicting RT and TP, respectively.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a) and (b), in the RT experiments, the two methods begin to converge after about 15 iterations and tend to be stable. LDCF is almost the same as NCF before convergence. As the number of iterations increases, the performance of the two methods is gradually improved. However, the performance of the LDCF model considering location information is better than the NCF model considering only the identifier information. Similarly, as shown in Fig. <ref type="figure" target="#fig_5">5(c</ref>) and (d), in the TP experiments, whether it is on MAE or RMSE, the performance of the LDCF model considering the location information is significantly better than the NCF model. Therefore, the introduction of location information of users and services can greatly contribute to the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Impact of Depths (RQ3):</head><p>It is well known that neurons are the smallest unit of arithmetic that constitutes a neural network framework, and their connection has an indispensable effect on neural networks. In this article, in order to allow the neural network to learn more nonlinear features, we use the tower structure. Under the framework of the tower structure, we let the neural network with only 1 MLP have 8 neurons, and the network topology map formed by it can be expressed as &lt;8&gt;. As depth is equal to 2, the network topology map can be expressed as &lt;16, 8&gt;, which means that the first layer of MLP has 8 neurons and the second layer has 16 neurons. Similarly, we can state that, if the depth is i, the neural network topology map can be expressed as [2 3 * 2 i-1 , 2 3 * 2 i-2 , . . . , 2 3 ].</p><p>The experimental results are shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a)-(d). It can be seen from the trend line that as the number of MLP changes from 1 to 8, the performance improves significantly at first, and then is gradually reduced, indicating that deep neural network can greatly improve the performance. However, as the nonlinear features are limited, the performance improvement of more than 6 MLPs is not significant. We believe that the MLP can learn the high-dimensional and nonlinear complex feature, thus making up for limitations of CF for feature learning. Although the addition of more than six layers of MLP can only slightly improve the performance of the model, the performance of deep neural networks should not be underestimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Impact of AC (RQ4):</head><p>In Section III, we propose a similarity AC at the Input Layer. Its main function is to calculate the similarity between the user's location and the location of  the service, and then we transmit it to the output layer to correct the prediction results of the model.</p><p>Before verifying the effect of the AC on the prediction results under the location-aware CF framework, we define the following three submethods.</p><p>a) AC-MLP: This method uses only an MLP and does not transmit the similarity between the user's location and the location of the service to the output layer for correction.</p><p>b) AC-EUC: This method transmits the Euclidean similarity between the user's location and the location of the service to the output layer based on the AC-MLP. c) AC-COS: This method transmits the cosine similarity between the user's location and the location of the service to the output layer based on the AC-MLP.</p><p>In order to study the influence of AC in depth, we conducted experiments on RT and TP by setting the matrix density = 0.05. As shown in Table <ref type="table">III</ref>, AC-MLP performance without the similarity is poor regardless of RT or TP. After we added the similarity of the AC, performance improved significantly. For example, in terms of TP, the MAE of AC-EUC is 17.539 and its RMSE is 57.721. As a comparison, the MAE of AC-MLP is 18.353 and its RMSE is 59.735. This phenomenon shows that AC is very helpful for improving the performance of the model.</p><p>Furthermore, in the comparison between AC-COS and AC-EUC, we can find that the performance of AC-COS is much better than that of AC-EUC. That is, the cosine similarity is more suitable for our model than the Euclidean one. Through the comparison analysis, we believe that Euclidean similarity can reflect the absolute difference of individual numerical features by measuring the linear distance between two spatial points, while cosine similarity can reflect the difference of angles between two vectors by measuring the directional distance between two spatial points. In our LDCF model, location information is represented as a vector, hence the cosine similarity is more suitable. Moreover, our proposed similarity AC can incorporate various CF similarity calculation methods into the deep learning framework to improve the performance of the model, as long as the similarity calculation has a tensor form of expression. It is enough to prove that our proposed similarity AC has very strong adaptability and extensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Impact of Huber Loss (RQ5):</head><p>In this article, the Huber loss function defined as ( <ref type="formula" target="#formula_17">15</ref>) is first introduced. We have conducted experiments to compare the performance of the Huber loss function against the L1 loss (i.e., absolute error loss) and L2 loss (i.e., squared error loss) functions with 0.05 matrix density. The experimental results are summarized in Table <ref type="table">IV</ref>.</p><p>In the comparison between the L1 loss and L2 loss functions, we can easily see that the L1 loss function wins the match on MAE but lost that on RMSE. The results show that the Huber loss function well balances between MAE and RMSE for the RT, and outperforms the L1 loss and the L2 loss function significantly in terms of both MAE and RMSE for the TP.</p><p>6) Threats to Validity: In this section, we discuss the related threats to the validity of our evaluation of LDCF, including the construct validity, external validity, internal validity, and conclusion validity.</p><p>Threats to Construct Validity: of main threats construct validity of our in the comparison of recommendation accuracy with recommendation methods. The recommendation methods are based and deep learning techniques, which are currently the most popular and widely used. Although other methods, such as the trust-based recommendation method <ref type="bibr" target="#b15">[16]</ref>, are not included in the evaluation, this threat is not particularly significant because our LDCF can be indirectly compared with those methods through inspecting the evaluation presented in the related literature using the methods included in the evaluation as reference methods. Another major threat to the construct validity of our experiments is the lack of consideration of time <ref type="bibr" target="#b17">[18]</ref> and trust <ref type="bibr" target="#b15">[16]</ref> during the recommendation. This threat is also not important, because although these aspects enhances the recommendation performance, they do not change the basic mechanism that improves the accuracy by adding contextual information. Although time can be included in LDCF in a manner similar to <ref type="bibr" target="#b42">[43]</ref>, a direct comparison between LDCF and other methods in the locationaware recommendation methods is more straightforward and representative.</p><p>Threats to External Validity: The main threat to the external validity of our evaluation is the nature of the dataset being used. It may not be able to exactly represent all the real-world applications. To minimize this threat, we used the dataset that has been widely employed in experiments on recommendation methods for Web services, i.e., the WS-Dream dataset. In the experiments, we generated matrices with different densities by randomly removing entries to simulate various recommendation scenarios in real word. In this way, we can compare LDCF with existing methods comprehensively and draw a conclusion that LDCF can alleviate the data sparsity problem properly. This also greatly reduces the threat to the external validity of our evaluation.</p><p>Threats to Internal Validity: The main threat to the internal validity of our evaluation is its representativeness. Although the experiments under other parameter settings (e.g., more selections of loss functions and optimizers) can be conducted, we believe that this threat is not significant because we have examined the effects of three most popular loss functions (i.e., L1 loss, L2 loss, and Huber loss) and the effects of widely used optimizer Adam. Thus, our evaluation is representative and the threat to the internal validity is not significant.</p><p>Threats to Conclusion Validity: The main threat to the conclusion validity of our evaluation is the lack of statistical tests, e.g., chi-square tests. In fact, we could have conducted chisquare tests to draw conclusions while evaluating LDCF. Since we have performed 20 experiments in each run and averaged the optimal run results after the model converges, this will lead to a large number of test cases, which results in a small p-value in the chi-square tests and lowers the practical significance of the test results <ref type="bibr" target="#b43">[44]</ref>. However, this number of runs is not even close to the number of observation samples concerned by Lin et al. <ref type="bibr" target="#b43">[44]</ref>. Thus, the threat to the conclusion validity due to the lack of statistical tests may be possible but not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This article proposes a new deep learning-based model, i.e., the LDCF model, which aims to solve the key problem of service recommendation-predicting the QoS. The proposed model can learn the high-dimensional and nonlinear relationships between users and services through MLP, and incorporate the similarity AC to correct the predictive values. Substantial experiments have been done on realworld Web service datasets to evaluate the performance of LDCF. Compared with traditional CF methods and the latest deep learning methods, our approach can greatly improve the performance of Web service recommendation. In the future, we will further consider the role and impact of other contextual information (e.g., time and trust) on service recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Location-aware service recommendation scenario.</figDesc><graphic coords="3,117.23,122.57,94.94,57.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of CF similarity calculation. (a) User-service interaction matrix. (b) User 2-D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2 illustrates how the similarity calculation limits the effectiveness of CF. CF-based methods employ similarity calculation for service recommendation based on similarity measurements, such as cosine similarity, Euclidean similarity, Pearson correlation coefficient, etc. This limits the ability of CF-based methods in mining features effectively. Fig. 2 exemplifies this limitation with the cosine similarity.From the above user service innovation matrix presented in Fig.2, we can obtain user u 1 and u 2 's feature vectors:u 1 = [0.70, 1.63, 0.33] and u 2 = [0.54, 0.31, 0.71]. The cosine similarity between u 1 and u 2 is: Sim (u 1 , u 2 ) = 0.66. Fig. 2(b) demonstrates their geometric relationship in a two-dimensional (2-D) space. Let us assume a new user u 3 = [2.47, 0.04, 0.93].There is Sim (u 1 , u 3 ) = 0.44 &lt; Sim (u 1 , u 2 ) = 0.66 &lt; Sim (u 2 , u 3 ) = 0.81. This indicates that u 3 is more similar to u 2 than u 1 . However, if a CF-based method places u 3 as the closest user to u 1 as demonstrated in Fig.1(b), u 3 will be closer to u 1 than u 2 , i.e., Sim (u 1 , u 3 ) &gt; Sim (u 2 , u 3 ). This will lead to inaccuracy and misjudgment in user similarity evaluation. A similar issue has been raised and resolved in work<ref type="bibr" target="#b4">[5]</ref>. To address this issue, in this article, we leverage the ability of deep learning to extract features effectively<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. LDCF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance comparison (RQ1). (a) RT@MAE. (b) RT@RMSE. (c) TP@MAE. (d) TP@RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance with respect to the number of iterations (RQ2). (a) RT@MAE. (b) RT@RMSE. (c) TP@MAE. (d) TP@RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance with respect to the number of MLP (RQ3). (a) RT@MAE. (b) RT@RMSE. (c) TP@MAE. (d) TP@RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Weight matrices and bias termsP 1 , Q 1 , W 2 , W 3 , . . . , W n , b 1 , b 2 , . . . ,b n . 1. sparse R according to d; 2. generate training entries R train and test entries R test ; 3. generate input features i u , g u , i s , g s ; 4. build neural networks according to t and Eq. (10)-(12); 5. initialize P 1 , Q 1 , W 2 , . . . , W n according to Gaussian distribution; 6. initialize b 1 , b 2 , . . . , b n to 0; 8. for epoch = 1, 2, . . . , i do 9.</figDesc><table><row><cell>Input: user-service invocation matrix R,</cell></row><row><cell>matrix density d,</cell></row><row><cell>neural network topology structure t,</cell></row><row><cell>learning rate l, decay ratio r,</cell></row><row><cell>number of iteration i.</cell></row><row><cell>Output:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/huber_loss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://github.com/ChunhuiYin/Location-aware_Deep_Collaborative_ Filtering</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61872002, in part by the Australian Research Council Discovery Project under Grant DP180100212, in part by the Anhui Key Research and Development Plan under Grant 201904a05020091, and in part by the Natural Science Foundation of Anhui Province of China under Grant 1808085MF197. This article was recommended by Associate Editor Z. Yu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Conf. Uncertainty Artif. Intell</title>
		<meeting>14th Conf. Uncertainty Artif. Intell<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. World Wide Web Conf., Hong Kong</title>
		<meeting>Int. World Wide Web Conf., Hong Kong</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WSRec: A collaborative filtering based Web service recommender system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Services</title>
		<meeting>Int. Conf. Web Services<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">QoS-aware Web service recby collaborative filtering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="152" />
			<date type="published" when="2011-06">Apr./Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. World Wide Web Conf</title>
		<meeting>Int. World Wide Web Conf<address><addrLine>Perth, WA, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Joint Conf</title>
		<meeting>6th Int. Joint Conf<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personalized QoS prediction for Web services via collaborative filtering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Service</title>
		<meeting>Int. Conf. Web Service<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient similarity measure for user-based collaborative filtering recommender systems inspired by the physical resonance principle</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27211" to="27228" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective missing data prediction for collaborative filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR Conf</title>
		<meeting>ACM SIGIR Conf<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting quality of service for selection by neighborhood-based collaborative filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="428" to="439" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Covering-based Web service quality prediction via neighborhood-aware matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constraint-aware approach to Web service composition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="770" to="784" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">QoS-aware multigranularity service composition: Modeling and optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1565" to="1577" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A transaction and QoS-aware service selection approach based on genetic algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collaborative filtering service recommendation based on a novel similarity computation method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="352" to="365" />
			<date type="published" when="2017-06">May/Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Trust-aware and location-based collaborative filtering for Web service QoS prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 41st Annu</title>
		<meeting>IEEE 41st Annu<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="143" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Personalized reliability prediction of Web services</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Time aware and data sparsity tolerant Web service recommendation based on improved collaborative filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="782" to="794" />
			<date type="published" when="2015-10">Sep./Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Location-aware collaborative filtering for QoS-based service recommendation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Services</title>
		<meeting>Int. Conf. Web Services<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Location-aware and personalized collaborative filtering for Web service recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="686" to="699" />
			<date type="published" when="2016-10">Sep./Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative Web service quality prediction via exploiting matrix factorization and network map</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Netw. Service Manag</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="126" to="137" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WSPred: A time-aware personalized QoS prediction framework for Web services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Softw. Rel. Eng</title>
		<meeting>IEEE Int. Symp. Softw. Rel. Eng<address><addrLine>Hiroshima, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A locationbased factorization machine model for Web service QoS prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Outer productbased neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Joint Conf</title>
		<meeting>6th Int. Joint Conf<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="2227" to="2233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional matrix factorization for document context-aware recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM Conf. Recommender Syst</title>
		<meeting>10th ACM Conf. Recommender Syst<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative neural social recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized LSTM based matrix factorization for online QoS prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C K</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Services</title>
		<meeting>Int. Conf. Web Services<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hybrid collaborative filtering for Web service recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DLTSR: A deep learning framework for recommendation of long-tail Web services</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Services Comput</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Socialized healthcare service recommendation using deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Khattak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2071" to="2082" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019-02">Feb. 2019</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A recommendation model based on deep neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9454" to="9463" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A probabilistic model for joint learning of word embeddings from texts and images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ailem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empir</title>
		<meeting>Conf. Empir<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">Oct./Nov. 2018</date>
			<biblScope unit="page" from="1478" to="1487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic dependency parsing via book embedding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th Annu. Meeting Assoc</title>
		<meeting>55th Annu. Meeting Assoc</meeting>
		<imprint>
			<date type="published" when="2017-08">Jul./Aug. 2017</date>
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentence embedding for neural machine translation domain adaptation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
		<meeting>Assoc<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Jul./Aug. 2017</date>
			<biblScope unit="page" from="560" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Conf. Uncertainty Artif. Intell</title>
		<meeting>25th Conf. Uncertainty Artif. Intell<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Represent</title>
		<meeting>3rd Int. Conf. Learn. Represent<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive sub-gradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RegionKNN: A Scalable Hybrid collaborative filtering algorithm for personalized Web service recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Services</title>
		<meeting>Int. Conf. Web Services<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent collaborative filtering for unifying general and sequential recommender</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="3350" to="3356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Yiwen Zhang received the Ph.D. degree in management science and engineering from the Hefei University of Technology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shmueli</surname></persName>
		</author>
		<ptr target="https://bigdata.ahu.edu.cn" />
	</analytic>
	<monogr>
		<title level="m">His current research interests include service computing, cloud computing, and big data. More details about his research can be</title>
		<meeting><address><addrLine>Hefei, China; Hefei; Hefei, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2017</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="906" to="917" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Anhui University</orgName>
		</respStmt>
	</monogr>
	<note>Chunhui Yin received the bachelor&apos;s degree in computer science and technology from Anhui University. where he is currently pursuing the master&apos;s degree with the School of Computer Science and Technology. His current research interests include deep learning, recommenders, and service computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
