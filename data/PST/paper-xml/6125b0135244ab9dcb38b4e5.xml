<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Graph Co-Training for Session-based Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-24">24 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>x.xia@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
							<email>shaoyx@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Graph Co-Training for Session-based Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-24">24 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482388</idno>
					<idno type="arXiv">arXiv:2108.10560v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Recommender systems;</term>
					<term>Theory of computation â†’ Semi-supervised learning Self-Supervised Learning, Contrastive Learning, Session-based Recommendation, Co-Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session-based recommendation targets next-item prediction by exploiting user behaviors within a short time period. Compared with other recommendation paradigms, session-based recommendation suffers more from the problem of data sparsity due to the very limited short-term interactions. Self-supervised learning, which can discover ground-truth samples from the raw data, holds vast potentials to tackle this problem. However, existing self-supervised recommendation models mainly rely on item/segment dropout to augment data, which are not fit for session-based recommendation because the dropout leads to sparser data, creating unserviceable self-supervision signals. In this paper, for informative sessionbased data augmentation, we combine self-supervised learning with co-training, and then develop a framework to enhance sessionbased recommendation. Technically, we first exploit the sessionbased graph to augment two views that exhibit the internal and external connectivities of sessions, and then we build two distinct graph encoders over the two views, which recursively leverage the different connectivity information to generate ground-truth samples to supervise each other by contrastive learning. In contrast to the dropout strategy, the proposed self-supervised graph co-training preserves the complete session information and fulfills genuine data augmentation. Extensive experiments on multiple benchmark datasets show that, session-based recommendation can be remarkably enhanced under the regime of self-supervised graph co-training, achieving the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems (RS) now have been pervasive and become an indispensable tool to facilitate online shopping and information delivery. Most traditional recommendation approaches share a common assumption that user behaviors are constantly recorded and available for access <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>. However, in some situations, recording long-term user profiles may be infeasible. For example, guests who do not log in, or users who keep personal information private do not have an accessible user profile. Session-based recommendation emerges to tackle this challenge <ref type="bibr" target="#b35">[36]</ref>, aiming at predicting the next item only with short-term user interaction data generated in a session. Owing to its promising prospect, in the past few years, session-based recommendation has received considerable attention, and a number of models have been successively developed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Early effort in this field brought Markov Chain into sessionbased scenarios to capture the temporal information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Afterwards, deep learning exhibited overwhelming advantage of modeling sequential data <ref type="bibr" target="#b55">[56]</ref>, and recurrent neural networks (RNNs) became the dominant paradigm in this line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Recently, graph neural networks (GNNs) <ref type="bibr" target="#b41">[42]</ref> have sparked heated discussions across multiple fields for its unprecedented effectiveness in solving graph-based tasks. As session-based data can also be modeled as sequence-like graphs, there also have been a proliferation of GNNs-based session-based recommendation models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>, which outperform RNNs-based models and show decent improvements. Despite the achievements, however, these approaches are still compromised by the same issue -data sparsity. Due to the inaccessibility of the long-term user behavior data, session-based recommenders can only leverage very limited useritem interactions generated within a short session to refine the corresponding user/session representations. In most cases, these data is too few to induce an accurate user preference, leading to sub-optimal recommendation performance.</p><p>Self-supervised learning (SSL) <ref type="bibr" target="#b20">[21]</ref>, as an emerging learning paradigm which can discover ground-truth samples from the raw data, is considered to be an antidote to the data sparsity issue. Inspired by its great success in the areas of graph and visual representation learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, recent advances seek to harness SSL for improving recommendation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref>. The typical idea of applying SSL to recommendation is conducting stochastic data augmentations by randomly dropping some items/segments from the raw user-item interaction graph/sequence to create supervisory signals, which is analogous to the strategy used in masked language models like BERT <ref type="bibr" target="#b6">[7]</ref>. Following this line of thought, Bert4Rec <ref type="bibr" target="#b29">[30]</ref> drives a cloze objective for sequential recommendation by predicting the random masked items in the sequence with their left and right contexts. ğ‘† 3 -Rec <ref type="bibr" target="#b56">[57]</ref> designs four types of pretexts to derive supervision signals from the segments, items and attributes of sequential data and then utilizes mutual information maximization to refine item representations. Similarly, CL4SRec <ref type="bibr" target="#b43">[44]</ref> adopts item cropping, masking and reordering to construct different data augmentations based on sequences for contrastive learning. With such random dropout strategies, SSL is compatible with sequential recommendation. However, when it comes to session-based recommendation, the same idea may not be practicable. It should be noted that, the user interaction data generated in a session is much less than a long-term user profile in sequential recommenders. Accordingly, conducting dropout on session-based data would create sparser sequences, which could be unserviceable for improving recommendation performance. To address this problem, in this paper, we propose a novel framework which combines SSL with semisupervised learning to create more informative self-supervision signals to enhance session-based recommendation.</p><p>Co-training <ref type="bibr" target="#b2">[3]</ref>, as a classical semi-supervised learning paradigm, exploits unlabeled data to improve classifiers. The basic idea of co-training is to train two classifiers over two different data views, and then predict pseudo-labels of unlabeled instances to supervise each other in an iterative way. In our framework, we first exploit the session-item graph to construct two views (item view and session view) that exhibit the internal and external connections of sessions. Then two asymmetric graph encoders (i.e. graph convolutional networks) are built over these two views and trained under the scheme of co-training. One of them (main encoder) is for recommendation and the other acts as the auxiliary encoder to boost the former. Specifically, given a session, we regard the items as unlabeled data. In each time, one encoder predicts its possible next items and delivers them to the other encoder, respectively. By doing so, both encoders can acquire complementary information from each other. And then a contrastive objective is optimized towards refining the encoders and item representations. Meanwhile, to prevent the mode collapse (i.e. two encoders become very similar and suggest the same item), we exploit adversarial examples to encourage divergence between the two views. As this co-training regime is built upon the graph views derived from the same data source for data augmentation, and is with a contrastive objective, we name it self-supervised graph co-training. By iterating this process, the benefits can be two-fold: <ref type="bibr" target="#b0">(1)</ref>. with the co-training proceeding, the generated item samples become more informative (a.k.a. hard examples), which can bring more useful information to each encoder compared with the dropout strategy that is only for self-discrimination; <ref type="bibr" target="#b1">(2)</ref>. the complete data of a session is preserved and two different aspects of connectivity information are exploited, generating more practicable self-supervision signals. Finally, the main encoder is significantly improved for recommendation.</p><p>Overall, the contributions of this paper are summarized as follows:</p><p>â€¢ We propose a novel self-supervised framework for session-based recommendation which can generate more informative and practicable self-supervision signals.</p><p>â€¢ The proposed framework is model-agnostic. Ideally, the architectures of the two used encoders can be diverse, which generalizes the framework to adapt to more scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Session-based Recommendation</head><p>Early studies on session-based recommendation focused on exploiting temporal information from session data with Markov chain <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">58]</ref>. Zimdars et al. <ref type="bibr" target="#b57">[58]</ref> investigated the order of temporal data based on Markov chain and used a probability decision-tree to model sequential patterns between items. Shan et al. <ref type="bibr" target="#b28">[29]</ref> developed a novel recommender system based on an Markov Decision Process model with appropriate initialization and generated recommendations based upon the transition probabilities between items.</p><p>With the boom of deep learning, recurrent neural networks (RNNs) <ref type="bibr" target="#b14">[15]</ref> have been applied to session-based recommendation models to capture sequential order between items and achieved great success <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>. Hidasi et al. <ref type="bibr" target="#b12">[13]</ref> was the first that applied RNNs to model the whole session and introduced several modifications to vanilla RNNs such as a ranking loss function and session-parallel mini-batch training to generate more accurate recommendations.</p><p>As a follow-up study <ref type="bibr" target="#b32">[33]</ref>, Tan et al. enhanced RNNs by utilizing the technique of data augmentation and handling the temporal shifts of session data. Besides, NARM <ref type="bibr" target="#b17">[18]</ref>, a neural attentive recommendation algorithm, employs a hybrid encoder with attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session. In <ref type="bibr" target="#b19">[20]</ref>, a short-term attention priority model is developed to capture both local and global user interests with simple multilayer perceptrons (MLPs) networks and attention mechanism. Graph Neural Networks (GNNs) <ref type="bibr" target="#b41">[42]</ref> are recently introduced to session-based recommendation and exhibit great performance <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Unlike RNN-based approaches, graph structure is an essential factor in graph-based methods, aiming to learn item transitions over session graphs. For example, SR-GNN <ref type="bibr" target="#b40">[41]</ref> is the first to model session sequences in session graphs and applies a gated GNN model to aggregate information between items into session representations. MGNN-SPred <ref type="bibr" target="#b36">[37]</ref> builds a multi-relational item graph based on all session clicks to learn global item associations and uses a gated mechanism to adaptively predict the next item. GC-SAN <ref type="bibr" target="#b46">[47]</ref> dynamically constructs session-educed graphs and employs self-attention networks on the graphs to capture item dependencies via graph information aggregation. FGNN <ref type="bibr" target="#b26">[27]</ref> rethinks the sequence order of items to exploit users' intrinsic intents using GNNs. GCE-GNN <ref type="bibr" target="#b37">[38]</ref> aggregates item information from both item-level and session-level through graph convolution and self-attention mechanism. LESSR <ref type="bibr" target="#b4">[5]</ref> proposes an edge-order preserving aggregation scheme based on GRU and a shortcut graph attention layer to address the lossy session encoding problem and effectively capture long-range dependencies, respectively. Although these graph-based methods outperform RNN-based methods, they all suffer data sparsity problem due to the limited short-term profiles in session-based scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning in RS</head><p>Recently, self-supervised learning (SSL) <ref type="bibr" target="#b13">[14]</ref>, as a novel machine learning paradigm which mines free labels from unlabeled data and supervises models using the generated labels, is under the spotlight. The information or intermediate representation learned from self-supervised learning are expected to carry good semantic or structural meanings and can be beneficial to a variety of downstream tasks. SSL was initially applied in the fields of visual representation learning and language modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, where it augments the raw data through image rotation/clipping and sentence masking. Recent advances of SSL start to focus on graphs, and have received considerable attention <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>. DGI <ref type="bibr" target="#b34">[35]</ref> maximizes mutual information between the local patch and the global graph to refine node representations, making them as the ground-truth of each other. In InfoGraph <ref type="bibr" target="#b30">[31]</ref>, graph-level representations encode different aspects of data by encouraging agreement between the representations of substructures with different scales (e.g., nodes, edges, triangles). Hassani et al. <ref type="bibr" target="#b9">[10]</ref> contrasted multiple views of graphs and nodes to learn their representations. Qiu <ref type="bibr" target="#b25">[26]</ref> et al. designed a self-supervised graph neural network pre-training framework to capture the structural representations of graphs by leveraging instance discrimination and contrastive learning.</p><p>Inspired by the success of SSL in other areas, there are also some studies that integrate self-supervised learning into sequential recommendation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>. Bert4Rec <ref type="bibr" target="#b29">[30]</ref> transfers the cloze objective from language modeling to sequential recommendation by predicting the random masked items in the sequence with the surrounding contexts. ğ‘† 3 -Rec <ref type="bibr" target="#b56">[57]</ref> utilizes the intrinsic data correlations among attribute, item, subsequence and sequence to generate self-supervision signals and enhance the data representations via pre-training. Xie et al. <ref type="bibr" target="#b43">[44]</ref> proposed three data augmentation strategies to construct self-supervision signals from the original user behavior sequences, extracting more meaningful user patterns and encoding effective user representation. Ma et al. <ref type="bibr" target="#b21">[22]</ref> proposed a sequence-to-sequence training strategy based on latent self-supervision and disentanglement of user intention behind behavior sequences. Besides, SSL is also applied to other recommendation paradigms such as general recommendation <ref type="bibr" target="#b47">[48]</ref> and social recommendation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref>. Although these self-supervised methods have achieved decent improvements in recommendation performance, they are not suitable for session-based recommendation for the reason that the random dropout strategy used in these models would lead to sparser session data and unserviceable self-supervision signals. The most relevant work to ours is ğ‘† 2 -DHCN <ref type="bibr" target="#b42">[43]</ref> which conducts contrastive learning between representations learned over different hypergraphs by employing selfdiscrimination without random dropout. But it cannot learn invariant representations against the data variance for its fixed groundtruths, leading to merely slight improvements. Besides, Yu et al. <ref type="bibr" target="#b50">[51]</ref> recently proposed a self-supervised tri-training framework that leverages different aspects of social information to generate complementary self-supervision signals to boost recommendation. As the first work to combine SSL with multi-view semi-supervised learning for recommendation, it gives us clues about applying cotraining to session-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD 3.1 Preliminaries</head><formula xml:id="formula_0">3.1.1 Notations. Let ğ» = {ğ‘– 1 , ğ‘– 2 , ğ‘– 3 , ..., ğ‘– ğ‘ } denote the set of items,</formula><p>where ğ‘ is the number of items. Each session is represented as a sequence ğ‘  = [ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 , ğ‘– ğ‘ ,3 , ..., ğ‘– ğ‘ ,ğ‘š ] ordered by timestamps and ğ‘– ğ‘ ,ğ‘˜ âˆˆ ğ» (1 â‰¤ ğ‘˜ â‰¤ ğ‘š) represents an interacted item of an anonymous user within the session ğ‘ . For learning presentations, we embed each item ğ‘– âˆˆ ğ¼ into the same space and let x</p><formula xml:id="formula_1">(ğ‘™) ğ‘– âˆˆ R ğ‘‘ (ğ‘™ )</formula><p>denote the representation of item ğ‘– of dimension ğ‘‘ (ğ‘™) in the ğ‘™-th layer of a deep neural network. The representation of the whole item set is denoted as</p><formula xml:id="formula_2">X (ğ‘™) âˆˆ R ğ‘ Ã—ğ‘‘ (ğ‘™ )</formula><p>, and ğ‘¿ (0) is randomly initialized with uniform distribution. Each session ğ‘  is represented by a vector s. The task of session-based recommendation is to predict the next item, namely ğ‘– ğ‘ ,ğ‘š+1 , for any given session ğ‘ . Given ğ¼ and ğ‘ , the output of session-based recommendation model is a ranked list ğ‘¦ = [ğ‘¦ 1 , ğ‘¦ 2 , ğ‘¦ 3 , ..., ğ‘¦ ğ‘ ] where ğ‘¦ ğ‘– (1 â‰¤ ğ‘– â‰¤ ğ‘ ) is the corresponding predicted probability of item ğ‘–. The top-K items (1 â‰¤ ğ¾ â‰¤ ğ‘ ) with highest probabilities in ğ‘¦ will be selected as the recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Co-Training.</head><p>Co-Training is a classical semi-supervised learning paradigm to exploit unlabeled data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>. Under this regime, two classifiers are separately trained on two views and then exchange confident pseudo labels of unlabeled instances to construct additional labeled training data for each other. Typically, the two views are two disjoint sets of features and can provide complementary information to each other. Blum et al. <ref type="bibr" target="#b2">[3]</ref> first proved that co-training can bring significant benefits when the two views are sufficient and conditionally independent. However, the required conditional dependence of two views is hard to be satisfied in many cases. To relax this assumption, Abney et al. <ref type="bibr" target="#b0">[1]</ref> found that weak dependence can also enable co-training success, which lifts the dependence restriction and makes co-training easily applied. Furthermore, co-training can also be applied when there is only single data representation if the data is processed by independent prediction models, such as two different classifiers <ref type="bibr" target="#b45">[46]</ref>. It should be mentioned that there have been several attempts that combine cotraining and recommendation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>. However, these methods are all based on shallow or KNN-based models, leaving much space to be explored by the graph neural models coupled with SSL.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Supervised Graph Co-Training</head><p>In this section, the proposed self-supervised graph CO-Training framework for session-based RECommendation (COTREC) is presented. The overview of COTREC is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">View Augmentation.</head><p>To conduct co-training, we first derive two different views from the session data, i.e. item view and session view, by exploiting the intra-and inter-connectivity patterns of sessions. The item-view captures the item-level connectivity information while the session view encodes the session-level structural patterns. Concretely, the item view is educed by aligning all sessions. In other words, any two items (ğ‘– ğ‘ and ğ‘– ğ‘ ) which are connected in a session also get connected as nodes in the item view with a weighted directed edge ğ¸ ğ‘ğ‘ , counting how many times they are adjacent in different sessions in the form of [ğ‘– ğ‘ , ğ‘– ğ‘ ]. As for the session view, two sessions (ğ‘  ğ‘— and ğ‘  ğ‘˜ ) are connected as nodes with a weighted undirected edge ğ¸ ğ‘—ğ‘˜ obtained by using the number of shared items to divide the number of total items in the two sessions (shown in the left part of Figure <ref type="figure" target="#fig_0">1</ref>). These two views are able to provide complementary information for each other while keeping independent and exhibiting divergence to some degree, which are subject to the weak dependence constraint in <ref type="bibr" target="#b0">[1]</ref>. To make an analogy, if we intuitively consider the session data presented in the left side of Fig. <ref type="figure" target="#fig_0">1</ref> as the complete information, which is analogous to the whole picture in the task of image recognition, then constructing these two views corresponds to the patch clipping in visual self-supervised learning <ref type="bibr" target="#b3">[4]</ref>. The augmented parts differ but inherit essential information from the original data, which can help learn more generalizable representations through a self-supervised task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Learning Graph</head><p>Encoders over Augmented Views. After the view construction, we have two types of graphs. Although we aim to devise a model-agnostic framework that can drive a multitude of session-based neural graph recommendation models, for a concrete architecture than can fulfill the capability of the proposed self-supervised graph co-training, we construct two different graph encoders with graph convolutions over the views as the base. However, the technical details can be modified to adapt to more scenarios.</p><p>Item View Encoding. The item encoder with a simplified graph convolution layer for the item view is defined as:</p><formula xml:id="formula_3">X (ğ‘™+1) ğ¼ = Dâˆ’1 ğ¼ Ã‚ğ¼ X (ğ‘™) ğ¼ W ğ‘™ ğ¼ ,<label>(1)</label></formula><p>where Ã‚ğ¼ = A ğ¼ + I and I is the identity matrix, Dğ¼,ğ‘,ğ‘ = ğ‘š ğ‘=1 Ã‚ğ¼,ğ‘,ğ‘ and A ğ¼ are the degree matrix and the adjacency matrix, X (ğ‘™) ğ¼ and W ğ‘™ ğ¼ represent the ğ‘™-th layer's item embeddings and parameter matrix of the item view, respectively. Here we do not use the non-linear function since it has been proved redundant in recommendation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b52">53]</ref>. After passing X (0) through ğ¿ graph convolution layers, we average the item embeddings obtained from each layer to be the final learned item embeddings</p><formula xml:id="formula_4">X ğ¼ = 1 ğ¿+1 ğ¿ ğ‘™=0 X (ğ‘™)</formula><p>ğ¼ . Although the graph convolution can perfectly capture item connections, it cannot encode the order of items in a specific session. Following <ref type="bibr" target="#b37">[38]</ref>, we concatenate the reversed position embeddings with the learned item representations by a learnable position matrix P ğ‘Ÿ = [p 1 , p 2 , p 3 , ..., p m ], where ğ‘š is the length of the current session and p m âˆˆ R ğ‘‘ represents the vector of position ğ‘š. The embedding of ğ‘¡-th item in session ğ‘  = [ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 , ğ‘– ğ‘ ,3 , ..., ğ‘– ğ‘ ,ğ‘š ] is:</p><formula xml:id="formula_5">x ğ‘¡ * ğ¼ = tanh W 1 x ğ‘¡ ğ¼ âˆ¥p ğ‘šâˆ’ğ‘¡ +1 + b ,<label>(2)</label></formula><p>where W 1 âˆˆ R ğ‘‘Ã—2ğ‘‘ , and ğ‘ âˆˆ R ğ‘‘ are learnable parameters. Session embeddings can be obtained by aggregating representations of items contained in that session. A soft-attention mechanism is often used in session-based recommendation methods where different items should have different priorities when learning session embeddings. We follow the strategy used in GCE-GNN <ref type="bibr" target="#b37">[38]</ref> to refine the embedding of session ğ‘  = [ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 , ğ‘– ğ‘ ,3 , ..., ğ‘– ğ‘ ,ğ‘š ]:</p><formula xml:id="formula_6">ğ›¼ ğ‘¡ = f âŠ¤ ğœ W 2 x ğ‘  + W 3 x ğ‘¡ * ğ¼ + c , ğœƒ ğ¼ = ğ‘š âˆ‘ï¸ ğ‘¡ =1 ğ›¼ ğ‘¡ x ğ‘¡ * ğ¼ (3)</formula><p>where x ğ‘  is the embedding of session ğ‘  and here it is obtained by averaging the embeddings of items within the session ğ‘ , i.e.</p><p>x ğ‘  = 1 ğ‘š ğ‘š ğ‘¡ =1 x ğ‘š ğ¼ . Session representation ğœƒ ğ¼ is represented by aggregating item embeddings considering their corresponding importance. f, c âˆˆ R ğ‘‘ , W 2 âˆˆ R ğ‘‘Ã—ğ‘‘ and W 3 âˆˆ R ğ‘‘Ã—ğ‘‘ are attention parameters used to learn the item weight ğ›¼ ğ‘¡ .</p><p>Session View Encoding. The session view depicts item and session relations from the other perspective. Similarly, the session encoder conducts graph convolution on the session graph. As there are no items involved in the session graph, we first initialize the session embeddings ğš¯ (0) ğ‘† by averaging the corresponding embeddings of items of each session in X (0) . And the graph convolution on session graph is defined as followed:</p><formula xml:id="formula_7">Î˜ (ğ‘™+1) ğ‘† = Dâˆ’1 ğ‘† Ã‚ğ‘† Î˜ (ğ‘™) ğ‘† W (ğ‘™) ğ‘† ,<label>(4)</label></formula><p>where Ã‚ğ‘† = A ğ‘† + I, A ğ‘† is the adjacency matrix, and Dğ‘† is the corresponding degree matrix, Î˜</p><p>ğ‘† and W</p><p>(ğ‘™)</p><p>ğ‘† represent the ğ‘™-th layer's session embeddings and the parameter matrix, respectively. Similarly, we pass initialized session embeddings into ğ¿ graph convolution layers to learn session-level information. The final session representations are obtained by averaging ğ¿ embeddings learned at different layers, which is formulated as Recall that, in the last two subsections, we build two graph encoders over two different views that can provide complementary information to each other. Therefore, it is natural to refine each encoder by exploiting the information from the other view. This can be achieved by following the regime of co-training. Given a session ğ‘ in the session view, we predict its positive and negative next-item samples using its representation learned over the item view:</p><formula xml:id="formula_9">Î˜ ğ‘† = 1 ğ¿+1 ğ¿ ğ‘™=0 Î˜ (ğ‘™) ğ‘† .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Mining Self-Supervision</head><formula xml:id="formula_10">y ğ‘ ğ¼ = Softmax ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘ ğ¼ , ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘ ğ¼ = X ğ¼ ğœƒ ğ‘ ğ¼<label>(5)</label></formula><p>where ğœƒ ğ‘ ğ¼ is the representation of session ğ‘ in the item view, and y ğ‘ ğ¼ âˆˆ R ğ‘ denotes the predicted probability of each item being recommended to session ğ‘ in the item view. ğœƒ ğ‘ ğ¼ can be seen as a linear classifier, and X ğ¼ is seen as the unlabeled sample set.</p><p>With the computed probabilities, we can select items with the top-K highest confidence as the positive samples which act as the augmented ground-truths to supervise the session encoder. Formally, the positive sample selection is as follows:</p><formula xml:id="formula_11">ğ‘ ğ‘ + ğ‘† = top-ğ¾ y ğ‘ ğ¼ .<label>(6)</label></formula><p>As for the way to select negative samples, a straightforward idea is to take the items with the lowest scores. However, such a way can only choose easy samples which contribute little. Instead, we randomly select ğ¾ negative samples from the items ranked in top 10% in y ğ‘ ğ¼ excluding the positives to construct ğ‘</p><formula xml:id="formula_12">ğ‘ âˆ’</formula><p>ğ‘† . These items can be seen as hard negatives which can contribute enough information, and meanwhile are less likely to fall into the set of false negatives which would mislead the learning. Analogously, we use the similar way to select informative samples for the item encoder,</p><formula xml:id="formula_13">y ğ‘ ğ‘† = Softmax ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘ ğ‘† , ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘ ğ‘† = X (0) ğœƒ ğ‘ ğ‘† (7)</formula><p>where the main difference is that when selecting the top-ğ‘˜ item samples, ğ‘¿ (0) is used rather than ğ‘¿ ğ¼ because the session encoder does not output convolved item embeddings.</p><p>In each training batch, the positive and negative pseudo-labels for each session in each view are iteratively reconstructed and then are delivered to the other view as the possible next item for refining session and item representations. The intuition behind this process is that, the item samples, which receive high confidence to be the next-item in one view, should also be valuable in the other view. Iterating this process is expected to generate more informative examples (a.k.a. harder examples). In turn, the encoders evolve under the supervision of informative examples as well, which will recursively distill harder examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Contrastive Learning.</head><p>With the generated pseudo-labels, the self-supervised task used to refine encoders can be conducted through a contrastive objective. In session-based scenarios, we assume that the last clicked item in a session is the most related to the next item. Therefore, we can maximize (minimize) the agreement between the representations of the last-clicked item and the predicted items samples, accompanied with the given session representation as the session context. Formally, given a session ğ‘ and the predicted ground-truths, we follow InfoNCE <ref type="bibr" target="#b22">[23]</ref>, which can maximize the lower bound of mutual information between the item pairs, as our learning objective:</p><formula xml:id="formula_14">L ğ‘ ğ‘ ğ‘™ = âˆ’ log ğ‘– âˆˆğ‘ ğ‘ + ğ¼ ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ ğ¼ , ğœ½ ğ‘ ğ¼ , x ğ‘– ğ¼ ğ‘– âˆˆğ‘ ğ‘ + ğ¼ ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ ğ¼ , ğœ½ ğ‘ ğ¼ , x ğ‘– ğ¼ + ğ‘— âˆˆğ‘ ğ‘ âˆ’ ğ¼ ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ ğ¼ , ğœ½ ğ‘ ğ¼ , x ğ‘— ğ¼ âˆ’ log ğ‘– âˆˆğ‘ ğ‘ + ğ‘† ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ (0) , ğœ½ ğ‘ ğ‘† , x ğ‘–<label>(0)</label></formula><formula xml:id="formula_15">ğ‘ âˆˆğ‘ ğ‘ + ğ‘† ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ (0) , ğœ½ ğ‘ ğ‘† , x ğ‘– (0) + ğ‘— âˆˆğ‘ ğ‘ âˆ’ ğ‘† ğœ“ x ğ‘™ğ‘ğ‘ ğ‘¡ (0) , ğœ½ ğ‘ ğ‘† , x ğ‘— (0)<label>(8)</label></formula><p>where x ğ‘™ğ‘ğ‘ ğ‘¡ is the embedding of the last-clicked item of the given session, ğœ“ (ğ’™ 1 , ğ’™ 2 , ğ’™ 3 ) = exp (ğ‘“ (ğ’™ 1 + ğ’™ 2 , ğ’™ 3 + ğ’™ 2 ) /ğœ) where ğœ is the temperature to amplify the effect of discrimination (we empirically use 0.2 in our experiments), and ğ‘“ (â€¢) : R ğ‘‘ Ã—R ğ‘‘ â†¦ âˆ’â†’ R is the discriminator function that takes two vectors as the input and then scores the agreement between them. We simply implement the discriminator by applying the cosine operation. Through the contrastive learning between the positive and negative pairs, the two views can exchange information and the last-clicked item representation can learn to infer related items with a session context, and thus item and session representations are refined.</p><p>In the vanilla co-training, the generated pseudo-labels are reused in subsequent training as training labels. However, that way will make our framework less efficient because adding pseudo-labels will lead to adjacency matrix reconstruction in each iteration. Also, it may misguide the training from then on when the pseudo-labels introduce false information because the added pseudo-labels would not be removed. Therefore, in our model, we decide not to add pseudo-labels into training set in view of the above considerations. Besides, compared with the dropout based SSL methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b56">57]</ref> which leverage the fragmentary sequences as self-supervision signals, our idea has the advantage of preserving the complete session information and fulfilling genuine label augmentation, and hence it is more suitable for session-based scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Divergence Constraint in Co-</head><p>Training. In our framework, the two data views for co-training are derived from the same data source by exploiting structural information in different aspects. On the one hand, this augmentation does not require two sufficient and independent data sources, which is the advantage. But on the other hand, it somehow might lead to the mode collapse problem, i.e., two encoders become similar and generated the same ground-truths when given the same session after a number of learning iterations. Therefore, it is necessary to make the two encoders differ to some degree. Following <ref type="bibr" target="#b24">[25]</ref>, we impose the divergence constraint on the self-supervised graph co-training regime by integrating adversarial examples into the training.</p><p>Theoretically, the adversarial examples targeting one encoder <ref type="bibr" target="#b7">[8]</ref> would mislead it to generate wrong predictions. However, if the two encoders are trained to be resistant to the adversarial examples generated by each other and still output the correct predictions, we can manage to achieve the goal of keeping them different. We define the divergence constraint as follows:</p><formula xml:id="formula_16">L diff =ğ¾ğ¿ ğ‘ƒğ‘Ÿğ‘œğ‘ ğ¼ (X ğ¼ ), ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘† X ğ¼ + Î” ğ¼ ğ‘ğ‘‘ğ‘£ + ğ¾ğ¿ ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘† (X ğ¼ ), ğ‘ƒğ‘Ÿğ‘œğ‘ ğ¼ X ğ¼ + Î” ğ‘† ğ‘ğ‘‘ğ‘£ ,<label>(9)</label></formula><p>where ğ‘ƒğ‘Ÿğ‘œğ‘ ğ¼ (â€¢) and ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘† (â€¢) represent the probabilities of each item to be recommended to a given session ğ‘, which are computed by two encoders:</p><formula xml:id="formula_17">ğ‘ƒğ‘Ÿğ‘œğ‘ ğ¼ (X ğ¼ ) = Softmax(ğ‘¿ ğ¼ ğœ½ ğ‘ ğ¼ ), and ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘† (X ğ¼ ) = Softmax(ğ‘¿ ğ¼ ğœ½ ğ‘ ğ‘† ), Î” ğ¼</formula><p>ğ‘ğ‘‘ğ‘£ and Î” ğ‘† ğ‘ğ‘‘ğ‘£ represent the adversarial perturbations on the item embeddings with regard to ğœ½ ğ‘ ğ¼ and ğœ½ ğ‘ ğ‘† , respectively, and ğ¾ğ¿(â€¢) denotes the KL divergence. To make it clear,</p><formula xml:id="formula_18">ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘† X ğ¼ + Î” ğ¼</formula><p>ğ‘ğ‘‘ğ‘£ is the probability distribution produced by the session encoder when ğ‘¿ ğ¼ is perturbed by Î” ğ¼ ğ‘ğ‘‘ğ‘£ . If the session encoder is immune to Î” ğ¼ ğ‘ğ‘‘ğ‘£ that is destructive to the item encoder, it will output a probability distribution similar to ğ‘ƒğ‘Ÿğ‘œğ‘ ğ¼ (X ğ¼ ) due to shared information, resulting in a smaller loss of Eq. ( <ref type="formula" target="#formula_16">9</ref>), otherwise not.</p><p>To create adversarial examples, we adopt the FGSM method proposed in <ref type="bibr" target="#b7">[8]</ref>, which adds adversarial perturbations on model parameters through fast gradient computation. In our paper, we add adversarial perturbations on item embeddings. The perturbations Î” are updated as:</p><formula xml:id="formula_19">Î” ğ‘ğ‘‘ğ‘£ = ğœ– Î“ âˆ¥Î“âˆ¥ where Î“ = ğœ•ğ‘™ ğ‘ğ‘‘ğ‘£ ( Å· | x + Î”) ğœ•Î” . (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>ğ‘™ ğ‘ğ‘‘ğ‘£ ( Å·) is the loss of aversarial examples and ğœ– is the control parameter (ğœ– is 0.5 on Diginetica and 0.2 on Tmall and RetailRocket in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Model Optimization.</head><p>Based on the learned representations, the score of each candidate item ğ‘– âˆˆ ğ¼ to be recommended for a session ğ‘  is computed by doing inner product:</p><formula xml:id="formula_21">áº‘ğ‘– = ğœƒ ğ‘ âŠ¤ ğ¼ x ğ‘– . (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>Since the item view can reflect the item connectivity in a finergrained granularity, we use the encoder over the item view as the main encoder to predict the final candidate items for recommendation. After that, a softmax function is applied:</p><formula xml:id="formula_23">Å· = softmax(áº‘). (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>Algorithm 1: The whole procedure of COTREC Input: Sessions S, node embeddings ğ‘¿ ; Output: Recommendation lists 1 Construct item view and session view ; 2 for each iteration do  <ref type="formula" target="#formula_10">5</ref>) -Eq. ( <ref type="formula">7</ref>);</p><formula xml:id="formula_25">7</formula><p>Compute self-supervised learning loss of two views via Eq.( <ref type="formula" target="#formula_15">8</ref>);</p><formula xml:id="formula_26">8 end 9</formula><p>Add divergence constraint by following Eq. ( <ref type="formula" target="#formula_16">9</ref>) -( <ref type="formula" target="#formula_19">10</ref>);</p><formula xml:id="formula_27">10</formula><p>Jointly optimize the overall objective in Eq. ( <ref type="formula" target="#formula_30">14</ref>);</p><formula xml:id="formula_28">11 end 12 end</formula><p>We then use cross entropy loss function to be the learning objective:</p><formula xml:id="formula_29">L ğ‘Ÿ = âˆ’ ğ‘ âˆ‘ï¸ ğ‘–=1 y ğ‘– log ( Å·ğ‘– ) + (1 âˆ’ y ğ‘– ) log (1 âˆ’ Å·ğ‘– ) .<label>(13)</label></formula><p>y is the one-hot encoding vector of the ground truth. For simplicity, we leave out the ğ¿ 2 regularization terms. Finally, we unify the recommendation task with the auxiliary SSL task. The total loss ğ¿ is defined as:</p><formula xml:id="formula_30">L = L ğ‘Ÿ + ğ›½L ğ‘ ğ‘ ğ‘™ + ğ›¼ L diff ,<label>(14)</label></formula><p>where ğ›¼, ğ›½ are hyperparameters to control the scale of the selfsupervised graph co-training and view difference constraint. It should be noted that, we jointly optimize the three throughout the training. Finally, the whole procedure of COTREC is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Tmall P@10 M@10 P@20 M@20 P@10 M@10 P@20 M@20 P@10 M@10 P@20 M@20 FPMC 13.10 comes from IJCAI-15 competition, which contains anonymized user's shopping logs on Tmall online shopping platform. Retail-Rocket is a dataset on a Kaggle contest published by an e-commerce company, which contains the user's browsing activity within six months. Diginetica dataset describes the music listening behavior of users, and Diginetica comes from CIKM Cup 2016. For convenience of comparing, we follow the experiment environment in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>. Specifically, we filter out all sessions whose length is 1 and items appearing less than 5 times. Latest data (such as, the data of last week) is set to be test set and previous data is used as training set. Then, we augment and label the training and test datasets by using a sequence splitting method, which generates multiple labeled sequences with the corresponding labels ([ğ‘– ğ‘ ,1 ], ğ‘– ğ‘ ,2 ), ( [ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 ], ğ‘– ğ‘ ,3 ), ..., ([ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 , ..., ğ‘– ğ‘ ,ğ‘šâˆ’1 ], ğ‘– ğ‘ ,ğ‘š ) for every session ğ‘  = [ğ‘– ğ‘ ,1 , ğ‘– ğ‘ ,2 , ğ‘– ğ‘ ,3 , ..., ğ‘– ğ‘ ,ğ‘š ]. Note that the label of each sequence is the last click item in it. The statistics of the datasets are presented in Table <ref type="table">1</ref>.</p><p>4.1.2 Baseline Methods. We compare COTREC with the following representative methods:</p><p>â€¢ FPMC <ref type="bibr" target="#b27">[28]</ref> is a sequential method based on Markov Chain. In order to adapt it to session-based recommendation, we do not consider the user latent representations when computing recommendation scores. â€¢ GRU4REC <ref type="bibr" target="#b12">[13]</ref> utilizes a session-parallel mini-batch training process and adopts ranking-based loss functions to model user sequences.</p><p>â€¢ NARM <ref type="bibr" target="#b17">[18]</ref>: is a RNN-based state-of-the-art model which employs attention mechanism to capture user's main purpose and combines it with the sequential behavior to generate the recommendations.</p><p>â€¢ STAMP <ref type="bibr" target="#b19">[20]</ref>: adopts attention layers to replace all RNN encoders in the previous work and employs the self-attention mechanism <ref type="bibr" target="#b33">[34]</ref> to enhance the session-based recommendation performance. â€¢ SR-GNN <ref type="bibr" target="#b40">[41]</ref>: applies a gated graph convolutional layer to obtain item embeddings and also employs a soft-attention mechanism to compute the session embeddings. â€¢ GCE-GNN <ref type="bibr" target="#b37">[38]</ref>: constructs two types of session-educed graphs to capture local and global information in different levels. â€¢ S 2 -DHCN <ref type="bibr" target="#b42">[43]</ref>: constructs two types of hypergraphs to learn inter-and intra-session information and uses self-supervised learning to enhance session-based recommendation.</p><p>4.1.3 Evaluation Metrics. Following <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>, we use P@K (Precision) and MRR@K (Mean Reciprocal Rank) to evaluate the recommendation results where K is 10 or 20.</p><p>4.1.4 Hyper-parameters Settings. Following previous works, we set the embedding size to 100, the batch size for mini-batch to 100, and the ğ¿ 2 regularization to 10 âˆ’5 . In our model, all parameters are initialized with the Gaussian Distribution N (0, 0.1). We use Adam with the learning rate of 0.001 to optimize our model. For the number of layers of graph convolution on the three datasets, a three-layer setting achieves the best performance. For the baseline models, we refer to their best parameter setups reported in the original papers and directly report their results if available, since we use the same datasets and evaluation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall</head><p>Performance. The experimental results of overall performance are reported in Table <ref type="table" target="#tab_3">2</ref>, where we highlight the best results of each column in boldface. From the results, we can draw some conclusions:</p><p>â€¢ Recent methods that consider temporal information (such as, GRU4REC, NARM, STAMP, SR-GNN) outperform traditional methods (FPMC) that do not, demonstrating the importance of capturing sequential dependency between items in session-based recommendation. Besides, among the methods based on RNNslike units (RNN, LSTM, GRU), NRAM and STAMP achieve better performance than GRU4REC. This is because NRAM and STAMP not only utilize recurrent neural networks to model sequential behavior, but also utilize an attention mechanism to learn importance of each item when learning session representations. GRU4REC which only uses GRU cannot handle the shift of user preference. â€¢ Graph-based baseline methods all outperform RNN-based methods, showing the great capacity of graph neural networks in modeling session data. Among them, GCE-GNN obtains higher accuracy than SR-GNN. This proves that capturing different levels of information (inter-and intra-session information) helps accurately predict user intent in session-based recommendation. ğ‘† 2 -DHCN also utilize both inter-and intra-session information in hypergraph modeling and achieves promising performance. However, compared to GCE-GNN, ğ‘† 2 -DHCN has lower results on Tmall and Diginetica, showing that self-discrimination based SSL method is not so successful in improving session-based recommendation performance, which is in line with our motivation. â€¢ Our proposed COTREC almost outperforms all the baselines on all the datasets. Particularly, it beats other models by a large margin on Tmall, showing the effectiveness of the self-supervised grpah co-training when applied to real e-commerce data. Compared with the other self-supervised model ğ‘† 2 -DHCN, the advantage is also obvious. Considering that ğ‘† 2 -DHCN and COTREC both have a two-branch architecture, we think that the improvements mainly derive from the multi-instance contrastive learning in Eq. ( <ref type="formula" target="#formula_15">8</ref>) while ğ‘† 2 -DHCN only conducts self-discrimination contrastive learning. Compared with another strong baseline GCE-GNN, COTREC is competitive in terms of both performance and efficiency. Although GCE-GNN can achieve comparable results on Diginetica, its more complex structure makes it suffer from the out-of-memory problem when performing on RetailRocket on our RTX 2080 Ti GPU. Besides, its performance is much lower than that of COTREC on Tmall. From Figure <ref type="figure" target="#fig_4">2</ref>, we can observe that each component consistently contributes on both datasets. The self-supervised co-training improves the base model the most, serving as the driving force of the performance improvement. When removing the self-supervised co-training, we can observe a remarkable performance drop on both the two metrics. Besides, the divergence constraint is effective to prevent mode collapse in co-training process. Without this module, the performance of COTREC is even worse than that of the base on Diginetica. Note that on Tmall, base-NP outperforms COTREC-base, proving that a strict temporal order of items may negatively influence the performance in some cases, which is in line with the our previous observation in ğ‘† 2 -DHCN. According to the results of base-NA, it is shown that learning different item importance across sessions is better than directly averaging representations of contained items for learning session representations in session-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Tmall Diginetica P@20 M@20 P@20 M@20 COTREC-base   The second compared SSL method is based on the item mask, which is an often used strategy where some items are randomly dropped in each session. The generated new session can be the positive example and other sessions can be negative samples. For a fair comparison, we employ these SSL strategies on the base model of COTREC. So we name the three as base-COTREC, base-DHCN, base-MASK. Besides, these SSL methods are used to establish auxiliary tasks in the model optimization and we use a hyperparameter to control the magnitude of SSL. Finally, we use grid-search to adjust the parameter to ensure the best performances of them, and the best results are shown in Table <ref type="table" target="#tab_4">3</ref>.</p><p>From Table <ref type="table" target="#tab_4">3</ref>, we can see that, only the self-supervised graph co-training can boost the recommendation performance on both datasets and it also achieves the best performance, while the other two methods can only take effect on Tmall, demonstrating that selfsupervised graph co-training is more effective than self-discrimination and dropout-based methods. We also find that the strategy of item mask is the least effective in most cases, proving that masked subsequences can only generate sub-optimal self-supervision signals in the scenario of session-based recommendation due to the very limited behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Handling Different Session Lengths.</head><p>In real world situations, sessions with various lengths are common, so it is interesting to know how stable our COTREC as well as the baseline models are when dealing with them, and it is also a critical indicator for production environments. To evaluate this, we follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref> to split the sessions of Tmall and Diginetica into two groups with different lengths and name them as Short and Long. Short contains sessions whose lengths are less than or equal to 5, while Long contains sessions whose lengths are larger than 5. Then, we compare the performance of COTREC with some representative baselines, i.e., STAMP, SR-GNN, GCE-GNN and ğ‘† 2 -DHCN in terms of Prec@20   on Short and Long. Results in Figure <ref type="figure">3</ref> show that almost outperforms all the baseline models on both datasets with different session lengths. It demonstrates the adaptability of COTREC in real-world session-based recommendation. Besides, it is shown that the performance on the short sessions is better than that on the long sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.5</head><p>The Impact of Hyperparameters. In COTREC, we have two hyperparameters to control the magnitude of the SSL taks and the effect of the divergence constraint, i.e. ğ›½ and ğ›¼. To investigate the influence of them, we report the performance with a set of representative ğ›½ values in { 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1} and ğ›¼ values in {0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5} on Tmall and Diginetica. We fix the other parameter as 0.005 when investigating ğ›½ or ğ›¼. According to the results in Figure <ref type="figure">4</ref>, our model achieves the best performance when jointly trained with the SSL taks and the divergence constraint. On both Tmall and Diginetica, the best setting is ğ›½ = 0.05 and ğ›¼ = 0.005. When using large ğ›¼, a huge performance drop is observed, demonstrating that when there is large divergence between two encoder, it is hard for them to supervise each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.6</head><p>The impact of the number of layers. To investigate the impact of the number of layers in graph convolution network, we range the number of layers in {1, 2, 3, 4, 5}. According to the results in Figure <ref type="figure" target="#fig_6">5</ref>, we can see that for both Tmall and Diginetica, a three-layer setting achieves the best performance. When the number becomes larger, performance will drop due to the over-smoothing issue. Besides, an obvious performance fluctuation is observed on Diginetica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Self-supervised learning is an emerging machine learning paradigm which exploits unlabeled data by generating ground-truth labels from the raw data itself and recently has been utilized in many fields to enhance deep learning models. Existing SSL-based recommendation methods usually adopt random dropout-based self-discrimination to generate self-supervision signals. However, we argue that it cannot adapt to session-based recommendation because it would create sparser data and cannot leverage informative self-supervision signals from other entities. In this paper, we design a self-supervised graph co-training framework to address this issue. In our framework, co-training can iteratively selects evolving pseudo-labels as informative self-supervision examples for each view to improve the session-based recommendation. Extensive experiments and empirical studies demonstrate the effectiveness of our framework and show its superiority over other recent baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the proposed COTREC framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Signals with Graph Co-Training. In this section, we show how graph co-training mines informative self-supervision signals to enhance session-based recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 for each batch do 4 5 for each session ğ‘  do 6</head><label>3456</label><figDesc>Learn item and session representations through Eq.(1) -(4) ; Predict the probabilities of items being the positive examples in different views and obtain positive and negative examples with Eq.(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 2 . 2</head><label>22</label><figDesc>Ablation Study. In this section, to investigate the contribution of each component in our model, we develop four variant versions of COTREC: COTREC-base, base-NP, base-NA, COTREC-ND, and we compare the four variants with the complete COTREC model on Tmall and Diginetica. In COTREC-base, we only use the item view to model session data, removing the session view and the self-supervised graph co-training. In base-NP, we remove the reversed position embeddings. base-NA means that we remove the soft-attention mechanism and replace it with averaging item representations as the representation of each session. In COTREC-ND, we only use self-supervised co-training without the divergence constraint. We show the results of these four variants in Figure2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ablation Study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: P@20 results on Long and Short.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The impacts of the number of layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>â€¢ Extensive experiments show that the proposed framework has overwhelming superiority over the state-of-the-art baselines and achieves statistically significant improvements on benchmark datasets. We release the code at https://github.com/xiaxin1998/ COTREC.</figDesc><table /><note>The rest of this paper is organized as follows. Section 2 summarizes the related work of session-based recommendation and selfsupervised learning. Section 3 presents the proposed framework. The experimental results are reported in Section 4. Finally, Section 5 concludes this paper.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tmall 1 , RetailRocket 2 and Diginetica 3 , which are often used in session-based recommendation methods. Tmall dataset</figDesc><table><row><cell>RetailRocket Diginetica 433,643 719,470 15,132 60,858 36,968 43,097 5.43 5.12 Table 1: Dataset Statistics training sessions 351,268 test sessions 25,898 # of items 40,728 average lengths 6.69 4 EXPERIMENTS 4.1 Experimental Settings 4.1.1 Datasets. We evaluate our model on three real-world bench-mark datasets: Method Tmall RetailRocket Diginetica</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>.65 36.35 18.04 48.61 29.46 56.17 29.97 41.88 18.16 54.18 19.07 Performances of all comparison methods on three datasets.</figDesc><table><row><cell></cell><cell></cell><cell>7.12</cell><cell>16.06</cell><cell>7.32</cell><cell>25.99</cell><cell>13.38</cell><cell>32.37</cell><cell>13.82</cell><cell>15.43</cell><cell>6.20</cell><cell>26.53</cell><cell>6.95</cell></row><row><cell>GRU4REC</cell><cell>9.47</cell><cell>5.78</cell><cell>10.93</cell><cell>5.89</cell><cell>38.35</cell><cell>23.27</cell><cell>44.01</cell><cell>23.67</cell><cell>17.93</cell><cell>7.33</cell><cell>29.45</cell><cell>8.33</cell></row><row><cell>NARM</cell><cell>19.17</cell><cell>10.42</cell><cell>23.30</cell><cell>10.70</cell><cell>42.07</cell><cell>24.88</cell><cell>50.22</cell><cell>24.59</cell><cell>35.44</cell><cell>15.13</cell><cell>49.70</cell><cell>16.17</cell></row><row><cell>STAMP</cell><cell>22.63</cell><cell>13.12</cell><cell>26.47</cell><cell>13.36</cell><cell>42.95</cell><cell>24.61</cell><cell>50.96</cell><cell>25.17</cell><cell>33.98</cell><cell>14.26</cell><cell>45.64</cell><cell>14.32</cell></row><row><cell>SR-GNN</cell><cell>23.41</cell><cell>13.45</cell><cell>27.57</cell><cell>13.72</cell><cell>43.21</cell><cell>26.07</cell><cell>50.32</cell><cell>26.57</cell><cell>36.86</cell><cell>15.52</cell><cell>50.73</cell><cell>17.59</cell></row><row><cell cols="2">GCE-GNN 28.01</cell><cell>15.08</cell><cell>33.42</cell><cell>15.42</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.16</cell><cell cols="3">18.15 54.22 19.04</cell></row><row><cell cols="2">ğ‘† 2 -DHCN 26.22</cell><cell>14.60</cell><cell>31.42</cell><cell>15.05</cell><cell>46.15</cell><cell>26.85</cell><cell>53.66</cell><cell>27.30</cell><cell>39.87</cell><cell>17.53</cell><cell>53.18</cell><cell>18.44</cell></row><row><cell cols="3">COTREC 30.62 17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of Different SSL Methods.</figDesc><table><row><cell></cell><cell>27.71</cell><cell>13.36</cell><cell>52.66</cell><cell>18.19</cell></row><row><cell>base-DHCN</cell><cell>32.94</cell><cell>16.22</cell><cell>52.02</cell><cell>17.70</cell></row><row><cell>base-MASK</cell><cell>32.54</cell><cell>16.37</cell><cell>51.61</cell><cell>17.64</cell></row><row><cell>COTREC</cell><cell>36.35</cell><cell>18.04</cell><cell>54.18</cell><cell>19.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>4.2.3 Comparison with Different SSL Methods. To further investigate the effectiveness of the proposed self-supervised graph cotraining, we also compare it with other different SSL methods that are based on self-discrimination and random dropout to generate self-supervision signals on Tmall and Diginetica. The first is the method proposed in ğ‘† 2 -DHCN. DHCN proposes to capture item-level and session-level information and maximize mutual information between the session representations learned at the two levels. Positive examples are two types of session representation of the same session, whereas negative pairs are representations of different sessions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://tianchi.aliyun.com/dataset/dataDetail?dataId=42</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.kaggle.com/retailrocket/ecommerce-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://cikm2016.cs.iupui.edu/cikm-cup/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by ARC Discovery Project (Grant No. DP190101985),ARC Future Fellowship (FT210100624), National Natural Science Foundation of China (No. U1936104), CCF-Baidu Open Fund, and The Fundamental Research Funds for the Central Universities 2020RC25.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
				<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Handling Information Loss of Graph Neural Networks for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CoRec: a co-training approach for recommender systems</title>
		<author>
			<persName><forename type="first">Arthur F Da</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><forename type="middle">G</forename><surname>Manzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Jgb</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Symposium on Applied Computing</title>
				<meeting>the 33rd Annual ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="696" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05582</idno>
		<title level="m">Contrastive Multi-View Representation Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with top-k gains for session-based recommendations</title>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Linas Baltrunas, and Domonkos Tikk</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting the next location: A recurrent model with spatial and temporal contexts</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">STAMP: shortterm attention/memory priority model for session-based recommendation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Star Neural Networks for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (eccv</title>
				<meeting>the european conference on computer vision (eccv</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An MDP-based recommender system</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-09">2005. Sep (2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
				<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Where to go next: Modeling long-and short-term user preferences for point-of-interest recommendation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Hongzhi Yin</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved recurrent neural networks for session-based recommendations</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Kiam Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A survey on session-based recommender systems</title>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04864</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond clicks: Modeling multi-relational item graph for session-based target behavior prediction</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3056" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Global context enhanced graph neural networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07342</idno>
		<title level="m">Selfsupervised on Graphs: Contrastive, Generative, or Predictive</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06852</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bolin Ding, and Bin Cui</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14395</idno>
	</analytic>
	<monogr>
		<title level="m">Contrastive Pre-training for Sequential Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Self-Supervised Reinforcement Learning forRecommender Systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon M</forename><surname>Jose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05779</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<title level="m">A survey on multi-view learning</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph contextualized selfattention network for session-based recommendation</title>
		<author>
			<persName><forename type="first">Chengfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Victor S Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf. Artif. Intell.(IJCAI)</title>
				<meeting>28th Int. Joint Conf. Artif. Intell.(IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3940" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Self-supervised learning for deep models in recommendations</title>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Kang</surname></persName>
		</author>
		<idno>arXiv-2007</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Spatio-temporal recommendation in social media</title>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive implicit friends identification over heterogeneous network for social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on information and knowledge management</title>
				<meeting>the 27th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03569</idno>
		<title level="m">Xin Xia, Xiangliang Zhang, and Nguyen Quoc Viet Hung. 2021. Socially-Aware Self-Supervised Tri-Training for Recommendation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Enhance Social Recommendation with Adversarial Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02340</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note>Nguyen Quoc Viet Hung, and Xiangliang Zhang</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Addressing cold start in recommender systems: A semi-supervised co-training algorithm</title>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
				<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sequential click prediction for sponsored search with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Discrete deep learning for fast content-aware recommendation</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
				<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="717" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07873</idno>
		<title level="m">SË†3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Using temporal data for making recommendations</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zimdars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.2320</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
