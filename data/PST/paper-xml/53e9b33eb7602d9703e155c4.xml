<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proposal Maps driven MCMC for Estimating Human Body Pose in Static Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lee</surname></persName>
							<email>munlee@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems Integrated Media Systems Center University of Southern California</orgName>
								<address>
									<postCode>90089-0273</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
							<email>icohen@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems Integrated Media Systems Center University of Southern California</orgName>
								<address>
									<postCode>90089-0273</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Proposal Maps driven MCMC for Estimating Human Body Pose in Static Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36BA9E3C8B09217F71BBBC276DABF8DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of estimating human body pose in static images. This problem is challenging due to the high dimensional state space of body poses, the presence of pose ambiguity, and the need to segment the human body in an image. We use an image generative approach by modeling the human kinematics, the shape and the clothing probabilistically. These models are used for deriving a good likelihood measure to evaluate samples in the solution space. We adopt a data-driven MCMC framework for searching the solution space efficiently. Our observation data include the face, head-shoulders contour, skin color blobs, and ridges; and they provide evidences on the positions of the head, shoulders and limbs. To translate these inferences into pose hypotheses, we introduce the use of 'proposal maps', which is an efficient way of consolidating the evidence and generating 3D pose candidates during the MCMC search. As experimental results show, the proposed technique estimates the human 3D pose accurately on various test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pose estimation in video sequences has been addressed in many previous works, using either multiple</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating human body pose is important for automatic recognition of human activities in image understanding applications. For static images, the major difficulties are the high dimensionality of the solution space, pose ambiguity, and body segmentation.</p><p>The human body has about 31 parameters and pose estimation involves searching in a high dimensional and multi-modal solution space. In addition, there is an inherent non-observability of some of the degrees of freedom, causing "forwards/backwards flipping ambiguities" <ref type="bibr" target="#b9">[10]</ref> in the depths of body joints. Ambiguity is also caused by noisy or spurious image features.</p><p>The segmentation of the human body is required because the human boundary in an image is dependent on the body pose, and this boundary affects the feature extraction needed to estimate the body pose. This calls for a method that simultaneously solves the dualproblem of segmentation and pose estimation.</p><p>We propose to address this problem by building an image generative model and using the Markov chain Monte Carlo (MCMC) framework <ref type="bibr" target="#b1">[2]</ref> to search the 31-D solution space. Our human model represents the kinematics structure, shape and clothing of the human body. Given a pose candidate, a human image can be synthesized and compared with the real image. This model-based approach is appealing as it seeks to "explain away the data" from the image generation standpoint <ref type="bibr" target="#b13">[14]</ref>; and it solves the segmentation problem simultaneously. The set of samples drawn by MCMC weakly converges to a stationary distribution equivalent to the posterior distribution. However, with only the use of random-walk sampler algorithm, the MCMC framework is inefficient.</p><p>The data-driven MCMC framework <ref type="bibr" target="#b13">[14]</ref> allows us to design complementary jump proposal functions, derived from image observations, to explore the solution space more efficiently. Each jump dynamic has a much larger scope and allows the transitions between nonneighboring regions of high densities. Useful image observations are obtained from appearance-based face detection, matching head and shoulders contours, skin color blobs detection, and limbs detection. However, these observations only provide 2D inferences on local body parts but not the 3D pose. Also, these observations have localization errors, contain false alarms, and may not be independent. A mechanism is needed to properly translate these observations into proposal distributions for the 3D pose, while addressing the above issues.</p><p>In this paper, we describe the use of proposal maps to consolidate the inferences provided by the collective set of image observations. Each proposal map represents the proposal distribution of the image position of a body joint and it is used to generate proposals of 3D pose during MCMC.</p><p>We focused on middle resolution images, where the body height is about 150 pixels. We make no restrictive assumptions about the background, the human shape, and clothing except for not wearing any headwear or gloves.</p><p>The paper is organized as follows: Section 2 discusses related work; Section 3 describes the MCMC framework and its extension to pose estimation; the generative model and image observation are presented in Sections 4 and 5, respectively; and Section 6 shows experimental results. or a single camera <ref type="bibr">[1][9]</ref>. Many of these works used particle filter to track the body poses, by relying on a good initialization, temporal smoothness, and sometimes a low dimensional dynamic model <ref type="bibr" target="#b0">[1]</ref>.</p><p>For static images, some works have been reported for recognizing prototypical body poses using shape context descriptors <ref type="bibr" target="#b4">[5]</ref>, mapping of features into body configurations <ref type="bibr" target="#b6">[7]</ref>, and parameter-sensitive hashing <ref type="bibr" target="#b7">[8]</ref>. These works rely on either a clean background or a presegmented human region and not suitable for automatic pose estimation.</p><p>There are reported works on detecting body parts in images. In <ref type="bibr" target="#b5">[6]</ref>[3], the authors model the appearance and the 2D geometric configuration of body parts. These methods focus on real-time detection of people and do not estimate the 3D body pose. Recovering 3D pose was studied in <ref type="bibr" target="#b10">[11]</ref>, but the proposed method assumes that the image positions of body joints are known and therefore simplifies the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Estimation Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data-Driven MCMC</head><p>In this section, we describe the MCMC framework <ref type="bibr" target="#b1">[2]</ref>[14] and its adaptation for pose estimation. Denoting x as the vector of model parameters and Y as the image observations, pose estimation is formulated as a Bayesian inference for estimating the posterior distribution:</p><formula xml:id="formula_0">) ( ) | ( ) | ( x x Y Y x p p p ∝ .<label>(1)</label></formula><p>The desired output is dependent on the application. A simple solution is the maximum a posteriori estimate (MAP) given by:</p><formula xml:id="formula_1">) | ( max arg Y x x x p MAP = . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Since the posterior distribution is multi-modal, it is often desirable to extract multiple solutions of x. A simple approach consists in approximating the posterior distribution as a mixture of Gaussians:</p><formula xml:id="formula_3">) , ( ) | ( Σ ≈ ∑ i i i N w p µ Y x . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>MCMC is a suitable methodology for finding these solutions by drawing samples from the posterior distribution using a Markov chain based on the Metropolis-Hastings algorithm <ref type="bibr" target="#b1">[2]</ref>. At the t-1 th iteration, a candidate x′ is sampled from a proposal distribution ) | (.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">-t q x</head><p>and accepted as the new state x t with a probability ( )</p><formula xml:id="formula_5">x x ′ → -1 t a where ( )       ′ ′ ′ = ′ → - - - - ) | ( ) | ( ) | ( ) | ( , 1 min 1 1 1 1 t t t t q p q p a x x Y x x x Y x x x . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>A simple proposal distribution is provided by the random-walk sampler <ref type="bibr" target="#b1">[2]</ref>.</p><p>In the data-driven MCMC paradigm, image observations are used to generate additional proposals to en-hance the convergence of the MCMC <ref type="bibr" target="#b13">[14]</ref>. As the mapping of 2D image features to 3D poses is non-trivial, we design a proposal mechanism based on the missing data u that represents the image positions of body joints (head, elbows, etc.). The observation Y is conditionally dependent on u only, i.e. p(Y|u,x) = p(Y|u). In addition, u can be computed from x by a deterministic function u=f(x) which is a many-to-one mapping. We can decompose u into its components u={u i }, where u i is the image position of the i th body joint, and use local observations, Y i ⊂ Y, to generate the proposal distribution for this joint, denoted by</p><formula xml:id="formula_7">) , | ( 1 - t i i i q x Y u</formula><p>. The function f(.) can be decomposed into its components f(.)={f i (.)} such that u i = f i (x). (See Figure <ref type="figure" target="#fig_0">1</ref> for graphical models of these variables.) </p><formula xml:id="formula_8">(a)<label>(b) (c)</label></formula><formula xml:id="formula_9">∫ - - - ′ = ′ i i t i i i t i i i t i d q q q u Y x u Y x u x Y x x ) , | ( ) , , | ( ) , | ( 1 1 1 . (5)</formula><p>The sampling of the proposal distribution is simplified in two ways. First, we construct the proposal for u i so that it is independent of the previous sample x t-1 :</p><formula xml:id="formula_10">) | ( ) , | ( 1 i i i i t i i q q Y u Y x u = - .<label>(6)</label></formula><p>Second, we construct the proposal</p><formula xml:id="formula_11">) , , | ( 1 i t i i q Y x u x - ′</formula><p>as a deterministic function. Given the previous state x t-1 and a sample i u′ , a candidate x′ is computed easily using direct inverse kinematics (IK) so that the image position of the i th body joint is shifted to i u′ , while all the other body joints are left unchanged: <ref type="bibr" target="#b6">(7)</ref> When multiple solutions exist, due to depth ambiguity, we choose the solution x′ with the smallest change in depth. Due to space limitation, we will not discuss the inverse kinematics in details. Instead, readers may refer to <ref type="bibr" target="#b9">[10]</ref> for a discussion. Denoting the IK computation as a function g(.), we have:</p><formula xml:id="formula_12">f(x) f i (x) Y x Y x u Y z x u i Y i u n(i) z Y n(i) 0-7695-2158-4/04 $20.00 (C) 2004 IEEE    = ′ ≠ = - i j i j f f i t j j when when ) ( ) ' ( 1 u x x</formula><formula xml:id="formula_13">) , ( 1 i t g u x x - = ′ . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>The proposal distribution then becomes:</p><formula xml:id="formula_15">∫ - - - ′ = ′ i i i i i t i t i d q g q u Y u u x x Y x x ) | ( )) , ( ( ) , | ( 1 1 δ ,<label>(9)</label></formula><p>where δ(.) is the Dirac delta function. We can draw a sample for x′, by first drawing a sample</p><formula xml:id="formula_16">i u′ from ) | ( i i i q Y u</formula><p>, and computing x′ using Equation <ref type="bibr" target="#b7">(8)</ref>. At each Markov chain iteration, this step is repeated for different body joint, in a partitioning approach.</p><p>Sometimes there is no valid IK solution due to kinematics and no self-penetrating constraints. (For example, the proposed hand position might be too far from the elbow.) In this case, the function g(.) outputs an invalid state, denoted by x null , which has the property p(x null )=0. The proposal driven by i u′ is then rejected according to Equation (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposal Maps</head><p>This section discusses the proposal mechanism for drawing a sample</p><formula xml:id="formula_17">) | ( ~i i i i q Y u u</formula><p>, where Y i represents the observation of the i th body joint. The observation usually includes false alarms and generates multiple weighted hypotheses on the image position of the i th joint. We express Y i as the set of hypotheses:</p><formula xml:id="formula_18">} ,..., 1 ; , { , , i k i k i i n k w = = Y Y ,<label>(10)</label></formula><p>where k i, Y represents each hypothesis, k i w , its confidence, and i n is the number of hypotheses. The inference of each hypothesis is approximated by a Gaussian distribution with mean k i, µ and covariance matrix k i, Σ corresponding to the measurement uncertainty. The proposal distribution for the joint image position derived from each hypothesis is given by:</p><formula xml:id="formula_19">) , , ( ) | ( , , , k i k i i k i i N q Σ ∝ µ u Y u ,<label>(11)</label></formula><p>and the contributions of all the hypotheses are combined as follows:</p><formula xml:id="formula_20">)} | ( { max ) | ( , , k i i k i k i i q w q Y u Y u × ∝ . (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>The hypotheses are, in general, not independent. For example, matching the head-shoulders contour to image edges usually results in multiple local minima. In addition, the hypotheses are generated using different types of image cues and this leads to redundancy. We therefore use the max function in Equation ( <ref type="formula" target="#formula_20">12</ref>) instead of the summation to avoid exaggerated dominant peaks.</p><p>We present in this section a method for improving sampling efficiency. The proposal distribution is approximated by a grid space representation called the proposal map, with samples corresponding to every pixel position. The proposal distribution is thus correctly bounded within the image. As this distribution is unchanged during the MCMC process, it is computed beforehand. Ignoring the quantization noise (which is small compared to the measurement errors), this proposal is reversible: for any valid proposal jump, there is another valid reverse jump because the function g(.) in Equation ( <ref type="formula" target="#formula_13">8</ref>) has a one-to-one mapping.</p><p>In Figure <ref type="figure">2</ref> we show the grey level representation of the proposal maps for various body joints. They are generated from image observations that we will describe in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Proposal Mechanisms</head><p>The Markov chain dynamic consists of three types of proposals: (i) data-driven proposal, which was described earlier, (ii) random-walk sampler, and (iii) flip kinematics jump. The last two are briefly described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random-walk Sampler.</head><p>This process serves as a local optimizer and the corresponding proposal distribution is given by:</p><formula xml:id="formula_22">) , 0 , ( ) | ( 1 1 diffusion t t N q Σ - ′ ∝ ′ - - x x x x . (<label>13</label></formula><formula xml:id="formula_23">)</formula><p>Flip Kinematic Jump. This dynamic involves flipping a body part (i.e. head, hand, lower arm, entire arm, lower leg, or entire leg) along the depth direction, around its pivotal joint <ref type="bibr" target="#b9">[10]</ref>. Flip dynamic is balanced so that forward and backward flips have the same proposal probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Generative Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Human Model</head><p>The human model is an explicit representation of the human body structure. It defines the pose parameters as well as the parameters for shape and clothing.</p><p>Human Kinematics Model. This model represents the articulated structure of the human body and has 31 degrees of freedom. The pose is described by a 6D vector g representing global position, scale, and orientation, and a 25D vector j representing the joint angles. We assumed an orthographic projection. The prior distributions of these parameters, denoted by p(g) and p(j), are learned from the training data. For simplicity, these distributions are approximated as Gaussians and the joint angles of non-neighboring body locations are assumed to be independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Shape Model.</head><p>Each human body part is represented by a truncated 3D cone and the shape of the human is represented by a vector s which has 23 parameters describing the relative lengths and widths of these 3D cones. PCA is used to reduce the shape space to 6 dimensions. The prior distribution p(s) is assumed to be Gaussian.</p><p>Clothing Model. This model describes the type of clothing the person is wearing and allows the prediction of whether skin is exposed so that skin blob features are correctly interpreted. As there are many clothing types, the modeling requires a trade-off between generality and simplicity. The clothing model has 3 parameters, c = [c 1 c 2 c 3 ]</p><p>T , representing the sleeve length, the hem length, and the socks length. For computation efficiency, these parameters are quantized into coarse discrete levels (5 levels for c 1 , and 10 levels for c 2 and c 3 ). The prior distribution, P(c), is learned from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prior Distribution</head><p>The parameters from the various components of the human model are combined into a complete state vector x, now consisting of four subsets:</p><formula xml:id="formula_24">x = { g, j, s, c } . (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>For simplicity, we assume that the subsets of parameters are independent and the prior distribution, denoted by p(x), is given by: p(x) ≈ p(g) p(j) p(s) P(c) .</p><p>This prior distribution is combined with the image likelihood function to form the posterior density function, which is used for evaluating samples and computing the acceptance probability for the Markov chain, as given by Equation ( <ref type="formula" target="#formula_5">4</ref>).</p><p>The following sub-section describes the image likelihood function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Likelihood Function</head><p>The image likelihood function p(Y|x) consists of two components, based on region and color respectively. This approach is motivated by the work in <ref type="bibr" target="#b14">[15]</ref> where similar likelihood measure is used for segmenting multiple persons in static image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region</head><formula xml:id="formula_27">N i,human = count pixels (u,v) ∈ { R i ∩ H x, } N i,background = count pixels (u,v) ∈ { R i ∩ x H } (16)</formula><p>We define a binary label, l i , for each region, so that</p><formula xml:id="formula_28">   ≥ = otherwise 0 if 1 , , background i human i i N N l . (<label>17</label></formula><formula xml:id="formula_29">)</formula><p>We count the number of incoherent pixels, denoted by N incoherent , given by:</p><formula xml:id="formula_30">( )( ) ∑ = - = region i i N i l human i l backgruond i incoherent N N N 1 ) 1 ( , , .<label>(18)</label></formula><p>The region-based likelihood function is defined by: ) exp( </p><formula xml:id="formula_31">incoherent region region N L λ - = ,<label>(19)</label></formula><formula xml:id="formula_32">∑ = = histogram N i i i b d B 1 b d, .<label>(21)</label></formula><p>The combined likelihood measure is given by:</p><formula xml:id="formula_33">color region L L p × = ) | ( x Y . (<label>22</label></formula><formula xml:id="formula_34">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Image Observations</head><p>Image observations are used to compute the proposal maps described in Section 3. These are local observations used to infer positions of various body joints, and they are weighted according to their saliency and joint probabilities. These observations are extracted in 4 stages: (i) face detection; (ii) head-shoulders contour matching; (iii) skin blobs detection; and (iv) ridges detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Face Detection</head><p>The AdaBoost cascade classification technique <ref type="bibr" target="#b12">[13]</ref> is used for detecting faces in the image. Each detected face provides a hypothesis of the head position. As the face constitutes the most reliable observation, the detected face is used to initiate the extraction of other image observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Head-Shoulders Contour Matching</head><p>An active shape model approach is used to detect the head and shoulders contour using a deformable shape model. The face observation is used to define a search region within which multiple candidates for the headshoulders contour are detected using a gradient descent search that aligns the shape model to image edges .</p><p>Each detected contour provides hypotheses on the positions of head, neck and shoulders (Figure <ref type="figure" target="#fig_2">3</ref>.b), using a joint probabilistic model of these variables. The edge matching error is used to adjust the confidence weight of each hypothesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Elliptical Skin Blob Detection</head><p>Skin color features provide important cues about the positions of the arms and sometimes the legs. Skin blobs are detected in four sub-stages: (i) the image is divided into regions using a color-based image segmentation; (ii) for each segmented region, the probability of skin is evaluated using a histogram-based skin color model; (iii) ellipses are fitted to the boundary of these regions to form skin ellipse candidates; and finally (iv) adjacent regions with high skin probabilities are merged to form larger regions and extract larger ellipses (see Figure <ref type="figure" target="#fig_2">3</ref>.c). The extracted skin ellipses are used for inferring the positions of limbs. Further details are given in <ref type="bibr" target="#b3">[4]</ref>. 0-7695-2158-4/04 $20.00 (C) 2004 IEEE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ridge Observations</head><p>In addition to skin color blobs, another type of observations useful for the segmentation of the limbs is based on ridges. If most of the limbs are clothed, especially the lower body, skin color blobs are less useful and ridge observations are more relied upon. The centers of the ridges provide hypotheses on the medial axis points of the limbs and therefore provide inference on the position of the legs. This approach is motivated by the work in <ref type="bibr" target="#b2">[3]</ref> where limbs are detected as rectangular segments.</p><p>In the following, we discuss two aspects of the observations: (i) the detection of ridges, and (ii) the computation of confidence weights for these observations. Detection of Ridges. We extract the medial axis points of image regions derived from color-based segmentation. Since the lower body limbs are usually not horizontal, the medial axis points are easily extracted by scanning each horizontal line in the image to find the centers of each region along the line. To overcome errors due to imperfect color segmentation, we first use an over-segmented image to find the first set of medial axis points. We then merge neighboring regions with similar color and extract additional medial axis points on the new regions. This method extracts many medial axis points efficiently.</p><p>Confidence weight. Each extracted point is weighted by a confidence measure based on the following criterions: (i) the likelihood of the point being on the medial axis of the leg, (ii) the likelihood of the region (to which the point belongs) being a subset of the leg, and (iii) the likelihood of the width of the region.</p><p>In order to compute these likelihoods, we need first a joint probability model, learned from training data, of the positions of the legs and the position of the torso. Estimates of the torso position are provided by headshoulder contour matching.</p><p>The confidence measures are used to prune out some of the medial axis points with low confidence (Figure <ref type="figure" target="#fig_2">3</ref>.d). We use the remaining points and the corresponding weights to generate the proposal maps for the knees and ankles, as described in Section 3.</p><p>Figure <ref type="figure">2</ref> shows examples of proposal maps for lower body joints. These distributions are generally more diffused, as the observations are less reliable. Because each observation could be associated to either side of the limbs, the maps for the left and right legs are similar to some extent. Nonetheless, the proposal maps do capture different regions of high densities to indicate plausible positions of legs and allow for proposal jumps to explore these regions during Markov chain iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Database and Ground Truth. We used a set of images representing various human activities on which we have generated ground truth by manually locating joint positions and estimating their relative depths. Among this set, we chose a subset for training (primarily for learning the prior distributions of model parameters), and the rest for testing the proposed method. This second subset is used for the experiments described in this section.</p><p>The experiments were conducted without any manual pre-processing such as background removal, scaling and centering of the person, or model initialization. At the start of the MCMC search, the human model was initialized in a standard upright pose in the center of the image.</p><p>Pose Estimation. Figure <ref type="figure" target="#fig_3">4</ref> shows the obtained 3D pose estimation on various images from the test set after 1000 Markov chain iterations. The estimated human model and its pose (the MAP solution) are projected onto the image and a 3D rendering from a sideward view is also shown to illustrate the depth estimation. Some small errors are observed and discussed in the figure caption; these are mostly due to the lack of image observable or features. The overall 3D pose estimation is good on this challenging set of images.</p><p>The estimated joint positions were compared with the ground truth data, and a RMS error was computed using all body joints. Since the depth has a higher uncertainty, we have computed two separate RMS errors: one for the 2D position and the other for the depth. We computed the average of these errors over all test images (average RMS error); the result (based on 20 images) is given in Table <ref type="table" target="#tab_1">1</ref>. As the posterior distribution is multi-modal, the MAP solution may be insufficient. As an alternative measure, we approximated the posterior distribution as a mixture model by clustering the samples using k-mean algorithm (we used k =20). (An alternative technique is the greedy "K-adventurers" algorithm <ref type="bibr" target="#b11">[12]</ref> which updates the mixture model after each iteration.). 0-7695-2158-4/04 $20.00 (C) 2004 IEEE The clusters were ranked by their sample sizes. Using the estimated cluster means, average RMS errors were computed based on Rank 5, 10, 15 criterions. (For example, Rank 5 result was obtained by finding the lowest error among the five highest ranked cluster means.) These results, presented in Table <ref type="table" target="#tab_1">1</ref>, show that the mixture model captures better estimates of the pose, especially the depth estimates. Good pose estimates are usually found within the 5 highest ranked components. To examine the spread of the RMS errors among test images, Figure <ref type="figure">5</ref> shows a histogram of these errors using Rank 5 results. Convergence Analysis. Figure <ref type="figure">6</ref> shows the RMS errors with respect to the MCMC iterations. The error for the 2D image position decreases rapidly from the start of the MCMC process; this is due largely to the observation-driven proposal dynamics. For the depth estimate, the kinematics flip dynamic is helpful for finding good depth estimates, but it requires a longer time for exploration. In the current implementation, 1000 iterations were considered and it took, on average, 8 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a data-driven MCMC framework for estimating 3D human pose in static images. Image observations of different cues provide inferences on the image positions of body joints. We introduce the use of proposal map as an effective mechanism to consolidate these inferences and generate 3D pose candidates for MCMC. As the results show, the technique is effective on a wide variety of images. The system currently has two main limitations. Firstly, the technique requires a good face detection algorithm. The face detection method used is reliable only for frontal faces. Secondly, the computational cost is still quite high, even with the use of data-driven proposal. As future work, we are exploring better techniques to detect non-frontal faces. In addition, we are designing techniques based on gradient-based diffusion and Gibbs sampling to improve the efficiency of the MCMC algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical Models: (a) shows the basic model between model parameters x and image observation Y, (b) introduces the missing data u, representing image positions of body joints, and z representing the depth, (c) shows relationship between the image position of the i th body part u i, and the local observation Y i , which is used to generate proposals. In this figure, u n(i) represents image positions of other body joints except the i th , and Y n(i) represents the corresponding local observations for these joints.Using a component-based proposal approach, the i th</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>where region λ (</head><label>λ</label><figDesc>=0.2) is a constant determined empirically with training data using a Poisson model. Color Likelihood. The likelihood measures the dissimilarity between the color distributions of the human region H x and the background region x H . Given the predicted region H x , and x H , we obtain the color distribution of human region d, and background region b. They are represented by normalized histograms with N histogram bins. The color likelihood is defined by: is a constant determined empirically and B d,b is the Bhattachayya coefficient measuring the similarity of two color distributions given by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image observations: (a) box indicates the detected face; (b) outline indicates one of the detected head-shoulders contour, ellipses indicate the corresponding hypotheses for head, left and right shoulders, and the ellipse size represents measurement uncertainty; (c) a grey-level map of skin probability with extracted skin ellipses; and (d) white pixels indicate ridges for the lower body.</figDesc><graphic coords="5,312.59,335.52,57.61,102.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Pose Estimation. 1st Row: Original images, 2nd row: estimated poses, 3rd row: side view. Due to space limitation, the images were cropped for display. Errors include: (1) the person's left lower leg in image A, as it is mostly hidden, (2) the left arm in B, where the elbow is highly bent, (3) the left foot in C which is dark and similar to background, (4) the feet in E is wrongly estimated to be tip-toed. In addition, there are errors in depth estimates such as the right elbow in B, the tilting of the torso in B, the left feet in C, and the right arm in D. 0-7695-2158-4/04 $20.00 (C) 2004 IEEE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Likelihood.Color-based segmentation is used to divide a given image into a set of regions denoted by N region }, where N region is the number of regions. For a given state candidate x, we predict the human body region in the image, denoted by H x . Ideally, this human region will coincide with the union of a certain subset of the segmented regions. In other words, each region R i should either belong to the human region H</figDesc><table><row><cell cols="4">x or to the background (non-human) region, denoted</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">by x H . This region likelihood function measures the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">degree of similarity between the human body and the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Image segmented regions. For each segmented region R i , we Head Neck</cell><cell>Hip</cell><cell>Left Shoulder</cell><cell>Right Shoulder</cell><cell>Left Elbow</cell></row><row><cell cols="4">count the number of pixels in R i , that belong to H x , and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>that belong to x H :</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Right Elbow</cell><cell>Left Hand</cell><cell>Right Hand</cell><cell cols="2">Left Knee</cell><cell>Right Knee</cell><cell>Left Ankle</cell><cell>Right Ankle</cell></row></table><note><p>Figure 2: Grey level representation of proposal maps for various body joints (overlaid on edge image for clarity). 0-7695-2158-4/04 $20.00 (C) 2004 IEEE {R i ; i = 1, ...,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average RMS errors in image position and depth, using MAP solutions and mixture model solution.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Average RMS Error (pixel)</cell></row><row><cell></cell><cell></cell><cell>(image position)</cell><cell>(depth)</cell></row><row><cell cols="2">MAP Solution</cell><cell>14.92</cell><cell>21.45</cell></row><row><cell>Mixture</cell><cell>Rank 5</cell><cell>12.44</cell><cell>15.04</cell></row><row><cell>Model</cell><cell>Rank 10</cell><cell>11.94</cell><cell>14.86</cell></row><row><cell>Solution</cell><cell>Rank 15</cell><cell>11.89</cell><cell>14.83</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was partially funded by the Integrated Media Systems Center, a National Science Foundation Engineering Research Center, under Cooperative Agreement No. EEC-9529152, and by the Advanced Research and Development Activity of the U.S. Government under contract: MDA-908-00-C-0036.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">People tracking with hybrid Monte Carlo</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Markov Chain Monte Carlo in Practice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic methods for finding people</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="68" />
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human Upper Body Pose Estimation in Static Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Estimating Human Body Configurations using Shape Context Matching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="666" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to parse pictures of people</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="700" to="714" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inferring body pose without tracking body parts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2000</title>
		<imprint>
			<biblScope unit="page" from="721" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast Pose Estimation with Parameter Sensitive Hashing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="750" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Covariance Scaled Sampling for Monocular 3D Body Tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kinematic Jump Processes for Monocular Human Tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2003</title>
		<imprint>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="363" />
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image Segmentation by Data-Driven Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="657" to="672" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating bottom-up/top-down for object recognition by data driven Markov chain Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2000</title>
		<imprint>
			<biblScope unit="page" from="738" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian Human Segmentation in Crowded Situations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2003</title>
		<imprint>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
