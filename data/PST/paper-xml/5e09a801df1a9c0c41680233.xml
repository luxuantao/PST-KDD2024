<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Jointly Generate and Separate Reflections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daiqian</forename><surname>Ma</surname></persName>
							<email>madaiqian@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">The National Engineering Lab for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
							<email>rjwan@ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">The Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
							<email>shiboxin@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
							<email>eackot@ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">The Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">The National Engineering Lab for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">The SECE of Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Jointly Generate and Separate Reflections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing learning-based single image reflection removal methods using paired training data have limitations about the generalization capability of dealing with real-world reflections due to the limited variations in training pairs. In this work, we propose to jointly generate and separate reflections within a weakly-supervised learning framework, aiming to model the reflection image formation more comprehensively with abundant unpaired supervision. By imposing the entanglement and disentanglement mechanisms, the proposed framework elegantly integrates two independent stages of reflection generation and separation into a unified model. For better performance, the image gradient constraint is incorporated into the concurrent training process of the multi-task learning as well. In particular, we built up an unpaired reflection dataset with 4,027 images, which is useful for investigating the problem of reflection removal in the weakly supervised learning manner, and further improving model performance. Extensive experiments on a public benchmark dataset show that our framework performs favorably against state-of-the-art methods and consistently produces visually appealing results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When taking photos through a piece of transparent glass, the presence of reflections accompanied with the background image is undesirable. In addition to the visual quality degradation, the reflections hinder the performance of computer vision systems by obstructing and deforming the background scene behind the glass. The classical representation for image formation with reflections is formulated as, where M, B, and R represent observed mixture images with reflections, background, and reflection images, respectively. Here, α and β are the mixing coefficients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id="formula_0">M = αB + βR,<label>(1)</label></formula><p>Reflection removal aims at removing the reflections R from M, such that the visibility of the background scenes B is enhanced. In this scenario, image priors such as different blur levels between the background and reflection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>, ghosting effects <ref type="bibr" target="#b21">[22]</ref>, and the non-local similarity in the images <ref type="bibr" target="#b24">[25]</ref>, have been explored. However, these low-level image priors are constrained by limited phenomena causing reflections, which may often be impractical in real-world applications. Moreover, these methods mainly rely on the linear additive formulation in Equation <ref type="formula" target="#formula_0">1</ref>to model the relationship between the mixture image, background, and reflections, which may not well reflect the real interactions.</p><p>In practical scenarios, the appearance of real-world reflection is quite complicated, as it is influenced by the interactions of various factors and much beyond the straightforward linear combination. For example, either non-uniform lighting conditions <ref type="bibr" target="#b11">[12]</ref> or the non-flat surface of glass <ref type="bibr" target="#b26">[27]</ref> may make Equation 1 invalid. As such, a general image formation with reflections is given by,</p><formula xml:id="formula_1">M = G(B, R),<label>(2)</label></formula><p>where G(•, •) is the mapping function to generate a mixture image. It's not trivial to learn this function accurately. Recently, deep learning based reflection removal methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref> with better generalization ability have been proposed to address the limitations arising from the handcrafted image priors. However, most existing methods work in a supervised manner, which requires abundant paired training data, i.e., in the form of a triplet of {M, B, R} containing perfectly registered images from the same scene. The recently proposed benchmark dataset <ref type="bibr" target="#b22">[23]</ref> is an example. Due to the high cost in capturing the real-world paired data, synthetic mixture images are often applied in accordance with the traditional representation in Equation 1, as shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>. Clearly, such a strategy ignores various factors in real world image formation process. In particular, the phases of image generation and separation are dealt with as two independent stages, which would degenerate the performance of models by improperly handling the mutual effects of two phases in training models.</p><p>In contrast with previous methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>, that heavily rely on the simplified model in Equation 1 and regard the image generation and separation as two independent stages, the proposed model leverages the mutual benefits of the image generation and separation in a joint learning manner to improve the robustness. It is worthy to note that traditional cycle-consistent network, like CycleGAN <ref type="bibr" target="#b29">[30]</ref>, cannot be directly applied to reflection removal, as its original setup of one-to-one mapping problem is less comprehensive for modeling the process of reflection generation. Accordingly, we propose to incorporate the entanglement and disentanglement mapping mechanisms between the mixture images and the associated background as well as reflection, which may contribute to more realistic generation results and clearer separation results. Moreover, we introduce the gradient constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> to make the model learning more effective, in which the edge map estimation is elegantly dealt with as an auxiliary task via a multi-task learning structure. We summarize the main contributions as follows:</p><p>• We propose to model the real world reflection image formation within a weakly supervised learning framework. Through jointly learning the process of generating and separating reflections, we have achieved encouraging reflection removal performance by leveraging abundant but lower cost unpaired supervision.  classified into two categories. The first category addresses this problem based on hand-crafted priors without learning. Due to the ill-posed nature, different priors have been employed to exploit the properties of the background and reflection layers, including the sparsity prior <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, the blur level differences between the background and reflection layer <ref type="bibr" target="#b16">[17]</ref>, the ghosting effects <ref type="bibr" target="#b21">[22]</ref> and the Laplacian data fidelity term <ref type="bibr" target="#b0">[1]</ref>. Other methods in this area remove reflections by virtue of multiple images. By exploiting different image correlation cues <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, the modelling based methods using the multiple images show more reliable results. However, the requirements for specific capturing conditions hinder such methods for practical use, especially for mobile devices or images downloaded from the Internet.</p><p>Another category attempts to address this problem in a data-driven learning manner. The comprehensive modeling ability of deep learning has benefited the reflection removal problems and shown very promising results. For example, Chandramouli et al. <ref type="bibr" target="#b3">[4]</ref> proposed a two-stage deep learning approach to learn the edge features of the reflections with the light field camera. The framework introduced in <ref type="bibr" target="#b4">[5]</ref> exploited the edge information when training the whole network to better preserve the image details. Though the deep learning based methods can better capture the image properties, the conventional two-stage framework ignores the intrinsic correlations, which largely limits their performances.</p><p>Generative Adversarial Networks (GAN). GAN <ref type="bibr" target="#b6">[7]</ref> has become one of the most successful approaches for imageto-image translation problems. In GANs, two networks are adversarially trained simultaneously, where the discrimina-tor is updated to distinguish the real samples from the output of the generator, and the generator is updated to generate fake data to fool the discriminator. For instance, pix2pix GAN <ref type="bibr" target="#b8">[9]</ref> learns the translation task in a supervised manner using cGANs <ref type="bibr" target="#b19">[20]</ref>. To alleviate the problem of obtaining data pairs, unpaired image-to-image translation frameworks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. UNIT <ref type="bibr" target="#b17">[18]</ref> combines variational autoencoders (VAEs) <ref type="bibr" target="#b10">[11]</ref> with CoGAN <ref type="bibr" target="#b18">[19]</ref>, a GAN framework where two generators share weights to learn the joint distribution of images in cross domains.</p><p>It is worthy to mention that some existing mature frameworks like CycleGAN <ref type="bibr" target="#b29">[30]</ref> and DiscoGAN <ref type="bibr" target="#b9">[10]</ref> are limited in handling reflection removal problem. They are only capable of learning the relationship between two different domains at a time, in which key attributes between the input and the translated images are preserved by utilizing a cycle consistency loss. Undoubtedly, the background is untransferable to the mixture image without the reflection. Unlike the aforementioned approaches, our specifically designed framework for reflection removal attempts to learn the mapping functions amongst three domains, including reflection, background and the mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unpaired Reflection Removal Dataset</head><p>In principle, the traditional triplet of {M, B, R} (mixture image, background image and reflection image) can be captured in a "remove-and-occlude" manner <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>: 1) Taking a photo of the mixture image through the glass; 2) capturing an image of the background scenes by removing the glass; and 3) capturing a reflection image by putting a black sheet of the paper behind the glass. However, the perfectly registered triplet sets with "remove-and-occlude" approach is quite time-consuming, which provides limited scalability when much more ground truth data is required by model training. Thanks to the capability of the proposed weakly supervised training framework, paired pixelwise correspondence is not required when collecting image dataset. So we capture 4000+ images, which allows for a much larger scale than those used in existing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. Finally, we build a dataset containing 4027 images under various scenes, and example images are shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The proposed dataset enriches the diversity and generality over existing datasets in the following aspects:</p><p>• Devices. Besides using the high-end devices (e.g., the DSLR camera with fully manual control model) like previous methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>, we also use the cameras on different types of mobile phones (iphone 8, iphone X, etc.) to capture images. • Illuminations. We capture images under different illumination conditions. More specifically, we capture reflection images in both cloudy and sunny days, at different time of the day (e.g., morning, afternoon, and night) and indoor scenes with different lighting conditions (e.g., office, living room, etc.). • Scene. Our images cover a variety of scenes, e.g., the campus, streets, parks, gardens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>In this section, we first discuss the motivation and the network architecture, following which the loss functions are introduced. Finally, the training strategy is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Framework of the Proposed Scheme</head><p>In contrast to the conventional pipelines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref> that treat the image generation and separation as two independent stages, we come up with a unified model, such that the mutual effects between two stages can benefit the robustness. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, our model contains a generator network to generate the mixture images, a separator network to separate the mixture image into background and reflection, and three discriminator networks to produce the adversarial losses. Existing method <ref type="bibr" target="#b4">[5]</ref> can be treated as a special instance of our method when the generator is simplified as a linear function.</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>, our framework involves two cycles of the generator and separator. In particular, each cycle serves as a two-step conversion to convert the generated image back to the original image. There are three reconstruction losses in these two cycles, aiming to incorporate the cycle-consistent constraints to guide the training procedure. Moreover, in contrast with the classical cycle-consistent  model with the one-to-one framework <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10]</ref>, we propose a joint mapping mechanism based on the additive relationship between the mixture image, the background, and the reflection, such that the whole procedure can be modelled in a better way. The details of our proposed framework are described as follows.</p><p>Generator (G). We propose to design an entanglement mechanism in the generator with a general formulation to derive more natural mixture image from the background and reflection.</p><p>In general image-to-image translation tasks, generators are mainly designed for the one-to-one mapping conversion. Due to the very nature of the reflection removal problem that the mixture image is a composite of the background and reflection, the traditional one-to-one mapping cannot be directly used to translate the background into the mixture images due to the lack of reflection. Instead of the one-toone mapping in previous methods, our generator learn the mapping as G : (B, R) → M, where the non-linear mappings can produces more realistic reflection appearances (first to third columns in Figure <ref type="figure" target="#fig_3">4</ref> 1 ) than previous linear functions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref> with fixed coefficients.</p><p>Separator (S). We perform a disentanglement in the separator for the mixture images by leveraging multi-task learning to estimate the background, reflection, and the background edge map (E) concurrently. Instead of one-toone framework in previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, our separator learns the mapping function as S : M → (B, R, E), where the multi-task learning framework models the image separation process in a more reasonable way, especially the auxiliary task of edge map estimation, that provides useful information to make the separator more efficient.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>, compared with the edge map extracted with Sobel operator, our proposed separator successfully removes the gradient information from the reflection and retains the edge map related with the background.</p><p>Network architecture. The generator and separator exhibit similar structures: a downsampling unit with two convolutional layers to increase the receptive field size, a feature extraction unit with 9 residual blocks <ref type="bibr" target="#b7">[8]</ref> to extract robust features and an upsampling unit at the last stage with two transposed convolutional layers. More specifically, the generator contains two downsampling units to receive the inputs of background and reflection, and the multi-task learning mechanism with three upsampling units (relative to three tasks) is employed in the separator to improve the reflection removal ability. For the discriminator networks, we use 70 × 70 PatchGANs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> which can be applied to arbitrarily-sized images in a fully convolutional fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss Functions</head><p>The learning of these two mappings are guided by the adversarial losses and reconstruction losses, with training samples B = {b i } K i=1 for the background, R = {r i } N i=1 for the reflection and M = {m i } L i=1 for the mixture.</p><p>Adversarial loss. Adversarial loss has been widely used in the image-to-image translation problems. Here, regarding the mapping function G : (B, R) → M and its discriminator D M , the objective is given by,</p><formula xml:id="formula_2">L m adv =E m∼p data (m) [log D M (m)]+ E b,r∼p data (b,r) [log(1 − D M (G(b, r)))],<label>(3)</label></formula><p>where G tries to generate images G(b, r) conditioned on both the background and reflection images, while D M aims 1 More examples are listed in the supplementary material.</p><p>to distinguish between the generated fake mixture image G(b, r) and real-world mixture image m. In other words, G aims to minimize this objective against an adversary D that tries to maximize it. Then we introduce two similar adversarial losses for the mapping function S : M → (B, R, E) and their discriminators D B and D R : L b adv and L r adv .</p><p>Reconstruction loss. We employ a reconstruction loss on the pixel and content domain to better preserve both the pixel level and feature level similarity. Though the minimization of the adversarial loss in Equation 3 can generate images with similar distributions towards the target domain, it does not guarantee that generated images preserve the content of its input images with only the regions covered by reflections changed. To alleviate this problem, we first adopt the pixel reconstruction loss for the generated mixture image L m prec to preserve the consistency in the pixel domain as follows:</p><formula xml:id="formula_3">L m prec = E m∼p data (m) [||m − G(S(m))|| 1 ],<label>(4)</label></formula><p>where G takes the separated b and r from S(m) and tries to reconstruct the original image m. The pixel reconstruction losses for the background and reflection (L b prec and L r prec ) are with similar scheme but in an inverted order of G and S.</p><p>On the other hand, the content reconstruction loss aims to constrain the whole procedure in a high-level feature space. The content reconstruction loss for the generated mixture image L m crec is defined as:</p><formula xml:id="formula_4">L m crec = E m∼p data (m) [ 1 W H ||φ(m) − φ(G(S(m)))|| 2 ],<label>(5)</label></formula><p>where φ is the feature map from the relu4 3 layer of a pretrained VGG-16 network, W and H indicate the dimensions of the relu4 3 layer. The content reconstruction losses for the background and reflection (L b crec and L r crec ) are defined in a similar fashion but in an inverted order of G and S.</p><p>Full objective. Finally, the objective functions to optimize G and S are written as:</p><formula xml:id="formula_5">L G =L m adv + λ p L m prec + λ c L m crec , L S =L b adv + λ p L b prec + λ c L b crec +λ r (L r adv + λ p L r prec + λ c L r crec ) + λ e L e ,<label>(6)</label></formula><p>where L e is the L 1 loss for the edge map estimation, λ r and λ e aim to balance the main and auxiliary tasks in the separator; and λ p and λ c aim to balance the pixel reconstruction loss and content reconstruction loss.</p><p>As such, we aim to solve:</p><formula xml:id="formula_6">G * , S * = arg min G,S max D M ,D R ,D B L G + L S .<label>(7)</label></formula><p>In the experiments, λ p , λ c , λ r and λ e are empirically set as 10, 2, 0.5 and 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Ground truth CycleGAN FY17 Zhang18 Wan18 Ours Figure <ref type="figure">6</ref>. Examples of the reflection removal results on four wild scenes in the SIR<ref type="foot" target="#foot_0">2</ref> dataset. The comparison methods include Wan18 <ref type="bibr" target="#b23">[24]</ref>, Zhang18 <ref type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, and FY17 <ref type="bibr" target="#b4">[5]</ref>. The yellow boxes highlight some noticeable differences.</p><p>Table <ref type="table">1</ref>. Quantitative evaluation results on SIR 2 with the state-ofthe-arts methods using three different error metrics.</p><p>SSIM r SSIM PSNR(dB)</p><p>LB14 <ref type="bibr" target="#b16">[17]</ref> 0.801 0.829 21.77 WS16 <ref type="bibr" target="#b25">[26]</ref> 0.833 0.877 22.39 NR17 <ref type="bibr" target="#b0">[1]</ref> 0.832 0.882 23.70 FY17 <ref type="bibr" target="#b4">[5]</ref> 0.820 0.871 22.51 CycleGAN <ref type="bibr" target="#b29">[30]</ref> 0.794 0.813 20.10 Zhang18 <ref type="bibr" target="#b28">[29]</ref> 0.842 0.885 24.01 Wan18 <ref type="bibr" target="#b23">[24]</ref> 0.854 0.891 24.08 Eq. ( <ref type="formula" target="#formula_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Strategy</head><p>The model is implemented with PyTorch 2 . To inject scale-invariance to the network <ref type="bibr" target="#b20">[21]</ref>, we adopt a multi-size training strategy by feeding images of two sizes: coarse scale 336 × 252 and fine scale 224 × 168. The learning rate is set to 2 × 10 −4 for the first 100 epochs and we linearly decay it to 0 over the next 100 epochs. We also augment the training data with three different operations: image translation, flipping and cropping. The sizes of mini-batch and momentum are set to 4 and 0.9, respectively. Figure <ref type="figure">7</ref>. Perceptual study results on the whole SIR 2 dataset for the three best methods (Zhang18 <ref type="bibr" target="#b28">[29]</ref>, Wan18 <ref type="bibr" target="#b23">[24]</ref>, and ours) in terms of the quantitative scores in Table <ref type="table">1</ref>. The stastistics are obtained by collecting the ranking results from 30 participants and 100 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To verify the effectiveness of our proposed method, we perform several experiments on the SIR 2 [23] benchmark dataset with state-of-the-art reflection removal methods. All results are evaluated in terms of both quantitative scores and visual quality. Due to the regional properties of the reflection <ref type="bibr" target="#b22">[23]</ref>, we also adopt SSIMr <ref type="bibr" target="#b20">[21]</ref> to assess the quality by focusing on local reflections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with State-of-the-art Methods</head><p>The proposed method is compared with seven state-ofthe-art single image reflection removal methods, including Wan18 <ref type="bibr" target="#b23">[24]</ref>, Zhang18 <ref type="bibr" target="#b28">[29]</ref>, CycleGAN <ref type="bibr" target="#b29">[30]</ref>, FY17 <ref type="bibr" target="#b4">[5]</ref>, NR17 <ref type="bibr" target="#b0">[1]</ref>, WS16 <ref type="bibr" target="#b25">[26]</ref>, and LB14 <ref type="bibr" target="#b16">[17]</ref>. For a fair comparison, we use the codes provided by their authors and set the parameters as suggested in their original papers, and we follow the same training protocol to retrain their networks using our dataset.</p><p>Quantitative Comparison. The comparisons with seven state-of-the-art methods are performed with three different error metrics. The results are summarized in Table <ref type="table">1</ref>, where the numbers displayed are the mean values over all 100 sets of wild images in the SIR 2 <ref type="bibr" target="#b22">[23]</ref> dataset. In particular, Ours + Eq. ( <ref type="formula" target="#formula_0">1</ref>) means that we set a random variable and use the data with probability 0.7 from our generator and probability 0.3 from the Equation 1 to train the separator. Though the left three columns in Figure <ref type="figure" target="#fig_3">4</ref> show that our generator better preserves the backgrounds while highlighting the reflection part, the performance of the generator may drop due to the limited number of our training dataset (see the fourth column in Figure <ref type="figure" target="#fig_3">4</ref>). Thus, to increase the stability, we propose to incorporate Equation 1 into the design of our whole framework. As shown in Table <ref type="table">1</ref>, our proposed model obviously outperforms other methods in terms of both PSNR and SSIM. The higher objective quality values indicate that our method recovers the background images with better fidelity. Note that almost all images in the SIR 2 dataset are partially reflected images, such that the global changes are small between the recovered background and the original mixture images. To deal with the limitations of global error metrics, we manually label the reflection dominant regions and evaluate the SSIM values in these regions analogously to the evaluation method proposed in <ref type="bibr" target="#b20">[21]</ref>. As a result, higher SSIM r results have been obtained as shown in Table <ref type="table">1</ref>, indicating that the proposed method can remove strong reflections more effectively in the regions overlaid with reflections than the state-of-the-art methods.</p><p>Note that our framework is inspired by the fact that the mixture images are complicated combinations of reflection and background images in a generative process, and our target is to explicitly model this mechanism in a weakly supervised manner. As shown in Figure <ref type="figure">6</ref> and Table <ref type="table">1</ref>, Cy-cleGAN shows poor performance in the reflection removal task, because it is rather difficult for CycleGAN to learn the mapping functions between the reflection-contaminated images and reflection-free images directly. Reflection-Removal Perceptual Study. Recent research <ref type="bibr" target="#b2">[3]</ref> pointed out that PSNR and SSIM may not exactly tell the perceptual visual quality. Since there is no suitable error metric specifically developed for the reflection removal task, we conduct a reflection-removal perceptual study and invite 30 subjects to evaluate the quality of 100 images from the SIR 2 dataset. In particular, we focus on the top three methods reported in Table <ref type="table">1</ref> for this perceptual study with the following procedures:</p><p>• The participants are well trained with the common reflection images to gain a general sense on this task. • Each participant is requested to view four images at a time, with the leftmost image showing the input reflection-contaminated image followed by three reflection-removed images generated by differ-  </p><formula xml:id="formula_7">φ k = 1 N i j (N − rank i,j,k + 1)</formula><p>, where N is the total number of evaluated methods and i, j, k indicate the i-th participant, j-th group of images and k-th method, respectively. The results in Figure <ref type="figure">7</ref> show that the rank-1 number of our method is even higher than the sum of the rest two methods and the rank-3 number of our method is obviously smaller, which demonstrates the superior perceptual quality of our method. Moreover, from the result in Figure <ref type="figure">6</ref>, we can find that our method removes the reflections more effectively and recovers the details of the background images more clearly. It should be noted that in the third row, our method is able to remove the reflection on the right vending machine, which is even clearer than the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Loss Ablation</head><p>Besides the basic cycle consistency with pixel construction loss, we further apply the content reconstruction loss and edge map loss to improve the performance. To analyze how these two loss functions contribute to the final performance, we remove the relative loss in the final objective function and re-train the network. The results are shown in Figure <ref type="figure" target="#fig_5">8</ref> and Table <ref type="table" target="#tab_2">2</ref>. Without the edge map loss, we notice that visible content of the reflection image appears in the background prediction. Moreover, the content reconstruction loss helps to recover cleaner and more natural results (the characters shown in second row). These results demonstrate the necessity in introducing these loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Efficiency Analysis</head><p>To evaluate the efficiency, we record the average execution time of an image with size 224 × 288 on a single Titan XP GPU, though these methods are implemented on different deep learning frameworks. The results are shown in Table <ref type="table" target="#tab_3">3</ref>. In particular, the SSIM-guided loss proposed by Wan18 <ref type="bibr" target="#b23">[24]</ref> performs well while our method is much more efficient (15 times faster) and achieves higher PSNR value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Discussions</head><p>In this paper, we propose a novel approach to jointly generate and separate reflections. Based on the public dataset SIR 2 <ref type="bibr" target="#b22">[23]</ref> and the proposed real-world dataset, our method outperforms state-of-the-art methods in terms of both the quantitative and subjective quality.</p><p>There remain several open issues for the future work. In some extreme cases like Figure <ref type="figure" target="#fig_6">9</ref>, the whole image can be dominated by the reflection, our method cannot remove the reflection completely and the estimated background still remains with some visible residual edges. However, even in this challenging case, our method still removes the majority of reflections and restores the background details, which performs better than other state-of-the-art methods. Moreover, when testing the models across datasets with different collecting protocols (e.g., the dataset of SIR 2 <ref type="bibr" target="#b22">[23]</ref> and the dataset of Zhang18 <ref type="bibr" target="#b28">[29]</ref>), we have observed that the dataset gap problem is worth further investigating to achieve consistently good performance on diverse real-world scenes. Meanwhile, the proposed framework can be further extended in various ways to facilitate other image restoration tasks (e.g., derain, dehaze, deshadow, etc.), which leaves more space for future exploration as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of two different training pipelines. S and G denote the separator and generator in our framework. M, B, and R represent the mixture image, background and reflection, respectively. * represents the generated images in the training procedure.</figDesc><graphic url="image-1.png" coords="1,308.51,255.23,237.02,240.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Examples of our collected unpaired reflection removal dataset (under various scenes and illumination conditions). The mixture images (M), background images (B), and reflection images (R) are shown from left to right.</figDesc><graphic url="image-2.png" coords="2,49.55,71.39,389.90,168.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our framework contains two mapping functions G : (B, R) → M and S : M → (B, R, E), where M, B, and R represent the real-world mixture image, background, and reflection, respectively. * represents the generated images in the training procedure. We introduce three reconstruction losses with two cycles (a) and (b). The reconstruction loss for cycle (a) is formulated as S(G(B, R)) ≈ (B, R), and the reconstruction loss for the mixture image from cycle (b) is formulated as G(S(M)) ≈ M. E (the ground truth edge map is calculated by Sobel operator) is an auxiliary edge map estimation task with L1 loss and the generation of the intermediate images in these two cycles are guided by the adversarial loss L adv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of mixture images with different generation methods. The references are the real-world mixture images with similar reflection properties. Please note the similarity of our generated reflections to the captured reference images in the yellow boxes.</figDesc><graphic url="image-30.png" coords="4,308.39,237.71,203.30,100.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The estimated edge map compared with the ground truth (GT) edge map.</figDesc><graphic url="image-31.png" coords="4,308.63,404.87,236.66,55.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visual quality comparisons on the ablation study of edge map loss and content reconstruction loss based on the SIR 2 dataset.</figDesc><graphic url="image-61.png" coords="7,49.55,71.39,354.50,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Illustration of an extreme example with the real-world vitrine, compared with Zhang18 [29] and Wan18 [24].</figDesc><graphic url="image-63.png" coords="8,49.79,71.63,495.62,98.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Numerical comparisons regarding the ablation study of the edge map loss and content reconstruction loss based on the SIR 2 dataset.</figDesc><table><row><cell></cell><cell cols="2">SSIM r SSIM PSNR(dB)</cell></row><row><cell>w/o L e</cell><cell>0.826 0.873</cell><cell>23.52</cell></row><row><cell>w/o L prec</cell><cell>0.845 0.886</cell><cell>24.31</cell></row><row><cell cols="2">Complete model 0.858 0.892</cell><cell>24.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Efficiency comparisons with FY17<ref type="bibr" target="#b4">[5]</ref>, Zhang18<ref type="bibr" target="#b28">[29]</ref> and Wan18<ref type="bibr" target="#b23">[24]</ref> of an image with size 224 × 288 on a single Titan XP GPU.</figDesc><table><row><cell></cell><cell cols="2">Framework Time (s)</cell></row><row><cell>FY17 [5]</cell><cell>Torch</cell><cell>0.0705</cell></row><row><cell cols="3">Zhang18 [29] Tensorflow 0.0438</cell></row><row><cell>Wan18 [24]</cell><cell>PyTorch</cell><cell>0.3488</cell></row><row><cell>Ours [29]</cell><cell>PyTorch</cell><cell>0.0214</cell></row><row><cell cols="3">ent methods displayed in a random order. They rank</cell></row><row><cell cols="3">the reflection removal quality without any time con-</cell></row><row><cell cols="3">straint. This test is performed for 100 groups.</cell></row><row><cell cols="3">• The average score φ for a method is calculated from</cell></row><row><cell>the ranking as</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">http://pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported by the National Natural Science Foundation of China under Grant 61661146005, Grant U1611461, and 61872012, in part by the Shenzhen Municipal Science and Technology Program under Grant JCYJ20170818141146428, and in part by the National Research Foundation, Prime Minister's Office, Singapore, through the NRF-NSFC Grant, under Grant NRF2016NRF-NSFC001-098. Renjie Wan is supported by the Microsoft Cloud Research Software Fellowships (CRSF) program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image reflection suppression</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Arvanitopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind separation of superimposed shifted images using parameterized joint diagonalization</title>
		<author>
			<persName><forename type="first">Erfrat</forename><surname>Be</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Ery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Yeredor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="353" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convnet-based depth estimation, reflection separation and deblurring of plenoptic images</title>
		<author>
			<persName><forename type="first">Paramanand</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
				<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A generic deep architecture for single image reflection removal and image smoothing</title>
		<author>
			<persName><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blind separation of superimposed moving images using image statistics</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="32" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A physically-based approach to reflection separation: from physical modeling to constrained optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative single image reflection separation</title>
		<author>
			<persName><forename type="first">Donghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04102</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deshadownet: A multi-context embedding deep network for shadow removal</title>
		<author>
			<persName><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryson</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reflection removal using ghosting cues</title>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Benchmarking single-image reflection removal algorithms</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CRRN: Multi-scale guided concurrent reflection removal network</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparsity based reflection removal using external patch search</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
				<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth of field guided reflection removal</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Hwee Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
				<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Separating reflection and transmission images in the wild</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computational approach for obstruction-free photography</title>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName><forename type="first">Xuaner</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
