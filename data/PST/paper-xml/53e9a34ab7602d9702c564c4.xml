<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affine-Invariant Local Descriptors and Neighborhood Statistics for Texture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@uiuc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<email>cordelia.schmid@inrialpes.fr</email>
						</author>
						<author>
							<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<email>ponce@cs.uiuc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Beckman Institute Inria Rhône-Alpes</orgName>
								<orgName type="institution">Beckman Institute University of Illinois</orgName>
								<address>
									<settlement>Urbana, Montbonnot</settlement>
									<country>USA, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affine-Invariant Local Descriptors and Neighborhood Statistics for Texture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E19A1C9375A6309990074B1CE387A990</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a framework for texture recognition based on local affine-invariant descriptors and their spatial layout. At modeling time, a generative model of local descriptors is learned from sample images using the EM algorithm. The EM framework allows the incorporation of unsegmented multi-texture images into the training set. The second modeling step consists of gathering co-occurrence statistics of neighboring descriptors. At recognition time, initial probabilities computed from the generative model are refined using a relaxation step that incorporates cooccurrence statistics. Performance is evaluated on images of an indoor scene and pictures of wild animals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Texture representations that are invariant to a wide range of geometric and photometric transformations are desirable for many applications, including wide-baseline matching <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, texture-based retrieval in image databases <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, segmentation of natural scenes <ref type="bibr" target="#b6">[7]</ref>, recognition of materials <ref type="bibr" target="#b15">[16]</ref>, and recognition of semantic texture categories, e.g., natural vs. man-made <ref type="bibr" target="#b2">[3]</ref>. In this paper, we investigate a texture representation that is invariant to any geometric transformations that can be locally approximated by an affine model, from perspective distortions to non-rigid deformations.</p><p>Recently, several affine-invariant region detectors have been developed for the applications of wide-baseline matching, indexing, and retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. As demonstrated in our earlier work <ref type="bibr">[4]</ref>, such detectors can also make effective texture analysis tools. In this paper, we use a texture representation based on a sparse set of affine-invariant regions to perform retrieval and segmentation of multi-texture images. This task is more challenging than the recognition of singletexture images: instead of comparing distributions of local features gathered over a large field, we are forced to classify each local feature individually. Since it is not always possible to unambiguously classify a small image region, we must augment the local representation with a description of the spatial relationship between neighoring regions. The systems developed by Malik et al. <ref type="bibr" target="#b6">[7]</ref> and Schmid <ref type="bibr" target="#b13">[14]</ref> are examples of this two-layer architecture, with intensity-based descriptors at the first level and histograms of texton distributions at the second.</p><p>This paper describes a conceptually similar two-stage approach to texture modeling. The first stage consists in estimating the distribution of local intensity descriptors. Unlike most existing methods, which use fixed-size windows to compute these descriptors, ours employs shape selection: the area over which the descriptors are computed is determined automatically using an affine adaptation process <ref type="bibr" target="#b4">[5]</ref>. We represent the distribution of descriptors in each class by a Gaussian mixture model where each component corresponds to a "sub-class". This generative model is used to assign the most likely sub-class label to each region extracted from a training image. At the second stage of the modeling process, co-occurrence statistics of different sub-class labels are computed over neighborhoods adaptively defined using the affine shape of local regions. Test images (which may contain multiple textures) are also processed in two stages. First, the generative model is used to assign initial probability estimates of sub-class membership to all feature vectors. These estimates are then refined using a relaxation step that incorporates co-occurrence statistics.</p><p>The most basic form of the modeling process is fully supervised, i.e., the training data contains only single-texture images. However, we show in Section 2.2 that a weaker form of supervision is possible: the training data may include unsegmented multi-texture images. In Section 3, we evaluate the proposed texture representation on two data sets. The first set consists of photographs of textured surfaces taken from different viewpoints and featuring significant scale changes and perspective distortions. The second set consists of images of animals whose appearance can be adequately modeled by texture-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Textures 2.1 Feature Extraction</head><p>At the feature extraction stage, our implementation uses an affine-adapted Laplacian blob detector based on the scale and shape selection framework developed by Lindeberg et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The detector begins by finding the locations in scale space where a normalized Laplacian measure attains a local maximum. Informally, the spatial coordinates of the maximum define the center of a circular "blob", and the scale at which the maximum is achieved becomes the characteristic scale of the blob. The second stage applies an affine adaptation process based on the second-moment matrix. The regions found by the detector are ellipses defined by (pp 0 ) T M (pp 0 ) ≤ 1, where p 0 is the center of the ellipse, and M is a 2 × 2 symmetric local shape matrix (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> for details). We can normalize the patch defined by M by applying to it any transformation that maps the ellipse onto a unit circle. It can be shown that if two image patches are initially related by an affine transformation, then the respective normalized patches are related by a rotation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. We eliminate this ambiguity by representing each normalized patch by a rotationally invariant descriptor.</p><p>The descriptors used in this work are intensity domain spin images <ref type="bibr">[4]</ref> inspired by the spin images used by Johnson and Hebert <ref type="bibr" target="#b1">[2]</ref> for matching range data. An intensity domain spin image is a two-dimensional histogram of brightness values in an affine-normalized patch. The two dimensions of the histogram are d, the distance from the center of the normalized patch, and i, the intensity value. Thus the "slice" of the spin image corresponding to a fixed d is simply the histogram of the intensity values of pixels located at a distance d from the center. In this work, the size of spin images is 10 × 10. Before using spin images as input to the density estimation process described in the next section, we rescale them to have a constant norm and "flatten" them into 100-dimensional feature vectors denoted x below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Density Estimation</head><p>In the supervised framework, the training data consists of single-texture sample images from classes with labels C ℓ , ℓ = 1, . . . , L. The class-conditional densities p(x|C ℓ ) can be estimated using all the feature vectors extracted from the images belonging to class C ℓ . We model class-conditional densities as p(x|C ℓ ) = M m=1 p(x|c ℓm ) p(c ℓm ), where the components c ℓm , m = 1, . . . , M , are thought of as sub-classes. Each p(x|c ℓm ) is assumed to be a Gaussian with mean µ ℓm and covariance matrix Σ ℓm . The EM algorithm is used to estimate the parameters of the mixture model, namely the means µ ℓm , covariances Σ ℓm , and mixing weights p(c ℓm ). EM is initialized with the output of the K-means algorithm. In this work, we use the same number of mixture components for each class (M = 15 and M = 10, respectively, for the experiments reported in Sections 3.1 and 3.2). We limit the number of free parameters and control numerical behavior by using spherical Gaussians with covariance matrices of the form Σ ℓm = σ 2 ℓm I. The EM framework provides a natural way of incorporating unsegmented multi-texture images into the training set. Our approach is inspired by the work of Nigam et al. <ref type="bibr" target="#b9">[10]</ref>, who have proposed techniques for using unlabeled training data in text classification. Suppose we are given a multitexture image annotated with the set L of class indices that it contains-that is, each feature vector x extracted from this image has a label set of the form C L = {C ℓ |ℓ ∈ L}. To accommodate label sets, the density estimation framework needs to be modified: instead of partitioning the training data into subsets belonging to each class and separately estimating L mixture models with M components each, we now use all the data simultaneously to estimate a single mixture model with L × M components. The estimation process must start by selecting some initial values for model parameters. During the expectation or E-step, we use the parameters to compute probabilistic sub-class membership weights given the feature vectors x and the label sets</p><formula xml:id="formula_0">C L : p(c ℓm |x, C L ) ∝ p(x|c ℓm ) p(c ℓm |C L ), where p(c ℓm |C L ) = 0 for all ℓ / ∈ L and ℓ∈L M m=1 p(c ℓm |C L ) = 1.</formula><p>During the maximization or M-step, we use the computed weights to re-estimate the parameters by maximizing the expected likelihood of the data in the standard fashion <ref type="bibr" target="#b0">[1]</ref>.</p><p>Overall, the incorporation of incompletely labeled data requires only a slight modification of the EM algorithm used for estimating class-conditional densities. However, this modification is of great utility, since the task of segmenting training examples by hand becomes an odious chore even for moderately-sized data sets. In situations where it is difficult to obtain large amounts of fully labeled examples, training on incompletely labeled or unlabeled data helps to improve classification performance <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the subsequent experiments, we exercise the EM framework in two different ways. The data set of Section 3.1 contains both single-and multi-texture training images, which are used respectively to initialize and refine the parameters of the generative model. The data set of Section 3.2 consists entirely of unsegmented multi-texture images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neighborhood Statistics</head><p>This section describes the second layer of our representation, which accumulates information about the distribution of pairs of sub-class labels in neighboring regions. After the density estimation step, each region in the training image is assigned the sub-class label that maximizes the posterior probability p(c ℓm |x, C L ). Next, we need a method for computing the neighborhood of a region centered at location p 0 and having local shape matrix M . The simplest approach is to define the neighborhood as the set of all points p such that (pp 0 ) T M (pp 0 ) ≤ α for some constant α. However, in practice this definition produces poor results: points with small ellipses get too few neighbors, and points with large ellipses get too many. A better approach is to "grow" the ellipse by adding a constant absolute amount (15 pixels in the implementation) to the major and minor axes, and to let the neighborhood consist of all points that fall inside this enlarged ellipse. In this way, the size and shape of the neighborhood still depends on the affine shape of the region, but the neighborhood structure is more balanced.</p><p>Once we have defined the neighborhood structure, we can think of the image as a directed graph with arcs emanating from the center of each region to other centers within its neighborhood. The existence of an arc from a region with sub-class label c to another region with label c ′ is a joint event (c, c ′ ) (note that the order is important since the neighborhood relation is not symmetric). We find the relative frequencies p(c, c ′ ) for all pairs (c, c ′ ), and also compute the marginals p(c) = c ′ p(c, c ′ ) and p(c ′ ) = c p(c, c ′ ). Finally, we compute the values</p><formula xml:id="formula_1">r(c, c ′ ) = p(c, c ′ ) -p(c) p(c ′ ) p(c) -p2 (c) p(c ′ ) -p2 (c ′ ) 1 2</formula><p>representing the correlations between the events that the labels c and c ′ , respectively, belong to the source and destination nodes of the same arc. The values of r(c, c ′ ) must lie between -1 and 1; negative (resp. positive) values indicate that c and c ′ rarely (resp. frequently) co-occur as labels at endpoints of the same edge.</p><p>In our experiments, we have found that the values of r(c, c ′ ) are reliable only when c and c ′ are sub-classes of the same class. Part of the difficulty in estimating correlations across classes is the lack of data in the training set. Even if the set contains multi-texture images, only a few arcs actually fall across texture boundaries. Unless the number of texture classes is very small, it is quite difficult to create a training set that would include samples of every possible boundary. Thus, whenever c and c ′ belong to different classes, we set r(c, c ′ ) to a constant negative value that serves as a "smoothness constraint" in the relaxation algorithm described in the next section (we use values between -0.5 and -1, all of which tend to produce similar results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relaxation</head><p>We have implemented the classic relaxation algorithm of Rosenfeld et al. <ref type="bibr" target="#b10">[11]</ref>. The initial estimate of the probability that the ith region has label c, denoted p (0) i (c), is obtained from the learned mixture model as the posterior p(c|x i ). Note that since we run relaxation on unlabeled test data, these probabilities must be computed for all L × M subclass labels corresponding to all possible classes. At each iteration, new estimates p (t+1) i (c) are obtained by updating the current probabilities p (t) i (c) using the equation</p><formula xml:id="formula_2">p (t+1) i (c) = p (t) i (c) 1 + q (t) i (c) c p (t) i (c) 1 + q (t) i (c) , q (t) i (c) = j w ij c ′ r(c, c ′ ) p (t) j (c ′ ) .<label>(1)</label></formula><p>The scalars w ij are weights that indicate how much influence region j exerts on region i. We treat w ij as a binary indicator variable that is nonzero if and only if the jth region belongs to the ith neighborhood. The weights are required to be normalized so that j w ij = 1 <ref type="bibr" target="#b10">[11]</ref>.</p><p>The update equation ( <ref type="formula" target="#formula_2">1</ref>) can be justified in qualitative terms as follows. Note that p (t) j (c ′ ) has no practical effect on p (t) i (c) when the ith and jth regions are not neighbors, when c and c ′ are uncorrelated, or when the probability p (t) j (c ′ ) low. However, the effect is significant when the jth region belongs to the ith neighborhood and the value of p (t) j (c ′ ) is high. The correlation r(c, c ′ ) expresses how "compatible" the labels c and c ′ are at nearby locations. Thus, p (t) i (c) is increased (resp. decreased) by the largest amount when r(c, c ′ ) has a large positive (resp. negative) value. Overall, the probabilities of different sub-class labels at neighboring locations reinforce each other in an intuitively satisfying fashion. Even though the iteration of (1) has no convergence guarantees, we have found it to behave well on our data. To obtain the results of Sections 3.1 and 3.2, we run relaxation for 200 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classification and Retrieval</head><p>Individual regions are classified by assigning them to the class that maximizes p i (C ℓ ) = M m=1 p i (c ℓm ). To perform classification and retrieval at the image level, we need to define a "global" score for each class. In the experiments of the next section, the score for class C ℓ is computed by summing the probability of C ℓ over all N regions found in the image:</p><formula xml:id="formula_3">N i=1 M m=1 p i (c ℓm )</formula><p>, where the p i (c ℓm ) are the probability estimates following relaxation. Classification of single-texture images is carried out by assigning the image to the class with the highest score, and retrieval for a given texture model proceeds from highest scores to lowest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Indoor Scene</head><p>Our first data set contains seven different textures present in a single indoor scene (Figure <ref type="figure" target="#fig_0">1</ref>). To test the invariance of our representation, we have gathered images over a wide range of viewpoints and scales. The data set is partitioned as follows: 10 single-texture training images of each class; 10 single-texture validation images of each class; 13 twotexture training images; and 45 multi-texture test images.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows classification results for the single-texture validation images following training on single-texture images only. The columns labeled "image" show the fraction of images classified correctly using the score described in Section 2.5. As can be seen from the first column, successful classification at the image level does not require relaxation: good results are achieved in most cases by using the probabilities output by the generative model. Interestingly, for class T6 (marble), the classification rate actually drops as an artifact of relaxation. When the right class has relatively low initial probabilities, the self-reinforcing nature of relaxation often serves to diminish these probabilities further. The columns labeled "region", which show the fraction of all individual regions in the validation images that were correctly classified based on the probabilities p i (C ℓ ),  are much more indicative of the impact of relaxation: for all seven classes, classification rates improve dramatically.</p><p>Next, we evaluate the performance of the system for retrieval of images containing a given texture. Figure <ref type="figure">2</ref> shows the results in the form of ROC curves that plot the positive detection rate (the number of correct images retrieved over the total number of correct images) against the false detection rate (the number of false positives over the total number of negatives in the data set). The top row shows results obtained after fully supervised training using singletexture images only, as described in Section 2.2. The bottom row shows the results obtained after re-estimating the generative model following the incorporation of 13 two-texture images into the training set. Following relaxation, a modest improvement in performance is achieved for most of the classes. A more significant improvement could probably be achieved by using more multi-texture training samples <ref type="bibr" target="#b9">[10]</ref>.</p><p>For the majority of test images, our system succeeds in providing an accurate segmentation of the image into regions of different texture. Part (a) of Figure <ref type="figure">3</ref> shows a typical example of the difference made by relaxation in the assignment of class labels to individual regions. Part (b) shows more examples where the relaxation was successful. Note in particular the top example of part (b), where the perceptually similar classes T4 and T5 are unambiguously separated. Part (c) of Figure <ref type="figure">3</ref> shows two examples of segmentation failure. In the bottom example, classes T2 (carpet) and T3 (chair) are confused, which can be partly explained by the fact that the scales at which the two textures appear in this image are not well represented in the training set. Overall, we have found the relaxation process to be sensitive to initialization, in the sense that poor initial probability estimates lead to artifacts in the final assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Animals</head><p>Our second data set consists of unsegmented images of three kinds of animals: cheetahs, giraffes, and zebras. The training set contains 10 images from each class, and the test set contains 20 images from each class, plus 20 "negative" images not containing instances of the target animals. To account for the lack of segmentation, we introduce an additional "background" class, and each training image is labeled as containing the appropriate animal and the background. To initialize EM on this data, we randomly assign each feature vector either to the appropriate animal class, or to the background. The ROC curves for each class are shown in Figure <ref type="figure">4</ref>, and segmentation results are shown in Figure <ref type="figure">5</ref>. Overall, our system appears to have learned very good models for cheetahs and zebras, but not for giraffes.</p><p>We conjecture that several factors account for the weakness of the giraffe model. Some of the blame can be placed on the early stage of feature extraction. Namely, the Laplacian-based affine region detector is not well adapted to the giraffe texture whose blobs have a relatively complex shape. At the learning stage, the system also appears to be "distracted" by background features, such as sky and trees, that occur more commonly in training samples of giraffes than of the other animals. In the bottom image of Figure <ref type="figure">5</ref>, "giraffe-ness" is associated with some parts of the background, as opposed to the animals themselves. The artificial "background" class is simply too inhomogeneous to be successfully represented in the mixture framework. A principled solution to this problem would involve partitioning the background into a set of natural classes (e.g., grass, trees, water, rocks, etc.) and building larger training sets that would include these classes in different combinations.</p><p>Overall, our results (though somewhat uneven) are promising. Unlike many other methods suitable for modeling natural textures, ours does not require negative examples. The EM framework shows surprising aptitude for automatically separating positive areas of the image from negative ones, without the need for specially designed significance scores such as the ones used by Schmid <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Future Work</head><p>The texture representation method proposed in this paper offers several important advantages over other methods proposed in recent literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>. The use of an interest  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Samples of the texture classes used in the experiments of Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Initial labeling of regions (top) vs. the final labeling following relaxation (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 5 :</head><label>35</label><figDesc>Figure 3: Segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification rates for single-texture images.</figDesc><table><row><cell></cell><cell cols="2">Before relaxation</cell><cell cols="2">After relaxation</cell></row><row><cell>Class</cell><cell>image</cell><cell>region</cell><cell>image</cell><cell>region</cell></row><row><cell>T1</cell><cell>1.00</cell><cell>0.61</cell><cell>1.00</cell><cell>0.97</cell></row><row><cell>T2</cell><cell>1.00</cell><cell>0.58</cell><cell>1.00</cell><cell>0.99</cell></row><row><cell>T3</cell><cell>0.90</cell><cell>0.70</cell><cell>0.90</cell><cell>0.85</cell></row><row><cell>T4</cell><cell>1.00</cell><cell>0.61</cell><cell>1.00</cell><cell>0.99</cell></row><row><cell>T5</cell><cell>1.00</cell><cell>0.45</cell><cell>1.00</cell><cell>0.95</cell></row><row><cell>T6</cell><cell>0.90</cell><cell>0.29</cell><cell>0.80</cell><cell>0.67</cell></row><row><cell>T7</cell><cell>0.60</cell><cell>0.41</cell><cell>0.70</cell><cell>0.73</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was partially funded by the UIUC Campus Research Board, the National Science Foundation grants IRI-990709 and IIS-0308087, the European Project LAVA (IST-2001-34405), and by a UIUC-CNRS collaboration agreement.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>point detector leads to a sparse representation that selects the most perceptually salient regions in an image, while the shape selection process provides affine invariance. Another important advantage of shape selection is the adaptive determination of both levels of image structure: the window size over which local descriptors are computed, and the neighborhood relationship between adjacent windows.</p><p>In the future, we will pursue several directions for the improvement of our system. We have found that the performance of relaxation is sensitive to the quality of the initial probability estimates; therefore, we need to obtain the best estimates possible. To this end, we plan to investigate the effectiveness of discriminative models, e.g. neural networks, that output confidence values interpretable as probabilities of class membership. Relaxation can also be made more effective by the use of stronger geometric neighborhood relations that take into account affine shape while preserving the maximum amount of invariance. Finally, we plan to extend our work to modeling complex texture categories found in natural imagery, e.g., cities, forests, and oceans.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Man-Made Structure Detection in Natural Images Using a Causal Multiscale Random Field</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Sparse Texture Representation Using Affine-Invariant Regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape-Adapted Smoothing in Estimation of 3-D Depth Cues from Affine Distortions of Local 2-D Brightness Structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gårding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature Detection with Automatic Scale Selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contour and Texture Analysis for Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing Based on Scale Invariant Interest Points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="525" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Affine Invariant Interest Point Detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="128" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text Classification from Labeled and Unlabeled Documents Using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="103" to="134" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene Labeling by Relaxation Operations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="420" to="433" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture-Based Image Retrieval Without Segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1018" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Viewpoint Invariant Texture Matching and Wide Baseline Stereo</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructing Models for Content-Based Image Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matching Widely Separated Views based on Affinely Invariant Neighbourhoods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">submitted to IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying Images of Materials: Achieving Viewpoint and Illumination Independence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="255" to="271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
