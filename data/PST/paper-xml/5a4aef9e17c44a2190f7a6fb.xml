<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NISP: Pruning Networks using Neuron Importance Score Propagation</title>
				<funder ref="#_VWVGNNG">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-21">21 Mar 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
							<email>anglili@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
							<email>chenrich@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="department">IBM T</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jui-Hsin</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
							<email>morariu@adobe.com</email>
						</author>
						<author>
							<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
							<email>xintong@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
							<email>mgao@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
							<email>cylin@graphen.ai</email>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">J</forename><surname>Watson Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Graphen</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">NISP: Pruning Networks using Neuron Importance Score Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-21">21 Mar 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1711.05908v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the "final response layer" (FRL), which is the secondto-last layer before classification, for a pruned network to retrain its predictive power. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and then fine-tuned to retain its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>CNNs require a large number of parameters and high computational cost in both training and testing phases. Recent studies have investigated the significant redundancy in deep networks <ref type="bibr" target="#b5">[6]</ref> and reduced the number of neurons and filters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> by pruning the unimportant ones. However, most current approaches that prune neurons and ? Figure <ref type="figure">1</ref>. We measure the importance of neurons in the final response layer (FRL), and derive Neuron Importance Score Propagation (NISP) to propagate the importance to the entire network. Given a pre-defined pruning ratio per layer, we prune the neurons/filters with lower importance score. We finally fine-tune the pruned model to recover its predictive accuracy.</p><p>filters consider only the statistics of one layer (e.g., prune neurons with small magnitude of weights <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>), or two consecutive layers <ref type="bibr" target="#b28">[29]</ref> to determine the "importance" of a neuron. These methods prune the "least important" neurons layer-by-layer either independently <ref type="bibr" target="#b13">[14]</ref> or greedily <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, without considering all neurons in different layers jointly.</p><p>One problem with such methods is that neurons deemed unimportant in an early layer can, in fact, contribute significantly to responses of important neurons in later layers. Our experiments (see Sec.4.4) reveal that greedy layer-by-layer pruning leads to significant reconstruction error propagation, especially in deep networks, which indicates the need for a global measurement of neuron importance across different layers of a CNN.</p><p>To address this problem, we argue that it is essential for a pruned model to retain the most important responses of the second-to-last layer before classification ("final re-sponse layer" (FRL)) to retrain its predictive power, since those responses are the direct inputs of the classification task (which is also suggested by feature selection methods, e.g., <ref type="bibr" target="#b33">[34]</ref>). We define the importance of neurons in early layers based on a unified goal: minimizing the reconstruction errors of the responses produced in FRL. We first measure the importance of responses in the FRL by treating them as features and applying some feature ranking techniques (e.g., <ref type="bibr" target="#b33">[34]</ref>), then propagate the importance of neurons backwards from the FRL to earlier layers. We prune only nodes which have low propagated importance (i.e., those whose removal does not result in large propagated error). From a theoretical perspective, we formulate the network pruning problem as a binary integer programming objective that minimizes the weighted 1 distance (proportional to the importance scores) between the original final response and the one produced by a pruned network. We obtain a closed-form solution to a relaxed version of this objective to infer the importance score of every neuron in the network. Based on this solution, we derive the Neuron Importance Score Propagation (NISP) algorithm, which computes all importance scores recursively, using only one feature ranking of the final response layer and one backward pass through the network as illustrated in Fig. <ref type="figure">1</ref>.</p><p>The network is then pruned based on the inferred neuron importance scores and fine-tuned to retain its predictive capability. We treat the pruning ratio per layer as a pre-defined hyper-parameter, which can be determined based on different needs of specific applications (e.g., FLOPs, memory and accuracy constraints). The pruning algorithm is generic, since feature ranking can be applied to any layer of interest and the importance scores can still be propagated. In addition, NISP is not hardware specific. Given a pretrained model, NISP outputs a smaller network of the same type, which can be deployed on the hardware devices designed for the original model. We evaluate our approach on MNIST <ref type="bibr" target="#b22">[23]</ref>, CIFAR10 <ref type="bibr" target="#b20">[21]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> using multiple standard CNN architectures such as LeNet <ref type="bibr" target="#b22">[23]</ref>, AlexNet <ref type="bibr" target="#b21">[22]</ref>, GoogLeNet <ref type="bibr" target="#b36">[37]</ref> and ResNet <ref type="bibr" target="#b15">[16]</ref>. Our experiments show that CNNs pruned by our approach outperform those with the same structures but which are either trained from scratch or randomly pruned. We demonstrate that our approach outperforms magnitude-based and layer-by-layer pruning. A comparison of the theoretical reduction of FLOPs and number of parameters of different methods shows that our method achieves faster full-network acceleration and compression with lower accuracy loss, e.g., our approach loses 1.43% accuracy on Alexnet and reduces FLOPs by 67.85% while Figurnov et al. <ref type="bibr" target="#b10">[11]</ref> loses more (2%) and reduces FLOPs less (50%). With almost zero accuracy loss on ResNet-56, we achieve a 43.61% FLOP reduction, significantly higher than the 27.60% reduction by Li et al. <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contribution</head><p>We introduce a generic network pruning algorithm which formulates the pruning problem as a binary integer optimization and provide a closed-form solution based on final response importance. We present NISP to efficiently propagate the importance scores from final responses to all other neurons. Experiments demonstrate that NISP leads to fullnetwork acceleration and compression for all types of layers in a CNN with small accuracy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There has been recent interest in reducing the redundancy of deep CNNs to achieve acceleration and compression. In <ref type="bibr" target="#b5">[6]</ref> the redundancy in the parameterization of deep learning models has been studied and demonstrated. Cheng et al. <ref type="bibr" target="#b1">[2]</ref> exploited properties of structured matrices and used circulant matrices to represent FC layers, reducing storage cost. Han et al. <ref type="bibr" target="#b13">[14]</ref> studied the weight sparsity and compressed CNNs by combining pruning, quantization, and Huffman coding. Sparsity regularization terms have been use to learn sparse CNN structure in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>. Miao et al. <ref type="bibr" target="#b29">[30]</ref> studied network compression based on float data quantization for the purpose of massive model storage.</p><p>To accelerate inference in convolution layers, Jaderberg et al. <ref type="bibr" target="#b17">[18]</ref> constructed a low rank basis of filters that are rank-1 in the spatial domain by exploiting cross-channel or filter redundancy. Liu et al. <ref type="bibr" target="#b27">[28]</ref> imposed a scaling factor in the training process and facilitated one channel-level pruning. Figurnov et al. <ref type="bibr" target="#b10">[11]</ref> speeded up the convolutional layers by skipping operations in some spatial positions, which is based on loop perforation from source code optimization. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b19">20]</ref>, low-rank approximation methods have been utilized to speed up convolutional layers by decomposing the weight matrix into low-rank matrices. Molchanov et al. <ref type="bibr" target="#b30">[31]</ref> prune CNNs based on Taylor expansion.</p><p>Focusing on compressing the fully connected (FC) layers, Srinivas et al. <ref type="bibr" target="#b34">[35]</ref> pruned neurons that are similar to each other. Yang et al. <ref type="bibr" target="#b42">[43]</ref> applied the "Fastfood" transform to reparameterize the matrix-vector multiplication of FC layers. Ciresan et al. <ref type="bibr" target="#b2">[3]</ref> reduced the parameters by randomly pruning neurons. Chen et al. <ref type="bibr" target="#b0">[1]</ref> used a low-cost hash function to randomly group connection weights into hash buckets and then fine-tuned the network with backpropagation. Other studies focused on fixed point computation rather than exploiting the CNN redundancy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>. Another work studied the fundamental idea about knowledge distillation <ref type="bibr" target="#b16">[17]</ref>. Wu et al. <ref type="bibr" target="#b38">[39]</ref> proposed to skip layers for speeding up inference. Besides the above work which focuses on network compression, other methods speedup deep network inference by refining the pipelines of certain tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref>. Our method prunes a pre-trained network and requires a fast-converging fine-tuning process, rather than re-training a network from scratch. To measure the importance of neurons in a CNN, the exact solution is very hard to obtain given the complexity of nonlinearity. Some previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> approximate it using 2nd-order Taylor expansion. Our work is a different approximation based on the Lipschitz continuity of a neural network.</p><p>Most similar to our approach, Li et al. <ref type="bibr" target="#b24">[25]</ref> pruned filters by their weight magnitude. Luo et al. <ref type="bibr" target="#b28">[29]</ref> utilized statistics information computed from the next layer to guide a greedy layer-by-layer pruning. In contrast, we measure neuron importance based not only on a neuron's individual weight but also the properties of the input data and other neurons in the network. Meanwhile, instead of pruning layer-by-layer in greedy fashion under the assumption that one layer can only affect its next layer, which may cause error propagation, we measure the importance across the entire network by propagating the importance from the final response layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>An overview of NISP is illustrated in Fig. <ref type="figure">1</ref>. Given a trained CNN, we first apply a feature ranking algorithm on this final response layer and obtain the importance score of each neuron. Then, the proposed NISP algorithm propagates importance scores throughout the network. Finally, the network is pruned based on the importance scores of neurons and fine-tuned to recover its accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Ranking on the Final Response Layer</head><p>Our intuition is that the final responses of a neural network should play key roles in full network pruning since they are the direct inputs of the classification task. So, in the first step, we apply feature ranking on the final responses.</p><p>It is worth noting that our method can work with any feature selection that scores features w.r.t. their classification power. We employ the recently introduced filtering method Inf-FS <ref type="bibr" target="#b33">[34]</ref> because of its efficiency and effectiveness on CNN feature selection. Inf-FS utilizes properties of the power series of matrices to efficiently compute the importance of a feature with respect to all the other features, i.e., it is able to integrate the importance of a feature over all paths in the affinity graph 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neuron Importance Score Propagation (NISP)</head><p>Our goal is to decide which intermediate neurons to delete, given the importance scores of final responses, so that the predictive power of the network is maximally retained. We formulate this problem as a binary integer programming (optimization) and provide a closed-form approximate solution. Based on our theoretical analysis, we 1 Details of the method are introduced in <ref type="bibr" target="#b33">[34]</ref> and its codes taken from develop the Neuron Importance Score Propagation algorithm to efficiently compute the neuron importance for the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Definition</head><p>The goal of pruning is to remove neurons while minimizing accuracy loss. Since model accuracy is dependent on the final responses, we define our objective as minimizing the weighted distance between the original final responses and the final responses after neurons are pruned of a specific layer. In following, we use bold symbols to represent vectors and matrices.</p><p>Most neural networks can be represented as a nested function. Thus, we define a network with depth n as a function (1) . The l-th layer f (l) is represented using the following general form,</p><formula xml:id="formula_0">F (n) = f (n) ? f (n-1) ? ? ? ? ? f</formula><formula xml:id="formula_1">f (l) (x) = ? (l) (w (l) x + b (l) ),<label>(1)</label></formula><p>where ? (l) is an activation function and w (l) , b (l) are weight and bias, and f(n) represents the "final response layer". Networks with branch connections such as the skip connection in ResNet can be transformed to this representation by padding weights and merging layers. We define the neuron importance score as a non-negative value w.r.t. a neuron, and use s l to represent the vector of neuron importance scores in the l-th layer. Suppose N l neurons are to be kept in the l-th layer after pruning; we define the neuron prune indicator of the l-th layer as a binary vector s * l , computed based on neuron importance scores s l such that s * l,i = 1 if and only if s l,i is among top N l values in s l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Objective Function</head><p>The motivation of our objective is that the difference between the responses produced by the original network and the one produced by the pruned network should be minimized w.r.t. important neurons. Let F (n) be a neural net-work with n layers. Suppose we have a dataset of M samples, and each is represented using x (m) 0 . For the m-th sample, we use x (m) l to represent the response of the l-th layer (which is the input to the (l + 1)-th layer). The final output of the network is x (m) n and its corresponding non-negative neuron importance is s n . We define</p><formula xml:id="formula_2">G (i,j) = f (j) ? f (j-1) ? ? ? ? ? f (i)<label>(2)</label></formula><p>as a sub-network of F (n) starting from the i-th layer to the j-th layer. Our goal is to compute for the l-th layer the neuron prune indicator s * l so that the influence of pruning the l-th layer on the important neurons of the final response is minimized. To accomplish this, we define an optimization objective w.r.t. the l-th layer neuron prune indicator, i.e., arg min</p><formula xml:id="formula_3">s * l M m=1 F(s * l |x (m) l , s n ; G (l+1,n) ) ,<label>(3)</label></formula><p>which is accumulated over all samples in the dataset. The objective function for a single sample is defined as</p><formula xml:id="formula_4">F(s * l |x, s n ; F ) = s n , |F (x) -F (s * l x)| ,<label>(4)</label></formula><p>where ?, ? is dot product, is element-wise product and | ? | is element-wise absolute value. The solution to Eq. 3 indicates which neurons should be pruned in an arbitrary layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Solution</head><p>The network pruning problem can be formulated as a binary integer program, finding the optimal neuron prune indicator in Eq. 3. However, it is hard to obtain efficient analytical solutions by directly optimizing Eq. 3. So we derive an upper bound on this objective, and show that a sub-optimal solution can be obtained by minimizing the upper bound. Interestingly, we find a feasible and efficient formulation for the importance scores of all neurons based on this sub-optimal solution.</p><p>Recall that the k-th layer is defined as</p><formula xml:id="formula_5">f (k) (x) = ? (k) (w (k) x+b (k)</formula><p>). We assume the activation function ? (k)  is Lipschitz continuous since it is generally true for most of the commonly used activations in neural networks such as Identity, ReLU, sigmoid, tanh, PReLU, etc. Then we know for any x, y, there exists a constant</p><formula xml:id="formula_6">C (k) ? such that |? (k) (x) -? (k) (y)| ? C (k) ? |x -y|. Then it is easy to see |f (k) (x) -f (k) (y)| ? C (k) ? |w (k) | ? |x -y| ,<label>(5)</label></formula><p>where | ? | is the element-wise absolute value. From Eq. 2, we see that 1) . Therefore, we have,</p><formula xml:id="formula_7">G (i,j) = f (j) ? G (i,j-</formula><formula xml:id="formula_8">|G (i,j) (x) -G (i,j) (y)| ? C (j) ? |w (j) ||G (i,j-1) (x) -G (i,j-1) (y)| .<label>(6)</label></formula><p>Applying Eq. 5 and Eq. 6 repeatedly, we have, ?i ? j ? n,</p><formula xml:id="formula_9">|G (i,n) (x) -G (i,n) (y)| ? C (i,n) ? W (i,n) |x -y|,<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">W (i,j) = |w (j) ||w (j-1) | ? ? ? |w (i) |,<label>and</label></formula><formula xml:id="formula_11">C (i,j) ? = j k=i C (k) ? . Substituting x = x (m) l , y = s * l x (m) l</formula><p>, i = l+1 into Eq. 7, we have</p><formula xml:id="formula_12">|G (l+1,n) (x (m) l ) -G (l+1,n) (s * l x (m) l )| ? C (l+1,n) ? W (l+1,n) |x (m) l -s * l x (m) l | . (8)</formula><p>Since s n is a non-negative vector,</p><formula xml:id="formula_13">F(s * l |x (m) l , s n ; G (l+1,n) ) = s n , |G (l+1,n) (x (m) l ) -G (l+1,n) (s * l x (m) l )| (9) ? s n , C (l+1,n) ? W (l+1,n) |x (m) l -s * l x (m) l | (10) = C (l+1,n) ? W (l+1,n) s n , (1 -s * l ) |x (m) l | . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>Let us define</p><formula xml:id="formula_15">r l = W (l+1,n) s n ; then M m=1 F(s * l |x (m) l , s n ; G (l+1,n) ) ? C (l+1,n) ? M m=1 r l , (1 -s * l ) |x (m) l | (12) ? C (l+1,n) ? M m=1 i r l,i (1 -s * l,i )|x (m) l,i | (13) = C (l+1,n) ? i r l,i (1 -s * l,i ) M m=1 |x (m) l,i | . (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>Since |x</p><formula xml:id="formula_17">(m) l,i | is bounded, there must exist a constant C x such that M m=1 |x (m) l,i | ? C x , ?i. Thus, we have M m=1 F(s * l |x (m) l , s n ; F (l+1) ) ? C i r l,i (1 -s * l,i ),<label>(15)</label></formula><p>where</p><formula xml:id="formula_18">C = C (l+1,n) ?</formula><p>C x is a constant factor. Eq. 15 reveals an upper-bound of our objective in Eq. 3. Thus, we minimize this upper-bound, i.e., arg min</p><formula xml:id="formula_19">s * l i r l,i (1 -s * l,i ) ? arg max s * l i s * l,i r l,i .<label>(16)</label></formula><p>The optimal solution to Eq.16 is sub-optimal with respect to the original objective in Eq. 3, however it still captures the importance of neurons. It is easy to see that if we keep N x neurons in the l-th layer after pruning, then the solution to Eq. 16 is that s * l,i = 1 if and only if r l,i is among the highest N x values in r l . According to the definition of neuron prune indicator in Sec. 3.2.1, r l = W (l+1,n) s n is a feasible solution to the importance scores of the l-th layer response. This conclusion can be applied to every layer in the network. Based on this result, we define the neuron importance of a network as follows.</p><p>Definition 1 (Neuron importance score). Given a neural network F (n) containing n layers and the importance score s (n) of the last layer response, the importance score of the k-th layer response can be computed as</p><formula xml:id="formula_20">s k = |w (k+1) | |w (k+2) | ? ? ? |w (n) | s n ,<label>(17)</label></formula><p>where w (i) is the weight matrix of the i-th layer.</p><p>An important property of neuron importance is that it can be computed recursively (or propagated) along the network.</p><p>Proposition 2 (Neuron importance score propagation). The importance score of the k th layer response can be propagated from the importance score of the (k + 1) th layer by</p><formula xml:id="formula_21">s k = |w (k+1) | s k+1 ,<label>(18)</label></formula><p>where w (k+1) is the weight matrix of the (k + 1) th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Algorithm</head><p>We propose the Neuron Importance Score Propagation (NISP) algorithm (shown in Fig. <ref type="figure" target="#fig_3">2</ref>) based on Proposition 2. Initially, we have the importance score of every neuron in the final response layer of the network. Definition 1 shows that the importance score of every other layer in the network is directly correlated with the importance of the final response. However, instead of computing the importance expensively using Definition 1, we see from Eq. 18 that the importance score of a lower layer can be propagated directly from the adjacent layer above it. An equivalent form of Eq. 18 is</p><formula xml:id="formula_22">s k,j = i |w (k+1) i,j |s k+1,i ,<label>(19)</label></formula><p>where s k,j is the importance score of the j-th neuron in the k-th layer response. We conclude from Eq. 19 that the importance of a neuron is a weighted sum of all the subsequent neurons that are directly connected to it. This conclusion also applies to normalization, pooling and branch connections in the network (i.e., a layer is directly connected with multiple layers) 2 . The NISP algorithm starts with the importance in FRL and repeats the propagation (Eq. <ref type="bibr" target="#b18">19)</ref> to obtain the importance of all neurons in the network with a single backward pass (Fig. <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pruning Networks Using NISP</head><p>Given target pruning ratios for each layer, we propagate the importance scores, compute the prune indicator of neurons based on their importance scores and remove neurons with prune indicator value 0. The importance propagation and layer pruning happens jointly in a single backward pass, 2 See supplementary material for more details and proofs. and the importance of a pruned neuron is not propagated to any further low-level layers. For fully connected layers, we prune each individual neuron. For convolution layers, we prune a whole channel of neurons together. The importance score of a channel is computed as the summation of the importance scores of all neurons within this channel 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on standard datasets with popular CNN networks. We first compare to random pruning and training-from-scratch baselines to demonstrate the effectiveness of our method. We then compare to two other baselines, magnitude-based pruning and layer-bylayer pruning to highlight the contributions of feature ranking and neuron importance score propagation, respectively. Finally, we benchmark the pruning results and compare to existing methods such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>We conduct experiments on three datasets, MNIST <ref type="bibr" target="#b22">[23]</ref>, CIFAR10 and ImageNet <ref type="bibr" target="#b4">[5]</ref>, for the image classification task. We evaluate using five commonly used CNN architectures: LeNet <ref type="bibr" target="#b22">[23]</ref>, Cifar-net<ref type="foot" target="#foot_0">3</ref> , AlexNet <ref type="bibr" target="#b21">[22]</ref>, GoogLeNet <ref type="bibr" target="#b36">[37]</ref> and ResNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>All experiments and time benchmarks are obtained using Caffe <ref type="bibr" target="#b18">[19]</ref>. The hyper-parameter of Inf-FS is a loading coefficient ? ? [0, 1], which controls the influence of variance and correlation when measuring the importance. We conduct PCA accumulated energy analysis (results shown in the supplementary material) as suggested in <ref type="bibr" target="#b46">[47]</ref> to guide our choice of pruning ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with Random Pruning and Trainfrom-scratch Baselines</head><p>We compare to two baselines: (1) randomly pruning the pre-trained CNN and then fine-tuning, and (2) training a small CNN with the same number of neurons/filters per layer as our pruned model from scratch. We use the same experimental settings for our method and baselines except for the initial learning rate. For training from scratch, we set the initial learning rate to the original one, while for finetuning tasks (both NISP and random pruning), the initial learning rate is reduced by a factor of 10.</p><p>LeNet on MNIST: We prune half of the neurons in FC layers and half of the filters in both convolution layers in Fig. <ref type="figure" target="#fig_0">3(a)</ref>. Our method is denoted as NISP Half , while the baseline methods that prune randomly or train from scratch are denoted as Random Half and Scratch Half . Our method outperforms the baselines in three aspects. First, for fine-tuning (after pruning), unlike the baselines, our method has very small accuracy loss at iteration 0; this implies that it retains the most important neurons, pruning only redundant or less discriminative ones. Second, our method converges much faster than the baselines. Third, our method has the smallest accuracy loss after fine-tuning. For LeNet on MNIST, our method only decreases 0.02% top-1 accuracy with a pruning ratio of 50% as compared to the pre-pruned network.</p><p>Cifar-net on CIFAR10: The learning curves are shown in Fig. <ref type="figure" target="#fig_0">3(b)</ref>. Similar to the observations from the experiment for LeNet on MNIST, our method outperforms the baselines in the same three aspects: the lowest initial loss of accuracy, the highest convergence speed and the lowest accuracy loss after fine-tuning. Our method has less than 1% top-1 accuracy loss with 50% pruning ratio for each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AlexNet on ImageNet:</head><p>To demonstrate that our method works on large and deep CNNs, we replicate experiments on AlexNet with a pruning ratio of 50% for all convolution layers and FC layers (denoted as NISP CF when we prune both conv and FC layers). Considering the importance of FC layers in AlexNet, we compare one more scenario in which our approach only prunes half of the filters but without pruning neurons in FC layers (denoted as NISP C ). We reduce the initial learning rate by a factor of 10, then fine-tune 90 epochs and report top-5 accuracy loss. Fig. <ref type="figure" target="#fig_0">3(c</ref>) shows that for both cases (pruning both convolution and FC layers and pruning only convolution layers), the advantages we observed on MNIST and CIFAR10 still hold. Layer-wise computational reduction analysis that shows the full-network acceleration can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GoogLeNet on ImageNet:</head><p>We denote the reduction layers in an inception module as "Reduce", and the 1?1 convolution layer without reduction as "1?1". We use the quick solver from Caffe in training. We conduct experiments between our method and the baselines for 3 pruning strategies: (Half ) pruning all convolution layers by half; (noReduce) pruning every convolution layer except for the reduction layers in inception modules by half; (no1x1 ) pruning every convolution layer by half except the 1?1 layers in inception modules. We show results for two of them in Fig. <ref type="figure" target="#fig_0">3(d)</ref>, and observe similar patterns to the experiments on other CNN networks <ref type="foot" target="#foot_1">4</ref> . For all GoogLeNet experiments, we train/finetune for 60 epochs and report top-5 accuracy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Selection v.s. Magnitude of Weights</head><p>How to define neuron importance is an open problem. Besides using feature ranking to measure neuron importance, other methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref> measure neuron importance by magnitude of weights. To study the effects of different criteria to determine neuron importance, we conduct experiments by fixing other parts of NISP and only comparing the pruning results with different measurements of importance: 1. using feature selection method in <ref type="bibr" target="#b33">[34]</ref> (NISP-FS) and 2. considering only magnitude of weights (NISP-Mag). For the Magnitude-based pruning, the importance of a neuron in the final response layer equals the absolute sum of all weights connecting the neuron with its previous layer. To compare only the two metrics of importance, we rank the importance of neurons in the final response layer based on the magnitude of their weight values, and propagate their importance to the lower layers. Finally, we prune and finetune the model in the same way as the NISP method.</p><p>For the "NISP-Mag" baseline, we use both AlexNet and Cifar-net architectures. The learning curves of those baselines are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. We observe that "NISP-FS" yields much smaller accuracy loss with the same pruning ratio than "NISP-Mag", but "NISP-Mag" still outperforms the random pruning and train-from-scratch baselines, which shows the effectiveness of NISP with different measurement of importance. In the remainder of this paper, we employ the feature ranking method proposed in <ref type="bibr" target="#b33">[34]</ref> in NISP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">NISP v.s. Layer-by-Layer Pruning</head><p>To demonstrate the advantage of the NISP's importance propagation, we compare with a pruning method that conducts feature ranking on every layer to measure the neuron importance and prune the unimportant neurons of each layer independently. All other settings are the same as NISP. We call this method "Layer-by-Layer" (LbL) pruning.  One challenge for the "LbL" baseline is that the computational cost of measuring neuron importance on each layer is huge. So we choose a small CNN structure trained on the CIFAR10 dataset. Fig. <ref type="figure" target="#fig_2">4(b)</ref> shows that although the "LbL" method outperforms the baselines, it performs much worse than NISP in terms of the final accuracy loss with the same pruning ratio, which shows the need for measuring the neuron importance across the entire network using NISP.</p><p>To further study the advantage of NISP over layer-bylayer pruning, we define the Weighted Average Reconstruction Error (WARE) to measure the change of the important neurons' responses on the final response layer after pruning (without fine-tuning) as:</p><formula xml:id="formula_23">WARE = M m=1 N i=1 s i ? |?i,m-yi,m| |yi,m| M ? N ,<label>(20)</label></formula><p>where M and N are the number of samples and number of retained neurons in the final response layer; s i is the importance score; y i,m and ?i,m is the response on the m th sample of the i th neuron before/after pruning. We design different Cifar-net-like CNNs with different numbers of Conv layers, and apply NISP and LbL pruning with different pruning ratios. We report the WARE on the retained neurons in the final response layer ("ip1" layer in Cifar-net-like CNNs) in Fig. <ref type="figure">5</ref>. We observe that: 1. As network depth increases, the WARE of the LbL-pruned network dramatically increases, which indicates the error propagation problem of layer-by-layer pruning, especially when the network is deep, and suggests the need for a global pruning method such as NISP; 2. The WARE of the LbL method becomes much larger when the pruning ratio is large, but is more stable when using NISP to prune a network; 3. NISP methods always reduce WARE on the retained neurons compared to LbL. The small reconstruction errors on the important neurons in the final response layer obtained by NISP provides a better initialization for fine-tuning, which leads to much lower accuracy loss of the pruned network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Existing Methods</head><p>We compare our method with existing pruning methods on AlexNet, GoogLeNet and ResNet, and show results in Table <ref type="table">1</ref>.</p><p>We show benchmarks of several pruning strategies in Table <ref type="table">1</ref>, and provide additional results in the supplementary materials. In Table <ref type="table">1</ref>, for AlexNet, the pruning ratio is 50%. NISP-A denotes pruning all Conv layers; NISP-B denotes pruning all Conv layers except for Conv5; NISP-C denotes pruning all Conv layers except for Conv5 and Conv4; NISP-D means pruning Conv2, Conv3 and FC6 layers. For GoogLeNet, we use the similar the pruning ratios of the 3?3 layers in <ref type="bibr" target="#b19">[20]</ref>, and we prune 20% of the reduce layers. Our method is denoted as "NISP".</p><p>To compare theoretical speedup, we report reduction in the number of multiplication and the number of parameters following <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b10">[11]</ref>, and denote them as [FLOPs?%] and [Params.?%] in the table. Pruning a CNN is a tradeoff between efficiency and accuracy. We compare different methods by fixing one metric and comparing the other.</p><p>On AlexNet, by achieving smaller accuracy loss (1.43% ours vs. 2.00% <ref type="bibr" target="#b10">[11]</ref>), our method NISP-A manages to reduce significantly more FLOPs (67.85%) than the one in <ref type="bibr" target="#b10">[11]</ref> (50%), denoted as "Perforate" in the table; compare to the method in <ref type="bibr" target="#b35">[36]</ref> (denoted as "Learning"), our method NISP-C achieves much smaller accuracy loss (0.54% ours vs. 1.20%) and prunes more FLOPs (53.70% ours vs. 48.19%). We manage to achieve 0 accuracy loss and reduce over 40% FLOPs and 47.09% parameters (NISP-D). On GoogLeNet, Our method achieves similar accuracy loss with larger FLOPs reduction (58.34% vs. 51.50%) Using ResNet on Cifar10 dataset, with top-1 accuracy loss similar to <ref type="bibr" target="#b24">[25]</ref> (56-A, 56-B. 110-A and 110-B), our method reduces more FLOPs and parameters.</p><p>We also conduct our ResNet experiments on ImageNet <ref type="bibr" target="#b4">[5]</ref>. We train a ResNet-34 and a ResNet-50 for 90 epochs. For both ResNet models, we prune 15% and 25% of filters for each layer (denote as "NISP-X-A" and "NISP-X-B" ("X" indicates the ResNet model) in Table <ref type="table">1</ref>), and obtain 27-44% FLOPs and parameter reduction with tiny top-1 accuracy loss, which shows superior performance when compared with the state-of-the-art methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Additional Analysis</head><p>Below, we provide case studies and ablation analysis to help understand the proposed NISP pruning algorithm.</p><p>Similar Predictive Power of Networks Before/After Pruning. To check whether the pruned network performs similarly with the original network, we compare the final classification results of the original AlexNet and the pruned one with fine-tuning using the ILSVRC2012 validation set. 85.9% of the top 1 predictions of the two networks agree with each other, and 95.1% top 1 predictions of the pruned network can be found in the top 5 predictions of the original network. The above experiments show that the network pruned by NISP performs similarly with the original one.</p><p>Sensitivity of pruning ratios. The selection of per-layer pruning ratios given a FLOPs budget is a challenging open problem with a large search space. Due to time limitation, we either choose a single pruning ratio for all layers or replicate the pruning ratios of baseline methods (e.g., <ref type="bibr" target="#b19">[20]</ref>), and NISP achieves smaller accuracy loss, which shows the effectiveness of NISP. In practice, if time and GPU resources permit, one can search the optimal hyper-parameters by trying different pruning ratio combinations on a validation set. We also evaluate NISP with very large pruning ratios. We test on pruning ratios of 75% (denoted as Quarter in the figures) and 90% using LeNet (Fig. <ref type="figure" target="#fig_4">6(a)</ref>) (denoted as Tenth) for both Conv and FC layers. For AlexNet (Fig. <ref type="figure" target="#fig_4">6(b</ref>)), we test on pruning ratios of 75% (Quarter) for both convolution and FC layers, and we test two pruning strategies: (1) prune 75% of neurons in FC layers and filters in Conv layers, denoted as FC; and (2) only prune 75% of the convolution filters without pruning FC layers, denoted as C.</p><p>The above experiments show that NISP still outperforms all baselines significantly with large pruning ratios, in terms of both convergence speed and final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a generic framework for network compression and acceleration based on identifying the importance levels of neurons. Neuron importance scores in the layer of interest (usually the last layer before classification) are obtained by feature ranking. We formulated the network pruning problem as a binary integer program and obtained a closed-form solution to a relaxed version of the formulation. We presented the Neuron Importance Score Propagation algorithm that efficiently propagates the importance to every neuron in the whole network. The network is pruned by removing less important neurons and fine-tuned to retain its predicative capability. Experiments demonstrated that our method effectively reduces CNN redundancy and achieves full-network acceleration and compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head><p>Despite their impressive predictive power on a wide range of tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref>, the redundancy in the parameterization of deep learning models has been studied and demonstrated <ref type="bibr" target="#b5">[6]</ref>. We present NISP to efficiently propagate the importance scores from final responses to all other neurons to guide network pruning to achieve acceleration and compression of a deep network. In the supplementary materials, we show details on how to propagate neuron importance from the final response layer, and some additional experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Neuron Importance Score Propagation (NISP)</head><p>Given the importance of a neuron, we first identify the positions in the previous layer that are used as its input, then propagate the importance to the positions proportional to the weights. We only propagate the importance of the selected feature extractors to the previous layers and ignore the pruned ones. The NISP process can be divided into three classes: from a 1-way tensor to a 1-way tensor, e.g. between FC layers; from a 1-way tensor to a 3-way tensor, e.g., from an FC layer to a conv/pooling layer; from a 3-way tensor to a 3-way tensor, e.g., from a pooling layer to a conv layer.</p><p>We simplify NISP by ignoring the propagation of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">NISP: from 1-way tensor to 1-way tensor</head><p>Given an FC layer with M input neurons and N output neurons, the N -by-1 importance vector (S) of the output feature is</p><formula xml:id="formula_24">S FCout = [S F Cout 1 , S F Cout 2 . . . S F Cout N ]</formula><p>T . We use W FC ? R M ?N to denote the weights of the FC layer.</p><p>The importance vector of the input neurons is:</p><formula xml:id="formula_25">S FC in = |W FC | ? S FCout ,<label>(21)</label></formula><p>where | ? | is element-wise absolute value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">NISP: from 1-way tensor to 3-way tensor</head><p>Given an FC layer with a 3-way tensor as input and N output neurons, the input has a size of X ? X ? C, where X is the spatial size and C is the number of input channels. The input can be the response of a convolutional layer or a pooling layer. We use W FC ? R (X 2 ?C)?N to denote the weights of the FC layer. The flattened importance vector S in ? R (X 2 ?C)?1 of the input tensor is:</p><formula xml:id="formula_26">S in = |W FC | ? S FCout .<label>(22)</label></formula><p>6.4. NISP: from 3-way tensor to 3-way tensor 6.4.1 Convolution Layer.</p><p>We derive NISP for a convolutional layer, which is the most complicated case of NISP between 3-way tensors. NISP for Algorithm 1 NISP: convolutional layer 1: Input : weights of the conv layer W ? R X?X?N ?F 2: , flattened importance of the f th output channel 3: S f out ? R 1?(X?X) 4: for n in 1 . . . N do 5:</p><p>for f in 1 . . . F do 6:</p><formula xml:id="formula_27">k f n ? |W[:, :, n, f ]| 7:</formula><p>Construct BP f n conv as ( <ref type="formula" target="#formula_31">25</ref>) and ( <ref type="formula" target="#formula_32">26</ref>)</p><formula xml:id="formula_28">8: S f n in ? S f out ? BP f n conv 9: S n in ? f S f n in 10: S in ? [S 1 in , S 2 in . . . , S N in ] 11: end</formula><p>pooling and local response normalization (LRN) can be derived similarly.</p><p>For a convolutional layer with the input 3-way tensor conv in ? R X?X?N and output tensor conv out ? R Y ?Y ?F ) , the filter size is k, stride is s and the number of padded pixels is p. During the forward propagation, convolution consists of multiple inner products between a kernel k f ? R k?k?N , and multiple corresponding receptive cubes to produce an output response. Fixing input channel n and output channel f , the spatial convolutional kernel is k f n . For position i in the n th channel of the input tensor, the corresponding response of the output channel f at position i is defined as Equation <ref type="formula" target="#formula_29">23</ref>:</p><formula xml:id="formula_29">R f (i) = n k f n ? in(i),<label>(23)</label></formula><p>where in(i) is the corresponding 2-D receptive field. Given the importance cube of the output response S out ? R Y ?Y ?F , we use a similar linear computation to propagate the importance from the output response to the input:</p><formula xml:id="formula_30">S n (i) = f k f n ? S out (i),<label>(24)</label></formula><p>where S n (i) is the importance of position i in the n th input channel, and S out (i) is the corresponding 2-D matrix that contains the output positions whose responses come from the value of that input position during forward propagation. We propagate the importance proportionally to the weights as described in Algorithm 1.</p><p>The propagation matrices used in algorithm 1 are defined in <ref type="bibr" target="#b24">(25)</ref> and ( <ref type="formula" target="#formula_32">26</ref>)</p><formula xml:id="formula_31">BP f n conv = ? ? ? ? ? ? ? ? b f n 1 . . . b f n j . . . b f n k b f n 1 . . . b f n j . . . b f n k . . . b f n 1 . . . b f n j . . . b f n k ? ? ? ? ? ? ? ? ,<label>(25)</label></formula><p>where b i c is the building block of size Y ? X defined as:</p><formula xml:id="formula_32">b f n i = ? ? ? ? ? ? k f n [i, 1] . . . . . . k f n [i, k] k f n [i, 1] . . . . . . k f n [i, k] . . . k f n [i, 1] . . . . . . k f n [i, k] ? ? ? ? ? ? ,<label>(26)</label></formula><p>Equation 24 implies that the propagation of importance between 3-way tensors in convolutional layers can be decomposed into propagation between 2-D matrices. Fixing the input channel n and the output channel f , the input layer size is X ? X and the output size is Y ? Y . Given the flattened importance vector S out f ? R 1?(Y ?Y ) of the output layer, the propagation matrix BP f n conv ? R (Y ?Y )?(X?X) is used to map from S out f to the importance of input layer S in f n ? R 1?(X?X) . BP f n conv (i, j) = 0, implies that the i th position in the output layer comes from a convolution operation with the j th position in the input layer, and we propagate the importance between the two positions. We use a Y ? X matrix b f n i to represent the mapping between a row in the output layer to the corresponding row in the input layer. In each row of b f n i , there are k non-zeros since each position in the output layer is obtained from a region with width k of the input layer. The non-zeros of each row of b f n i are the i th row of the convolutional kernel k f n . The offset of the beginning of the weights in each row is the stride s. The entire propagation matrix BP f n conv is a block matrix with each submatrix being a Y ? X matrix of either b f n i or a zero matrix. Each row of BP f n conv has b f n 1 to b f n k because the height of a convolutional kernel is k. The offset of the beginning of the bs in each row of BP f n conv is the stride s. We use the case when X = 4, Y = 2, k = 3, s = 1 as an example shown in Figure <ref type="figure" target="#fig_5">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Pooling Layer.</head><p>Assume a pooling layer with input tensor of size X ?X ?F and output size Y ? Y ? F . The pooling filter size is k and the stride is s. The basic idea of most pooling techniques is the same: use a fixed 2-dimensional filter to abstract local responses within each channel independently. For example, in max pooling each output response consists of the max of k ? k values from the input responses. Due to the large variance of input data, it is safe to assume a uniform distribution on which value within the receptive field is the largest is a uniform distribution. Consequently, for an output response location, the contributions from the corresponding k?k values of the input response are equal. Since pooling is a spatial operation that does not cross channels, we can propagate the importance of each channel independently. Given a flattened importance vector of a channel f S out f ? R 1?(Y ?Y ) of the output 3-way tensor, the flattened importance vector of the input tensor is calculated as: </p><formula xml:id="formula_33">S in f = S out f ? BP pooling ,<label>(27)</label></formula><p>where BP pooling is the back-propagation matrix of size Y 2 ? X 2 defined as: </p><formula xml:id="formula_34">BP pooling = ? ? ? ? ?</formula><p>where b p is the building block of size Y ? X defined as:</p><formula xml:id="formula_36">b p = ? ? ? ? ? ? 1 . . . 1 . . . 1 1 . . . 1 . . . 1 . . . 1 . . . 1 . . . 1 ? ? ? ? ? ? ,<label>(29)</label></formula><p>Consider one channel with input size X ? X and the output size Y ? Y . Given the flattened importance vector S out f ? R 1?(Y ?Y ) of the output layer, the propagation matrix BP pooling ? R (Y ?Y )?(X?X) is used to map from S out f to the importance of input layer S in f ? R 1?(X?X) . If BP pooling (i, j) = 1, the i th position in the output layer comes from a pooling operation and involves the j th position in the input layer, so we propagate the importance between the two positions. We use a Y ? X matrix b p to represent the mapping between a row in the output layer to the corresponding row in the input layer. In each row of  Krizhevsky et al. <ref type="bibr" target="#b21">[22]</ref> proposed Local Response Normalization (LRN) to improve CNN generalization. For crosschannel LRN, sums over adjacent kernel maps at the same spatial position produce a response-normalized activation at that position. Since LRN is a non-linear operation, it is intractable to conduct exact importance propagation between the input and output tensors. One way to approximate propagation is to assume the kernel maps at one spatial position contribute equally to the response at that position of the output tensor when considering the large variance of the input data. Then, given the X ? X ? N importance tensor for the response of a LRN layer with local size = l, which is the number of adjacent kernel maps summed for a spatial position, considering all N channels of a spatial position (i, j), the importance vector of that spatial position is S ij out ? R 1?N . The corresponding importance vector of the input S ij in ? R 1?N is:</p><formula xml:id="formula_37">S ij in = S ij out ? BP LRN ,<label>(30)</label></formula><p>where BP LRN ? R N ?N is defined as:</p><formula xml:id="formula_38">BP LRN = ? ? ? ? ? ? ? ? ? ? ? ? 1 1 ? ? ? 1 1 1 ? ? ? 1 1 ? ? ? ? ? ? 1 1 ? ? ? 1 1 ? ? ? 1 ? ? ? 1 1 ? ? ? 1 ? ? ? 1 1 1 1 ? ? ? ? ? ? ? ? ? 1 ? ? ? 1 1 ? ? ? 1 1 1 ? ? ? 1 1 ? ? ? ? ? 1 1 ? ? ? 1 1 1 ? ? ? 1 1 ? ? ? ? ? ? ? ? ? ? ? ? . (<label>31</label></formula><formula xml:id="formula_39">)</formula><p>For a cross-channel LRN, the output response tensor has the same shape as the input. For a spatial position (i, j) of the output tensor, given its importance vector S ij out , we construct a N ? N symmetric matrix BP LRN to propagate its importance to the corresponding input vectorS ij in at that position. Since the center of the LRN operation is at position (i, j), the operation will cover l+1 2 positions to the left and right. When the operation is conducted on the positions at the center of the vector S ij out (from column l+1 2 to N -l+1 2 + 1), the operation covers l cross-channel position so that the corresponding columns in BP LRN have l 1's. When the LRN operation is conducted at the margin of the vector, there are missing cross-channel positions so that from column l+1 2 to column 1 (similar for the right-bottom corner), the 1's in the corresponding column of BP LRN decreases by 1 per step from the center to the margin. We use the case when l = 3, N = 5 as an example of LRN layer with cross-channel in Figure <ref type="figure" target="#fig_9">9</ref>.  For within-channel LRN, following our equal distribution assumption, the importance can be propagated similarly as in a pooling layer.</p><p>6.5. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">PCA Accumulated Energy Analysis</head><p>One way to guide the selection of pruning ratio is the PCA accumulated energy analysis <ref type="bibr" target="#b46">[47]</ref> on the responses of a pre-pruned layer. The PCA accumulated energy analysis shows how many PCs it needs for that layer to capture the majority of variance of the samples, which implies a proper range of how many neurons/kernels we should keep for that layer. We show the PCA accumulated energy analysis results on the last FC layers before the classification part for LeNet (ip1) and AlexNet (fc7) in Figure <ref type="figure" target="#fig_11">10</ref>(a) and 10(b). By setting variance threshold as 0.95, 120 out of 500 PCs are required for LeNet, 2234 out of 4096 PCS are required for AlexNet to capture the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Experiments on AlexNet: Convolutional Layers v.s. FC Layers</head><p>From the experiments in the main paper, we found that FC layers have significant influence on accuracy loss, model size and memory usage. To exploit the impact of pruning FC layers and convolutional layers, we conduct experiments on pruning half of the neurons in FC layers and some convolutional layers. We categorize the 5 convolutional layers into three-level feature extractors: low (Conv1-Conv2 layers), middle (Conv3 layer) and high (Conv4-Conv5 layers). Figure <ref type="figure">11</ref> displays learning curves and shows that although FC layers are important in AlexNet, powerful local feature extractors (more kernels in convolutional layers) can compensate the loss from pruning neurons in FC layers, or even achieve better predictive power (High and Low curves).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Experiments on GoogLeNet</head><p>The learning curves for "no Reduce" is shown in Figure <ref type="figure" target="#fig_3">12</ref>. We observe that our importance based pruning method The pruning ratio is 50%. We prune all layers but the reduction layers in the inception modules. importance based pruning method converges much faster and can achieve the smallest accuracy loss.</p><p>leads to better initialization, faster convergence and smaller final accuracy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9.">Layer-wise Improvements</head><p>In our experiments of AlexNet on Titan X, the empirical computation time for the intermediate layers (all layers except for convolutional layers and FC layers) accounts for 17% of the entire testing time; therefore, those layers must be considered as well while designing an acceleration method. One of our advantages over existing methods is that all layers in the network can be sped up due to the fact that the data volume or feature dimension at every layer is</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Learning curves of random pruning and training from scratch baselines and NISP using different CNNs on different datasets. The pruning ratio of neurons and filters is 50%. Networks pruned by NISP (orange curves) converge the fastest with the lowest accuracy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison with layer-by-layer (LbL) and magnitude based (Mag) pruning baselines. We prune 50% of neurons and filters in all layers for both CNNs. NISP-FS outperforms NISP-Mag and LbL in terms of prediction accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Evaluations for different pruning ratios (a) LeNet: pruning 75% and 90%, (b) AlexNet: pruning 75%. CNNs pruned by NISP converge fastest with the lowest accuracy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. importance propagation: Convolutional layer. X = 4, Y = 2, k = 3, s = 1.Fixing the f th input channel and c th output channel, the upper-left X-by-X grid is the corresponding input feature map, and the upper-right Y-by-Y grid is the output map after convolution is applied. k f c is the corresponding 2D convolutional kernel. Given the flattened importance vector for the output feature map Sout f,c , we use BPconv to propagate the importance and obtain S in f,c , which contains the importance of the input feature map. The structure of BPconv is determined by the kernel size k and stride s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>? b p . . . b p . . . b p b p . . . b p . . . b p . . . b p . . . b p . . . b p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. NISP: Pooling layer. X = 4, Y = 2, k = 2, s = 2.The upper-left X-by-X grid is the f th feature map of the input channel, and the upper-right Y-by-Y grid is the output channel after pooling is applied. Given the importance vector Sout f , we use BP pooling to propagate the importance and obtain S in f , which contains the importance of each position of the input feature map. The structure of BP pooling relates to the kernel size k and stride s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>b p , there are k 1 s since each element in the output layer is pooled from a region with width k of the input layer. The offset of the beginning of the 1 s is the stride s. The entire propagation matrix BP pooling is a block matrix with each submatrix being a Y ? X matrix of either b p or a zero matrix. Each row of BP pooling has k b p s because the height of pooling filter is k. The offset of the beginning of the k b p s is the stride s. The ones in b p will be normalized by the number of positions covered by a pooling filter (the same for LRN layers shown below). The other elements are all zeros. We use the case that X = 4, Y = 2, k = 2, s = 2 as an example shown in Figure8. 6.4.3 Local Response Normalization Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Importance propagation: LRN layer (cross-channel). l = 3, N = 5. The red vector is the cross-channel vector at spatial position (i, j) of the input tensor, and the yellow vector is the cross-channel vector at the same position of the output tensor after LRN is applied. Given the Sout ij , we use BPLRN to propagate the importance and obtain S in ij , which contains the importance of each position of the input feature map. The structure of BPLRN relates to the local size l and number of channels N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. PCA accumulated energy analysis: LeNet on MNIST (a) and AlexNet on ImageNet (b). The y axis measures the PCA accumulated energy. The x axis shows the number of PCs.</figDesc><graphic url="image-22.png" coords="12,51.23,76.98,115.76,86.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. Learning Curves of AlexNet on ImageNet: The subscript CF means we prune both convolutional kernels and neurons in FC layers, and C means we only prune convolutional kernels. High, Mid and Low mean we prune the entire CNN except for the high/middle/low level convolutional layers (Conv4-Conv5, Conv3 and Conv1-Conv2 respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>? ? FRL ? ? Input Backward 0.18 0.98 0.75 0.23 0.62 0.87 0.56 0.71 0.12 0.56 0.91 0.81 0.20 0.11 0.07 0.88 0.71 0.92 0.61 0.79</head><label></label><figDesc></figDesc><table><row><cell>? ?</cell></row><row><cell>Figure 2. We propagate the neuron importance from the final re-</cell></row><row><cell>sponse layer (FRL) to previous layers, and prune bottom-ranked</cell></row><row><cell>neurons (with low importance scores shown in each node) given</cell></row><row><cell>a pre-defined pruning ratio per layer in a single pass. The impor-</cell></row><row><cell>tance of pruned neurons (with backslash) is not propagated.</cell></row><row><cell>https://www.mathworks.com/matlabcentral/fileexchange/</cell></row><row><cell>54763-infinite-feature-selection-2016.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 5. Weighted Average Reconstruction Error (WARE) on the final responses without fine-tuning: we set pruning ratios as 25% and 50% and evaluate the WARE on the final responses of models with different depths pruned using NISP or LbL. It is clear that networks pruned by NISP have the lowest reconstruction errors.</figDesc><table><row><cell>Average Reconstruction Error</cell><cell>0.05 0.1 0.15 0.2 0.25 0.3</cell><cell>NISP-25% LbL-25% NISP-50% LbL-50%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">3 Number of Conv Layers 4 5 6</cell><cell>7</cell></row><row><cell></cell><cell>Model</cell><cell cols="4">Accu.?% FLOPs?% Params.?%</cell></row><row><cell>AlexNet</cell><cell>NISP-A</cell><cell></cell><cell>1.43</cell><cell>67.85</cell><cell>33.77</cell></row><row><cell cols="3">on ImageNet Perforated [11]</cell><cell>2.00</cell><cell>50.00</cell><cell>-</cell></row><row><cell></cell><cell>NISP-B</cell><cell></cell><cell>0.97</cell><cell>62.69</cell><cell>1.96</cell></row><row><cell></cell><cell cols="2">Tucker [20]</cell><cell>1.70</cell><cell>62.55</cell><cell>-</cell></row><row><cell></cell><cell>NISP-C</cell><cell></cell><cell>0.54</cell><cell>53.70</cell><cell>2.91</cell></row><row><cell></cell><cell cols="2">Learning [36]</cell><cell>1.20</cell><cell>48.19</cell><cell>-</cell></row><row><cell></cell><cell>NISP-D</cell><cell></cell><cell>0.00</cell><cell>40.12</cell><cell>47.09</cell></row><row><cell>GoogLeNet</cell><cell>NISP</cell><cell></cell><cell>0.21</cell><cell>58.34</cell><cell>33.76</cell></row><row><cell cols="3">on ImageNet Tucker [20]</cell><cell>0.24</cell><cell>51.50</cell><cell>31.88</cell></row><row><cell>ResNet</cell><cell>NISP-56</cell><cell></cell><cell>0.03</cell><cell>43.61</cell><cell>42.60</cell></row><row><cell cols="2">on CIFAR10 56-A [25]</cell><cell cols="2">-0.06 5</cell><cell>10.40</cell><cell>9.40</cell></row><row><cell></cell><cell>56-B [25]</cell><cell></cell><cell>-0.02</cell><cell>27.60</cell><cell>13.70</cell></row><row><cell></cell><cell>NISP-110</cell><cell></cell><cell>0.18</cell><cell>43.78</cell><cell>43.25</cell></row><row><cell></cell><cell cols="2">110-A [25]</cell><cell>0.02</cell><cell>15.90</cell><cell>2.30</cell></row><row><cell></cell><cell cols="2">110-B [25]</cell><cell>0.23</cell><cell>38.60</cell><cell>32.40</cell></row><row><cell>ResNet</cell><cell cols="2">NISP-34-A</cell><cell>0.28</cell><cell>27.32</cell><cell>27.14</cell></row><row><cell cols="3">on ImageNet NISP-34-B</cell><cell>0.92</cell><cell>43.76</cell><cell>43.68</cell></row><row><cell></cell><cell cols="2">Res34 [25]</cell><cell>1.06</cell><cell>24.20</cell><cell>-</cell></row><row><cell></cell><cell cols="2">NISP-50-A</cell><cell>0.21</cell><cell>27.31</cell><cell>27.12</cell></row><row><cell></cell><cell cols="2">NISP-50-B</cell><cell>0.89</cell><cell>44.01</cell><cell>43.82</cell></row><row><cell></cell><cell cols="2">Res50 [29]</cell><cell>0.84</cell><cell>36.79</cell><cell>33.67</cell></row><row><cell cols="6">Table 1. Compression Benchmark. [Accu.?%] denotes the abso-</cell></row><row><cell cols="6">lute accuracy loss; [FLOPs?%] denotes the reduction of computa-</cell></row><row><cell cols="6">tions; [Params.?%] demotes the reduction of parameter numbers;</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://code.google.com/p/cuda-convnet/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>See supplementary materials for the results of noReduce.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>A negative value here indicates an improved model accuracy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The research was partially supported by the <rs type="funder">Office of Naval Research</rs> under Grant <rs type="grantNumber">N000141612713</rs>: <rs type="affiliation">Visual Common Sense Reasoning for Multi-agent Activity Prediction and Recognition</rs>.</p></div>
			</div>
			<div type="funding">
<div> *   <p>This work was done while the author was at <rs type="institution">University of Maryland</rs>. ? This work was done while the author was at IBM.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VWVGNNG">
					<idno type="grant-number">N000141612713</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>reduced. For example, by pruning kernels in convolutional layers, we reduce the number of both output channels of the current layer and input channels of the next layer. In theory, given a pruning ratio of 50%, except for the first layer whose input channels cannot be pruned, all of the convolutional layers can be sped up by 4?. The intermediate pooling, non-linearity and normalization layers have a theoretical speedup ratio of around 2?. The layer-wise acceleration ratios (both theoretical and empirical) of our method when the pruning ratio 50% for both convolutional layers and FC layers are shown in Figure <ref type="figure">13</ref>. We that the and empirical speedup almost the same for pooling, non-linearity and normalization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An exploration of parameter redundancy in deep networks with circulant projections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="2857" to="2865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flexible, high performance volutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cires ?an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, IJCAI&apos;11</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence, IJCAI&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient transfer learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>CoRR, abs/1611.06440</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05282</idno>
		<title level="m">C-wsl: Count-guided weakly supervised localization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic zoom-in network for fast object detection in large images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual try-on network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia, MM&apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based applied to document tion</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent signal processing</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="306" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating holistic 3d scene abstractions for text-based image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Penksy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards unified data and lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 33rd International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Infinite feature selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Roffo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4202" to="4210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data-free parameter pruning for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="31" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning the architecture of deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="104" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep regionlets for object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>CoRR, abs/1712.02408</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Template regularized sparse coding for face verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition, ICPR 2016</title>
		<meeting><address><addrLine>Canc?n, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 4-8, 2016. 2016</date>
			<biblScope unit="page" from="1448" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bridging the domain shift by domain adaptive dictionary learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2015, BMVC 2015</title>
		<meeting>the British Machine Vision Conference 2015, BMVC 2015<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 7-10, 2015. 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="96" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep fried convnets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="1476" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The role of context selection in object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Remotenet: Efficient relevant motion event detection for large-scale home surveillance videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning rich features for image manipulation detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
