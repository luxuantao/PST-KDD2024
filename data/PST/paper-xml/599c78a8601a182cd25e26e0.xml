<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<email>weizhao_90@163.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center of Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">Ministry of Education of China</orgName>
								<orgName type="department" key="dep3">School of Electronic Engineer-ing</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Perception and Image Understanding</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center of Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">Ministry of Education of China</orgName>
								<orgName type="department" key="dep3">School of Electronic Engineer-ing</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Perception and Image Understanding</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
							<email>lchjiao@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center of Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">Ministry of Education of China</orgName>
								<orgName type="department" key="dep3">School of Electronic Engineer-ing</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Perception and Image Understanding</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5470681F483A1DE3D874774D3F95BCA</idno>
					<idno type="DOI">10.1109/TGRS.2017.2689018</idno>
					<note type="submission">received September 2, 2016; revised February 24, 2017; accepted March 20, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolution neural network (CNN)</term>
					<term>image classification</term>
					<term>multiple local regions joint representation</term>
					<term>panchromatic and multispectral (MS) images</term>
					<term>superpixel-based</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, very high resolution (VHR) panchromatic and multispectral (MS) remote-sensing images can be acquired easily. However, it is still a challenging task to fuse and classify these VHR images. Generally, there are two ways for the fusion and classification of panchromatic and MS images. One way is to use a panchromatic image to sharpen an MS image, and then classify a pan-sharpened MS image. Another way is to extract features from panchromatic and MS images, respectively, and then combine these features for classification. In this paper, we propose a superpixel-based multiple local convolution neural network (SML-CNN) model for panchromatic and MS images classification. In order to reduce the amount of input data for the CNN, we extend simple linear iterative clustering algorithm for segmenting MS images and generating superpixels. Superpixels are taken as the basic analysis unit instead of pixels. To make full advantage of the spatial-spectral and environment information of superpixels, a superpixel-based multiple local regions joint representation method is proposed. Then, an SML-CNN model is established to extract an efficient joint feature representation. A softmax layer is used to classify these features learned by multiple local CNN into different categories. Finally, in order to eliminate the adverse effects on the classification results within and between superpixels, we propose a multi-information modification strategy that combines the detailed information and semantic information to improve the classification performance. Experiments on the classification of Vancouver and Xi'an panchromatic and MS image data sets have demonstrated the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N OWADAYS, satellites can collect very high spatial reso- lution panchromatic (PAN) image and multispectral (MS) image, such as QuickBird (2.44-m MS and 0.61-m PAN images), IKONOS (4-m MS and 1-m PAN images), SPOT 5 (10-m MS and 5-m PAN images), or DEIMOS-2 (4-m MS and 1-m PAN images) <ref type="bibr" target="#b0">[1]</ref>. The resolution of MS image is often lower than that of panchromatic image. But the MS image has more spectral information than the corresponding PAN image. In order to achieve more accurate classification results, we can make full use of the information of the PAN and MS images. So, a good data processing strategy is an essential step for the classification task. In the literature, most of PAN and MS images classification methods can be grouped into two classes; one is to pan-sharpen the MS images and then to classify them <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b8">[9]</ref>, and the other is to extract features from PAN and MS images, respectively, and then to classify these features into different categories <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p><p>The pan-sharpening then classification method uses PAN to sharpen MS images, then classify the sharpened MS images. A good sharpening method can improve the classification results. Various pan-sharpening methods have been proposed, such as <ref type="bibr" target="#b13">[14]</ref>, which is a component substitution strategy based on the Gram-Schmidt transformation. The MS bands are multiplied by the PAN image to enhance the spatial information in Multiplicative <ref type="bibr" target="#b14">[15]</ref>. High-pass filter <ref type="bibr" target="#b15">[16]</ref> combines the high-frequency components in the PAN image and the lowfrequency components in the MS bands. The first principal component of the MS bands is replaced by the PAN image with the fusion method of principal component analysis <ref type="bibr" target="#b16">[17]</ref>. Wavelet transform (WT) <ref type="bibr" target="#b17">[18]</ref> and contourlet transform <ref type="bibr" target="#b18">[19]</ref> can also be used for pan-sharpening. References <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> use deep learning model to pan-sharpen MS images. Shackelford and Davis <ref type="bibr" target="#b1">[2]</ref> use a color normalization method <ref type="bibr" target="#b19">[20]</ref> to pan-sharpening MS images; then they are first using traditional maximum-likelihood approach to classify pan-sharpening MS images. And a number of different texture measures are used to increase the discrimination of spectrally similar classes. Finally, they propose a fuzzy logic methodology to improve classification accuracy. Huang et al. <ref type="bibr" target="#b2">[3]</ref> and Shingare et al. <ref type="bibr" target="#b3">[4]</ref> evaluate many kinds of pan-sharpening methods; then they use several kinds of information indices, such as NDWI, NDVI, MBI, MSI, NDBI, and SAVI to classify the pan-sharpening MS images. Tuia et al. <ref type="bibr" target="#b4">[5]</ref> propose two variations of active learning models to classify the pan-sharpening MS images. The first is the margin sampling by closest support vector, and the second is the entropy query-by-bagging. Pham et al. <ref type="bibr" target="#b5">[6]</ref> use local texture characterization to classify the pan-sharpening MS images. They extract characteristic pixels from MS images, then a weighted graph is used to combine these feature pixels, and finally, textural features can be extracted from this graph and used to classify the MS images. Wei et al. <ref type="bibr" target="#b6">[7]</ref> use textural and local spatial statistics to classify the pan-sharpening MS images.</p><p>The second part of PAN and MS images classification methods extract features from PAN and MS images separately, and then these features are used to classify PAN and MS images. Mao et al. <ref type="bibr" target="#b9">[10]</ref> present a unified Bayesian framework to probabilistically generate both PAN and MS images. Then the rich spatial information in PAN and the fine spectral information in MS are, respectively, used by the processes of locally semantic segmentation and globally image clustering. Finally, they obtain the effective classification results. Moser and Serpico <ref type="bibr" target="#b10">[11]</ref> present a novel contextual method based on Markov random fields to fuse and classify PAN and MS images. They use a linear mixture model and a graphcut approach to Markovian energy minimization to generate a contextual classification result. Reference <ref type="bibr" target="#b12">[13]</ref> uses the similar method to fuse and classify PAN and MS images. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> combine the midlevel bag-of-visual-words representation and the histogram intersection kernel. Then, these features are sent into SVM for images classification.</p><p>These methods first extract features from the given images, and then classify these features into different classes by learning classifier or clustering. In recent years, deep learning <ref type="bibr" target="#b20">[21]</ref> has achieved a great success in the field of image processing, especially for image classification and object recognition. And, it is also a very efficient way for image feature extraction. Generally, deep learning models are used to process nature images. However, Zhang et al. <ref type="bibr" target="#b21">[22]</ref> provide a tutorial on deep learning models for remote-sensing data. Xie et al. <ref type="bibr" target="#b22">[23]</ref> use deep learning model to denoise and inpaint image. Huang et al. <ref type="bibr" target="#b23">[24]</ref> use deep neural networks to pan-sharpening remote-sensing image. In <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b31">[32]</ref>, deep learning models are used to process remote-sensing image classification task. Remote-sensing image target recognition and scene classification can also use deep learning models, such as <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b38">[39]</ref>. These papers show that deep learning models can be used to process remote-sensing image. So, we use deep learning model to obtain the classification results in this paper.</p><p>In this paper, we extend simple linear iterative clustering (SLIC) <ref type="bibr" target="#b39">[40]</ref> to generate superpixels and present a multiple local regions joint representation convolution neural network (CNN) model to extract MS images features and use softmax to classify it, and then the semantic information of land covers and the detailed information of PAN are combined to modify the classification results. To reduce the amount of data, superpixel is taken as the basic unit of classification. The extended SLIC (e-slic), which is more suitable for MS images than original SLIC (o-slic), is used to generate superpixels. Inspired by <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref>, we propose a multiple local regions joint representation CNN model. And, compared with <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref>   regions CNN model, which use above-mentioned six local regions as the input data to extract MS images features. And, a softmax classifier is used to classify each superpixel. In order to obtain more accurate classification results, a multiinformation modification strategy is presented. The detailed information of the PAN image is applied to modify pixel level errors, which are generated by e-slic. The semantic relation information of each land cover is used to modify superpixel level errors, which are produced by classification error. We compared the superpixel-based multiple local CNN (SML-CNN) model, the general CNN model (single region), and pixel-based CNN (pixel-cnn) model in the experiment, and the results show that the SML-CNN model is effective.</p><p>The remainder of this paper is organized as follows. The methodology is introduced in Section II. The experimental results are discussed in Section III. The conclusion is summarized in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SUPERPIXEL-BASED MULTIPLE LOCAL CNN MODEL</head><p>In this section, the proposed method is introduced in detail, including superpixel generation strategy, multiple local regions joint representation CNN model, and multi-information modification strategy. The whole framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Superpixel Generation Strategy</head><p>In this section, superpixel generation strategy is described in detail. For remote-sensing image classification, some methods are based on pixels, and the other methods are based on superpixels. In this paper, the CNN model is adopted to extract features from MS images. So, if pixels have been used as the basic unit of classification, then the efficiency of this method is very low. In order to reduce the data size for training CNN, superpixel is taken as the basic analysis unit instead of pixel. It is required that the superpixel should have good internal consistency, and each superpixel should have similar size. Superpixels are generated by SLIC <ref type="bibr" target="#b39">[40]</ref>, which has the above-mentioned properties. So, we decide to use SLIC to produce superpixels. It is generally known that SLIC is used for natural image oversegmentation. However, unlike natural images, MS images often have more than three bands. If o-slic is used to segment MS images, only three bands can be used for superpixel segmentation. To better consider all bands available, we extend the o-slic for segmenting MS images with more than three bands.</p><p>o-slic is simple to use and understand, and it uses local k-means clustering to generate superpixels. A pixel's color is converted to CIELAB color space [l a b]. The pixel's position is set as [x y]. Euclidean distance is used to measure the spectral and spatial distance between each pixel. The spectral distance can be defined as</p><formula xml:id="formula_0">d c = (l i -l j ) 2 + (a i -a j ) 2 + (b i -b j ) 2 .</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>The spatial distance can be defined as</p><formula xml:id="formula_2">d s = (x i -x j ) 2 + (y i -y j ) 2<label>(2)</label></formula><p>where N is the total pixel number, and k is the superpixel number, then the superpixel size should be S 2 = N/k. The combined distance D is defined as</p><formula xml:id="formula_3">D = d c m 2 + d s S 2 (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where m is defined as a parameter to control the relative weight between d c and d s .</p><p>MS images contain four bands, including r, g, b, and nir. The r, g, and b bands can be converted to CIELAB color space and used directly to calculate CIELAB color space distance d c . Because of the fact that the nir band is not belonging to CIELAB color space, we cannot combine nir with [l a b] into [l a b nir] to calculate the color space distance d c . So, the distance of the nir band must be calculated independently. However, there is only one nir value [nir] for each pixel. If only the [nir] is used to calculate the nir band distance d nir (d nir = ((nir inir j ) 2 ) 1/2 ), that distance d nir is susceptible to noise. Here, the correlation distance between the 3 × 3 neighborhood of pixel i and pixel j is used to replace the Euclidean distance as the nir space distance d nir of two pixels. The nir space distance can be defined as</p><formula xml:id="formula_5">d nir = 1 - Cov(N I R i , N I R j ) √ D(N I R i ) D(N I R j ) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where N I R i is a vector that is composed of all the pixels in 3 × 3 neighborhood of the pixel i . The whole distance of lab, nir , and x y space is defined as</p><formula xml:id="formula_7">D w = D + d nir = d c m 2 + d s S 2 + d nir .</formula><p>(5) For MS image, the e-slic uses C i = [l i a i b i N I R i x i y i ] as the initial cluster center and D w as the distance measure. The other parts of e-slic are as similar as the o-slic algorithm. At last, the superpixels are generated by the e-slic algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple Local Regions Joint Representation CNN Model</head><p>Nowadays, deep learning plays a huge role in image processing field. Among those deep learning models, the CNN is one of the most representative models. It can be very effective for image interpretation. It is generally known that remote-sensing image classification task is mainly composed of two parts, feature extraction and classification. So, CNN will be used to extract the feature of MS image in this paper.</p><p>In Section II-A, the MS image is divided into superpixels by e-slic. If only the superpixel itself is fed into the CNN, then only the spectral information and internal structure information are used for classification, which is limited. Therefore, the surrounding environment of superpixel should be used as the additional information to enhance the performance of image classification. So, a multiple local regions joint representation CNN model, which is inspired by <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref>, is proposed, and the architecture of this model can be seen in Fig. <ref type="figure" target="#fig_3">3</ref>. It consists of three parts, the details are as follows.</p><p>1) Multiple Local Regions Joint Representation: Multiple local regions joint representation well describes the characteristic of the superpixel R i itself and the semantic environment in which the superpixel may exist. So, we utilize multiple local regions to jointly represent these superpixels {R i } k i=1 mainly for two purposes. First, it helps to distinguish the superpixels, which have similar spectral information and different semantic environment information. For example, R i is a boat and R j is a building, assuming that these superpixels have similar spectral information and are red. It is difficult to distinguish these superpixels. However, semantic environment information, say boat is shipped on water or building is on the ground, can modify the negative influence of spectral information. Second, multiple local regions joint representation helps to analyze the same class superpixels, which have different spectral information and similar semantic environment information. For example, R i and R j represent two different types of boats, although these superpixels have different spectral information, but stay in the same environment. In this case, the semantic environment information becomes very important. We utilize three types of local regions to jointly represent each superpixel, and the details are as follows.</p><p>a) Central region: We know that the input data of the CNN must be a rectangle region. However, the superpixel, which is extracted by the e-slic, is an irregular region. So, we use superpixel R as the center to build a rectangular region as central region R c . The pixels value of superpixel R at the center is invariable, and that of other locations are all the same and set to the average value of the MS images. The CNN model trained on central region is guided to extract the pure spectral features of each superpixel. These features are not affected by the environment surrounding each superpixel.</p><p>b) Original region: We take superpixel R as the center to extract original MS image blocks as an original region R o . The CNN model trained on this region is guided to extract the whole spectral information and semantic environment information of superpixel R. All of these local regions are used as input data for the CNN model and shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>2) Multiple Local Regions Feature Extraction: In order to obtain effective features, the CNN is acted as a feature extractor for each local region. We know that the CNN is a hierarchical neural network, which are inspired by biolog-ical research results. Generally, the CNN contains multiple convolution processes and fully connected process. For each convolution process, it consists of four parts, which are convolution (conv) layer, pooling layer, nonlinear transformation layer, and local response normalization layer. Here, we use central region R c as an example to describe the each part of convolution process. Let R c ∈ R h×w×c be the input image of the CNN, where each dimension is the height, the width, and the channels. Convolution layer computes the convolution of the input image with filter kernels W and adds a bias b where ⊗ is convolution process, and Z ∈ R h ×w ×c is the output. The rectified linear unit (ReLU) <ref type="bibr" target="#b42">[43]</ref> is used as the nonlinear transformation layer</p><formula xml:id="formula_8">Z = W ⊗ R c + b (6)</formula><formula xml:id="formula_9">Z = max {0, Z }.<label>(7)</label></formula><p>For local response normalization layer <ref type="bibr" target="#b43">[44]</ref>, it has to be applied after the ReLU</p><formula xml:id="formula_10">Ẑ i x,y = Z i x,y / ⎛ ⎝ k + α min(N-1,i+n/2) j =max(0,i-n/2) Z i x,y 2 ⎞ ⎠ β (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where the Z i x,y values denote the activity of a network computed by kernel i at position (x, y) after applying the ReLU, Ẑ i x,y is the activity of local response normalization, N is the total number of kernels, the constants k, n, α, and β are hyperparameters. Here, the max-pooling is selected to downsampling</p><formula xml:id="formula_12">Ẑ Z = max h× w,s { Ẑ }<label>(9)</label></formula><p>where h × w is the subwindow, and s is the step size of sliding window. The fully connected layers are linked with the CNN behind it, and then the whole local region feature-extraction CNN is defined as</p><formula xml:id="formula_13">f R c = (R c , θ) (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where is contained convolution process and fully connected process, R c is the input data, θ consists of W and b, and</p><formula xml:id="formula_15">f R c ∈ R 1×l represents the feature of R c .</formula><p>Then, the features in multiple local regions are extracted by the CNN model, the structure of this model is shown in Fig. <ref type="figure" target="#fig_3">3</ref>, and the details are described in Section III-B2.</p><p>3) Multiple Local Regions Feature Fusion: By Section II-B2, we obtain the features of each local region, and these features can be combined to represent the corresponding superpixel R. However, it is difficult to make use of these features directly. Therefore, it is necessary to fuse these features and reduce features dimension. Because the features of each local region are composed of vector, here we use autoencoding (AE) <ref type="bibr" target="#b44">[45]</ref> network to fuse these features and reduce the dimension. First, these features vectors are cascaded with others as a whole feature vector</p><formula xml:id="formula_16">f = [ f R c , f R o , f R tl , f R tr , f R bl , f R br ],</formula><p>and the f is acted as the input data. Then, a hierarchical AE network is established to fuse these features and reduce the dimension, and f is acted as the input data of this network. The output of this network is the final feature representation of the corresponding superpixel R. Finally, the softmax layer is used to predict the class of each local region. The structure is shown in Fig. <ref type="figure" target="#fig_3">3</ref> and the details are shown in Section III-B2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Information Modification Strategy</head><p>We can obtain initial classification results through multiple local regions joint representation CNN model. However, the error analysis shows that there are two kinds of errors in the initial results. The detailed information and semantic information are used to modify these errors.</p><p>1) Pixel Level Error: It exists inside of superpixel. In this paper, an e-slic is used to generate superpixel, which is acted as the basic unit of classification. Because of the fact that the size of superpixel is limited, it cannot be too large or too small. Therefore, it may contain more than one kind of land covers in the superpixel. Panchromatic image has a higher spatial resolution and more detailed information than MS images, and so it is used to modify pixel level error. We will first map the superpixels of MS images to panchromatic image. In our method, every superpixel is resegmented to few parts. The largest part represents the superpixel, and the hole inside of this part is eliminated directly. The other parts are integrated in the neighboring superpixels. Then, the pixel level error will be modified at some degree.</p><p>2) Superpixel Level Error: Due to the fact that the classification error always exists in classification results, once a superpixel is classified into error class, then all pixels in this superpixel will be classified into error category. In order to modify this kind of error, we construct the semantic relation matrix S of land cover. The matrix S contains the semantic information between each land cover. For example, boat will not stay in the forest. So, if the superpixel R, which represents boat, is surrounded by superpixels, which represent forest, then the category of superpixel R can be considered as a wrong category. At last, the category of the superpixel R is modified as forest. These are superpixel level error and modification. However, this approach is not entirely effective to modify superpixel level error. In the future work, we will consider two ways to reduce this kind of error. One is to establish a more effective classification model, and the other one is to propose a novel modification strategy.</p><p>For these errors, we first deal with pixel level error, and then superpixel level error is processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND DISCUSSION</head><p>In this section, the experimental results and analysis are given. We compare the classification accuracy about using multiple local regions method, single-region method, o-slic + multiple local regions method, pixel-cnn method, and the method of <ref type="bibr" target="#b12">[13]</ref>. The single-region method applies central region and original region as CNN input data, respectively. The impact of all components and hyperparameters of the proposed method on classification accuracy is analyzed. The  experiments are run on HP-Z820 Workstation with TITAN X 12-GB GPU. The multiple local regions feature extraction and feature fusion models are trained on caffe <ref type="bibr" target="#b45">[46]</ref>. Other steps in our experiments are implemented using MATLAB. Table <ref type="table">V</ref> shows the means and the standard deviations of the values of OA and Kappa over these ten runs for all data sets and for  both the proposed method and the technique described in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Table VI shows the McNemar s test for all data sets and for both the proposed method and the technique described in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>The proposed multiple local regions joint representation CNN model is validated on two panchromatic and MS remote-sensing image data sets: gr ss_d f c_2016 data set and xi'an_Q B data set. The two data sets are discussed in detail in the following.</p><p>1) Vancouver: gr ss_d f c_2016 <ref type="bibr" target="#b46">[47]</ref>. It is provided by The 2016 IEEE GRSS Data Fusion Contest. These data sets were acquired on March 31, 2015 and May 30, 2015, over Vancouver, Canada (49 • 15'N 12 • 36'W), from the DEIMOS-2 satellite. The satellite of DEIMOS-2 operates from a Sun-synchronous orbit at a mean altitude of 620 km. DEIMOS-2 carries a push-broom with a very high-resolution camera with five spectral channels (one panchromatic, four MS with red, green, blue, and NIR bands). For this data, four groups of images are provided, each of which contains a panchromatic image at 1-m resolution and an MS image (R, G, B, and NIR) at 4-m resolution, two groups at levels 1B (a calibrated and radiometrically corrected product, not resampled) and others at 1C (a calibrated and radiometrically corrected product, manually orthorectified and resampled to a map grid). Level 1C images cover exactly the same ground area for both the dates. In this paper, the proposed method is validated on two groups images, a large scenes level 1B image group, and a small scenes level 1C image group. These images are shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a) and (b).</p><p>For level 1B images, the MS images consist of 3249 × 2928 pixels, which is 4-m resolution, and the panchromatic image consists of 12996 × 11712 pixels, which is 1-m resolution. The scene was divided into 11 categories, which consist of vegetation, four kinds of construction areas, boat, road, port, bridge, tree, and water. We manually built a ground truth with labels summarized in Fig. <ref type="figure" target="#fig_4">4(a)</ref>.</p><p>For level 1C images, the MS images consist of 1311 × 873 pixels, which is 4-m resolution, and the panchromatic image consists of 5244 × 3492 pixels, which is 1-m resolution. The scene was divided into eight categories, which consist of vegetation, three kinds of construction areas, boat, road, tree, and water. We manually built a ground truth with labels summarized in Fig. <ref type="figure" target="#fig_4">4(b)</ref>.</p><p>2) Xi'an: xi 'an_Q B. The imaging data were acquired on May 30, 2008, over Xi'an, China, from the QuickBird satellite. For this data, it contains a panchromatic image at 0.61-m resolution and an MS image (R, G, B, and NIR) at 2.44-m resolution, and it covers a Xi'an area of 200 km 2 . This data set consists of an urban area and a part of the suburban area. We extract two different areas as the experiment data shown in Fig. <ref type="figure" target="#fig_4">4(c)</ref> and<ref type="figure">(d)</ref>.</p><p>The first area is a suburban area, which is located in the southwest corner of Xi'an. The MS images consist of 1650 × 1550 pixels, and the panchromatic image consists of 6600 × 6200 pixels. The scene was divided into eight categories, which consist of two kinds of vegetation, four kinds of construction areas, road, and land. We manually built a ground truth with labels summarized in Fig. <ref type="figure" target="#fig_4">4(c)</ref>.</p><p>The second area is an urban area, which is located in the east of Xi'an. The MS images consist of 800 × 830 pixels, and the panchromatic image consists of 3200 × 3320 pixels. The scene was divided into six categories, which consist of construction areas, road, tree, soil, flat land, water, and shadow. The flat land represents a variety of types of ground except soil. We manually built a ground truth with labels summarized in Fig. <ref type="figure" target="#fig_4">4(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results With Vancouver Level 1B Image</head><p>The proposed method consists of three parts, which are superpixel generation strategy, multiple local regions joint representation CNN model, and multi-information modification strategy. So, we show the results and the analysis of Vancouver level 1B images at these three parts.</p><p>1) Superpixels Generation Strategy: We use SLIC to generate superpixels. The o-slic is designed for segmenting nature image including r, g, and b bands. However, the experimental data are MS images, which consist of r, g, b, and nir bands.   the bottom-left corner of the result map, it is clear that e-slic is more sensitive for the spectral change of MS image. The e-slic can circle a small boat at the bottom-left corner of the result map, but o-slic) cannot. These results show that e-slic, which use more band information to segment image, is more suited to generate superpixels.</p><p>The size of the Vancouver level 1B image is, 3249 × 2928 pixels. S is set as 5. The e-slic is generated 377 815 superpixels that 134 422 of which are labeled, and improve the OA of classification result of 1.21% that shows in Table <ref type="table" target="#tab_0">I</ref>. In Table <ref type="table" target="#tab_0">I</ref> For these six local regions, the feature extraction CNNs have the same structure. For each CNN, it has two conv layers, two pooling layers, and two fully connection layers. The kernel size and stride of each conv layers are 5 × 5 and 1. The conv feature map quantity is 20 and 40. The kernel size and stride of each pooling layer are 2 × 2 and 1. The size of two fully connected layers is 500 and 200. These layers are connected ReLU, local response normalization, and dropout <ref type="bibr" target="#b47">[48]</ref> layer. The rate of dropout is 0.5.</p><p>The multiple local regions features are fused by four-layer AE feature fusion networks. The size of each layer is 1200, 2000, 800, and 300. ReLU and dropout are used in this feature fusion network. The rate of dropout is 0.5. At last, a softmax is used to classify input data.</p><p>In these experiments, we randomly select 25% labeled data as training samples, and the others are used as test samples. The total sample number is 134 422, training samples have 33 605, and test samples have 100 817. Caffe is used to train this model. The initial learn rate is 0.01. Batch size is 256. The iteration number is 50 000. The weight of convolutional kernels and fully connected layers is initialized by Glorot and Bengio <ref type="bibr" target="#b48">[49]</ref>. The bias is used a constant value 0. The training time is 3000 s. The testing time is 7013 s.</p><p>The initial classification results of multiple local regions (m-lr) joint representation CNN model are shown in Fig. <ref type="figure" target="#fig_6">6</ref> and Table <ref type="table" target="#tab_0">I</ref>. In order to verify the effectiveness of multiple local regions joint representation, we compare the results with the general CNN model, which only use central region (c-r) and original region (o-r), respectively, and pixel-cnn model. The details are shown in Figs. <ref type="figure" target="#fig_6">6</ref> and<ref type="figure" target="#fig_7">7</ref> and Table <ref type="table" target="#tab_0">I</ref>.</p><p>The c-r results show that boat, road, and bridge have very low accuracy. It contains two reasons. First, it only uses the central region that does not contain the surrounding information. Second, it has only a small amount of training data. For o-r results, it has surrounding information that improve overall accuracy rate by over 8.8%. The m-lr (initial) classification results show that multiple local regions joint representation is effective. It improves overall accuracy rate by 1.79%. And, it can better identify boat, road, port, and bridge. The accuracy rate of these classes is improved by about 10%. It shows that multiple local regions joint representation is conducive to the classification of a small sample. From Table <ref type="table" target="#tab_0">I</ref>, the result of the super pixel-cnn is much better than that of pixel-cnn. The OA of pixel-cnn is 86.81%, and the Kappa is 80.84%.</p><p>3) Multi-Information Modification Strategy: In order to reduce the influence of superpixels on the classification results, multi-information modification strategy is used to modify the pixel level error and the superpixel level error. We first deal with the pixel level error and then deal with the superpixel level error. For pixel level error, the Maximum Between-Class Variance (Otsu method) <ref type="bibr" target="#b49">[50]</ref> is used to segment pixels in each superpixel. Then, these pixels are reassigned in other superpixels. For superpixel-level error, semantic relation matrix is used to reassign each superpixel. The results are shown in Fig. <ref type="figure" target="#fig_6">6</ref>(k) and (l) and Table <ref type="table" target="#tab_0">I</ref>. The OA is improved by 0.13% and the Kappa is improved by 0.17%. These steps are more effective for building, boat, port, road, and bridge. It shows that this strategy is sensitive to complex scenes. However, this approach to enhance the overall effect is not obvious. So, in the future, we will design a more effective strategy to improve initial classification results.</p><p>For traditional method, the OA and the Kappa of <ref type="bibr" target="#b12">[13]</ref> are not good enough. The classification results of some cate-gories are good and others are bad. For Vancouver Level 1B image, the accuracy of vegetation, boat, tree, and water is 0.8107, 0.9448, 0.8860, and 0.9123. However, the accuracy of road, port, and bridge are 0.1143, 0.0076, and 0.2963. It means that the method of <ref type="bibr" target="#b12">[13]</ref> cannot distinguish road, port, various buildings, and bridge. Through our analysis, these land covers have complex spectral information and semantic information. For example, the spectral information of road and bridge is very similar. However, these have different semantic information (road and bridge). Port and buildings have similar spectral characteristics, but these have different uses. In <ref type="bibr" target="#b12">[13]</ref>, it combines spectral information with Markov random fields and graph-cut-based method to classify the MS images. It does not consider the semantic information of each land cover. Therefore, the method of <ref type="bibr" target="#b12">[13]</ref> generates unsatisfactory results in our data sets. The similar situation also appears in Vancouver Level 1C Image, Xi'an Suburban Area, and Xi'an Urban Area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results With Vancouver Level 1C Image</head><p>The resolution of Vancouver level 1C image is the same as that of Vancouver level 1B image. Then, the parameter of e-slic, the structure, and the hyperparameter of Compared with the o-r, m-lr has a more detailed and diverse environmental information, which improves the classification results. The OA of m-lr is 97.67%. It is improved by 5.48% over o-r results. The Kappa is 96.93% that is better than o-r by 7.19%. Road, which has complicated environment information, has low accuracy rate that c-r is 42.15% and o-r is 68.31%. When m-lr is used, the accuracy rate of road is 86.69%. It shows that m-lr has an effective performance to represent land cover in complex scene. The accuracy rate of boat by c-r is 28.49%, o-r is 67.86%, and then m-lr is 87.87%. It shows that m-lr contributes to classify the small sample set. The other class of accuracy rate has improved significantly. It is shown in Table <ref type="table" target="#tab_1">II</ref>. The m-lr accuracy rate, which use o-slic to generate local region, is shown in Fig. <ref type="figure" target="#fig_8">8</ref>(g) and (h) and Table <ref type="table" target="#tab_1">II</ref>. The OA of m-lr (o-slic) is 92.07%, and the Kappa is 89.56%. These results show that e-slic is contributed to classify. The modified results are improved by 0.15%. The Kappa is improved by 0.20%. The details are shown in Figs. <ref type="figure" target="#fig_8">8(k</ref>) and (l) and 9. The OA and the Kappa of pixel-cnn are 87.44% and 83.55%. The results show that pixel-cnn model is an effective method for water (98.68%) and vegetation (98.40%), which have simple spectral information, and is valid for building2 (69.08%), which has complicated spectral information. So, the results show that the SML-CNN model is much better for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results With Xi'an Suburban Area</head><p>The resolution of Xi'an area image is similar to that of Vancouver area. So, e-slic with the same parameter is used to generate superpixels. The structure and the hyperparameter of multiple local regions joint representation CNN model and the modification strategy are the same as that of Vancouver area. The e-slic is generated 102 487 superpixels that 16 427 of which are labeled. The 4106 superpixels are used to train the model, and the other 12 321 superpixels are used to test. The training time is 3000 s. The testing time is 2358 s.</p><p>In Table <ref type="table" target="#tab_2">III</ref>, it shows that the OA of c-r is 82.17%, and the Kappa is 78.19%. It is similar to Vancouver image that only using the spectral information in superpixel is not enough to classify the image. The OA of o-r is 97.29%, and the Kappa is 96.54. It is improved by 15.12% and 18.35%. The accuracy of building2, vegetation2, and road is improved by 30.06%, 20.71%, and 18.40%. The OA of m-lr is 98.55%, and the Kappa is 98.15%. It is improved by 1.26% and 1.61%. The accuracy of building2, vegetation2, and road is improved by 3.42%, 2.70%, and 1.10%. The above-mentioned data show that the accuracy rate is improved with the increasing of the environmental information. We find that the OA of this result is very high. However, the overall effect is not satisfactory. For example, the classification results are not good at the boundary of building2 and vegetation1. Through the analysis, we found that these labeled samples are extracted from some simple areas. It means that the labeled building2 is surrounded by other building2, which are not labeled. So, the labeled samples, which have various environmental information, help to improve overall effect. In the future work, we will try to deal with a classification task that the labeled samples stay in a simple environment, while the whole sample stay in a different environment. The OA of m-lr (o-slic) is 96.64%, and the Kappa is 95.72%. The OA of m-lr is higher by 1.91% than m-lr (o-slic), and the Kappa is improved by 2.43%. This shows that expanded SLIC is effective. The OA of m-lr (modify) is 98.59%, and the Kappa is 98.20%. The OA of pixel-cnn is 94.49%, and the Kappa is 92.95%. These results show that pixel-cnn is worse than that of superpixel-cnn model. The details are shown in Figs. <ref type="figure" target="#fig_11">10</ref> and<ref type="figure" target="#fig_12">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Results With Xi'an Urban Area</head><p>In this section, we use Xi'an urban area to verify our algorithm. The urban area and the suburban area are different. In the urban area, all buildings are seen as the same class, and these buildings have a variety of shapes and colors. There are a large number of shadows and trees around the building. A variety of flat land, such as concrete, badminton court, tennis court, and so on, are distributed in different places in urban area. The e-slic and the multiple local regions joint representation CNN model use the same parameter as before. The e-slic is generated 26 509 superpixels that 16 924 of which are labeled. The 4231 superpixels are used to train the model, and the other 12 693 superpixels are used to test. The training time is 3000 s. The testing time is 543 s.</p><p>In Table <ref type="table" target="#tab_3">IV</ref>, it shows that the OA of c-r is 84.98%, and the Kappa is 80.58%. The accuracy rate of building, flat land, road, and shadow are not satisfying. These land covers are stay in a complex environment. So, only using the spectral information in superpixel is not enough to classify the image. When o-r, which has some environment information, is used to classify the image, the accuracy rate of these four kinds of land covers is improved by about 10%. The other land covers are improved by about 5%. Then, the OA of o-r is 91.34%, and the Kappa is 88.78%. When m-lr is used to classify the image, the accuracy rate of building, flat land, road, and shadow is improved by about 6% than o-r. The other land covers are improved by about 2%. Then, the OA of m-lr is 95.88%, and the Kappa is 94.69%. These results show that multiple local regions joint representation is effective. The OA of m-lr (o-slic) is 90.81%, and the Kappa is 88.13%. These results show that e-slic is conducive to improve the  <ref type="bibr" target="#b12">[13]</ref>.</p><p>THE C-R REPRESENTS CENTRAL REGION-BASED METHOD </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we have proposed a superpixel-based multiple local regions joint representation CNN model. We combine the spatial-spectral information of the MS image, the detailed information of PAN image, and the semantic relation information of each land cover for classification. The SLIC is extended that is more effective for the MS image. We proved this point through experiments. The e-slic is used to generate superpixels, which are produced multiple local regions joint representation. Effective results are produced by the proposed CNN model and modification strategy. The experimental results of <ref type="bibr" target="#b12">[13]</ref>, the pixels-based CNN, the single region, and multiple local regions were compared and shown that multiple local regions method is effective. But there are still some shortcomings. In the future work, we will improve this model and try to use semisupervised or unsupervised deep learning method and more effective modification strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of this paper. (a) Image preprocessing shows superpixels generated by e-slic. Red box represents a superpixel and the environment of itself. (b) Multiple local regions joint representation CNN model. This model consists of multiple local regions joint representation, multiple CNN model, and feature fusion network. (c) Multi-information modification strategy. The detailed information of panchromatic image and the semantic information of land cover are used to modify the initial results.</figDesc><graphic coords="2,449.39,196.97,95.18,75.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>models, we have redesigned the input data pattern, the structure of CNN model, feature fusion network, and multiple local regions, such as central region, original region, and four corner regions, to jointly represent each superpixel. The multiple local regions joint representation can make full use of the spectral information, the spatial structure information, and the environment information in each superpixel. Since the CNN model has a powerful feature extraction and representation capability, we designed a multiple local</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multiple local regions joint representation. (a) MS image. (b) Superpixels are generated by expanded SLIC. The red box shows a superpixel and the environment of it. (c) Superpixel. The highlighting part represents the superpixel itself, and the dark part represents the environment in which the superpixel is located. This superpixel is used to generate six local regions. (d)-(i). (d) Central region. (e) Original region. (f)-(i) Four corner regions. The multiple local regions joint representation consists of these six local regions.</figDesc><graphic coords="3,60.23,207.17,69.62,69.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration on the Multiple Local Regions Joint Representation CNN Model. (a) Input data of this model. It contains six local regions, which are central region, original region, and four corner regions. (b) Six feature extraction subnetworks for each local region. Each subnetwork consists of two conv-pooling-ReLU layers and two fully connected layers. The features of each local region are extracted by these subnetworks. (c) Multiple feature fusion subnetwork. It contains four AE layers. The fused feature is obtained by this subnetwork. (d) Output of this model. A softmax layer is used to predict the class of input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. First column: MS images. Second column: panchromatic images. Third column: labeled maps. Last column: number of pixels for each class. (a) Vancouver 1B images. (b) Vancouver 1C images. (c) Xi'an suburban area. (d) Xi'an urban area.</figDesc><graphic coords="5,73.19,442.49,119.78,114.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Segmentation results. (a) o-slic. (b) e-slic.</figDesc><graphic coords="6,49.31,59.33,124.46,119.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Initial classification results. (a) and (b) Central region classification results. (c) and (d) Classification results of the method [13]. (e) and (f) Multiple local regions classification results. (a), (b), (e), and (f) Use e-slic to classify image. (g) and (h) Use o-slic to classify image and show the multiple local regions classification results. (i) and (j) pixel-cnn classification results. (k) and (l) Modified multiple local regions classification results. (a), (c), (e), (g), (i), and (k) Classification results of overall image. (b), (d), (f), (h), (j), and (l) Classification results of the image, which are labeled. These have the same meaning in Figs. 8, 10, and 12.</figDesc><graphic coords="7,49.31,490.49,124.94,138.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Final classification results of Vancouver level 1B image.</figDesc><graphic coords="8,50.51,172.25,247.22,202.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Initial classification results. (a) and (b) Central region classification results. (c) and (d) Classification results of the method [13]. (e) and (f) Multiple local regions classification results. (a), (b), (e), and (f) Use e-slic to classify image. (g) and (h) Use o-slic to classify image and show the multiple local regions classification results. (i) and (j) pixel-cnn classification results. (k) and (l) Modified multiple local regions classification results.</figDesc><graphic coords="9,55.31,441.65,113.54,170.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Final classification results of Vancouver level 1C image.</figDesc><graphic coords="10,50.51,175.85,247.22,210.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>, the cr shows that only the central region is used to classify the whole image. The o-r shows that only the original region is used to classify the whole image. The m-lr(o-slic) shows that the superpixel-based multiple local regions are used to classify the whole image and the superpixel is generated by o-slic method. The m-rl(initial) shows that the proposed method uses multiple local regions to classify the whole image. However, the superpixel is generated by e-slic method. The (initial) means that the classification results are not modified by multi-information modification strategy. The pixel-cnn means the pixel-cnn model. The m-lr(modify) means that the results are modified by multi-information modification strategy. 2) Multiple Local Regions Joint Representation CNN Model: We take each superpixel as the center to generate central region, original region, and four corner regions. Each local regions size is 31 ×31. These six local regions constitute multiple local regions joint representation. The details are shown in Section II-B1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Initial classification results. (a) and (b) Central region classification results. (c) and (d) Classification results of the method [13]. (e) and (f) Multiple local regions classification results. (a), (b), (e), and (f) Use e-slic to classify image. (g) and (h) Use o-slic to classify image and show the multiple local regions classification results. (i) and (j) pixel-cnn classification results. (k) and (l) Modified multiple local regions classification results.</figDesc><graphic coords="11,49.31,367.13,124.94,132.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Final classification results of Xi'an suburban area image.</figDesc><graphic coords="12,50.51,58.25,247.58,216.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>multiple local regions joint representation CNN model and the modification strategy are the same as that of Vancouver level 1B image. The e-slic is generated 45 467 superpixels that 12 840 of which are labeled. The 3210 superpixels are used to train the model, and the other 9630 superpixels are used to test. The training time is 3000 s. The testing time is 867 s. For central region (c-r) classification results in Fig. 8(a) and (b), tree and water have convincing result, and boat and road are not. It is shown that c-r can only distinguish some simple scenes land cover, such as water and tree. It only used the central region information to classify the wrong class, which has similar spectral information. When the land cover contains more complicated environment information the central region cannot effectively express it. The OA of c-r is just 78.95%, and the Kappa is 72.22%. For Fig. 8(c) and (d), the result shows that the original region (o-r) classifies the image more effectively. Through the analysis, the whole model has a stronger representation of all kinds of land cover because of the addition of the information of the surrounding environment. The accuracy rate of all kinds of land cover is obviously improved. The OA of o-r is 92.19%. It is improved by 13.24%. The Kappa is 89.74%, and improved by 17.52%. For Fig. 8(e) and (f), it shows the multiple local regions (m-lr) classification results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Initial classification results. (a) and (b) Central region classification results. (c) and (d) Classification results of the method [13]. (e) and (f) Multiple local regions classification results. (a), (b), (e), and (f) Use e-slic to classify image. (g) and (h) Use o-slic to classify image and show the multiple local regions classification results. (i) and (j) pixel-cnn classification results. (k) and (l) Modified multiple local regions classification results.</figDesc><graphic coords="13,56.87,333.41,121.34,117.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Final classification results of Xi'an urban area image.</figDesc><graphic coords="14,50.51,418.49,247.46,216.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>ACCURACY OF VANCOUVER LEVEL 1B IMAGE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ACCURACY OF VANCOUVER LEVEL 1C IMAGE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ACCURACY OF XI'AN SUBURBAN AREA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY OF XI'AN URBAN AREATABLE V MEANS AND THE STANDARD DEVIATIONS OF THE VALUES OF OA AND KAPPA OVER THESE TEN RUNS FOR ALL DATA SETS AND FOR BOTH THE PROPOSED METHOD AND THE TECHNIQUE DESCRIBED IN [13] TABLE VI MCNEMAR's TEST FOR ALL DATA SETS AND FOR BOTH THE PROPOSED METHOD AND THE TECHNIQUE DESCRIBED IN</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Deimos Imaging for acquiring and providing the data used in this paper, and the IEEE GRSS Image Analysis and Data Fusion Technical Committee.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program (973 Program) of China under Grant 2013CB329402, in part by the Major Research Plan of the National Natural Science Foundation of China under Grant 91438201 and Grant 91438103, and in part by the Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project) under Grant B07048.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>She has been a Professor in Electrical Engineering with Xidian University. Her research interests include machine learning and multiscale geometric analysis.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Resolution enhancement of multispectral image data to improve classification accuracy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Munechika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Warnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Schott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="72" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical fuzzy classification approach for high-resolution multispectral data over urban areas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shackelford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1920" to="1932" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quality assessment of panchromatic and multispectral image fusion for the ZY-3 satellite: From an information extraction perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="753" to="757" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fusion classification of multispectral and panchromatic image using improved decision tree algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Shingare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Hemane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Dandekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Signal Propag</title>
		<meeting>Int. Conf. Signal Propag</meeting>
		<imprint>
			<date type="published" when="2014-07">Jul. 2014</date>
			<biblScope unit="page" from="598" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active learning methods for remote sensing image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kanevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2218" to="2232" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointwise graph-based local texture characterization for very high resolution multispectral image classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1962" to="1973" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Textural and local spatial statistics for the objectoriented classification of urban areas using high resolution imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3105" to="3117" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Remote sensing image fusion with convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sens. Imag</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pansharpening by convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">594</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generalized metaphor of Chinese restaurant franchise to fusing both panchromatic and multispectral images for unsupervised classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4594" to="4604" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint classification of panchromatic and multispectral images by multiresolution fusion through Markov random fields and graph cuts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digit. Signal Process</title>
		<meeting>Int. Conf. Digit. Signal ess</meeting>
		<imprint>
			<date type="published" when="2011-07">Jul. 2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic classification of high-resolution remote-sensing images based on mid-level features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2343" to="2353" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiresolution supervised classification of panchromatic and multispectral images by Markov random fields and graph cuts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5054" to="5070" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Process for enhancing the spatial resolution of multispectral imagery using pan-sharpening</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Laben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Brower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2000-04">Jan. 4, 2000</date>
		</imprint>
	</monogr>
	<note>875 A</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multisensor image fusion in remote sensing: Concepts, methods and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Polh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Van Genderen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="854" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of three different methods to merge multiresolution and multispectral data: Landsat TM and SPOT panchromatic</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Chavez</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Sides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generalized component substitution technique for spatial enhancement of multispectral images using a higher resolution data set</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Shettigara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="561" to="567" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparison and improvement of wavelet-based image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="673" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multispectral images fusion by a joint multidirectional and multiresolution representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lillo-Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="4065" to="4079" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multispectral imagery advanced band sharpening study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vrabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new pan-sharpening method with deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1037" to="1041" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminant deep belief network for high-resolution sar image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="686" to="701" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wishart deep stacking network for fast POLSAR image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3273" to="3286" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">POL-SAR image classification based on wishart DBN and local spatial information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3292" to="3308" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sens</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">258619</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral image based on deep belief networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="5132" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3368" to="3379" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vehicle detection in satellite images by hybrid deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1797" to="1801" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building detection in very high resolution multispectral data with deep learning features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="122" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4895" to="4909" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1174" to="1185" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object recognition in remote sensing images using sparse deep belief networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="745" to="754" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scene classification via a gradient boosting random convolutional network framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1793" to="1802" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning based feature selection for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2321" to="2325" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">segDeepM: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="4703" to="4711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2012</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<ptr target="http://www.grss-ieee.org/community/technical-committees/data-fusion" />
		<title level="m">IEEE GRSS Data Fusion Contest</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="212" to="223" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
