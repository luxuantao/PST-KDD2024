<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-30">30 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<addrLine>Alibaba Group, Belle-vue</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<addrLine>Alibaba Group, Belle-vue</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<addrLine>Alibaba Group, Belle-vue</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<address>
									<addrLine>Alibaba Group, Belle-vue</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-30">30 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.12740v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for longterm prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by 14.8% and 22.6% for multivariate and univariate time series, respectively. the code will be released soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Long-term time series forecasting is a long-standing challenge in various applications (e.g., energy, weather, traffic, economics). Despite the impressive results achieved by <ref type="bibr">RNN-type methods (Rangapuram et al., 2018;</ref><ref type="bibr" target="#b10">Flunkert et al., 2017)</ref>, they often suffer from the problem of gradient vanishing or exploding <ref type="bibr" target="#b22">(Pascanu et al., 2013)</ref>, significantly limiting their performance. Following the recent suc-Despite the progress made by Transformer-based methods for time series forecasting, they tend to fail in capturing the overall characteristics/distribution of time series in some cases. In Figure <ref type="figure" target="#fig_0">1</ref>, we compare the time series of ground truth with that predicted by the vanilla Transformer method <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> in a real-world ETTm1 dataset <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref>. It is clear that the predicted time series shared a different distribution from that of ground truth. The discrepancy between ground truth and prediction could be explained by the point-wise attention and prediction in Transformer. Since prediction for each timestep is made individually and independently, it is likely that the model fails to maintain the global property and statistics of time series as a whole. To address this problem, we exploit two ideas in this work. The first idea is to incorporate a seasonal-trend decomposition approach <ref type="bibr" target="#b6">(Cleveland et al., 1990;</ref><ref type="bibr" target="#b33">Wen et al., 2019)</ref>, which is widely used in time series analysis, into the Transformerbased method. Although this idea has been exploited before <ref type="bibr" target="#b21">(Oreshkin et al., 2019;</ref><ref type="bibr" target="#b34">Wu et al., 2021)</ref>, we present a special design of network that is effective in bringing the distribution of prediction close to that of ground truth, according to Kologrov-Smirnov distribution test. Our second idea is to combine Fourier analysis with the Transformerbased method. Instead of applying Transformer to the time domain, we apply it to the frequency domain which helps Transformer better capture global properties of time series. Combining both ideas, we propose a Frequency Enhanced Decomposition Transformer, or, FEDformer for short, for long-term time series forecasting.</p><p>One critical question with FEDformer is which subset of frequency components should be used by Fourier analysis to represent time series. A common wisdom is to keep lowfrequency components and throw away the high-frequency ones. This may not be appropriate for time series forecasting as some of trend changes in time series are related to important events, and this piece of information could be lost if we simply remove all high-frequency components. We address this problem by effectively exploiting the fact that time series tend to have (unknown) sparse representations on a basis like Fourier basis. According to our theoretical analysis, a randomly selected subset of frequency components, including both low and high ones, will give a better representation for time series, which is further verified by extensive empirical studies. Besides being more effective for long term forecasting, combining Transformer with frequency analysis allows us to reduce the computational cost of Transformer from quadratic to linear complexity. We note that this is different from previous efforts on speeding up Transformer, which often leads to a performance drop.</p><p>In short, we summarize the key contributions of this work as follows:</p><p>1. We propose a frequency enhanced decomposed Transformer architecture with mixture of experts for seasonal-trend decomposition in order to better capture global properties of time series.</p><p>2. We propose Fourier enhanced blocks and Wavelet enhanced blocks in the Transformer structure that allows us to capture important structures in time series through frequency domain mapping. They serve as substitutions for both self-attention and crossattention blocks.</p><p>3. By randomly selecting a fixed number of Fourier components, the proposed model achieves linear computational complexity and memory cost. The effectiveness of this selection method is verified both theoretically and empirically.</p><p>4. We conduct extensive experiments over 6 benchmark datasets across multiple domains (energy, traffic, economics, weather and disease). Our empirical studies show that the proposed model improves the performance of state-of-the-art methods by 14.8% and 22.6% for multivariate and univariate forecasting, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Compact Representation of Time Series in Frequency Domain</head><p>It is well-known that time series data can be modeled from the time domain and frequency domain. One key contribu- tion of our work which separates from other long-term forecasting algorithms is the frequency-domain operation with a neural network. As Fourier analysis is a common tool to dive into the frequency domain, while how to appropriately represent the information in time series using Fourier analysis is critical. Simply keeping all the frequency components may result in inferior representations since many high-frequency changes in time series are due to noisy inputs. On the other hand, only keeping the low-frequency components may also be inappropriate for series forecasting as some trend changes in time series represent important events. Instead, keeping a compact representation of time series using a small number of selected Fourier components will lead to efficient computation of transformer, which is crucial for modelling long sequences. We propose to represent time series by randomly selecting a constant number of Fourier components, including both highfrequency and low-frequency. Below, an analysis that justifies the random selection is presented theoretically. Empirical verification can be found in the experimental session.</p><p>Consider we have m time series, denoted as X 1 (t), . . . , X m (t).</p><p>By applying Fourier transform to each time series, we turn each X i (t) into a vector a i = (a i,1 , . . . , a i,d ) ⊤ ∈ R d . By putting all the Fourier transform vectors into a matrix, we have A = (a 1 , a 2 , . . . , a m ) ⊤ ∈ R m×d , with each row corresponding to a different time series and each column corresponding to a different Fourier component. Although using all the Fourier components allows us to best preserve the history information in the time series, it may potentially lead to overfitting of the history data and consequentially a poor prediction of future signals. Hence, we need to select a subset of Fourier components, that on the one hand should be small enough to avoid the overfitting problem and on the other hand, should be able to preserve most of the history information. Here, we propose to select s components from the d Fourier components (s &lt; d) uniformly at random. More specifically, we denote by i 1 &lt; i 2 &lt; . . . &lt; i s the randomly selected components. We construct matrix S ∈ {0, 1} s×d , with S i,k = 1 if i = i k and S i,k = 0 otherwise. Then, our representation of multivariate time series becomes A ′ = AS ⊤ ∈ R m×s . </p><formula xml:id="formula_0">+ + + Output M K V Encoder Input Seasoal Init Trend Init T l,2 de T l,1 de T l,3 de S l,2 X 0 en ∈ R I×D X 0 de ∈ R (I/2+O)×D T 0 de ∈ R (I/2+O)×D X l−1 en T l−1 de X l−1 de T l de S l,3 de (or X l de )</formula><p>Figure Below, we will show that, although the Fourier basis are randomly selected, under a mild condition, A ′ is able to preserve most of the information from A.</p><p>In order to measure how well A ′ is able to preserve information from A, we project each column vector of A into the subspace spanned by the column vectors in A ′ . We denote by P A ′ (A) the resulting matrix after the projection, where Theorem 1. Assume that µ(A), the coherence measure of matrix A, is Ω(k/n). Then, with a high probability, we have</p><formula xml:id="formula_1">P A ′ (•)</formula><formula xml:id="formula_2">|A − P A ′ (A)| ≤ (1 + ǫ)|A − A k | if s = O(k 2 /ǫ 2 ).</formula><p>The detailed analysis can be found in Appendix C.</p><p>For real-world multivariate times series, the corresponding matrix A from Fourier transform often exhibit low rank property, since those univaraite variables in multivariate times series depend not only on its past values but also has dependency on each other, as well as share similar frequency components. Therefore, as indicated by the Theorem 1, randomly selecting a subset of Fourier components allows us to appropriately represent the information in Fourier matrix A.</p><p>Similarly, wavelet orthogonal polynomials, such as Legendre Polynomials, obey restricted isometry property (RIP) and can be used for capture information in time series as well. Compared to Fourier basis, wavelet based representa-tion is more effective in capturing local structures in time series and thus can be more effective for some forecasting tasks. We defer the discussion of wavelet based representation in Appendix B. In the next section, we will present the design of frequency enhanced decomposed Transformer architecture that incorporate the Fourier transform into transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Structure</head><p>In this section, we will introduce (1) the overall structure of FEDformer, as shown in Figure <ref type="figure">2</ref>, (2) two subversion structures for signal process: one uses Fourier basis and the other uses Wavelet basis, (3) the mixture of experts mechanism for seasonal-trend decomposition, and (4) the complexity analysis of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FEDformer Framework</head><p>Preliminary Long-term time series forecasting is a sequence to sequence problem. We denote the input length as I and output length as O. We denote D as the hidden states of the series. The input of the encoder is a I × D matrix and the decoder has</p><formula xml:id="formula_3">(I/2 + O) × D input.</formula><p>FEDformer Structure Inspired by the seasonal-trend decomposition and distribution analysis as discussed in Section 1, we renovate Transformer as a deep decomposition architecture as shown in Figure <ref type="figure">2</ref>, including Frequency Enhanced Block (FEB), Frequency Enhanced Attention (FEA) connecting encoder and decoder, and the Mixture Of Experts Decomposition block (MOEDecomp). The detailed description of FEB, FEA, and MOEDecomp blocks will be given in the following Section 3.2, 3.3, and 3.4 respectively.</p><p>The encoder adopts a multilayer structure as: X l en = Encoder(X l−1 en ), where l ∈ {1, • • • , N } denotes the output of l-th encoder layer and X 0 en ∈ R I×D is the embedded historical series. The Encoder(•) is formalized as</p><formula xml:id="formula_4">S l,1 en ,− = MOEDecomp(FEB X l−1 en + X l−1 en ), S l,2 en ,− = MOEDecomp(FeedForward S l,1 en + S l,1 en ), X l en = S l,2 en ,<label>(1)</label></formula><p>where S l,i en , i ∈ {1, 2} represents the seasonal component after the i-th decomposition block in the l-th layer respectively. For FEB module, it has two different versions (FEB-f &amp; FEB-w) which are implemented through Discrete Fourier transform (DFT) and Discrete Wavelet transform (DWT) mechanism respectively and can seamlessly replace the self-attention block.</p><p>The decoder also adopts a multilayer structure as: X l de , T l de = Decoder(X l−1 de , T l−1 de ), where l ∈ {1, • • • , M } denotes the output of l-th decoder layer. The Decoder(•) is formalized as</p><formula xml:id="formula_5">S l,1 de , T l,1 de = MOEDecomp FEB X l−1 de + X l−1 de , S l,2 de , T l,2 de = MOEDecomp FEA S l,1 de , X N en + S l,1 de , S l,3 de , T l,3 de = MOEDecomp FeedForward S l,2 de + S l,2 de , X l de = S l,3 de , T l de = T l−1 de + W l,1 •T l,1 de + W l,2 •T l,2 de + W l,3 •T l,3 de ,<label>(2)</label></formula><p>where S l,i de , T l,i de , i ∈ {1, 2, 3} represent the seasonal and trend component after the i-th decomposition block in the lth layer respectively. W l,i , i ∈ {1, 2, 3} represents the projector for the i-th extracted trend T l,i de . Similar to FEB, FEA has two different versions (FEA-f &amp; FEA-w) which are implemented through DFT and DWT projection respectively with attention design, and can replace the cross-attention block. The detailed description of FEA(•) will be given in the following Section 3.3.</p><p>The final prediction is the sum of the two refined decomposed components as W S • X M de + T M de , where W S is to project the deep transformed seasonal component X M de to the target dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fourier Enhanced Structure</head><p>Discrete Fourier Transform (DFT) The proposed Fourier Enhanced Structures use discrete Fourier transform (DFT). Let F denotes the Fourier transform and F −1 denotes the inverse Fourier transform. Given a sequence of real numbers x n in time domain, where n = 1, 2...N . DFT is defined as</p><formula xml:id="formula_6">X l = N −1 n=0</formula><p>x n e −iωln , where i is the imaginary unit and X l , l = 1, 2...L is a sequence of complex numbers in the frequency domain. Similarly, the inverse</p><formula xml:id="formula_7">F F −1 q ∈ R L×D Sampling × Q ∈ C N ×D C M ×D Q ∈ R ∈ Ỹ ∈ C M ×D padding Y ∈ C N ×D y ∈ R Lde×D C M ×D×D X l−1 en/de MLP Figure 3</formula><p>. Frequency Enhanced Block with Fourier transform (FEB-f) structure. DFT is defined as</p><formula xml:id="formula_8">× MLP v F + Sampling MLP MLP Q K Ṽ F + Sampling F + Sampling σ(•) × Padding + F −1 y ∈ R Lde×D q ∈ R Lde×D k Q K⊤ X l en S l</formula><formula xml:id="formula_9">x n = L−1 l=0</formula><p>X l e iωln . The complexity of DFT is O(N 2 ). With fast Fourier transform (FFT), the computation complexity can be reduced to O(N log N ).</p><p>Here a random subset of the Fourier basis is used and the scale of the subset is bounded by a scalar. When we choose the mode index before DFT and reverse DFT operations, the computation complexity can be further reduced to O(N ).</p><p>Frequency Enhanced Block with Fourier Transform (FEB-f) The FEB-f is used in both encoder and decoder as shown in Figure <ref type="figure">2</ref>. The input (x ∈ R N ×D ) of the FEB-f block is first linearly projected with w ∈ R D×D , so q = x•w. Then q is converted from the time domain to the frequency domain. The Fourier transform of q is denoted as Q ∈ C N ×D . In frequency domain, only the randomly selected M modes are kept so we use a select operator as</p><formula xml:id="formula_10">Q = Select(Q) = Select(F(q)),<label>(3)</label></formula><p>where Q ∈ C M×D and M &lt;&lt; N . Then, the FEB-f is defined as</p><formula xml:id="formula_11">FEB-f(q) = F −1 (Padding( Q ⊙ R)),<label>(4)</label></formula><p>where</p><formula xml:id="formula_12">R ∈ C D×D×M is a parameterized kernel initialized randomly. Let Y = Q ⊙ C, with Y ∈ C M×D . The pro- duction operator ⊙ is defined as: Y m,do = D di=0 Q m,di • R di,do,m</formula><p>, where d i = 1, 2...D is the input channel and d o = 1, 2...D is the output channel. The result of Q ⊙ R is then zero-padded to C N ×D before performing inverse Fourier transform back to the time domain. The structure is shown in Figure <ref type="figure">3</ref>.</p><p>Frequency Enhanced Attention with Fourier Transform (FEA-f) We use the expression of the canonical transformer. The input: queries, keys, values are denoted as q ∈ R L×D , k ∈ R L×D , v ∈ R L×D . In cross-attention, the queries come from the decoder and can be obtained by q = x en • w q , where w q ∈ R D×D . The keys and values are from the encoder and can be obtained by k = x de • w k and v = x de • w v , where w k , w v ∈ R D×D . Formally, the canonical attention can be written as</p><formula xml:id="formula_13">Atten(q, k, v) = Softmax( qk ⊤ dq )v.<label>(5)</label></formula><p>In FEA-f, we convert the queries, keys, and values with Fourier Transform and perform a similar attention mechanism in the frequency domain, by randomly selecting M modes. We denote the selected version after Fourier Transform as</p><formula xml:id="formula_14">Q ∈ C M×D , K ∈ C M×D , Ṽ ∈ C M×D . The FEA-f is defined as Q = Select(F(q)) K = Select(F(k)) Ṽ = Select(F(v)) (6) FEA-f(q, k, v) = F −1 (Padding(σ( Q • K⊤ ) • Ṽ )), (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>where σ is the activation function. We use softmax or tanh for activation, since their converging performance differs in different data sets. Let Y = σ( Q • K⊤ ) • Ṽ , and Y ∈ C M×D needs to be zero-padded to C L×D before performing inverse Fourier transform. The FEA-f structure is shown in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Wavelet Enhanced Structure</head><p>Discrete Wavelet Transform (DWT) While the Fourier transform creates a representation of the signal in the frequency domain, the Wavelet transform creates a representation in both the frequency and time domain, allowing efficient access of localized information of the signal. The multiwavelet transform synergizes the advantages of orthogonal polynomials as well as wavelets. For a given f (x), the multiwavelet coefficients at the scale n can be defined as</p><formula xml:id="formula_16">s n l = f, φ n il µn k−1 i=0 , d n l = f, ψ n il µn k−1 i=0</formula><p>, respectively, w.r.t. measure µ n with s n l , d n l ∈ R k×2 n . φ n il are wavelet orthonormal basis of piecewise polynomials. The decomposition/reconstruction across scales is defined as</p><formula xml:id="formula_17">s n l = H (0) s n+1 2l + H (1) s n+1 2l+1 , s n+1 2l = Σ (0) H (0)T s n l + G (0)T d n l , d n l = G (0) s n+1 2l + H (1) s n+1 2l+1 , s n+1 2l+1 = Σ (1) H (1)T s n l + G (1)T d n l ,<label>(8)</label></formula><p>where 1) are linear coefficients for multiwavelet decomposition filters. They are fixed matrices used for wavelet decomposition. The multiwavelet representation of a signal can be obtained by the tensor product of multiscale and multiwavelet basis. Note that the basis at various scales are coupled by the tensor product, so we need to untangle it. Inspired by <ref type="bibr" target="#b11">(Gupta et al., 2021)</ref>, we adapt a non-standard wavelet representation to reduce the model complexity. For a map function F (x) = x ′ , the map under multiwavelet domain can be written as</p><formula xml:id="formula_18">H (0) , H (1) , G (0) , G<label>(</label></formula><formula xml:id="formula_19">Decomposed Matrix X(L) Lf :X(L+1) FEB-f FEB-f Hf Hf Ud(L) Us(L) Ud(L) Us(L) X'(L+1) + X'(L) Reconstruction matrix FEB-f Lf + Decomposed Matrix q(L) k(L) v(L) Lf :q(L+1) k(L+1) v(L+1) FEA-f FEA-f Hf_q Hf_k Hf_v Ud(L) Us(L) FEA-f + Hf_q Hf_k Hf_v Lf_q Lf_k Lf_v FEA-f X'(L+1)</formula><formula xml:id="formula_20">U n dl = And n l + Bns n l , U n ŝl = Cnd n l , U L sl = F s L l ,<label>(9)</label></formula><p>where (U n sl , U n dl , s n l , d n l ) are the multiscale, multiwavelet coefficients, L is the coarsest scale under recursive decomposition, and A n , B n , C n are three independent FEB-f blocks modules used for processing different signal during decomposition and reconstruction. Here F is a single-layer of perceptrons which processes the remaining coarsest signal after L decomposed steps. More designed detail is described in Appendix D.</p><p>Frequency Enhanced Block with Wavelet Transform (FEB-w) The overall FEB-w architecture is shown in Figure 5. It differs from FEB-f in the recursive mechanism: the input is decomposed into 3 parts recursively and operates individually. For the wavelet decomposition part, we implement the fixed Legendre wavelets basis decomposition matrix. Three FEB-f modules are used to process the resulting high-frequency part, low-frequency part, and remaining part from wavelet decomposition respectively. For each cycle L, it produces a processed high-frequency tensor U d(L), a processed low-frequency frequency tensor U s(L), and the raw low-frequency tensor X(L+1). This is a ladder-down approach, and the decomposition stage performs the decimation of the signal by a factor of 1/2, run-ning for a maximum of L cycles, where L &lt; log 2 (M ) for a given input sequence of size M . In practice, L is set as a fixed argument parameter. The three sets of FEB-f blocks are shared during different decomposition cycles L. For the wavelet reconstruction part, we recursively build up our output tensor as well. For each cycle L, we combine X(L+1), U s(L), and U d(L) produced from the decomposition part and produce X(L) for the next reconstruction cycle. For each cycle, the length dimension of the signal tensor is increased by 2 times.</p><p>Frequency Enhanced Attention with Wavelet Transform (FEA-w) FEA-w contains the decomposition stage and reconstruction stage like FEB-w. Here we keep the reconstruction stage unchanged. The only difference lies in the decomposition stage. The same decomposed matrix is used to decompose q, k, v signal separately, and q, k, v share the same sets of module to process them as well. As shown above, a frequency enhanced block with wavelet decomposition block (FEB-w) contains three FEB-f blocks for the signal process. We can view the FEB-f as a substitution of self-attention mechanism. We use a straightforward way to build the frequency enhanced cross attention with wavelet decomposition, substituting each FEB-f with a FEA-f module. Besides, another FEA-f module is added to process the coarsest remaining q(L), k(L), v(L) signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mixture of Experts for Seasonal-Trend Decomposition</head><p>Because of the commonly observed complex periodic pattern coupled with the trend component on real-world data, extracting the trend can be hard with fixed window average pooling. To overcome such a problem, we design a Mixture Of Experts Decomposition block (MOEDecomp). It contains a set of average filters with different sizes to extract multiple trend components from the input signal and a set of data-dependent weights for combining them as the final trend. Formally, we have</p><formula xml:id="formula_21">X trend = Softmax(L(x)) * (F (x)),<label>(10)</label></formula><p>where F (•) is a set of average pooling filters and Softmax(L(x)) is the weights for mixing these extracted trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity Analysis</head><p>For FEDformer-f, the computational complexity for time and memory is O(L) with a fixed number of randomly selected modes in FEB &amp; FEA blocks. We set modes number M = 64 as default value. Though the complexity of full DFT transformation by FFT is (O(L log(L)), our model only needs O(L) cost and memory complexity with the pre-selected set of Fourier basis for quick implemen- </p><formula xml:id="formula_22">(L) O(L) 1 Autoformer O(L log L) O(L log L) 1 Informer O(L log L) O(L log L) 1 Transformer O L 2 O L 2 L LogTrans O(L log L) O L 2 1 Reformer O(L log L) O(L log L) L LSTM O(L) O(L) L</formula><p>tation. For FEDformer-w, when we set the recursive decompose step to a fixed number L and use a fixed number of randomly selected modes the same as FEDformer-f, the time complexity and memory usage are O(L) as well. In practice, we choose L = 3 and modes number M = 64 as default value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the proposed FEDformer, we conduct extensive experiments on six popular real-world datasets, including energy, economics, traffic, weather, and disease. Since classic models like ARIMA and basic RNN/CNN models perform relatively inferior as shown in <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref> and <ref type="bibr" target="#b34">(Wu et al., 2021)</ref>, we mainly include four state-of-theart transformer-based models for comparison, i.e., Autoformer <ref type="bibr" target="#b34">(Wu et al., 2021)</ref>, Informer <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref>, Log-Trans <ref type="bibr" target="#b17">(Li et al., 2019)</ref> and Reformer <ref type="bibr" target="#b15">(Kitaev et al., 2020)</ref> as baseline models. Note that since Autoformer holds the best performance in all the six benchmarks, it is used as the main baseline model for comparison. More details about baseline models, datasets, and implementation are described in Appendix A.2, F.1, and F.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>For better comparison, we follow the experiment settings of Autoformer in <ref type="bibr" target="#b34">(Wu et al., 2021)</ref> where the input length is fixed to 96, and the prediction lengths for both training and evaluation are fixed to be 96, 192, 336, and 720, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multivariate Results</head><p>For the multivariate forecasting, FEDformer achieves the best performance on all six benchmark datasets at all horizons as shown in Table <ref type="table" target="#tab_4">2</ref>. Compared with Autoformer, the proposed FEDformer yields an overall 14.8% relative MSE reduction. It is worth noting that for some of the datasets, such as Exchange and ILI, the improvement is even more significant (over 20%). Note that the Exchange dataset does not exhibit clear periodic-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Univariate Results</head><p>The results for univariate time series forecasting are summarized in Table <ref type="table" target="#tab_5">3</ref>. Compared with Autoformer, FEDformer yields an overall 22.6% relative MSE reduction, and on some datasets, such as traffic and weather, the improvement can be more than 30%. It again verifies that FEDformer is more effective in long-term forecasting. Note that due to the difference between Fourier and wavelet basis, FEDformer-f and FEDformer-w perform well on different datasets, making them complementary choice for long term forecasting. More detailed results on ETT full benchmark are provided in Appendix F.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>In this section, the ablation experiments are conducted, aiming at comparing the performance of frequency enhanced block and its alternatives. The current SOTA results of Autoformer which uses the autocorrelation mechanism serve as the baseline. Three ablation variants of FEDformer are tested: 1) FEDformer V1: we use FEB to substitute self-attention only; 2) FEDformer V2: we use FEA to substitute cross attention only; 3) FEDFormer V3: we use FEA to substitute both self and cross attention. The ablated versions of FEDformer-f as well as the SOTA models are compared in Table <ref type="table" target="#tab_6">4</ref>, and we use a bold number if the ablated version brings improvements compared with Autoformer. We omit the similar results in FEDformer-w due to space limit. It can be seen in Table <ref type="table" target="#tab_6">4</ref> that FEDformer V1 brings improvement in 10/16 cases, while FEDformer V2 improves in 12/16 cases. The best performance is achieved in our FEDformer with FEB and FEA blocks which improves performance in all 16/16 cases. This verifies the effectiveness of the designed FEB, FEA for substituting self and cross attention. Furthermore, experiments on ETT and Weather datasets show that the adopted MOEDecomp (mixture of experts decomposition) scheme can bring an average of 2.96% improvement compared with the single decomposition scheme. More details are provided in Appendix F.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Mode Selection Policy</head><p>The selection of discrete Fourier basis is the key to effectively representing the signal and maintaining the model's linear complexity. As we discussed in Section 2, random Fourier mode selection is a better policy in forecast- ing tasks. more importantly, random policy requires no prior knowledge of the input and generalizes easily in new tasks. Here we empirically compare the random selection policy with fixed selection policy, and summarize the experimental results in Figure <ref type="figure">6</ref>. It can be observed that the adopted random policy achieves better performance than the common fixed policy which only keeps the low frequency modes. Meanwhile, the random policy exhibits some mode saturation effect, indicating an appropriate random number of modes instead of all modes would bring better performance, which is also consistent with the theoretical analysis in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Distribution Analysis of Forecasting Output</head><p>In this section, we evaluate the distribution similarity between the input sequence and forecasting output of different transformer models quantitatively. In Table <ref type="table" target="#tab_7">5</ref>, we applied the Kolmogrov-Smirnov test to check if the forecasting results of different models made on ETTm1 and ETTm2 are consistent with the input sequences. In particular, we test if the input sequence of fixed 96-time steps come from the same distribution as the predicted sequence, with the null hypothesis that both sequences come from the same distribution. On both datasets, by setting the com- mon P-value as 0.01, various existing Transformer baseline models have much less values than 0.01 except Autoformer, which indicates their forecasting output have a higher probability to be sampled from the different distributions compared to the input sequence. In contrast, Autoformer and FEDformer have much larger P-value compared to others, which mainly contributes to their seasonal-trend decomposition mechanism. Though we get close results from ETTm2 by both models, the proposed FEDformer has much larger P-value in ETTm1. And it's the only model whose null hypothesis can not be rejected with P-value larger than 0.01 in all cases of the two datasets, implying that the output sequence generated by FEDformer shares a more similar distribution as the input sequence than others and thus justifies the our design motivation of FEDformer as discussed in Section 1. More detailed analysis are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper proposes a frequency enhanced transformer model for long-term series forecasting which achieves state-of-the-art performance and enjoys linear computa-tional complexity and memory cost. We propose an attention mechanism with low-rank approximation in frequency and a mixture of experts decomposition to control the distribution shifting. The proposed frequency enhanced structure decouples the input sequence length and the attention matrix dimension, leading to the linear complexity. Moreover, we theoretically and empirically prove the effectiveness of the adopted random mode selection policy in frequency. Lastly, extensive experiments show that the proposed model achieves the best forecasting performance on six benchmark datasets in comparison with four state-ofthe-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>In this section, an overview of the literature for time series forecasting will be given. The relevant works include traditional times series models (A.1), deep learning models (A.1), Transformer-based models (A.2), and the Fourier Transform in neural networks (A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Traditional Time Series Models</head><p>Data-driven time series forecasting helps researchers understand the evolution of the systems without architecting the exact physics law behind them. After decades of renovation, time series models have been well developed and served as the backbone of various projects in numerous application fields. The first generation of data-driven methods can date back to 1970. ARIMA <ref type="bibr" target="#b2">(Box &amp; Jenkins, 1968;</ref><ref type="bibr">Box &amp; Pierce, 1970)</ref> follows the Markov process and builds an auto-regressive model for recursively sequential forecasting. However, an autoregressive process is not enough to deal with nonlinear and non-stationary sequences. With the bloom of deep neural networks in the new century, recurrent neural networks (RNN) was designed especially for tasks involving sequential data. Among the family of RNNs, LSTM <ref type="bibr" target="#b12">(Hochreiter &amp; Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b5">(Chung et al., 2014)</ref> employ gated structure to control the information flow to deal with the gradient vanishing or exploration problem. DeepAR <ref type="bibr" target="#b10">(Flunkert et al., 2017)</ref> uses a sequential architecture for probabilistic forecasting by incorporating binomial likelihood. Attention based RNN <ref type="bibr" target="#b24">(Qin et al., 2017)</ref> uses temporal attention to capture long-range dependencies. However, the recurrent model is not parallelizable and unable to handle long dependencies. The temporal convolutional network <ref type="bibr" target="#b29">(Sen et al., 2019)</ref> is another family efficient in sequential tasks. However, limited to the reception field of the kernel, the features extracted still stay local and long-term dependencies are hard to grasp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Transformers for Time Series Forecasting</head><p>With the innovation of transformers in natural language processing <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b7">Devlin et al., 2019)</ref> and computer vision tasks <ref type="bibr" target="#b8">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b27">Rao et al., 2021)</ref>, transformer-based models are also discussed, renovated, and applied in time series forecasting <ref type="bibr" target="#b37">(Zhou et al., 2021;</ref><ref type="bibr" target="#b34">Wu et al., 2021)</ref>. In sequence to sequence time series forecasting tasks an encoder-decoder architecture is popularly employed. The self-attention and cross-attention mechanisms are used as the core layers in transformers. However, when employing a point-wise connected matrix, the transformers suffer from quadratic computation complexity.</p><p>To get efficient computation without sacrificing too much on performance, the earliest modifications specify the attention matrix with predefined patterns. Examples include: <ref type="bibr" target="#b25">(Qiu et al., 2020)</ref> uses block-wise attention which reduces the complexity to the square of block size. Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> employs a stride window with fixed intervals. LogTrans <ref type="bibr" target="#b17">(Li et al., 2019)</ref> uses logsparse attention and achieves N log 2 N complexity. Htransformer <ref type="bibr" target="#b38">(Zhu &amp; Soricut, 2021</ref>) uses a hierarchical pattern for sparse approximation of attention matrix with O(n) complexity. Some work uses a combination of patterns (BIGBIRD <ref type="bibr" target="#b36">(Zaheer et al., 2020)</ref>) mentioned above. Another strategy is to use dynamic patterns: Reformer <ref type="bibr" target="#b15">(Kitaev et al., 2020)</ref> introduces a local-sensitive hashing which reduces the complexity to N log N . <ref type="bibr" target="#b38">(Zhu &amp; Soricut, 2021)</ref> introduces a hierarchical pattern. Sinkhorn <ref type="bibr" target="#b30">(Tay et al., 2020)</ref> employs a block sorting method to achieve quasi-global attention with only local windows.</p><p>Similarly, some work employs a top-k truncating to accelerate computing: Informer <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref> uses a KLdivergence based method to select top-k in attention matrix. This sparser matrix costs only N log N in complexity. Autoformer <ref type="bibr" target="#b34">(Wu et al., 2021)</ref> introduces an auto-correlation block in place of canonical attention to get the sub-series level attention, which achieves N log N complexity with the help of Fast Fourier transform and top-k selection in an auto-correlation matrix.</p><p>Another emerging strategy is to employ a low-rank approximation of the attention matrix. Linformer <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> uses trainable linear projection to compress the sequence length and achieves O(n) complexity and theoretically proves the boundary of approximation error based on JL lemma. Luna <ref type="bibr" target="#b19">(Ma et al., 2021)</ref> develops a nested linear structure with O(n) complexity. Nyströformer <ref type="bibr" target="#b35">(Xiong et al., 2021)</ref> leverages the idea of Nyström approximation in the attention mechanism and achieves an O(n) complexity. Performer <ref type="bibr" target="#b4">(Choromanski et al., 2021)</ref> adopts an orthogonal random features approach to efficiently model kernelizable attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Fourier Transform in Transformers</head><p>Thanks to the algorithm of fast Fourier transform (FFT), the computation complexity of Fourier transform is compressed from N 2 to N log N . The Fourier transform has the property that convolution in the time domain is equivalent to multiplication in the frequency domain. Thus the FFT can be used in the acceleration of convolutional networks <ref type="bibr" target="#b20">(Mathieu et al., 2014)</ref>. FFT can also be used in efficient computing of auto-correlation function, which can be used as a building neural networks block <ref type="bibr" target="#b34">(Wu et al., 2021)</ref> and also useful in numerous anomaly detection tasks <ref type="bibr" target="#b13">(Homayouni et al., 2020)</ref>. <ref type="bibr" target="#b18">(Li et al., 2020;</ref><ref type="bibr" target="#b11">Gupta et al., 2021)</ref> first introduced Fourier Neural Operator in solving partial differential equations (PDEs). FNO is used as an inner block of networks to perform efficient representation learning in the low-frequency domain. FNO is also proved efficient in computer vision tasks <ref type="bibr" target="#b27">(Rao et al., 2021)</ref>. It also serves as a working horse to build the Wavelet Neural Operator (WNO), which is recently introduced in solving PEDs <ref type="bibr" target="#b11">(Gupta et al., 2021)</ref>. While FNO keeps the spectrum modes in low frequency, random Fourier method use randomly selected modes. <ref type="bibr" target="#b26">(Rahimi &amp; Recht, 2008)</ref> proposes to map the input data to a randomized low-dimensional feature space to accelerate the training of kernel machines. <ref type="bibr" target="#b28">(Rawat et al., 2019)</ref> proposes the Random Fourier softmax (RF-softmax) method that utilizes the powerful Random Fourier Features to enable more efficient and accurate sampling from an approximate softmax distribution.</p><p>To the best of our knowledge, our proposed method is the first work to achieve fast attention mechanism through low rank approximated transformation in frequency domain for time series forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Low-rank Approximation of Attention</head><p>In this section, we discuss the low-rank approximation of the attention mechanism. First, we present the Restricted Isometry Property (RIP) matrices whose approximate error bound could be theoretically given in B.1. Then in B.2, we follow prior work and present how to leverage RIP matrices and attention mechanisms.</p><p>If the signal of interest is sparse or compressible on a fixed basis, then it is possible to recover the signal from fewer measurements. <ref type="bibr" target="#b32">(Wang et al., 2020;</ref><ref type="bibr" target="#b35">Xiong et al., 2021)</ref> suggest that the attention matrix is low-rank, so the attention matrix can be well approximated if being projected into a subspace where the attention matrix is sparse. For the efficient computation of the attention matrix, how to properly select the basis of the projection yet remains to be an open question. The basis which follows the RIP is a potential candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. RIP Matrices</head><p>The definition of the RIP matrices is:</p><p>Definition B.1. RIP matrices. Let m &lt; n be positive integers, Φ be a m × n matrix with real entries, δ &gt; 0, and K &lt; m be an integer. We say that Φ is (K, δ) − RIP , if for every K-sparse vector x ∈ R n we have</p><formula xml:id="formula_23">(1 − δ) x ≤ Φx ≤ (1 + δ) x .</formula><p>RIP matrices are the matrices that satisfy the restricted isometry property, discovered by D. Donoho, E. Candès and T. Tao in the field of compressed sensing. RIP matrices might be good choices for low-rank approximation because of their good properties. A random matrix has a negligible probability of not satisfying the RIP and many kinds of matrices have proven to be RIP, for example, Gaussian basis, Bernoulli basis, and Fourier basis.</p><p>Theorem 2. Let m &lt; n be positive integers, δ &gt; 0, and K = O( m log 4 n ). Let Φ be the random matrix defined by one of the following methods:</p><p>(Gaussian basis) Let the entries of Φ be i.i.d. with a normal distribution N (0, 1 m ). (Bernoulli basis) Let the entries of Φ be i.i.d. with a Bernoulli distribution taking the values ± 1 √ m m, each with 50% probability.</p><p>(Random selected Discrete Fourier basis) Let A ⊂ {0, ..., n − 1} be a random subset of size m. Let Φ be the matrix obtained from the Discrete Fourier transform matrix (i.e. the matrix F with entries F [l, j] = exp −2πilj/n / √ n) for l, j ∈ {0, .., n − 1} by selecting the rows indexed by A.</p><formula xml:id="formula_24">Then Φ is (K, σ) − RIP with probability p ≈ 1 − e −n .</formula><p>Theorem 2 states that Gaussian basis, Bernoulli basis and Fourier basis follow RIP. In the following section, the Fourier basis is used as an example and show how to use RIP basis in low-rank approximation in the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Low-rank Approximation with Fourier</head><p>Basis/Legendre Polynomials Linformer <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> demonstrates that the attention mechanism can be approximated by a low-rank matrix. Linformer uses a trainable kernel initialized with Gaussian distribution for the low-rank approximation, While our proposed FEDformer uses Fourier basis/Legendre Polynomials, Gaussian basis, Fourier basis, and Legendre Polynomials all obey RIP, so similar conclusions could be drawn.</p><p>Starting from Johnson-Lindenstrauss lemma <ref type="bibr" target="#b14">(Johnson, 1984)</ref> and using the version from <ref type="bibr" target="#b0">(Arriaga &amp; Vempala, 2006)</ref>, Linformer proves that a low-rank approximation of the attention matrix could be made.</p><p>Let Φ ∈ R N ×M be the random selected Fourier basis/Legendre Polynomials. Φ is RIP matrix. Referring to Theorem 2, with a probability p ≈ 1−e −n , for any x ∈ R N , we have</p><formula xml:id="formula_25">(1 − δ) x ≤ Φx ≤ (1 + δ) x .<label>(11)</label></formula><p>Referring to <ref type="bibr" target="#b0">(Arriaga &amp; Vempala, 2006)</ref>, with a probability p ≈ 1 − 4e −n , for any x 1 , x 2 ∈ R N , we have</p><formula xml:id="formula_26">(1 − δ) x 1 x ⊤ 2 ≤ x 1 Φ ⊤ Φx ⊤ 2 ≤ (1 + δ) x 1 x ⊤ 2 . (<label>12</label></formula><formula xml:id="formula_27">)</formula><p>With the above inequation function, we now discuss the case in attention mechanism. Let the attention matrix</p><formula xml:id="formula_28">B = sof tmax( QK ⊤ √ d ) = exp(A) • D −1 A , where (D A ) ii = N n=1 exp(A ni ).</formula><p>Following Linformer, we can conclude a theorem as (please refer to <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> for the detailed proof) Theorem 3. For any row vector p ∈ R N of matrix B and any column vector v ∈ R N of matrix V, with a probability p = 1 − o(1), we have</p><formula xml:id="formula_29">bΦ ⊤ Φv ⊤ − bv ⊤ ≤ δ bv ⊤ . (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>Theorem 3 points out the fact that, using Fourier basis/Legendre Polynomials Φ between the multiplication of attention matrix (P ) and values (V ), the computation complexity can be reduced from O(N 2 d) to O(N M d), where d is the hidden dimension of the matrix. In the meantime, the error of the low-rank approximation is bounded. However, Theorem 3 only discussed the case which is without the activation function.</p><p>Furthermore, with the Cauchy inequality and the fact that the exponential function is Lipchitz continuous in a compact region (please refer to <ref type="bibr" target="#b32">(Wang et al., 2020)</ref> for the proof), we can draw the following theorem:</p><p>Theorem 4. For any row vector</p><formula xml:id="formula_31">A i ∈ R N in matrix A (A = QK ⊤ √ d )</formula><p>, with a probability of p = 1 − o(1), we have</p><formula xml:id="formula_32">exp(A i Φ ⊤ )Φv ⊤ −exp(A i )v ⊤ ≤ δ exp(A i )v ⊤ . (<label>14</label></formula><formula xml:id="formula_33">)</formula><p>Theorem 4 states that with the activation function (softmax), the above discussed bound still holds.</p><p>In summary, we can leverage RIP matrices for low-rank approximation of attention. Moreover, there exists theoretical error bound when using a randomly selected Fourier basis for low-rank approximation in the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fourier Component Selection</head><p>Let X 1 (t), . . . , X m (t) be m time series. By applying Fourier transform to each time series, we turn each X i (t) into a vector a i = (a i,1 , . . . , a i,d ) ⊤ ∈ R d . By putting all the Fourier transform vectors into a matrix, we have A = (a 1 , a 2 , . . . , a m ) ⊤ ∈ R m×d , with each row corresponding to a different time series and each column corresponding to a different Fourier component. Here, we propose to select s components from the d Fourier components (s &lt; d) uniformly at random. More specifically, we denote by i 1 &lt; i 2 &lt; . . . &lt; i s the randomly selected components. We construct matrix S ∈ {0, 1} s×d , with S i,k = 1 if i = i k and S i,k = 0 otherwise. Then, our representation of multivariate time series becomes A ′ = AS ⊤ ∈ R m×s . The following theorem shows that, although the Fourier basis is randomly selected, under a mild condition, A ′ can preserve most of the information from A.</p><p>Theorem 5. Assume that µ(A), the coherence measure of matrix A, is Ω(k/n). Then, with a high probability, we have</p><formula xml:id="formula_34">|A − P A ′ (A)| ≤ (1 + ǫ)|A − A k | if s = O(k 2 /ǫ 2 ).</formula><p>Proof. Following the analysis in Theorem 3 from <ref type="bibr" target="#b9">(Drineas et al., 2007)</ref>, we have</p><formula xml:id="formula_35">|A − P A ′ (A)| ≤ |A − A ′ (A ′ ) † A k | = |A − (AS ⊤ )(AS ⊤ ) † A k | = |A − (AS ⊤ )(A k S ⊤ ) † A k |.</formula><p>Using Theorem 5 from <ref type="bibr" target="#b9">(Drineas et al., 2007)</ref>, we have, with a probability at least 0.7,</p><formula xml:id="formula_36">|A − (AS ⊤ )(A k S ⊤ ) † A k | ≤ (1 + ǫ)|A − A k | if s = O(k 2 /ǫ 2 × µ(A)n/k). The theorem follows because µ(A) = O(k/n).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Wavelets</head><p>In this section, we present some technical background about Wavelet transform which is used in our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Continuous Wavelet Transform</head><p>First, let's see how a function f (t) is decomposed into a set of basis functions ψ s,τ (t), called the wavelets. It is known as the continuous wavelet transform or CW T . More formally it is written as</p><formula xml:id="formula_37">γ(s, τ ) = f (t)Ψ * s,τ (t)dt,</formula><p>where * denotes complex conjugation. This equation shows the variables γ(s, τ ), s and τ are the new dimensions, scale, and translation after the wavelet transform, respectively.</p><p>The wavelets are generated from a single basic wavelet Ψ(t), the so-called mother wavelet, by scaling and translation as</p><formula xml:id="formula_38">ψ s,τ (t) = 1 √ s ψ t − τ s ,</formula><p>where s is the scale factor, τ is the translation factor, and √ s is used for energy normalization across the different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Discrete Wavelet Transform</head><p>Continues wavelet transform maps a one-dimensional signal to a two-dimensional time-scale joint representation which is highly redundant. To overcome this problem, people introduce discrete wavelet transformation (DWT) with mother wavelet as</p><formula xml:id="formula_39">ψ j,k (t) = 1 s j 0 ψ t − kτ 0 s j 0 s j 0</formula><p>DWT is not continuously scalable and translatable but can be scaled and translated in discrete steps. Here j and k are integers and s 0 &gt; 1 is a fixed dilation step. The translation factor τ 0 depends on the dilation step. The effect of discretizing the wavelet is that the time-scale space is now sampled at discrete intervals. We usually choose s 0 = 2 so that the sampling of the frequency axis corresponds to dyadic sampling. For the translation factor, we usually choose τ 0 = 1 so that we also have a dyadic sampling of the time axis.</p><p>When discrete wavelets are used to transform a continuous signal, the result will be a series of wavelet coefficients and it is referred to as the wavelet decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Orthogonal Polynomials</head><p>The next thing we need to focus on is orthogonal polynomials (OPs), which will serve as the mother wavelet function we introduce before. A lot of properties have to be maintained to be a mother wavelet, like admissibility condition, regularity conditions, and vanishing moments. In short, we are interested in the OPs that are non-zero over a finite domain and are zero almost everywhere else. Legendre is a popular set of OPs used it in our work here. Some other popular OPs can also be used here like Chebyshev without much modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Legendre Polynomails</head><p>The Legendre polynomials are defined with respect to (w.r.t.) a uniform weight function w L (x) = 1 for −1</p><formula xml:id="formula_40">x 1 or w L (x) = 1 [−1,1] (x) such that 1 −1 P i (x)P j (x)dx = 2 2i+1 i = j, 0 i = j.</formula><p>Here the function is defined over [−1, 1], but it can be extended to any interval [a, b] by performing different shift and scale operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Multiwavelets</head><p>The multiwavelets which we use in this work combine advantages of the wavelet and OPs we introduce be-fore. Other than projecting a given function onto a single wavelet function, multiwavelet projects it onto a subspace of degree-restricted polynomials. In this work, we restricted our exploration to one family of OPs: Legendre Polynomials.</p><p>First, the basis is defined as: A set of orthonormal basis w.r.t. measure µ, are φ 0 , . . . , φ k−1 such that φ i , φ j µ = δ ij . With a specific measure (weighting function w(x)), the orthonormality condition can be written as φ i (x)φ j (x)w(x)dx = δ ij .</p><p>Follow the derivation in <ref type="bibr" target="#b11">(Gupta et al., 2021)</ref>, through using the tools of Gaussian Quadrature and Gram-Schmidt Orthogonalizaition, the filter coefficients of multiwavelets using Legendre polynomials can be written as</p><formula xml:id="formula_41">H (0) ij = √ 2 1/2 0 φ i (x)φ j (2x)w L (2x − 1)dx = 1 √ 2 1 0 φ i (x/2)φ j (x)dx = 1 √ 2 k i=1 ω i φ i x i 2 φ j (x i ) .</formula><p>For example, if k = 3, following the formula, the filter coefficients are derived as follows   </p><formula xml:id="formula_42">H 0 = [ 1 √ 2 0 0 − √ 3 2 √ 2 1 2 √ 2 0 0 − √ 15 4 √ 2 1 4 √ 2 ], H 1 = [ 1 √ 2 0 0 √ 3 2 √ 2 1 2 √ 2 0 0 √ 15 4 √ 2 1 4 √ 2 ], G 0 = [ 1 2 √ 2 √ 3 2 √ 2 0 0 1 4 √ 2 √ 15 4 √ 2 0 0 1 √ 2 ], G 1 = [ − 1 2 √ 2 √ 3 2 √ 2 0 0 − 1 4 √ 2 √ 15 4 √ 2 0 0 − 1 √ 2 ] E. Output</formula><formula xml:id="formula_43">D n,m = sup x |F 1,n (x) − F 2,m (x)|</formula><p>where F 1,n and F 2,m are the empirical distribution functions of the first and the second sample respectively, and sup is the supremum function. For large samples, the null hypothesis is rejected at level α if</p><formula xml:id="formula_44">D n,m &gt; − 1 2 ln α 2 • n + m n • m ,</formula><p>where n and m are the sizes of the first and second samples respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Distribution Experiments and Analysis</head><p>Though the KS test omits the temporal information from the input and output sequence, it can be used as a tool to measure the global property of the foretasting output sequence compared to the input sequence. The null hypothesis is that the two samples come from the same distribution. We can tell that if the P-value of the KS test is large and then the null hypothesis is less likely to be rejected for true output distribution.</p><p>We applied KS test on the output sequence of 96-720 prediction tasks for various models on the ETTm1 and ETTm2 datasets, and the results are summarized in Table <ref type="table" target="#tab_9">6</ref>. In the test, we compare the fixed 96-time step input sequence distribution with the output sequence distribution of different lengths. Using a 0.01 P-value as statistics, various existing Transformer baseline models have much less P-value than 0.01 except Autoformer, which indicates they have a higher probability to be sampled from the different distributions. Autoformer and FEDformer have much larger P value compared to other models, which mainly contributes to their seasonal trend decomposition mechanism. Though we get close results from ETTm1 by both models, the proposed FEDformer has much larger P-values in ETTm1. And it is the only model whose null hypothesis can not be rejected with P-value larger than 0.01 in all cases of the two datasets, implying that the output sequence generated by FEDformer shares a more similar distribution as the input sequence than others and thus justifies the our design motivation of FEDformer as discussed in Section 1.</p><p>Note that in the ETTm1 dataset, the True output sequence has a smaller P-value compared to our FEDformer's predicted output, it shows that the model's close output distribution is achieved through model's control other than merely more accurate prediction. This analysis shed some light on why the seasonal-trend decomposition architecture can give us better performance in long-term forecasting.</p><p>The design is used to constrain the trend (mean) of the output distribution. Inspired by such observation, we design frequency enhanced block to constrain the seasonality (frequency mode) of the output distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Supplemental Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Dataset Details</head><p>In this paragraph, the details of the experiment datasets are summarized as follows: 1) ETT <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref> dataset contains two sub-dataset: ETT1 and ETT2, collected from two electricity transformers at two stations. Each of them has two versions in different resolutions (15min &amp; 1h). ETT dataset contains multiple series of loads and one series of oil temperatures. 2) Electricity 1 dataset contains the electricity consumption of clients with each column corresponding to one client. 3) Exchange <ref type="bibr" target="#b16">(Lai et al., 2018)</ref> contains the current exchange of 8 countries. 4) Traffic 2 dataset contains the occupation rate of freeway system across the State of California. 5) Weather 3 dataset contains 21 meteorological indicators for a range of 1 year in Germany. 6) Illness 4 dataset contains the influenza-like illness patients in the United States. Table <ref type="table" target="#tab_10">7</ref> summarizes feature details (Sequence Length: Len, Dimension: Dim, Frequency: Freq) of the six datasets. All datasets are split into the training set, validation set and test set by the ratio of 7:1:2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Implementation Details</head><p>Our model is trained using ADAM (Kingma &amp; Ba, 2017) optimizer with a learning rate of 1e −4 . The batch size is set to 32. An early stopping counter is employed to stop the training process after three epochs if no loss degradation on the valid set is observed. The mean square error (MSE) and mean absolute error (MAE) are used as metrics.</p><p>All experiments are repeated 5 times and the mean of the metrics is used in the final results. All the deep learning networks are implemented in PyTorch <ref type="bibr" target="#b23">(Paszke et al., 2019)</ref> and trained on NVIDIA V100 32GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. ETT Full Benchmark</head><p>We present the full-benchmark on the four ETT datasets <ref type="bibr" target="#b37">(Zhou et al., 2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Cross Attention Visualization</head><p>The σ( Q • K⊤ ) can be viewed as the cross attention weight for our proposed frequency enhanced cross attention block. Several different activation functions can be used for attention matrix activation. Tanh and softmax are tested in this work with various performances on different datasets. We use tanh as the default one. Different attention patterns are visualized in Figure <ref type="figure" target="#fig_6">8</ref>. Here two samples of cross attention maps are shown for FEDformer-f training on the ETTm2 dataset using tanh and softmax respectively. It can be seen that attention with Softmax as activation function seems to be more sparse than using tanh. Overall we can see attention in the frequency domain is much sparser compared to the normal attention graph in the time domain, which indicates our proposed attention can represent the signal more compactly. Also this compact representation supports our random mode selection mechanism to achieve linear complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5. Improvements of Mixture of Experts Decomposition</head><p>We design a mixture of experts decomposition mechanism which adopts a set of average pooling layers to extract the trend and a set of data-dependent weights to combine them. The default average pooling layers contain filters with kernel size 7, 12, 14, 24 and 48 respectively. For compari-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1 dataset. Left: frequency mode and trend shift. Right: trend shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2. FEDformer Structure. The FEDformer consists of N encoders and M decoders. The Frequency Enhanced Block (FEB, green blocks) and Frequency Enhanced Attention (FEA, red blocks) are used to perform representation learning in frequency domain. Either FEB or FEA has two subversions (FEB-f &amp; FEB-w or FEA-f &amp; FEA-w), where '-f' means using Fourier basis and '-w' means using Wavelet basis. The Mixture Of Expert Decomposition Blocks (MOEDecomp, yellow blocks) are used to extract seasonal-trend patterns from the input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represents the projection operator. If A ′ preserves a large portion of information from A, we would expect a small error between A and P A ′ (A), i.e. |A − P A ′ (A)|. Let A k represent the approximation of A by its first k largest single value decomposition. The theorem below shows that |A−P A ′ (A)| is close to |A−A k | if the number of randomly sampled Fourier components s is on the order of k 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Top Left: Wavelet frequency enhanced block decomposition stage. Top Right: Wavelet block reconstruction stage shared by FEB-w and FEA-w. Bottom: Wavelet frequency enhanced cross attention decomposition stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Distribution Analysis E.1. Bad Case Analysis Using vanilla Transformer as baseline model, we demonstrate two bad long-term series forecasting cases in ETTm1 dataset as shown in the following Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1 dataset. Left: frequency mode and trend shift. Right: trend shift.</figDesc><graphic url="image-9.png" coords="15,55.44,586.45,116.95,87.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Multihead attention map with 8 heads using tanh (top) and softmax (bottom) as activation map for the FEDformer-f training on ETTm2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Complexity analysis of different forecasting models.</figDesc><table><row><cell>Methods</cell><cell>Time</cell><cell>Training Memory</cell><cell>Testing Steps</cell></row><row><cell>FEDformer</cell><cell>O</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The comparisons of the time complexity and memory usage in training and the inference steps in testing are summarized in Table 1. It can be seen that the proposed FEDformer achieves the best overall complexity among Transformer-based forecasting models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Multivariate long-term series forecasting results on six datasets with input length I = 96 and prediction length O ∈ {96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O ∈ {24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell>Metric</cell><cell>96</cell><cell>ETTm2 192 336 720</cell><cell>96</cell><cell>Electricity 192 336 720</cell><cell>96</cell><cell>Exchange 192 336 720</cell><cell>96</cell><cell>Traffic 192 336</cell><cell>720</cell><cell>96</cell><cell>Weather 192 336 720</cell><cell>24</cell><cell>36</cell><cell>ILI</cell><cell>48</cell><cell>60</cell></row><row><cell>FEDformer-f</cell><cell cols="17">MSE 0.203 0.269 0.325 0.421 0.193 0.201 0.214 0.246 0.148 0.271 0.460 1.195 0.587 0.604 0.621 0.626 0.217 0.276 0.339 0.403 3.228 2.679 2.622 2.857 MAE 0.287 0.328 0.366 0.415 0.308 0.315 0.329 0.355 0.278 0.380 0.500 0.841 0.366 0.373 0.383 0.382 0.296 0.336 0.380 0.428 1.260 1.080 1.078 1.157</cell></row><row><cell>FEDformer-w</cell><cell cols="17">MSE 0.204 0.316 0.359 0.433 0.183 0.195 0.212 0.231 0.139 0.256 0.426 1.090 0.562 0.562 0.570 0.596 0.227 0.295 0.381 0.424 2.203 2.272 2.209 2.545 MAE 0.288 0.363 0.387 0.432 0.297 0.308 0.313 0.343 0.276 0.369 0.464 0.800 0.349 0.346 0.323 0.368 0.304 0.363 0.416 0.434 0.963 0.976 0.981 1.061</cell></row><row><cell>Autoformer</cell><cell cols="17">MSE 0.255 0.281 0.339 0.422 0.201 0.222 0.231 0.254 0.197 0.300 0.509 1.447 0.613 0.616 0.622 0.660 0.266 0.307 0.359 0.419 3.483 3.103 2.669 2.770 MAE 0.339 0.340 0.372 0.419 0.317 0.334 0.338 0.361 0.323 0.369 0.524 0.941 0.388 0.382 0.337 0.408 0.336 0.367 0.395 0.428 1.287 1.148 1.085 1.125</cell></row><row><cell>Informer</cell><cell cols="17">MSE 0.365 0.533 1.363 3.379 0.274 0.296 0.300 0.373 0.847 1.204 1.672 2.478 0.719 0.696 0.777 0.864 0.300 0.598 0.578 1.059 5.764 4.755 4.763 5.264 MAE 0.453 0.563 0.887 1.338 0.368 0.386 0.394 0.439 0.752 0.895 1.036 1.310 0.391 0.379 0.420 0.472 0.384 0.544 0.523 0.741 1.677 1.467 1.469 1.564</cell></row><row><cell>LogTrans</cell><cell cols="17">MSE 0.768 0.989 1.334 3.048 0.258 0.266 0.280 0.283 0.968 1.040 1.659 1.941 0.684 0.685 0.7337 0.717 0.458 0.658 0.797 0.869 4.480 4.799 4.800 5.278 MAE 0.642 0.757 0.872 1.328 0.357 0.368 0.380 0.376 0.812 0.851 1.081 1.127 0.384 0.390 0.408 0.396 0.490 0.589 0.652 0.675 1.444 1.467 1.468 1.560</cell></row><row><cell>Reformer</cell><cell cols="17">MSE 0.658 1.078 1.549 2.631 0.312 0.348 0.350 0.340 1.065 1.188 1.357 1.510 0.732 0.733 0.742 0.755 0.689 0.752 0.639 1.130 4.400 4.783 4.832 4.882 MAE 0.619 0.827 0.972 1.242 0.402 0.433 0.433 0.420 0.829 0.906 0.976 1.016 0.423 0.420 0.420 423 0.596 0.638 0.596 0.792 1.382 1.448 1.465 1.483</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Univariate long-term series forecasting results on six datasets with input length I = 96 and prediction length O ∈ {96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O ∈ {24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell>Metric</cell><cell>96</cell><cell>ETTm2 192 336 720</cell><cell>96</cell><cell>Electricity 192 336 720</cell><cell>96</cell><cell>Exchange 192 336 720</cell><cell>96</cell><cell>Traffic 192 336 720</cell><cell>96</cell><cell>Weather 192 336</cell><cell>720</cell><cell>24</cell><cell>36</cell><cell>ILI</cell><cell>48</cell><cell>60</cell></row><row><cell>FEDformer-f</cell><cell cols="17">MSE 0.072 0.102 0.130 0.178 0.253 0.282 0.346 0.422 0.154 0.286 0.511 1.301 0.207 0.205 0.219 0.244 0.0062 0.0060 0.0041 0.0055 0.708 0.584 0.717 0.855 MAE 0.206 0.245 0.279 0.325 0.370 0.386 0.431 0.484 0.304 0.420 0.555 0.879 0.312 0.312 0.323 0.344 0.062 0.062 0.050 0.059 0.627 0.617 0.697 0.774</cell></row><row><cell>FEDformer-w</cell><cell cols="17">MSE 0.063 0.110 0.147 0.219 0.262 0.316 0.361 0.448 0.131 0.277 0.426 1.162 0.170 0.173 0.178 0.187 0.0035 0.0054 0.008 0.015 0.693 0.554 0.699 0.828 MAE 0.189 0.252 0.301 0.368 0.378 0.410 0.445 0.501 0.284 0.420 0.511 0.832 0.263 0.265 0.266 0.286 0.046 0.059 0.072 0.091 0.629 0.604 0.696 0.770</cell></row><row><cell>Autoformer</cell><cell cols="17">MSE 0.065 0.118 0.154 0.182 0.341 0.345 0.406 0.565 0.241 0.300 0.509 1.260 0.246 0.266 0.263 0.269 0.011 0.0075 0.0063 0.0085 0.948 0.634 0.791 0.874 MAE 0.189 0.256 0.305 0.335 0.438 0.428 0.470 0.581 0.387 0.369 0.524 0.867 0.346 0.370 0.371 0.372 0.081 0.067 0.062 0.070 0.732 0.650 0.752 0.797</cell></row><row><cell>Informer</cell><cell cols="17">MSE 0.080 0.112 0.166 0.228 0.258 0.285 0.336 0.607 1.327 1.258 2.179 1.280 0.257 0.299 0.312 0.366 0.004 0.002 0.004 0.003 5.282 4.554 4.273 5.214 MAE 0.217 0.259 0.314 0.380 0.367 0.388 0.423 0.599 0.944 0.924 1.296 0.953 0.353 0.376 0.387 0.436 0.044 0.040 0.049 0.042 2.050 1.916 1.846 2.057</cell></row><row><cell>LogTrans</cell><cell cols="17">MSE 0.075 0.129 0.154 0.160 0.288 0.432 0.430 0.491 0.237 0.738 2.018 2.405 0.226 0.314 0.387 0.437 0.0046 0.0060 0.0060 0.007 3.607 2.407 3.106 3.698 MAE 0.208 0.275 0.302 0.322 0.393 0.483 0.483 0.531 0.377 0.619 1.070 1.175 0.317 0.408 0.453 0.491 0.052 0.060 0.054 0.059 1.662 1.363 1.575 1.733</cell></row><row><cell>Reformer</cell><cell cols="17">MSE 0.077 0.138 0.160 0.168 0.275 0.304 0.370 0.460 0.298 0.777 1.833 1.203 0.313 0.386 0.423 0.378 0.012 0.0098 0.013 0.011 3.838 2.934 3.755 4.162 MAE 0.214 0.290 0.313 0.334 0.379 0.402 0.448 0.511 0.444 0.719 1.128 0.956 0.383 0.453 0.468 0.433 0.087 0.044 0.100 0.083 1.720 1.520 1.749 1.847</cell></row><row><cell cols="8">ity in its time series, but FEDformer can still achieve su-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">perior performance. Overall, the improvement made by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FEDformer is consistent with varying horizons, implying</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">its strength in long term forecasting. More detailed results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">on ETT full benchmark are provided in Appendix F.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies: multivariate long-term series forecasting results on ETTm1 and ETTm2 with input length I = 96 and prediction length O ∈ {96, 192, 336, 720}. Three variants of FEDformer-f are compared with baselines. The best results are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Methods</cell><cell cols="2">Transformer</cell><cell></cell><cell cols="3">Informer</cell><cell cols="2">Autoformer</cell><cell cols="2">FEDformer V1 FEDformer V2 FEDformer V3</cell><cell>FEDformer-f</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Self-att</cell><cell>FullAtt</cell><cell></cell><cell></cell><cell cols="2">ProbAtt</cell><cell></cell><cell cols="2">AutoCorr</cell><cell cols="2">FEB-f(Eq. 4)</cell><cell>AutoCorr</cell><cell>FEA-f(Eq. 7)</cell><cell>FEB-f(Eq. 4)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cross-att</cell><cell>FullAtt</cell><cell></cell><cell></cell><cell cols="2">ProbAtt</cell><cell></cell><cell cols="2">AutoCorr</cell><cell cols="2">AutoCorr</cell><cell>FEA-f(Eq. 7)</cell><cell>FEA-f(Eq. 7)</cell><cell>FEA-f(Eq. 7)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Metric</cell><cell cols="9">MSE MAE MSE MAE MSE MAE MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE MAE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTm1</cell><cell cols="11">96 192 0.526 0.502 0.564 0.521 0.628 0.526 0.417 0.442 0.556 0.499 0.552 0.493 0.426 0.441 0.525 0.486 0.458 0.465 0.481 0.463 0.378 0.419 0.539 0.490 0.534 0.482 0.379 0.419 336 0.514 0.502 0.672 0.559 0.728 0.567 0.480 0.477 0.541 0.498 0.565 0.503 0.445 0.459 720 0.564 0.529 0.714 0.596 0.658 0.548 0.543 0.517 0.558 0.507 0.585 0.515 0.543 0.490</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTm2</cell><cell cols="11">96 192 0.304 0.355 0.300 0.360 0.281 0.340 0.285 0.344 0.274 0.331 0.272 0.329 0.269 0.328 0.268 0.346 0.227 0.305 0.255 0.339 0.259 0.337 0.216 0.297 0.211 0.292 0.203 0.287 336 0.365 0.400 0.382 0.410 0.339 0.372 0.320 0.373 0.334 0.369 0.327 0.363 0.325 0.366 720 0.475 0.466 1.637 0.794 0.422 0.419 0.761 0.628 0.427 0.420 0.418 0.415 0.421 0.415</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Count</cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell cols="2">0</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>6</cell><cell>7</cell><cell>7</cell><cell>8</cell><cell>8</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>h1 rand h1 fix m1 rand</cell><cell></cell><cell>0.440</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m1 fix</cell><cell></cell><cell>0.435</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSE</cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MSE</cell><cell>0.425 0.430</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.420</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>h2 rand h2 fix</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m2 rand</cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="2">16 32 64 128 256</cell><cell></cell><cell>0.415</cell><cell>2</cell><cell>4</cell><cell cols="3">8 16 32 64 128 256 m2 fix</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mode number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mode number</cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison of two base-modes selection method (Fix&amp;Rand). Rand policy means randomly selecting a subset of modes, Fix policy means selecting the lowest frequency modes. Two policies are compared on a variety of base-modes number M ∈ {2, 4, 8...256} on ETT full-benchmark (h1, m1, h2, m2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>P-values of Kolmogrov-Smirnov test of different transformer models for long-term forecasting output on ETTm1 and ETTm2 dataset. Larger value indicates the hypothesis (the input sequence and forecasting output come from the same distribution) is less likely to be rejected. The best results are highlighted.</figDesc><table><row><cell cols="6">Methods Transformer Informer Autoformer FEDformer</cell><cell>True</cell></row><row><cell>ETTm1</cell><cell>96 192 336 720</cell><cell>0.0090 0.0052 0.0022 0.0023</cell><cell>0.0055 0.0029 0.0019 0.0016</cell><cell>0.020 0.015 0.012 0.008</cell><cell>0.048 0.028 0.015 0.014</cell><cell>0.023 0.013 0.010 0.004</cell></row><row><cell>ETTm2</cell><cell>96 192 336 720</cell><cell>0.0012 0.0011 0.0005 0.0008</cell><cell>0.0008 0.0006 0.00009 0.0002</cell><cell>0.079 0.047 0.027 0.023</cell><cell>0.071 0.045 0.028 0.021</cell><cell>0.087 0.060 0.042 0.023</cell></row><row><cell cols="2">Count</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>5</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Kolmogrov-Smirnov test P value for long sequence time-series forecasting output on ETT dataset (full experiment)</figDesc><table><row><cell cols="9">Methods Transformer LogTrans Informer Reformer Autoformer FEDformer True</cell></row><row><cell>ETTm1</cell><cell>96 192 336 720</cell><cell>0.0090 0.0052 0.0022 0.0023</cell><cell>0.0073 0.0043 0.0026 0.0064</cell><cell>0.0055 0.0029 0.0019 0.0016</cell><cell>0.0055 0.0013 0.0006 0.0011</cell><cell>0.020 0.015 0.012 0.008</cell><cell>0.048 0.028 0.015 0.014</cell><cell>0.023 0.013 0.010 0.004</cell></row><row><cell>ETTm2</cell><cell>96 192 336 720</cell><cell>0.0012 0.0011 0.0005 0.0008</cell><cell>0.0025 0.0011 0.0011 0.0005</cell><cell>0.0008 0.0006 0.00009 0.0002</cell><cell>0.0028 0.0015 0.0007 0.0005</cell><cell>0.078 0.047 0.027 0.023</cell><cell>0.071 0.045 0.028 0.021</cell><cell>0.087 0.060 0.042 0.023</cell></row><row><cell cols="2">Count</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>5</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Summarized feature details of six datasets.</figDesc><table><row><cell>DATASET</cell><cell>LEN</cell><cell>DIM</cell><cell>FREQ</cell></row><row><cell>ETTM2</cell><cell>69680</cell><cell>8</cell><cell>15 MIN</cell></row><row><cell cols="3">ELECTRICITY 26304 322</cell><cell>1H</cell></row><row><cell>EXCHANGE</cell><cell>7588</cell><cell>9</cell><cell>1 DAY</cell></row><row><cell>TRAFFIC</cell><cell cols="2">17544 863</cell><cell>1H</cell></row><row><cell>WEATHER</cell><cell>52696</cell><cell>22</cell><cell>10 MIN</cell></row><row><cell>ILI</cell><cell>966</cell><cell>8</cell><cell>7 DAYS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>in Table 8 (multivariate forecasting) and Table 9 (univariate forecasting). The ETTh1 and ETTh2 are 1 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams 20112014 2 http://pems.dot.ca.gov 3 https://www.bgc-jena.mpg.de/wetter/ 4 https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html Multivariate long sequence time-series forecasting results on ETT full benchmark. The best results are highlighted in bold. hourly while ETTm1 and ETTm2 are recorded every 15 minutes. The time series in ETTh1 and ETTm1 follow the same pattern, and the only difference is the sampling rate, similarly for ETTh2 and ETTm2. On average, our FEDformer yields a 11.5% relative MSE reduction for multivariate forecasting, and a 9.4% reduction for univariate forecasting over the SOTA results from Autoformer.</figDesc><table><row><cell cols="2">Methods FEDformer-f FEDformer-w</cell><cell>Autoformer</cell><cell>Informer</cell><cell>LogTrans</cell><cell>Reformer</cell></row><row><cell>Metric</cell><cell cols="5">MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</cell></row><row><cell>ET T h1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>recorded</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">de S l,1 de S l,1 en S l,2en (or X l en )</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>son, we use single expert decomposition mechanism which employs a single average pooling layer with a fixed kernel size of 24 as the baseline. In Table <ref type="table">10</ref>, a comparison study of multivariate forecasting is shown using FEDformer-f model on two typical datasets. It is observed that the designed mixture of experts decomposition brings better performance than the single decomposition scheme.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithmic theory of learning: Robust concepts and random projection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="182" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno>CoRR, abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distribution of residual autocorrelations in autoregressive-integrated moving average time series models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1509" to="1526" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
	<note>Some recent advances in forecasting and control</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Francis</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations (ICLR), Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stl: A seasonal-trend decomposition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Terpenning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of official statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="73" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relative-error CUR matrix decompositions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<idno>CoRR, abs/0708.3696</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><surname>Deepar</surname></persName>
		</author>
		<idno>CoRR, abs/1704.04110</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiwavelet-based operator learning for differential equations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1530" to="888X" />
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An autocorrelation-based lstmautoencoder for anomaly detection on time-series data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Homayouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gondalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5068" to="5077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into hilbert space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">A Method for Stochastic Optimization</title>
				<imprint>
			<date type="published" when="1984-01">1984. January 2017</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>CoRR, abs/2010.08895</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Luna</surname></persName>
		</author>
		<idno>CoRR, abs/2106.01540</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast training of convolutional networks through ffts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations (ICLR)</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>N-Beats</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
				<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013. 2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 19-25, 2017. 2017</date>
			<biblScope unit="page" from="2627" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blockwise self-attention for long document understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">16-20 November 2020. 2020</date>
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
	<note>EMNLP 2020 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008. 2018</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Global filter networks for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2107.00645</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sampled softmax with random fourier features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="13834" to="13844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Think globally, act locally: A deep neural network approach to highdimensional time series forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), December 8-14</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4838" to="4847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A robust seasonal-trend decomposition algorithm for long time series</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Robuststl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5409" to="5416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for longterm series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>the Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nyströmformer: A nyström-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14138" to="14148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast onedimensional hierarchical attention for sequences</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><surname>H-Transformer-1d</surname></persName>
		</author>
		<idno>2021. 96 0.376 0.419 0.395 0.424 0.449 0.459 0.865 0.713 0.878 0.740 0.837 0.728 192 0.420 0.448 0.469 0.470 0.500 0.482 1.008 0.792 1.037 0.824 0.923 0.766 336 0.459 0.465 0.530 0.499 0.521 0.496 1.107 0.809 1.238 0.932 1.097 0.835 720 0.506 0.507 0.598 0.544 0.514 0.512 1.181 0.865 1.135 0.852 1.257 0.889</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL) 2021, Virtual Event</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL) 2021, Virtual Event</meeting>
		<imprint>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="page" from="3801" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Univariate long sequence time-series forecasting results on ETT full benchmark. The best results are highlighted in bold</title>
		<imprint/>
	</monogr>
	<note>Methods FEDformer-f FEDformer-w Autoformer Informer LogTrans Reformer Metric MSE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
