<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINITE SCALAR QUANTIZATION: VQ-VAE MADE SIMPLE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-27">27 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">FINITE SCALAR QUANTIZATION: VQ-VAE MADE SIMPLE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-27">27 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.15505v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Vector quantization (VQ), initially introduced by <ref type="bibr" target="#b19">Gray (1984)</ref>, has recently seen a renaissance in the context of learning discrete representations with neural networks. Spurred by the success of VQ-VAE <ref type="bibr" target="#b39">(Van Den Oord et al., 2017)</ref>, <ref type="bibr" target="#b18">Esser et al. (2020)</ref> and <ref type="bibr" target="#b40">Villegas et al. (2022)</ref> showed that training an autoregressive transformer on the representations of a VQ-VAE trained with a GAN loss enables powerful image and video generation models, respectively. At the same time, VQ has become popular component in image <ref type="bibr" target="#b6">(Bao et al., 2021;</ref><ref type="bibr" target="#b27">Li et al., 2023)</ref> and audio <ref type="bibr" target="#b4">(Baevski et al., 2019)</ref> representation learning, and is a promising building block for the next generation of multimodal large language models <ref type="bibr" target="#b0">(Aghajanyan et al., 2022;</ref><ref type="bibr" target="#b22">Kim et al., 2023;</ref><ref type="bibr" target="#b1">Aghajanyan et al., 2023)</ref>.</p><p>When training VQ-VAE, the goal is to learn a codebook C whose elements induce a compressed, semantic representation of the input data (typically images). In the forward pass, an image x is encoded into a representation z (typically a sequence of feature vectors), and each vector in z quantized to (i.e., replaced with) the closest vector in C. The quantization operation is not differentiable. When training a VAE with VQ in the latent representation, <ref type="bibr" target="#b39">Van Den Oord et al. (2017)</ref> use the straightthrough estimator (STE) <ref type="bibr" target="#b7">(Bengio et al., 2013)</ref>, copying the gradients from the decoder input to the encoder output, resulting in gradients to the encoder. Since this still does not produce gradients for the codebook vectors, they further introduce two auxiliary losses to pull the codeword vectors towards the (unquantized) representation vectors and vice-versa.</p><p>The above formulation is challenging to optimize, and leads to the well-documented problem of underutilized codebooks <ref type="bibr" target="#b25">(?a?cucki et al., 2020;</ref><ref type="bibr" target="#b36">Takida et al., 2022;</ref><ref type="bibr" target="#b16">Dhariwal et al., 2020;</ref><ref type="bibr" target="#b21">Huh et al., 2023)</ref>: as the size of C is increased, many codewords will be unused. Subsequent works aimed to improve this with various tricks such as reinitializing the entire codebook or some codewords <ref type="bibr" target="#b16">Dhariwal et al. (2020)</ref>; <ref type="bibr">?a?cucki et al. (2020)</ref>, stochastic formulations <ref type="bibr" target="#b36">Takida et al. (2022)</ref>, etc. (see Sec. 2).</p><p>? Significant technical contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSQ</head><p>(1, 0, -1) 1 0 -1 VQ Figure <ref type="figure">1</ref>: FSQ (left): the final encoder layer projects to d dimensions (d = 3 shown). We bound each dimension of the encoder output z to L values (L = 3 shown), and then round to integers, resulting in the quantized ?, the nearest point in this hypercube. VQ (right): The final encoder layer projects to d dimensions (d = 7 shown, as d is typically much larger for VQ). The resulting vector z is replaced with the closest vector from the codebook, ?, by nearest neighbor lookup.</p><p>Here, we are interested in simplifying the original VQ-VAE formulation <ref type="bibr" target="#b39">(Van Den Oord et al., 2017)</ref> with the following goals: i) remove auxiliary losses, ii) achieve high codebook utilization by design, and iii) keep the functional setup the same to the extent that we obtain a drop-in replacement for VQ.</p><p>To this end, we draw inspiration from the neural compression literature, where discrete codes are typically obtained with scalar quantization, following initial work <ref type="bibr" target="#b5">(Ball? et al., 2016;</ref><ref type="bibr" target="#b37">Theis et al., 2017)</ref>: Each (scalar) entry in the representation z is independently quantized to the nearest integer by rounding. The majority of the current compression literature uses unbounded scalar quantization, where the range of integers is not limited by the encoder, only by constraining the entropy of the representation. Other compression work relied on bounding the range of the quantizer <ref type="bibr" target="#b30">(Mentzer et al., 2018;</ref><ref type="bibr">Tschannen et al., 2018;</ref><ref type="bibr" target="#b2">Agustsson et al., 2019)</ref>.</p><p>We call this approach finite scalar quantization (FSQ). The important insight is that by carefully choosing how to bound each channel, we can get an implicit codebook of (almost) any desired size: Consider a vector z with d channels. If we map each entry z i to L values (e.g., via z i ? ?L/2?tanh(z i ) followed by rounding to integers), we obtain a quantized ?, where ? is one of L d unique possible vectors. Fig. <ref type="figure">1</ref> shows FSQ for d=3, L=3, implying a codebook C = {(-1, -1, -1), (-1, -1, 0), (-1, -1, 1), . . . , (1, 1, 1)}, where |C| = L d = 27.</p><p>To get gradients through the rounding operation, we use the STE like VQ-VAE. Thus, using FSQ inside an autoencoder trained with a reconstruction loss, we get gradients to the encoder that force the model to spread the information into multiple quantization bins, as that reduces the reconstruction loss. As a result, we obtain a quantizer that uses all codewords without any auxiliary losses.</p><p>To the best of our knowledge, FSQ has not been used outside of compression, where VQ remains dominant. We aim to change this by revisiting FSQ in conjunction with powerful transformers/language models. In summary, our contributions are:</p><p>1. We show that FSQ can serve as a drop-in replacement for VQ in various architectures, for different datasets and tasks, by applying it to MaskGIT <ref type="bibr" target="#b10">(Chang et al., 2022)</ref> for image generation, and in UViM <ref type="bibr" target="#b23">(Kolesnikov et al., 2022)</ref> for depth estimation, colorization, and panoptic segmentation. We observe a reduction of only 0.5 -3% in the respective metrics, and correspondingly get highly similar visual results. We emphasize that the two model families have very different designs (convolutional vs. transformer-based autoencoders, masked vs. fully autoregressive transformers, decoder-only vs. encoder-decoder transformers, etc.). 2. We analyze the trade-offs for VQ vs. FSQ, characterize the scaling behaviors w.r.t. codebook size of the two models, and analyze the representation complexity from a compression angle. We find that FSQ is able to leverage large codebooks for better reconstruction metrics, and better sample quality. The codebook usage is very high for FSQ (?100% for most models), without relying on any auxiliary losses. 3. We show that the full generality of the VQ formulation gives little benefits over our simpler FSQ method (VQ is actually worse for large codebooks C). This can be attributed to VQ being difficult to optimize, whereas FSQ can be viewed as the standard VQ formulation changed such that a) the encoder output is bounded and b) C is fixed. We note that the (implicit) FSQ C has much smaller dimensionality vs. VQ (typically d &lt; 10 for FSQ, vs. d ? 512 for VQ). VQ Alternatives Residual quantization (RVQ) has been used for image <ref type="bibr" target="#b26">(Lee et al., 2022)</ref> and audio <ref type="bibr" target="#b43">(Zeghidour et al., 2021)</ref> generation. There, quantized codes are refined by additionally storing (quantized) residuals. In Product quantization (PQ) <ref type="bibr" target="#b17">(El-Nouby et al., 2022)</ref>, the codebook is factored into a product of smaller codebooks. In a similar spirit, there is a body of literature around reducing the number of tokens output by VQ-VAEs for more efficient inference, see, e.g., <ref type="bibr" target="#b20">Huang et al. (2023)</ref>.</p><p>Neural compression Many works <ref type="bibr" target="#b5">(Ball? et al., 2016;</ref><ref type="bibr" target="#b33">Minnen et al., 2018;</ref><ref type="bibr" target="#b29">Lu et al., 2019;</ref><ref type="bibr" target="#b31">Mentzer et al., 2020;</ref><ref type="bibr" target="#b12">Cheng et al., 2020)</ref> rely on unbounded scalar quantization and constrain the entropy of the quantized representation to prevent spreading to all integers. Bounded scalar quantization (i.e., FSQ), has been used to represent images with high fidelity <ref type="bibr" target="#b30">(Mentzer et al. (2018)</ref> use d=16, L=5), and for "extreme compression" <ref type="bibr" target="#b2">(Agustsson et al. (2019)</ref> used d=5, L=5). To the best of our knowledge, FSQ has not been used outside of compression. We note that the neural image compression field generally targets "high bitrate" reconstructions, and the challenge is to reduce the entropy of the complex representations, whereas in representation learning with VQ-VAE, the goal is usually the opposite: increase the entropy of a heavily constrained representation to maximally use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We start with some high-level intuition. VQ defines a learnable Voronoi partition in the highdimensional latent space of VQ-VAE, which leads to a complex non-linear partitioning of the VQ-VAE input space (e.g., images). FSQ, by contrast, relies on a simple, fixed grid partition in a much lower-dimensional space. Intuitively this is feasible because VAEs have a relatively high model capacity in typical applications (see Sec. 2), and thus the non-linearity of VQ can be "absorbed" into encoder and decoder, so that FSQ enables partitions of the VAE input space of similar complexity as VQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FINITE SCALAR QUANTIZATION</head><p>Given a d-dimensional representation z ? R d , our goal is to quantize z to a finite set of codewords.</p><p>To this end, we first apply a bounding function f , and then round to integers. We chose f such that each channel/entry in ? = round(f (z)) takes one of L unique values (e.g., f : z ? ?L/2?tanh(z)).</p><p>Thereby, we have ? ? C, where C is the implied codebook, given by the product of these per-channel codebook sets, with |C| = L d . The vectors in C can simply be enumerated leading to a bijection from any ? to an integer in {1, . . . , L d }. Therefore, VQ can be replaced with FSQ in any neural networkrelated setup where VQ is commonly used, e.g., to train transformers, after appropriately adapting the output and input dimension of the layers before and after VQ, respectively. We generalize the above exposition to the case where the i-th channel is mapped to L i values and get |C| = d i=1 L i . We visualize FSQ in Fig. <ref type="figure">1</ref> (left) and in Fig. <ref type="figure">2</ref>. Since quantization is performed by round to integers, supporting even L requires an asymmetric f . We show the general f used throughout this paper as code in App. A.1. To propagate gradients throughout the round operation, we use the STE throughout, replacing the gradients with 1. In ML frameworks, this can easily be implemented via the "stop gradient" (sg) operation as round ste : x ? x + sg(round(x) -x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HYPERPARAMETERS</head><p>FSQ has the following hyper-parameters: the number of channels d and the number of levels per channel, L = [L 1 , . . . , L d ]. In most of our experiments, to obtain fair comparisons, we will choose target codebook sizes |C| based on the VQ codebooks we aim to replace with FSQ. However, various configurations of d and L i can approximate a given |C| (i.e., any L where i L i ? |C| is a candidate). We explore various configurations in our study, and find that not all choices lead to optimal results. However, we found a simple heuristic that performs well in all considered tasks: Use L i ? 5 ?i. In Table <ref type="table" target="#tab_2">1</ref> we tabulate L for common target |C|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PARAMETER COUNT</head><p>We note that FSQ has fewer parameters than VQ, since in VQ, a codebook of size |C| ? d is learned. For example, for a typical |C|=2 12 =4096 and d=512, this results in 2M parameters, which FSQ lacks. Additionally, since for FSQ, d tends to be much smaller than for VQ (e.g., d=5 for FSQ for this |C|, see Tab. 1), the final encoder layer also has fewer parameters when training FSQ. To compensate for this, we explored adding more dense layers at the end of the VAE encoder, resp. at the start of the decoder, but found no further gains from doing so. Thus, in all models in this paper, FSQ with the same codebook size has fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">REVIEW OF MASKGIT AND UVIM</head><p>We start with a brief review of MaskGIT <ref type="bibr" target="#b10">(Chang et al., 2022)</ref> and UViM <ref type="bibr" target="#b10">(Chang et al., 2022)</ref>. In MaskGIT, the authors first train a (convolutional) VQ-GAN autoencoder <ref type="bibr" target="#b18">(Esser et al., 2020)</ref> for reconstruction (Stage I). They then freeze the autoencoder, and train a masked transformer BERTstyle <ref type="bibr" target="#b13">(Devlin et al., 2018)</ref> to predict the quantized representations (Stage II): Given a representation ?, a fraction of tokens is randomly "masked out", i.e., replaced with a special MASK token. The resulting sequence ?M is fed to a transformer in addition to a class token, and the transformer predicts a distribution for each masked token. During inference, initially only MASK tokens along with the class token are fed to the transformer. Then, some of the token locations are selected based on prediction confidence, and corresponding tokens are sampled (see <ref type="bibr" target="#b10">(Chang et al., 2022</ref>  <ref type="bibr">[8, 6, 5] [8, 5, 5, 5] [7, 5, 5, 5, 5] [8, 8, 8, 6, 5] [8, 8, 8, 5, 5, 5</ref>]  We see that Reconstruction FID correlates with codebook size for FSQ, and improves as we scale the codebook size. FSQ gets better Sampling FID and higher codebook usage for codebook size exceeding 2 10 , while the metrics start deteriorating for VQ.</p><p>These tokens are used to replace mask tokens at the input, and the model is ran again, until all input tokens have been uncovered.</p><p>UViM <ref type="bibr" target="#b23">(Kolesnikov et al., 2022</ref>) is a general architecture to tackle various (dense) prediction tasks in computer vision. In the first stage a transformer-based VQ-VAE is trained to model the label space of the target task. Optionally, both the VQ-VAE encoder and decoder can rely on the task input (RGB image for depth estimation and segmentation, grayscale image for colorization) as side information or "context", which was found beneficial for some tasks. In the second stage, an encoder-decoder transformer is trained to predict the dense label as quantized tokens produced by the VQ-VAE encoder, given the task input. For inference, a code is sampled autoregressively using the transformer conditioned on the input and then fed to the VQ-VAE decoder. The architecture is shared for the three tasks, but different weights are learned for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CHARACTERISTICS AND TRADE-OFFS FOR VQ AND FSQ REPRESENTATIONS</head><p>We For VQ, we use the auxiliary entropy loss from MaskGIT, that aims to increase the entropy of the codebook (to increase utilization). We only sweep the codebook size. For FSQ, we explore various d and L i to match these codebook sizes.</p><p>We track the following metrics: Reconstruction FID, the FID obtained by the GAN-trained autoencoder when the 50k validation images are fed through the quantized autoencoder. This is the FID that the Stage II transformer would achieve if it would perfectly model the data. We use the well established ADM TensorFlow Suite <ref type="bibr" target="#b15">(Dhariwal &amp; Nichol, 2023)</ref> predicts a distribution over discrete codes can be used to losslessly compress the corresponding representation. For masked transformers, the only requirement is a deterministic masking schedule, that gradually uncovers the input. Using such a schedule, we can compress any ? to bits, by pairing the transformer outputs with entropy coding. We use the deterministic masking schedule employed in M2T <ref type="bibr" target="#b32">(Mentzer et al., 2023)</ref> and refer to Section 1 in that work for further details on the theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MASKGIT</head><p>We </p><formula xml:id="formula_0">? = l c + ?(l c -l ? ),</formula><p>where ? is the CFG inference weight. Intuitively, this pulls the predicted distribution towards the unconditional one. We emphasize that this has previously been explored in the context of masked transformers, e.g., by <ref type="bibr">(Chang et al., 2023, Sec. 2.7)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">UVIM</head><p>We retrain the public UViM GitHub code for all three tasks (panoptic segmentation, depth estimation, colorization). As in the paper, we train each Stage II transformer 3 times, and report averaged metrics. For VQ, we use 4096 codewords (12 bits), and we use the codebook splitting (described below), as in the published results. We obtain similar metrics to what is reported in the GitHub repo, see Sec. 5. For FSQ, we use L = [7, 5, 5, 5, 5] from Tab. 1.</p><p>Following the UViM paper, we report panoptic quality (PQ) for panoptic segmentation, RMSE for depth estimation, and FID-5k for colorization. For all tasks, we use the evaluation suite provided by the UViM github repository. We refer to <ref type="bibr" target="#b23">(Kolesnikov et al., 2022)</ref> for more details on these tasks and corresponding data sets.</p><p>We ablate the effect of VAE context input (i.e., the RGB image, see above) on the performance of VQ and FSQ in the panoptic segmentation task. Further, we investigate the codebook splitting employed by UViM to avoid unused codewords in VQ-VAE. Specifically, they adopt the algorithm from <ref type="bibr" target="#b28">Linde et al. (1980)</ref>, where throughout training, unused vectors are detected. These are then replaced by splitting most frequently used embeddings into two new embeddings, adding noise to each. Since we observe training instabilities when deactivating codebook splitting in the panoptic segmentation task, we use the depth estimation task for this ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TRADEOFF STUDY</head><p>In Fig. <ref type="figure" target="#fig_0">3</ref> we show the results for the trade-off study. On the x-axis, we always show the codebook size |C|, representing the maximal amount of information the codebook can store. We observe the following:</p><p>Codebook size correlates with Reconstruction FID for FSQ In Fig. <ref type="figure" target="#fig_0">3</ref> a), we see that as we increase the codebook size, the reconstruction FID for FSQ keeps improving. This is what one would expect from a compression perspective: as we have more bits to store information, we should get better reconstruction metrics. However, we see that VQ struggles with utilizing large codebooks (despite entropy regularization of the codes), and reconstruction FID achieves a minimum at 2 11 codes, co-inciding with the point where the codebook usage starts decreasing (cf. Fig. <ref type="figure" target="#fig_0">3 c</ref>)). We note that for low codebook sizes (Fig. <ref type="figure" target="#fig_0">3</ref> a), left), VQ marginally outperforms FSQ, likely owning to the its more expressive nature (see Contribution 3 in the Section 1).</p><p>FSQ gets better Sampling FID A similar picture emerges in Fig. <ref type="figure" target="#fig_0">3 b</ref>), where we see that the better Stage I behavior of FSQ translates to better Sampling FID as we scale the codebook.</p><p>FSQ gets high codebook usage In Fig. <ref type="figure" target="#fig_0">3 c</ref>) we see that FSQ uses almost all codewords for a codebook size of 2 14 =16k, without employing any tricks. At the same time, VQ starts dropping below 50% usage for codebooks larger than 2 11 and is not able to utilize more than 2 10 codewords for larger codebooks. In contrast, for FSQ usage continues growing with more than 2 15 codewords utilized for a codebook of size 2 16 .  <ref type="bibr" target="#b24">(Kumar et al., 2021)</ref> 19.37</p><p>Table <ref type="table">2</ref>: UVIM results for the three tasks. For each, we show results in the corresponding metric averaged over three runs with std. dev. (as in UViM). We show the numbers reported by the reference GitHub repository, as well as one well established baseline per task. For our models, we show Codebook usage. For Depth Estimation, we train an ablation where we do not employ the codebook splitting in VQ. Overall, FSQ obtains competitive but marginally worse results on all tasks. ? We use the UViM GitHub evaluation suite.</p><p>Diminishing gains from codebook scaling One might wonder whether just scaling the codebook size more would lead to ever lower sampling FID. However, as shown in Fig. <ref type="figure" target="#fig_0">3 d</ref>), the compression cost of the representation keeps increasing. This indicates that the quantized representations get more complex to model for the transformer. Indeed, we see in Fig. <ref type="figure" target="#fig_0">3 b</ref>) that the Sampling FID saturates for FSQ starting when using about 2 12 codewords. We note that in general, for this task, the discrete distribution underlying the FSQ representations are slightly harder to model (as seen by the higher Compression Cost when training the same transformer on different VAEs, Fig. <ref type="figure" target="#fig_0">3 d</ref>)). We also note how the Compression Cost for VQ correlates with the codebook usage: when the usage drops, the code becomes easier to model again. Similarly, within a model group (i.e., considering only FSQ or VQ models), the compression cost is anti-correlated with sampling FID.</p><p>Selecting the number of levels per channel L In Appendix A.4.1 we also show the effect of different L on the Sampling FID. We find that L i &lt; 5 leads to subpar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MASKGIT</head><p>In Fig. <ref type="figure">4</ref> we show the metrics for MaskGIT on 256?256 ImageNet. We sweep the CFG weight for both VQ and FSQ. The following can be observed:</p><p>FSQ and VQ achieve comparable metrics and visual results Fig. <ref type="figure">4</ref> shows that both quantizers achieve very comparable FID, as well as precision and recall. To put the numbers in context, we show the well established diffusion-based ADM model <ref type="bibr" target="#b14">(Dhariwal &amp; Nichol, 2021)</ref>. When inspecting the visual results in Fig. <ref type="figure" target="#fig_2">5</ref>, we see that both quantizers lead to qualitatively similar samples. Motivated by the tradeoff study (sec. 5.1), we explored a larger codebook for these models, but did not observe further gains.</p><p>Semantics It is commonly argued in the literature that the codebook in VQ-VAEs and VQ-GANs learns semantically meaningful codes. Yet, we see that we get similar samples from both VQ and FSQ, even though FSQ does not learn an explicit codebook (and thus has less parameters). We performed a small study to see whether either representation is more semantically meaningful than the other, shown in Appendix A.3. We found no evidence that a particular code represents a fixed visual concept in either quantizer. Indeed, both behave very similary in that study.</p><p>Precision-Recall trade-offs Note that precision is a measure for the "quality" of the samples, while recall measures the proportion of the true distribution that is covered by the samples <ref type="bibr" target="#b35">(Sajjadi et al., 2018)</ref>. When we sweep the CFG weight ? during inference, we obtain models that cover a very similar space in Precision &amp; Recall (bottom, left), and that obtain very similar minimal FID (bottom, right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">UVIM</head><p>Table <ref type="table">2</ref> shows the results for the three tasks trained with UViM along with some baselines from the literature.</p><p>FSQ is competitive with VQ on all tasks We can see that across all tasks, FSQ obtains competitive metrics compared to VQ. This is also reflected in the visual results shown in Fig. <ref type="figure" target="#fig_3">6</ref> (for depth estimation) and App. A.2 (for panoptic segementation and colorization).</p><p>FSQ performs better in absence of side information (context) Table 2 also shows removing the VAE context in UViM (panoptic segmentation), i.e., removing the original RGB image input to the VAE encoder and decoder (see Sec. 4.1). In this setting, both the FSQ and VQ-based models obtain lower PQ numbers than with context, but the performance of the FSQ-based model degrades less.</p><p>FSQ does not rely on codebook splitting We explore disabling the codebook splitting on the NYU Depth task, and we observe signficantly worse RMSE, while Codebook usage drops by more than two orders of magnitude to 0.78%. In the predictions, we observe jagged edges, see Fig. <ref type="figure" target="#fig_3">6</ref> (right most column). At the same time, FSQ does not rely on any auxiliary algorithms to obtain 99% codebook usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we showed that we can replace the vector quantizer in VQ-VAEs with a simple scalar quantization scheme, where the representation is projected to very few dimensions which are bounded and rounded. We studied and compared the behavior of FSQ and VQ as a function of the codebook size and observed that FSQ achieves much better codebook utilization for large codebook sizes. Despite the much more constrained setup, we were able to obtain comparable metrics on image generation with MaskGIT, and dense computer vision tasks with UViM. We hope future work will explore FSQ in even more applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Characteristics and trade-offs for VQ and FSQ for 128 ? 128 ImageNet. We see that Reconstruction FID correlates with codebook size for FSQ, and improves as we scale the codebook size. FSQ gets better Sampling FID and higher codebook usage for codebook size exceeding 2 10 , while the metrics start deteriorating for VQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>start with a study, where we train MaskGIT models on lower resolution 128 ? 128 ImageNet images and for shorter time compared to the paper Chang et al. (2022) (100 epochs for Stage I, 200 epochs for Stage II. Please see Appendix A.4.1 for more hyperparameters). This allows us to sweep the codebook size and other hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Non-cherry-picked samples from our FSQ (top) and VQ (bottom) MaskGIT models for 4 imagenet classes (330, 320, 510, 454). We show two samples per model per category. Both models get very comparable sample quality, as reflected by the metrics in Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Samples from UViM for the depth estimation task. Other tasks in Appendix A.2. We observe that VQ and FSQ lead to comparable samples. VQ without splitting leads to jagged edges.</figDesc><graphic url="image-17.png" coords="9,108.00,152.25,76.07,57.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Figure 7: Visualization for panoptic segmentation (first two rows) and colorization (last two rows).</figDesc><graphic url="image-42.png" coords="14,409.44,399.58,93.84,93.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, Sec 3.2)).</figDesc><table><row><cell>Target Size |C|</cell><cell>2 8</cell><cell>2 10</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell></row><row><cell>Proposed L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Recommended sets of FSQ levels L to approximately match a given codebook size |C|.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">a) Reconstruction FID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">b) Sampling FID</cell><cell></cell><cell></cell></row><row><cell>10 12 14 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FSQ VQ</cell><cell>11 12 13 14 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FSQ VQ</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 4</cell><cell>2 6</cell><cell>2 8</cell><cell>2 10 Codebook Size</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell><cell>2 4</cell><cell>2 6</cell><cell>2 8</cell><cell>2 10 Codebook Size</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell></row><row><cell>2 9 2 11 2 13 2 15</cell><cell cols="4">c) Codebook Usage Maximum FSQ VQ 50%</cell><cell></cell><cell></cell><cell>8 10 12 14 16</cell><cell cols="4">d) Compression Cost [bits] Uniform FSQ VQ</cell><cell></cell><cell></cell></row><row><cell>2 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 4</cell><cell>2 6</cell><cell>2 8</cell><cell>2 10 Codebook Size</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell><cell>2 4</cell><cell>2 6</cell><cell>2 8</cell><cell>2 10 Codebook Size</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>train MaskGIT models on ImageNet 256 based on the public GitHub code, training Stage I for 1M steps with batch size 512, and Stage II for 2.5M steps with batch size 256. For inference, we use 12 steps with the cosine to sample an image. Initial experiments with the public code showed a slight instability in the Stage II transformer loss, which we were able to mitigate by lower bounding the minimal masking ratio used during training. Please see Appendix A.4.3 for further details and hyper parameters. We train VQ with codebook size 1024 (10 bits) and the entropy loss, as in the published model. For FSQ, we use L = [8, 5, 5, 5] as suggested in Tab. 1.Following the paper, we report Sampling FID as well as Precision and Recall<ref type="bibr" target="#b35">(Sajjadi et al., 2018)</ref> to assess the quality of the generative model. Additionally, we also report Codebook usage. We again use the well-established ADM TensorFlow Suite, leading to an (ADM-)-FID-train of 4.916 for the official checkpoint published in the MaskGIT GitHub, vs. 6.19 reported in the MaskGIT paper.</figDesc><table><row><cell>Early experiments showed that FSQ lands at a different Precision &amp; Recall point compared to VQ</cell></row><row><cell>(FSQ had higher recall, lower precision). Inspired by the diffusion literature, we thus add classifier</cell></row><row><cell>free guidance (CFG) (Ho &amp; Salimans, 2022) to MaskGIT: During training, we replace 10% of</cell></row></table><note><p>the class labels with the MASK token to let the model learn the unconditional distribution. During inference, we interpolate logits: Let l c be the logits obtained when conditioning on the class label c, and l ? be unconditional logits. During inference, we compute new logits l</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reproducibility We refer to Section A.1 for reference code.</p><p>Ethics Statement This work proposes a drop-in replacement for VQ, and can thus be applied in all domains where VQ is used. A domain where care w.r.t. biases has to be taken is generative models. However, no new ethical concern arises from our method that would not be a concern for VQ-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX -FINITE SCALAR QUANTIZATION: VQ-VAE MADE SIMPLE</head><p>A.1 CODE We refer to the MaskGIT GitHub and the UViM GitHub for the model code used in this paper. The FSQ method is implemented in full generality for Jax <ref type="bibr" target="#b8">(Bradbury et al., 2018)</ref>  produce similar "soup of patches". We also visualize representations sharing a single code across all spatial locations. We further stitch together representations obtained by encoding real images in Fig. <ref type="figure">8</ref>. We see that both decoders smoothly blend the the stitched representations when decoding to RGB space.</p><p>Overall, this investigation seems to imply that individual codes do not learn very abstract concepts. Instead it is the combination of codes decoder weights which determine the final RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 TRAINING DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 TRADEOFF STUDY</head><p>We use MaskGIT and train stages I and II on 128 ? 128 ImageNet. We explore a range of configurations for the quantization levels L in FSQ models and show the results in Fig. <ref type="figure">10</ref>. We find that L i ? 5 leads to the best performance. Motivated by this we recommend the following codebook sizes for L for FSQ: <ref type="bibr">5, 3] [8, 8] [8, 6, 5] [8, 8, 8] [8, 5, 5, 5] [8, 8, 6, 5] [7, 5, 5, 5] [8, 8, 8, 6, 5] [8, 8, 8, 5, 5, 5]</ref> We use 100 epochs for Stage I, split into ? 500k steps of batch size 256, and 200 epochs split into ? 1M steps for Stage II, also using batch size 256.</p><p>As mentioned in the main text, we employ a minimal masking ratio to stabilize Stage II training described in Sec A.4.2. All other hyperparameters are copied from the vqgan config.py and maskgit class cond config.py configs from the MaskGIT GitHub. We emphasize that for VQ we use the entropy loss from MaskGIT with weight 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 LOWERBOUNDING THE MASKGIT MASKING RATIO</head><p>MaskGIT uses a cosine schedule to sample masking ratios during training, where first a ratio r ? U [0, 1] is sampled, and then N M = ?cos(?/2(1 -r))S? randomly selected tokens are masked for each example in the mini batch. S is the sequence length, which is 16 2 = 256 for models trained on ImageNet 256. We found that this causes instability, likely because there are training steps, where N M = 1, i.e., only one token is masked, and we only get a loss from the corresponding prediction. Instead, we lower-bound r to r min = 1 -(arccos(0.45)2/?), which results in N M &gt; 0.45S for every training step. We later explored various alternatives to 0.45 and found that any value above 0.2 helps with stabilization, but use 0.45 throughout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 MASKGIT ON IMAGENET256</head><p>Again, we base all experiments on the vqgan config.py and maskgit class cond config.py configs from the MaskGIT GitHub repo. To speed up iteration, we change the VQGAN config to use 1M steps with batch size 512 (for Stage I), instead of 2M steps with batch size 256. We again lower bound the masking ratio as described in Sec. A.4.2. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cm3: A causal masked multimodal model of the internet</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07520</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scaling laws for generative mixed-modal language models</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.03728</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for extreme learned image compression</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01704</idno>
		<title level="m">End-to-end optimized image compression</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Masked generative image transformer</title>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Maskgit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11315" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarred</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00704</idno>
		<title level="m">Text-to-image generation via masked generative transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learned image compression with discretized gaussian mixture likelihoods and attention modules</title>
		<author>
			<persName><forename type="first">Zhengxue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaru</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Katto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7939" to="7948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
		<ptr target="https://github.com/openai/guided-diffusion/tree/main/evaluations" />
		<title level="m">ADM TensorFlow Suite</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image compression with product quantized masked image modeling</title>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.07372</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis. 2021 ieee</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12868" to="12878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
	</analytic>
	<monogr>
		<title level="m">Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="1984">1984. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="29" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Not all image regions matter: Masked vector quantization for autoregressive image generation</title>
		<author>
			<persName><forename type="first">Mengqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Straightening out the straightthrough estimator: Overcoming optimization challenges in vector quantized networks</title>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08842</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pulkit Agrawal, and Phillip Isola</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Magvlt: Masked generative visionand-language transformer</title>
		<author>
			<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daejin</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23338" to="23348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uvim: A unified modeling approach for vision with learned guiding codes</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26295" to="26308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04432</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Colorization transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust training of vector quantized bottleneck models</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>?a?cucki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricard</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jga</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Dolfing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanel</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Alum?e</surname></persName>
		</author>
		<author>
			<persName><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoregressive image generation using residual quantization</title>
		<author>
			<persName><forename type="first">Doyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wook-Shin</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11523" to="11532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Masked generative encoder to unify representation learning and image synthesis</title>
		<author>
			<persName><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlok</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Mage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2142" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Yoseph</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dvc: An end-to-end deep video compression framework</title>
		<author>
			<persName><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunlei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11006" to="11015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional probability models for deep image compression</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-fidelity generative image compression</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11913" to="11924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07313</idno>
		<title level="m">Eirikur Agustsson, and Michael Tschannen. M2t: Masking transformers twice for faster decoding</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint autoregressive and hierarchical priors for learned image compression</title>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Theory and experiments on vector quantized autoencoders</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11063</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization</title>
		<author>
			<persName><forename type="first">Yuhta</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihsiang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junki</forename><surname>Ohmura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshimitsu</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusuke</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Kumakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07547</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00395</idno>
		<title level="m">Lossy image compression with compressive autoencoders</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep generative models for distributionpreserving lossy compression</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Phenaki: Variable length video generation from open domain textual descriptions</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernan</forename><surname>Moraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taghi Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical quantized autoencoders</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4524" to="4535" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04627</idno>
		<title level="m">Vector-quantized image modeling with improved vqgan</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Soundstream: An end-to-end neural audio codec</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="495" to="507" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
