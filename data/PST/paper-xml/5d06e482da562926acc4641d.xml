<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep ReLU Networks Have Surprisingly Few Activation Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
							<email>bhanin@math.tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
							<email>drolnick@seas.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep ReLU Networks Have Surprisingly Few Activation Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">491B22C3746714F940D0E3CC06B9CADF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep networks has been attributed in part to their expressivity: per parameter, deep networks can approximate a richer class of functions than shallow networks. In ReLU networks, the number of activation patterns is one measure of expressivity; and the maximum number of patterns grows exponentially with the depth. However, recent work has showed that the practical expressivity of deep networks -the functions they can learn rather than express -is often far from the theoretical maximum. In this paper, we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound, which is independent of the depth, is tight both at initialization and during training, even on memorization tasks that should maximize the number of activation patterns. Our work suggests that realizing the full expressivity of deep networks may not be possible in practice, at least with current methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A fundamental question in the theory of deep learning is why deeper networks often work better in practice than shallow ones. One proposed explanation is that, while even shallow neural networks are universal approximators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>, there are functions for which increased depth allows exponentially more efficient representations. This phenomenon has been quantified for various complexity measures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. However, authors such as Ba and Caruana have called into question this point of view <ref type="bibr" target="#b1">[2]</ref>, observing that shallow networks can often be trained to imitate deep networks and thus that functions learned in practice by deep networks may not achieve the full expressive power of depth.</p><p>In this article, we attempt to capture the difference between the maximum complexity of deep networks and the complexity of functions that are actually learned (see Figure <ref type="figure" target="#fig_2">1</ref>). We provide theoretical and empirical analyses of the typical complexity of the function computed by a ReLU network N . Given a vector ✓ of its trainable parameters, N computes a continuous and piecewise linear function x 7 ! N (x; ✓). Each ✓ thus is associated with a partition of input space R nin into activation regions, polytopes on which N (x; ✓) computes a single linear function corresponding to a fixed activation pattern in the neurons of N .</p><p>We aim to count the number of such activation regions. This number has been the subject of previous work (see §1.1), with the majority concerning large lower bounds on the maximum over all ✓ of the number of regions for a given network architecture. In contrast, we are interested in the typical behavior of ReLU nets as they are used in practice. We therefore focus on small upper bounds for the average number of activation regions present for a typical value of ✓. Our main contributions are:</p><p>• We give precise definitions and prove several fundamental properties of both linear and activation regions, two concepts that are often conflated in the literature (see §2). • We prove in Theorem 5 an upper bound for the expected number of activation regions in a ReLU net N . Roughly, we show that if n in is the input dimension and C is a cube in input Figure <ref type="figure" target="#fig_2">1</ref>: Schematic illustration of the space of functions f : R nin ! R nout . For a given neural network architecture, there is a set F express of functions expressible by that architecture. Within this set, the functions corresponding to networks at initialization are concentrated within a set F init .</p><p>Intermediate between F init and F express is a set F learn containing the functions which the network has non-vanishing probability of learning using gradient descent. (None of these is of course a formal definition.) This paper seeks to demonstrate the gap between F express and F learn and that, at least for certain measures of complexity, there is a surprisingly small gap between F init and F learn . space R nin , then, under reasonable assumptions on network gradients and biases,</p><formula xml:id="formula_0">#activation regions of N that intersect C vol(C)  (T #neurons) nin n in ! , T &gt; 0. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>• This bound holds in particular for deep ReLU nets at initialization, and is in sharp contrast to the maximum possible number of activation patterns, which is exponential in depth <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. • Theorem 5 also strongly suggests that the bounds on number of activation regions continue to hold approximately throughout training. We empirically verify that this behavior holds, even for networks trained on memorization-based tasks (see §4 and Figures <ref type="figure" target="#fig_1">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>). It may seem counterintuitive that the number of activation patterns in a ReLU net is effectively capped far below its theoretical maximum during training, even for tasks where a higher number of regions would be advantageous (see §4). We provide in §3.2-3.3 two intuitive explanations for this phenomenon. The essence of both is that many activation patterns can be created only when a typical neuron z in N turns on/off repeatedly, forcing the value of its pre-activation z(x) to cross the level of its bias b z many times. This requires (i) significant overlap between the range of z(x) on the different activation regions of x 7 ! z(x) and (ii) the bias b z to be picked within this overlap. Intuitively, (i) and (ii) require either large or highly coordinated gradients. In the former case, z(x) oscillates over a large range of outputs and b z can be random, while in the latter z(x) may oscillate only over a small range of outputs and b z is carefully chosen. Neither is likely to happen with a proper initialization. Moreover, both appear to be difficult to learn with gradient-based optimization.</p><p>The rest of this article is structured as follows. Section 2 gives formal definitions and some important properties of both activation regions and the closely related notion of linear regions (see Definitions 1 and 2).Section 3 contains our main technical result, Theorem 5, stated in §3.1. Sections 3.2 and 3.3 provide heuristics for understanding Theorem 5 and its implications. Finally, §4 is devoted to experiments that push the limits of how many activation regions a ReLU network can learn in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Relation to Prior Work</head><p>We consider the typical number of activation regions in ReLU nets. Interesting bounds on the maximum number of regions are given in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. Our main theoretical result, Theorem 5, is related to <ref type="bibr" target="#b13">[14]</ref>, which conjectured that our Theorem 5 should hold and proved bounds for other notions of average complexity of activation regions. Theorem 5 is also related in spirit to <ref type="bibr" target="#b7">[8]</ref>, which uses a mean field analysis of wide ReLU nets to show that they are biased towards simple functions. Our empirical work (e.g. §4) is related both to the experiments of <ref type="bibr" target="#b19">[20]</ref> and to those of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. The last two observe that neural networks are capable of fitting noisy or completely random data. Theorem 5 and experiments in §4 give a counterpoint, suggesting limitations on the complexity of random functions that ReLU nets can fit in practice (see Figures <ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">How to Think about Activation Regions</head><p>Before stating our main results on counting activation regions in §3, we provide a formal definition and contrast them with linear regions in §2.1. We also note in §2.1 some simple properties of activation regions that are useful both for understanding how they are built up layer by layer in a deep ReLU net and for visualizing them. Then, in §2.2, we explain the relationship between activation regions and arrangements of bent hyperplanes (see Lemma 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Activation Regions vs. Linear Regions</head><p>Our main objects of study in this article are activation regions, which we now define. Definition 1 (Activation Patterns/Regions). Let N be a ReLU net with input dimension n</p><p>in . An activation pattern for N is an assignment to each neuron of a sign:</p><formula xml:id="formula_2">A := {a z , z a neuron in N } 2 { 1, 1} #neurons .</formula><p>Fix ✓, a vector of trainable parameters in N , and an activation pattern A. The activation region corresponding to A, ✓ is</p><formula xml:id="formula_3">R(A; ✓) := {x 2 R nin | ( 1) az (z(x; ✓) b z ) &gt; 0, z a neuron in N },</formula><p>where neuron z has pre-activation z(x; ✓), bias b z , and post-activation max{0, z(x; ✓) b z }. We say the activation regions of N at ✓ are the non-empty activation regions R(A, ✓).</p><p>Perhaps the most fundamental property of activation regions is their convexity. Lemma 1 (Convexity of Activation Regions). Let N be a ReLU net. Then for every activation pattern A and any vector ✓ of trainable parameters for N each activation region R(A; ✓) is convex.</p><p>We note that Lemma 1 has been observed before (e.g. Theorem 2 in <ref type="bibr" target="#b22">[23]</ref>), but in much of the literature the difference between linear regions (defined below), which are not necessarily convex, and activation regions, which are, is ignored. It turns out that Lemma 1 holds for any piecewise linear activation, such as leaky ReLU and hard hyperbolic tangent/sigmoid. This fact seems to be less well-known (see Appendix B.1 for a proof). To provide a useful alternative description of activation regions for a ReLU net N , a fixed vector ✓ of trainable parameters and neuron z of N , define</p><formula xml:id="formula_4">H z (✓) := {x 2 R nin | z(x; ✓) = b z }.<label>(2)</label></formula><p>The sets H z (✓) can be thought of as "bent hyperplanes" (see <ref type="bibr">Lemma 4)</ref>. The non-empty activation regions of N at ✓ are the connected components of R nin with all the bent hyperplanes H z (✓) removed: Lemma 2 (Activation Regions as Connected Components). For any ReLU net N and any vector ✓ of trainable parameters</p><formula xml:id="formula_5">activation regions (N , ✓) = connected components R nin ✏ [ neurons z H z (✓) .<label>(3)</label></formula><p>We prove Lemma 2 in Appendix B.2. We may compare activation regions with linear regions, which are the regions of input space on which the network defines different linear functions. Definition 2 (Linear Regions). Let N be a ReLU net with input dimension n in , and fix ✓, a vector of trainable parameters for N . Define</p><formula xml:id="formula_6">B N (✓) := {x 2 R nin | rN (• ; ✓) is discontinuous at x}.</formula><p>The linear regions of N at ✓ are the connected components of input space with B N removed:</p><formula xml:id="formula_7">linear regions (N , ✓) = connected components (R nin \B N (✓)) .</formula><p>Linear regions have often been conflated with activation regions, but in some cases they are different. This can, for example, happen when an entire layer of the network is zeroed out by ReLUs, leading many distinct activation regions to coalesce into a single linear region. However, the number of activation regions is always at least as large as the number of linear regions. Lemma 3 (More Activation Regions than Linear Regions). Let N be a ReLU net. For any parameter vector ✓ for N , the number of linear regions in N at ✓ is always bounded above by the number of activation regions in N at ✓. In fact, the closure of every linear region is the closure of the union of some number of activation regions. Lemma 3 is proved in Appendix B.3. We prove moreover in Appendix B.4 that generically, the gradient of rN is different in the interior of most activation regions and hence that most activation regions lie in different linear regions. In particular, this means that the number of linear regions is generically very similar to the number of activation regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Activation Regions and Hyperplane Arrangements</head><p>Activation regions in depth 1 ReLU nets are given by hyperplane arrangements in R nin (see <ref type="bibr" target="#b26">[27]</ref>). Indeed, if N is a ReLU net with one hidden layer, then the sets H z (✓) from ( <ref type="formula" target="#formula_4">2</ref>) are simply hyperplanes, giving the well-known observation that the activation regions in a depth 1 ReLU net are the connected components of R nin with the hyperplanes H z (✓) removed. The study of regions induced by hyperplane arrangements in R n is a classical subject in combinatorics <ref type="bibr" target="#b26">[27]</ref>. A basic result is that for hyperplanes in general position (e.g. chosen at random), the total number of connected components coming from an arrangement of m hyperplanes in R n is constant:</p><formula xml:id="formula_8">#connected components = n X i=0 ✓ m i ◆ ' ⇢ m n n! , m n 2 m , m n .<label>(4)</label></formula><p>Hence, for random w j , b j drawn from any reasonable distributions the number of activation regions in a ReLU net with input dimension n in and one hidden layer of size m is given by ( <ref type="formula" target="#formula_8">4</ref>). The situation is more subtle for deeper networks. By Lemma 2, activation regions are connected components for an arrangement of "bent" hyperplanes H z (✓) from (2), which are only locally described by hyperplanes. To understand their structure more carefully, fix a ReLU net N with d hidden layers and a vector ✓ of trainable parameters for N . Write N j for the network obtained by keeping only the first j layers of N and ✓ j for the corresponding parameter vector. The following lemma makes precise the observation that the hyperplane H z (✓) can only bend only when it meets a bent hyperplane H b z (✓) corresponding to some neuron b z in an earlier layer. Lemma 4 (H z (✓) as Bent Hyperplanes). Except on a set of ✓ 2 R #params of measure 0 with respect to Lebesgue measure, the sets H z (✓ 1 ) corresponding to neurons from the first hidden layer are hyperplanes in R nin . Moreover, fix 2  j  d. Then, for each neuron z in layer j, the set H z (✓ j ) coincides with a single hyperplane in the interior of each activation region of N j 1 .</p><p>Lemma 4, which follows immediately from the proof of Lemma 7 in Appendix B.1, ensures that in a small ball near any point that does not belong to S z H z (✓), the collection of bent hyperplanes H z (✓) look like an ordinary hyperplane arrangement. Globally, however, H z (✓) can define many more regions than ordinary hyperplane arrangements. This reflects the fact that deep ReLU nets may have many more activation regions than shallow networks with the same number of neurons.</p><p>Despite their different extremal behaviors, we show in Theorem 5 that the average number of activation regions in a random ReLU net enjoys depth-independent upper bounds at initialization. We show experimentally that this holds throughout training as well (see §4). On the other hand, although we do not prove this here, we believe that the effect of depth can be seen through the fluctuations (e.g. the variance), rather than the mean, of the number of activation regions. For instance, for depth 1 ReLU nets, the variance is 0 since for a generic configuration of weights/biases, the number of activation regions is constant (see ( <ref type="formula" target="#formula_8">4</ref>)). The variance is strictly positive, however, for deeper networks.</p><p>3 Main Result</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Statement</head><p>Theorem 5 gives upper bounds on the average number of activation regions per unit volume of input space for a feed-forward ReLU net with random weights/biases. Note that it applies even to highly correlated weight/bias distributions and hence holds throughout training. Also note that although we require no tied weights, there are no further constraints on the connectivity between adjacent layers. Theorem 5 (Counting Activation Regions). Let N be a feed-forward ReLU network with no tied weights, input dimension n in , output dimension 1, and random weights/biases satisfying: 1. The distribution of all weights has a density with respect to Lebesgue measure on R #weights .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Every collection of biases has a density with respect to Lebesgue measure conditional on</head><p>the values of all weights and other biases (for identically zero biases, see Appendix D).</p><p>3. There exists C grad &gt; 0 so that for every neuron z and each m 1, we have sup</p><formula xml:id="formula_9">x2R n in E [krz(x)k m ]  C m grad .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">There exists C</head><p>bias &gt; 0 so that for any neurons z 1 , . . . , z k , the conditional distribution of the biases ⇢ </p><formula xml:id="formula_10">vol(C)  ⇢ (T #neurons) nin /n in ! #neurons n in 2 #neurons #neurons  n in .</formula><p>(5) Here, the average is with respect to the distribution of weights and biases in N . Remark 1. The heuristic of §3.3 suggests the average number of activation patterns in N over all of R nin is at most (#neurons) nin /n in !, its value for depth 1 networks (see ( <ref type="formula" target="#formula_8">4</ref>)). This is confirmed in our experiments (see Figures <ref type="figure" target="#fig_1">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>).</p><p>We state and prove a generalization of Theorem 5 in Appendix C. Note that by Theorem 1 (and Proposition 2) in <ref type="bibr" target="#b11">[12]</ref>, Condition 3 is automatically satisfied by a fully connected depth d ReLU net N with independent weights and biases whose marginals are symmetric around 0 and satisfy Var[weights] = 2/fan-in with the constant C grad in 3 depending only on an upper bound for the sum P d j=1 1/n j of the reciprocals of the hidden layer widths of N . For example, if the layers of N have constant width n, then C grad depends on the depth and width only via the aspect ratio d/n of N , which is small for wide networks. Also, at initialization when all biases are independent, the constant C</p><p>bias can be taken simply to be the maximum of the density of the bias distribution. Below are two heuristics for the second <ref type="bibr" target="#b4">(5)</ref>. First, in §3.2 we derive the upper bound ( <ref type="formula">5</ref>) via an intuitive geometric argument. Then in §3.3, we explain why, at initialization, we expect the upper bounds <ref type="bibr" target="#b4">(5)</ref> to have matching, depth-independent, lower bounds (to leading order in the number of neurons). This suggests that the average total number of activation regions at initialization should be the same for any two ReLU nets with the same number of neurons (see <ref type="bibr" target="#b3">(4)</ref> and Figure <ref type="figure" target="#fig_1">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geometric Intuition</head><p>We give an intuitive explanation for the upper bounds in Theorem 5, beginning with the simplest case of a ReLU net N with n in = 1. Activation regions for N are intervals, and at an endpoint x of such an interval the pre-activation of some neuron z in N equals its bias: i.e. z(x) = b z . Thus,</p><formula xml:id="formula_11">#activation regions of N in [a, b]  1 + X neurons z #{x 2 [a, b] | z(x) = b z }.</formula><p>Geometrically, the number of solutions to z(x) = b z for inputs x 2 I is the number of times the horizontal line y = b z intersects the graph y = z(x) over x 2 I. A large number of intersections at a given bias b z may only occur if the graph of z(x) has many oscillations around that level. Hence, since b z is random, the graph of z(x) must oscillate many times over a large range on the y axis. This can happen only if the total variation R</p><formula xml:id="formula_12">x2I |z 0 (x)| of z(x) over I is large. Thus, if |z 0 (x)| is typically of moderate size, we expect only O(1) solutions to z(x) = b z per unit input length, suggesting E [#activation regions of N in [a, b]] = O ((b a) • #neurons) ,</formula><p>in accordance with Theorem 5 (cf. Theorems 1,3 in <ref type="bibr" target="#b13">[14]</ref>). When n in &gt; 1, the preceding argument, shows that density of 1-dimensional regions per unit length along any 1-dimensional line segment in input space is bounded above by the number of neurons in N . A unit-counting argument therefore suggests that the density of n in -dimensional regions per unit n in -dimensional volume is bounded above by #neurons raised to the input dimension, which is precisely the upper bound in Theorem 5 in the non-trivial regime where #neurons n in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Is Theorem 5 Sharp?</head><p>Theorem 5 shows that, on average, depth does not increase the local density of activation regions. We give here an intuitive explanation of why this should be the case in wide networks on any fixed subset of input space R nin . Consider a ReLU net N with random weights/biases, and fix a layer index ` 1. Note that the map x 7 ! x (` 1) from inputs x to the post-activations of layer ` 1 is itself a ReLU net. Note also that in wide networks, the gradients rz(x) for different neurons in the same layer are only weakly correlated (cf. e.g. <ref type="bibr" target="#b16">[17]</ref>). Hence, for the purpose of this heuristic, we will assume that the bent hyperplanes H z (✓) for neurons z in layer `are independent. Consider an activation region R for x (` 1) (x). By definition, in the interior of R, the gradient rz(x) for neurons z in layer `are constant and hence the corresponding bent hyperplane from (2) inside R is the hyperplane {x 2 R | hrz, xi = b z }. This in keeping with Lemma 4. The 2/fan-in weight normalization ensures that for each x</p><formula xml:id="formula_13">E ⇥ @ xi @ xj z(x) ⇤ = 2 • i,j</formula><p>) Cov[rz(x)] = 2 Id.</p><p>See, for example, equation <ref type="bibr" target="#b16">(17)</ref> in <ref type="bibr" target="#b10">[11]</ref>. Thus, the covariance matrix of the normal vectors rz of the hyperplanes H z (✓) \ R for neurons in layer `are independent of `! This suggests that, per neuron, the average contribution to the number of activation regions is the same in every layer. In particular, deep and shallow ReLU nets with the same number of neurons should have the same average number of activation regions (see (4), Remark 1, and Figures <ref type="figure" target="#fig_1">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Maximizing the Number of Activation Regions</head><p>While we have seen in Figure <ref type="figure" target="#fig_1">3</ref> that the number of regions does not strongly increase during training on a simple task, such experiments leave open the possibility that the number of regions would go up markedly if the task were more complicated. Will the number of regions grow to achieve the theoretical upper bound (exponential in the depth) if the task is designed so that having more regions is advantageous? We now investigate this possibility. See Appendix A for experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Memorization</head><p>Memorization tasks on large datasets require learning highly oscillatory functions with large numbers of activation regions. Inspired by the work of Arpit et. al. in <ref type="bibr" target="#b0">[1]</ref>, we train on several tasks interpolating between memorization and generalization (see Figure <ref type="figure" target="#fig_3">4</ref>) in a certain fraction of MNIST labels have been randomized. We find that the maximum number of activation regions learned does increase with the amount of noise to be memorized, but only slightly. In no case does the number of activation regions change by more than a small constant factor from its initial value. Next, we train a network to memorize binary labels for random 2D points (see Figure <ref type="figure" target="#fig_4">5</ref>). Again, the number of activation regions after training increases slightly with increasing memorization, until the task becomes too hard for the network and training fails altogether. Varying the learning rate yields similar results (see Figure <ref type="figure" target="#fig_5">6(a)</ref>), suggesting the small increase in activation regions is probably not a result of hyperparameter choice.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Effect of Initialization</head><p>We explore here whether varying the scale of biases and weights at initialization affects the number of activation regions in a ReLU net. Note that scaling the biases changes the maximum density of the bias, and thus affects the upper bound on the density of activation regions given in Theorem 5 by increasing T bias . Larger, more diffuse biases reduce the upper bound, while smaller, more tightly concentrated biases increase it. However, Theorem 5 counts only the local rather than global number of regions. The latter are independent of scaling the biases: Lemma 6. Let N be a deep ReLU network, and for c &gt; 0 let N bias c be the network obtained by multiplying all biases in N by c. Then, N (x) = N bias c (cx)/c. Rescaling all biases by the same constant therefore does not change the total number of activation regions. In the extreme case of biases initialized to zero, Theorem 5 does not apply. However, as we explain in Appendix D, zero biases only create fewer activation regions (see Figure <ref type="figure" target="#fig_6">7</ref>). We now consider changing the scale of weights at initialization. In <ref type="bibr" target="#b22">[23]</ref>, it was suggested that initializing the weights of a network with greater variance should increase the number of activation regions. Likewise, the upper bound in Theorem 5 on the density of activation regions increases as gradient norms increase, and it has been shown that increased weight variance increases gradient norms <ref type="bibr" target="#b11">[12]</ref>. However, this is again a property of the local, rather than global, number of regions. Indeed, for a network N of depth d, write N weight c for the network obtained from N by multiplying all its weights by c, and let N bias 1/c⇤ be obtained from N by dividing the biases in the kth layer by c k .  A scaling argument shows that N weight c (x) = c d N bias 1/c⇤ (x). We therefore conclude that the activation regions of N weight c and N bias 1/c⇤ are the same. Thus, scaling the weights uniformly is equivalent to scaling the biases differently for every layer. We have seen from Lemma 6 that scaling the biases uniformly by any amount does not affect the global number of activation regions. Therefore, it makes sense (though we do not prove it) that scaling the weights uniformly should approximately preserve the global number of activation regions. We test this intuition empirically by attempting to memorize points randomly drawn from a 2D input space with arbitrary binary labels for various initializations (see Figure <ref type="figure" target="#fig_5">6</ref>). We find that neither at initialization nor during training is the number of activation regions strongly dependent on the weight scaling used for initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented theoretical and empirical evidence that the number of activation regions learned in practice by a ReLU network is far from the maximum possible and depends mainly on the number of neurons in the network, rather than its depth. This surprising result implies that, at least when network gradients and biases are well-behaved (see conditions 3,4 in the statement of Theorem 5), the partition of input space learned by a deep ReLU network is not significantly more complex than that of a shallow network with the same number of neurons. We found that this is true even after training on memorization-based tasks, in which we expect a large number of regions to be advantageous for fitting many randomly labeled inputs. Our results are stated for ReLU nets with no tied weights and biases (and arbitrary connectivity). We believe that analogous results and proofs hold for residual and convolutional networks but have not verified the technical details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Function defined by a ReLU network of depth 5 and width 8 at initialization. Left: Partition of the input space into regions, on each of which the activation pattern of neurons is constant. Right: the function computed by the network, which is linear on each activation region.</figDesc><graphic coords="2,159.03,339.45,117.72,114.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The average number of activation regions in a 2D cross-section of input space, for fully connected networks of various architectures training on MNIST. Left: a closeup of 0.5 epochs of training. Right: 20 epochs of training. The notation [20, 20, 20] indicates a network with three layers, each of width 20. The number of activation regions starts at approximately (#neurons) 2 /2, as predicted by Theorem 5 (see Remark 1). This value changes little during training, first decreasing slightly and then rebounding, but never increasing exponentially. Each curve is averaged over 10 independent training runs, and for each run the number of regions is averaged over 5 different 2D cross-sections, where for each cross-section we count the number of regions in the (infinite) plane passing through the origin and two random training examples. Standard deviations between different runs are shown for each curve. See Appendix A for more details.</figDesc><graphic coords="3,108.80,157.65,197.12,108.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>bz 1 ,</head><label>1</label><figDesc>...,bz k of these neurons given all the other weights and biases in N satisfies sup b1,...,b k 2R ⇢ bz 1 ,...,bz k (b 1 , . . . , b k )  C k bias . Then, there exists 0 , T &gt; 0 depending on C grad , C bias with the following property. Suppose that &gt; 0 . Then, for all cubes C with side length , we have E [#non-empty activation regions of N in C]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Depth 3, width 32 network trained on MNIST with varying levels of label corruption. Activation regions are counted along lines through input space (lines are selected to pass through both the origin and randomly selected MNIST examples), with counts averaged across 100 such lines. Theorem 5 and [14] predict the expected number of regions should be approximately the number of neurons (in this case, 96). Left: average number of regions plotted against epoch. Curves are averaged over 40 independent training runs, with standard deviations shown. Right: average number of regions plotted against average training accuracy. Throughout training the number of regions is well-predicted by our result. There are slightly, but not exponentially, more regions when memorizing more datapoints. See Appendix A for more details.</figDesc><graphic coords="7,307.54,294.27,192.46,108.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Depth 3, width 32 fully connected ReLU net trained for 2000 epochs to memorize random 2D points with binary labels. The number of regions predicted by Theorem 5 for such a network is 96 2 /2! = 4608. Left: number of regions plotted against epoch. Curves are averaged over 40 independent training runs, with standard deviations shown. Right: #regions plotted against training accuracy. The number of regions increased during training, and increased more for greater amounts of memorization. The exception was for the maximum amount of memorization, where the network essentially failed to learn, perhaps because of insufficient capacity. See Appendix A for more details.</figDesc><graphic coords="7,112.00,293.30,193.04,109.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Depth 3, width 32 network trained to memorize 5000 random 2D points with independent binary labels, for various learning rates and weight scales at initialization. All networks start with ⇡ 4608 regions, as predicted by Theorem 5. Left: None of the learning rates gives a number of regions larger than a small constant times the initial value. Learning rate 10 3 , which gives the maximum number of regions, is the learning rate in all other experiments, while 10 2 is too large and causes learning to fail. Center: Different weight scales at initialization do not strongly affect the number of regions. All weight scales are given relative to variance 2/fan-in. Right: For a given accuracy, the number of regions learned grows with the weight scale at initialization. However, poor initialization impedes high accuracy. See Appendix A for details.</figDesc><graphic coords="8,110.92,67.02,128.91,97.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Activation regions within input space, for a network of depth 3 and width 64 training on MNIST. (a) Cross-section through the origin, shown at initialization, after one epoch, and after twenty epochs. The plane is chosen to pass through two sample points from MNIST, shown as black dots. (b) Cross-section not through the origin, shown at initialization. The plane is chosen to pass through three sample points from MNIST. For discussion of activation regions at zero bias, see Appendix D.</figDesc><graphic coords="8,108.00,279.15,398.35,120.21" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximation and estimation bounds for artificial neural networks</title>
		<author>
			<persName><surname>Andrew R Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the complexity of neural network classifiers: A comparison between shallow and deep architectures</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1553" to="1565" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Provable robustness of ReLU networks via maximization of linear regions</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Palma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Toussi Kiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Lloyd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10156</idno>
		<title level="m">Deep neural networks are biased towards simple functions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Universal function approximation by deep neural nets with bounded width and ReLU activations</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02691</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Which neural net architectures give rise to exploding and vanishing gradients?</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Products of many large random matrices and gradients in deep neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Nica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05994</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complexity of linear regions in deep networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks as Gaussian processes</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Why does deep and cheap learning work so well</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Henry W Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1223" to="1247" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Guido F Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Daniel A Abolafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approximation theory of the MLP model in neural networks</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta numerica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The power of deeper networks for expressing natural functions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Empirical bounds on linear regions of deep rectifier networks</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03370</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bounding and counting linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tjandraatmadja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An introduction to hyperplane arrangements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geometric combinatorics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="389" to="496" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation benefits of deep feedforward networks</title>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08101</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
