<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Wavelet Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Sardy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Tseng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Bruce</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paulo</forename><forename type="middle">S R S</forename><surname>Diniz</surname></persName>
						</author>
						<author>
							<persName><surname>Sardy</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">MathSoft, Inc</orgName>
								<address>
									<postCode>98109</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Franche-Comté</orgName>
								<address>
									<postCode>1989</postCode>
									<settlement>Besancon</settlement>
									<region>in</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="laboratory">Laboratory for Information and Decision Systems at MIT. He joined</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>1990</postCode>
									<settlement>Seattle</settlement>
									<country>in</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Wavelet Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8384BB65B85A37D768D020CC5DBBF75D</idno>
					<note type="submission">received February 1, 2000; revised February 22, 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Basis pursuit</term>
					<term>block coordinate relaxation</term>
					<term>interior point</term>
					<term>robustness</term>
					<term>wavelet</term>
					<term>waveshrink</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For extracting a signal from noisy data, waveshrink and basis pursuit are powerful tools both from an empirical and asymptotic point of view. They are especially efficient at estimating spatially inhomogeneous signals when the noise is Gaussian. Their performance is altered when the noise has a long tail distribution, for instance, when outliers are present.</p><p>We propose a robust wavelet-based estimator using a robust loss function. This entails solving a nontrivial optimization problem and appropriately choosing the smoothing and robustness parameters. We illustrate the advantage of the robust wavelet denoising procedure on simulated and real data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S UPPOSE we observe a signal generated from <ref type="bibr" target="#b0">(1)</ref> where the equally spaced sampling locations are points on the line for one dimensional (1-D) signals or on a grid for images. For now, we assume that the s are identically and independently distributed Gaussian random variables with mean zero and variance one. Our goal is to denoise the signal , i.e., to find a good estimate of the underlying signal . The hat on top of a letter is the notation used throughout this paper to indicate the estimate of the corresponding parameter. Waveshrink <ref type="bibr" target="#b0">[1]</ref> and basis pursuit <ref type="bibr" target="#b1">[2]</ref> are two nonparametric expansion based estimators. They assume that can be well represented by a linear combination of wavelet basis functions , namely <ref type="bibr" target="#b1">(2)</ref> where are the wavelet coefficients. Waveshrink is defined for orthonormal wavelets only (i.e.,</p><p>), whereas basis pursuit can also use an "overcomplete" basis (i.e.,</p><p>). The advantage of an overcomplete wavelet dictionary is discussed by Chen et al. <ref type="bibr" target="#b1">[2]</ref>. The goal of Waveshrink and basis pursuit is to estimate the wavelet coeffi-cients for to have a good mean squared error</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE E</head><p>where the expectation is taken over . Waveshrink uses orthonormal wavelets, which has two important consequences: First, the least squares estimate is simply , where is the matrix of discretized , and denotes the transpose of ; second, is an unbiased estimate of , and its covariance matrix is so that the estimated least squares coefficients are independent if the noise is Gaussian. For a smaller mean squared error at the cost of introducing some bias, Donoho and Johnstone <ref type="bibr" target="#b0">[1]</ref> apply the hard or the soft function sign <ref type="bibr" target="#b2">(3)</ref> where is the identity function on , and where is for and zero otherwise. For Gaussian noise, the shrinkage can be applied to component-wise because its components are independent.</p><p>The hard and soft estimates are, interestingly, the closed-form solution to two optimization problems that are, in general, difficult to solve unless is orthonormal.</p><p>• Best Subset: is "the best subset of size " with as the th smallest element of in the sense that it minimizes the residual sum of squares among all sets with nonzero wavelet coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• -Penalized Least Squares:</head><p>is the closed-form solution to the following optimization problem: <ref type="bibr" target="#b3">(4)</ref> This property leads to the relaxation algorithm of Section II-A and to the definition of basis pursuit. When is no longer orthonormal but overcomplete, the least squares estimate no longer has independent components, and the shrinkage idea cannot be applied as such. Basis pursuit generalizes to that situation using the optimization problem formulation (4), whose solution is not trivial when is not orthonormal.</p><p>The selection of the smoothing parameter is important. Several ways of selecting have been proposed for Gaussian noise. They are based on a minimax argument (see Donoho and Johnstone <ref type="bibr" target="#b0">[1]</ref> for real-valued noise and Sardy <ref type="bibr" target="#b2">[3]</ref> for complex-valued noise) or on minimizing the Stein unbiased risk estimate (SURE) (see Donoho and Johnstone <ref type="bibr" target="#b3">[4]</ref>). Nason <ref type="bibr" target="#b4">[5]</ref> selects the smoothing parameter by cross validation.</p><p>The predictive performance of waveshrink and basis pursuit deteriorates when the noise is not Gaussian. It is because of the loss function in (4). It arises naturally as the log-likelihood when the noise is Gaussian, but it is no longer appropriate when the departure from Gaussianity is too strong. In such a case, the quadratic loss function pulls the estimated function toward the outliers. We illustrate this phenomenon on a 1-D signal in Fig. <ref type="figure" target="#fig_0">1</ref>; the true and noisy signal (90% standard Gaussian noise and 10% Gaussian noise with a standard deviation of 4) are plotted on the left side; on the right side, basis pursuit gives poor estimation near the outliers. The aim of this paper is to develop a robust waveletbased estimator that is less affected by a long-tailed noise. Some work has already been done in this direction. Bruce et al. <ref type="bibr" target="#b5">[6]</ref> preprocess the estimation of the wavelet coefficients by a "fast and robust smooth/cleaner" at each multiresolution level to downweight the effect of the outliers in the estimation of the wavelet coefficients. Kovac and Silverman <ref type="bibr" target="#b6">[7]</ref> preprocess the original signal to remove "bad" observations by means of a rough statistical test involving a running median smoother with a window of, say, 5; their procedure has the drawback of losing information by throwing out "bad" observations. For the situation of a known symmetric long tail noise, Averkamp and Houdré <ref type="bibr" target="#b7">[8]</ref> derive minimax rules to select the smoothing parameter. Krim and Schick <ref type="bibr" target="#b8">[9]</ref> derive a robust estimator of the wavelet coefficients based on minimax description length; their assumption of independent noise in the wavelet domain is not realistic, however.</p><p>In this paper, we propose a different approach that has the advantages of having a simple definition, of assuming a realistic independent contamination in the measurements , and of being able to deal with overcomplete wavelets as well. Its challenges are in finding an efficient algorithm (see Section II) and in choosing appropriately two tuning parameters (see Section III). Since the nonrobust behavior is due to the loss function, we simply replace it by a robust loss function and define the coefficient estimate of the robust wavelet denoising procedure as the solution to <ref type="bibr" target="#b4">(5)</ref> where</p><p>. We use the Huber loss function <ref type="bibr" target="#b9">[10]</ref>, which is a hybrid between for small residuals and for large residuals, namely <ref type="bibr" target="#b5">(6)</ref> where is some cutpoint. Using an even loss function, we implicitly assume that the noise is symmetric around zero. Note that when and , it becomes the and loss functions, respectively, both of which have the advantage of being convex.</p><p>Our proposal of a robust wavelet denoising procedure raises two issues. On the one hand, we must solve the nontrivial optimization problem defined in ( <ref type="formula">5</ref>) for a given pair ; on the other hand, we must select the smoothing parameter and the cutpoint . In Section II, we propose two algorithms to solve the robust wavelet denoising optimization problem (5): a block coordinate relaxation algorithm and an interior point algorithm. In Section III, we discuss the problem of selecting the cutpoint and the smoothing parameter . In Section IV, we give the result of a simulation to compare the efficiency of the two algorithms and to compare the denoising performance of the robust versus the nonrobust estimators. In Section V, we illustrate robust basis pursuit on two real data sets. We conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TWO OPTIMIZATION ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Block Coordinate Relaxation (BCR) Algorithm</head><p>The BCR algorithm relies on the following observation. The Huber loss function ( <ref type="formula">6</ref>) may be rewritten as <ref type="bibr" target="#b6">(7)</ref> This nontrivial fact can be inferred from results on infimal convolution as discussed in Rockafellar <ref type="bibr">[11, ch. 16</ref>]. Interestingly, ( <ref type="formula">5</ref>) becomes <ref type="bibr" target="#b7">(8)</ref> The reformulated problem (8) has a separable structure to which the BCR algorithm of Sardy et al. <ref type="bibr" target="#b11">[12]</ref> can be applied. It solves exactly a succession of subproblems using the soft shrinkage function <ref type="bibr" target="#b2">(3)</ref>. The BCR algorithm assumes that the matrix (here ) is the union of a finite number of orthonormal matrices . This assumption is verified for many wavelet dictionaries including nondecimated wavelets, wavelet packets, local cosine packets, chirplets <ref type="bibr" target="#b12">[13]</ref>, and brushlets <ref type="bibr" target="#b13">[14]</ref>. Sardy et al. <ref type="bibr" target="#b11">[12]</ref> propose two strategies for choosing in step 2 of the BCR algorithm and prove convergence for real and complex-valued signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interior Point (IP) Algorithm</head><p>The interior point algorithm has the advantage of not requiring to be the union of orthonormal blocks. It does not apply to complex-valued signals, however, and is computationally less efficient than the BCR algorithm (see Section IV-A).</p><p>1) Transformation to Quadratic Programming: First, we rewrite the optimization problem (5) as subject to <ref type="bibr" target="#b8">(9)</ref> By attaching Lagrange multipliers to the linear constraints, this, in turn, can be written as</p><p>The dual to this problem, which is obtained by exchanging the order of "min" and "max,"</p><p>where is the th column of . Since the objective function in ( <ref type="formula">9</ref>) is convex, defined everywhere, and the constraint is linear, it is known from convex duality theory (see, e.g., Rockaffelar [11, th. 28.2 and 28.4]) that the duality gap between the primal (9) and the dual (10) problems is zero. Using (6) and some algebra, the dual problem ( <ref type="formula" target="#formula_0">10</ref>) is with <ref type="bibr" target="#b10">(11)</ref> This is a quadratic programming problem. Notice that in the case of , the dual problem ( <ref type="formula">11</ref>) is a linear programming problem, implying that the primal problem can be transformed into a linear programming problem. For brevity, we omit the derivation (see Sardy <ref type="bibr" target="#b14">[15]</ref>).</p><p>2) Interior Point Algorithm: We solve the quadratic programming problem using a primal-dual log-barrier interior point algorithm inspired by Chen et al. <ref type="bibr" target="#b1">[2]</ref>. The log-barrier subproblem corresponding to <ref type="bibr" target="#b10">(11)</ref> is where is the log-barrier penalty that is chosen identically for all the penalty terms. Letting be the th canonical basis vector, the first-order optimality condition for the log-barrier subproblem is Letting and letting and the first-order optimality condition is a set of nonlinear equations <ref type="bibr" target="#b11">(12)</ref> where diag and diag with and . The variables , , and are called, respectively, the primal, the dual, and the dual slack variables. This IP problem could alternatively have been derived from (8) and its dual; for instance, corresponds to , and corresponds to in <ref type="bibr" target="#b6">(7)</ref>. In an interior point approach, one typically takes a single Newton step to solve the nonlinear system (12) inexactly and then decreases . More precisely, given , , , and , one computes the Newton direction , which is obtained by solving the following system of linear equations: <ref type="bibr" target="#b12">(13)</ref> and then, one updates the variables according to and empirically, the choice of has worked well. The parameter may be updated in many ways. For example, Chen et al. <ref type="bibr" target="#b1">[2]</ref> suggested . Typically, only a small number of interior-point iterations is required to obtain a solution of desired accuracy.</p><p>3) Conjugate Gradient Solver for the Newton Step: Most of the computational effort is spent in computing the Newton direction at each iteration. From (13), we have that is the solution of</p><formula xml:id="formula_1">(19)</formula><p>where is a diagonal matrix. The dual slack and primal Newton directions are then obtained by and . We adopt the algorithm of Chen et al. <ref type="bibr" target="#b1">[2]</ref> and use the conjugate gradient method to solve the dense system (19). Because multiplication by and are typically fast (on the order of or operations), the conjugate gradient method is attractive. In practice, however, the number of conjugate gradient iterations required to solve (19) accurately can become very large as approaches a solution, thus degrading the performance of the IP algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Finding an Initial Point:</head><p>The IP algorithm requires an initial point and , which ideally would not be too far from the solution. Let the ridge regression estimate (obtained by replacing in (4) by ) be an initial guess for the coefficients. Let and . With and , let and . Then, the primal variables are positive. In addition, let sign , and let . Then, the dual variables satisfy , , and the dual slack variables are positive. 5) Convergence: Although there have been many convergence studies of IP algorithms, the algorithms that work well in practice, including the one described above, often have no guarantee of convergence. Specifically, convergence requires the existence of positive constants , , such that and , for , at all iterations. We can enforce convergence by updating in a more conservative manner, but this would slow down its convergence in practice. (See, e.g., Kojima et al. <ref type="bibr" target="#b15">[16]</ref> for discussions of these issues in linear programming problems.)</p><p>A stopping rule for the IP algorithm is when all of the following conditions are satisfied for a small : The selection of the smoothing parameter and the cutpoint is a difficult problem. Two different situations can be distinguished.</p><p>In one situation, the pair can be tuned "by eye." For instance, in the first application of Section V, the noise is non-Gaussian, the signal to recover is known to be an aircraft. and the signal-to-noise ratio (SNR) is inherent to the infrared sensor used. In that situation, the smoothing parameter and the cutpoint can be tuned on a training set of images and then used on future images.</p><p>In the other situation, the underlying signal is not known, and neither is the SNR; therefore, an automatic selection of the pair is needed. Our radar application of Section V is an example of this situation. Several procedures have been developed to select the smoothing parameter for non-Gaussian noise. Nason <ref type="bibr" target="#b4">[5]</ref> observes, on a simulation using i.i.d., Student noise, that the -based cross validation gives a better prediction than the minimax or SURE rules derived for Gaussian noise. With smoothing splines, Xiang and Wahba <ref type="bibr" target="#b16">[17]</ref> develop a generalized cross validation criterion for a differentiable smoothness penalty and for a known noise distribution in the exponential family. The knowledge of a specific noise distribution is also required by Averkamp and Houdré <ref type="bibr" target="#b7">[8]</ref>, who develop a minimax rule for specific long tail noise distributions; their selection can only do so much to cope with the problem of using the unnatural loss function. Crouse et al. <ref type="bibr" target="#b17">[18]</ref> propose wavelet-Markov models for the dependence between scales, and they employ an EM algorithm to estimate their parameters by maximum likelihood. EM is typically slow and gets trapped into local maxima, however [this cannot happen with our convex cost function ( <ref type="formula">5</ref>)].</p><p>In this paper, we propose a pragmatic approach that does not require the specific knowledge of the noise distribution. First, an estimate of scale is required. We use the median absolute deviation of the high-frequency wavelet coefficients of Donoho and Johnstone <ref type="bibr" target="#b3">[4]</ref>. This estimate of scale is robust to features of the underlying signal and to outliers especially if, as suggested by Kovac and Silverman <ref type="bibr" target="#b6">[7]</ref>, the Haar wavelet (which support has length two) is used. For the cutpoint in (6), we follow Huber <ref type="bibr" target="#b9">[10]</ref> and choose . The default in software packages is often based on the following statistical consideration: Suppose you observe i.i.d. data with mean . Then, the asymptotic relative efficiency of the Huber estimate of is asymptotically 95% efficient with respect to the sample average when the noise is Gaussian (G). We can read this value on the continuous curve (G) of Fig. <ref type="figure" target="#fig_4">2</ref>. The relative efficiency is also plotted as a function of for the distributions used in the simulation. Fig. <ref type="figure" target="#fig_4">2</ref> gives a guideline for the selection of in our nonparametric regression problem: We see that a value of between one and three gives overall a good efficiency. Based on the simulation of Section IV, we recommend using a cutpoint of at least . Finally, for the smoothing parameter , we use the minimax developed by Donoho and Johnstone [1] since the residuals within do not depart dramatically from Gaussian residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computational Efficiency</head><p>Empirically, Sardy et al. <ref type="bibr" target="#b11">[12]</ref> found the BCR algorithm to be more efficient than the IP algorithm at solving basis pursuit.  We observe the same advantage for solving its robust version <ref type="bibr" target="#b4">(5)</ref>. The reason is that the matrix now used only has to be augmented by the columns of the identity matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Predictive Performance</head><p>To illustrate graphically on a 1-D signal the advantage of using a robust procedure, Fig. <ref type="figure" target="#fig_7">4</ref> shows the robust estimation of heavisine for the same contaminated data as in Fig. <ref type="figure" target="#fig_0">1</ref>. The robust procedure gives a better reconstruction; in particular, it pre-  We choose two sample sizes of and , and the "s8" wavelet packet dictionary with all but four levels. The minimax smoothing parameters are 2.232 and 2.594. Following the discussion of Section III, the smoothing parameter is set to and the cutpoint of the Huber loss function to with and . For each combination of noise (G, C, T) of underlying function (blocks, bumps, heavisine, Doppler), of sample size ( , ), and of procedure (nonrobust, robust), we estimate the MSE by averaging (40)(1024)/ model errors (i.e., 40 for and 10 for to get the same number of points is generated for the two sample sizes). Table I reports the estimated MSEs of the competing estimators for the 24 scenarios.</p><p>In light of Table <ref type="table">I</ref>, a cutpoint of at least is advisable for robust basis pursuit; the standard value of derived in the parametric context from asymptotic considerations is not large enough. With a cutpoint of , the gain in efficiency can be dramatic for non-Gaussian noise using robust basis pursuit. Its counterperformance on the bumps signal is due to the nature of the signal whose features are difficult to distinguish with noise in the upper tail when the sampling is light ( ); with an heavier sampling ( ), robust basis pursuit again beats the nonrobust estimator for non-Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATIONS</head><p>The first data is an image taken by a long-wavelength infrared sensor. Just visible above the center of the image is an A-37 trainer aircraft flying above the Sierra Nevada at some distance from the sensor platform. The 128 by 128 original image plotted in the top left of Fig. <ref type="figure" target="#fig_9">6</ref> clearly shows some "outlier pixels." A standard median filter (with a window) gets rid of the bad pixels but does not preserve the aircraft well (top right). The two bottom plots of Fig. <ref type="figure" target="#fig_9">6</ref> show the denoised image using (left) basis pursuit and (right) robust basis pursuit. The robust denoising procedure has the definite advantage of cleaning the image of the bad pixels while preserving the outline of the airplane. To clean the image with the two wavelet-based techniques, we used the 2-D nondecimated wavelet transform with the "s8" wavelet. In this application, we know the underlying image we want to recover; therefore, we tried several values of and (which was feasible in a finite time thanks to the BCR algorithm) and chose for the best visually appealing reconstruction of the aircraft. Using this parameter setting , the robust procedure can be used to clean future images.</p><p>The second data are radar glint observations and consist of angles of a target in degrees. The signal contains a number of glint spikes, causing the apparent signal to behave erratically. From physical considerations, a good model for the true signal is a low-frequency oscillation about . The estimated standard deviation is . To get a nice "noisefree" visual display, we choose the universal threshold , and for the robust version, we choose . Fig. <ref type="figure" target="#fig_10">7</ref> shows the (top) original signal and (left: nonrobust; right: robust) the denoised estimates at the bottom. The robust estimate is a low-frequency oscillation, as expected, whereas the nonrobust estimate remains jagged. Note that the median filter's estimate still shows a highly oscillating denoised signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have proposed a robust version of basis pursuit by replacing the nonrobust loss function by the so-called Huber loss function. We solved the corresponding nontrivial optimization problem for a given smoothing parameter and a given cutpoint with an efficient and converging block coordinate relaxation method that is empirically faster than an interior point competitor. The two techniques are available in the wavelet module of the Splus statistical software; the BCR algorithm can otherwise be easily implemented. We proposed a rule to choose the smoothing parameter and the cutpoint loss function . We showed on a particular simulation that robust basis pursuit has a good predictive performance with both Gaussian and long-tailed symmetric additive noise; in particular, we recommend using a cutpoint of at least for the Huber loss function. As illustrated with two applications, robust basis pursuit has a definite advantage over both a nonrobust wavelet-based estimator and a median filter estimator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Nonrobust estimation of heavisine. (Top) True signal. (Bottom left) Noisy signal. (Bottom right) Nonrobust estimate.</figDesc><graphic coords="2,61.56,62.28,204.24,218.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and as the corresponding coefficients in ; 3) Define the residual vector . Find the improved by solving the subproblem arg using soft shrinkage; 4) If convergence criterion is not met, go to step 1;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Asymptotic relative efficiency as a function of c of the Huber estimate with respect to the sample average of an i.i.d. sample generated from the distributions used in the simulation: Gaussian (G), Gaussian mixture (C), and Student t with three degrees of freedom (T).</figDesc><graphic coords="5,85.14,62.28,160.08,172.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Decrease in the robust basis pursuit objective function as a function of the CPU time (in seconds) for the BCR algorithm and the IP algorithm.</figDesc><graphic coords="5,55.26,306.36,219.84,248.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 3 illustrates, on four contaminated signals, the superiority of the BCR algorithm that is up to 10 times faster than the IP algorithm in achieving the desired precision [ in (20)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Robust estimation of heavisine. (Top) True signal. (Bottom left) Noisy signal. (Bottom right) Robust estimate to compare with Fig. 1.</figDesc><graphic coords="5,326.58,62.28,203.28,217.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Four signals used in the Monte Carlo experiment.</figDesc><graphic coords="5,317.40,332.28,221.52,247.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Top: (Left) Noisy infrared sensor data and (Right) 3 23 robust median filter denoised image. Bottom: (Left) Nonrobust and (Right) robust wavelet based denoised images.</figDesc><graphic coords="6,324.24,221.28,204.96,259.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Top: (Left) Noisy radar glint data and (Right) robust median filter estimate. Bottom: (Left) Nonrobust and (right) robust wavelet-based estimate (with local cosine packet).</figDesc><graphic coords="7,67.86,62.28,194.64,273.60" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank the associate editor and five anonymous referees for their careful review and A. Davison for providing help with Fig. <ref type="figure">2</ref>.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by an SBIR Phase I contract with the Naval Air Warfare Center at China Lake, CA. The associate editor coordinating the review of this paper and approving it for publication was Prof. He then received the M.S. degree in mathematics in 1991 and the M.S. degree in statistics in 1992, both from Utah State University, Logan. After doing his military service for France at the Nuclear Engineering Department of the Massachusetts Institute of Technology, Cambridge, he received the Ph.D. degree in statistics from the University of Washington, Seattle. He is now a Postdoctoral Assistant with the Swiss Federal Institute of Technology (EPFL), Lausanne, in the Mathematics Department (DMA). Paul Tseng received the B.Sc. degree in engineering mathematics from Queen's University, Kingston, ON, Canada, in 1981 and the Ph.D. degree in operations research from the Massachusetts Institute of Technology (MIT), Cambridge, in 1986.</p><p>From 1986 to 1987, he was a University Research Fellow with the Department of Management Science,</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ideal spatial adaptation via wavelet shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Minimax threshold for denoising complex signals with waveshrink</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1023" to="1028" />
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapting to unknown smoothness via wavelet shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1200" to="1224" />
			<date type="published" when="1995-12">Dec. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wavelet function estimation using cross-validation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nason</surname></persName>
		</author>
		<editor>Wavelets and Statistics, A. Antoniadis and G. Oppenheim</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="261" to="280" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising and robust nonlinear wavelet analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wavelet Applicat</title>
		<imprint>
			<biblScope unit="volume">2242</biblScope>
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending the scope of wavelet regression methods by coefficient-dependent thresholding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kovac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="172" to="183" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wavelet thresholding for non (necessarily) gaussian noise: A preliminary report</title>
		<author>
			<persName><forename type="first">R</forename><surname>Averkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Houdré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Georgia Inst. Technol., Atlanta, Tech. Rep</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimax description length for signal denoising and optimized representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Krim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Schick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="898" to="908" />
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton Univ. Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Block coordinate relaxation methods for nonparametric wavelet denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Statist</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="379" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive &quot;chirplet&quot; transform: And adaptive generalization of the wavelet transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1243" to="1256" />
			<date type="published" when="1992-06">June 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biorthogonal brushlet bases for directional image compression</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Anal. Optimization Syst</title>
		<meeting>12th Int. Conf. Anal. Optimization Syst</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Regularization Techniques for Linear Regression with a Large Set of Carriers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Seattle</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A primal-dual exterior point algorithm for linear programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mizuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Progr</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="261" to="280" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generalized approximate cross validation for smoothing splines with nongaussian data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="675" to="692" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Signal estimation using wavelet-markov models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3429" to="3432" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
