<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Framework for CNN Based Speech Enhancement in the Time Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ashutosh</forename><surname>Pandey</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Guwahati</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A New Framework for CNN Based Speech Enhancement in the Time Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0928826C9665EB82510E69CE613CAAE8</idno>
					<idno type="DOI">10.1109/TASLP.2019.2913512</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2019.2913512, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech enhancement</term>
					<term>fully convolutional neural network</term>
					<term>time domain enhancement</term>
					<term>deep learning</term>
					<term>mean absolute error</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a new learning mechanism for a fully convolutional neural network (CNN) to address speech enhancement in the time domain. The CNN takes as input the time frames of noisy utterance and outputs the time frames of the enhanced utterance. At the training time, we add an extra operation that converts the time domain to the frequency domain. This conversion corresponds to simple matrix multiplication, and is hence differentiable implying that a frequency domain loss can be used for training in the time domain. We use mean absolute error (MAE) loss between the enhanced shorttime Fourier transform (STFT) magnitude and the clean STFT magnitude to train the CNN. This way the model can exploit the domain knowledge of converting a signal to the frequency domain for analysis. Moreover, this approach avoids the wellknown invalid STFT problem since the proposed CNN operates in the time domain. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed method is easy to implement and applicable to related speech processing tasks that require time-frequency (T-F) masking or spectral mapping.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Speech enhancement is the task of removing or attenuating additive noise from a speech signal, and it is generally concerned with improving the intelligibility and quality of degraded speech. Speech enhancement is employed as a preprocessor in many applications such as robust automatic speech recognition, teleconferencing and hearing aids design. The purpose of monaural (single-channel) speech enhancement is to provide a versatile and cost-efficient approach to the problem that utilizes recordings from only a single microphone. Single-channel speech enhancement is considered a very challenging problem especially at low signal-to-noise ratios (SNRs). This study focuses on single-channel speech enhancement in the time domain.</p><p>Traditional monaural speech enhancement approaches include statistical enhancement methods <ref type="bibr" target="#b0">[1]</ref> and computational auditory scene analysis <ref type="bibr" target="#b1">[2]</ref>. In the last few years, supervised methods for speech enhancement using deep neural networks (DNNs) have become the mainstream <ref type="bibr" target="#b2">[3]</ref>. Among the most popular deep learning methods are denoising autoencoders <ref type="bibr" target="#b3">[4]</ref>, feedforward neural networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and CNNs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This research was supported in part by two NIDCD grants (R01 DC012048 and R02 DCDC015521) and the Ohio Supercomputer Center.</p><p>A. Pandey is with the Department of Computer Science and Engineering, The Ohio State University, Columbus, OH 43210 USA (e-mail: pandey.99@osu.edu). D. L. Wang is with the Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH 43210 USA (e-mail: dwang@cse.ohio-state.edu) Most frequently employed methods for supervised speech enhancement use T-F masking or spectral mapping <ref type="bibr" target="#b2">[3]</ref>. Both of these approaches reconstruct the speech signal in the time domain from the frequency domain using the phase of the noisy signal. It means that the learning machine learns a mapping in the frequency domain but the task of going from the frequency domain to the time domain is not subject to the learning process. Integrating the domain knowledge of going from the frequency domain to the time domain, or the other way around, could be helpful for the core task of speech enhancement. A similar approach of incorporating such domain knowledge inside the network is found to be useful in <ref type="bibr" target="#b9">[10]</ref>, which employs a time-domain loss for T-F masking.</p><p>Recently in <ref type="bibr" target="#b10">[11]</ref>, the authors integrate a fixed number of steps of the iterative Multiple Input Spectrogram Inversion (MISI) algorithm <ref type="bibr" target="#b11">[12]</ref> inside DNN, which is found to be helpful for the speaker separation task.</p><p>We design a fully convolutional neural network that takes as input the noisy speech signal in the time domain and outputs the enhanced speech signal in the time domain. One way to train this network is to minimize the mean squared error (MSE) or the MAE loss between the clean speech signal and the enhanced speech signal <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>. However, our experiments show that, using a time domain loss, some of the phonetic information in the estimated speech is distorted probably because the underlying phones are difficult to distinguish from the background noise. Also, using a loss function in the time domain does not produce a good speech quality. So, we believe that it is important to use a frequency domain loss, which can discriminate speech sounds from nonspeech noises and produce speech with high quality.</p><p>Motivated by the above considerations, we propose to add an extra operation in the model at the training time that converts the estimated speech signal in the time domain to the frequency domain. The conversion from the time domain to the frequency domain is differentiable, so a loss in the frequency domain can be used to train a network in the time domain. We propose to use the MAE loss between the clean STFT magnitude and the estimated STFT magnitude.</p><p>The two approaches proposed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> employ a DNN in the frequency domain and use a loss function in the time domain to train the DNN. However, these methods use the noisy phase to reconstruct a time domain signal, and may suffer from the well-known invalid STFT problem <ref type="bibr" target="#b12">[13]</ref>. In our approach, a model is employed in the time domain and trained using a loss function in the frequency domain, so the generated signal is always a valid signal and we do not need to use the phase of the noisy signal. The neural network learns a phase structure itself in the process of optimizing the proposed loss. We show in Section VI that the learned phase is better than the noisy phase.</p><p>Other researchers have explored speech enhancement in the time domain using deep learning. In <ref type="bibr" target="#b8">[9]</ref>, authors explore CNNs for speech enhancement and claim that fully connected layers inside a DNN are not suitable for the time domain enhancement and instead propose to use a fully-convolutional neural network. Similar to our work, a time domain network has been proposed using a loss based on a short-term objective intelligibility metric <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, authors propose a generative adversarial network <ref type="bibr" target="#b14">[15]</ref> for speech enhancement in which the generator is an autoencoder based fully-convolutional network that is trained with the help of a discriminator. Recently, the Bayesian wavenet <ref type="bibr" target="#b15">[16]</ref> has been explored for speech enhancement followed by <ref type="bibr" target="#b16">[17]</ref>, which makes the wavenet <ref type="bibr" target="#b17">[18]</ref> faster and uses a discriminative approach rather than a generative approach. Very recently, a fully convolutional network is proposed for speaker separation in the time domain <ref type="bibr" target="#b18">[19]</ref> and it is trained using a loss based on scale-invariant signal-to-noise ratio (SI-SNR).</p><p>The work presented in this paper is an extension of our preliminary work in <ref type="bibr" target="#b19">[20]</ref>. Here, we present more extensive experiments, provide a justification for the observed behavior, and evaluate the proposed model in a speaker-and noiseindependent way. The rest of the paper is organized as follows.</p><p>In the next Section, we describe the method to compute a frequency domain loss for a CNN in the time domain. Section III explains the details about the model architecture. In Section IV, we briefly describe the invalid STFT problem. Experiments and comparisons are described in Sections V and VI. Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FREQUENCY DOMAIN LOSS FUNCTION</head><p>Given a real-valued vector x t of size N in the time domain, we can convert it to the frequency domain by multiplying it with a complex-valued discrete Fourier transform (DFT) matrix D using the following equation</p><formula xml:id="formula_0">x f = Dxt (1)</formula><p>where x f is the DFT of x t and D is of size N xN . Since x t is real-valued, the relation in ( <ref type="formula">1</ref>) can be rewritten as</p><formula xml:id="formula_1">x f = (Dr + iD i )xt = Drxt + iD i xt<label>(2)</label></formula><p>where D r and D i are real-valued matrices formed by taking the element-wise real and imaginary part of D and i denotes the imaginary unit. This relation can be separated into two Equations involving only real-valued vectors as given in the following Equation.</p><p>x fr = Drxt</p><formula xml:id="formula_2">x f i = D i xt<label>(3)</label></formula><p>Here, x fr and x fi are real-valued vectors formed by taking element-wise real and imaginary part of x f . A frequency domain loss can thus be defined using x fr and x fi . One such loss defined as the average of the MSE losses on the real and imaginary part of x f is:</p><formula xml:id="formula_3">L(x f , x f ) = 1 N N n=1 ((x fr (n) -x fr (n)) 2 + (x f i (n) -x f i (n)) 2 ) (4)</formula><p>where xf is an estimate of x f . x(n) denotes the n th component of x. It should be noted that this loss function has both magnitude and phase because it uses the real as well as the imaginary part. However, we find that using both the magnitude and the phase does not give as good performance as using only the magnitude. So, we use the following loss function defined using only the magnitudes.</p><formula xml:id="formula_4">L(x f , x f ) = 1 N N n=1 |(|x fr (n)| + |x f i (n)|) -(|x fr (n)| + |x f i (n)|)| (5)</formula><p>This loss can also be described as the MAE loss between the estimated STFT magnitude and the clean STFT magnitude when the magnitude of a complex number is defined using the L 1 norm. We also compare the proposed loss function with an L 2 loss defined as:</p><formula xml:id="formula_5">L(x f , x f ) = 1 N N n=1 | xfr (n) 2 + xf i (n) 2 + α- x fr (n) 2 + x f i (n) 2 + α|<label>(6)</label></formula><p>Here, α is a small positive constant added to stabilize the training. Our use of the MAE loss is partly motivated by a recent observation that the MAE loss works better in terms of objective quality scores when a spectral mapping based DNN is trained <ref type="bibr" target="#b20">[21]</ref>. In the present study we have confirmed that the MAE loss performs better than the MSE loss for objective intelligibility and quality. Fig. <ref type="figure" target="#fig_0">1</ref> shows a schematic diagram for computing a frequency domain loss from enhanced time domain frames. The proposed model operates on the frame size of 2048 samples, meaning that it takes as input a frame of duration 128 ms with the sampling frequency of 16 kHz, and outputs a frame of the same length. All the enhanced frames of an utterance at the network output are combined using the overlap-and-add (OLA) method to obtain the enhanced utterance. A frame shift of 256 samples is used for OLA. The enhanced utterance is then divided into frames of size 512. The obtained frames are multiplied by the Hamming window and then separately with two matrices D r and D i , each of size 512x512 as defined in Equation <ref type="formula" target="#formula_1">2</ref>. The matrix multiplication gives the real and imaginary part of the STFT. Next, the real and imaginary part of the STFT are combined to get the STFT magnitude. The computed STFT magnitude is compared with the clean STFT magnitude to obtain a frequency domain loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL ARCHITECTURE</head><p>We use a fully convolutional neural network that is comprised of a series of convolutional and deconvolutional layers. We first describe the convolution and deconvolution operation and then the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolution</head><p>Formally, a 1-D discrete convolution operator * , which convolves signal f with kernel k of size 2m + 1 and with stride r, is defined as</p><formula xml:id="formula_6">(f * k)(p) = s+t=(r×p) f (s)k(t)<label>(7)</label></formula><p>where p, s ∈ Z and t ∈ [-m, m] ∩ Z. Here, Z denotes the set of integers. A strided convolution used in this work is a convolution meant to reduce the size at the output by sliding the kernel over the input signal with a step greater than one. For example, given an input of length 2N , a kernel of size 2m + 1 and zero padding of size m on both sides of the input, a convolution with stride 2 will produce an output of size N . Hence the input of size 2N is effectively downsampled to an output of size N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deconvolution</head><p>A deconvolution layer <ref type="bibr" target="#b21">[22]</ref>, also known as transposed convolution, is a convolution meant to increase the size at the output. For a deconvolution with stride length r, r -1 zeroes are first inserted between the consecutive samples of the input signal. Then it is zero padded on both sides with an appropriate amount so that the convolution with a kernel of size k and stride 1 produces the output of desired size. For example, given an input of length N , kernel of size (2m + 1), and stride of 2, first, one zero will be inserted between the consecutive samples of the input, giving a signal of length 2N -1. Then the signal will be zero-padded on left and right by m and m + 1 respectively to get the signal of length 2N + 2m. After this, a convolution with a filter of size 2m + 1 will produce an output of size 2N . Hence the input of size N is effectively upsampled to an output of size 2N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed Model</head><p>We use an autoencoder based fully convolutional neural network with skip connections first proposed for time domain speech enhancement in <ref type="bibr" target="#b7">[8]</ref>, which was adopted from U-Net <ref type="bibr" target="#b22">[23]</ref>. The schematic diagram of the proposed model is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, which illustrates the processing of one frame. The first layer of the encoder is a convolutional layer that increases the number of channels from 1 to 64. Each of the next eight layers successively reduces the dimension of the input signal to half using convolutions with a stride of 2, while either doubling or maintaining the number of channels. The final output of the encoder is of size 8 with 256 channels. The decoder mirrors the encoder, consisting of a series of eight deconvolutional layers <ref type="bibr" target="#b21">[22]</ref> with a stride of 2 that double the dimension of its input making the number of channels the same as in the corresponding symmetric layer of the encoder. The output of each layer in the decoder is concatenated with the output from the corresponding symmetric layer of the encoder along the channel axis.</p><p>The skip connections are included because a signal can not be well reconstructed from the final output of the encoder as it has a much reduced dimension compared to the input and is a bottleneck. Furthermore, the skip connections help to provide gradients to the layers close to the input layer and avoid the vanishing gradient problem <ref type="bibr" target="#b23">[24]</ref>. The final output layer is a simple convolutional layer which reduces the number of channels from 64 to 1. Each layer in the network uses the activation of parametric ReLU non-linearity <ref type="bibr" target="#b24">[25]</ref> except for the output layer which uses the Tanh. A dropout <ref type="bibr" target="#b25">[26]</ref>   For the speaker-and noise-independent model trained on the WSJ0 SI-84 dataset <ref type="bibr" target="#b26">[27]</ref> (see Section V-A), we use a deeper network that takes as input a frame of size 16384 samples. The model mentioned above gives good performance for this task but is not able to improve the state-of-the-art performance. For the enhancement task on this dataset, the importance of future and past context was first established in <ref type="bibr" target="#b27">[28]</ref>. We also observe a considerable performance improvement when the context is increased by increasing the frame size. The dimensionality of the successive layers in the network used for this task is 16384x1 (input), 16384x32, 8192x32, 4096x32, 2048x64, 1024x64, 512x64, 256x128, 128x128, 64x128, 32x256, 16x256, 8x256, 16x512, 32x512, 64x256, 128x256, 256x256, 512x128, 1024x128, 2048x128, 4096x64, 8192x64, 16384x64 (output).</p><formula xml:id="formula_7">rate of 0.2 Output, 2048x1 2048x(64+64) 1024x(64+64) 512x(64+64) 256x(128+128) 128x(128+128) 64x(128+128) 32x(256+256) 16x(256+256)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INVALID SHORT-TIME FOURIER TRANSFORM</head><p>The STFT of a signal is obtained by taking the DFT of overlapped frames of a signal. The overlap between consecutive frames causes the adjacent frames to have common samples at the boundary of frames. This correlation between adjacent frames appears in the frequency domain as well and results in a certain relationship between the STFT magnitude and the STFT phase. This relationship needs to be maintained to reconstruct the original signal in the time domain. In <ref type="bibr" target="#b12">[13]</ref>, the authors show that not all 2-dimensional complex-valued signals correspond to a valid STFT. A 2-dimensional complexvalued signal, X(m, k) is a valid STFT if and only if the following holds.</p><formula xml:id="formula_8">STFT(ISTFT(X(m, k))) = X(m, k)<label>(8)</label></formula><p>Here, ISTFT denotes inverse STFT, m frame number, and k is frequency index. An STFT obtained by taking the STFT of a real signal in the time domain is always a valid STFT. It means that given a real signal x(t) in the time domain, the following relations will always hold.</p><formula xml:id="formula_9">ISTFT(STFT(x(t))) = x(t) STFT(ISTFT(STFT(x(t)))) = STFT(x(t))<label>(9)</label></formula><p>The problem arises when the STFT magnitude of a given real or the STFT phase of the signal are altered separately.</p><p>In such a case, the required relationship between the STFT magnitude and STFT phase is not guaranteed, and Equation ( <ref type="formula" target="#formula_5">6</ref>) may not hold. In the frequency domain speech enhancement, popular approaches are T-F masking and spectral mapping.</p><p>Both of these methods require using the STFT phase of the noisy speech with enhanced STFT magnitude to reconstruct a time domain signal. The combination of the noisy phase with the enhanced magnitude of STFT is unlikely a valid STFT, as recently demonstrated in <ref type="bibr" target="#b28">[29]</ref>. Invalid STFT causes unpleasant signal distortions. To deal with this problem, an iterative method has been proposed to get a signal in the time domain which produces the STFT magnitude close to the enhanced STFT magnitude while maintaining the valid STFT <ref type="bibr" target="#b12">[13]</ref>.</p><p>The proposed framework can be thought of as a supervised way of resolving the invalid STFT problem by a CNN, which produces a speech signal in the time domain but is trained with a loss function which minimizes the distance measured in terms of the STFT magnitudes.</p><p>The proposed model generates consecutive frames one by one and combines them using the OLA method as we find that OLA is better than a simple concatenation. Even though we use a loss function based on the magnitude of a valid STFT at the training time, this does not guarantee that generated consecutive frames will have matching samples at the boundary. But we use a frame size of 128 ms so that, even though the analysis window size is 32 ms, the validity of signal is guaranteed over the duration of 128 ms. The proposed model does not guarantee the whole utterance to be a valid signal. The simple concatenation of consecutive frames would give a valid signal, but it exhibits boundary discontinuity, giving worse objective scores than OLA. A different approach to generate a valid signal at the utterance level is to design a model that produces one sample at a time. But, such models based on deep learning are very slow and have not been established for speech enhancement. One such example is wavenet <ref type="bibr" target="#b17">[18]</ref>, which has been explored for denoising in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>. The proposed model in <ref type="bibr" target="#b15">[16]</ref> is very slow, and in <ref type="bibr" target="#b16">[17]</ref>, it operates at the frame level for efficiency and hence suffers from the same problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETTINGS A. Datasets</head><p>In our first set of experiments, we evaluate and analyze the proposed framework on the TIMIT dataset <ref type="bibr" target="#b29">[30]</ref> which consists of utterances from many male and female speakers. We use 2000 randomly chosen utterances from the TIMIT training set as the training utterances. The TIMIT core test set consisting of 192 utterances is used as the test set. We evaluate models for the noise-dependent and noise-independent case. Five noisedependent models are trained on the following five noises: babble, factory1, oproom, engine, and speech-shaped noise (SSN). A single noise-independent model is trained using the five noises mentioned above and evaluated on two untrained noises: factory2 and tank. All the noises except SSN are from the NOISEX <ref type="bibr" target="#b30">[31]</ref> dataset. All noises are around 4 minutes long. The training set is created by mixing random cuts from the first half of noises at the SNRs of -5 dB and 0 dB. The test set is created by adding random cuts from the second half of the noises at the SNRs of -5 dB, 0 dB, and 5 dB. Here, 5 dB is an untrained SNR condition that is used to asses SNR generalization of the trained models. The datasets are similar to the ones used in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Further, we evaluate the proposed framework in a speakerway on the IEEE database <ref type="bibr" target="#b32">[33]</ref>. This experiment is performed to evaluate if the proposed framework scales well for untrained noises after training using a large number of noises. The IEEE database consists of 720 utterances of a single male speaker. We create a large training set by mixing randomly selected 560 IEEE sentences with 10000 non-speech sounds from a sound-effect library (available at www.soundideas.com). The remaining 160 utterances are used as test utterances. The training set consists of 640000 noisy utterances. To create a training utterance an IEEE sentence is first randomly selected from the 560 training utterances. The selected utterance is then mixed at a fixed SNR of -2 dB with a random segment of a randomly selected noise. The total duration of the training noises is around 125 hours, and that of the training mixtures is around 380 hours. The test set is created by mixing the selected 160 test sentences with babble and cafeteria noise from the Auditec CD (available at http://www.auditec.com) at the SNRs of -5 dB, -2 dB, 0 dB and 5 dB. Here, -5 dB, 0 dB and 5 dB are untrained SNRs. The training and test set used in this experiment are similar to the ones used in <ref type="bibr" target="#b33">[34]</ref>, hence facilitating a direct comparison.</p><p>We also evaluate the proposed framework on the WSJ0 SI-84 dataset <ref type="bibr" target="#b26">[27]</ref> that consists of 7138 utterances from 83 speakers. A speaker-and noise-independent model is trained using a large training set. Seventy-seven speakers are selected to produce training utterances. The test set is created from the utterances of 6 speakers that are not included in the training set. The training and test mixtures are generated in the same manner as described in the previous paragraph. The only difference is that, in this experiment, 320000 utterances are at five SNRs of -5 dB, -4 dB, -3 dB, -2 dB, -1 dB. An utterance, a noise, and an SNR value are randomly selected first. Then the selected utterance is mixed with a random cut from the selected noise at the selected SNR. The SNRs used for evaluation are -5 dB, 0 dB and 5 dB. The training and the test set for this experiment are the same as in <ref type="bibr" target="#b34">[35]</ref>, again facilitating quantitative comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Setup</head><p>As described in the last subsection, we perform training on three datasets: TIMIT, IEEE, and WSJ0 SI-84. All the utterances are resampled to 16 kHz. The noisy and the clean utterances are normalized to the value range [-1, 1], and frames are extracted from the normalized utterances. A frame size of 2048 samples (128 ms) is used for the experiments on the TMIT and IEEE databases. A frame size of 16384 samples is used, as mentioned in Section III, for the experiments on WSJ0 SI-84. The frame shift to generate training frames is half of the frame size for IEEE and WSJ0 and 256 samples for the experiments on TIMIT.</p><p>A filter size of 11 is used. All the weights in CNNs are initialized using the Xavier initializer with normally distributed random initialization <ref type="bibr" target="#b35">[36]</ref>. The Adam optimizer <ref type="bibr" target="#b36">[37]</ref> is used for SGD (stochastic gradient descent) based optimization with a batch size of 4 utterances. The shorter utterances in a batch are zero padded to match the size of the longest utterance. The loss value computed over the zero padded region is ignored for gradient computation. The learning rate is set to 0.0002. The major difference between the architecture of the generator in SEGAN <ref type="bibr" target="#b7">[8]</ref> and the architecture of our model is that the input frame size to SEGAN is equal to 16384 samples whereas our input frame size is 2048 samples. The filter size in SEGAN is 31 as compared to 11 in our model. The total number of parameters in the SEGAN model is around 58 million whereas our model has around 6.4 million parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Models</head><p>To compare the noise-dependent and noise-independent models trained on the TIMIT dataset, we use three baseline models. First, we train a DNN model using the MAE loss to estimate the ideal ratio mask (IRM) <ref type="bibr" target="#b31">[32]</ref>. This model is a 3-layered fully connected DNN that takes as input the noisy STFT magnitudes of five consecutive frames (centered at the current frame) concatenated together and outputs the IRM of the corresponding five frames together. Multiple predictions of the IRM are averaged. The second baseline is the SEGAN model <ref type="bibr" target="#b7">[8]</ref>. In the method in <ref type="bibr" target="#b7">[8]</ref>, the generator of the generative adversarial network (GAN) <ref type="bibr" target="#b14">[15]</ref> is trained using two loss functions; adversarial loss and MAE loss in the time domain. We train two versions of this model. The first is trained using both the loss functions, adversarial loss and the MAE loss as in the original paper. We call this model SEGAN. The second is trained using only the loss on time domain samples. We call this model SEGAN-T in our experiments.</p><p>The model trained on the IEEE dataset is compared with a five-layered DNN model proposed in <ref type="bibr" target="#b33">[34]</ref> which uses the same dataset as in our work, and generates the training and test mixtures in the same manner. The input to their DNN is the concatenation of the power ( <ref type="formula">1</ref>15 ) compressed cochleagram of 23 consecutive frames (centered at the current frame) of noisy speech. The output of the DNN is the IRM of 5 consecutive frames. Multiple predictions of the IRM are also averaged. Each hidden layer has 2048 units.</p><p>Our speaker-and noise-independent model trained on WSJ0 SI-84 dataset is compared with a recently proposed model <ref type="bibr" target="#b34">[35]</ref>. This gated residual network (GRN) model is a 62-layer deep fully convolutional network with residual connections. It takes as input the spectrogram of the whole utterance at once and outputs the phase-sensitive mask (PSM) <ref type="bibr" target="#b37">[38]</ref> of the whole utterance. The layers in this model are comprised of dilated convolutional layers having gated linear units with an exponentially increasing rate of dilation. This model has the state-of-the-art performance for this speaker-and noiseindependent enhancement task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics and Comparisons</head><p>In our experiments, models are compared using short-term objective intelligibility (STOI) <ref type="bibr" target="#b38">[39]</ref>, perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b39">[40]</ref>, and scale-invariant signal-todistortion ratio (SI-SDR) <ref type="bibr" target="#b40">[41]</ref> scores, all of which represent the standard metrics for speech enhancement. STOI has a typical value range from 0 to 1, which can be roughly interpreted as percent correct. PESQ values range from -0.5 to 4.5.</p><p>All the time domain models are compared for both the MAE loss and the MSE loss training. For the experiments on the TIMIT dataset, the first comparison is done between the baseline models and the proposed model trained using different loss functions. The proposed model can be trained using a loss in the time domain or different types of loss in the frequency domain. For a frequency domain loss, two possible loss functions are a loss on both the real and the imaginary part of STFT and a loss on the STFT magnitude. We call our model autoencoder convolutional neural network (AECNN). The corresponding abbreviated names for our models trained using different loss functions are AECNN-T for using a loss on time domain samples, AECNN-RI for using a loss on both the real and the imaginary part of the STFT, and AECNN-SM for using a loss on STFT magnitudes. AECNN-SM1 denotes the loss defined on L 1 norm and AECNN-SM2 denotes the loss defined on L 2 norm of STFT coefficients.</p><p>Note that a model trained using a loss in the time domain or a loss on both the real and the imaginary part of the STFT utilizes phase information during training and hence training is supposed to learn both the magnitude and the phase. The model proposed in <ref type="bibr" target="#b41">[42]</ref> uses a loss on the real and imaginary part of the STFT but it operates in the frequency domain and thus is fundamentally different from our approach.</p><p>AECNN-SM trains a given model using a loss on STFT magnitudes, with no phase used at the training time. With the proposed time-domain enhancement, however, AECNN learns a phase structure itself. It is interesting to explore whether the learned phase is better than the mixture phase. We compare the learned phase with the mixture phase and the clean phase. First, we combine the STFT magnitude of the noisy utterance with two different kinds of phase: the learned phase and the clean phase. These are named MIX-SM and MIX-CLN respectively. Second, we combine the STFT magnitude predicted by the baseline IRM estimator with three kinds of phase: the noisy phase, the learned phase, and the clean phase. They are named IRM-MIX, IRM-SM, and IRM-CLN respectively. The STOI, PESQ and SI-SDR scores are compared at the SNRs of -5, 0, and 5 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSIONS</head><p>First, we present the results of noise-dependent models trained on the TIMIT dataset. Table I lists the average results over all the five noises at -5 dB, 0 dB, and 5 dB SNR. We divide our loss functions into two categories, those with phase, i.e., AECNN-T, SEGAN-T or AECNN-RI, and those without phase, i.e., AECNN-SM1 and AECNN-SM2. We observe that the models trained with a loss with phase perform better using MSE whereas the models trained with a loss without phase perform better using MAE. The SEGAN-T, AECNN-T, and AECNN-RI, have better STOI and SI-SDR scores with MAE but significantly worse PESQ. AECNN-SM1 and AECNN-SM2 produce better scores with the MAE loss. AECNN-SM1 and AECNN-SM2 have similar STOI and PESQ scores but AECNN-SM1 is consistently better in terms of SI-SDR.</p><p>Next, we observe that AECNN-T is significantly better than SEGAN-T and SEGAN, suggesting that the AECNN architecture is better than SEGAN for speech enhancement in the time domain. Additionally, the time domain models are much better than the baseline IRM based DNN model. The proposed AECNN-SM1 is significantly better in terms of STOI and PESQ compared to the AECNN-RI with MSE. AECNN-RI is marginally better in terms of SI-SDR. Note that the SI-SDR of AECNN-RI with MAE is better than AECNN-SM1, but its PESQ score is very low, implying that the AECNN-SM1 is better as it substantially improves PESQ while maintaining SI-SDR. In summary, the proposed AECNN-SM1 is the best in terms of STOI and PESQ, whereas AECNN-RI is the best for SI-SDR. A similar performance profile is observed for the noiseindependent models at the three SNRs as given in Table <ref type="table" target="#tab_1">II</ref>. AECNN-SM1 and AECNN-SM2 are the best in terms of STOI and PESQ. AECNN-RI is the best for SI-SDR at 5 dB but AECNN-SM1 is better at 0 dB and -5 dB. Note that we compare AECNN-SM1-MAE with AECNN-RI-MSE. AECNN-SM1 and AECNN-SM2 have similar STOI and PESQ scores but AECNN-SM1 is consitently better in terms of SI-SDR, making it the better enhancement approach.</p><p>For illustration, we plot spectrograms of a sample utterance in Fig. <ref type="figure">3</ref>. We can observe that the DNN based IRM estimator does not appear to distort the speech signal by much but is not able to remove some of the noise as can be seen for lowfrequency regions around 1.25s. AECNN-T reduces the noise more but still retains some noise as can be observed in the low-frequency region around 2.5s. The enhanced spectrogram using the STFT magnitude loss looks closest to the clean spectrogram, and is better not only for noise reduction but also seems to introduce relatively negligible distortions compared to the clean spectrogram.</p><p>The frequency domain loss functions using the real and imaginary part of the STFT do not perform as well as a loss based on the STFT magnitude. One explanation for this observation is that the STFT magnitude exhibits clearer  temporal structure than the real part or imaginary part. Also, it is non-negative, and likely easier to learn. As analyzed in <ref type="bibr" target="#b42">[43]</ref>, structure exists in the absolute of the real and the imaginary part of the STFT of a speech signal. Fig. <ref type="figure" target="#fig_3">4</ref> shows the signs and the absolute values of the real and imaginary parts of STFT for a sample utterance. To plot the absolute values, we first normalize the relevant data to the value range [10 -8 , 1] and then take their logarithm. We observe that the signs of real and imaginary parts are very noisy whereas there is a clear structure in their absolute values. This suggests that the real and the imaginary parts that are obtained by multiplying the signs and the absolute values would be unstructured. It further suggests that a model trained using a loss on the absolute values of the real or the imaginary part of STFT should perform better than a loss defined directly on the real and imaginary parts. To verify, we have trained models using a loss on the absolute value of the real and the imaginary part of the STFT. The PESQ scores for noise-dependent models trained on babble noise are plotted in Fig. <ref type="figure" target="#fig_4">5</ref>. We can see that using the absolute values improves the PESQ score and makes the performance comparable to the AECNN-SM1 model. We obtain similar results for the other four noises and noiseindependent models. Next, we compare learned phase with noisy phase and clean phase. The noisy test utterances constructed from the TIMIT corpus are used to obtain two STFT magnitudes: noisy STFT magnitude and STFT magnitude enhanced by the baseline DNN estimator. The two magnitudes are combined with three kinds of phase: noisy phase, learned phase and clean phase, to reconstruct a signal in the time domain. STOI, PESQ and SI-SDR values are compared with their corresponding mixture values. These scores at 3 SNR conditions are plotted in Fig. <ref type="figure" target="#fig_5">6</ref>. Note that the last column is for the factory2 noise which is enhanced using the noise-independent model. In all the plots, the three lower lines are for the speech reconstructed from the noisy STFT magnitudes and upper three lines are for the speech reconstructed using the enhanced STFT magnitude. We observe that the results with the learned phase are consistently better than those with the mixture phase. This suggests that the learned phase is better than the noisy phase for both the noise-dependent and noise-independent models. Using the clean phase all the scores are significantly better over the noisy phase. This performance gap is partly filled by using the learned phase. On the other hand, there is room for improvement if a neural network can learn to estimate a phase closer to the clean phase. The proposed method is further evaluated on the IEEE dataset for speaker-dependent but noise-independent training on a large number of noises. The performance and comparison with the DNN baseline in STOI are given in Table <ref type="table" target="#tab_2">III</ref>. The DNN model improves the STOI score by 16.6% at the -5 dB SNR, which is a difficult and untrained SNR condition. In this cndition the proposed framework improves the STOI score by 23.3%, which represents a substantial improvement. The STOI improvement over the baseline is 5.5% at -2 dB, 4% at 0 dB and 1.6% at 5 dB.</p><p>The baseline DNN used here has 20.5 million parameters whereas our model has only 6.4 million parameters. The DNN takes the acoustic features of 23 consecutive frames and outputs the IRM of 5 consecutive frames. It means that it uses information from 240 ms of speech and outputs 60 ms of speech. Our model operates on a speech of duration 128 ms Finally, we evaluate the proposed method for noise-and speaker-independent speech enhancement trained on a large training set created from the WSJ0 SI-84 dataset. The evaluation results for untrained speakers on untrained babble and cafeteria are given in Table <ref type="table" target="#tab_3">IV</ref>. Again, the proposed framework produces substantially better STOI and PESQ scores than the baseline GRN which is 62-layer deep and operates on a whole utterance. It means that the past and future context are maximal for the baseline model. Our model operates on a frame of duration 1024 ms. For this difficult task, the past and future contexts are very important, as analyzed in <ref type="bibr" target="#b33">[34]</ref>. We can increase the context in our model by increasing the size of the input frame, but we find that the performance does not improve further by increasing the frame size. One explanation for our better performance is that the GRN model uses the mixture phase whereas our model learns the phase itself, which we have shown is better than the mixture phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUDING REMARKS</head><p>In this paper, we have proposed a novel approach to train a fully convolutional neural network for speech enhancement in the time domain. The key idea is to use a frequency domain loss to train the CNN. We have investigated different types of loss function in the frequency domain. Our main observation is that frequency domain loss is better than a time domain loss. Using a frequency domain loss helps to improve objective quality and intelligibility. The highest improvement is obtained using an MAE loss computed on STFT magnitudes defined using L 1 norm. The best SI-SDR score is achieved using a loss on the real and imaginary part of STFT. We have evaluated our method for speech enhancement in speaker-dependent but noise-independent, and speaker-and noise-independent scenarios. In all the cases, the proposed method substantially outperforms the current state-of-the-art methods.</p><p>Other frequency domain losses do not perform as well as an STFT magnitude loss. This might be due to better structure in the STFT magnitude. We also observe that there is clear structure in the absolute of the real and imaginary part of the STFT. Training a model with a loss on the absolute value of the real and imaginary part of the STFT gives comparable performance to the STFT magnitude based loss.</p><p>We have also tried to make the real and imaginary part of STFT bounded or squashed with a Tanh or sigmoidal function, but not much improvement is obtained. Also, one might think that the logarithm of STFT magnitude should perform better, but that is not what we observe, probably because of a log operation involved in the training process. We also find that the performance of the proposed framework drops significantly when input and output lengths are reduced to one frame from four frames. Future research needs to develop methods for real-time implementation with comparable performance.</p><p>Although trained with a loss on only the STFT magnitudes, the proposed framework learns a phase structure itself as it generates a signal in the time domain. We find that learned phase is better than mixture phase but not as good as clean phase. A future research direction is to explore ways to train a deep model to improve phase estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Block diagram showing the steps involved in computing a frequency domain loss from the time domain frames at the output of network. L denotes the number of frames of size 2048, and N the length of the enhanced utterance obtained after overlap-and-add. M is the number of frames of size 512. D r and D i represent the real and the imaginary part of the DFT matrix respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2:A schematic diagram illustrating the architecture of the proposed model. The numbers represent the output dimension after each layer where M xN denotes a signal with dimension M and number of channels equal to N . Note that the number of channels in the decoder is double of that in the encoder because of the concatenation of the incoming connection from the symmetric layer of the encoder. Arrows denote skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.</head><label></label><figDesc>Fig. Spectrogram of a sample utterance mixed with babble noise at -5 dB SNR, and its enhanced spectrograms using different models. a) noisy spectrogram, b) spectrogram enhanced using a DNN based IRM estimator, c) spectrogram enhanced using AECNN trained with a time domain loss, d) spectrogram enhanced using AECNN trained with an STFT magnitude loss, e) clean spectrogram.</figDesc><graphic coords="6,172.22,164.91,101.04,61.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: STFT structure. Plots of the sign of real and imaginary part of STFT of a sample utterance are shown in a) and b). In these plots, black and white dots denote -1 and 1 respectively. The signs of the real and imaginary parts of STFT are very noisy. Plots c) and d) show the structure of the absolute of the real and imaginary part of STFT of the same utterance.</figDesc><graphic coords="7,337.25,149.45,92.76,52.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: PESQ comparisons of real-valued and absolute-valued loss functions. a) PESQ scores of the models trained on the real part and the absolute of the real part of STFT. b) PESQ scores of the models trained on the imaginary part and the absolute of the imaginary part of STFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: STOI, PESQ and SI-SDR comparisons between learned phase, noisy phase and clean phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison between various loss functions and network models for noise-dependent models trained on the TIMIT dataset.</figDesc><table><row><cell>SNR</cell><cell></cell><cell></cell><cell>-5 dB</cell><cell></cell><cell></cell><cell>0 dB</cell><cell></cell><cell></cell><cell>5 dB</cell><cell></cell></row><row><cell>Metric</cell><cell></cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell></row><row><cell>Mixture</cell><cell></cell><cell>56.5</cell><cell>1.41</cell><cell>-4.8</cell><cell>68.2</cell><cell>1.72</cell><cell>0.1</cell><cell>78.9</cell><cell>2.05</cell><cell>5.1</cell></row><row><cell>DNN</cell><cell></cell><cell>69.7</cell><cell>1.88</cell><cell>2.8</cell><cell>80.4</cell><cell>2.34</cell><cell>7.5</cell><cell>87.7</cell><cell>2.74</cell><cell>11.9</cell></row><row><cell>SEGAN</cell><cell></cell><cell>76.8</cell><cell>1.77</cell><cell>7.1</cell><cell>86.0</cell><cell>2.28</cell><cell>10.3</cell><cell>90.2</cell><cell>2.60</cell><cell>12.5</cell></row><row><cell>SEGAN-T</cell><cell>MAE MSE</cell><cell>77.7 77.9</cell><cell>1.73 2.01</cell><cell>7.6 7.7</cell><cell>87.2 87.3</cell><cell>2.22 2.46</cell><cell>11.2 11.2</cell><cell>91.3 91.5</cell><cell>2.57 2.78</cell><cell>13.5 13.6</cell></row><row><cell>AECNN-T</cell><cell>MAE MSE</cell><cell>78.9 78.6</cell><cell>1.88 2.00</cell><cell>8.2 8.0</cell><cell>88.2 88.0</cell><cell>2.41 2.60</cell><cell>11.7 11.5</cell><cell>92.3 92.2</cell><cell>2.80 2.90</cell><cell>14.3 13.9</cell></row><row><cell>AECNN-RI</cell><cell>MAE MSE</cell><cell>80.0 78.6</cell><cell>1.92 2.00</cell><cell>8.6 8.1</cell><cell>89.0 88.1</cell><cell>2.48 2.56</cell><cell>12.0 11.6</cell><cell>92.8 92.2</cell><cell>2.86 2.90</cell><cell>14.3 14.1</cell></row><row><cell>AECNN-SM1</cell><cell>MAE MSE</cell><cell>80.3 78.9</cell><cell>2.20 2.20</cell><cell>8.0 7.5</cell><cell>89.0 88.0</cell><cell>2.68 2.60</cell><cell>11.4 11.1</cell><cell>92.8 92.2</cell><cell>3.01 2.90</cell><cell>13.7 13.6</cell></row><row><cell>AECNN-SM2</cell><cell>MAE MSE</cell><cell>80.2 78.9</cell><cell>2.20 2.20</cell><cell>7.8 7.3</cell><cell>89.0 88.1</cell><cell>2.70 2.60</cell><cell>10.8 10.7</cell><cell>92.7 92.1</cell><cell>3.02 2.9</cell><cell>12.7 12.9</cell></row><row><cell cols="5">is applied at every 3 layers. To summarize, the dimensionality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">of the outputs from the successive layers in the proposed net-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">work is 2048x1 (input), 2048x64, 1024x64, 512x64, 256x128,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">128x128, 64x128, 32x256, 16x256, 8x256, 16x512, 32x512,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">64x256, 128x256, 256x256, 512x128, 1024x128, 2048x128,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2048x1 (output).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance comparison between various loss functions and network models noise-independent models trained on the TIMIT dataset.</figDesc><table><row><cell>SNR</cell><cell></cell><cell></cell><cell>-5 dB</cell><cell></cell><cell></cell><cell>0 dB</cell><cell></cell><cell></cell><cell>5 dB</cell><cell></cell></row><row><cell>Metric</cell><cell></cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell><cell>STOI (%)</cell><cell cols="2">PESQ SI-SDR</cell></row><row><cell>Mixture</cell><cell></cell><cell>66.8</cell><cell>1.63</cell><cell>-4.8</cell><cell>76.3</cell><cell>2.00</cell><cell>0.1</cell><cell>84.4</cell><cell>2.35</cell><cell>5.1</cell></row><row><cell>DNN</cell><cell></cell><cell>76.3</cell><cell>2.21</cell><cell>6.5</cell><cell>84.8</cell><cell>2.64</cell><cell>10.7</cell><cell>90.0</cell><cell>2.99</cell><cell>14.6</cell></row><row><cell>SEGAN</cell><cell></cell><cell>83.3</cell><cell>2.15</cell><cell>9.1</cell><cell>89.1</cell><cell>2.56</cell><cell>11.5</cell><cell>91.3</cell><cell>2.77</cell><cell>12.8</cell></row><row><cell>SEGAN-T</cell><cell>MAE MSE</cell><cell>83.9 84.0</cell><cell>2.01 2.33</cell><cell>10.3 10.1</cell><cell>90.4 90.1</cell><cell>2.47 2.70</cell><cell>13.2 13.0</cell><cell>93.0 92.7</cell><cell>2.75 2.94</cell><cell>15.0 14.9</cell></row><row><cell>AECNN-T</cell><cell>MAE MSE</cell><cell>84.7 84.5</cell><cell>2.14 2.35</cell><cell>10.7 10.1</cell><cell>91.1 90.8</cell><cell>2.64 2.79</cell><cell>13.8 13.1</cell><cell>93.6 93.3</cell><cell>2.93 3.05</cell><cell>15.7 14.8</cell></row><row><cell>AECNN-RI</cell><cell>MAE MSE</cell><cell>84.8 84.5</cell><cell>2.12 2.36</cell><cell>11.0 10.1</cell><cell>91.3 90.7</cell><cell>2.66 2.79</cell><cell>13.9 13.3</cell><cell>93.8 93.2</cell><cell>2.96 3.05</cell><cell>15.7 15.2</cell></row><row><cell>AECNN-SM1</cell><cell>MAE MSE</cell><cell>85.7 85.1</cell><cell>2.50 2.50</cell><cell>10.6 10.2</cell><cell>91.7 91.0</cell><cell>2.93 2.85</cell><cell>13.4 13.2</cell><cell>93.9 93.4</cell><cell>3.15 3.08</cell><cell>14.9 15.1</cell></row><row><cell>AECNN-SM2</cell><cell>MAE MSE</cell><cell>85.9 85.1</cell><cell>2.51 2.51</cell><cell>10.0 9.7</cell><cell>91.9 91.3</cell><cell>2.94 2.87</cell><cell>12.6 12.7</cell><cell>94.2 93.7</cell><cell>3.19 3.10</cell><cell>14.0 14.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison between the baseline DNN and the proposed framework for noise-independent models trained on the IEEE dataset. The scores denote the percent improvement in STOI over mixture.</figDesc><table><row><cell></cell><cell></cell><cell>Babble</cell><cell></cell><cell>Cafeteria</cell></row><row><cell></cell><cell>DNN</cell><cell>AECNN-SM1</cell><cell>DNN</cell><cell>AECNN-SM1</cell></row><row><cell>5 dB</cell><cell>12</cell><cell>13.6</cell><cell>13.3</cell><cell>14.4</cell></row><row><cell>0 dB</cell><cell>17.1</cell><cell>21.1</cell><cell>18.1</cell><cell>21.1</cell></row><row><cell>-2 dB</cell><cell>18</cell><cell>23.5</cell><cell>18.7</cell><cell>22.5</cell></row><row><cell>5 dB</cell><cell>16.6</cell><cell>23.3</cell><cell>17.5</cell><cell>21.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison between the baseline GRN and the proposed method for speaker-and noise-independent model trained on the WSJ0 S1-84 dataset and tested on untrained speakers. The scores denote the improvement over mixture.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>STOI</cell><cell></cell><cell>PESQ</cell></row><row><cell></cell><cell></cell><cell>GRN</cell><cell>AECNN-SM1</cell><cell>GRN</cell><cell>AECNN-SM1</cell></row><row><cell>Babble</cell><cell>-5 dB -2 dB</cell><cell>17.3 17</cell><cell>22.6 21.9</cell><cell>0.43 0.59</cell><cell>0.61 0.84</cell></row><row><cell>Cafeteria</cell><cell>-5 dB -2 dB</cell><cell>17.8 17.7</cell><cell>22.5 21.9</cell><cell>0.67 0.77</cell><cell>0.77 0.93</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Ke Tan for providing GRN results for our comparison.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech Enhancement: Theory and Practice</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fully convolutional neural network for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Raw waveform-based speech enhancement by fully convolutional networks</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02205</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep neural network for time-domain signal reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4390" to="4394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-toend speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2708" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterative phase estimation for the synthesis of separated sources from single-channel mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="421" to="424" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech enhancement using bayesian wavenet</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Florêncio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2013" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of ICASSP</title>
		<meeting>eddings of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">TasNet: Surpassing ideal time-frequency masking for speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07454</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new framework for supervised speech enhancement in the time domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1136" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On adversarial training and loss functions for speech enhancement</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5414" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<editor>J. Kolen and S. Kremer</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>in Field Guide to Dynamical Recurrent Networks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer vision</title>
		<meeting>the IEEE International Conference on Computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based CSR corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Speech and Natural Language</title>
		<meeting>the Workshop on Speech and Natural Language</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory for speaker generalization in supervised speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4705" to="4714" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="982" to="992" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Darpa timit acoustic-phonetic continous speech corpus CD-ROM. nist speech disc 1-1.1</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NASA STI/Recon technical report n</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">IEEE recommended practice for speech quality measurements</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2604" to="2612" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gated residual networks with dilated convolutions for supervised speech separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Complex spectrogram enhancement by convolutional neural network with multi-metrics learning</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
