<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING END-TO-END SPEECH RECOGNITION WITH PRONUNCIATION-ASSISTED SUB-WORD MODELING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hainan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuoyang</forename><surname>Ding</surname></persName>
							<email>dings@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<email>shinjiw@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING END-TO-END SPEECH RECOGNITION WITH PRONUNCIATION-ASSISTED SUB-WORD MODELING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>end-to-end models</term>
					<term>speech recognition</term>
					<term>sub-word modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most end-to-end speech recognition systems model text directly as a sequence of characters or sub-words. Current approaches to sub-word extraction only consider character sequence frequencies, which at times produce inferior sub-word segmentation that might lead to erroneous speech recognition output. We propose pronunciation-assisted sub-word modeling (PASM), a sub-word extraction method that leverages the pronunciation information of a word. Experiments show that the proposed method can greatly improve upon the characterbased baseline, and also outperform commonly used byte-pair encoding methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, end-to-end models have become popular among the speech community. Compared to hybrid-systems that consist of separate pronunciation, acoustic and language models, all of which need to be independently trained, an end-to-end system is a single neural-network which implicitly models all three. Although modular training of those components is possible <ref type="bibr" target="#b0">[1]</ref>, an end-to-end model is usually jointly optimized during training. Among the different network typologies for end-to-end systems, the attention-based encoder-decoder mechanism has proven to be very successful in a number of tasks, including automatic speech recognition (ASR) <ref type="bibr" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref> and neural machine translation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>Due to lack of a pronunciation dictionary, most end-toend systems do not model words directly, but instead model the output text sequence in finer units, usually characters. This is one of the most attractive benefits of an end-to-end system as it greatly reduce the complexities of overall architecture. However, it only works best for languages where there is a strong link between the spelling and the pronunciation, e.g. Spanish. For languages like English, however, this approach might limit the performance of the system, especially when there is no enough data for the system to learn all the subtleties in the language. On the other hand, linguists have developed very sophisticated pronunciation dictionaries of high quality for most languages, which can potentially improve the performance of end-to-end systems <ref type="bibr" target="#b6">[7]</ref>.</p><p>Sub-word representations have recently seen their success in ASR <ref type="bibr" target="#b7">[8]</ref>. Using sub-word features has a number of benefits for ASR, in that it can speed up both training and inference, while helping the system better learn the pronunciation patterns of a language. For example, if a sub-word algorithm segments the word "thank" into "th-an-k", this will make it easier for the ASR system to learn the association between the spelling "th" and the corresponding sound, which is not a concatenation of "t" and "h". However, it should also be noted that lots of these methods are designed for text processing tasks such as neural machine translation, and thus are only based on word spellings and do not have access to pronunciation information. It is therefore possible for these algorithms to break a word sequence into units that do not imply well-formed correspondence to phonetic units, making it even more difficult to learn the mapping between phonemes and spellings. For example, if a sub-word model sees a lot of "hys" in the data, it might process the word "physics" into "phys-ics", making the association with the "f" phoneme hard to learn. We argue it is far from ideal to directly apply these methods to ASR and improvements should be made to incorporate pronunciation information when determining subword segmentation.</p><p>This paper is an effort on this direction by utilizing a pronunciation dictionary and an aligner. We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align <ref type="bibr" target="#b8">[9]</ref> to align a pronunciation lexicon arXiv:1811.04284v2 [cs.CL] 21 Feb 2019 file and use the result to figure out common correspondence between sub-word units and phonetic units. We then use the statistics collected from this correspondence to guide our segmentation process such that it better caters to the need of ASR. The proposed method would work on a variety of languages with known lexicon, and would also work in other tasks, e.g. speech translation.</p><p>This paper is organized as follows. In section 2, we describe prior work; in section 3, we give a detail description of our proposed method, followed by section 4, where we report our experiment results. We will conduct an analysis and discussion of the results in section 5 and then talk about future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The use of a pronunciation dictionary is the standard approach in hybrid speech recognition. <ref type="bibr" target="#b9">[10]</ref> use the phone-level alignment to generate a probabilistic lexicon and proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, <ref type="bibr" target="#b6">[7]</ref> investigated the value of a lexicon in end-to-end ASR. Sub-word methods have a long history of application in a number of language related tasks. <ref type="bibr" target="#b10">[11]</ref> used sub-words units in particular for detecting unseen words. <ref type="bibr" target="#b11">[12]</ref> used sub-words units in building text-independent speech recognition systems. <ref type="bibr" target="#b12">[13]</ref> improved upon sub-word methods in WFST-based speech recognition.</p><p>Apart from the application in ASR, the most recent tide of adopting sub-word representations is largely driven by neural machine translation. <ref type="bibr" target="#b4">[5]</ref> proposed to use byte-pair encoding (BPE) <ref type="bibr" target="#b13">[14]</ref> to build a sub-word dictionary by greedily keep the most frequent co-occurring character sequences. Concurrently, <ref type="bibr" target="#b14">[15]</ref> borrow the practice in voice search <ref type="bibr" target="#b15">[16]</ref> to segment words into wordpiece which maximizes the language model probability. <ref type="bibr" target="#b5">[6]</ref> augments the training data with subword segmentation sampled from the segmentation lattice, thus increasing the robustness of the system to segmentation ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>The high-level idea of our method is as follows: instead of generating a sub-word segmentation scheme by collecting spelling statistics from the tokenized text corpus, we collect such statistics only from the consistent letter-phoneme pairs extracted from a pronunciation lexicon. The automatically extracted consistent letter-phoneme pairs can be treated as an induced explanation for the pronunciation of each word, and hence, such pairs will ideally contain no letter sequences, i.e. sub-words, that will lead to ill configurations such as "p-hys-ics".</p><p>We generate sub-word segmentation schemes in 3 steps: To simplify the model and generalize to unseen words, we do not perform word-dependent sub-word modeling in this work.</p><p>Our model generates a list of sub-words with weights, and we split any word with those sub-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Letter-phoneme Alignment Generation</head><p>We use fast align to generate an alignment between letters and phonemes i.e. its pronunciation, which will be able to find common patterns of letter sequences that correspond to certain phonetic units. For example, for the alignment shown in Figure <ref type="figure" target="#fig_0">1</ref>, it is represented as a set, {(0, 0), (1, 1), (2, 2), (3, 2), (4, 3)}</p><p>where each element in the set is a pair of (letter index, phone index), both being 0-based. In this case, letters 2 and 3 are aligned to the same phoneme 2. In practice, we could have one-to-one (e.g. "cat"), one-to-many (e.g. "ex"), many-toone (e.g "ah") and even many-to-many alignments (linguistically this should not happen for most languages but this is a good indicator of an "outlier" case, e.g. a French word in an English corpus which the aligner does not know how to process properly).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Finding Consistent Letter-phoneme Pairs</head><p>Formally, a consistent letter-phoneme pair (L, P ) is consisted of a letter sequence (or sub-word) L = (l 1 , ..., l n ) and a phoneme sequence P = (p 1 , ..., p m ). These pairs are heuristically extracted from the letter-phoneme alignment generated by fast align, and are then further refined to reduce noise mostly introduced by erroneous alignments. Extraction As fast align is a re-parameterization of IBM model 2, a typical alignment method for statistical machine translation, it does not limit itself in generating Fig. <ref type="figure">2</ref>. An Alignment with Crossovers monotonic alignments. There could be cross-overs in its output, like in Figure <ref type="figure">2</ref>, as well as "null-alignments", where a letter is aligned to a "null" symbol.</p><p>In the case of non-crossing alignments like the one shown in Figure <ref type="figure" target="#fig_0">1</ref>, we simply extract each connected sub-sequences. The extracted consistent pairs of this example would be (s, s), (p, p), (e-a, iy), (k, k). When there are cross-overs in the generated alignments, like in Figure <ref type="figure">2</ref>, we take the maximum clustered sub-graph as a consistent pair, i.e. extracting <ref type="figure">(b-c-d, g-h-i</ref>).</p><p>If a letter is aligned to a "null" symbol, we do not count this as a "cross-over" and keep the letter-to-null mapping for later processing.</p><p>Refinement Refinement over the consistent letterphoneme pairs is performed under the following criteria:</p><p>1. min-count constraint: L must occur at least N times in the training corpus, 2. proportion constraint: of all the words containing L in the corpus, at least a certain fraction p of all occurrences is mapped to a particular phone-sequence P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Collecting Letter Sequence Statistics</head><p>Recall that while we use pronunciation lexicon to extract consistent letter-phoneme pairs, our ultimate goal is to collect reliable statistics of the letter sequences (i.e. sub-word) to guide the sub-word segmentation process. Such statistics has nothing to do with phonemes, which means it needs to be marginalized. We perform the marginalization by summing up the counts of each type of letter sequence over all possible types of phoneme sequences. The marginalized counts would act as weights of sub-word units, where higher counts indicate higher weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text Processing</head><p>As with all the sub-word modeling methods, our text processing step takes tokenized word sequences as input and segment them into sequences of sub-words. The segmentation process is essentially a search problem operating on the lattice of all possible sub-word segmentation schemes over the word-level input. This segmentation space is constrained by the complete set of sub-words in the segmentation scheme generated above, with hypothesis priorities assigned by the associated weight statistics, where sub-words with higher weights would have higher priorities. For example, if both "ab" and "bc" are chosen as sub-words, and "ab" occurs more often than "bc" according to the statistics, then "abc" would be split as "ab c" instead of "a bc".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We conduct our experiment using the open-source end-toend speech recognition toolkit ESPnet <ref type="bibr" target="#b16">[17]</ref>. We report the ASR performance on the Wall Street Journal (WSJ) and LibriSpeech (100h) datasets. Our baseline is the standard character-based recipe, using bi-directional LSTMs with projection layers as the encoder, location-based attention, and LSTM decoder, with a CTC-weight of 0.5 during training <ref type="bibr" target="#b3">[4]</ref>. To fully see the effect of sub-word methods, we do not perform language model rescoring but report the 1st pass numbers directly. We also compare our systems with BPE baselines. The BPE procedure follows the algorithm described in <ref type="bibr" target="#b4">[5]</ref>. All the PASM segmentation schemes are trained using the lexicon included in its default recipe, and we use N = 100 and p = 0.5. All the other hyper-parameters are independently tuned.</p><p>For the WSJ setup, we have kept the number of sub-word units to be the same in BPE and PASM systems (both = 108). The results are shown in Table <ref type="table" target="#tab_1">2</ref>, where we report the worderror-rates on the dev93 and eval92 sets. We see that, the use of BPE improves dev93 performance but hurts performance on eval92. PASM method gives consistent improvements in the 2 datasets. the s al e of the ho t e l s is p ar t of ho l id ay ' s s t r ate g y to s e l l of f as s e t s and con c ent r ate on pro p er t y m an a ge ment</p><p>We also report the more BPE results on WSJ, adjusting number of BPE units in Table <ref type="table" target="#tab_0">1</ref>. We can see that having more BPEs actually hurts the performance <ref type="foot" target="#foot_0">1</ref> . This is likely because of the limited data-size of WSJ, which makes it hard to learn reliable BPE units.</p><p>In Table <ref type="table" target="#tab_2">3</ref>, we report the WER results on the LibriSpeech dataset, using the parameters described in <ref type="bibr" target="#b7">[8]</ref>. We have seen that PASM significantly improves the character-based baseline; BPEs do not help in this case, possibly due to poor hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANALYSIS</head><p>In Table <ref type="table" target="#tab_3">4</ref>, we show the output after the BPE procedure of the first sentence in the WSJ training data, and compare that with the result of the PASM algorithm <ref type="foot" target="#foot_1">2</ref> .</p><p>From the examples above, we observe the following:</p><p>• The PASM method correctly learns linguistic units, including "le", "th", "ay", "ll", "ll", "ss", "ge", which correspond to only one phoneme, but were not correctly handled in the BPE case.</p><p>• The BPE learns some non-linguistic but frequent-seen units in data, e.g. "the", "ate". In particular, the pronunciation associated with "ate" in the 2 occurrences are very different (concentr-ate vs str-ate-gy), which might make it harder for the system to learn the associations.</p><p>• As the number of BPE units increases, we see more sub-word units that do not conform to linguistic constraints, e.g. "as-s-e-t-s" and "of-f" in BPE-400. In this case, the 2nd "s" "asset" and 2nd "f" in "off" would have to be silent in terms of pronunciation, which would likely confuse the training of end-to-end systems unless there is a huge amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a sub-word modeling method for end-to-end ASR based on information from their pronunciations. Experiments show that the proposed method gives substantial gains over the letter-based baseline, as measured by word-error-rates. The method also outperforms BPEbased systems. We postulate that the improvement comes from the fact that, the proposed method learns more phonetically meaningful sub-words for speech tasks, unlike BPE which only take the spelling into consideration.</p><p>There are a lot of future work directions that we plan to explore. We will design new algorithms for aligning pronunciation dictionaries that is tailored for speech tasks; we will combine the proposed method with BPE to further improve ASR performances and speed up systems; we also plan to investigate the application of the proposed method in hybrid ASR, machine translation, as well as speech translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A Simple Alignment for the Word "SPEAK"</figDesc><graphic url="image-1.png" coords="2,378.50,91.98,117.20,88.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>WER Results of BPE Systems on WSJ</figDesc><table><row><cell cols="2">Num-BPEs 50</cell><cell>108 200 400</cell></row><row><cell>dev93</cell><cell cols="2">20.7 19.5 21.3 24.6</cell></row><row><cell>eval92</cell><cell cols="2">15.2 15.6 17.7 20.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>WER Results on WSJ</figDesc><table><row><cell cols="3">Baseline PASM BPE</cell></row><row><cell>dev93 20.7</cell><cell>18.5</cell><cell>19.5</cell></row><row><cell>eval92 15.2</cell><cell>14.3</cell><cell>15.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>WER Results on LibriSpeech</figDesc><table><row><cell cols="2">Baseline BPE PASM</cell></row><row><cell>dev-clean 23.8</cell><cell>29.5 21.4</cell></row><row><cell>dev-other 52.8</cell><cell>53.1 50.7</cell></row><row><cell>test-clean 23.2</cell><cell>29.5 21.3</cell></row><row><cell>test-other 54.8</cell><cell>55.3 52.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Samples of Segmented Text Under the PASM Scheme and BPE Schemes with Various Vocabulary Sizes</figDesc><table><row><cell>Scheme</cell><cell>Text</cell><cell></cell><cell></cell></row><row><cell>original</cell><cell cols="4">the sale of the hotels is part of holiday's strategy to sell off assets and</cell></row><row><cell></cell><cell cols="2">concentrate on property management</cell><cell></cell></row><row><cell>PASM</cell><cell cols="4">th e s a le o f th e h o t e l s i s p a r t o f h o l i d ay ' s s t</cell></row><row><cell></cell><cell cols="4">r a t e g y t o se ll o ff a ss e t s a n d c o n c e n t r a t e o n p</cell></row><row><cell></cell><cell cols="2">r o p er ty m a n a ge m e n t</cell><cell></cell></row><row><cell>BPE-108</cell><cell>the s al e of the</cell><cell>h o t e l s is p ar t of</cell><cell cols="2">h o l i d a y ' s st r</cell></row><row><cell></cell><cell cols="3">ate g y to s e l l of f a s s e t s and con c en t r ate</cell><cell>on pro p er t</cell></row><row><cell></cell><cell>y m an a g e m en t</cell><cell></cell><cell></cell></row><row><cell>BPE-200</cell><cell cols="4">the s al e of the ho t e l s is p ar t of ho l id ay ' s s t r ate g</cell></row><row><cell></cell><cell cols="4">y to s e l l of f as s e t s and con c ent r ate on pro p er t y m an</cell></row><row><cell></cell><cell>a ge me nt</cell><cell></cell><cell></cell></row><row><cell>BPE-400</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The character-based baseline has a vocabulary-size of 50.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">For clear presentation, we use the underline character to represent a "start-of-word" symbol.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On modular training of neural acoustics-to-word model for lvcsr</title>
		<author>
			<persName><forename type="first">Zhehuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01090</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 15-20. 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No need for a lexicon? evaluating the value of the pronunciation lexica in end-to-end models</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungji</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Schogol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5859" to="5863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03294</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pronunciation and silence probability modeling for asr</title>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Subword speech recognition for detection of unseen words</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Bulyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Herrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mihelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vocabulary independent speech recognition system and method using subword units</title>
		<author>
			<persName><forename type="first">Jean-Manuel</forename><surname>Van Thong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">398</biblScope>
			<date type="published" when="1920-02">Feb. 20 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved subword modeling for wfst-based speech recognition</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2017-18t Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">March 25-30, 2012, 2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
