<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
							<email>aditir@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
							<email>jsteinhardt@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most = 0.1 can cause more than 35% test error.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite the impressive (and sometimes even superhuman) accuracies of machine learning on diverse tasks such as object recognition <ref type="bibr" target="#b18">(He et al., 2015)</ref>, speech recognition <ref type="bibr" target="#b54">(Xiong et al., 2016)</ref>, and playing Go <ref type="bibr" target="#b45">(Silver et al., 2016)</ref>, classifiers still fail catastrophically in the presence of small imperceptible but adversarial perturbations <ref type="bibr" target="#b49">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b17">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b25">Kurakin et al., 2016)</ref>. In addition to being an intriguing phenonemon, the existence of such "adversarial examples" exposes a serious vulnerability in current ML systems <ref type="bibr" target="#b13">(Evtimov et al., 2017;</ref><ref type="bibr" target="#b44">Sharif et al., 2016;</ref><ref type="bibr">Carlini et al., 2016)</ref>. While formally defining an "imperceptible" perturbation is difficult, a commonly-used proxy is perturbations that are bounded in ∞ -norm <ref type="bibr" target="#b17">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b30">Madry et al., 2017;</ref><ref type="bibr" target="#b52">Tramèr et al., 2017)</ref>; we focus on this attack model in this paper, as even for this proxy it is not known how to construct high-performing image classifiers that are robust to perturbations.</p><p>While a proposed defense (classifier) is often empirically shown to be successful against the set of attacks known at the time, new stronger attacks are subsequently discovered that render the defense useless. For example, defensive distillation <ref type="bibr" target="#b41">(Papernot et al., 2016c)</ref> and adversarial training against the Fast Gradient Sign Method <ref type="bibr" target="#b17">(Goodfellow et al., 2015)</ref> were two defenses that were later shown to be ineffective against stronger attacks <ref type="bibr">(Carlini &amp; Wagner, 2016;</ref><ref type="bibr" target="#b52">Tramèr et al., 2017)</ref>. In order to break this arms race between attackers and defenders, we need to come up with defenses that are successful against all attacks within a certain class.</p><p>However, even computing the worst-case error for a given network against all adversarial perturbations in an ∞ -ball is computationally intractable. One common approximation is to replace the worst-case loss with the loss from a given heuristic attack strategy, such as the Fast Gradient Sign Method <ref type="bibr" target="#b17">(Goodfellow et al., 2015)</ref> or more powerful iterative methods <ref type="bibr" target="#b7">(Carlini &amp; Wagner, 2017a;</ref><ref type="bibr" target="#b30">Madry et al., 2017)</ref>. Adversarial training minimizes the loss with respect to these heuristics. However, this essentially minimizes a lower bound on the worst-case loss, which is problematic since points where the bound is loose have disproportionately lower objective values, which could lure and mislead an optimizer. Indeed, while adversarial training often provides robustness against a specific attack, it often fails to generalize to new attacks, as described above. Another approach is to compute the worst-case perturbation exactly using discrete optimization <ref type="bibr" target="#b21">(Katz et al., 2017a;</ref><ref type="bibr">Carlini (a)</ref> (b)</p><p>Figure <ref type="figure">1</ref>: Illustration of the margin function f (x) for a simple two-layer network. (a) Contours of f (x) in an ∞ ball around x. Sharp curvature near x renders a linear approximation highly inaccurate, and f (A fgsm (x)) obtained by maximising this approximation is much smaller than f (A opt (x)).</p><p>(b) Vector field for ∇f (x) with length of arrows proportional to ∇f (x) 1 . In our approach, we bound f (A opt (x)) by bounding the maximum of ∇f (x) 1 over the neighborhood (green arrow).</p><p>In general, this could be very different from ∇f (x) 1 at just the point x (red arrow).</p><p>et <ref type="bibr">al., 2017)</ref>. Currently, these approaches can take up to several hours or longer to compute the loss for a single example even for small networks with a few hundred hidden units. Training a network would require performing this computation in the inner loop, which is infeasible.</p><p>In this paper, we introduce an approach that avoids both the inaccuracy of lower bounds and the intractability of exact computation, by computing an upper bound on the worst-case loss for neural networks with one hidden layer, based on a semidefinite relaxation that can be computed efficiently. This upper bound serves as a certificate of robustness against all attacks for a given network and input. Minimizing an upper bound is safer than minimizing a lower bound, because points where the bound is loose have disproportionately higher objective values, which the optimizer will tend to avoid. Furthermore, our certificate of robustness, by virtue of being differentiable, is trainable-it can be optimized at training time jointly with the network, acting as a regularizer that encourages robustness against all ∞ attacks.</p><p>In summary, we are the first (along with the concurrent work of <ref type="bibr" target="#b24">Kolter &amp; Wong (2017)</ref>) to demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples on two-layer networks. We train a network on MNIST whose test error on clean data is 4.2%, and which comes with a certificate that no attack can misclassify more than 35% of the test examples using ∞ perturbations of size = 0.1.</p><p>Notation. For a vector z ∈ R n , we use z i to denote the i th coordinate of z. For a matrix Z ∈ R m×n , Z i denotes the i th row. For any activation function σ : R → R (e.g., sigmoid, ReLU) and a vector</p><formula xml:id="formula_0">z ∈ R n , σ(z) is a vector in R n with σ(z) i = σ(z i ) (non-linearity is applied element-wise). We use B (z) to denote the ∞ ball of radius around z ∈ R d : B (z) = {z | |z i − z i | ≤ for i = 1, 2, . . . d}.</formula><p>Finally, we denote the vector of all zeros by 0 and the vector of all ones by 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SETUP</head><p>Score-based classifiers. Our goal is to learn a mapping C : X → Y, where X = R d is the input space (e.g., images) and Y = {1, . . . , k} is the set of k class labels (e.g., object categories). Assume C is driven by a scoring function f i : X → R for all classes i ∈ Y, where the classifier chooses the class with the highest score: C(x) = arg max i∈Y f i (x). Also, define the pairwise margin</p><formula xml:id="formula_1">f ij (x) def = f i (x) − f j (x)</formula><p>for every pair of classes (i, j). Note that the classifier outputs C(x) = i iff f ij (x) &gt; 0 for all alternative classes j = i. Normally, a classifier is evaluated on the 0-1 loss</p><formula xml:id="formula_2">(x, y) = I[C(x) = y].</formula><p>This paper focuses on linear classifiers and neural networks with one hidden layer. For linear classifiers, f i (x) def = W i x, where W i is the i th row of the parameter matrix W ∈ R k×d .</p><p>For neural networks with one hidden layer consisting of m hidden units, the scoring function is</p><formula xml:id="formula_3">f i (x) = V i σ(W x)</formula><p>, where W ∈ R m×d and V ∈ R k×m are the parameters of the first and second layer, respectively, and σ is a non-linear activation function applied elementwise (e.g., for ReLUs, σ(z) = max(z, 0)). We will assume below that the gradients of σ are bounded: σ (z) ∈ [0, 1] for all z ∈ R; this is true for ReLUs, as well as for sigmoids (with the stronger bound σ (z) ∈ [0, 1 4 ]). Attack model. We are interested in classification in the presence of an attacker A : X → X that takes a (test) input x and returns a perturbation x. We consider attackers A that can perturb each feature x i by at most ≥ 0; formally, A(x) is required to lie in the ∞ ball B (x <ref type="bibr" target="#b49">Szegedy et al. (2014)</ref>. Define the adversarial loss with respect to A as A (x, y) = I[C(A(x)) = y].</p><formula xml:id="formula_4">) def = {x | x − x ∞ ≤ }, which is the standard constraint first proposed in</formula><p>We assume the white-box setting, where the attacker A has full knowledge of C. The optimal (untargeted) attack chooses the input that maximizes the pairwise margin of an incorrect class i over the correct class y: A opt (x) = arg max x∈B (x) max i f iy (x). For a neural network, computing A opt is a non-convex optimization problem; heuristics are typically employed, such as the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b17">(Goodfellow et al., 2015)</ref>, which perturbs x based on the gradient, or the Carlini-Wagner attack, which performs iterative optimization <ref type="bibr" target="#b8">(Carlini &amp; Wagner, 2017b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CERTIFICATE ON THE ADVERSARIAL LOSS</head><p>For ease of exposition, we first consider binary classification with classes Y = {1, 2}; the multiclass extension is discussed at the end of Section 3.3. Without loss of generality, assume the correct label for x is y = 2. Simplifying notation, let f (x) = f 1 (x) − f 2 (x) be the margin of the incorrect class over the correct class. Then A opt (x) = arg max x∈B (x) f (x) is the optimal attack, which is successful if f (A opt (x)) &gt; 0. Since f (A opt (x)) is intractable to compute, we will try to upper bound it via a tractable relaxation.</p><p>In the rest of this section, we first review a classic result in the simple case of linear networks where a tight upper bound is based on the 1 -norm of the weights (Section 3.1). We then extend this to general classifiers, in which f (A opt (x)) can be upper bounded using the maximum 1 -norm of the gradient at any point x ∈ B (x) (Section 3.2). For two-layer networks, this quantity is upper bounded by the optimal value f QP (x) of a non-convex quadratic program (QP) (Section 3.3), which in turn is upper bounded by the optimal value f SDP (x) of a semidefinite program (SDP). The SDP is convex and can be computed exactly (which is important for obtainining actual certificates). To summarize, we have the following chain of inequalities:</p><formula xml:id="formula_5">f (A(x)) ≤ f (A opt (x)) (3.2) ≤ f (x) + max x∈B (x) ∇f (x) 1 (3.3) ≤ f QP (x) (3.3) ≤ f SDP (x),<label>(1)</label></formula><p>which implies that the adversarial loss A (x) = I[f (A(x)) &gt; 0] with respect to any attacker A is upper bounded by I[f SDP (x) &gt; 0]. Note that for certain non-linearities such as ReLUs, ∇f (x) does not exist everywhere, but our analysis below holds as long as f is differentiable almost-everywhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LINEAR CLASSIFIERS</head><p>For (binary) linear classifiers, we have f (x) = (W 1 − W 2 ) x, where W 1 , W 2 ∈ R d are the weight vectors for the two classes. For any input x ∈ B (x), Hölder's inequality with x − x ∞ ≤ gives:</p><formula xml:id="formula_6">f (x) = f (x) + (W 1 − W 2 ) (x − x) ≤ f (x) + W 1 − W 2 1 .<label>(2)</label></formula><p>Note that this bound is tight, obtained by taking A opt (x) i = x i + sign(W 1i − W 2i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GENERAL CLASSIFIERS</head><p>For more general classifiers, we cannot compute f (A opt (x)) exactly, but motivated by the above, we can use the gradient to obtain a linear approximation g:</p><formula xml:id="formula_7">f (x) ≈ g(x) def = f (x) + ∇f (x) x − x ≤ f (x) + ∇f (x) 1 .<label>(3)</label></formula><p>Using this linear approximation to generate A(x) corresponds exactly to the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b17">(Goodfellow et al., 2015)</ref>. However, f is only close to g when x is very close to x, and people have observed the gradient masking phenomenon <ref type="bibr" target="#b52">(Tramèr et al., 2017;</ref><ref type="bibr" target="#b40">Papernot et al., 2016b)</ref> in several proposed defenses that train against approximations like g, such as saturating networks <ref type="bibr" target="#b33">(Nayebi &amp; Ganguli, 2017)</ref>, distillation <ref type="bibr" target="#b41">(Papernot et al., 2016c)</ref>, and adversarial training <ref type="bibr" target="#b17">(Goodfellow et al., 2015)</ref>. Specifically, defenses that try to minimize ∇f (x) 1 locally at the training points result in loss surfaces that exhibit sharp curvature near those points, essentially rendering the linear approximation g(x) meaningless. Some attacks <ref type="bibr">(Carlini &amp; Wagner, 2016;</ref><ref type="bibr" target="#b52">Tramèr et al., 2017)</ref> evade these defenses and witness a large f (A opt (x)). Figure <ref type="figure">1a</ref> provides a simple illustration.</p><p>We propose an alternative approach: use integration to obtain an exact expression for f (x) in terms of the gradients along the line between x and x:</p><formula xml:id="formula_8">f (x) = f (x) + 1 0 ∇f tx + (1 − t)x x − x dt ≤ f (x) + max x∈B (x) ∇f (x) 1 ,<label>(4)</label></formula><p>where the inequality follows from the fact that tx</p><formula xml:id="formula_9">+ (1 − t)x ∈ B (x) for all t ∈ [0, 1].</formula><p>The key difference between ( <ref type="formula" target="#formula_8">4</ref>) and ( <ref type="formula" target="#formula_7">3</ref>) is that we consider the gradients over the entire ball B (x) rather than only at x (Figure <ref type="figure">1b</ref>). However, computing the RHS of ( <ref type="formula" target="#formula_8">4</ref>) is intractable in general. For two-layer neural networks, this optimization has additional structure which we will exploit in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TWO-LAYER NEURAL NETWORKS</head><p>We now unpack the upper bound (4) for two-layer neural networks. Recall from Section 2 that</p><formula xml:id="formula_10">f (x) = f 1 (x) − f 2 (x) = v σ(W x), where v def = V 1 − V 2 ∈ R m</formula><p>is the difference in second-layer weights for the two classes. Let us try to bound the norm of the gradient ∇f (x) 1 for x ∈ B (x). If we apply the chain rule, we see that the only dependence on x is σ (W x), the activation derivatives. We now leverage our assumption that σ (z) ∈ [0, 1] m for all vectors z ∈ R m , so that we can optimize over possible activation derivatives s ∈ [0, 1] m directly independent of x (note that there is potential looseness because not all such s need be obtainable via some x ∈ B (x)). Therefore:</p><formula xml:id="formula_11">∇f (x) 1 (i) = W diag(v)σ (W x) 1 (ii) ≤ max s∈[0,1] m W diag(v)s 1 (iii) = max s∈[0,1] m ,t∈[−1,1] d t W diag(v)s,<label>(5)</label></formula><p>where (i) follows from the chain rule, (ii) uses the fact that σ has bounded derivatives σ (z) ∈ [0, 1], and (iii) follows from the identity z 1 = max t∈[−1,1] d t z. (Note that for sigmoid networks, where σ (z) ∈ [0, 1 4 ], we can strengthen the above bound by a corresponding factor of 1 4 .) Substituting the bound ( <ref type="formula" target="#formula_11">5</ref>) into (4), we obtain an upper bound on the adversarial loss that we call f QP :</p><formula xml:id="formula_12">f (A opt (x)) ≤ f (x) + max x∈B (x) ∇f (x) 1 ≤ f (x) + max s∈[0,1] m ,t∈[−1,1] d t W diag(v)s def = f QP (x).<label>(6)</label></formula><p>Unfortunately, (6) still involves a non-convex optimization problem (since W diag(v) is not necessarily negative semidefinite). In fact, it is similar to the NP-hard MAXCUT problem, which requires maximizing x Lx over x ∈ [−1, 1] d for a graph with Laplacian matrix L.</p><p>While MAXCUT is NP-hard, it can be efficiently approximated, as shown by the celebrated semidefinite programming relaxation for MAXCUT in <ref type="bibr" target="#b15">Goemans &amp; Williamson (1995)</ref>. We follow a similar approach here to obtain an upper bound on f QP (x).</p><p>First, to make our variables lie in [−1, 1] m instead of [0, 1] m , we reparametrize s to produce:</p><formula xml:id="formula_13">max s∈[−1,1] m ,t∈[−1,1] d 1 2 t W diag(v)(1 + s).<label>(7)</label></formula><p>Next pack the variables into a vector y ∈ R m+d+1 and the parameters into a matrix M :</p><formula xml:id="formula_14">y def = 1 t s M (v, W ) def =   0 0 1 W diag(v) 0 0 W diag(v) diag(v) W 1 diag(v) W 0   .<label>(8)</label></formula><p>In terms of these new objects, our objective takes the form:</p><formula xml:id="formula_15">max y∈[−1,1] (m+d+1) 1 4 y M (v, W )y = max y∈[−1,1] (m+d+1) 1 4 M (v, W ), yy .<label>(9)</label></formula><p>Note that every valid vector y ∈ [−1, +1] m+d+1 satisfies the constraints yy 0 and (yy ) jj = 1. Defining P = yy , we obtain the following convex semidefinite relaxation of our problem:</p><formula xml:id="formula_16">f QP (x) ≤ f SDP (x) def = f (x) + 4 max P 0,diag(P )≤1 M (v, W ), P .<label>(10)</label></formula><p>Note that the optimization of the semidefinite program depends only on the weights v and W and does not depend on the inputs x, so it only needs to be computed once for a model (v, W ).</p><p>Semidefinite programs can be solved with off-the-shelf optimizers, although these optimizers are somewhat slow on large instances. In Section 4 we propose a fast stochastic method for training, which only requires computing the top eigenvalue of a matrix.</p><p>Generalization to multiple classes. The preceding arguments all generalize to the pairwise margins f ij , to give:</p><formula xml:id="formula_17">f ij (A(x)) ≤ f ij SDP (x) def = f ij (x) + 4 max P 0,diag(P )≤1 M ij (V, W ), P , where<label>(11)</label></formula><formula xml:id="formula_18">M ij (V, W ) is defined as in (9) with v = V i − V j .</formula><p>The adversarial loss of any attacker, A (x, y) =</p><formula xml:id="formula_19">I[max i =y f iy (A(x)) &gt; 0]</formula><p>, can be bounded using the fact that f iy SDP (x) ≥ f iy (A(x)). In particular,</p><formula xml:id="formula_20">A (x, y) = 0 if max i =y f iy SDP (x) &lt; 0.<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING THE CERTIFICATE</head><p>In the previous section, we proposed an upper bound (12) on the loss A (x, y) of any attack A, based on the bound (11). Normal training with some classification loss cls (V, W ; x n , y n ) like hinge loss or cross-entropy will encourage the pairwise margin f ij (x) to be large in magnitude, but won't necessarily cause the second term in (11) involving M ij to be small. A natural strategy is thus to use the following regularized objective given training examples (x n , y n ), which pushes down on both terms:</p><formula xml:id="formula_21">(W , V ) = arg min W,V n cls (V, W ; x n , y n ) + i =j λ ij max P 0,diag(P )≤1 M ij (V, W ), P ,<label>(13)</label></formula><p>where λ ij &gt; 0 are the regularization hyperparameters. However, computing the gradients of the above objective involves finding the optimal solution of a semidefinite program, which is slow.</p><p>Duality to the rescue. Our computational burden is lifted by the beautiful theory of duality, which provides the following equivalence between the primal maximization problem over P , and a dual minimization problem over new variables c (see Section A for details):</p><formula xml:id="formula_22">max P 0,diag(P )≤1 M ij (V, W ), P = min c ij ∈R D D • λ + max M ij (V, W ) − diag(c ij ) + 1 max(c, 0),<label>(14)</label></formula><p>where D = (d + m + 1) and λ + max (B) is the maximum eigenvalue of B, or 0 if all eigenvalues are negative. This dual formulation allows us to introduce additional dual variables c ij ∈ R D that are optimized at the same time as the parameters V and W , resulting in an objective that can be trained efficiently using stochastic gradient methods.</p><p>The final objective. Using ( <ref type="formula" target="#formula_22">14</ref>), we end up optimizing the following training objective:</p><formula xml:id="formula_23">(W , V , c ) = arg min W,V,c n cls (V, W ; x n , y n ) + i =j λ ij • D • λ + max (M ij (V, W ) − diag(c ij )) + 1 max(c ij , 0) . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>The objective in (15) can be optimized efficiently. The most expensive operation is λ + max , which requires computing the maximum eigenvector of the matrix M ij − diag(c ij ) in order to take gradients. This can be done efficiently using standard implementations of iterative methods like Lanczos. Further implementation details (including tuning of λ ij ) are presented in Section 6.3.</p><p>Dual certificate of robustness. The dual formulation is also useful because any value of the dual is an upper bound on the optimal value of the primal. Specifically, if (W [t], V [t], c[t]) are the parameters at iteration t of training, then</p><formula xml:id="formula_25">f ij (A(x)) ≤ f (x) + 4 D • λ + max M ij (V [t], W [t]) − diag(c[t] ij ) + 1 max(c[t] ij , 0) ,<label>(16)</label></formula><p>for any attack A. As we train the network, we obtain a quick upper bound on the worst-case adversarial loss directly from the regularization loss, without having to optimize an SDP each time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OTHER UPPER BOUNDS</head><p>In Section 3, we described a function f ij SDP that yields an efficient upper bound on the adversarial loss, which we obtained using convex relaxations. One could consider other simple ways to upper bound the loss; we describe here two common ones based on the spectral and Frobenius norms.</p><formula xml:id="formula_26">Spectral bound: Note that v (σ(W x) − σ(W x)) ≤ v 2 σ(W x) − σ(W x) 2 by Cauchy- Schwarz. Moreover, since σ is contractive, σ(W x) − σ(W x) 2 ≤ W (x − x) 2 ≤ W 2 x − x 2 ≤ √ d W 2</formula><p>, where W 2 is the spectral norm (maximum singular value) of W . This yields the following upper bound that we denote by f spectral :</p><formula xml:id="formula_27">f ij (A(x)) ≤ f ij spectral (x) def = f ij (x) + √ d W 2 V i − V j 2 . (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>This measure of vulnerability to adversarial examples based on the spectral norms of the weights of each layer is considered in <ref type="bibr" target="#b49">Szegedy et al. (2014)</ref> and <ref type="bibr" target="#b12">Cisse et al. (2017)</ref>.</p><p>Frobenius bound: For ease in training, often the Frobenius norm is regularized (weight decay) instead of the spectral norm. Since W F ≥ W 2 , we get a corresponding upper bound f frobenius :</p><formula xml:id="formula_29">f ij (A(x)) ≤ f ij frobenius (x) = f ij (x) + √ d W F V i − V j 2 . (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>In Section 6, we empirically compare our proposed bound using f ij SDP to these two upper bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We evaluated our method on the MNIST dataset of handwritten digits, where the task is to classify images into one of ten classes. Our results can be summarized as follows: First, in Section 6.1, we show that our certificates of robustness are tighter than those based on simpler methods such as Frobenius and spectral bounds (Section 5), but our bounds are still too high to be meaningful for general networks. Then in Section 6.2, we show that by training on the certificates, we obtain networks with much better bounds and hence meaningful robustness. This reflects an important point: while accurately analyzing the robustness of an arbitrary network is hard, training the certificate jointly leads to a network that is robust and certifiably so. In Section 6.3, we present implementation details, design choices, and empirical observations that we made while implementing our method.</p><p>Networks. In this work, we focus on two layer networks. In all our experiments, we used neural networks with m = 500 hidden units, and TensorFlow's implementation of Adam <ref type="bibr" target="#b23">(Kingma &amp; Ba, 2014)</ref> as the optimizer; we considered networks with more hidden units, but these did not substantially improve accuracy. We experimented with both the multiclass hinge loss and cross-entropy. All hyperparameters (including the choice of loss function) were tuned based on the error of the Projected Gradient Descent (PGD) attack <ref type="bibr" target="#b30">(Madry et al., 2017</ref>) at = 0.1; we report the hyperparameter settings below. We considered the following training objectives providing 5 different networks:</p><p>1. Normal training (NT-NN). Cross-entropy loss and no explicit regularization. 2. Frobenius norm regularization (Fro-NN). Hinge loss and a regularizer λ( W F + v 2 ) with λ = 0.08. Evaluating upper bounds. Below we will consider various upper bounds on the adversarial loss Aopt (based on our method, as well as the Frobenius and spectral bounds described in Section 5). Ideally we would compare these to the ground-truth adversarial loss Aopt , but computing this exactly is difficult. Therefore, we compare upper bounds on the adversarial loss with a lower bound on Aopt instead. The loss of any attack provides a valid lower bound and we consider the strong Projected Gradient Descent (PGD) attack run against the cross-entropy loss, starting from a random point in B (x), with 5 random restarts. We observed that PGD against hinge loss did not work well, so we used cross-entropy even for attacking networks trained with the hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">QUALITY OF THE UPPER BOUND</head><p>For each of the five networks described above, we computed upper bounds on the 0-1 loss based on our certificate (which we refer to as the "SDP bound" in this section), as well as the Frobenius and spectral bounds described in Section 5. While Section 4 provides a procedure for efficiently obtaining an SDP bound as a result of training, for networks not trained with our method we need to solve an SDP at the end of training to obtain certificates. Fortunately, this only needs to be done once for every pair of classes. In our experiments, we use the modeling toolbox YALMIP <ref type="bibr" target="#b26">(Löfberg, 2004)</ref> with Sedumi <ref type="bibr" target="#b48">(Sturm, 1999)</ref> as a backend to solve the SDPs, using the dual form (14); this took roughly 10 minutes per SDP (around 8 hours in total for a given model).</p><p>In Figure <ref type="figure" target="#fig_0">2</ref>, we display average values of the different upper bounds over the 10, 000 test examples, as well as the corresponding lower bound from PGD. We find that our bound is tighter than the Frobenius and spectral bounds for all the networks considered, but its tightness relative to the PGD lower bound varies across the networks. For instance, our bound is relatively tight on Fro-NN, but unfortunately Fro-NN is not very robust against adversarial examples (the PGD attack exhibits large error). In contrast, the adversarially trained network AT-NN does appear to be robust to attacks, but our certificate, despite being much tighter than the Frobenius and spectral bounds, is far away from the PGD lower bound. The only network that is both robust and has relatively tight upper bounds is SDP-NN, which was explicitly trained to be both robust and certifiable as described in Section 4; we examine this network and the effects of training in more detail in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EVALUATING PROPOSED TRAINING OBJECTIVE.</head><p>In the previous section, we saw that the SDP bound, while being tighter than simpler upper bounds, could still be quite loose on arbitrary networks. However, optimizing against the SDP certificate seemed to make the certificate tighter. In this section, we explore the effect of different optimization objectives in more detail. First, we plot on a single axis the best upper bound (i.e., the SDP bound) and the lower bound (from PGD) on the adversarial loss obtained with each of the five training objectives discussed above. This is given in Figure <ref type="figure" target="#fig_1">3a</ref>.</p><p>Neither spectral nor Frobenius norm regularization seems to be helpful for encouraging adversarial robustness-the actual performance of those networks against the PGD attack is worse than the upper bound for SDP-NN against all attacks. This shows that the SDP certificate actually provides a useful training objective for encouraging robustness compared to other regularizers.</p><p>Separately, we can ask whether SDP-NN is robust to actual attacks. We explore the robustness of our network in Figure <ref type="figure" target="#fig_1">3b</ref>, where we plot the performance of SDP-NN against 3 attacks-the PGD attack from before, the Carlini-Wagner attack <ref type="bibr" target="#b8">(Carlini &amp; Wagner, 2017b</ref>) (another strong attack), and the weaker Fast Gradient Sign Method (FGSM) baseline. We see substantial robustness against all 3 attacks, even though our method was not explicitly trained with any of them in mind.</p><p>Next, we compare to other bounds reported in the literature. A rough ceiling is given by the network of <ref type="bibr" target="#b30">Madry et al. (2017)</ref>, which is a relatively large four-layer convolutional network adversarially trained against PGD. While this network has no accompanying certificate of robustness, it was evaluated against a number of attack strategies and had worst-case error 11% at = 0.3. Another set of numbers comes from <ref type="bibr" target="#b10">Carlini et al. (2017)</ref>, who use formal verification methods to compute A opt exactly on 10 input examples for a small (72-node) variant of the Madry et al. network. The authors reported to us that this network misclassifies 6 out of 10 examples at = 0.05 (we note that 4 out of 10 of these were misclassified to start with, but 3 of the 4 can also be flipped to a different wrong class with some &lt; 0.07).</p><p>At the value = 0.1 for which it was tuned, SDP-NN has error 16% against the PGD attack, and an upper bound of 35% error against any attack. This is substantially better than the small 72-node network, but also much worse than the full Madry et al. network. How much of the latter looseness comes from conservatism in our method, versus the fact that our network has only two layers? We can get some idea by considering the AT-NN network, which was trained similarly to Madry et al., but uses the same architecture as SDP-NN. From Figure <ref type="figure" target="#fig_1">3a</ref>, we see that the error of SDP-NN against PGD (16%) is not much worse than that of AT-NN (11%), even though AT-NN was explicitly trained against the PGD attack. This suggests that most of the gap comes from the smaller network depth, rather than from conservatism in the SDP bound. We are currently in the process of extending our approach to deeper networks, and optimistic about obtaining improved bounds with such networks.</p><p>Finally, we compare with the approach proposed in <ref type="bibr" target="#b24">Kolter &amp; Wong (2017)</ref> whose work appeared shortly after an initial version of our paper. They provide an upper bound on the adversarial loss using linear programs (LP) followed by a method to efficiently train networks to minimize this upper bound. In order to compare with SDP-NN, the authors provided us with a network with the same architecture as SDP-NN, but trained using their LP based objective. We call this network LP-NN.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows that LP-NN and SDP-NN are comparable in terms of their robustness against PGD, and the robustness guarantees that they come with.</p><p>Interestingly, the SDP and LP approaches provide vacuous bounds for networks not trained to minimize the respective upper bounds (though these networks are indeed robust). This suggests that these two approaches are comparable, but complementary. Finally, we note that in contrast to this work, the approach of Kolter &amp; Wong (2017) extends to deeper networks, which allows them to train a four-layer CNN with a provable upper bound on adversarial error of 8.4% error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">IMPLEMENTATION DETAILS</head><p>We implemented our training objective in TensorFlow, and implemented λ + max as a custom operator using SciPy's implementation of the Lanczos algorithm for fast top eigenvector computation; occasionally Lanczos fails to converge due to a small eigen-gap, in which case we back off to a full SVD. We used hinge loss as the classification loss, and decayed the learning rate in steps from 10 −3 to 10 −5 , decreasing by a factor of 10 every 30 epochs. Each gradient step involves computing top eigenvectors for 45 different matrices, one for each pair of classes (i, j). In order to speed up computation, for each update, we randomly pick i t and only compute gradients for pairs (i t , j), j = i t , requiring only 9 top eigenvector computations in each step.</p><p>For the regularization parameters λ ij , the simplest idea is to set them all equal to the same value; this leads to the unweighted regularization scheme where λ ij = λ for all pairs (i, j). We tuned λ to 0.05, which led to reasonably good bounds. However, we observed that certain pairs of classes tended to have larger margins f ij (x) than other classes, which meant that certain label pairs appeared in the maximum of (12) much more often. That led us to consider a weighted regularization scheme with λ ij = w ij λ, where w ij is the fraction of training points for which the the label i (or j) appears as the maximizing term in (12). We updated the values of these weights every 20 epochs. Figure <ref type="figure" target="#fig_2">4a</ref> compares the PGD lower bound and SDP upper bound for the unweighted and weighted networks. The weighted network is better than the unweighted network for both the lower and upper bounds.</p><p>Finally, we saw in Equation <ref type="formula" target="#formula_25">16</ref>of Section 4 that the dual variables c ij provide a quick-to-compute certificate of robustness. Figure <ref type="figure" target="#fig_2">4b</ref> shows that the certificates provided by these dual variables are very close to what we would obtain by fully optimizing the semidefinite programs. These dual certificates made it easy to track robustness across epochs of training and to tune hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this work, we proposed a method for producing certificates of robustness for neural networks, and for training against these certificates to obtain networks that are provably robust against adversaries.</p><p>Related work. In parallel and independent work, <ref type="bibr" target="#b24">Kolter &amp; Wong (2017)</ref> also provide provably robust networks against ∞ perturbations by using convex relaxations. While our approach uses a single semidefinite program to compute an upper bound on the adversarial loss, Kolter &amp; Wong (2017) use separate linear programs for every data point, and apply their method to networks of depth up to four. In theory, neither bound is strictly tighter than the other, and our experiments (Table <ref type="table" target="#tab_0">1</ref>) suggest that the two bounds are complementary. Combining the approaches seems to be a promising future direction. <ref type="bibr" target="#b21">Katz et al. (2017a)</ref> and the follow-up <ref type="bibr" target="#b10">Carlini et al. (2017)</ref> also provide certificates of robustness for neural networks against ∞ perturbations. That work uses SMT solvers, which are a tool from the formal verification literature. The SMT solver can answer the binary question "Is there an adversarial example within distance of the input x?", and is correct whenever it terminates. The main drawback of SMT and similar formal verification methods is that they are slow-they have worst-case exponential-time scaling in the size of the network; moreover, to use them during training would require a separate search for each gradient step. <ref type="bibr" target="#b20">Huang et al. (2017)</ref> use SMT solvers and are able to analyze state-of-the-art networks on MNIST, but they make various approximations such that their numbers are not true upper bounds. <ref type="bibr" target="#b2">Bastani et al. (2016)</ref> provide tractable certificates but require to be small enough to ensure that the entire ∞ ball around an input lies within the same linear region. For the networks and values of that we consider in our paper, we found that this condition did not hold. Recently, Hein &amp; Andriushchenko (2017) proposed a bound for guaranteeing robustness to p -norm perturbations, based on the maximum p p−1 -norm of the gradient in the -ball around the inputs. <ref type="bibr" target="#b19">Hein &amp; Andriushchenko (2017)</ref> show how to efficiently compute this bound for p = 2, as opposed to our work which focuses on ∞ and requires different techniques to achieve scalability. <ref type="bibr" target="#b30">Madry et al. (2017)</ref> perform adversarial training against PGD on the MNIST and CIFAR-10 datasets, obtaining networks that they suggest are "secure against first-order adversaries". However, this is based on an empirical observation that PGD is nearly-optimal among gradient-based attacks, and does not correspond to any formal robustness guarantee.</p><p>Finally, the notion of a certificate appears in the theory of convex optimization, but means something different in that context; specifically, it corresponds to a proof that a point is near the optimum of a convex function, whereas here our certificates provide upper bounds on non-convex functions. Additionally, while robust optimization <ref type="bibr" target="#b3">(Bertsimas et al., 2011)</ref> provides a tool for optimizing objectives with robustness constraints, applying it directly would involve the same intractable optimization for A opt that we deal with here.</p><p>Other approaches to verification. While they have not been explored in the context of neural networks, there are approaches in the control theory literature for verifying robustness of dynamical systems, based on Lyapunov functions <ref type="bibr" target="#b27">(Lyapunov, 1892;</ref><ref type="bibr" target="#b28">1992)</ref>. We can think of the activations in a neural network as the evolution of a time-varying dynamical system, and attempt to prove stability around a trajectory of this system <ref type="bibr" target="#b50">(Tedrake et al., 2010;</ref><ref type="bibr" target="#b51">Tobenkin et al., 2011)</ref>. Such methods typically use sum-of-squares verification <ref type="bibr" target="#b37">(Papachristodoulou &amp; Prajna, 2002;</ref><ref type="bibr">2005;</ref><ref type="bibr" target="#b42">Parrilo, 2003)</ref> and are restricted to relatively low-dimensional dynamical systems, but could plausibly scale to larger settings. Another approach is to construct families of networks that are provably robust a priori, which would remove the need to verify robustness of the learned model; to our knowledge this has not been done for any expressive model families.</p><p>Adversarial examples and secure ML. There has been a great deal of recent work on the security of ML systems; we provide only a sampling here, and refer the reader to <ref type="bibr" target="#b0">Barreno et al. (2010)</ref>, <ref type="bibr" target="#b4">Biggio et al. (2014a)</ref>, <ref type="bibr" target="#b40">Papernot et al. (2016b), and</ref><ref type="bibr" target="#b14">Gardiner &amp;</ref><ref type="bibr" target="#b14">Nagaraja (2016)</ref> for some recent surveys.</p><p>Adversarial examples for neural networks were first discovered by <ref type="bibr" target="#b49">Szegedy et al. (2014)</ref>, and since then a number of attacks and defenses have been proposed. We have already discussed gradientbased methods as well as defenses based on adversarial training. There are also other attacks based on, e.g., saliency maps <ref type="bibr" target="#b39">(Papernot et al., 2016a)</ref>, KL divergence <ref type="bibr" target="#b32">(Miyato et al., 2015)</ref>, and elastic net optimization <ref type="bibr" target="#b11">(Chen et al., 2017)</ref>; many of these attacks are collated in the cleverhans repository <ref type="bibr" target="#b16">(Goodfellow et al., 2016)</ref>. For defense, rather than making networks robust to adversaries, some work has focused on simply detecting adversarial examples. However, <ref type="bibr" target="#b7">Carlini &amp; Wagner (2017a)</ref> recently showed that essentially all known detection methods can be subverted by strong attacks.</p><p>As explained in <ref type="bibr" target="#b0">Barreno et al. (2010)</ref>, there are a number of different attack models beyond the testtime attacks considered here, based on different attacker goals and capabilities. For instance, one can consider data poisoning attacks, where an attacker modifies the training set in an effort to affect test-time performance. <ref type="bibr" target="#b34">Newsome et al. (2006</ref><ref type="bibr">), Laskov &amp; Šrndic (2014</ref><ref type="bibr" target="#b5">), and Biggio et al. (2014b)</ref> have demonstrated poisoning attacks against real-world systems.</p><p>Other types of certificates. Certificates of performance for machine learning systems are desirable in a number of settings. This includes verifying safety properties of air traffic control systems <ref type="bibr" target="#b21">(Katz et al., 2017a;</ref><ref type="bibr">b)</ref> and self-driving cars <ref type="bibr" target="#b35">(O'Kelly et al., 2016;</ref><ref type="bibr">2017)</ref>, as well as security applications such as robustness to training time attacks <ref type="bibr" target="#b47">(Steinhardt et al., 2017)</ref>. More broadly, certificates of performance are likely necessary for deploying machine learning systems in critical infrastructure such as internet packet routing <ref type="bibr" target="#b53">(Winstein &amp; Balakrishnan, 2013;</ref><ref type="bibr" target="#b46">Sivaraman et al., 2014)</ref>. In robotics, certificates of stability are routinely used both for safety verification <ref type="bibr" target="#b29">(Lygeros et al., 1999;</ref><ref type="bibr" target="#b31">Mitchell et al., 2005)</ref> and controller synthesis <ref type="bibr" target="#b1">(Başar &amp; Bernhard, 2008;</ref><ref type="bibr" target="#b50">Tedrake et al., 2010)</ref>.</p><p>In traditional verification work, Rice's theorem <ref type="bibr" target="#b43">(Rice, 1953)</ref> is a strong impossibility result essentially stating that most properties of most programs are undecidable. Similarly, we should expect that verifying robustness for arbitrary neural networks is hard. However, the results in this work suggest that it is possible to learn neural networks that are amenable to verification, in the same way that it is possible to write programs that can be formally verified. Optimistically, given expressive enough certification methods and model families, as well as strong enough specifications of robustness, one could even hope to train vector representations of natural images with strong robustness properties, thus finally closing the chapter on adversarial vulnerabilities in the visual domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility.</head><p>All code, data and experiments for this paper are available on the Codalab platform at https://worksheets.codalab.org/worksheets/ 0xa21e794020bb474d8804ec7bc0543f52/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Upper bounds on adversarial error for different networks on MNIST.</figDesc><graphic url="image-6.png" coords="7,153.29,191.91,124.73,93.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Upper bound (SDP) and lower bound (PGD) on the adversarial error for different networks. (b) Error of SDP-NN against 3 different attacks.</figDesc><graphic url="image-8.png" coords="8,112.53,82.67,193.79,109.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Weighted and unweighted regularization schemes. The network produced by weighting has a better certificate as well as lower error against the PGD attack. (b) The dual certificate of robustness (SDP dual), obtained automatically during training, is almost as good as the certificate produced by exactly solving the SDP.</figDesc><graphic url="image-10.png" coords="10,129.97,81.86,155.52,116.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the bound (LP bound) and training approach (LP-NN) of<ref type="bibr" target="#b24">Kolter &amp; Wong (2017)</ref>. Numbers are reported for = 0.1. LP-NN has a certificate (provided by the LP bound) that no attack can misclassify more than 26% of the examples.</figDesc><table><row><cell cols="4">Network PGD error SDP bound LP bound</cell></row><row><cell>SDP-NN</cell><cell>15%</cell><cell>35%</cell><cell>99%</cell></row><row><cell>LP-NN</cell><cell>22%</cell><cell>93%</cell><cell>26%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially supported by a Future of Life Institute Research Award and Open Philanthrophy Project Award. JS was supported by a Fannie &amp; John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship. We are also grateful to Guy Katz, Zico Kolter and Eric Wong for providing relevant experimental results for comparison, as well as to the anonymous reviewers for useful feedback and references.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DUALITY</head><p>In this section we justify the duality relation <ref type="bibr">(14)</ref>. Recall that the primal program is maximize M, P (19) subject to P 0, diag(P ) ≤ 1.</p><p>Rather than taking the dual directly, we first add the redundant constraint tr(P ) ≤ d + m + 1 (it is redundant because the SDP is in d + m + 1 dimensions and diag(P ) ≤ 1). This yields maximize M, P (20) subject to P 0, diag(P ) ≤ 1, tr(P ) ≤ d + m + 1.</p><p>We now form the Lagrangian for the constraints diag(P ) ≤ 1, leaving the other two constraints as-is. This yields the equivalent optimization problem maximize</p><p>subject to P 0, tr(P ) ≤ d + m + 1. Now, we apply minimax duality to swap the order of min and max; the value of ( <ref type="formula">21</ref>) is thus equal to minimize max P 0, tr(P )≤d+m+1</p><p>subject to c ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published as a conference paper at ICLR 2018</head><p>The inner maximum can be simplified as</p><p>Therefore, ( <ref type="formula">22</ref>) simplifies to</p><p>This is almost the form given in ( <ref type="formula">14</ref>), except that c is constrained to be non-negative and we have 1 c instead of 1 max(c, 0). However, note that for the λ + max term, it is always better for c to be larger; therefore, replacing c with max(c, 0) means that the optimal value of c will always be nonnegative, thus allowing us to drop the c ≥ 0 constraint and optimize c in an unconstrained manner. This finally yields the claimed duality relation ( <ref type="formula">14</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The security of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="121" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">H-infinity optimal control and related minimax design problems: a dynamic game approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Başar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bernhard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring neural net robustness with constraints</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lampropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory and applications of robust optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="501" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Poisoning behavioral malware clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Artificial Intelligence and Security (AISec)</title>
				<imprint>
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Defensive distillation is not robust to adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden voice commands</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<title level="m">Ground-truth adversarial examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">EAD: Elastic-net attacks to deep neural networks via adversarial examples</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Robust physical-world attacks on machine learning models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the security of machine learning in malware c&amp;c detection: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagaraja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1115" to="1145" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<title level="m">cleverhans v2.0.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2263" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Safety verification of deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification (CAV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01135</idno>
		<title level="m">Reluplex: An efficient SMT solver for verifying deep neural networks</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
		<title level="m">Towards proving the adversarial robustness of deep neural networks</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Laskov and N. Šrndic. Practical evasion of a learning-based classifier: A case study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2014">2016. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Adversarial examples in the physical world</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">YALMIP: A toolbox for modeling and optimization in MATLAB</title>
		<author>
			<persName><forename type="first">J</forename><surname>Löfberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CACSD</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The general problem of the stability of motion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lyapunov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1892">1892</date>
			<publisher>Kharkov Mathematical Society</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>in Russian</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The general problem of the stability of motion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lyapunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="531" to="534" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Controllers for reachability specifications for hybrid systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lygeros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="370" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Tomlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="947" to="957" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Biologically inspired protection of deep networks from adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09202</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Paragraph: Thwarting signature learning by training maliciously</title>
		<author>
			<persName><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Recent Advances in Intrusion Detection</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">APEX: Autonomous vehicle plan verification and execution</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mangharam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Computer-aided design for safe autonomous vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mangharam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the construction of lyapunov functions using the sum of squares decomposition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papachristodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prajna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Analysis of non-polynomial systems using the sum of squares decomposition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papachristodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prajna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Positive polynomials in control</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on</title>
				<imprint>
			<date type="published" when="2016">2016. 2016a</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
	<note>Security and Privacy</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wellman</surname></persName>
		</author>
		<title level="m">Towards the science of security and privacy in machine learning</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016c</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semidefinite programming relaxations for semialgebraic problems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="320" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classes of recursively enumerable sets and their decision problems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="358" to="366" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSAC Conference on Computer and Communications Security</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V D</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An experimental study of the learnability of congestion control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Winstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Certified defenses for data poisoning attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="653" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LQR-trees: Feedback motion planning via sums of squares verification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Manchester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tobenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1038" to="1052" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Invariant funnels around trajectories using sum-of-squares programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tobenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Manchester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFAC Proceedings Volumes</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">TCP ex machina: Computer-generated congestion control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Winstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
