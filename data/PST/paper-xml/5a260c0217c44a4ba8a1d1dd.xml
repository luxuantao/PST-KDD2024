<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BestConfig: Tapping the Performance Potential of Systems via Automatic Configuration Tuning *</title>
				<funder>
					<orgName type="full">Huawei BI</orgName>
				</funder>
				<funder ref="#_PURBuKj">
					<orgName type="full">State Key Development Program for Basic Research of China</orgName>
				</funder>
				<funder ref="#_gquYbSR">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_fuWEuAu">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-10">10 Oct 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
							<email>zhuyuqing@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
							<email>liujianxun@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Mengying</forename><surname>Guo</surname></persName>
							<email>guomengying@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenlong</forename><surname>Ma</surname></persName>
							<email>mawenlong@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhuoyue</forename><surname>Liu</surname></persName>
							<email>liuzhuoyue@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
							<email>songkunpeng@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Yingchun</forename><surname>Yang</surname></persName>
							<email>yingchun.yang@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences Huawei</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>SoCC &apos;17, September 24-27</addrLine>
									<postCode>2017</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BestConfig: Tapping the Performance Potential of Systems via Automatic Configuration Tuning *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-10">10 Oct 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3127479.3128605</idno>
					<idno type="arXiv">arXiv:1710.03439v1[cs.PF]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic configuration tuning</term>
					<term>ACT</term>
					<term>performance optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting leads to the best performance. While such task requires the strong expertise in both the system and the application, users commonly lack such expertise.</p><p>To help users tap the performance potential of systems, we present BestConfig, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general systems. To tune system configurations within a resource limit, we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>More and more configuration parameters are provided to users, as systems in the cloud aim to support a wide variety of use cases <ref type="bibr" target="#b40">[42]</ref>. For example, Hadoop <ref type="bibr" target="#b17">[18]</ref>, a popular big data processing system in the cloud, has more than 180 configuration parameters. The large number of configuration parameters lead to an ever-increasing complexity of configuration issues that overwhelms users, developers and administrators. This complexity can result in configuration errors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35]</ref>. It can also result in unsatisfactory performances under atypical application workloads <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">41]</ref>. In fact, configuration settings have strong impacts on the system performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b34">36]</ref>. To tap the performance potential of a system, system users need to find an appropriate configuration setting through configuration tuning.</p><p>A good configuration setting can greatly improve the system performance. For instance, changing the query_cache_type parameter of MySQL from zero to one can result in more than 11 times performance gain for an application workload, as shown in Figure <ref type="bibr">1(a)</ref>. This performance gain can be significant if the workload is recurring on a daily base-this is very likely, especially for systems like databases or web servers. Nevertheless, configuration tuning for general systems is difficult due to the following three matters.</p><p>Variety. Systems for configuration tuning can be data analytic systems like Hadoop <ref type="bibr" target="#b17">[18]</ref> and Spark <ref type="bibr" target="#b30">[31]</ref>, database systems like MySQL <ref type="bibr" target="#b27">[28]</ref>, or web servers like Tomcat <ref type="bibr" target="#b35">[37]</ref>. Various deployments for a system are possible in the cloud. Performance goals concerning users can be throughput, latency, running time, etc. Among the variety of performance goals, some need to be maximized, while some minimized. The configuration tuning process must also take the application workload into account, and there are a variety of possible workloads. Furthermore, various combinations of systems, performance goals and workloads are possible. Complexity. Given different performance goals and applied different workloads, a deployed system has different performance surfaces for a given set of configuration parameters. Different systems can have highly diverse and complex performance surfaces.  Take Figure <ref type="figure" target="#fig_1">1</ref>(a), 1(b) and 1(c) for example, MySQL has no performance surface but only two lines, while Tomcat has a bumpy performance surface and Spark has a relatively smooth performance surface. Previously, an unexpected performance surface is reported for PostgreSQL <ref type="bibr" target="#b10">[11]</ref>, which even costs system developers months of efforts to reason about the underlying interactions.</p><p>Overhead. Configuration tuning involves solving a problem with a high-dimensional parameter space, thus a large sample set is commonly needed to find a solution <ref type="bibr" target="#b15">[16]</ref>. However, collecting a large set of performance-configuration samples is impractical for configuration tuning. As no performance simulator exists for general systems, the samples can only be generated through real tests on the deployed system. Hence, configuration tuning must restrain the overhead of sample collection. Besides, the time overhead of the optimization process must also be considered.</p><p>Existing solutions do not fully address all the above challenges. Though sporadic proposals are found on automatically suggesting configuration settings for Web servers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43]</ref>, databases <ref type="bibr" target="#b10">[11]</ref> and Hadoop <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> respectively, these solutions are not generally applicable to the variety of systems in the cloud. A few statistical or machine learning models are proposed for distributed systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, but these models are not applicable to the complicated cases as shown from Figure <ref type="figure" target="#fig_1">1</ref>(a) to 1(c). Configuration tuning is related to the problem of optimizing the performance for systems with high-dimensional parameters <ref type="bibr" target="#b20">[21]</ref>, but previous research typically studies the problem based on simulations <ref type="bibr" target="#b15">[16]</ref>; the overhead aspect is rarely considered to the extent as required by configuration tuning for general systems.</p><p>In this paper, we present BestConfig-an automatic configuration tuning system that can optimize performance goals for general systems in the cloud by adjusting configuration parameters and that can recommend the best configuration setting found within a given resource limit. A typical resource limit is the number of tests allowed for configuration tuning. To address the resource limit challenge, BestConfig adopts an effective sampling method with wide space coverage and this coverage will be improved as more resources are provided. With the variety of systems and workloads, as well as the complexity of their interactions, it is impossible to build a useful performance model on a limited number of samples. Hence, BestConfig adopts a search-based optimization algorithm and exploits the general properties of performance models. To facilitate the usage with the variety of deployed systems and workloads, we design for BestConfig a software architecture that has loosely coupled but extensible components and that adopts a sample-test-optimize process in closed loop.</p><p>In the evaluation with extensive experiments, BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra <ref type="bibr" target="#b6">[7]</ref> by 63%, that of MySQL by 430%, and reduce the running time of Hive-over-Hadoop <ref type="bibr" target="#b19">[20]</ref> join job by about 50% and that of Spark join job by about 80%, as compared to the default configuration setting, simply by adjusting configuration settings.</p><p>In sum, this paper makes the following contributions.</p><p>? To the best of our knowledge, we are the first to propose and the first to implement an automatic configuration tuning system for general systems. And, our system successfully automates the configuration tuning for six systems widely deployed in the cloud. ? We propose an architecture ( ?3.4) that can be easily plugged in with general systems and any known system tests. It also enables the easy testing of other configuration tuning algorithms. ? We propose the divide-and-diverge sampling method ( ?4. <ref type="bibr" target="#b0">1)</ref> and the recursive-bound-and-search method ( ?4.2) to enable configuration tuning for general systems within a resource limit.</p><p>? We demonstrate the feasibility and the benefits of BestConfig through extensive experiments ( ?5), while refusing the possibility of using common model-based methods such as linear or smooth prediction models for general systems ( ?5.1). ? We have applied BestConfig to a real use case ( ?6) showing that, even when a cloud deployment of Tomcat has a full resource consumption rate, BestConfig can still improve the system performance solely by configuration tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we describe the background and the motivation of automatic configuration tuning for general systems. We also analyze the challenges in solving this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Configuration tuning is crucial to obtaining a good performance from a deployed system. For an application workload, a configuration setting leads to the best performance of the system, but it might not be optimal given another application workload. Take Figure 1(a) for example. Under the uniform-read workload, the value of query_cache_type is key to a good performance; but, as shown in Figure <ref type="figure" target="#fig_2">2</ref>, the query_cache_type value has no obvious relation with the system performance for a Zipfian read-write workload. In fact, the default setting generally cannot achieve the best performance of a system under all workloads. Configuration tuning is highly time-consuming and laborious. It requires the users: (1) to find the heuristics for tuning; (2) to manually change the system configuration settings and run workload tests; and, (3) to iteratively go through the second step many times till a satisfactory performance is achieved. Sometimes, the heuristics in the first step might be misguiding, as some heuristics are correct for one workload but not others; then, the latter two steps are in vain. In our experience of tuning MySQL, it has once taken five junior employees about half a year to find an appropriate configuration setting for our cloud application workloads.</p><p>Configuration tuning is even not easy for experienced developers. For example, it has been shown that, although PostgreSQL's performance under the workload of a TPC-H query is a smooth surface with regard to the configuration parameters of cache size and buffer size <ref type="bibr" target="#b10">[11]</ref>, the cache size interacts with the buffer size in a way that even takes the system developers great efforts to reason about the underlying interactions. In fact, the system performance models can be highly irregular and complicated, as demonstrated by Figure <ref type="figure" target="#fig_1">1</ref>(a) to 1(c) and Figure <ref type="figure" target="#fig_2">2</ref>. How the configuration settings can influence the system performance can hardly be anticipated by general users or expressed by simple models.</p><p>Benefits. Automatic configuration tuning can greatly benefit system users. First, a good configuration setting will improve the system performance by a large margin, while automatic configuration tuning can help users find the good configuration setting. Second, a good configuration setting is even more important for repetitive workloads, and recurring workloads are in fact a common phenomenon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Third, automatic configuration tuning enables fairer and more useful benchmarking results, if the system under test is automatically tuned for a best configuration setting before benchmarking-because the system performance is related to both the workload and the configuration setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges</head><p>Several challenges exist for automatic configuration tuning for general systems. These challenges must be addressed simultaneously.</p><p>Variety of performance goals: Users can have different performance goals for configuration tuning. For a data analytical job on Spark, the performance goal is normally to reduce the running time, while for a data-access workload on MySQL, it would be to increase the throughput. Sometimes, users can have multiple performance goals, e.g., increasing the throughput and decreasing the average latency of individual operations for MySQL. Some users would also require to improve the performance goal such as throughput but not to worsen other metrics such as memory usage. Besides, some performance goals need to be maximized, while some need to be minimized.</p><p>Variety of systems and workloads: To tune the variety of systems and workloads, we cannot build or have users build performance models for the tuning purpose as previously done for Hadoop <ref type="bibr" target="#b18">[19]</ref>. Some deployed systems are distributed, e.g., Spark and Hadoop, while some are standalone, e.g., Tomcat or one-node Hadoop. A system's performance model can be strongly influenced by the hardware and software settings of the deployment environment <ref type="bibr" target="#b44">[46]</ref>. Hence, the automatic configuration tuning system must handle the variety of deployment environments. It must enable an easy usage with the deployed systems and workloads. The heterogeneity of deployed systems and workloads can have various performance models for tuning, leading to different best configuration settings and invalidating the reuse of samples across different deployments <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b44">46]</ref>.</p><p>High-dimensional parameter space: As mentioned previously, many systems in the cloud now have a large number of configuration parameters, i.e., a high-dimensional parameter space for configuration tuning. On the one hand, it is impossible to get the complete image of the performance-configuration relations without samples covering the whole parameter space. On the other hand, collecting too many samples is too costly. The typical solutions to the optimization problem over high-dimensional spaces generally assume the abundance of samples. For example, some solve the optimization problem with around 10 parameters using about 2000 samples <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">43]</ref>. Except through simulations, it is too costly to collect such an amount of samples in practice; thus, such solutions are not applicable to the configuration tuning problem of real systems.</p><p>Limited samples: It is impractical to collect a large number of performance-configuration samples in practice. Besides, it is impossible to build a performance simulator for every system in the cloud, thus making the simulation-based sample collection infeasible. We have to collect samples through real tests against the deployed systems. Thus, methods used for configuration tuning cannot rely on a large sample set. Rather, it should produce results even on a limited number of samples. And, as the number of samples is increased, the result should be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BESTCONFIG DESIGN</head><p>BestConfig is designed to automatically find, within a given resource limit, a configuration setting that can optimize the performance of a deployed system under a specific application workload. We call the process of adjusting configuration settings as configuration tuning (or just tuning) and the system to adjust as SUT (System Under Tune).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Overview</head><p>To satisfy users' various needs on performance optimization and simultaneously simplify the optimization problem, we adopt the utility function approach to amalgamating multiple performance optimization needs into a single maximization goal. BestConfig exposes an interface for users to express their performance optimization goals ( ?3.3).</p><p>To handle the variety of deployed systems and workloads, Best-Config is designed with a flexible architecture that has loosely coupled components, as sketched in Figure <ref type="figure" target="#fig_3">3</ref>. These components are connected through data flows. The system manipulator component is an interface to interact with an SUT deployed in the target environment, while the workload generator component allows the easy plug-in of any target workload.</p><p>With limited samples, the tuning process must collect samples by careful choices. Different combinations of deployed systems and workloads can require different sampling choices. Thus, we design the BestConfig architecture with a sampler component that interacts with the system manipulator on sample collection. Besides, the performance optimization process can introduce more information on which configuration settings to sample; thus, the performance optimizer component of BestConfig is designed to interact with the sampler to pass on such knowledge. The resulting architecture of BestConfig is detailed in Section 3.4.</p><p>To address the configuration tuning problem, we must solve the two subproblems of sampling and performance optimization (PO) simultaneously. Due to the challenges of high-dimensional parameter space and limited samples, the sampling subproblem differs from the random sampling in related works. It must be solved with additional conditions as detailed in Section 3.6. The PO subproblem also faces similar conditions ( ?3.6). A feasible solution to automatic configuration tuning for general systems must address all the conditions for the two subproblems.</p><p>BestConfig exploits the sampling information when solving the PO subproblem, and vice versa. In contrast, sampling and performance optimization are generally addressed separately in related works. We combine the sampling method DDS (Divide and Diverge Sampling) with the optimization algorithm RBS (Recursive Bound and Search) as a complete solution. DDS will sample for later RBS rounds in subspaces that are not considered in early RBS rounds. In this way, the requirement on wide space coverage for sampling is better satisfied. Furthermore, RBS exploits DDS in the bounded local search to reduce the randomness and increase the effectiveness of the search. In comparison, sampling was rarely considered and exploited for the local search step in related works. We detail DDS and RBS in Section 4. In the following, we first describe the key steps for automatic configuration tuning ( ?3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Key Steps for Configuration Tuning</head><p>Figure <ref type="figure" target="#fig_3">3</ref> sketches the automatic configuration tuning process of BestConfig. The tuning process is in closed loop. It can run in as many loops as allowed by the resource limit. The resource limit is typically the number of tests that are allowed to run in the tuning process. It is provided as an input to the tuning process. Other inputs include the configuration parameter set and their lower/upper bounds (denoted as configuration constraints). The output of the process is a configuration setting with the optimal performance found within a given resource limit.</p><p>Given configuration constraints, the configuration sampler generates a number of configuration settings as allowed by the resource limit. The configuration settings are then used to update the configuration setting of the SUT. For each configuration setting, a test is run against the SUT; and, the corresponding performance results are collected. The performance results are then transformed into a scalar performance metric through the utility function.</p><p>All the sample pairs of the performance metric and the corresponding configuration setting are used by the performance optimization (PO) algorithm. The PO algorithm finds a configuration setting with the best performance. If the resource limit permits more tests and samples, the PO algorithm will record the found configuration setting and output a new set of configuration constraints for the next tuning loop. Otherwise, the tuning process ends and BestConfig outputs the configuration setting with the best performance found so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Metric by Utility Function</head><p>BestConfig optimizes towards a scalar performance metric, which has only a single value. The scalar performance metric is defined by a utility function, with user-concerned performance goals as inputs. If only one performance goal is concerned, e.g., the throughput or the latency, the utility function is the identity function, i.e., f (x) = x, where x is the performance goal. If multiple performance goals are concerned simultaneously, we can define the utility function as a weighted summation. For example, if a user wants to increase the throughput and decrease the latency, the utility function can be defined as f (x t , x l ) = x t /x l , where x t is the throughput and x l the latency. In case that the throughput must be increased and the memory usage must not exceed the threshold c m , an example utility function is f (x t , x m ) = x t ? S(c mx m -5), where x m is the memory usage and S(x) is the sigmoid function S(x) = 1 1+e -x . BestConfig allows users to define and implement their own utility functions through a Performance interface <ref type="bibr" target="#b43">[45]</ref>.</p><p>During the configuration tuning process, BestConfig maximizes the performance metric defined by the utility function. Although users can have performance goals that need to be minimized, we can easily transform the minimization problem into a maximization one, e.g., taking the inverse of the performance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Highly-Extensible Architecture</head><p>BestConfig has a highly flexible and extensible architecture. The architecture implements the flexible configuration tuning process in closed loop. It allows BestConfig to be easily used with different deployed systems and workloads, requiring only minor changes.</p><p>The main components in BestConfig's architecture include Configuration Sampler, Performance Optimizer, System Manipulator and Workload Generator. Configuration Sampler implements the scalable sampling methods. Performance Optimizer implements the scalable optimization algorithms. System Manipulator is responsible for updating the SUT's configuration setting, monitoring states of the SUT and tests, manipulating the SUT, etc. Workload Generator generates application workloads. It can be a benchmark system like YCSB <ref type="bibr" target="#b9">[10]</ref> or BigOP <ref type="bibr" target="#b9">[10]</ref> running a benchmarking workload; or, it can be a user-provided testing system regenerating the real application workloads. The system manipulator and the workload generator are the only two components interacting with the SUT.</p><p>For extensibility, the components in the architecture are loosely coupled. They only interact with each other through the data flow of configuration constraints, configuration settings and performance metrics. The configuration sampler inputs the system manipulator with sets of configuration settings to be sampled. The system manipulator inputs the performance optimizer with the samples of performance-configuration pairs. The performance optimizer adaptively inputs new configuration constraints to the configuration sampler. With such a design, BestConfig's architecture allows different scalable sampling methods and scalable PO algorithms to be plugged into the configuration tuning process. On coping with different SUTs or workloads, only the system manipulator and the workload generator need to be adapted. With the extensible architecture, BestConfig can even optimize systems emerging in the future, with only slight changes to the system manipulator and the workload generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">An Example of Extending BestConfig</head><p>With the current Java implementation of BestConfig <ref type="bibr" target="#b43">[45]</ref>, one can define a new sampling method by implementing the ConfigSampler interface. The ConfigSampler interface accepts the sample set size limit and a list of configuration constraints as inputs, and returns a list of configuration settings.</p><p>Similarly, to plug in a new PO algorithm, one can implement the Optimization interface. The implementation of the Optimization interface can accept a list of configuration settings and their corresponding performance metrics from the system manipulator. It must decide whether to continue the automatic configuration process or not, based on the given resource limit, e.g., the number of tests allowed. The best configuration setting can be output to a file, while the new configuration constraints are directly passed to the configuration sampler.</p><p>At present, extending the system manipulator requires only changing a few shell scripts that interact with the SUT and the workload generator. The workload generator is loosely coupled with other system components. Thus, it is highly convenient to integrate userprovided workload generation systems, e.g., YCSB and HiBench bundled in the BestConfig source <ref type="bibr" target="#b43">[45]</ref>. Thanks to the highly extensible architecture, we have already applied BestConfig to six systems as listed in Table <ref type="table" target="#tab_0">1</ref>. The corresponding shell scripts for these systems are provided along with the BestConfig source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Subproblems: Sampling and PO</head><p>The subproblem of sampling must handle all types of parameters, including boolean, enumeration and numeric. The resulting samples must have a wide coverage of the parameter space. To guarantee resource scalability, the sampling method must also guarantee a better coverage of the whole parameter space if users allow more tuning tests to be run. Thus, the sampling method must produce sample sets satisfying the following three conditions: (1) the set has a wide coverage over the high-dimensional space of configuration parameters; (2) the set is small enough to meet the resource limit and to reduce test costs; and, (3) the set can be scaled to have a wider coverage, if the resource limit is expanded.</p><p>The subproblem of performance optimization (PO) is to maximize the performance metric based on the given number of samples. It is required that the output configuration setting must improve the system performance than a given configuration setting, which can be the default one or one manually tuned by users. To optimize the output of a function/system, the PO algorithm must satisfy the following conditions: (1) it can find an answer even with a limited set of samples; (2) it can find a better answer if a larger set of samples is provided; and, (3) it will not be stuck in local sub-optimal areas and has the possibility to find the global optimum, given enough resources. Two categories of PO algorithms exist, i.e., model-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">39]</ref> and search-based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43]</ref>. In the design of BestConfig, we exploit the search-based methods.</p><p>We do not consider model-based PO methods for the following reasons. First, with the large number of configuration parameters, model-based methods would require a large number of samples to construct a useful model, thus violating the first condition of the PO subproblem. Second, model-based methods require the user to have a priori knowledge about the model, e.g., whether the model should be linear or quadratic, but it is mission impossible for general users to input such a priori information for each combination of SUT, deployment setting and workload. Third, model-based methods have hyper-parameters, which strongly impact how the model works; but setting these hyper-parameters is as hard as tuning the configuration setting of the SUT. Without enough samples, precise a priori knowledge or carefully-tuned hyper-parameters, model-based methods will not even work. In Section 5.1, we experiment with two model-based methods using limited samples. We demonstrate that these model-based methods hardly work in the configuration tuning problem with a resource limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DDS &amp; RBS IN COOPERATION</head><p>To address the two subproblems of automatic configuration tuning, we propose the divide-and-diverge sampling (DDS) method and the recursive bound-and-search (RBS) algorithm. Although the sampling and PO methods can work separately, DDS and RBS in cooperation enables the effective tuning process of BestConfig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DDS: Divide &amp; Diverge Sampling</head><p>Parameter space coverage. To guarantee a wide coverage over the high-dimensional parameter space, we divide the space into subspaces. Then we can randomly select one point from each subspace. Thus, each subspace is represented by one sample. In comparison to the random sampling without subspace division, it is very likely that some subspaces are not represented, especially when the dimension of the space is high. Given n parameters, we can divide the range of each parameter into k intervals and collect combinations of the intervals. There are k n combinations, thus k n subspaces and samples. This way of sampling is called gridding or stratified sampling. Thanks to subspace division, gridding guarantees a complete coverage of the whole parameter space. But it also results in a sample set with a large cardinality, which is in exponential relation to the number of parameter dimensions. Hence, it violates the second requirement of the sampling subproblem.</p><p>Resource limit. To meet the second requirement, we reduce the number of subspaces to be sampled. We observe that, the impact of an influential parameter's values on the performance can be demonstrated through comparisons of performances, disregard of other parameters' values. For example, consider the performance model of MySQL as plotted in Figure <ref type="figure" target="#fig_1">1(a)</ref>. If the value of a parameter has great impacts on the performance like quer _cache_t pe, we actually do not need to examine all combinations of the parameter's values with every other parameter's values. Instead, we need only examine each potentially outstanding value of the parameter once and compare the resulting performance with other samples. Thus, given a limited resource, we consider each interval of a parameter once, rather than making full combinations of all intervals.</p><p>After dividing parameter ranges into k intervals, we do not make a full combination of all intervals. Rather, we take a permutation of intervals for each parameter; then, we align the interval permutation for each paremeter and get k samples. For example, with two parameters X and Y divided into 6 range intervals respectively, we can take 6 samples as demonstrated in Figure <ref type="figure" target="#fig_4">4</ref>. Each range interval of X is represented exactly once by the sample set. So is that of Y . For a given sample-set size, we diverge the set of sample points the most by representing each interval of each parameter exactly once.</p><p>Scalability. The third requirement for sampling is to be scalable with regard to the resource limit, e.g., the number of tests allowed, while meeting the previous two requirements. In fact, the above divide-and-diverge sampling (DDS) method directly meets the third requirement. The value of k is set according to the resource limit, e.g., k being equal to the number of tests allowed. Increasing the number of allowed tests, the number of samples will increase equally; thus, the parameter space will be divided more finely and the space coverage will be increased.</p><p>Furthermore, as the configuration tuning process is in closed loop, multiple times of sampling can be run. For the sake of scalability and coverage, DDS do not complete restart a new sampling process by redividing the whole space. Rather, on a request of resampling, DDS reuses its initial division of the whole parameter space and samples in subspaces not considered previously, while diverging the sample points as much as possible. Heterogeneity of parameters. Although DDS considers the continuous range of a parameter, DDS can be applied to parameters of boolean or categorical types by transforming them into parameters with continuous numeric ranges. Take the boolean type for example. We can first represent the parameter value of true and false by 1 and 0 respectively. Then, we let the values be taken from the range of [0, 2), to which the DDS method can be directly applied. We can map a sampled value within ranges of [0, 1) and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2)</ref> respectively to the values of 0 or 1, which are equal to false and true respectively. Similar mappings can be carried out for categorical or enumerative parameters as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RBS: Recursive Bound &amp; Search</head><p>Consider the performance surfaces in Figure <ref type="figure" target="#fig_1">1</ref>(b) and Figure <ref type="figure" target="#fig_1">1(c)</ref>. These performance plots have parameters with numeric values and continuous ranges, thus the performance surfaces are continuous surfaces. Given a continuous surface, there is a high possibility that we find other points with similar or better performances around the point with the best performance in the sample set. Even if the continuous performance surface might not be smooth, e.g., that in Figure <ref type="figure" target="#fig_1">1(b)</ref>, or if the performance surface is continuous only when projected to certain dimensions, e.g., that in Figure <ref type="figure" target="#fig_1">1</ref>(a) when constrained to specific subspaces, the above observation still applies. Based on this observation, we design the RBS (Recursive Bound and Search) optimization algorithm.</p><p>Bound step. Given an initial sample set, RBS finds the point C 0 with the best performance. Then, it asks for another set of points sampled in the bounded space around C 0 . Based on the observation in the last paragraph, there is a high possibility that we will find another point (say C 1 ) with a better performance. We can again sample in a bounded space around C 1 . We can recursively carry out this bound-and-sample step until we find no point with a better performance in a sample set.</p><p>Here, there is a problem of how large the bounded space should be. According to the observation in Section 4.1, if a performance value shall have influential and positive impacts on the performance, it shall lead to a high performance in the sample set. For the initial sample set, parameter values other than those represented by C 0 are actually not having positive impacts as influential as C 0 on the performance, thus we should not consider them again given the limited resource. In other words, the bounded space around C 0 shall not include parameter values represented by any other points in the sample set. In addition, a high performance might be achieved by any parameter value of those around C 0 but unrepresented in the sample set.</p><p>RBS fixes the bounds of the bounded space as follows. For each parameter p i , RBS finds the largest value p f i that is represented in the sample set and that is smaller than that of C 0 . It also finds the smallest value p c i that is represented in the sample set and that is larger than that of C 0 . For the dimension represented by the parameter p i , the bounded space has the bounds of (p f i , p c i ). Figure <ref type="figure" target="#fig_4">4</ref> demonstrates this bounding mechanism of RBS. The same bounding mechanism can be carried out for every C j , j = 0, 1, ... in each bound-and-sample step.</p><p>By now, RBS has addressed the first two requirements for the PO subproblem. It finds an answer even with a limited set of samples by recursively taking the bound-and-sample step around the point C j , which is the point with the best performance in a sample set. Let each bound-and-sample step called a round. RBS can adjust the size of the sample set and the number of rounds to meet the resource limit requirement. For example, given a limit of nr tests, RBS can run in r rounds with each sample set sized n. Given more resources, i.e., a larger number of allowed tests, RBS can carry out more bound-and-sample steps to search more finely in promising bounded subspaces.</p><p>Recursion step. To address the third requirement and avoid being stuck in a sub-optimal bounded subspace, RBS restarts from the beginning of the search by having the sampler to sample in the complete parameter space, if no point with a better performance can be found in a bound-and-sample step. This measure also enables RBS to find a better answer if a larger set of samples is provided. This is made possible through searching around more promising points scattered in the huge high-dimensional parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Why Combining DDS with RBS Works</head><p>In this section, we discuss about why combing DDS with RBS works in the configuration tuning problem. The performance of a system can be represented by a measurable objective function f (x) on a parameter space D. In DDS, D is divided into orthogonal subspaces D i . We define the distribution function of objective function values as:</p><formula xml:id="formula_0">? D i ( 0 ) = m({x ? D i | f (x) ? 0 }) m(D)<label>(1)</label></formula><p>where 0 is the performance for the default configuration setting P 0 and m(?) denotes Lebesgue measure, a measure of the size of a set. For example, Lebesgue measure is area for a set of 2-dimensional points, and volume for a set of 3-dimensional points, and so on. The above equation thus represents the portion of points that have no greater performance values than P 0 in the subspace. The values of ? D i ( 0 ) fall within the range of [0, 1]. If a subspace has no points with greater performance values than 0 , it will have a zero value of ?( 0 ). When all points in a subspace have higher performances, the subspace will have ?( 0 ) evaluated to one. DDS divides the whole high-dimensional space into subspaces and then samples in each subspace. A sample can be either greater or no greater than the default performance 0 . Assume all points with no greater performances are in set s i 0 and those with greater ones are in set s i 1 , we have m(s i ) = m(s i 0 ) + m(s i 1 ). Given ? D i ( 0 ) for subspace D i , randomly sampling according to the uniform distribution will result in a ? D i ( 0 ) probability of getting points with no better performances and a 1? D i ( 0 ) probability of getting points with better performances.</p><p>Randomly sampling according to the uniform distribution, DDS will output samples with greater performances after around n = 1/(1? D i ( 0 )) samples for subspace D i . Although the exact value of n is not known, the principle underlying the uniform-random number generation guarantees that more samples will finally lead to the answer. In other words, given enough resources (i.e., samples), DDS will get a point with a greater performance than P 0 .</p><p>RBS bounds and samples around the point with the best performance in a sample set. This key step works because, if a point C j in a subspace D i is found with the best performance, it is highly probable that the subspace D i has a larger value of 1 -? D i ( 0 ) than the other subspaces, as all subspaces are sampled for the same number of times in all rounds of RBS. According to the definition of ? D i ( 0 ), subspaces with larger values of 1-? D i ( 0 ) shall have more points that lead to performances greater than 0 , as compared to subspaces with smaller values of 1? D i ( 0 ). Thus, RBS can scale down locally around C j to search again for points with better performances. As a result, the bound step of RBS, recursively used with DDS, will lead to a high probability of finding the point with the optimal performance. If the small probability event happens that the bound step runs in a subspace with a relatively small value of 1-? D i ( 0 ), the phenomenon of trapping in the local sub-optimal areas occurs. The recursion step of RBS is designed to handle this situation by sampling in the whole parameter space again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We evaluate BestConfig on six widely deployed systems, namely Hadoop <ref type="bibr" target="#b17">[18]</ref>, Hive <ref type="bibr" target="#b19">[20]</ref>, Spark <ref type="bibr" target="#b30">[31]</ref>, Cassandra <ref type="bibr" target="#b6">[7]</ref>, Tomcat <ref type="bibr" target="#b35">[37]</ref>, and MySQL <ref type="bibr" target="#b27">[28]</ref>. These systems are deployed for Huawei's applications named Cloud+ and BI. To generate workloads towards systems under tune, we embed widely adopted benchmark tools in the workload generator. We use HiBench <ref type="bibr" target="#b21">[22]</ref> for Hive+Hadoop and Spark, YCSB <ref type="bibr" target="#b9">[10]</ref> for Cassandra, SysBench <ref type="bibr" target="#b23">[24]</ref> for MySQL and JMeter <ref type="bibr" target="#b22">[23]</ref> for Tomcat. Table <ref type="table" target="#tab_0">1</ref> summarizes the evaluated systems along with the corresponding numbers of tuned parameters respectively. The detailed lists of the tuned parameters, as well as the detailed descriptions of the SUTs and the evaluated workloads, are accessible on the Web <ref type="bibr" target="#b43">[45]</ref>. Cassandra NoSQL database Java 28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MySQL</head><p>Database server C++ 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tomcat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web server Java</head><p>Our experimental setup involves multiple local clusters of servers to deploy the six systems. If not specifically mentioned, the server is equipped with two 1.6GHz processors that have two physical cores, and 32GB memory, running CentOs 6.0 and Java 1.7.0_55. To avoid interference and comply with the actual deployment, we run the system under tune, the workload generator and other components of BestConfig on different servers. Further details can be found on the Web <ref type="bibr" target="#b43">[45]</ref>.</p><p>In the evaluation, we answer five questions:</p><p>(1) Why the configuration tuning problem with a resource limit is nontrivial ( ?5.1); (2) How well BestConfig can optimize the performance of SUTs ( ?5.2); (3) How effective the cooperation of DDS and RBS is ( ?5.3); (4) How the sample set size and the number of rounds affect the tuning process ( ?5.4); (5) Whether the configuration setting found by BestConfig will maintain its advantage over the given setting in tests outside the tuning process ( ?5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Infeasibility of Model-based Methods</head><p>The difficulty of the configuration tuning problem can be demonstrated by the infeasibility of model-based methods. Common machine learning methods are model-based methods. They were previously used in optimization problems that have only a limited number of parameters. Based on the highly extensible architecture of BestConfig, we implement two PO algorithms, adopting the machine learning approach.</p><p>One is based on the COMT (Co-Training Model Tree) method <ref type="bibr" target="#b16">[17]</ref>, which assumes a linear relation between parameters and the performance. COMT divides the parameter space into subspaces and builds linear models for each subspace. Therefore, many common linear models can be taken as special cases of COMT. Besides, COMT is a semi-supervised machine learning method, which is designed and expected to work with a limited number of samples.</p><p>We train the COMT model using training sets of 100, 200 and 300 samples respectively. Each training set is randomly selected from a pool of 4000 samples, which are generated in a BestConfig tuning experiment over the Tomcat deployment described above. According to the COMT algorithm, the training not only exploits the training set, but also another set of unsampled points to reduce generalization errors. We validate the three learned models on the testing set with all samples in the sample pool. We summarize the prediction errors in Table <ref type="table" target="#tab_1">2</ref>, where Avg. err. rate is the average error rate and Max. err. rate the maximum error rate. Here, error rate is computed as the actual performance dividing the difference between the predicted performance and the actual performance. From Table <ref type="table" target="#tab_1">2</ref>, we can see that the predictions are in fact very much inaccurate. Although the first two average error rates look small, the corresponding models can make highly deviated predictions. The reason that more samples lead to worse predictions is twofold. One is because of model overfitting, and the other is due to the highly irregular performance surface of the SUT.</p><p>The other machine learning model we have tried is the GPR (Gaussian Process Regression) method <ref type="bibr" target="#b10">[11]</ref>, which assumes a differentiable performance function on parameters. It is the state-ofthe-art model-based method adopted in a recent work on database tuning <ref type="bibr" target="#b37">[39]</ref>. GPR does not predict the performance for a given point. Rather, it constructs the model based on the covariances between sample points and outputs points that are most probably to increase the performance the most, i.e., to achieve the best performance.</p><p>We experiment GPR using training sets with 100, 200 and 300 samples respectively. These sample sets are also collected from BestConfig tuning experiments over the Tomcat deployment described above. Among all the provided samples, GPR make a guess on which point would lead to the best performance (best guess).</p><p>We then run a test on the best-guess point to get the actual performance. We compare the actual performance for GPR's best guess with that for the default configuration setting (default). Besides, we compare GPR's best guess with the real best point that has the optimal performance among all the provided samples, denoted as real best. The results are given in Table <ref type="table" target="#tab_2">3</ref>. We can see that, although the prediction is improving as the number of samples increases, GPR's predictions about best points are hardly accurate. Because of the complexity of performance models, common modelbased optimization methods, e.g. COMT and GPR, do not work well in the configuration tuning problem. In essence, the assumptions of such algorithms do not hold for the SUTs. As a result, methods like COMT and GPR cannot output competitive configuration settings. Moreover, their results do not guarantee to improve as the number of samples is increased, i.e., not scalable with the resource limit; instead, their results might worsen because of overfitting, violating the conditions of the PO subproblem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic Configuration Tuning Results</head><p>Figure <ref type="figure" target="#fig_5">5</ref> presents the automatic configuration tuning results for Cassandra, MySQL, Tomcat, Spark and Hive+Hadoop using Best-Config. For the latter three systems, we ran two tuning experiments with different benchmark workloads on each system. In all experiments, we set the sample set size to be 100 and the round number to be one. As demonstrated by the results, BestConfig improves the system performances in all experiments. Even though the Hive+Hadoop system has 109 parameters to tune, BestConfig can still make a performance gain. In comparison to the other SUTs, the performance gain for Hive+Hadoop is relatively small. The underlying reason is that this SUT has almost 10 times as many configuration parameters as the other SUTs.</p><p>However, BestConfig can improve the tuning result as the size of the sample set is increased. Setting the sample set size to be 500, we carry out another experiment of Hive+Hadoop under the HiBench Join workload. The result is demonstrated in Figure <ref type="figure" target="#fig_6">6</ref>. The BestConfig setting reduces 50% running time of the Join job.</p><p>To sum up the results, BestConfig has improved the throughput of Tomcat by about 75%, that of Cassandra by about 25%, that of MySQL by about 430%, and reduced the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustments.</p><p>Invalidating manual tuning guidelines. The results produced by BestConfig have invalidated some manual tuning rules. For example, some guideline for manually tuning MySQL says that the value of thread_cache_size should never be larger than 200. However, according to BestConfig's results demonstrated in Figure <ref type="figure" target="#fig_7">7</ref>, we can set the parameter to the large value of 11987, yet we get a much better performance than following the guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cooperation of DDS and RBS</head><p>We have evaluated the DDS (divide-and-diverge sampling) method of BestConfig as compared to uniform random sampling and gridding sampling. We carry out the comparisons based on Tomcat through tuning two configuration parameters. In the experiments, we use all sampling methods with RBS. We set the initial sample set size to be 100 and the round number to be 2. Thus, after sampling for the first round, each sampling method will sample around a promising point in the second round, denoted as bound and sample. The results are plotted in Figure <ref type="figure" target="#fig_8">8</ref>.</p><p>In the initial round, the three sampling methods have sampled points with similar best performances. The effectiveness and advantage of DDS is demonstrated in the bound-and-sample round. As shown in Figure <ref type="figure" target="#fig_8">8</ref>, DDS have sampled points with the best performance as much as three times more than those of the gridding and the uniform sampling. The advantage of DDS over the gridding is due to its diverging step, while that over the uniform sampling is due to the complete coverage of the sampling space. DDS considers 100 diversities for each parameter, while the gridding considers only 10. And, there is a possibility that the uniform sampling will take samples locating at some restricted area of the space, while DDS is guaranteed to scatter samples across the space and with divergence.</p><p>We also compare DDS with LHS (Latin Hypercube Sampling) <ref type="bibr" target="#b24">[25]</ref>. LHS can produce the same sample sets as DDS in one-time sampling. However, DDS differs from LHS in that DDS remembers previously sampled subspaces and resamples towards a wider coverage of the whole parameter space. This difference leads to the DDS method's advantage of coverage and scalability over LHS. This advantage is demonstrated in Figure <ref type="figure" target="#fig_9">9</ref> through a configuration tuning process for Tomcat. In this tuning process, we set the sample  set size for each round as 100 (according to the experimental results of Table <ref type="table" target="#tab_3">4</ref>). We can see that DDS in cooperation with RBS makes progress in earlier rounds than LHS with RBS.</p><p>Furthermore, we try replacing RBS with RRS (Recursive Random Search) <ref type="bibr" target="#b41">[43]</ref>. RRS is a search-based optimization method with the exploitation and exploration steps, similar to the bound and recursion steps of RBS. However, RBS are designed with space coverage and scalability, while RRS has no such preferred properties. Hence, when we compare RBS+DDS with RRS+LHS in experiments on a Tomcat deployment, the former outperforms the latter given the same resource limit. The results are demonstrated in Figure <ref type="figure" target="#fig_10">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Varied Sample-Set Sizes &amp; Rounds</head><p>To understand how the sample set size and the number of rounds affect the optimization process, we limit the number of tests to 100 and carry out five sets of experiments with varied sample-set sizes and rounds. We vary the sample-set sizes from 5 to 100 and the number of rounds from 20 to 1 accordingly. The experiments are run upon Tomcat using the webpage navigation workload, tuning 13 parameters.</p><p>The first five rows of Table <ref type="table" target="#tab_3">4</ref> summarizes the results for each set of experiments, regarding the performance gains for the initial sampling-search round and the whole tuning process. As the size of the sample set increases, both performance gains are increasing. This fact implies that DDS is scalable. In the meantime, given a limited resource, we should set a sample-set size as large as possible, before we increase the number of rounds.  However, a larger sample-set size for one round does not necessarily always indicate a better tuning result. We have experimented with 500 samples for one round, tuning the Tomcat deployment. We find that little performance gain is obtained over the tuning process with 100 samples for one round, as demonstrated by the last row of Table <ref type="table" target="#tab_3">4</ref>. In comparison, the tuning process of Figure <ref type="figure" target="#fig_6">6</ref>, which also uses 500 samples for one round, makes much more performance gain than when using 100 samples for one round. The reason behind the difference lies in the number of parameters. In our experiments, Tomcat has only 13 parameters, while Hive+Hadoop has 109. The more parameters, the larger sample-set size is required.</p><p>Rounds matter. Despite that a large initial sample-set size is important, more rounds are necessary for better tuning results. Consider Figure <ref type="figure" target="#fig_9">9</ref> again. Because of randomness, it is not guaranteed that more rounds mean definitely better results. For example, the second round of DDS+RBS does not actually produce a better result than the first round. However, more rounds can lead to better results, e.g., the third round and the fifth round of DDS+RBS in Figure <ref type="figure" target="#fig_9">9</ref>. In fact, the third round and the fifth round are executing the recursion step of RBS. This step is key to avoiding suboptimal results. Thus, by searching in the whole parameter space again, the third and fifth rounds find configuration settings with higher performances. How much BestConfig can improve the performance of a deployed system depends on factors like SUT, deployment setting, workload and configuration parameter set. But BestConfig can usually tune a system better when given more resource and running more rounds than when given less resource and running fewer rounds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Stable Advantage of BestConfig Setting</head><p>BestConfig generally adopts short tests in the tuning process. For the long running application workloads, it might seem that the iterative testing and configuration tuning process cannot work. We argue that, even though the automatic configuration tuning process for such workloads might be long, the performance gained from the long tuning process is still worthwhile. Besides, as the benchmarking community has proved theoretically and practically <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b45">47]</ref>, the long-running application workloads can be represented by some short-running workloads.</p><p>As demonstrated by our experience with an actual BestConfig deployment, short tests can represent long-running workloads, if the tests are specified and generated properly. We have deployed BestConfig to tune the Cassandra system for Huawei's Cloud+ applications. For confidentiality reasons, we simulated the application workloads using the YCSB benchmark, which is then integrated to the workload generator of BestConfig. In the automatic configuration tuning process, the simulated workload is run for about ten minutes. We set the sample set size to be 60 and the round number as one. As output by BestConfig, a configuration setting was found with about 29% performance gain than the setting tuned by Huawei engineers. Later, we applied the configuration setting found by BestConfig to Cassandra and ran the workload for about 40 minutes. A similar long-running test is carried out with the Huawei-tuned configuration setting as well. The resulting throughput is demonstrated in Figure <ref type="figure" target="#fig_1">11</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_1">11</ref>, BestConfig's configuration setting keeps its advantage over the one set by Huawei engineers. In fact, Best-Config's setting has an average throughput of 9679 ops/sec, while Huawei's rule-based setting can only achieve one of 5933 ops/sec. Thus, BestConfig has actually made a 63% performance gain. This performance improvement is made merely through configuration adjustments.</p><p>In fact, BestConfig can usually tune a system to a better performance than the manual tuning that follows common guidelines recommended for general system deployments. The reasons are twofold. First, such manual tuning might achieve a good performance for many system deployments under many workloads, but the tuned setting is usually not the best for a specific combination of SUT, workload and deployment environment. As demonstrated in Section 5.2, some tuning guidelines might work for some situations but not the others. Second, the number of configuration parameters is too large and the interactions within a deployed system are too complex to be comprehended by human <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussion</head><p>Users ought to specify a resource limit in proportion to the number of parameters for tuning. Although BestConfig can improve the performance of a system based on a small number of samples, there is a minimum requirement on the number of samples. Consider Table <ref type="table" target="#tab_0">1</ref>. If the user allows only 5 samples in a round for tuning 13 parameters, the user is not likely to get a good result. When the number of samples exceeds that of parameters, e.g., from the second row of Table <ref type="table" target="#tab_0">1</ref>, the performance of the system gets improved obviously. Similarly, for a system with more than 100 parameters to tune, BestConfig can only improve the system performance by about 5%, if only 100 samples are provided (Figure <ref type="figure" target="#fig_5">5</ref>). However, when the sample set size is increased to 500, BestConfig can improve the system performance by about 50% (Figure <ref type="figure" target="#fig_6">6</ref>).</p><p>If we can reduce the number of parameters to tune, we can reduce the number of tuning tests and fasten the tuning process, since the number of parameters is related to the number of samples needed for tuning. A recent related work on configuration tuning has proposed to reduce the number of parameters through a popular linear-regression-based feature selection technique called Lasso <ref type="bibr" target="#b37">[39]</ref>. We consider integrating similar parameter reduction methods into BestConfig as future work.</p><p>BestConfig can generally do a great job if given the whole set of parameters. Even if the set of parameters is not complete, BestConfig can generally improve the system performance as long as the set contains some parameters affecting the system performance. In case that the set of parameters to tune are totally unrelated to an SUT's performance, BestConfig will not be able to improve the SUT's performance. Besides, BestConfig cannot improve an SUT's performance if (1) the SUT is co-deployed with other systems, which are not tuned by BestConfig and which involve a performance bottleneck affecting the SUT's performance; or, (2) the SUT with the default configuration setting is already at its optimal performance.</p><p>However, if the above situations occur, it means that the SUT's performance cannot be improved merely through configuration adjustments. Instead, other measures must be taken such as adding influential parameters for tuning, removing bottleneck components or improving the system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USE CASE: TOMCAT FOR CLOUD+</head><p>BestConfig has been deployed to tune Tomcat servers for Huawei's Cloud+ applications. The Tomcat servers run on virtual machines, which run on physical machines equipped with ARM CPUs. Each virtual machine is configured to run with 8 cores, among which four are assigned to process the network communications. Under the default configuration setting, the utilizations of the four cores serving network communications are fully loaded, while the utilizations of the other four processing cores are about 80%. With such CPU behaviors, Huawei engineers have considered that the current throughput of the system is the upper bound and no more improvement is possible. Using BestConfig and setting the overall throughput as the performance metric for optimization, we then found a configuration setting that can improve the performance of the deployment by 4%, while the CPU utilizations remain the same. Later, the stability tests demonstrate that the BestConfig setting can guarantee the performance improvement stably. The results of the stability tests are demonstrated in Table <ref type="table" target="#tab_4">5</ref>. We can observe improvements on every performance metric by using the BestConfig setting.</p><p>Thus, BestConfig has made it possible to improve the performance of a fully loaded system by simply adjusting its configuration setting. It has expanded our understanding on the deployed systems through automatic configuration tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>The closest related works for BestConfig are the classic Latin Hypercube Sampling (LHS) method <ref type="bibr" target="#b24">[25]</ref> and the recursive random search (RRS) algorithm <ref type="bibr" target="#b41">[43]</ref>. DDS differs from LHS in that DDS remembers previously sampled subspaces and resamples towards a wider coverage of the whole parameter space. This difference leads to the DDS method's advantage of coverage and scalability over LHS. RBS differs from RRS in two aspects. First, RRS requires the users to set multiple hyper-parameters, which have strong impacts on the optimization results, but setting hyper-parameters is as hard as configuration tuning. Second, RRS searches a local subspace by examining one sample after another. Such a design is efficient only if the local search is limited to a small space, but this is generally not true for high-dimensional spaces. If the hyper-parameters are carefully set as for a narrow local search, then the space not examined would be too large. Such trade-off is difficult. Besides, searching a local space by taking one sample at a time involves too much randomness; in comparison, the local search of BestConfig takes advantage of the sampling method. One more crucial difference is that BestConfig exploits RBS and DDS together.</p><p>Quite a few past works have been devoted to automatic configuration tuning for Web systems. These works either choose a small number of parameters to tune, e.g., smart hill climbing <ref type="bibr" target="#b39">[41]</ref>, or require a huge number of initial testings <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">44]</ref>, e.g., simulated annealing <ref type="bibr" target="#b38">[40]</ref> and genetic algorithms <ref type="bibr" target="#b14">[15]</ref>. Although constructing performance models might help finding appropriate configuration settings <ref type="bibr" target="#b28">[29]</ref>, a large number of samples will be required for modeling in a large configuration parameter space. But collecting a large set of samples requires to test the SUT for many times. This is a highly costly process. A related work uses reinforcement learning in the same tuning problem <ref type="bibr" target="#b5">[6]</ref>. It formulates the performance optimization process as a finite Markov decision process (MDP), which consists of a set of states and several actions for each state. The actions are increasing or decreasing the values of individual parameters. As mentioned previously, the performance functions of deployed systems can be complicated, e.g., with many sudden ups and downs on the surface. A seemingly wise step with some performance gain might result in a bad final setting, due to the local suboptimal problem.</p><p>Works on automatic configuration tuning for database systems also exist. iTuned <ref type="bibr" target="#b10">[11]</ref> assumes a smooth performance surface for the SUT so as to employ the Gaussian process regression (GPR) for automatic configuration tuning. But the assumption can be inapplicable to other SUTs, e.g., Tomcat or MySQL given some specific set of configuration parameters. The recent work of Otter-Tune <ref type="bibr" target="#b37">[39]</ref> also exploits GPR. It additionally introduces a featureselection step to reduce the number of parameters. This step reduces the complexity of the configuration tuning problem. We are examining the possibility of integrating similar feature selection methods into BestConfig to reduce the number of configuration parameters before starting the tuning process.</p><p>Automatic configuration tuning is also proposed for the Hadoop system. Starfish <ref type="bibr" target="#b18">[19]</ref> is built based upon a strong understanding of the Hadoop system and performance tuning. Thus, the method used in Starfish cannot be directly applied to other systems. Aloja <ref type="bibr" target="#b3">[4]</ref> adopts the common machine learning methods, exploiting a large database of samples. But, samples are costly to obtain. As analyzed in Sectio 3.6 and 5.1, Aloja's approach is not applicable to the configuration tuning of general systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have presented the automatic configuration tuning system Best-Config. BestConfig can automatically find, within a given resource limit, a configuration setting that can optimize the performance of a deployed system under a specific application workload. It is designed with a highly flexible and extensible architecture, the scalable sampling method DDS and the scalable performance optimization algorithm RBS. As an open-source package, BestConfig is available for developers to use and extend in order to effectively tune cloud systems. We have used BestConfig to tune the configuration settings of six widely used systems and observed the obvious performance improvements after tuning. Furthermore, tuning the Tomcat system on virtual machines in the Huawei cloud, Best-Config has actually made it possible to improve the performance of a fully loaded system by simply adjusting its configuration settings. These results highlight the importance of an automatic configuration tuning system for tapping the performance potential of systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) MySQL under uniform reads (b) Tomcat under webpage navigation workload (c) Spark under HiBench-KMeans workload</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diverging performance surfaces of MySQL, Tomcat and Spark. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance surface for MySQL under the Zipfian read-write workload. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The automatic configuration tuning process and the major components of BestConfig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of running DDS and RBS for a 2D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: BestConfig's optimization capability with regard to the default configuration setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: BestConfig reduces 50% running time of HiBench-Join on Hive+Hadoop within 500 tests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Throughputs for varied thread_cache_size of MySQL, invalidating the manual tuning guideline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sampling method comparisons: DDS outperforms gridding and uniform in the latter round.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: DDS+RBS makes progress in earlier rounds than LHS+RBS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: RBS+DDS vs. RRS+LHS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The evaluated systems and parameters.</figDesc><table><row><cell>Software</cell><cell>Description</cell><cell>Language</cell><cell># Parameters Tuned</cell></row><row><cell>Spark</cell><cell cols="2">Distributed computing Scala</cell><cell>30</cell></row><row><cell>Hadoop Hive</cell><cell cols="2">Distributed computing Java Data analytics Java</cell><cell>109 (in all)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Linear-model based performance predictions.</figDesc><table><row><cell>Sample set size</cell><cell cols="2">Avg. err. rate Max. err. rate</cell></row><row><cell>100</cell><cell>14%</cell><cell>240%</cell></row><row><cell>200</cell><cell>15%</cell><cell>1498%</cell></row><row><cell>300</cell><cell>138%</cell><cell>271510%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>GPR-based predictions on best points.</figDesc><table><row><cell>Sample set size</cell><cell>Bst. guess/dflt.</cell><cell>Bst. guess/rl. bst.</cell></row><row><cell>100</cell><cell>93%</cell><cell>56%</cell></row><row><cell>200</cell><cell>104%</cell><cell>63%</cell></row><row><cell>300</cell><cell>121%</cell><cell>58%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance gains on varied sample-set sizes and rounds.</figDesc><table><row><cell cols="3">Exps (SetSize*rounds) Initial gain Best gain</cell></row><row><cell>5*20</cell><cell>0%</cell><cell>5%</cell></row><row><cell>10*10</cell><cell>0%</cell><cell>14%</cell></row><row><cell>20*5</cell><cell>26%</cell><cell>29%</cell></row><row><cell>50*2</cell><cell>39%</cell><cell>39%</cell></row><row><cell>100*1</cell><cell>42%</cell><cell>42%</cell></row><row><cell>500*1</cell><cell>43%</cell><cell>43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>BestConfig improving performances of a fullyloaded Tomcat.</figDesc><table><row><cell>Metrics</cell><cell cols="3">Default BestConfig Improvement</cell></row><row><cell cols="2">Txns/seconds 978</cell><cell>1018</cell><cell>4.07% ?</cell></row><row><cell cols="2">Hits/seconds 3235</cell><cell>3620</cell><cell>11.91% ?</cell></row><row><cell>Passed Txns</cell><cell cols="2">3184598 3381644</cell><cell>6.19% ?</cell></row><row><cell>Failed Txns</cell><cell>165</cell><cell>144</cell><cell>12.73% ?</cell></row><row><cell>Errors</cell><cell>37</cell><cell>34</cell><cell>8.11% ?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank our shepherd, <rs type="person">Ennan Zhai</rs>, and the anonymous reviewers for their constructive comments and inputs to improve our paper. We would like to thank the <rs type="person">Huawei Cloud+</rs> and the <rs type="funder">Huawei BI</rs> teams in helping us verify BestConfig towards their online applications. This work is in part supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61303054</rs> and No. <rs type="grantNumber">61420106013</rs>), the <rs type="funder">State Key Development Program for Basic Research of China</rs> (Grant No. <rs type="grantNumber">2014CB340402</rs>) and gifts from <rs type="person">Huawei</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gquYbSR">
					<idno type="grant-number">61303054</idno>
				</org>
				<org type="funding" xml:id="_PURBuKj">
					<idno type="grant-number">61420106013</idno>
				</org>
				<org type="funding" xml:id="_fuWEuAu">
					<idno type="grant-number">2014CB340402</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Re-optimizing data-parallel computing</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association</title>
		<meeting>the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="21" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ras</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Christopher Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>James Gebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parry</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Lester Plishker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Webb</forename><surname>Williams</surname></persName>
		</author>
		<idno>. UCB/EECS-2006-183</idno>
		<title level="m">The landscape of parallel computing research: A view from Berkeley</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brodie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ceri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><surname>Jagadish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Asilomar report on database research. ACM Sigmod record</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aloja-ml: A framework for automating characterization and knowledge discovery in hadoop deployments</title>
		<author>
			<persName><forename type="first">Llu?s</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Berral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Call</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daron</forename><surname>Reinauer</surname></persName>
		</author>
		<author>
			<persName><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Why Gmail went down: Google misconfigured load balancing servers</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Brodkin</surname></persName>
		</author>
		<ptr target="http://arstechnica.com/information-technology/2012/12/why-gmail-went-down-google-misconfigured-chromes-sync-server/" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A reinforcement learning approach to online web systems auto-configuration</title>
		<author>
			<persName><forename type="first">Xiangping</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Zhong</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
	<note>In Distributed Computing Systems, 2009. ICDCS&apos;09. 29th IEEE</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Apache Cassandra Website</title>
		<author>
			<persName><forename type="first">Cassandra</forename><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://cassandra.apache.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experience transfer for the configuration tuning in large-scale computing systems</title>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="388" to="401" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cloudera</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="http://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_ig_yarn_tuning.html" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st SoCC</title>
		<meeting>the 1st SoCC</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Raghu Ramakrishnan, and Russell Sears</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tuning database configuration parameters with iTuned</title>
		<author>
			<persName><forename type="first">Songyun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vamsidhar</forename><surname>Thummala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivnath</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1246" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A measure of transaction processing power</title>
		<author>
			<persName><surname>Anon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Datamation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="112" to="118" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jockey: guaranteed job latency in data parallel clusters</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Andrew D Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM european conference on Computer Systems</title>
		<meeting>the 7th ACM european conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Configuring Distributed Computations Using Response Surfaces</title>
		<author>
			<persName><forename type="first">Efe</forename><surname>Adem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emin</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robbert</forename><surname>G?n Sirer</surname></persName>
		</author>
		<author>
			<persName><surname>Van Renesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Middleware Conference</title>
		<meeting>the 16th Annual Middleware Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="235" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Genetic algorithms and machine learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="99" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated network management and configuration using Probabilistic Trans-Algorithmic Search</title>
		<author>
			<persName><forename type="first">Gurhan</forename><surname>Bilal Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName><surname>Yuksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective and efficient microprocessor design space exploration using unlabeled design configurations</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1671</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Apache Hadoop Website</title>
		<author>
			<persName><surname>Hadoop</surname></persName>
		</author>
		<author>
			<persName><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No one (cluster) size fits all: automatic cluster sizing for data-intensive analytics</title>
		<author>
			<persName><forename type="first">Herodotos</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivnath</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Symposium on Cloud Computing</title>
		<meeting>the 2nd ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Apache Hive Website</title>
		<author>
			<persName><surname>Hive</surname></persName>
		</author>
		<author>
			<persName><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://hive.apache.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated algorithm configuration and parameter tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous search</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The hibench benchmark suite: Characterization of the mapreduce-based data analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDEW 2010</title>
		<meeting>of ICDEW 2010</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Jmeter</surname></persName>
		</author>
		<author>
			<persName><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://jmeter.apache.org" />
		<title level="m">Apache JMeter T M</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SysBench: System evaluation benchmark</title>
		<author>
			<persName><surname>Launchpad</surname></persName>
		</author>
		<author>
			<persName><surname>Net</surname></persName>
		</author>
		<ptr target="http://github.com/nuodb/sysbench" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparison of three methods for selecting values of input variables in the analysis of output from a computer code</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Michael D Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName><surname>Conover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Aurimas</forename><surname>Mikalauskas</surname></persName>
		</author>
		<ptr target="http://www.speedemy.com/17-key-mysql-config-file-settings-mysql-5-7-proof/" />
		<title level="m">17 KEY MYSQL CONFIG FILE SETTINGS (MYSQL 5.7 PROOF)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft: Misconfigured Network Device Led to Azure Outage</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://www.datacenterknowledge.com/archives/2012/07/28/microsoft-misconfigured-network-device-caused-azure-outage/" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Mysql</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="http://www.mysql.com/" />
		<title level="m">MySQL Website</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing system configurations quickly by guessing at the performance</title>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Osogami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sei</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Kim</forename><surname>Shanley</surname></persName>
		</author>
		<title level="m">History and Overview of the TPC</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Apache Spark Website</title>
		<author>
			<persName><surname>Spark</surname></persName>
		</author>
		<author>
			<persName><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://spark.apache.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation (SPEC)</title>
		<author>
			<persName><surname>Spec</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://www.spec.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holistic configuration management at Facebook</title>
		<author>
			<persName><forename type="first">Chunqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thawan</forename><surname>Kooburat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Venkatachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Chander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Dowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles</title>
		<meeting>the 25th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="328" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Amazon: The Cloud Crash Reveals Your Importance</title>
		<author>
			<persName><forename type="first">Keir</forename><surname>Thomas</surname></persName>
		</author>
		<ptr target="http://www.pcworld.com/article/226033/thanks_amazon_for_making_possible_much_of_the_internet.html" />
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Al&apos;s Cassandra 2.1 tuning guide</title>
		<ptr target="https://tobert.github.io/pages/als-cassandra-21-tuning-guide.html" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tomcat</surname></persName>
		</author>
		<author>
			<persName><surname>Apache</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://tomcat.apache.org/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Apache Tomcat Website</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><surname>Tpc</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="http://www.tpc.org/" />
		<title level="m">Transaction Processing Performance Council (TPC)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic Database Management System Tuning Through Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1009" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simulated annealing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emile</forename><forename type="middle">Hl</forename><surname>Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName><surname>Aarts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulated Annealing: Theory and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A smart hill-climbing algorithm for application server configuration</title>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Raghavachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on World Wide Web</title>
		<meeting>the 13th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software</title>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuepeng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rukma</forename><surname>Talwadker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="307" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A recursive random search algorithm for large-scale network parameter configuration</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivkumar</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic Configuration of Internet Services</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thu</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems</title>
		<meeting>the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="219" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://github.com/zhuyuqing/bestconf" />
		<title level="m">Better Configurations for Large-Scale Systems (BestConf)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ACTS in Need: Automatic Configuration Tuning with Scalability Guarantees</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th SIGOPS Asia-Pacific Workshop on Systems</title>
		<meeting>the 8th SIGOPS Asia-Pacific Workshop on Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bigop: Generating comprehensive big data workloads as a benchmarking framework</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghunath</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
