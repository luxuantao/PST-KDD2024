<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Software-Defined Address Mapping: A Case on 3D Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
							<email>jlzhang@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Swift</surname></persName>
							<email>swift@cs.wisc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison Madison</orgName>
								<address>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Li</surname></persName>
							<email>janeli@seas.upenn.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Software-Defined Address Mapping: A Case on 3D Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503222.3507774</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Software defined memory</term>
					<term>3D memory</term>
					<term>Address mapping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D-stacking memory such as High-Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) provides orders of magnitude more bandwidth and significantly increased channel-level parallelism (CLP) due to its new parallel memory architecture. However, it is challenging to fully exploit the abundant CLP for performance as the bandwidth utilization is highly dependent on address mapping in the memory controller. Unfortunately, CLP is very sensitive to a program's data access pattern, which is not made available to OS/hardware by existing mechanisms.</p><p>In this work, we address these challenges with software-defined address mapping (SDAM) that, for the first time, enables user program to obtain a direct control of the low-level memory hardware in a more intelligent and fine-grained manner. In particular, we develop new mechanisms that can effectively communicate a program's data access properties to the OS and hardware and to use it to control data placement in hardware. To guarantee correctness and reduce overhead in storage and performance, we extend Linux kernel and C-language memory allocators to support multiple address mappings. For advanced system optimization, we develop machine learning methods that can automatically identify access patterns of major variables in a program and cluster these with similar access patterns to reduce the overhead for SDAM. We demonstrate the benefits of our design on real system prototype, comprising (1) a RISC-V processor, near memory accelerators and HBM modules using Xilinx FPGA platform, and (2) modified Linux and glibc. Our evaluation on standard CPU benchmarks and data-intensive benchmarks (for both CPU and accelerators) demonstrates 1.41×, 1.84× speedup on CPU and 2.58× on near memory accelerators in our system with SDAM compared to a baseline system that uses a fixed address mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Software and its engineering → Main memory; • Computer systems organization → Reconfigurable computing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>DDR-based main memory is a critical performance bottleneck in today's data centers as memory bandwidth cannot keep pace with the explosion in data. The problem becomes more pronounced with new trends in processor technology and applications: 1) Increasing the number of general-purpose cores and accelerators integrated into a single chip raises competition for access to DRAM and demands higher bandwidth from main memory. 2) Emerging applications such as data analytics and graph processing are data intensive and increasingly becoming memory bound. As such, 3D-memory is a promising alternative to tackle these problems by providing more bandwidth. The two realizations of 3D-memory are High Bandwidth Memory (HBM) <ref type="bibr" target="#b22">[23]</ref> and Hybrid Memory Cube (HMC) <ref type="bibr" target="#b36">[37]</ref>. These memories exploit through-silicon via (TSV)-based stacking technology and organizes memory banks into multiple independentlyoperated channels, offering &gt; 10× more channel-level parallelism (CLP) than DDR memory. Hence, they can achieve &gt; 10× higher peak hardware bandwidth. Such CLP is expected to grow more for future-generation 3D memory devices <ref type="bibr" target="#b35">[36]</ref>.</p><p>One of the key challenge of using 3D memory is how to fully exploit the CLP offered by the new parallel architecture. To increase CLP and hence bandwidth utilization, it is important to spread concurrent memory requests across as many channels as possible. This can be controlled via the address mapping in the memory controller that transforms the flat 1D of physical addresses into an internal 3D hierarchical structure of channels, banks and rows. However, in comparison to DDR DRAM, 3D memory has many more channels but smaller row buffer size. Thus, to better exploit the CLP in 3D memory, consecutive memory access (cache-line size granularity) should be sent to distinct channels via fine-grained control of data placement. The mapping has to vary with different data access patterns to fully exploit its potential: when program strides through data, the generated memory references may cause contention on one or few memory channels. Thus, we need a mechanism that can effectively predict/capture the access patterns of different data structure in a program and leverage such knowledge to support multiple address mappings for data structures with different data access patterns.</p><p>Many existing works address the data placement issue using hardware-only mechanisms. These methods rely on dedicated hardware to control data placement based on a single address mapping <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> and apply one global address mapping to all physical addresses after address translation or a very large address range. To predict access patterns, they use physical addresses that only provide a localized view (page) of accesses and cannot capture different access patterns to different data structures in virtual memory. Hence, they often result in limited performance gain.</p><p>There have also been a number of software-only methods proposed to learn the knowledge of a program's access pattern in memory system optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. For instance, page coloring places virtual pages that may be accessed concurrently in physical frames associated with different cache blocks to reduce conflict misses. However, page coloring and its variations have a common problem: their control on data placement via virtual-to-physical address translation is coarse-grained page granularity. However, in 3D memory, we need fine-grained control at cacheline granularity in order to distribute concurrent requests across as many channels as possible to fully exploit its CLP. Simply reducing page size to cache-line size may incur prohibitively high paging overheads.</p><p>To summarize, the key insights are: 1) the hardware-only methods can achieve fine-grained control of data placement in physical memory but cannot support multiple access patterns; 2) softwareonly methods can support multiple access patterns but can only provide control mechanism at coarse-grained granularity due to the limitation of paging. To better exploit CLP, we need a cooperative hardware-software mechanism that can combine the best of both world which motivates our software-defined address mapping.</p><p>In this work, we propose Software-Defined Address Mapping (SDAM) -a collaborative software/hardware technique -to address these challenges. To the best of our knowledge, SDAM is the first to provide new system mechanisms that enable user program to obtain a direct control of the low-level memory hardware in a more intelligent and fine-grained manner. We apply SDAM to the case of 3D-stacking memory to better exploit its CLP by allowing the program to specify desired mappings for different data structures according to their access patterns. The proposed mechanisms include a runtime memory allocator that passes the address mapping information to the kernel and additional architectural support in the memory controller hardware for selecting the desired mapping. More specifically, the memory allocator can satisfy memory allocation call from pools of preconfigured physical memory chunks, or reconfigure free memory into the desired mapping. On a request to memory, the memory controller uses the desire mapping to calculate the hardware address of the data including which channel to use for access. The only added new hardware to the memory controller are the address mapping unit (AMU) that maps physical address to the channels/banks/rows of internal memory structure and the chunk mapping table (CMT) that is a small SRAM (67 KB) to store the AMU configuration. To guarantee the data in one chunk is associated with only one address mapping, it only requires to add two simple constraints to the allocation of virtual pages and physical frames, complying with allocation rules in Linux kernel.</p><p>To absolve programmers of responsibility for finding access pattern information to select an address mapping, we develop machine learning methods that can automatically learn the access patterns of the variables that contribute most to the external memory access and cluster these access patterns into a reduced set. The reduced number of data access patterns makes it easier for the OS to control and manage address mappings. For that, we propose chunk-based address mapping. We allocate coarse-grained regions and store a mapping for the region, then allocate space for individual data structures within the region. To expand or shrink, we allocate/free memory to/from the region in the unit of chunks. The granularity of chunk can vary, independent of page size and is typically larger than a page. The chunk size has to be selected carefully to meet a set of constraints to ensure correctness with low storage and performance overhead.</p><p>We demonstrate the benefits of proposed research methodology on real hardware comprising an FPGA-based RISC-V processor and accelerators integrated with HBM. It also includes a bootable Linux for a realistic software flow, allowing us to change Linux Kernel functions to evaluate the proposed cross-stack address mapping management. In detail, we implement a 4-core RISC-V CPU with near memory accelerators on VCU37P FPGA platforms equipped with 8GB HBM2 modules. On top of the standard RISC-V architecture, we add the proposed CMT and AMU to implement the SDAM. The software modification is based on Linux kernel 4.15 and glibc 2.26. The evaluation covers a wide range of workloads, including standard benchmarks i.e. SPEC2006, PARSEC and emerging data-intensive benchmarks on both CPU and accelerators.</p><p>The primary contributions of the paper are:</p><p>• We propose the software defined address mapping (SDAM) that employs a coarse-grained chunk-based address mapping management while achieving fine-grained data placement in hardware to fully exploit the CLP in 3D memory. The proposed method leverages knowledge of variable-level data access information in a program and associates meta-data containing address mapping information with contiguous physical memory addresses that have the same data access pattern and thus address mapping. • We present the necessary architectural support and system software modification to enable SDAM for both CPU-only systems and systems comprising both CPU and near memory accelerators. The modifications to existing processor and system software are minimal. • We develop machine learning methods for advanced system optimization. They can automatically identify access patterns and cluster variables that have similar access pattern to further reduce the overhead of SDAM. • We demonstrate the effectiveness of SDAM on an FPGA-based full-system prototyping platform with full OS stack.The evaluation results show that the system implemented with SDAM achieves 1.41× speedup on standard CPU benchmarks including SPEC2006, PARSEC and 1.84× speedup on emerging dataintensive benchmarks compared to a baseline system that uses a fixed address mapping. Moreover, it achieves 2.58× speedup for near memory accelerator on the evaluated data-intensive benchmarks, compared to the case without SDAM, indicating future systems with accelerators is likely to benefit more from the proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 CLP in 3D-Stacking Memory</head><p>3D-stacking memories provide significantly higher per socket peak memory bandwidth (960 GB/s) than the DDR-SDRAM family (102.4 GB/s) by employing new parallel memory architectures that vertically stack multiple DRAM dies using through-silicon-via (TSV) technology. The two realizations of 3D memory are JEDEC High Bandwidth Memory (HBM) <ref type="bibr" target="#b22">[23]</ref> and Micron's Hybrid Memory Cube (HMC) <ref type="bibr" target="#b36">[37]</ref>. A HBM stack comprises multiple DRAM layers organized hierarchically, where each layer is further divided into two channels, and each memory channel consists of multiple memory banks. Since TSV technology provides high pin density (1024 per HBM stack), 3D-stacking memories have significantly more independent memory channels and can serve more than 10× concurrent memory requests in parallel that traditional DDR-SDRAM-based memory.</p><p>In general, memory architecture (DDR and 3D memory) offers three levels of parallelism: channel-level parallelism (CLP), banklevel parallelism (BLP) and row-level parallelism (RLP) to exploit parallelism across channels, banks and within a row. Among them, CLP <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref><ref type="bibr" target="#b24">(25)</ref><ref type="bibr" target="#b25">(26)</ref><ref type="bibr" target="#b26">(27)</ref><ref type="bibr" target="#b27">(28)</ref><ref type="bibr" target="#b28">(29)</ref><ref type="bibr" target="#b29">(30)</ref><ref type="bibr" target="#b30">(31)</ref><ref type="bibr" target="#b31">(32)</ref> is the highest degree of parallelism for 3D-Memory and thus is most performance-critical as memory accesses to independent channels can be served fully in parallel. In comparison to CLP, BLP <ref type="bibr" target="#b7">(8)</ref> and RLP (4) have limited impact on performance since memory accesses to different banks in a channel or to different columns in a row within a bank has to be serialized due to contention for shared resources in the same memory channel.</p><p>In DDR-based memory, most prior efforts focus on utilization of RLP and BLP as the memory subsystem usually has large rows (high RLP) but few memory channels (low CLP). For instance, a modern CPU can have up to 4 DDR4 channels per socket and a row buffer size of 2 KB per channel <ref type="bibr" target="#b1">[2]</ref>. However, one socket can integrate up to 4 HBM stacks that have a row size of 256B and each socket provides 32 independent channels <ref type="bibr" target="#b22">[23]</ref>. Therefore, 3D memory offers 8× more CLP than DDR memory but with 8× smaller row.</p><p>Due to the key difference in architectural organization between 3D memory and traditional DDR-DRAM, it is important to maximally utilize the CLP in 3D memory when serving multiple memory requests. To better illustrate the importance of utilizing CLP for performance optimization, Fig. <ref type="figure" target="#fig_0">1</ref> shows that the throughput of HBM linearly with increasing the number of utilized channels to exploit CLP and sub-linearly increasing with the number of utilized columns in a row to exploit RLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DRAM Address Mapping</head><p>Data placement in physical memory hierarchy across channels/banks /rows is achieved via the two steps: (1) virtual to physical address translation (VA-to-PA translation) and (2) physical to hardware address mapping (PA-to-HA mapping). The VA-to-PA translation maps a large per-process virtual address space to a limited-size physical address space determined by the available hardware resources and shared by all processes. Such translation is managed by the OS's virtual memory system.</p><p>The PA-to-HA mapping 1 further transforms the flat 1D of physical addresses into an internal 3D hierarchical structure of channels, banks, and rows. The PA-to-HA mapping is typically managed by hardware (memory controller). As an illustrative example, Fig. <ref type="figure" target="#fig_1">2</ref> 1 PA-to-HA mapping is also refereed to as address mapping in the paper  compares two different address mappings: (1) a 32-bit physical address is split into a 18-bit row address, a 4-bit bank address, a 4-bit channel address, and a 6-bit column address; (2) the 18-bit row address is splitted into two parts and the 3 LSBs are put between the channel address and the column address. The address mapping determines the CLP utilization and is highly dependent on the access patterns. In the example, when accessing data with a stride of 1 (streaming), mapping 1 allows consecutive memory access to be evenly served by different channels and to exploit the CLP better than mapping 2. With the same address mapping, accessing data with a stride of 16 leads to severe channel contention, as only 1 out of 16 memory channels is used. Similarly, the mapping 2 can better exploit the CLP for data access with a stride of 16, but causes channel conflict for streaming access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Address Mapping Mechanisms</head><p>Hardware-only methods: The first class of methods to control data placement across channels/banks/rows to improve bandwidth utilization are hardware-based via the direct control of PA-to-HA address mapping.</p><p>Most commercial computer systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref> use a boot-time configured PA-to-HA mapping. During the device discovery phase, the BIOS determines the address mapping based on the DIMM configuration. Memory accesses to consecutive cache blocks are uniformly distributed to different memory channels (channel interleaving), resulting in higher throughput for streaming data access. However, such fixed and global address mapping may lead to imbalanced channel utilization and performance degradation for non-streaming access patterns.</p><p>To illustrate these problems, we construct a synthetic workload that reads data with different strides from HBM using Xilinx VU37P As in Fig. <ref type="figure" target="#fig_2">3</ref>, using the default HBM address mapping defined in the Xilinx HBM controller IP <ref type="bibr" target="#b22">[23]</ref>, the throughput drops sharply by 20× when increasing the stride from 1 to 16 in the unit of cache-line size.</p><p>That is because only a subset of the memory channels are utilized.</p><p>In the worst case, e.g., a stride of 32, only one channel is utilized.</p><p>To improve channel utilization (and CLP), we need to make the PA-to-HA mapping adapt to the different data access patterns while keeping fine-grained control to distribute consecutive memory access to distinct channels.</p><p>Software-only methods: The software-only approaches use OS support <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> to more intelligently place data in physical memory by changing the VA-to-PA translation in virtual memory (VM) according to the access patterns of data structures. These works attempt to find variables/data structures in a program that may be accessed concurrently and use the information to direct the operating system's VM page mapping strategy to place them to physical pages that do not contend for the same cache line, thereby reducing conflict misses. By effectively leveraging high-level program semantic information, these methods can better predict the program's behavior in data access and achieve higher and more deterministic cache performance. However, these methods assume fixed hardware and allow controlling allocation to place data in hardware (caches) at coarse-grained page granularity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>. However, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, to exploit CLP for two different access patterns (stride-1 and stride-16), the page-based data placement becomes insufficient. We need more fine-grained control at cacheline granularity to distribute concurrent memory requests across as many channels as possible to fully exploit its CLP.</p><p>To summarize, the key limitations are: 1) the hardware-only methods can achieve fine-grained control of data placement in physical memory but cannot support multiple access patterns; 2) software-only methods can support multiple access patterns of a program but only provide control mechanism at coarse-grained granularity due to the limitation of paging. To better exploit CLP, we need a cooperative hardware-software mechanism that can combine the best of both worlds, which motivates our softwaredefined address mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATING EXPERIMENTS</head><p>In this section, we present three experimental studies to motivate our proposed research approach. We use the same experimental setup as that in Section 7 using a Xilinx VU37P FPGA <ref type="bibr" target="#b45">[46]</ref> with two HBM2 stacks which have 32 channels in total. Experiment 1: In Fig. <ref type="figure" target="#fig_2">3</ref>(a), we demonstrate the relationship between data access patterns and memory throughput. We construct  three synthetic workloads, which read data from memory with different strides. We apply the same method as prior hardware-only approach <ref type="bibr" target="#b0">[1]</ref> to use bit flip rate as a metric to select desired address mapping. We pick bits of physical addresses that vary the most to be mapped onto the channels to distribute accesses across the most number of channels. Fig. <ref type="figure" target="#fig_2">3</ref>(b) shows the normalized bit flip rate in the stream of memory access. With increasing stride, the flip rate peak moves to the left, indicating the optimal address mapping for different access patterns varies.</p><p>Observation 1: The address mapping needs to adapt to different memory access patterns to best exploit the CLP. Experiment 2: The second experiment shows the impact of mixing multiple data access patterns. Fig. <ref type="figure" target="#fig_3">4</ref> shows the throughput of the four strides from experiment 1 with the globally best mapping (maximum bit flip rate) and with a separate mapping for each stride. In case-1, we apply the same method as experiment 1 to select the optimal address mapping based on overall bit flip rate when running the workload mix comprising different concurrent memory accesses. Alternatively, in case-2, we independently choose the optimal address mapping for each access pattern and measure the aggregated throughput. The results show that a global address mapping cannot deliver the best performance as compared with the case of independently selecting address mapping for each individual access pattern.</p><p>Observation 2: It is important to capture the detailed program's access behaviors i.e., the diversity of the access patterns to guide address mapping selection for higher CLP. Experiment 3: In the third experiment, we profile SPEC2006 and PARSEC workloads to identify the number of unique data structures that may have different access patterns. The detailed profiling process is described in Section 5. As illustrated in Table <ref type="table" target="#tab_0">1</ref>, we run 19 applications to collect variable-level statistics of SPEC2006 and PARSEC. As defined in <ref type="bibr" target="#b23">[24]</ref>, a variable is the reference symbol in the program for a piece of allocated memory and is the granularity of memory management from programmer's view. These applications show highly diverse distributions in the number of variables (ranging from 3 to 49k). While the programs have a wide range and often large number of variables, when we focus on those that comprise 80% of referencesmajor variables -we find that they are fewer in number and large in size. This makes it feasible to track these variables in hardware and provide distinct address mappings for each one if necessary.</p><p>Observation 3: A limited number of major variables contribute to most of the external memory accesses and have large memory footprints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SOFTWARE-DEFINED ADDRESS MAPPING</head><p>From Section 3, we learned that a limited number of major variables in a program have a large memory footprint and contribute to the vast majority of external memory accesses in hardware. It inspires our SDAM: Applications provide access pattern information to the OS, which configures large chunks of memory for the aggregate allocations reached by a variable. For efficiency, the OS maintains pools of memory for each address mapping, and only reconfigures when memory is reclaimed or more memory with a specific mapping is requested. The hardware provides a table to record the address mapping for each chunk of memory.</p><p>Coarse-grained Chunk-based Address Mapping. Observation 3 in Section 3 inspires our chunk-based address mapping, which manages address mapping at a coarse-grained chunk granularity ( 2MB in our case). More specifically, we allocate coarse-grained regions and store mapping for the region, then allocate space for individual data structures within the region from one or more processes. And, to expand or shrink, we allocate/free memory to/from the region in the unit of chunks. The chunk size can be selected, independent of page size, to meet a set of constraints to ensure correctness with low storage and performance overhead, described later in this section. In chunk-based address mapping, we map a pool of data with the same (or similar) access patterns to chunks during the allocation of virtual pages and physical frames. Each chunk consists of contiguous physical pages containing data with the same access pattern. The kernel maintains a chunk mapping table (CMT) that store address mapping information for each chunk. At runtime, the memory controller retrieves the mapping on each reference from CMT to determine how to use bits from the address to select channel, row, and bank indexes. Therefore, at high level, chunk-based address mapping is a collaborative software/hardware technique that improves PA-to-HA address mapping but does not need any modification to existing VA-to-PA translation.</p><p>The required modification to system software is straightforward. To coalesce data with the same access pattern into chunks, we add additional constraints to the allocation of physical frames and virtual pages in software. For physical frame allocation, SDAM configures all physical frames in one chunk have the same address mapping. For virtual allocations, it uses pages from the chunks with the same address mapping. For this, we modify the malloc() function to take the desired address mapping as an additional argument, and store the address mapping as meta-data for allocated virtual pages and chunks. Malloc will allocate pages with the desired mapping, and can call the kernel for more memory with the desired mapping. The kernel will allocate pages from any available chunks with that mapping, and if none exist, create a new chunk for the desired mapping. The detailed modification to the system software can be found in Section VI-A. In addition, in Section VI-B, we discuss a mechanism to statically and dynamically analyze data structure reference pattern, and apply machine learning to determine a set of variables with distinct reference patterns to further reduce the overhead of SDAM.</p><p>The chunk-based address mapping has two steps: (1) looking up the address mapping based on the PA of chunks, (2) calculating the HA based on the PA and address mapping and using HA to place the cache blocks across channels/banks/rows in 3D memory. As in Fig. <ref type="figure" target="#fig_4">5</ref>, the chunk mapping table stores the address mapping for each chunk. Compared to the page table in virtual memory system, the CMT is much smaller, since (1) the physical memory space is much smaller than the virtual memory space and is globally shared by all the processes and (2) the chunk size is much larger than typical page size. (3) 3D-stacked memory is often much smaller due to physical constraints. The physical address of chunks is divided into two parts: chunk number and chunk offset. The chunk number is used as an index to look up the corresponding address mapping in the CMT and the chunk offset is applied as inputs to the hardware address mapping unit, which calculates the HA based on the address mapping. Finally, according to the HA bit field information, memory controller performs data placement in internal structure of physical 3D memory.</p><p>The proposed chunk-based address mapping has low implementation overhead. First of all, the only new hardware is the address mapping unit (AMU) that converts the PA to HA based on the selected address mapping. Secondly, we can use a small table to store the meta-data associated with each access pattern (address mapping), resulting in low storage overhead and fast CMT lookup. Finally, the modifications to the virtual and physical memory allocators are minor.</p><p>Next, we discuss a few properties of SDAM. Functional correctness guarantee: As explained previously, the proposed method keeps standard virtual memory unchanged to ensure correct VA-to-PA translation. As such, we only need to guarantee the correct PA-to-HA mapping, i.e., one PA can map to only one HA or verse versa. To ensure correct PA-to-HA mapping, we need to consider two cases: inter-chunk and intra-chunk: (i) In case of inter-chunk mapping, we keep the chunk number (high order bits of the physical address) unchanged during PA-to-HA address mapping. Since the chunk number is directly copied from the MSBs of the physical frame number (PFN), it guarantees no inter-chunk mapping conflict. (ii) For intra-chunk mapping, all addresses in a chunk use the same and invertible address mapping function, which can guarantee 1-to-1 mapping from PA to HA. A rigorous mathematical proof for that can be found in <ref type="bibr" target="#b0">[1]</ref>. Chunk size: To determine the chunk size, we first consider a hard constraint. Since address mapping only applies to chunk offset, so chunk size should large enough to cover variety strides. Also, since we need to keep track of chunks using a mapping table, to keep a low storage overhead, we would prefer a large chunk size. However, similar to pages in VM, larger chunk sizes may cause internal fragmentation, as the free space in one chunk cannot be allocated to others due to the address mapping constraint. In our implementation, we choose a chunk size of 2MB to balance storage overhead and internal fragmentation based on the following overhead analysis.</p><p>(i). Storage overhead: The 2MB chunk size leads to low storage overhead. The physical memory space is much smaller than the virtual memory space and is globally shared by all the processes, so the total number of chunks in physical memory is limited. In our system with 8GB HBM, we only have 4096 chunks, that is much less than the number of physical frames. Moreover, as the number of data access patterns is significantly reduced after applying machine learning-based optimization (discussed in Section 6.2), we only need to store 1 byte to encode them. As a result, our CMT size consumes only 68 KB. Due to the compactness of CMT, it can be implemented using a small fast on-chip SRAM with negligible latency compared to the HBM access latency. More details can be found in Section 5.</p><p>(ii). Fragmentation: The 2MB chunk size does not suffer from the same internal fragmentation issue as that in huge page. Internal fragmentation at the chunk level in SDAM is bounded by the number of access patterns rather than the the number of chunks as that in huge page. Our system supports up to 256 access patterns, which is confirmed to be sufficient in our evaluation. In the worst memory allocation pattern, we would only waste 256 chunks (6.25% of the total number of chunks, 256/4096=6.25%) due to the internal fragmentation. Furthermore, since SDAM does not need to expand the number of chunk groups for larger memory capacity, the overhead of non-allocatable memory caused by internal fragmentation would be even smaller for larger HBM capacity. Finally, SDAM uses paging for memory allocation that does not have external fragmentation.</p><p>The chunk-based address mapping does not introduce security vulnerabilities. Nevertheless, it can be exploited to mitigate row hammer attacks. As each chunk consists of a large number of contiguous rows within a bank, we can mitigate the row hammer attack by adding guard rows to the sensitive data to ensure the strong physical isolation between data belonging to different security domains, following the same methodology in <ref type="bibr" target="#b6">[7]</ref>. More detailed study on extending SDAM to address security challenges will be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ARCHITECTURAL SUPPORT 5.1 Overview</head><p>To implement SDAM, the required hardware modifications to existing processor are relatively minor. There is no need to modify the architecture/micro-architecture of CPU cores, e.g., ISA, TLB, page walker. The only modification is to add two dedicated hardware components in the memory controller: an address mapping unit (AMU) and on-chip SRAM to store the CMT. (i) The AMU contains a simple crossbar to support arbitrary address remapping. (ii) To achieve compact storage, we use two-level CMT that stores the indices of the address mapping to associate with chunks and address mappings separately. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, for each external memory access, the physical address is divided into two parts: the chunk number (MSBs) and the chunk offset (LSBs). The chunk number is used to index the CMT to find the corresponding entry for address mapping. Then, the AMU maps the chunk offset from the physical address to the hardware address. Finally, the chunk number and the mapped chunk offset are sent to the memory controller. Next, we describe the detailed design of the AMU and the CMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Address Mapping Unit</head><p>The address mapping unit (AMU) maps the chunk offset in the physical memory space to that in the hardware address space.</p><p>We choose to implement the bit-shuffle mechanism in the AMU, to rearrange the address bits in an arbitrary order. From the discussion in <ref type="bibr" target="#b0">[1]</ref> and our evaluation in Section 7, we confirmed that the bit-shuffle is flexible enough to support different address mappings for the workloads of interest and has low implementation overheads. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, AMU implements the bit-shuffle using a simple crossbar, which is an array of switches. In the crossbar, the bit-shuffle allows only one closed switch in each column. The crossbar can be configured by the address mapping information stored in the CMT. Since each input to the crossbar is a single bit, the crossbar requires only 𝑛 2 switches, where 𝑛 is the size of the chunk offset. Using a 2MB chunk size and a 64B cache-line size, our implementation has a 15-bit chunk offset. Thus, the AMU only adds 2 % area to our RISC-V CPU. It is negligible compared with the performance benefit from improved CLP utilization. More evaluation are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chunk Mapping Table</head><p>The CMT stores the meta-data in a compact table containing address mapping information for each entry. To keep storage overhead low, the CMT stores the address mapping in two tables. As in Fig. <ref type="figure" target="#fig_5">6</ref>, entries in the first table stores the index of the address mapping and those in the second table store the actual address mapping that is used to control the AMU. Although the AMU supports a large number of address mappings, only a few of them are used concurrently. Using these two tables can reduce the storage requirement since there are more chunks than the concurrent address mapping in the program.</p><p>As discussed in Section 5.2, the crossbar in the AMU allows only one closed switch in each column. Hence, the crossbar configuration for rearranging 𝑛 chunk offset bits, requires only 𝑛 integers, that represent the locations of closed switch in columns. Since there are 𝑛 switches in each column, each integer should have 𝑙𝑜𝑔 2 (𝑛) bits. In our implementation, as the chunk offset is 15-bit, the configuration of the AMU requires 15 × 𝑙𝑜𝑔 2 (15) ≈ 60 bits. Considering the 2MB chunk size used in this work and a system configuration that has 128GB 3D-stacking memory per socket, the chunk-mapping   <ref type="figure">8</ref>: Multi-heap memory allocation table has 64k entries. To support 256 concurrent address mappings, which is confirmed to be sufficient in our evaluation (Section 7), each entry in the first table has only 𝑙𝑜𝑔 2 (256) = 8 bits. The twolevel table design only requires 67.94 KB (64𝑘 entries×8 bits/entry+ 256 entries × 60 bits/entry) in storage. In comparison, storing the meta-data using a flat table would require 491 kB (64𝑘 entries × 60 bits/entry). Due to the compact storage of the two-level address mapping table, we can implement CMT using a small fast on-chip SRAM with 6 ns latency that is negligible in comparison to the HBM access latency (&gt; 130ns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SOFTWARE SUPPORT 6.1 Address-Mapping-Aware Memory Allocation</head><p>The chunk-based address mapping requires modest modifications to the system software. For both virtual memory and physical frame allocations, rather than using a single allocator, we separate memory based on the address mappings. Then, we can have an allocator for each mapping, and provide a way to move memory between mappings.</p><p>Virtual Memory Allocator. We extend the malloc() in glibc to associate heaps with address mappings and leverage the standard heap memory allocation procedure for each memory allocation. As each heap is associated with one address mapping, we only need to use the address mapping to select the heap, and use the existing heap code to handle the allocation/deallocation within heap. As in the standard malloc() in glibc, the proposed mechanism keeps separate memory pools (arena in glibc) for each thread to reduce the lock contention <ref type="bibr" target="#b15">[16]</ref>.</p><p>As in Fig. <ref type="figure">8</ref>, in the first heap, we maintain an array to track the heaps assigned by address mapping, i.e. heap-mapping array, and also an array to track the address mapping used by the process. We add a new API add_addr_map(), that adds a new address mapping and returns the ID for this address mapping. Also, we add the ID of address mapping as an optional input argument of the malloc() function. Each malloc() call starts with checking if any heap matches its address mapping. If it fails, a new heap is created and attached to the heap list in the first heap. Otherwise, it will check if there is enough space. If not, it will also create a new heap. Then it will allocate the memory in the heap with the matched address mapping. Since addresses allocated to heaps are page-aligned and heaps allocate/free memory independently, this method can guarantee that each page contains data that only have the same address mapping. To free the allocated heap space, the free() function compares the base address of the variable with the ar_ptr and size (Fig. <ref type="figure">8</ref>) to find the corresponding heap.</p><p>Physical Page Allocator. As shown in Fig. <ref type="figure">7</ref>, in the physical memory space, we manage chunks as chunk groups. Each chunk group contains chunks with the same mapping (and thus the same access pattern). At the beginning, all chunks are in the global free-list that contains all the unused chunks. Then, we allocate/free chunks to/from the chunk group associated with different address mappings as needed.</p><p>Similar to the modifications to the malloc(), we add the address mapping ID as an argument of the mmap() that allocates the physical page. For each mmap() call, if there is not enough memory in the corresponding chunk group, it acquires one or more chunks from the global chunk free-list and writes the chunk index and address mapping to the hardware CMT. The proposed method is compatible with on-demand paging. To support the on-demand paging, for each modified mmap() call, we store the address mapping ID in the vm_area_struct in the kernel and move the proposed chunkbased physical page allocator to the page fault handler. The page fault handler can allocate the physical page based on the address mapping ID.</p><p>We rely on the original Linux buddy allocator to free the chunks. If all the blocks within one chunk are freed by the user program, the chunk buddy allocator sets all the bits in the block to 0 and adds it to the chunk free-list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Address Mapping Selection</head><p>For programs with simple repetitive data access such as element size and stride, programmers can identify the access pattern and select the address mapping directly from the source code.</p><p>In more complex scenarios, to absolve programmers of responsibility for finding access pattern information to select an address mapping, we develop a method that automatically i) finds the variables that contribute most to the external memory access (or referred to as major variables), ii) applies machine learning to identify the access patterns of these major variables, and iii) further clusters the access patterns into a reduced set to reduce the storage overhead of the meta-data. Depending on the number of identified major variables of co-run applications and the hardware constraint such as the size of the CMT, we provide additional flexibility of making quality-time trade offs for selecting address mapping, i.e. fast runtime using in which the clustering quality is likely constrained vs. slow runtime using (DL)-assist K-Means for high quality. After profiling and training, the CMT chunk table will be updated over time.</p><p>First, we use profiling to find major variables and associate each major variable with its corresponding physical memory address (PA) trace. At compile time, we use gcc to create a table that maps program counters (PC) to the variables referenced at that PC. Then, we run the application and collect the physical address of each external memory access and its corresponding PC value using the profiling tools as described in <ref type="bibr" target="#b47">[48]</ref> (identified by call-stack matching <ref type="bibr" target="#b23">[24]</ref>). The call-stack matching has two passes. In the first pass, we run the program with a modified malloc() by preloading a shared library that intercepts all heap memory allocation routine and collects allocation call stack. Once we have the call-stack information, we rerun the program with the profiler and find the allocation sites corresponding to a memory reference by matching the call stack. In this way, we can separate the collected PA trace into sub-traces associated with each allocation site(variables) and the address mapping are learned for each variable.</p><p>Based on the PC, we can associate each variable in the source code with a corresponding PA trace.</p><p>For programs known to have few major variables or in situations with constrained hardware that can only support a small number of mappings, we can use classical machine learning K-Means to identify the few best (most representative) reference patterns with which to create address mappings. In particular, we calculate the bit-flip rate vector BFRV below:</p><formula xml:id="formula_0">BFRV = [bfr 1 , bfr 2 , . . . , bfr 𝑚 ], bfr i = 𝑛−1 𝑗 =1 bit 𝑖,𝑗 ⊕ bit 𝑖,𝑗 +1 𝑛 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where 𝑚 is the total number of address bits and bfr 𝑖 is the 𝑖-th element of the BFRV, 𝑛 is the number of memory accesses in the PA address trace and bit 𝑖,𝑗 is the 𝑖-th address bit of 𝑗-th memory access in the trace associated with each major variable. We then apply K-Means clustering to minimize the clustering loss <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_2">L cluster = 𝑘 ∑︁ 𝑖=1 ∑︁ BFRV∈𝑆 𝑖 | |BFRV − 𝜇 𝑖 | | 2 , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where 𝑘 is the number of clusters, 𝑆 𝑖 is a cluster containing BFRVs with similar access pattern and 𝜇 𝑖 is the averaged BFRV in each cluster which determines the address mapping of these variables.</p><p>The bits with higher bit-flip rate are used for channel address while the bits with lower bit-flip rate are mapped onto banks and rows. For complex programs with more major variables, K-Means may be less effective, as the bit-flip rate vector becomes a poor representation for clustering when further increasing data set size. To improve the clustering quality, we propose to learn a clusteringfriendly representation (embedding) for each application from the trace using Autoencoder <ref type="bibr" target="#b3">[4]</ref> and classifies the bit-flip patterns using the learned embedding. To the best of our knowledge, this work is the first to use deep learning to improve the quality of address mapping selection, which is an unsupervised learning problem since there is no groundtruth (label) of address mapping. In comparison, previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> focus on applying deep learning for prefetching which is a supervised learning problem since the groundtruth of a prefetching decision is the address of the following memory access. We summarize the key steps of the DL-assisted address mapping selection using the embedding Long Short Term Memory (LSTM) model below.</p><p>(1) As shown in Fig. <ref type="figure" target="#fig_7">9</ref>, the input is a sequence of (Δ, VID) pairs, where Δ is the address difference (XOR result) between two consecutive memory accesses and VID is the index of the variable identified by gcc. Δ and VID are separately embedded (mapping of a discrete-categorical-variable to a vector of continuous numbers) and the embedding is concatenated and fed as an input to an LSTM-based auto-encoder <ref type="bibr" target="#b17">[18]</ref> of which the hyper-parameters are summarized in Table <ref type="table" target="#tab_1">2</ref>. We first train the auto-encoder by minimizing the reconstruction loss function, defined as follows:</p><formula xml:id="formula_4">L 𝑟𝑒𝑐𝑜𝑛𝑠𝑡𝑟𝑢𝑐𝑡 = L (Δ, Δ,𝑊 ) = 𝑛 ∑︁ 𝑖 𝑚 ∑︁ 𝑗 | Δ 𝑖,𝑗 − 𝑓 𝑤 (Δ 𝑖,𝑗 )|, (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where 𝑚 is the total number of address bits, 𝑛 is the number of memory accesses and Δ 𝑖,𝑗 is the 𝑗-th address bit of 𝑖-th memory access in a Δ trace. (2) We then apply K-Means on the learned embedding vectors and continue the training by minimizing the MSE loss and clustering loss jointly using the loss function L total = L reconstruct + 𝜆L cluster . This DL-assisted K-Means on the high-dimensional embedding vector (256-dim) compared to the K-Means on the bit-flip rate vector (15-dim), improves the clustering quality as the learned embedding can meaningfully represent categories in the transformed space <ref type="bibr" target="#b32">[33]</ref>. (3) Once we have a cluster, we use the bit flip rate to select the desired address mapping for all variables in the cluster.</p><p>The highly flipping bits correspond to frequent accesses in a short time and are mapped onto channel address bits to best exploit the CLP, while the less frequently flipping bits are mapped onto banks and rows. (4) We add the ID of the selected address mapping to the input argument of the malloc() function as in Section 6.1.  We note that, profiling is done offline, once for each application. This will not degrade the runtime performance when input changes. As we will show in our evaluation (Section 7.4), similar speedup can be achieved even when different inputs are used for profiling and execution. For software that runs many times or has a long execution time, the profiling cost added is incidental. Moreover, in the context of software development, the added cost can be further amortized since the profiling result can be reused across variations of the program as long as the data structure and memory allocation site do not change. In practice, profiling-guided optimization (PGO) has been widely deployed by different types of commercial software such as browsers, OS kernel, and datacenter applications. Moreover, a similar high level motivation for using PGO to improve system performance has been used in prior work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> that applies profiling to improve the performance of software prefetching. As discussed earlier, SDAM differs from these work in several key aspects. In addition to the fundamental machine learning method (supervised vs unsupervised), SDAM is developed to improve data placement in 3D memory instead of cache performance. In Section 7.4, we will show more detailed results to compare the profiling time and quality. In practice, the address mapping selection method can be judiciously determined based on the number of major variables and the number of clusters (K) and the CMT size to balance profiling complexity and quality and make the optimal quality-time trade off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION 7.1 Prototyping Platform</head><p>To accurately capture the detailed HBM-specific behaviors and comprehensive interactions between OS and the hardware, we evaluate our design on an open source full system FPGA prototyping platform MEG <ref type="bibr" target="#b47">[48]</ref> with integrated HBM devices and full OS stack. Our baseline system comprises a BOOM RISC-V processor <ref type="bibr" target="#b9">[10]</ref> and shared-memory interfaces for near-memory accelerators on the AlphaData 9H7 FPGA board equipped with an Xilinx VU37P FPGA and an in-package 8GB HBM2 memory <ref type="bibr" target="#b44">[45]</ref>. It also supports Linux, allowing us to evaluate our modifications to the functions in glibc and Linux.</p><p>Specifically, the BOOM CPU consists of four 64-bit out-of-order cores with 64 KB L1 caches that run at 200 MHz. Multiple applicationspecific accelerators generated by SystemVerilog are integrated with the CPU through shared memory interface provided by MEG platform. The Xilinx VU37P FPGA integrates two HBM GEN2 stacks  with 32 independent memory channels. Due to the hardware limitation of our prototyping platform, in this work, we did not evaluate more comprehensive memory subsystem which comprises heterogeneous memories (e.g. DDR, HBM, NVM). A large body of prior work have explored the topic <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> and provide mechanisms to migrate performance critical data from slow DDR memory to fast HBM. These works are orthogonal to ours and can benefit from our technique when combing them.</p><p>To implement our design, the modified hardware is highlighted in Fig. <ref type="figure" target="#fig_8">10</ref>. We add the AMU and the CMT (implemented in System-Verilog) between the HBM2 controller and the Boom core. The AMU is attached to the memory bus, and its upstream is the last level cache (LLC). The AMU maps the PA to HA and sends the HA to the memory controller. We duplicate the AMU eight times to guarantee the peak HBM bandwidth can be achieved (4 billion HBM access (64B) per second). In the real CPU implementation, the duplication of AMUs is unnecessary as the CMOS-based logic is much faster than the FPGA-based logic (4GHz vs 500 MHz) The CMT is attached to the I/O bus of the CPU, so that OS can modify the content of the table through memory-mapped IO. The design is implemented using Xilinx Vivado 2018.4 and Xilinx AXI-HBM IP 1.0. The resource utilization including the duplicated AMU is summarized in Table <ref type="table" target="#tab_2">3</ref>. The two newly added hardware components (AMU and CMT) have negligible area overhead compared to the baseline system.</p><p>We implement the software modification in linux 4.15 and glibc 2.26. The lines of code changed split up by functionality are shown in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Workloads</head><p>In the experiments, to perform comprehensive evaluation, we select a broad set of benchmarks, including synthetic memory-access benchmark, standard benchmarks i.e. SPEC2006 and PARSEC, as well as representative emerging data-intensive benchmarks in three important domains covering large-scale graph processing, in memory data analytics, machine learning and information retrieval. In addition to the evaluation on CPU, we also evaluate the performance gain of offloading data-intensive workloads to hardware accelerators which becomes increasingly important in data center and HPC. Below are the details of evaluated benchmarks. Synthetic benchmark: We use data copy with different strides as a synthetic benchmark. The size of each data copy is the same as the cache-line size of the RISC-V processor (64B). SPEC2006: We studied all 12 integer applications <ref type="bibr" target="#b18">[19]</ref>. PARSEC: PARSEC <ref type="bibr" target="#b5">[6]</ref> is a popular benchmark for evaluating sharedmemory multi-processor. The benchmark covers a wide range of memory behaviours including working set size, locality and external memory access. We studied all 7 applications. Data-intensive benchmarks: In addition to the synthetic and standard benchmarks on CPU, we also evaluate 8 representative data-intensive benchmarks written in both C++ and SystemVerilog for both cases of running on CPU alone and offloading to accelerators, including large-scale graph processing (Breadth-First Search <ref type="bibr" target="#b46">[47]</ref>, PageRank <ref type="bibr" target="#b20">[21]</ref>, Single-Source Shortest Path <ref type="bibr" target="#b33">[34]</ref>), inmemory data analytics (Hash Join <ref type="bibr" target="#b4">[5]</ref>, Merge-Sort Join <ref type="bibr" target="#b42">[43]</ref>), machine learning and information retrieval (K-Means <ref type="bibr" target="#b30">[31]</ref>, HNSW <ref type="bibr" target="#b24">[25]</ref> and IVFPQ <ref type="bibr" target="#b24">[25]</ref>). The custom accelerators allow concurrent execution by adding more pipeline stages and parallel compute units. Thus, they can generate more concurrent memory accesses to the external memory than CPU, and its performance is more sensitive to CLP utilization compared to the CPU workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">System Configuration</head><p>We compare the proposed SDAM with three baseline systems that adopt the hardware-only address mapping approach proposed in prior work <ref type="bibr" target="#b0">[1]</ref> [30] as a default address mapping (DM). We also include two more complex address mapping schemes: bit-shuffle address mapping(BSM) and hashing-based address mapping (HM) <ref type="bibr" target="#b29">[30]</ref>. The bit-shuffle approach rearranges the order of address bits according to the distribution of the bit flip rate collected from profiling. The hashing-based approach maximizes the entropy the channel bits by XORing a number of address bits. As summarized in Section 2.3, software-based solutions use fixed hardware and control the data allocation at page granularity. Therefore, the three baseline systems which use a single static address mapping can represent software-based solutions. Baseline system + default address mapping (BS+DM): A single, fixed address mapping as defined in the Xilinx HBM controller IP <ref type="bibr" target="#b22">[23]</ref>) is applied globally to all applications. Baseline system + bit-shuffle address mapping (BS+BSM): This configuration still uses a single address mapping for all applications but trends to optimize the address mapping selection according to the profiling results. In particular, we collect physical addresses of 500 million of cache misses per benchmark and calculate the bit flip rate for the workload mix combining all benchmarks. Then, we select the optimal address mapping based on the bit flip rate where the highly flipping bits are mapped onto channels to best exploit the CLP, while the rest are mapped onto banks and rows.</p><p>Baseline system + hashing-based address mapping (BS+HM): Similar to BS+DM and BS+BSM, this configuration uses a single address mapping for all applications but applies a different optimization method, i.e., using hashing <ref type="bibr" target="#b29">[30]</ref> for address mapping selection. The selected hash function is capable of harvesting entropy from many and randomly selected address bits and concentrating the entropy into channel bits. In comparison to BSM, HM does not rely on profiling. In this configuration, we refer to the method in a recent work <ref type="bibr" target="#b29">[30]</ref> which provides a good balance between implementation complexity and performance gain. In our study, we found theoretically perfect hashing function leads to marginal speedup (&lt;3%) over <ref type="bibr" target="#b29">[30]</ref> at the cost of significantly increased overhead. We defer more comprehensive hashing methods to our future work. Software-defined mapping + bit-shuffle address mapping without machine learning (SDM+BSM): This configuration uses the proposed SDAM (with the AMU and the CMT and running modified Linux and glibc). For each process, it uses a single address mapping chosen using bit-flip rate. Software-defined mapping + bit-shuffle address mapping with machine learning: K-Means (SDM +BSM+ML) and DLassisted K-Means (SDM+BSM+DL): These two configurations evaluate our complete design. As compared with the case of SDM+ BSM that selects one address mapping for one application, SDM +BSM +ML and SDM+BSM+DL select the optimal address mapping for each individual variable within an application using the K-Means-based and DL-assisted K-Means address mapping selection, respectively.</p><p>For systems of BS+BSM, SDM+ BSM, SDM+ BSM+ML, SDM+BSM +DL that require profiling, we compare the results using different program input for profiling and evaluation. In detail, for SPEC2006 and PARSEC, we use the training dataset for profiling and test dataset for the evaluation. Both datasets are selected by the script provided by the benchmark suite (e.g. runspec). For Graph Workload, we generate different graphs using graph500 generator with different seed for profiling and test. The scale is 20 and edge factor is 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results</head><p>In the synthetic benchmark, we run four threads for data copy and vary the strides. Since the optimal address mapping can be derived from the strides directly, we do not need to use profiling. As shown in Fig. <ref type="figure" target="#fig_9">11</ref>(a), when there is only one access pattern (number of strides = 1), both BS+BSM and SDM+BSM achieve the maximum throughput and outperform the other two baseline systems (BS+DM and BS+HM). It demonstrates the bit-shuffle approach can be effective if the address mapping can be optimally selected according to a single access pattern. However, in more complex cases of increasing concurrent data accesses with different strides, we observe a significant performance degradation for BS+BSM, as a fixed and global address mapping becomes less effective in capturing the diversity in access behaviors. In comparison, the performance of BS+HM is nearly kept constant across all the three cases but is still worse than SDM+BSM. The reason for the constant performance is that the hash function used by BS+HM is optimally selected to be less sensitive to different access behaviors, as it takes many address bits as input to cover a majority of access patterns <ref type="bibr" target="#b29">[30]</ref>. We plot the distribution of the CLP utilization of different strides (1 to 64) using BS+BSM, BS+HM and SDM+BSM in Fig. <ref type="figure" target="#fig_9">11(b</ref>). For better illustration, we sort the CLP utilization in ascending order. We can see that the BS+HM tends to maximize the averaged CLP utilization for a large number of access patterns statistically, but the hashing function cannot cover all possible ones and thus for some cases may lead to underutilized CLP compared to BS+BSM. In contrast, SDM+BSM try to select the optimal address mapping deterministically for each access pattern. Thus, as shown in Fig. <ref type="figure" target="#fig_9">11</ref>(a) and Fig. <ref type="figure" target="#fig_9">11(b</ref>), the proposed SDM+BSM consistently outperforms the other three systems for all the strides and the performance benefit of SDM+BSM grows with respect to the other three as data access patterns become more complex. These results validate the effectiveness of SDAM as compared with the hardware-only global address mapping in the baseline systems.</p><p>We further provide evaluation results for the standard CPU benchmarks. In Fig. <ref type="figure" target="#fig_10">12</ref>, we compare the speedup of BS+BSM, BS+HM, SDM+BSM, SDM+BSM+ML and SDM+BSM+DL against the baseline BS+DM using different inputs for profiling and evaluation and perform cross-validation. For SDM+BSM+ML and SDM+BSM+DL, we include results using different numbers of clusters (4 and 32). Using 4 clusters per application represents the case in which several variables may need to share the same address mapping when there is a large number of co-run applications but only a limited number of chunk table entries can be used for each application. In comparison, in the case of using 32 clusters per application, each application may acquire enough chunk table entries so that each major variable can have its address mapping, resulting in improved accuracy.</p><p>As expected, the BS+BSM only has an average speedup of 1.01× over the baseline BS+DM. For some benchmarks e.g., perl and stream, the SDM+BSM shows worse performance than BS+BSM. It is because the global address mapping cannot effectively capture the different access patterns within/across applications, even when address mapping is optimally selected. It is not practical to find a single address mapping to benefit all applications. The SDM+BSM achieves more performance improvement (1.08×, 1.09×) than BS+DM and BS+BSM, as it provides application-dependent address mapping (one address mapping for one application) to better adapt to different access patterns.</p><p>In addition, we observe the same trend as that in the synthetic benchmark. The BS+HM achieves better performance than both BS+BSM and BS+DM for almost all standard benchmarks (1.14×). The reason for such performance improvement across the majority of benchmarks is because the channel bits are generated from multiple address bits in hashing-based method, making it possible to adapt to a wide range of access patterns within/across different applications. However, due to the nature of the pseudo-random hashing function generation, it may not be optimal for a specific set of access patterns.</p><p>Our results show by selecting the optimal address mapping for each individual data structure within an application, SDM+BSM+ML can further improve the performance by 1.16× and 1.27× over the baseline BS+DM for 4 and 32 clusters per application respectively. Increasing the number of clusters leads to better speedup. In comparison to K-Means based method (SDM+BSM+ML), the DL-assisted K-Means (SDM+BSM+DL) achieves 1.33× and 1.43× speedup over the BS+DM for 4 clusters and 32 clusters respectively. Comparing the two methods, we can see DL-assisted K-Means achieves consistently better performance, insensitive to the number of clusters chosen for each application. However, SDM+BSM+ML works well for applications with smaller number of major variables such as perl (1 major variable) and mcf (3 major variables). For applications with a large number of major variables such as omnetpp (65 major variables) and astar (38 major variables), SDM+BSM+ML has nearly no performance improvement (1.04×, 1.09×), but SDM+BSM+DL can deliver a speedup of 1.27× and 1.22×.</p><p>For data-intensive benchmarks, the BS+HM achieves similar speedup as that observed in standard benchmarks. In comparison, the and SDM+BSM+DL have higher speedup (1.44× vs. 1.27×, 1.84× vs. 1.41×) on the data-intensive benchmarks than standard benchmarks as they have more memory accesses. The results are consistent with our expectation that emerging data intensive applications would benefit more from our techniques.</p><p>We can observe that, even with different input, the proposed method could still detect the access pattern and achieve 1.84× speedup. The reason is that address mapping is a function of data structure, so the changes of the program do not affect the choice of address mapping as long as the data structure and program structure (e.g. memory allocation site) remain the same.</p><p>In principle, SDAM achieves higher performance gain with more powerful cores (count, frequency etc.), due to the increased memory requests and thus more chances for memory channel contention. To emulate such effect, we conducted experiments by varying 1) the number of cores, 2) the working frequency of HBM to further stress the memory system. As expected, our measurement results confirmed, on average, the speedup increases from 1.27× to 1.32× when increasing the number of cores from 1 to 4, and the speedup increases by 19% on all the benchmarks when slowing down the HBM to a quarter of its maximum frequency. Near-data acceleration: To account for the growing importance of accelerators in future system, we also evaluate the case of offloading a set of data-intensive applications to near memory accelerators. We observe the similar trends as running these benchmarks on CPU in Fig. <ref type="figure" target="#fig_12">15</ref> and show that SDM+BSM+DL outperforms the other four systems for the evaluated benchmark. But they achieves much higher performance gain (2.58× speedup of pairing SDAM with accelerators over the baseline of accelerator without SDAM). It indicates the hardware accelerator can benefit more from our techniques. The reasons are: (i) accelerators typically have more parallelisms, and thus can generate more concurrent memory accesses to external memory; (ii) accelerator tends to have smaller caches, leading to higher cache miss rate. Profiling time: We measure the profiling time of four SDAM configurations on our workstation with intel i7-9700 CPU for all applications: SDM+BSM+ML (4 patterns), SDM+BSM+ML (32 patterns), SDM+BSM+DL (4 patterns), and SDM+BSM+DL (32 patterns). As shown in Fig. <ref type="figure" target="#fig_11">13</ref>, we observe that SDM+BSM+ML (0.3 minutes for 4 patterns and 2 minutes for 32 patterns) has much lower overhead than SDM+BSM+ML (26 minutes for 4 patterns and 29 minutes for 32 patterns). Also, the SDM+BSM+ML approach is more sensitive to the number of clusters (0.3 minutes vs 2 minutes) as the K-Means algorithm converges with fewer iterations for a smaller number of clusters. For benchmarks such as perl and bzip, SDM+BSM+ML (4 patterns) is sufficient to achieve the same speedup as the other three configurations but only takes 0.3 minutes which is much lower than the execution time of the application itself (e.g., 5.9 minutes for perl and 4.4 minutes for bzip). We see similar low overhead for the other benchmark applications when using the appropriate profiling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Address mapping schemes: DRAM address mapping schemes have been previously proposed for single-core CPUs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref>, multicore CPUs <ref type="bibr" target="#b26">[27]</ref>, and GPUs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>. Bit-shuffle rearranges the order of the address bits that map to the bank, channel, and column address fields. For instance, Kaseridis et al. <ref type="bibr" target="#b26">[27]</ref> use the LSBs as the bank and channel bits to better exploit BLP in the streaming applications. This mapping method is also widely used in commercial computer systems. A common issue of these works is that a global fixed address mapping is configured at the system boot time and cannot be changed at runtime to adapt to different access patterns. Alternatively, hashing-based methods take XOR of several address bits to increase the entropy of the bank address bits to improve BLP on CPU <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref> and GPU <ref type="bibr" target="#b29">[30]</ref>, which could be extended to improve CLP. However, this one-size-fit-all approach relies on the pseudo-random permutation generation that avoids very bad channel contention but cannot achieve optimal CLP for a access pattern <ref type="bibr" target="#b38">[39]</ref>.</p><p>Numerous research work target ways of remapping address dynamically at run-time to improve memory access performance. The most similar works to ours are Akin et. al. <ref type="bibr" target="#b0">[1]</ref> and Mohsen et al. <ref type="bibr" target="#b16">[17]</ref>. Akin et al. <ref type="bibr" target="#b0">[1]</ref> proposes to shuffle the order of physical address, while Mohsen et al. <ref type="bibr" target="#b16">[17]</ref> proposes to take the XOR of bank addresses and row addresses with the bank index. However, the common problem of these works is that they select address mapping based on the physical address only whereas the physical address does not contain useful information of the program, such as thread ID, data structure. Without such knowledge, it is difficult to infer the program's behavior accurately at runtime from the statistics collected from hardware to capture the different access patterns in the memory access stream, especially with the interference of other co-running applications. Compared to these hardware-only approaches, our work is a collaborative software/hardware technique. Our SDAM associates the physical address with the high-level program's access semantics and thus can effectively adapt to the different access patterns of applications. Software-based data-placement optimization: Another lines of research target to leverage the benefits of static program analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, dynamic profiling <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> or a combination of them to guide data-placement in the physical memory hierarchy for performance optimization. The annotations are used to guide the data placement in the different locations of cache <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, heterogeneous memories <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref> and NUMA nodes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref> to better exploit MLP and/or to improve the locality. However, a common problem of these methods is that they often rely on a given memory access pattern and become ineffective to cases when applications have different patterns <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Moreover, these works can only control the data-placement at coarse-grained granularity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. However, as discussed in Section 3, to best exploit CLP in 3D-stacking memory, we need to have more fine-grained control at the granularity of cache block. As discussed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref>, simply decreasing the granularity leads to prohibitively high overhead in both storage and performance, especially for programs with a large number of data structures <ref type="bibr" target="#b40">[41]</ref>. In comparison, by clustering data with the same access pattern into chunks, our work can effectively reduce management overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we propose software-defined address mapping (SDAM) which is a new system mechanism (architecture/OS support) that enables user program to obtain a direct control of the low-level memory hardware in a more intelligent and fine-grained manner. As a case study, we apply SDAM to 3D-stacking memory to better exploit its CLP by controlling the address mapping, adaptive to the data access pattern to different data structures across applications at runtime, which, is not currently supported in existing systems. We apply machine learning for system optimization to further reduce the overhead of SDAM and improve performance. Our evaluation results not only confirmed the effectiveness of SDAM but also showed new trends with emerging data-intensive applications and new system components such as near memory accelerators will benefit more from the proposed technique. We believe SDAM is an essential step towards Software-Defined Memory (SDM).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing HBM throughput for different number of channels and row buffer hit rate</figDesc><graphic url="image-2.png" coords="3,329.97,189.38,216.21,113.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of channel conflicts (in red) for different access patterns and address mappings. The access granularity is 64B (cache-line size of RISC-V architecture).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) HBM throughput with different stride using the default address mapping; (b) Bit flip distribution for different stride using the default address mapping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Throughput comparison of using single and multiple address mapping for workloads with mixed access patterns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Chunk-based address mapping management</figDesc><graphic url="image-6.png" coords="5,353.99,83.69,168.17,113.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hardware modification</figDesc><graphic url="image-9.png" coords="7,65.81,251.29,216.21,77.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure8: Multi-heap memory allocation table has 64k entries. To support 256 concurrent address mappings, which is confirmed to be sufficient in our evaluation (Section 7), each entry in the first table has only 𝑙𝑜𝑔 2 (256) = 8 bits. The twolevel table design only requires 67.94 KB (64𝑘 entries×8 bits/entry+ 256 entries × 60 bits/entry) in storage. In comparison, storing the meta-data using a flat table would require 491 kB (64𝑘 entries × 60 bits/entry). Due to the compact storage of the two-level address mapping table, we can implement CMT using a small fast on-chip SRAM with 6 ns latency that is negligible in comparison to the HBM access latency (&gt; 130ns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The embedding LSTM model.</figDesc><graphic url="image-10.png" coords="8,328.85,83.68,216.22,71.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (a) Diagram of the prototyping system; (b) Photo of the FPGA prototyping platform with integrated HBM devices</figDesc><graphic url="image-11.png" coords="9,76.70,139.21,192.19,59.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (a) Throughput of running four-thread data copy with different stride, normalized to the peak streaming throughput. (b) Distribution of HBM bandwidth utilization for 64 different strides using BS+BSM, BS+HM and SDM+BSM.</figDesc><graphic url="image-12.png" coords="10,56.28,83.69,233.03,65.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup on CPU for (a) standard benchmarks and (b) emerging data-intensive benchmarks.SDM+BSM try to select the optimal address mapping deterministically for each access pattern. Thus, as shown in Fig.11(a) and Fig.11(b), the proposed SDM+BSM consistently outperforms the other three systems for all the strides and the performance benefit of SDM+BSM grows with respect to the other three as data access patterns become more complex. These results validate the effectiveness of SDAM as compared with the hardware-only global address mapping in the baseline systems.We further provide evaluation results for the standard CPU benchmarks. In Fig.12, we compare the speedup of BS+BSM, BS+HM, SDM+BSM, SDM+BSM+ML and SDM+BSM+DL against the baseline BS+DM using different inputs for profiling and evaluation and perform cross-validation. For SDM+BSM+ML and SDM+BSM+DL, we include results using different numbers of clusters (4 and 32). Using 4 clusters per application represents the case in which several variables may need to share the same address mapping when there is a large number of co-run applications but only a limited number of chunk table entries can be used for each application. In comparison, in the case of using 32 clusters per application, each application may acquire enough chunk table entries so that each major variable can have its address mapping, resulting in improved accuracy.As expected, the BS+BSM only has an average speedup of 1.01× over the baseline BS+DM. For some benchmarks e.g., perl and stream, the SDM+BSM shows worse performance than BS+BSM. It is because the global address mapping cannot effectively capture the different access patterns within/across applications, even when address mapping is optimally selected. It is not practical to find a single address mapping to benefit all applications. The SDM+BSM achieves more performance improvement (1.08×, 1.09×) than BS+DM and BS+BSM, as it provides application-dependent address mapping (one address mapping for one application) to better adapt to different access patterns.In addition, we observe the same trend as that in the synthetic benchmark. The BS+HM achieves better performance than both BS+BSM and BS+DM for almost all standard benchmarks (1.14×). The reason for such performance improvement across the majority of benchmarks is because the channel bits are generated from multiple address bits in hashing-based method, making it possible to adapt to a wide range of access patterns within/across different applications. However, due to the nature of the pseudo-random hashing function generation, it may not be optimal for a specific set of access patterns.Our results show by selecting the optimal address mapping for each individual data structure within an application, SDM+BSM+ML can further improve the performance by 1.16× and 1.27× over the baseline BS+DM for 4 and 32 clusters per application respectively. Increasing the number of clusters leads to better speedup. In comparison to K-Means based method (SDM+BSM+ML), the DL-assisted K-Means (SDM+BSM+DL) achieves 1.33× and 1.43× speedup over</figDesc><graphic url="image-13.png" coords="11,57.72,83.69,494.32,69.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Profiling time for K-Means vs DL-assisted K-Means</figDesc><graphic url="image-15.png" coords="11,317.96,242.83,240.25,75.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Speedup on accelerators (baseline: BS+DM).</figDesc><graphic url="image-16.png" coords="12,53.80,83.69,240.24,64.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of variable-level statistics</figDesc><table><row><cell cols="2">Benchmark</cell><cell># of Var.</cell><cell># of Major Var.</cell><cell>Avg. Major Var. Size (MB)</cell><cell>Min. Major Var. Size (MB)</cell></row><row><cell>SPEC2006</cell><cell>perlbench bzip2 gcc mcf gobmk hmmer sjeng libquantum h264ref omnetpp astar xalancbmk</cell><cell>7268 10 49690 3 43 84 4 10 193 9400 178 4802</cell><cell>1 10 34 3 5 10 4 7 8 65 38 4</cell><cell>910 32 59 1215 8 6 60 212 24 3 1.8 230</cell><cell>910 4 4 953 7 4 54 4 7 1 9 78</cell></row><row><cell>PARSEC</cell><cell>bodytrack cenneal dedup ferret freqmine streamcluster vips</cell><cell>220 17 29 109 60 35 892</cell><cell>12 9 15 22 9 9 25</cell><cell>212 365 215 65 215 234 125</cell><cell>36 69 12 23 37 68 36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Training hyper-parameters</figDesc><table><row><cell>Network size Steps Embedding Size</cell><cell>256x2 LSTM Learning Rate 500k Sequential Length 32 0.001 256 𝜆 0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FPGA resource utilization</figDesc><table><row><cell>LOGIC SRAM Boom Core 91.8% 88.0% HBM 7.5% 10.2% Controller AMU 0.5% 0% CMT 0.2% 1.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>LOC changed</figDesc><table><row><cell>Feature</cell><cell>LOC changed</cell></row><row><cell>VM allocator PM allocator Driver Miscellaneous</cell><cell>131 97 98 33</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data reorganization in memory using 3D-stacked DRAM</title>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="131" to="143" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">JEDEC Solid State Technology Association et al. 2012</title>
	</analytic>
	<monogr>
		<title level="m">JEDEC Standard: DDR4 SDRAM. JESD79-4</title>
				<imprint>
			<date type="published" when="2012-09">Sep (2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying Memory Access Patterns for Prefetching</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378498</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Lausanne, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="513" to="526" />
		</imprint>
	</monogr>
	<note>ASP-LOS &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Learning and Deep Architectures</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop -Volume</title>
				<meeting>the 2011 International Conference on Unsupervised and Transfer Learning Workshop -Volume<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mainmemory hash joins on multi-core CPUs: Tuning to the underlying hardware</title>
		<author>
			<persName><forename type="first">Cagri</forename><surname>Balkesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Teubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Tamer</forename><surname>Özsu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2013.6544839</idno>
		<ptr target="https://doi.org/10.1109/ICDE.2013.6544839" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 29th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PARSEC benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder</forename><forename type="middle">Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel architectures and compilation techniques</title>
				<meeting>the 17th international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CAn&apos;t touch this: Software-only mitigation against Rowhammer attacks targeting kernel memory</title>
		<author>
			<persName><forename type="first">Ferdinand</forename><surname>Brasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Davi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Liebchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad-Reza</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th {USENIX} Security Symposium</title>
		<title level="s">{USENIX} Security</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compiler-directed page coloring for multiprocessors</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="244" to="255" />
			<date type="published" when="1996">1996</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cacheconscious data placement</title>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Krintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simmi</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="1998">1998</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pi-Feng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borivoje</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<title level="m">BOOM v2</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Managing DRAM latency divergence in irregular GPGPU applications</title>
		<author>
			<persName><forename type="first">Niladrish</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuwan</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="128" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dymaxion: Optimizing memory access patterns for heterogeneous systems</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2011 international conference for high performance computing, networking, storage and analysis</title>
				<meeting>2011 international conference for high performance computing, networking, storage and analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BATMAN: Maximizing Bandwidth Utilization of Hybrid Memory Systems</title>
		<author>
			<persName><forename type="first">Aamerjaleel</forename><surname>Chiachenchou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddink</forename><surname>Qureshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cache-conscious structure layout</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Trishul M Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1999">1999</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple but effective heterogeneous main memory with on-chip memory controller support</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A scalable concurrent malloc (3) implementation for FreeBSD</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the bsdcan conference</title>
				<meeting>of the bsdcan conference<address><addrLine>ottawa, canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dream: Dynamic re-arrangement of address mapping to improve the performance of DRAMs</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Ghasempour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><forename type="middle">D</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Luján</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Memory Systems</title>
				<meeting>the Second International Symposium on Memory Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Memory Access Patterns</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SPEC CPU2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">John L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Physical Address Decoding in Intel Xeon v3/v4 CPUs: A Supplemental Datasheet</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Hillenbrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Karlsruhe Institute of Technology, Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient Parallel Graph Exploration on Multi-Core CPU and GPU</title>
		<author>
			<persName><forename type="first">Sungpack</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayo</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2011.14</idno>
		<ptr target="https://doi.org/10.1109/PACT.2011.14" />
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Parallel Architectures and Compilation Techniques</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<title level="m">Intel Xeon Processor E7 v4 Product Family Datasheet</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High bandwidth memory (hbm) dram</title>
	</analytic>
	<monogr>
		<title level="j">JEDEC</title>
		<imprint>
			<biblScope unit="page">D235</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding object-level memory access patterns across the spectrum</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nosayba</forename><surname>El-Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sudharshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Vazhkudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HeteroOS-OS design for heterogeneous memory management in datacenter</title>
		<author>
			<persName><forename type="first">Sudarsun</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="521" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bandwidth-aware memory-subsystem resource management using non-invasive resource profilers for large cmp systems</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kaseridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stuecheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture. IEEE</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NUMA (Non-Uniform Memory Access): An Overview</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lameter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm queue</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A software memory partition approach for eliminating bank-level interference in multicore systems</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on Parallel architectures and compilation techniques</title>
				<meeting>the 21st international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Get out of the valley: power-efficient address mapping for GPUs</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="166" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heterogeneous memory architectures: A HW/SW approach for mixing die-stacked and off-package memories</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Mitesh R Meswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Slice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Ignatowski</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of clustering with deep learning: From the perspective of network architecture</title>
		<author>
			<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="39501" to="39514" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introducing the graph 500</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">B</forename><surname>Richard C Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><surname>Ang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MOCA: Memory object classification and allocation in heterogeneous memory systems</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaizeen</forename><surname>Aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayse</forename><surname>Coskun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fine-grained DRAM: energy-efficient DRAM for extreme bandwidth systems</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladrish</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyuk</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="41" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hybrid memory cube (HMC)</title>
		<author>
			<persName><forename type="first">Pawlowski</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Hot Chips 23 Symposium (HCS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MLP aware heterogeneous memory system</title>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Phadke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Narayanasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Design, Automation &amp; Test in Europe. IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pseudo-randomly interleaved memory</title>
		<author>
			<persName><forename type="first">Rau</forename><surname>Ramakrishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual international symposium on Computer architecture</title>
				<meeting>the 18th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Micro-pages: increasing DRAM efficiency with locality-aware data placement</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Sudan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladrish</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Locality Descriptor: A Holistic Cross-Layer Abstraction to Express Data Locality In GPUs</title>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00074</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00074" />
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="829" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory</title>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diptesh</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nastaran</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A parallel sort merge join algorithm for managing data skew</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/71.205654</idno>
		<ptr target="https://doi.org/10.1109/71.205654" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exposing memory access regularities using object-relative memory profiling</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Pyatakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Easwaran</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization</title>
				<meeting>the international symposium on Code generation and optimization: feedback-directed and runtime optimization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">315</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">AXI High Bandwidth Memory Controller v1</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">UltraScale+ FPGA Product Tables and Product Selection Guide</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Boosting the Performance of FPGA-based Graph Processor Using Hybrid Memory Cube: A Case for Breadth First Search</title>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroosh</forename><surname>Khoram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3020078.3021737</idno>
		<ptr target="https://doi.org/10.1145/3020078.3021737" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays<address><addrLine>Monterey, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
	<note>FPGA &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MEG: A RISCV-Based System Simulation Infrastructure for Exploring Memory Optimization Using FPGAs and Hybrid Memory Cube</title>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The impulse memory controller</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Binu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Schaelicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">C</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1117" to="1132" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A permutation-based page interleaving scheme to reduce row-buffer conflicts and exploit data locality</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 33rd Annual IEEE/ACM International Symposium on Microarchitecture. MICRO-33 2000</title>
				<meeting>33rd Annual IEEE/ACM International Symposium on Microarchitecture. MICRO-33 2000</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
