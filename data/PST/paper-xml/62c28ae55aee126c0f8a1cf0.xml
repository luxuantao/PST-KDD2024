<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Position Prediction as an Effective Pretraining Strategy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
							<email>&lt;szhai@apple.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">Yitan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Position Prediction as an Effective Pretraining Strategy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> have gained increasing popularity in a wide range of applications, including Natural Language Processing (NLP), Computer Vision and Speech Recognition, because of their powerful representational capacity. However, harnessing this representational capacity effectively requires a large amount of data, strong regularization, or both, to mitigate overfitting. Recently, the power of the Transformer has been unlocked by self-supervised pretraining strategies based on masked autoencoders which rely on reconstructing masked inputs, directly, or contrastively from unmasked content. This pretraining strategy which has been used in BERT models in NLP <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, Wav2Vec models in Speech <ref type="bibr" target="#b1">(Baevski et al., 2020)</ref> and, recently, in MAE models in Vision (Bao et al., 2021;<ref type="bibr" target="#b17">He et al., 2021)</ref>, forces the model to learn about relationships between the content in different parts of the input using autoencoding related objectives. In this paper, we propose a novel, but surprisingly simple alternative to content reconstruction -that of predicting locations from content, without providing positional information for it. Doing so requires the Transformer to understand the positional relationships between different parts of the input, from their content alone. This amounts to an efficient implementation where the pretext task is a classification problem among all possible positions for each input token. We experiment on both Vision and Speech benchmarks, where our approach brings improvements over strong supervised training baselines and is comparable to modern unsupervised/self-supervised pretraining methods. Our method also enables Transformers trained without position embeddings to outperform ones trained with full position information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> have become a unified architecture in NLP, Computer Vision and Speech. Their high capacity and lack of domain specific inductive biases means Transformers require large amounts of training data to achieve good generalization. One effective remedy, first developed in the NLP community, is unsupervised pretraining. For example, BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> trains a Transformer with unlabeled text by solving masked token prediction. This greatly benefits downstream applications, and has become the standard approach for various NLP tasks.</p><p>Recently, there have been a few attempts to apply the BERT pretraining idea to Computer Vision, with Vision Transformers (ViTs) <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref> being the backbone architecture. In particular, BEiT <ref type="bibr" target="#b2">(Bao et al., 2021)</ref> converts image patches to discrete tokens with a separately trained VQVAE <ref type="bibr">(van den Oord et al., 2017)</ref>. This makes it possible to use the same cross entropy loss for masked image patch prediction as for token prediction in BERT. MAE <ref type="bibr" target="#b17">(He et al., 2021)</ref> further simplifies the recipe of BEiT by directly predicting the masked patches with a regression loss in the pixel space.</p><p>In this paper, we propose a simple and effective approach for Transformer pretraining that removes the need for reconstructing dense patch values. Our idea is inspired by the observation that Transformers are relatively insensitive to the order of input tokens. In <ref type="bibr" target="#b26">(Naseer et al., 2021)</ref>, it is shown that pretrained ViTs demonstrate strong robustness to image patch shuffling perturbation at test time. <ref type="bibr" target="#b30">(Sinha et al., 2021)</ref> shows that training the BERT model with randomly shuffled word order gives surprisingly competitive performance. <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> also suggests that a ViT without positional embeddings shows only a small degradation in the linear probing task for self-supervised learning. This evidence suggests that much of the power of Transformers results from the ability to reason about the co-occurrence of the set of unordered input tokens. We thus ask the question: How much can unsupervised pretraining learn using only contents for prediction? This motivates us to formulate a novel pretraining strategy, explained as follows.</p><p>In the pretraining phase, the model (e.g., a ViT) receives a set of tokens (e.g., image patches) but not their positions, and the pretext task is to recover the position of each input token, cast as a classification problem among all positions.</p><p>By doing so, we formulate a special case of masked autoencoder, where the positions, rather than tokens, are removed from the input and predicted by the model. This training objective can also be interpreted as training a Set Transformer <ref type="bibr" target="#b23">(Lee et al., 2019)</ref> to solve a Jigsaw puzzle <ref type="bibr" target="#b27">(Noroozi &amp; Favaro, 2016)</ref>. In order to solve the task, the Transformer needs to reason about the high order interaction of the input tokens, which amounts to understanding the underlying semantics (e.g., part-whole relationship of the given object) represented by the inputs. Empirically, we have found that large Transformers can often achieve near perfect accuracy on the position prediction task 1 . We then propose to increase the task's difficulty by selecting a random subset of tokens as context, and modify the attention layers such that only context tokens are used as keys and values. In this way, the Transformer not only needs to order the context tokens, but also infer the positions for masked out tokens by querying into the context. We hence dub our method MP3, denoting Masked Patch Position Prediction.</p><p>During finetuning, we disable token masking and add absolute positional embeddings in the same way as standard Transformers. We then remove the linear position prediction head and replace it with a linear head for the downstream task (e.g., classification). All the parameters are updated for a desired number of finetuning steps with the downstream 1 Except for Speech, where a patch (audio frame) is a small part of the full sequence, and it is much more challenging without providing some reference points with known positions. task's training objective.</p><p>MP3 amounts to a simple implementation. In the pretraining phase, no additional modules are needed other than a linear head with d × n parameters, where d is the model's feature dimension and n is the number of positions. The training objective is simply the cross entropy loss. Also, thanks to the context masking, full self-attention is reduced to sparse attention, which effectively makes the pretraining cost lower than that of finetuning.</p><p>We conduct experiments on both Vision and Speech tasks. MP3 consistently improves the performance of Transformer models compared to strong supervised training baselines, and matches other more sophisticated unsupervised/selfsupervised pretraining methods, despite is simplicity. Remarkably, MP3 enables strong finetuning performance even without using position embeddings, sometimes outperforming the supervised training baselines by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Denoising Autoencoders (DAEs). DAEs <ref type="bibr" target="#b33">(Vincent et al., 2010)</ref> are well studied models in the context of unsupervised pretraining. The idea is to reconstruct the inputs given noisy versions of themselves. Masked autoencoder (MAE) is a special case of DAE, where part of the inputs are masked out (with a multiplicative Bernoulli noise). When combined with Transformers, MAEs have shown great success as an unsupervised pretraining technique, with BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, BEiT <ref type="bibr" target="#b2">(Bao et al., 2021)</ref> and MAE <ref type="bibr" target="#b17">(He et al., 2021)</ref> as notable examples. MP3 can also be viewed as a special case of MAEs, but it masks out the positional information (and optionally input tokens), rather than input tokens. The reconstruction objective is then turned into a sorting task, which has very different implications than reconstructing missing tokens given positions.</p><p>Self-supervised learning with order prediction. Unsupervised feature learning with order prediction of image patches is first proposed in <ref type="bibr" target="#b27">(Noroozi &amp; Favaro, 2016)</ref>, and then extended in followup works such as <ref type="bibr" target="#b22">(Lee et al., 2017;</ref><ref type="bibr" target="#b0">Ahsan et al., 2019;</ref><ref type="bibr" target="#b36">Xu et al., 2019;</ref><ref type="bibr" target="#b28">Santa Cruz et al., 2018;</ref><ref type="bibr" target="#b15">El-Nouby et al., 2019)</ref>. What these works share is that they often adopt a CNN based encoder for an image patch or a video clip, and an MLP based prediction network to output the correct order of a set of inputs (except <ref type="bibr" target="#b15">(El-Nouby et al., 2019)</ref> which uses order prediction to approximate future prediction in videos). The output of these methods is then a local representation for image patches or video clips, as the order prediction network is discarded. This is in stark contrast with our work, MP3 focuses on learning the global representation via attention. This is only made possible by the powerful Transformer architecture, which focuses on learning the interactions between input elements, and the same global knowledge is transferred to downstream tasks in the finetuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of positional embedding in Transformers.</head><p>Positional embeddings (PEs) are of unique importance to Transformers, and improving PEs is an active research area, see <ref type="bibr" target="#b14">(Dufter et al., 2021)</ref> for an overview. However, it is empirically observed that the performance of Tranformers is surprisingly robust to the order of the inputs. For ViTs, <ref type="bibr" target="#b26">(Naseer et al., 2021)</ref> shows that pretrained ViTs suffer much less from patch shuffling perturbations than CNNs. <ref type="bibr" target="#b30">(Sinha et al., 2021)</ref> shows that masked language models perform well even when trained with shuffled sentences. <ref type="bibr" target="#b6">(Chen et al., 2021</ref>) also shows that a Transformer without PEs shows only a small degradation when evaluated with linear probing in a contrastive learning setup. MP3 confirms the hypothesis that much of the Transformer's power lies in its ability to model the co-occurrence of input tokens. In particular, our pretraining method does not use or train PEs at all (instead of randomly shuffling input tokens while using PE), and still performs competitively compared to other baselines.<ref type="foot" target="#foot_0">2</ref> </p><p>Contrastive Learning. This is a family of methods for selfsupervised learning, where the learning objective is to assign high similarity to augmented views from the same example <ref type="bibr">(van den Oord et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2020;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b4">Caron et al., 2021)</ref>. MP3 differs as it does not rely on data augmentation as the source of training signal, which gives it much more flexibility. Besides, MP3 does not enforce clustering of the representation for different positions within an input, which makes it not suitable for linear probing tasks. These differences also suggest a possibility of combining MP3 and contrastive learning to achieve the best of both worlds. There has also been attempts combining contrastive learning with predictive tasks <ref type="bibr" target="#b10">(Dangovski et al., 2021)</ref>, which suggests possible ways of combining MP3 with contrastive learning in a similar fashion.</p><p>Position prediction in NLP. In concurrent works, the idea of position prediction has also been explored in the NLP domain <ref type="bibr" target="#b9">(Cui et al., 2022;</ref><ref type="bibr" target="#b3">Brüel-Gabrielsson &amp; Scarvelis, 2022)</ref>. These works, combined with MP3, suggest that position prediction is a promising technique across a wide range of problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>For Vision, our architecture is based on ViTs <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref>. In a nutshell, ViTs divide an image into nonoverlapping patches of a given size (e.g., 16 × 16). A linear projection with shared weights to all image patches to obtain a sequence of image tokens is then applied. Token vectors are additively combined with their respective positional embeddings to form the input sequence. Standard self-attention layers are then applied to process the input sequence.</p><p>For Speech, our architecture is based on the vanilla Transformer. The input to the model is a sequence of frames of 40 mel filterbank cepstral coefficients (MFCCs), computed from 30ms of raw waveforms, strided by 10ms between frames, following <ref type="bibr" target="#b7">(Choi et al., 2019)</ref>. Each frame is transformed by the same linear projection into the dimension of the transformer model (thus, each frame is treated as a 1D patch). A fixed sinusoidal positional embedding <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> is added to these projected representation and the result is fed into an 8 layer Transformer. As with ViTs, we add a learnable "cls" token frame at the beginning of the model input sequence. Compared to Vision, in Speech we have a patch that is 1D rather than 2D as we ignore the structure in the frequency domain. Later in the text, we refer to a frame as a patch in the context of Speech tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Masked Position Prediction Pretraining</head><p>In the pretraining phase, we apply the same patch projection as standard ViTs but remove the positional embeddings from all the patch representations. This results in a set of patch representations. We next randomly select 1 − η fraction of patches as "context patches", where η denotes the masking ratio. We then modify the self-attention layers accordingly, where only the context patches take part in the computation of keys and values; queries are computed for all patches. In other words, we perform cross attention from Table <ref type="table">1</ref>. Overview of datasets and baseline models. The ViT-S and ViT-B architectures are defined according to <ref type="bibr" target="#b4">(Touvron et al., 2021)</ref> all input patches to the context patches. With η &gt; 0, the Transformer needs to formulate a good representation of the input given only a subset of the input patches, while ordering all the input patches. This forces the model to reason about the relationship of the context patches and infer masked patches at the same time. As a byproduct, a high masking ratio effectively reduces the computational cost of the Transformer in the pretraininig phase.</p><p>We attach a linear prediction head after the last attention layer, with input and output dimensions being the feature dimension d and number of patches n, respectively. The outputs of the linear head are passed to Softmax to form a distribution over patch positions. The position prediction loss is obtained with the cross entropy between the position index and the prediction head's outputs. See Figure <ref type="figure" target="#fig_0">1</ref> for an illustration, and Appendix A for a sketch implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervised Finetuning</head><p>After the unsupervised pretraining step, we finetune the network with labels. Specifically, we remove the position prediction head, and attach a linear classifier head after the "cls" token, as in standard ViTs. We also apply randomly initialized (learned) positional embeddings (or fixed sinusoidal) to the patch embeddings, also following standard ViTs. Random masking is disabled during this stage and full self-attention is used. The remaining setting of the finetuning step largely resembles that of the supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>While there has been a lot of interest in scaling Transformers on large datasets in the literature, their performance on small datasets remains under explored. As Transformers tend to overfit easily with pure supervised learning, we believe that it is of great importance to investigate the power of unsupervised pretraining in scarce data settings. In the domain of vision, we experiment with small to medium sized datasets: CIFAR-100 <ref type="bibr" target="#b21">(Krizhevsky et al., 2009)</ref>, Tiny ImageNet 3 and ImageNet-1K <ref type="bibr" target="#b11">(Deng et al., 2009)</ref>. In the Speech domain we did not attempt a full blown application of MP3 to Automatic Speech Recognition because the 3 http://cs231n.stanford.edu/tiny-imagenet-200.zip notion of locations is vague, with the streaming nature of the data. Instead we opted here to show proof of concept by applying MP3 to the keyword spotting task, which is a classification problem on a fixed length snippet of audio.</p><p>We use the Google Speech Commands dataset v1 <ref type="bibr" target="#b34">(Warden, 2018)</ref> and implemented our models using the publicly available implementation of TC-ResNet <ref type="bibr" target="#b7">(Choi et al., 2019)</ref> <ref type="foot" target="#foot_1">4</ref> , keeping their audio preprocessing routines, data splits and other details intact. For each dataset above, we choose a baseline Transformer model configuration, the details of which are summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pretraining and Finetuning on Vision Data</head><p>Implementation details. For CIFAR-100, Tiny ImageNet and ImageNet-1k, both our pretraining and finetuning settings largely follow DeiT <ref type="bibr" target="#b4">(Touvron et al., 2021)</ref>, which uses AdamW <ref type="bibr" target="#b25">(Loshchilov &amp; Hutter, 2017</ref>) optimizer, weight decay of 0.05, drop path <ref type="bibr" target="#b16">(Ghiasi et al., 2018</ref>) rate of 0.1, Ran-dAugment <ref type="bibr" target="#b8">(Cubuk et al., 2020)</ref>, CutMix <ref type="bibr" target="#b37">(Yun et al., 2019)</ref>, MixUp <ref type="bibr" target="#b38">(Zhang et al., 2017)</ref>, Random Erasing <ref type="bibr" target="#b39">(Zhong et al., 2020)</ref>, Repeated Augmentation <ref type="bibr" target="#b18">(Hoffer et al., 2020)</ref> and label smoothing. In the pretraining phase, we do not use CutMix, MixUP, Random Erasing, Repeated Augmentation and label smoothing. The finetuning phase follows exactly the same protocol as the supervised training recipes suggested in <ref type="bibr" target="#b4">(Touvron et al., 2021)</ref>. We search for optimal η for each dataset in the pretraining phase, which is 0.5, 0.8, 0.75 for CIFAR-100, Tiny ImageNet and ImageNet-1K, respectively. The batch size is 256, 512 and 2048, respectively.</p><p>Baselines. On each dataset, the supervised baseline is trained with strong regularizations. We fix the total training epoch to 400 epochs for CIFAR-100 and Tiny ImageNet, and 300 for ImageNet-1K. We also consider two additional supervised training baselines, one without positional embeddings and another with 2D relative position biases <ref type="bibr" target="#b29">(Shaw et al., 2018)</ref>. We also consider two Transformer based selfsupervised pretraining methods, MOCO V3 <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> and MAE <ref type="bibr" target="#b17">(He et al., 2021)</ref>. In both cases, we use the official code bases and search for the optimal hyper parameter for each case (data augmentation, learning rate for MOCO V3; masking ratio and learning rate for MAE). In Table <ref type="table" target="#tab_2">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">POSITION PREDICTION</head><p>Next we examine a Transformer's ability to solve the position prediction task. We show the results for ImageNet-1K where vary the masking ratio in {0, 0.75} and train for 100 epochs. We measure the position prediction accuracy on the validation sets with different test η. The results are shown in Figure <ref type="figure" target="#fig_2">3</ref>. Interestingly, when trained with η = 0, the Transformers are able to solve the task almost perfectly. Large masking ratio η = 0.75 leads to decreasing accuracy as expected, but the accuracy remains decent up to a high masking ratio. This suggests that there is enough information in the input patches alone to recover their corresponding position information.</p><p>In order to understand the behavior of MP3 with large η, we show one example in Figure <ref type="figure" target="#fig_1">2</ref>. Specifically, we obtain a model trained with η = 0.75, and vary η at test time. For each test η, we generate a random set of context patches, and show the reconstructed images with the predicted positions. We see that the model makes sensible reconstructions, even when the overall accuracy is not high (e.g., with test η = 0.75). This suggests the model can learn to reason effectively about the underlying objects given only a small, positionless subset of input patches. More examples can be seen in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">QUANTITATIVE RESULTS</head><p>We report the finetuning accuracy in Table <ref type="table">3</ref> for CIFAR-100 and Table <ref type="table" target="#tab_4">4</ref> for ImageNet-1K. In all our experiments, MP3 significantly improves upon the supervised training Table <ref type="table">3</ref>. Classification results on CIFAR-100 and Tiny ImageNet. We include a strong ResNeXT baseline <ref type="bibr" target="#b35">(Xie et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2021)</ref> as a reference in both cases. For baseline ViT-S, we train three versions with absolute, relative and no positional embeddings. We also compare to MOCO V3 <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> and MAE <ref type="bibr" target="#b17">(He et al., 2021)</ref>  ImageNet-1K for train masking ratios η ∈ {0.00, 0.75}. The number of total positions is 196. For train η = 0, the position prediction task can be solved near perfectly at evaluation masking ratio η = 0 (which is a standard Jigsaw puzzle), and a large η consistently leads to decreasing accuracy. Interesting the converse is true for train η = 0.75, with a patch performance maximum occurring around evaluation η = 0.55. baseline's accuracy, sometimes by a large margin. Note that we do not change the finetuning hyper parameters, compared to the supervised training baseline, and the gain comes completely from effective pretraining.</p><p>Compared to other self-supervised pretraining methods, MP3 achieves comparable results. This is also surprising to some extent, as MP3 does not use or train positional embeddings information in the pretraining phase. We further performed studies of adding zero initialized relative position biases, similar to BEiT <ref type="bibr" target="#b2">(Bao et al., 2021)</ref>, and not using PE during finetuning. Relative position bias consistently improves upon the absolute PE version, though with a small margin. Interestingly, the version of not using PE shows strong performance, outperforming all the supervised training baselines (including ones with relative position biases).</p><p>Finally, on our largest dataset ImageNet-1K, MP3 requires only 100 pretraining epochs to outperform the supervised trainining baseline, where the total number of epochs are equated. Due to the large masking ratio (η = 0.75) and the use of masked attention, this results in an effective reduction of total training costs (see Table <ref type="table" target="#tab_2">2</ref> for efficiency measures). We have also experimented with a larger backbone ViT-L. With 150 epochs of pretraining, we are able to outperform the supervise training baseline by 1 point, as well as MAE pretrained with 200 epochs (number taken from the paper).</p><p>Note that although MP3 does not outperform the state of the art MAE's performance, we believe that MP3 learns complementary representations. To show this, we performed a simple ensembling test by averaging the outputs of an MP3 and MAE finetuned model from Tab 4 (the ones with 83.0% and 82.7% top 1 accuracy, respectively). This results in a strong classifier with 84.0% accuracy, outperforming MAE pretrained with 1600 epochs. This suggests great potential of potentially combining MP3 and MAE and achieve even greater benefits from pretraining.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">ABLATIONS</head><p>Pretraining epochs. We vary the total number of pretraining epochs with everything else fixed, and show the resultant accuracy in Figure <ref type="figure" target="#fig_3">4</ref>. We see that MP3 works well with a small number of epochs (e.g., 100) but consistently benefits from more pretraining.</p><p>Finetuning epochs. For MP3, the position embeddings are not learned or used during pretraining, which suggests that it can potentially benefit from longer finetuning epochs. To see this, we take a ViT-B based MP3 model pretrained at 100 epochs (see Table <ref type="table" target="#tab_4">4</ref>) and vary the finetuning epochs. In Figure <ref type="figure">5</ref>, we see that this is indeed the case. Moreover, MP3 is able to outperform the supervised training baseline with as few as 60 finetuning epochs (which amounts to 160 total training epochs). This corresponds to an approximate 50% reduction on the training time. Masking ratio. We evaluate models pretrained with the same number of epochs ( <ref type="formula">200</ref>) under different masking ratios. Figure <ref type="figure">6</ref> shows that there exists an optimal value that induces the highest finetuning accuracy. Extreme large η leads to notable degradation, which suggests that it is important to train with a reasonably large context token set.</p><p>Patch size. For ViTs, the patch size affects the model's performance. We experimented with two additional patch configurations on CIFAR-100 with the default ViT-S architecture. Figure <ref type="figure" target="#fig_6">7</ref> shows the accuracy for the supervised training baselines and the finetuning results. We see consistent improvements across small and large patch sizes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">VISUALIZING AND UNDERSTANDING ATTENTION</head><p>The improvements demonstrated by MP3 in Section 4 raise two important questions: what are the qualitative properties of the attention mechanism that MP3 learns, and which We observe that, at all layers, MP3 yields heads that are more local, as well as heads that are more global than those found in supervised ViTs. Upon finetuning, head locality becomes more similar to that of a supervised ViT, with early layer locality being much less modified than the locality of later layers. The results for highly local heads at masking ratio η = 0 are illustrated in Figure <ref type="figure" target="#fig_7">8</ref>. For a full unbiased selection and more details, see Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pretraining and Finetuning on Speech Data</head><p>For Google Speech Commands we use a Transformer model with 8 self-attention layers, a dropout of 0.1, feature dimension of 32 and fully connected feedforward layer dimension of 64. The model has around 70K parameters in total to be comparable with the smallest convolutional models from <ref type="bibr" target="#b7">(Choi et al., 2019)</ref>. All pretraining and finetuning models are trained with exactly the same experimental setting as follows. Optimization is done with Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> with a batch size of 256 and early stopping is done based on validation accuracy. Warmup of learning is done for 500 updates with a constant learning rate of 10 −4 . Subsequently the learning rate is increased to 10 −3 and dropped by a factor of 2 every 10k updates. For supervised baselines and finetuning phase we also use label smoothing (=0.1) for regularization and we train the models for 30K updates.</p><p>Compared to Vision, the position prediction task (pretraining step) is very hard -even with η = 0 the top-1 accuracy is only 4%. Nevertheless, a higher value of top-5 accuracy of 11% demonstrates that the model is able to learn to roughly position the patches but cannot resolve it further. This result shows the difference between image and audio data: different granularity and locality properties. To sim- i,j /N , and heads of the fine-tuned models are selected to match those of the MP3 model. MP3 learns strong localizations in layers 6, 10 and 11, despite not having access to any explicit positional encoding. Although localization does occur in early layers of supervised models, we do not see early locality in MP3. We expect this is because of the lack of positional encoding, and a context sufficient for localization has not yet been formed. The attention patterns in layer 12 are unlike those of a standard supervised ViT, and we assume they display behaviour specific to the MP3 task. Under both finetuning scenario, the later layers are dramatically altered, whereas the earlier layers are less changed. The primary difference between when using position encoding is that some localization appears in early layers, whereas in it the absense of positional encoding, there is not. Each attention map is of size 27 × 27, with the class token excluded.</p><p>plify the position prediction task, in contrast to vision, we use η = 0 and, moreover, provide positional information for a randomly chosen 5% of the patches for every sample. Table <ref type="table">6</ref> shows the results achieved with different amounts of pretraining steps of MP3. It can be seen that 5K steps of pretraining is sufficient to improve model accuracy. The test set result for the base validation model above is 94.2% which is 2.3% better than supervised baseline with the same architecture (=91.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented MP3, a simple yet effective method for Transformer pretraining. MP3 differs from most other Transformer and token prediction based pretraining method, which bypasses the complexity of designing sophisticated decoder for dense inputs, such as images and speech. MP3 provides competitive performance on small to medium sized data and model sizes, for both Vision and Speech. In particular, MP3 finetuned without position embbedding outperform strong supervised training baselines. We also demonstrate the intriguing properties of the position prediction task, which is of independent interest from the pretraining setting. We believe that the strong performing permutation invariant Transformers will be of great interest to the robust ML community.</p><p>There are obvious limitations of this work. First of all, MP3 is not designed to produce linearly separable features which many self-supervised methods excel at (e.g., contrastive learning). Also, despite the high level intuition on sorting input tokens and its relation to semantic understanding, it is not entirely clear how the finetuning benefits from such a pretraining objective. Finally, it is also interesting to test MP3 on NLP applications, and we leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>Algorithm 1 illustrates the MP3 implementation in the pretraining mode. We see that only two simple modifications to the forward pass of a standard Transformer model is needed, which results in a more efficient masked Transformer. The loss is also very easy to compute with the help of a linear prediction head.</p><p>Algorithm 1 Pseudo code of MP3 in a PyTorch-like style, where we ignore the 'cls' token for simplicty. In the pretraining phase, we first call mask sample to randomly sample the context tokens; the context token indices are then passed to masked attention in each attention block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer Learning Results</head><p>We further test MP3's ability in Transfer Learning. We obtain a ViT-B model pretrained with 150 epochs with η = 0.75, and finetune it on CIFAR-10 and Stanford Cars <ref type="bibr" target="#b20">(Krause et al., 2013)</ref>   <ref type="table" target="#tab_6">7</ref> shows that MP3 gives competitive performance with supervised and self-supervised models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Layerwise KNN probe</head><p>For self-supervised learning, k-nearest neighbor classification (KNN) is a popular way of testing the linear separability of the pretrained features (which is similar to linear probing). We perform a study on ImageNet-1K, where we vary the masking ratio η in pretraining, and examine each layer's average pooled representation with an KNN classifier. The results are shown in Figure <ref type="figure">9</ref>. We see that all trained layers show significantly higher validation accuracy than a random model.</p><p>There is also a positive correlation between η and the peak performance on KNN classification. The optimal layer also appears in the middle, rather than at the very top. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Visualization</head><p>We also visualize the correlation pattern of the representations within an image, and across images. To do so, we conduct two experiments. In the first one, we compute the Pearson Correlation of the last layer's representation between each position pair within the same image, averaged across the ImageNet-1K validation set. In the second, we compute the correlation between each position pair of two random images. Each experiment results in a correlation matrix of 196 × 196, which is reshaped to a 14 × 14 × (14 × 14) grid. The results are shown in Figure <ref type="figure" target="#fig_0">10</ref>. We see that the representations are biased to their positions. However, there is stronger correlations within the same images than across different ones, which demonstrates that their is an implicit clustering effect of representations within the same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relative attention maps</head><p>Here we present the full (all layers and heads) relative attention maps for MP3 (η = 0.75), finetuning with/without positional encoding, and supervised baseline with positional encoding. The results are shown in Figure <ref type="figure" target="#fig_11">11</ref>. We see that finetuning drastically changes the locality patterns of the last three layers, while lower layers remain similar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ImageNet Reconstruction Visualization</head><p>As performed in Figure <ref type="figure" target="#fig_1">2</ref>, additional images from the ImageNet validation set were used to test the position prediction of a model trained with η = 0.5 (Figure <ref type="figure" target="#fig_1">12</ref>) and η = 0.75 (Figure <ref type="figure" target="#fig_2">13</ref>). Reconstructions were generated by placing each patch in the predicted position, and patches falling in the same position were averaged. Different η was used at test time, ranging in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Illustration of our method MP3 on images. MP3 removes the position information for all tokens (image patches); it then randomly select a subset of tokens as context tokens. A Masked Transformer is used, where in each attention layer only context tokens contribute to the keys and values, and all tokens contribute to the queries. Each token predicts its position with a linear classifier head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An image from the ImageNet validation set (top left corner) and its reconstructed images for a model trained with η = 0.75. Column 1 -4: different η used at test time, ranging in {0, 0.25, 0.5, 0.75}. Row 1: the random context patches, placed in their original locations. Row 2: the unordered inputs to the model, with the context patch tokens outlined in green. Row 3: each patch is placed in the predicted position, and patches falling in the same position are averaged. The content in the reconstructed images are still apparent despite distortions. See Appendix F for additional examples.</figDesc><graphic url="image-21.png" coords="5,104.55,274.67,97.06,97.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Validation accuracy for the position prediction task on ImageNet-1K for train masking ratios η ∈ {0.00, 0.75}. The number of total positions is 196. For train η = 0, the position prediction task can be solved near perfectly at evaluation masking ratio η = 0 (which is a standard Jigsaw puzzle), and a large η consistently leads to decreasing accuracy. Interesting the converse is true for train η = 0.75, with a patch performance maximum occurring around evaluation η = 0.55.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Finetuning accuracy of MP3 on CIFAR-100 (left) and Tiny ImageNet (right) as the pretraining epochs are varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Test top 1 accuracy on ImageNet-1K as the finetunig epochs is varied, with pretraining epochs fixed at 100. MP3 matches the 300 epoch supervised training baseline with as few as 160 total training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The CIFAR-100 finetuning validation accuracy for MP3 (pretrained for 1000 epochs) and the supervised training baselines, as the patch size is varied. MP3 provides consistent improvements under different patch resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Average relative 2D attention maps for (left) MP3, (center) MP3 + finetuning without positional encoding, and (right) MP3 + finetuning with positional encoding. Both fine tuned models are tuned from the same MP3 model (left), which was trained with masking ratio η = 0. The heads of the MP3 model are those with the lowest attention entropy H = −Ep x N i=1 M i=1 α (x) i,j log α (x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>def mask_sample(x, eta): # x: input tokens, shape (batch_size, num_tokens, input_dim) # eta: masking ratio in range [0, 1) # return kv_ind, indices of context tokens, shape (batch_size, num_context_tokens) B, N, D = x.size() M = int(N * eta) # number of context tokens rand_ind = torch.randn(B, N).argsort(dim=1) # generate a random permutation of positions per input kv_ind = rand_ind[torch.arange(B).unsqueeze(1), rand_ind[:, :M]] # get the first M positions per example return kv_ind def masked_attention(x, kv_ind): # x: input tokens, shape (batch_size, num_tokens, input_dim) # kv_ind: indices of context tokens returned by mask_sample, shape (batch_size, num_context_tokens) # return y, output of masked attention B, N, D = x.size() q = query_proj(x) # apply query projection to all tokens k = key_proj(x[torch.arange(B).unsqueeze(1), kv_ind]) v = value_proj(x[torch.arange(B).unsqueeze(1), kv_ind]) # apply key and value projection to context tokens y = multi_head_attention(q, k, v) # perform standard multi-head attention return y def mp3_loss(x): # x: output of the Transformer backbone, shape (batch_size, num_tokens, input_dim) # return a scalar loss B, N, D = x.size() targets = torch.arange(N).repeat(B) # the targets is each patch's original position # apply a linear projection to get the predictions pred = linear_head(x) # shape (batc_size, num_tokens, num_tokens) loss = cross_entropy(pred, targets) # classification across all positions return loss def forward(x, eta): # x: input tokens (e.g., image patches), eta: masking ratio. # x = x + pos_embed --we do not use position embeddings kv_ind = mask_sample(x, eta) x = masked_transformer(x, kv_ind) # with masked_attention loss = mp3_loss(x) return loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>dataset, which have 50K training examples in 10 classes and 8144 training examples in 196 classes, respectively. We compare with ViT-B, DeiT-B and DINO, all trained with the same architecture. Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure 9. KNN classification accuracy on ImageNet-1K, as the pretraining η and the target layer are varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Average relative attention visualization for MP3 pretrained and finetuned models, compared with the supervised training baseline. Top left: MP3 pretrained; top right: MP3 finetuned with PE; bottom left: MP3 finetuned without PE; bottom right: supervised baseline with PE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>Figure 14. Patches from two images from the ImageNet validation set (top left images) were shuffled together and separated into two distinct sets. Rows 1-4: positions were predicted using the shuffled set of patches with different η used at test time, ranging in {0, 0.25, 0.5, 0.75}. Columns 3 &amp; 5: the unordered inputs to the model, with the context patch tokens outlined in green. Columns 4 &amp; 6: each patch was placed in the predicted position, and patches falling in the same position were averaged. Coherent dog-like animal can be seen in the final reconstructions with the background placed around the dog.</figDesc><graphic url="image-1236.png" coords="17,79.74,440.30,75.09,75.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Dataset</cell><cell>Input size</cell><cell cols="2">#Examples #Classes</cell><cell>Model config</cell><cell cols="3">Patch size Patch stride #Positions</cell></row><row><cell>CIFAR-100 Tiny ImageNet ImageNet-1K Google Speech Commands</cell><cell>32 × 32 64 × 64 224 × 224 1s</cell><cell>50K 100K 1.3M 22246</cell><cell>100 200 1K 12</cell><cell>ViT-S ViT-B ViT-B 8 layer Transformer</cell><cell>4 × 4 8 × 8 16 × 16 30ms</cell><cell>4 8 16 10ms</cell><cell>64 64 196 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Training time and memory efficiency for MP3, MAE and the ViT-B baseline, while varying the masking ratio η. MP3 has favorable speed and memory efficiency than both MAE and ViT-B in most settings.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Time (Seconds / Iter)</cell><cell></cell><cell cols="2">Memory (GB / Batch)</cell></row><row><cell>η</cell><cell>0.3</cell><cell>0.5</cell><cell>0.75 0.9</cell><cell>0.3</cell><cell>0.5</cell><cell>0.75 0.9</cell></row><row><cell>MAE MP3 ViT-B</cell><cell cols="6">OOM 0.57 0.47 0.41 OOM 35.1 28.0 24.4 0.52 0.51 0.46 0.44 30.0 27.4 24.2 22.3 0.67 33.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. MP3 achieves much better results than the supervised learning baseline ViT-S, and is comparable to MOCO V3 and MAE with the same number of pretraining epochs. MP3 without PE achieves surprisingly competitive results in both cases.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>PT epochs</cell><cell>PE</cell><cell>CIFAR-100 Acc Tiny ImageNet Acc</cell></row><row><cell></cell><cell cols="2">ResNeXT</cell><cell></cell><cell>0</cell><cell>conv</cell><cell>82.7</cell><cell>72.2</cell></row><row><cell></cell><cell cols="3">ViT-S Baseline ViT-S Baseline ViT-S Baseline</cell><cell>0 0 0</cell><cell>absolute 2D relative none</cell><cell>73.4 75.0 64.6</cell><cell>57.6 59.4 60.0</cell></row><row><cell></cell><cell cols="2">MOCO V3 MAE</cell><cell></cell><cell>2K 2K</cell><cell>2D absolute 2D absolute</cell><cell>83.3 84.5</cell><cell>73.4 73.7</cell></row><row><cell></cell><cell>MP3 MP3 MP3</cell><cell></cell><cell></cell><cell>2K 2K 2K</cell><cell>absolute 2D relative none</cell><cell>84.0 84.2 82.6</cell><cell>72.8 73.2 68.2</cell></row><row><cell>0.0</cell><cell>0.2</cell><cell>0.4 Evaluation</cell><cell>0.6</cell><cell>0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Classification results on ImageNet-1K. MP3 outperforms the supervised training ViT-B baseline with the same number of total training epochs, which is less overall training cost due to the efficiency of the pretraining phase. Remarkably, MP3 finetuned without any positional information outperforms the full ViT model. MP3's finetuning performance is on par with competitive methods, while with much fewer pretraining epochs.</figDesc><table><row><cell>Method</cell><cell cols="2">PT Epochs FT Epochs</cell><cell>PE</cell><cell>Acc</cell></row><row><cell>ViT-B (Touvron et al., 2021) ViT-B (Touvron et al., 2021) ViT-B DINO (Caron et al., 2021) ViT-B MOCO V3 (Chen et al., 2021) ViT-B BEiT (Bao et al., 2021) ViT-B MAE (He et al., 2021) ViT-B MAE (He et al., 2021) ViT-B MP3 ViT-B MP3</cell><cell>0 0 300 300 800 1600 150 100 100</cell><cell cols="3">300 300 300 150 2D absolute 83.2 absolute 81.8 none 79.1 absolute 82.8 100 2D relative 83.2 100 2D absolute 83.6 150 2D absolute 82.7 150 absolute 83.0 300 none 81.9</cell></row><row><cell>ViT-L (He et al., 2021) ViT-L MAE (He et al., 2021) ViT-L MAE (He et al., 2021) ViT-L MP3</cell><cell>0 200 1600 150</cell><cell cols="3">200 50 2D absolute 83.3 absolute 82.6 50 2D absolute 85.1 150 absolute 83.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with other baselines on Speech Commands (test accuracy %).</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell cols="4">TC-ResNet8 (Choi et al., 2019) Transformer Transformer + MP3</cell><cell>96.1 91.9 94.2</cell></row><row><cell cols="5">Table 6. Validation accuracy (%) after finetuning (η = 0) with dif-</cell></row><row><cell cols="5">ferent amount of pretraining (PT) updates on Speech Commands</cell></row><row><cell cols="5">with 0.05 fraction of the patches being provided positional infor-</cell></row><row><cell>mation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">1K PT 5K PT 10K PT 25K PT 50K PT</cell></row><row><cell>91.7</cell><cell>93.3</cell><cell>94.2</cell><cell>93.1</cell><cell>93.8</cell></row><row><cell cols="4">aspects are preserved under finetuning?</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Transfer learning result on CIFAR-10 and Stanford Cars datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">ViT (Dosovitskiy et al., 2021) DeiT-B (Touvron et al., 2021) DINO (Caron et al., 2021) MP3</cell></row><row><cell>CIFAR-10 Stanford Cars</cell><cell>98.1 -</cell><cell>99.1 92.1</cell><cell>99.1 93.0</cell><cell>98.0 91.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Note that for Speech some positions are needed to be addedotherwise, the pretraining task is too hard for the model to solve.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://github.com/hyperconnect/TC-ResNet</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>{0, 0.25, 0.5, 0.75}. In Figure <ref type="figure">12</ref> with η = 0.5, the model can accurately predict majority of the patches for η &lt; 0.75. In Figure <ref type="figure">13</ref> with η = 0.75, the patches were not placed in the absolute true location, but they were placed in positions that still made sense semantically.</p><p>To visualize the role of the context patches with the query patches, patches from two different images in the ImageNet validation set were shuffled together and separated into two distinct sets. A random subset of the patches were used as context patches to predict positions for both context and query patches. The final reconstructions are visualized in Figures. 14 and 15. In Figure <ref type="figure">14</ref>, the two original images of dogs look visually similar in content, composition, and color distribution.</p><p>The resulting images created a dog-like animal in the center of the image. In Figure <ref type="figure">15</ref>, two different contents were mixed together: boat in one image and a hot air balloon in another. Similar patches were grouped together creating coherent boat-like structure in one part of the image and a balloon-like structure in another part. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Essa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Bert pre-training of image transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Relative position prediction as pre-training for text encoders</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brüel-Gabrielsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scarvelis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01145</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<title level="m">Temporal convolution for real-time keyword spotting on mobile devices. Proc. Interspeech</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Pert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06906</idno>
		<title level="m">Pre-training bert with permuted language model</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soljačić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00899</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Equivariant contrastive learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/papers/N/N19/N19-1423/" />
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Position information in transformers: An overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11090</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12770</idno>
		<title level="m">Skip-Clip: Self-Supervised Spatiotemporal Representation Learning by Future Clip Order Ranking</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dropblock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12890</idno>
		<title level="m">A regularization method for convolutional networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
				<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attentionbased permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Boosting discriminative visual representation learning with scenarioagnostic mixup</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15454</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10497</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName><forename type="first">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3100" to="3114" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06644</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf.vandenOord" />
	</analytic>
	<monogr>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limitedvocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno>CoRR, abs/1804.03209</idno>
		<ptr target="http://arxiv.org/abs/1804" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10334" to="10343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond Empirical Risk Minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
