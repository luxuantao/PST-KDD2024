<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Global Memory for Document-level Argument Extraction</title>
				<funder ref="#_HsYZN5J">
					<orgName type="full">U.S. DARPA KAIROS</orgName>
				</funder>
				<funder ref="#_k5jHUSJ #_ZYES7YQ">
					<orgName type="full">U.S. DARPA AIDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
							<email>xinyadu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<email>hengji@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Global Memory for Document-level Argument Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An event is a specific occurrence involving participants <ref type="bibr">(people, objects, etc.)</ref>. Understanding events in the text is necessary for building machine reading systems, as well as for downstream tasks such as information retrieval, knowledge base population, and trend analysis of real-life world events <ref type="bibr" target="#b25">(Sundheim, 1992)</ref>. Event Extraction has long been studied as a local sentence-level task <ref type="bibr" target="#b9">(Grishman and Sundheim, 1996;</ref><ref type="bibr">Ji and Grishman, 2008b;</ref><ref type="bibr" target="#b8">Grishman, 2019;</ref><ref type="bibr" target="#b20">Lin et al., 2020)</ref>. This has driven researchers to focus on developing approaches for sentence-level predicate-argument extraction. This is problematic when events and their arguments spread across multiple sentences -in 1 Our code and resources are available at https:// github.com/xinyadu/memory_docie for research purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? [S3]</head><p>After having a shootout with several [policemen including Collin] last Thursday, both [Tamerlan Tsarnaev] and his younger brother <ref type="bibr">[Dzhokhar]</ref> were captured a day later. ? [S6] Two week ago, in Boston, authorities on Wednesday reopened [Boylston Street], the city thoroughfare where the explosion occurred near the finish line of the race.</p><p>[S7] ? a memorial service for campus policeman Sean Collin, who authorities say the brothers shot to death three days after the bombings ? Trigger "captured" Jailer "policemen including Collin"</p><p>Detainee "Tamerlan Tsarnaev", "Dzhokhar" real-world cases, events are often written throughout a document. <ref type="foot" target="#foot_0">2</ref>In Figure <ref type="figure" target="#fig_0">1</ref>, the excerpt of a news article describes two events in the 3rd sentence (an arrest event triggered by "captured") and the 6th sentence (an attack event triggered by "explosion"). S6 on its own contains little information about the arguments/participants of the explosion event, but together with the context of S3 and S7, we can find the informative arguments for the ATTACKER role. In this work, we focus on the informative argument extraction problem, which is more practical and requires much a broader view of crosssentence context <ref type="bibr" target="#b19">(Li et al., 2021)</ref>. For example, although "the brothers" also refers to "Tamerlan T."</p><p>and "Dzhokhar" (and closer to the trigger word), it should not be extracted as an informative argument.</p><p>In recent years, there have been efforts focusing on event extraction beyond sentence boundaries with end-to-end learning <ref type="bibr" target="#b7">(Ebner et al., 2020;</ref><ref type="bibr" target="#b4">Du, 2021;</ref><ref type="bibr" target="#b19">Li et al., 2021)</ref>. Most of the work still focuses on modeling each event independently <ref type="bibr" target="#b19">(Li et al., 2021)</ref> and ignores the global context partially because of the pretrained models' length limit and their lack of attention for distant context <ref type="bibr" target="#b14">(Khandelwal et al., 2018)</ref>. <ref type="bibr" target="#b6">Du et al. (2021)</ref> propose to model dependency between events directly via the design of generation output format, yet it is not able to handle longer documents with more eventswhereas in real-world news articles there are often more than fifteen inter-related events (Table <ref type="table" target="#tab_2">2</ref>).</p><p>In addition, previous work often overlooks the consistency between extracted event structures across the long document. For example, if one person has been identified as a JAILER in an event, it's unlikely that the same person is an ATTACKER in another event in the document (Figure <ref type="figure" target="#fig_0">1</ref>), according to world event knowledge <ref type="bibr" target="#b23">(Sap et al., 2019;</ref><ref type="bibr" target="#b28">Yao et al., 2020)</ref>.</p><p>In this paper, to tackle these challenges and have more consistent/coherent extraction results, we propose a document-level memory-enhanced training and decoding framework (Figure <ref type="figure" target="#fig_1">2</ref>) for the problem. It can leverage relevant and necessary context beyond the length constraint of end-to-end models, by using the idea of a dynamic memory store. It helps the model leverage previously generated/extracted event information during both training (implicitly) and during test/decoding (explicitly). More specifically, during training, it retrieves the most similar event sequence in the memory store as additional input context to mode. Plus, it performs constrained decoding based on the memory store and our harvested global knowledge-based argument pairs from the ontology.</p><p>We conduct extensive experiments and analysis on the WIKIEVENTS corpus and show that our framework significantly outperforms previous methods either based on neural sequence labeling or text generation. We also demonstrate that the framework achieves larger gains over baseline non memory-based models as the number of events grows in the document, and it is more robust to manually designed adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>In this work, we focus on the challenging problem of extracting informative arguments of events<ref type="foot" target="#foot_1">3</ref> from the document. Each event consists of (1) a trigger expression which is a continuous span in the document, it is of a type E which is predefined in an ontology; (2) and a set of arguments {arg 1 , arg 2 , ...}, each of them has a role predefined in the ontology, for event type E. In the annotation guideline/ontology, the "template" that describes the connections between arguments of the event type is also provided. For example, when E is Arrest, its corresponding arguments to be extracted should have roles: JAILER (&lt;arg1&gt;), DETAINEE (&lt;arg2&gt;), CRIME (&lt;arg3&gt;), PLACE (&lt;arg4&gt;). Its description template is: &lt;arg1&gt; arrested or jailed &lt;arg2&gt; for &lt;arg3&gt; crime at &lt;arg4&gt; place Given a long news document Doc = {..., &lt;Trg1&gt;, ..., x i , ..., &lt;Trg2&gt;, ..., x n } with given event triggers, our goal is to extract all the informative argument spans to fill in the role of E1, E2, etc. For the example piece in Figure <ref type="figure" target="#fig_0">1</ref>, E1 is Arrest (triggered by &lt;Trg1&gt; "captured") and E2 is Attack-Detonate (&lt;Trg2&gt; is "explosion").</p><p>The ontology is constructed by the DARPA KAIROS project<ref type="foot" target="#foot_2">4</ref> for event annotation. It defines 67 event types in a three-level hierarchy, which is richer than the ACE05 ontology with only 33 event types for sentence-level extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our memory-enhanced neural generation-based framework (Figure <ref type="figure" target="#fig_1">2</ref>) for extracting informative event arguments from the document. Our base model is based on a sequenceto-sequence pretrained language model for text generation. We first introduce how we leverage previously extracted events as additional context for training the text generation-based event extraction model to help the model automatically capture event dependency knowledge (Section 3.1). To explicitly help the model satisfy the global event knowledge-based constraints (e.g., it is improbable that one person would be JAILER in event A and then ATTACKER in event B), we propose a dynamic &lt;S&gt; [Police] arrested or jailed <ref type="bibr">[Dzhokhar]</ref> for &lt;arg&gt; crime at Boston place &lt;/S&gt; &lt;S&gt; &lt;Template for Attack-Detonate Event&gt; &lt;/S&gt; ? Meanwhile, ? authorities on Wednesday reopened Boylston Street, the city thoroughfare where the explosions occurred near the finish line of the race. ? <ref type="bibr">(m, x)</ref> [Police] arrested or jailed <ref type="bibr">[Dzhokhar]</ref> for &lt;arg&gt; crime at Boston place [Tamerlan Tsarnaev] died at &lt;arg&gt; place from &lt;arg&gt; medical issue .. decoding process with world knowledge-based argument pair constraints (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory-enhanced Generation Model for Argument Extraction</head><p>Following <ref type="bibr" target="#b19">Li et al. (2021)</ref>, the main model of our framework is based on the pretrained encoderdecoder model BART <ref type="bibr" target="#b17">(Lewis et al., 2020)</ref>. The intuition behind using BART for the extraction task is that it is pre-trained as a denoising autoencoder -reconstruct the original input sequence. This fits our objective of extracting argument spans from the input document because the extracted arguments' tokens are from the input sequence. The generation model takes (1) context: the concatenation of the piece of text x (of document D) containing the current event trigger 5 and the event type's corresponding template in the ontology;</p><p>(2) memory store m: of previously extracted events of the same document D, as input, and learns a distribution p(y|x, m) over possible outputs y. The ground truth sequence y is a sequence of a template where the placeholder &lt;arg&gt;s are filled by 5 Up to the maximum length limit of the pre-trained model.</p><p>the gold-standard argument spans of the current event.<ref type="foot" target="#foot_3">6</ref> </p><p>p(y|x, m)</p><formula xml:id="formula_0">= N i p ( y i |y 1:i-1 , x, m)<label>(1)</label></formula><p>To be more specific on building the dependency between events across the document, we use the most relevant event in the memory store m as additional context, instead of the entire memory store. To retrieve the most relevant "event" (i.e., a generated sequence) from the memory store m = {m 1 , m 2 , ...}, we use S-BERT <ref type="bibr" target="#b22">(Reimers and Gurevych, 2019)</ref> for dense retrieval (i.e., retrieval with dense representations provided by NN). S-BERT is a modification of the BERT model <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> that uses siamese and triplet network structures to obtain semantically meaningful embeddings for text sequences. We can compare the distance between two input sequences with cosinesimilarity in an easier and faster way. Given a current input document piece x, we encode all of the previously generated event sequences in the memory store and x. Then we calculate the similarity scores with vector space cosine-similarity and normalization:</p><formula xml:id="formula_1">score(m i |x) = exp f (x, m i ) m i ?m exp f (x, m i ) f (x, m i ) = Embed(x) T Embed(m i )</formula><p>Afterwards, we select the m i with the highest similarity score:</p><formula xml:id="formula_2">m R = arg max i score(m i |x)</formula><p>To summarize, the input sequence for the memory-enhanced model consists of the retrieved generated event sequence (m R ), the template for the current event type (T ) -provided by the ontology/dataset, and the context words from the document (x 1 , ...,</p><formula xml:id="formula_3">x n ): &lt;S&gt; m R 1 , m R 2 , ..., &lt;/S&gt; &lt;S&gt; T 1 , T 2 , ... &lt;/S&gt; x 1 , x 2 , ..., x n [EOS]</formula><p>During training time, the memory store consists of gold-standard event sequences -while at test time, it contains real generated event sequences. The training objective is to minimize the negative log likelihood over all ((x, m R , T ), y) instances. Since we fix the parameters from S-BERT, the retrieval module's parameters are not updated during training. Thus the training time cost of our memory-based training is almost the same to the simple generation-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constrained Decoding with Global Knowledge-based Argument Pairs</head><p>The constrained/dynamic decoding is an important stage in our framework. We first harvest a number of world knowledge-based event argument pairs that are probable/improbable of happening with the same entity being the argument. For example, (&lt;Event Type: Arrest, Argument Role: JAILER&gt; | &lt;Event Type: Attack-Detonate, Argument Role: ATTACKER&gt;) is an improbable pair. In the framework (Figure <ref type="figure" target="#fig_1">2</ref>), they are called "argument pairs". Then based on the argument pairs constraints, the dynamic decoding is conducted throughout the document -if one entity is decoded in an event in the earlier part of the document, it should not be decoded later in another event if the results are incompatible with the improbable argument pairs. </p><formula xml:id="formula_4">for i ? 1 to |O| do 4 for j ? i + 1 to |O| do 5 cnt(i, j) = count # of (Ei, Ej)</formula><p>co-occurrence in the training documents;</p><formula xml:id="formula_5">6 if cnt(i, j) == 0 then continue; // Enumerate argument pairs 7 for A i k ? Ei args (A i 1 , A i 2 , ...) do 8 for A j h ? Ej args (A j 1 , A j 2 , ...) do 9 if entity_type(A i k )! = entity_type(A j h ) then continue; 10 cnt_args(A i k , A j h ) = count # of (A i k , A j h )</formula><p>being the same entity in the training set documents; Harvesting Global Knowledge-based Argument Pairs from the Ontology We first run an algorithm to automatically harvest all candidate argument pairs (Algorithm 1). Basically, we</p><formula xml:id="formula_6">11 if cnt_args(A i k ,A j h ) cnt(i,j) &gt; 0.001 then impro_arg_pairs.add((&lt; Ei, A i k &gt; | &lt; Ej, A j h &gt;)); 12 else 13 pro_arg_pairs.add((&lt; Ei, A i k &gt; | &lt; Ej, A j h &gt;))</formula><p>? First enumerate all possible event type pairs, and count how many times they co-occur in the training set (Line 2-6).</p><p>? Then we enumerate all possible argument types pairs that share the same entity type from the ontology (e.g., argument ORGANIZATION (ORG) and argument VICTIM (PER) don't have the same entity type), and count how many times both of the args are of the same entity in training docs (e.g., "Dzhokhar" are both DETAINEE and AT-TACKER in two events in Figure <ref type="figure" target="#fig_0">1</ref>) (Line 7-11).</p><p>? Finally we add into the set of probable argument pairs, whose normalized score is above a threshold (99% of the candidate arguments with nonzero score); and the rest into the set of improba- After automatic harvesting, since there is noise in the dataset as well as cases not covered, we conduct a human curation process to mark certain improbable argument pairs as probable, based on world knowledge. Finally, we obtain 1,568 improbable argument pairs and 687 probable pairs.  <ref type="figure" target="#fig_3">3</ref>). During decoding the arguments of later events in the document, assuming we are at a time step t for generating the sequence for event E i , to generate token y t , we first determine the argument role (A k ) it corresponds to. Then we search through the memory store if there are extracted entities e that have argument role A h , where &lt; A k , A h &gt; is an improbable argument pair. Then when decoding to token at time step t, we decrease the probability (after softmax) of generating/extracting tokens in entity e according to the improbable argument pair rule. Compared to decreasing the probability of extracting certain conflicting entities, we are more reserved in utilizing the probable argument pairs, only if the same entity has been assigned the argument role for more than 5 times in the document, we are increasing the probability of extracting the same entity (generat-ing the token of the entity) for the corresponding argument role (the most co-occurred).</p><p>After the generation process for the current event, we add the newly generated event sequence (extracted arguments) back into the memory store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We conduct evaluations on the newly released WIKIEVENTS dataset <ref type="bibr" target="#b19">(Li et al., 2021)</ref>. As compared to the ACE05<ref type="foot" target="#foot_4">7</ref> sentence-level extraction benchmark, WIKIEVENTS focuses on annotations for informative arguments and for multiple events in the document-level event extraction setting, and is the only benchmark dataset for this purpose to now. It contains real-world news articles annotated with the DARPA KAIROS ontology. As shown in the dataset paper, the distance between informative arguments and event trigger is 10 times larger than the distance between local/uninformative arguments (including pronouns) and event triggers. This demonstrates more needs for modeling long document context and event dependency and thus it requires a good benchmark for evaluating our proposed models. The statistics of the dataset are shown in Table <ref type="table" target="#tab_2">2</ref>. We use the same data split and preprocessing step as in the previous work. As for evaluation, we use the same criteria as in previous work. We consider an argument span to be correctly identified if its offsets match any of the gold/reference informative arguments of the current event (i.e., argument identification); and it is correctly classified if its semantic role also matches (i.e., argument classification) <ref type="bibr" target="#b18">(Li et al., 2013)</ref>.</p><p>To judge whether the extracted argument and the gold-standard argument span match, since the exact match is too strict that some correct candidates are considered as spurious (e.g., "the 22 policemen" and "22 policemen" do not match under the exact match standard). Following <ref type="bibr" target="#b11">Huang and Riloff (2012)</ref> F1 (Head F1). We also report performance under a more lenient metric "Coref F1": the extracted argument span gets full credit if it is coreferential with the gold-standard arguments (Ji and Grishman, 2008a). The coreference links information between informative arguments across the document are given in the gold annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We compare our framework to a number of competitive baselines. <ref type="bibr" target="#b24">(Shi and Lin, 2019</ref>) is a popular baseline for semantic role labeling (predicateargument prediction). It performs sequence labeling based on automatically extracted features from BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> and uses Conditional Random Fields <ref type="bibr" target="#b16">(Lafferty et al., 2001)</ref> for structured prediction (BERT-CRF). <ref type="bibr" target="#b19">Li et al. (2021)</ref> propose to use conditional neural text generation model for the document-level argument extraction problem, it handles each event in isolation (BART-Gen).</p><p>For our proposed memory-enhanced training with retrieved additional context, we denote it as Memory-based Training. We also present the argument pairs constrained decoding results separately to see both components' contributions. 8  In Table <ref type="table" target="#tab_3">3</ref>, we present the main results for the document-level informative argument extraction. The score for argument identification is strictly higher than argument classification since it only requires span offset match. We observe that:</p><p>? The neural generation-based models (BART-Gen and our framework) are superior in this document-level informative argument extraction problem, as compared to the sequence labeling-based approaches. Plus, generation-8 All significance tests for F-1 are computed using the paired bootstrap procedure of 5k samples of generated sequences <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2012)</ref> based methods only require one pass as compared to span enumeration-based methods <ref type="bibr" target="#b26">(Wadden et al., 2019;</ref><ref type="bibr" target="#b5">Du and Cardie, 2020)</ref>.</p><p>? As compared to the raw BART-Gen, with our memory-based training -leveraging previously closest extracted event information substantially helps increase precision (P) and F-1 scores, with smaller but notable improvement in recall especially under Coref Match.</p><p>? With additional argument pair constrained decoding, there is an additional significant improvement in precision and F-1 scores. This can be mainly attributed to two factors: (I) during constrained decoding, we relied more on "improbable arg. pairs" as a checklist to make sure that the same entity not generated for conflicting argument roles in the same document, and only utilize very few top "probable arg. pairs" for promoting the decoding for frequently appearing entities; (II) If an entity has been decoded in previous event A by mistake then under the argument pair rule, it will not be decoded in event B even if it correctwhich might hurt the recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to Adversarial Examples</head><p>To test how the models react to specially designed adversarial examples, we select a quarter of documents from the original test set, and add one more adversarial event into each of them by adding a few new sentences. The additional event is designed to "attract" the model to make mistakes that are against our global knowledge-based argument pair rules.<ref type="foot" target="#foot_5">9</ref> An excerpt for one example:</p><p>Tandy, then 19, talks to his close friend, Stephen Silva, about ... Tandy and Silva both died as lifeguards together at the Harvard pool. Later a kid was killed by a Stephen Silva-lookalike guy.</p><p>In this example, we know "Stephen Silva" died in the second event "Life.Die" triggered by died. Although it is also mentioned in the last sentence, "Stephen Silva" should not be extracted as the KILLER. In Table <ref type="table" target="#tab_4">4</ref>, we summarize the F-1 scores of argument classification models. Firstly we see on the adversarial examples, the performance scores all drop as compared to the normal setting (Table <ref type="table" target="#tab_3">3</ref>), proving it's harder to maintain robustness in this setting. Our best model with argument pair constrained decoding outperforms substantially both BART-Gen and our memory-based training model. The gap is larger than the general evaluation setting, which shows the advantage of explicitly enforcing the reasoning/constraint rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further Analysis</head><p>In this section, we further provide more insights with quantitative and qualitative analysis, as well as error analysis for the remaining challenges.</p><p>Influence of Similarity-based Retrieval In Table 5, we first investigate what happens when our similarity-based retrieval module is removed -we find that the F-1 scores substantially drop. There's also a drop of scores across metrics when we retrieve a random event from the memory store. It is interesting that the model gets slightly better performance with random memory than not using any retrieved/demonstration sequences. This corresponds to the findings in other domains of NLP on how demonstrations lead to performance gain when using pre-trained language models (especially in the few-shot learning setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Length and # of Events</head><p>In Figure <ref type="figure" target="#fig_4">4</ref>, we examine how performances change as the document length and the number of events per document grow. First we observe that as the document length grows, challenges grow for both the baseline and our framework (F-1 drops from over 70% to around 55%). While our framework maintains a larger advantage when document is longer than 250 words.</p><p>As the number of events per document grows (from &lt;=8 to around 25), our model's performance is not affected much (F-1 all over 60%). While the baseline system's F-1 score drops to around 50%. Qualitative Analysis We present a couple of representative examples (Table <ref type="table" target="#tab_7">6</ref>). In the first example, for the event triggered by wounds, it's hard to find the VICTIM argument "Ahmad Khan Rahimi" since it's explicitly mentioned far before the current sentence. But with retrieved additional context, both our framework variants are able to extract the full name correctly. In the second example, "Cuba" was mentioned in two sentences with two events (Impede event triggered by sidesteps and Arrest   Finally we examine deeper the example predictions and categorize reasons for errors into the following types: (1) Challenge to obtain an accurate boundary of the argument span. In the example excerpt "On Sunday, a suicide bombing in the southeastern province of [Logar] left eight ...", our model extracts "southeastern province" as PLACE. Similarly in "... were transported to [Kabul] city..", our model extracts "city" as DESTINATION. In both cases the model gets no credit. To mitigate this problem, models should be able to identify certain noun phrase boundaries with external knowledge. Plus, the improvement of data annotation and evaluation is also needed -the model should get certain credit though the span does not overlap but related to the gold argument. (2) Long distance dependency and deeper context understanding. In news, most of the contents are written by the author while certain content is cited from participants. While models usually do not distinguish the difference and consider the big stance difference. In the excerpt "Bill Richard, whose son, Martin, was the youngest person killed in the bombing, said Tsarnaev could have backed out ... Instead, ::::::: Richard ::::: said, :: he :::::: chose ::::: hate. ::: he ::::: chose ::::::::::: destruction.</p><p>::: He ::::: chose :::::: death. :: ...", the full name of the informative argument ("D. Tsarnaev") was mentioned at the very beginning of the document. Although our model can leverage previously decoded events, it is not able to fully understand the speaker's point of view and misses the full KILLER argument span.</p><p>Event Knowledge There has been work on acquiring event-event knowledge/subevent knowledge with heuristic-based rules or crowdsourcingbased methods. <ref type="bibr" target="#b23">Sap et al. (2019)</ref> propose to use crowdsourcing for obtaining if-then relations between events. <ref type="bibr" target="#b1">Bosselut et al. (2019)</ref> use generative language models to generate new event knowledge based on crowdsourced triples. <ref type="bibr" target="#b28">Yao et al. (2020)</ref> propose a weakly-supervised approach to extract sub-event relation tuples from the text. In our work, we focus on harvesting knowledge-based event argument pair constraints from the predefined ontology with training data co-occurrence statistics. Plus, the work above on knowledge acquisition has not investigated explicitly encoding the knowledge/constraints for improving the performance of models of document-level event extraction related tasks.</p><p>Document-level Event Extraction Event extraction has been mainly studied under the document-level setting (the template filling tasks from the MUC conferences <ref type="bibr" target="#b9">(Grishman and Sundheim, 1996)</ref>) and the sentence-level setting (using the ACE data <ref type="bibr" target="#b3">(Doddington et al., 2004)</ref> and BioNLP shared tasks <ref type="bibr" target="#b15">(Kim et al., 2009)</ref>). In this paper, we focus on the document-level event argument extraction task which is a less-explored and challenging topic <ref type="bibr" target="#b6">(Du et al., 2021;</ref><ref type="bibr" target="#b19">Li et al., 2021)</ref>. To support the progress for the problem, <ref type="bibr" target="#b7">Ebner et al. (2020)</ref> built RAMS dataset, and it contains annotations for cross-sentence arguments but for each document it contains only one event. Later <ref type="bibr" target="#b19">Li et al. (2021)</ref> built the benchmark WIKIEVENTS with complete event annotations for each document. Regarding the methodology, neural text generationbased models have been proved to be superior at this document-level task <ref type="bibr" target="#b10">(Huang et al., 2021;</ref><ref type="bibr" target="#b6">Du et al., 2021;</ref><ref type="bibr" target="#b19">Li et al., 2021)</ref>. But they are still limited by the maximum length context issue and mainly focus on modeling one event at a time. <ref type="bibr" target="#b27">Yang and Mitchell (2016)</ref> proposed a joint extraction approach that models cross-event dependencies -but it's restricted to events co-occurring within a sentence and only does trigger typing. In our framework, utilizing the memory store can help better capture global context and avoid the document length constraint. Apart from event extraction, in the future, it's worth investigating how to leverage the global memory idea for other document-level IE problems like (N -ary) relation extraction <ref type="bibr" target="#b21">(Quirk and Poon, 2017;</ref><ref type="bibr" target="#b29">Yao et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this work, we examined the effect of global document-level "memory" on informative event argument extraction. In the new framework, we propose to leverage the previously extracted events as additional context to help the model learn the dependency across events. At test time, we propose to use a dynamic decoding process to help the model satisfy global knowledge-based argument constraints. Experiments demonstrate that our approach achieves substantial improvements over prior methods and has a larger advantage when document length and events number increase. For future work, we plan to investigate how to extend our method to multi-document event extraction cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Document-level event argument extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[Figure 2 :</head><label>2</label><figDesc>Figure 2: Our Framework for Memory-enhanced Training and Decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Constrained/Dynamic Decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of doc length and events # per doc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Automatic Harvesting Argument Pairs from the Event Ontology Input : Event Ontology O, consisting of |O| events' information. For each event Ei ? O, it has a set of argument roles (A i 1 , A i 2 , ...).</figDesc><table><row><cell>Output :A set of (&lt;Event Type, Argument Role&gt; |</cell></row><row><cell>&lt;Event Type, Argument Role&gt;) pairs with</cell></row><row><cell>"probable" or "improbable" denotation.</cell></row><row><cell>1 impro_arg_pairs ?-{};</cell></row><row><cell>2 pro_arg_pairs ?-{};</cell></row><row><cell>// Enumerate event type pairs</cell></row><row><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Harvested Argument Pairs.</figDesc><table><row><cell cols="2"># pairs with global</cell><cell># pairs after</cell></row><row><cell cols="2">co-occurrence stats</cell><cell>human curation</cell></row><row><cell>improbable</cell><cell>1,855</cell><cell>1,568</cell></row><row><cell>probable</cell><cell>400</cell><cell>687</cell></row><row><cell cols="3">Dynamic Decoding Process During the decod-</cell></row><row><cell cols="3">ing process, we keep an explicit data structure in</cell></row><row><cell cols="3">the memory store, to record what entities have</cell></row><row><cell cols="3">been decoded and what argument roles they are</cell></row><row><cell>assigned to (Figure</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics</figDesc><table><row><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Documents</cell><cell>206</cell><cell>20</cell><cell>20</cell></row><row><cell>Sentences</cell><cell>5262</cell><cell>378</cell><cell>492</cell></row><row><cell>Avg. number of events</cell><cell>15.73</cell><cell>17.25</cell><cell>18.25</cell></row><row><cell cols="4">Avg. number of tokens 789.33 643.75 712.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>;<ref type="bibr" target="#b19">Li et al. (2021)</ref>, we use head word match Performance (%) on the informative argument extraction task.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Argument Identification</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Argument Classification</cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell cols="2">Head Match</cell><cell cols="2">Coref Match</cell><cell></cell><cell></cell><cell cols="2">Head Match</cell><cell cols="2">Coref Match</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>BERT-CRF (Shi and Lin, 2019)</cell><cell>-</cell><cell>-</cell><cell>52.71</cell><cell>-</cell><cell>-</cell><cell>58.12</cell><cell>-</cell><cell>-</cell><cell>43.29</cell><cell>-</cell><cell>-</cell><cell>47.70</cell></row><row><cell>BART-Gen (Li et al., 2021)</cell><cell cols="12">58.62 55.64 57.09 62.84 59.64 61.19 54.02 51.27 52.61 57.47 54.55 55.97</cell></row><row><cell cols="13">Memory-based Training 61.07 56.18 58.52 66.21 60.91 63.45 55.93 51.45 53.60 60.47 55.64 57.95</cell></row><row><cell>w/ knowledge constrained decoding</cell><cell cols="12">62.45 56.55 59.35 67.67 61.27 64.31  *  57.23 51.82 54.39 61.85 56.00 58.78  *</cell></row></table><note><p>* indicates statistical significance (p &lt; 0.05).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>. Performance (%) on adversarial examples.</figDesc><table><row><cell>Arg. Classification</cell></row><row><cell>Head M. Coref. M.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation (%) for similarity-based retrieval.</figDesc><table><row><cell>Models</cell><cell>Arg. I. H. M. C. M. H. M. C. M. Arg. C.</cell></row><row><cell cols="2">Memory-based Training 58.52 63.45 53.60 57.95</cell></row><row><cell>w/o retrieval</cell><cell>56.84 61.82 51.29 55.69</cell></row><row><cell>w/ random memory</cell><cell>57.65 62.69 52.22 57.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>... Accused New York bomber Ahmad Khan Rahimi on Thursday to federal charges that he set off ... [S4] ... He spoke only once, when U.S. District Judge Richard Berman asked him to ... [S9] The confrontation left him with several gunshot wounds, delaying the filing of federal charges ... Colombia asked Cuba to capture ELN rebel commander Nicolas Rodriguez and provide information about the presence of other commanders in the Cuban territory. ...[S13] The Cuban government did not respond publicly to that request or made a statement ...</figDesc><table><row><cell>BART-Gen Baseline</cell><cell>Memory-enhanced Training</cell><cell>w/ Constrained Decoding</cell></row><row><cell>Input Doc. 1 [S1] Decoded Seq. Richard Berman[VICTIM] was injured by &lt;arg&gt; ...</cell><cell>Ahmad Khan Rahimi[VICTIM] was injured by &lt;arg&gt; ...</cell><cell>Ahmad Khan Rahimi[VICTIM] was injured by &lt;arg&gt; ...</cell></row><row><cell cols="2">[S1] Cuba sidesteps Colombia 2019s request to ...</cell><cell></cell></row><row><cell>Input Doc. 2 [S11] In November, Decoded Seq. Cuba[JAILER] arrested or jailed Nicolas Rodriguez[DETAINEE] ...</cell><cell>Cuba[JAILER] arrested or jailed Nicolas Rodriguez[DETAINEE] ...</cell><cell>&lt;arg&gt; arrested or jailed Nicolas Rodriguez[DETAINEE] ...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Decoded Seq. (Extracted Arguments) by BART-Gen and Our Models.</figDesc><table><row><cell>Missing</cell><cell>Spurious</cell><cell>Misclassified</cell></row><row><cell cols="3">Head M 239 (52.88%) 187 (41.37%) 26 (5.75%)</cell></row><row><cell cols="3">Coref M 213 (52.85%) 161 (39.95%) 29 (7.20%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Types of Errors Made by Our Framework.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the trigger (avg. 80.41 words), the missing argu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ments are far away (avg. 136.39 words) -show-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ing the hardness of extracting distant arguments as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>compared to local arguments.</cell></row><row><cell></cell><cell>0.07 0.08</cell><cell></cell><cell cols="2">all arguments missing arguments</cell></row><row><cell>Probability</cell><cell>0.03 0.04 0.05 0.06</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>20 Distance to Trigger (# of Words) 40 60 80</cell><cell>100</cell></row><row><cell cols="5">Figure 5: Distribution of Distance between Informative</cell></row><row><cell cols="4">Arguments and the Gold-standard Triggers.</cell></row><row><cell cols="5">triggered by capture). But it only participated in</cell></row><row><cell cols="5">the first event. According to our argument pair</cell></row><row><cell cols="5">constraints -it's improbable that one entity is both</cell></row><row><cell cols="5">an IMPEDER and a JAILER, our framework with</cell></row><row><cell cols="5">constrained decoding conducts reasoning to avoid</cell></row><row><cell cols="4">the wrong extraction.</cell></row><row><cell cols="5">Error Analysis and Remaining Challenges Ta-</cell></row><row><cell cols="5">ble 7 categorizes types of argument extraction er-</cell></row><row><cell cols="5">rors made by our best model. The majority of errors</cell></row><row><cell cols="5">is from missing arguments and only around 7% of</cell></row><row><cell cols="5">cases are caused by incorrectly-assigned argument</cell></row><row><cell cols="5">roles (e.g., a PLACE argument is mistakenly labeled</cell></row><row><cell cols="5">as a TARGET argument). Interestingly, from Fig-</cell></row><row><cell cols="5">ure 5's distribution, we see that as compared to the</cell></row><row><cell cols="5">distance of gold-standard informative arguments to</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In WIKIEVENTS<ref type="bibr" target="#b19">(Li et al., 2021)</ref>, nearly 40% of events have an argument outside the sentence containing the trigger.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Name entity mentions are recognized as more informative than nominal mentions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://www.darpa.mil/news-events/2019-01-04</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>The gold sequence for the 1st event in Figure1would be "[policemen including Collin] arrested or jailed[Tamerlan T.  and Dzhokhar]  for &lt;arg&gt; crime at &lt;arg&gt; place"</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>http://www.itl.nist.gov/iad/mig/tests/ace/2005/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>In our open-sourced repository, readers will be able to find our designed adversarial examples under the data folder.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank the anonymous reviewers helpful suggestions. This research is based upon work supported by <rs type="funder">U.S. DARPA KAIROS</rs> Program No. <rs type="grantNumber">FA8750-19-2-1004</rs>, <rs type="funder">U.S. DARPA AIDA</rs> Program No. <rs type="grantNumber">FA8750-18-2-0014</rs> and LORELEI Program No. <rs type="grantNumber">HR0011-15-C-0115</rs>. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of <rs type="institution">DARPA</rs>, or the <rs type="institution">U.S. Government</rs>. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HsYZN5J">
					<idno type="grant-number">FA8750-19-2-1004</idno>
				</org>
				<org type="funding" xml:id="_k5jHUSJ">
					<idno type="grant-number">FA8750-18-2-0014</idno>
				</org>
				<org type="funding" xml:id="_ZYES7YQ">
					<idno type="grant-number">HR0011-15-C-0115</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Examples of Argument Pairs</head><p>We list a couple of improbable argument pairs from the "checklist". </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in NLP</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Jeju Island</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">COMET: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<title level="m">Towards More Intelligent Extraction of Information from Documents</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University. Copyright -Database copyright Pro-Quest LLC</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>ProQuest does not claim copyright in the individual underlying works</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Template filling with generative transformers</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.70</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="909" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-sentence argument linking</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Culkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.718</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8057" to="8077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twenty-five years of information extraction</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324919000512</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="677" to="692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Message Understanding Conference-6: A brief history</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document-level entity-based extraction as template generation</title>
		<author>
			<persName><forename type="first">Kung-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5257" to="5269" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling textual cohesion for event extraction</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refining event extraction through unsupervised cross-document inference</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association of Computational Linguistics<address><addrLine>Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sharp nearby, fuzzy far away: How neural language models use context</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="284" to="294" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of BioNLP&apos;09 shared task on event extraction</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</title>
		<meeting>the BioNLP 2009 Workshop Companion Volume for Shared Task<address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<publisher>Colorado. Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001)</title>
		<meeting>the Eighteenth International Conference on Machine Learning (ICML 2001)<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001-06-28">2001. June 28 -July 1, 2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.69</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A joint end-to-end neural model for information extraction with global features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The 58th Annual Meeting of the</title>
		<meeting>The 58th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ATOMIC: an atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013027</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. 2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simple BERT models for relation extraction and semantic role labeling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1904.05255</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of the fourth Message Understanding Evaluation and Conference</title>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Message Uunderstanding Conference</title>
		<meeting><address><addrLine>McLean, Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-06-16">1992. June 16-18, 1992</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of a Conference Held in</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly Supervised Subevent Knowledge Acquisition</title>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maitreyi</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.430</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5345" to="5356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
