<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subsequence Kernels for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Razvan</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
							<email>razvan@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>Austin 1 University Station C0500</addrLine>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
							<email>mooney@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>Austin 1 University Station C0500</addrLine>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Subsequence Kernels for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C7B77513D1A6B70DFAA58B168B91814</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information Extraction (IE) is an important task in natural language processing, with many practical applications. It involves the analysis of text documents, with the aim of identifying particular types of entities and relations among them. Reliably extracting relations between entities in natural-language documents is still a difficult, unsolved problem. Its inherent difficulty is compounded by the emergence of new application domains, with new types of narrative that challenge systems developed for other, well-studied domains. Traditionally, IE systems have been trained to recognize names of people, organizations, locations and relations between them (MUC <ref type="bibr" target="#b0">[1]</ref>, ACE <ref type="bibr" target="#b1">[2]</ref>). For example, in the sentence "protesters seized several pumping stations", the task is to identify a LOCATED AT relationship between protesters (a PERSON entity) and stations (a LOCATION entity). Recently, substantial resources have been allocated for automatically extracting information from biomedical corpora, and consequently much effort is currently spent on automatically identifying biologically relevant entities, as well as on extracting useful biological relationships such as protein interactions or subcellular localizations. For example, the sentence "TR6 specifically binds Fas ligand", asserts an interaction relationship between the two proteins TR6 and Fas ligand. As in the case of the more traditional applications of IE, systems based on manually developed extraction rules <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> were soon superseded by information extractors learned through training on supervised corpora <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. One challenge posed by the biological domain is that current systems for doing part-of-speech (POS) tagging or parsing do not perform as well on the biomedical narrative as on the newspaper corpora on which they were originally trained. Consequently, IE systems developed for biological corpora need to be robust to POS or parsing errors, or to give reasonable performance using shallower but more reliable information, such as chunking instead of parsing.</p><p>Motivated by the task of extracting protein-protein interactions from biomedical corpora, we present a generalization of the subsequence kernel from <ref type="bibr" target="#b6">[7]</ref> that works with sequences containing combinations of words and word classes. This generalized kernel is further tailored for the task of relation extraction. Experimental results show that the new relation kernel outperforms two previous rule-based methods for interaction extraction. With a small modification, the same kernel is used for extracting top-level relations from ACE corpora, providing better results than a recent approach based on dependency tree kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>One of the first approaches to extracting protein interactions is that of Blaschke et al., described in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Their system is based on a set of manually developed rules, where each rule (or frame) is a sequence of words (or POS tags) and two protein-name tokens. Between every two adjacent words is a number indicating the maximum number of intervening words allowed when matching the rule to a sentence. An example rule is "interaction of (3) &lt;P&gt; (3) with (3) &lt;P&gt;", where '&lt;P&gt;' is used to denote a protein name. A sentence matches the rule if and only if it satisfies the word constraints in the given order and respects the respective word gaps.</p><p>In <ref type="bibr" target="#b5">[6]</ref> the authors described a new method ELCS (Extraction using Longest Common Subsequences) that automatically learns such rules. ELCS' rule representation is similar to that in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, except that it currently does not use POS tags, but allows disjunctions of words. An example rule learned by this system is "-( <ref type="formula">7</ref>) interaction (0) [between | of] ( <ref type="formula" target="#formula_7">5</ref>) &lt;P&gt; ( <ref type="formula">9</ref>) &lt;P&gt; (17) .". Words in square brackets separated by '|' indicate disjunctive lexical constraints, i.e. one of the given words must match the sentence at that position. The numbers in parentheses between adjacent constraints indicate the maximum number of unconstrained words allowed between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extraction using a Relation Kernel</head><p>Both Blaschke and ELCS do interaction extraction based on a limited set of matching rules, where a rule is simply a sparse (gappy) subsequence of words or POS tags anchored on the two protein-name tokens. Therefore, the two methods share a common limitation: either through manual selection <ref type="bibr">(Blaschke)</ref>, or as a result of the greedy learning procedure (ELCS), they end up using only a subset of all possible anchored sparse subsequences. Ideally, we would want to use all such anchored sparse subsequences as features, with weights reflecting their relative accuracy. However explicitly creating for each sentence a vector with a position for each such feature is infeasible, due to the high dimensionality of the feature space. Here we can exploit dual learning algorithms that process examples only via computing their dot-products, such as the Support Vector Machines (SVMs) <ref type="bibr" target="#b7">[8]</ref>. Computing the dot-product between two such vectors amounts to calculating the number of common anchored subsequences between the two sentences. This can be done very efficiently by modifying the dynamic programming algorithm used in the string kernel from <ref type="bibr" target="#b6">[7]</ref> to account only for common sparse subsequences constrained to contain the two protein-name tokens. We further prune down the feature space by utilizing the following property of natural language statements: when a sentence asserts a relationship between two entity mentions, it generally does this using one of the following three patterns:</p><p>• [FB] Fore-Between: words before and between the two entity mentions are simultaneously used to express the relationship. Examples: 'interaction of P 1 with P 2 ', 'activation of P 1 by P 2 '.</p><p>• [B] Between: only words between the two entities are essential for asserting the relationship. Examples: ' P 1 interacts with P 2 ', ' P 1 is activated by P 2 '.</p><p>• [BA] Between-After: words between and after the two entity mentions are simultaneously used to express the relationship. Examples: ' P 1 -P 2 complex', ' P 1 and P 2 interact'.</p><p>Another observation is that all these patterns use at most 4 words to express the relationship (not counting the two entity names). Consequently, when computing the relation kernel, we restrict the counting of common anchored subsequences only to those having one of the three types described above, with a maximum word-length of 4. This type of feature selection leads not only to a faster kernel computation, but also to less overfitting, which results in increased accuracy (see Section 5 for comparative experiments).</p><p>The patterns enumerated above are completely lexicalized and consequently their performance is limited by data sparsity. This can be alleviated by categorizing words into classes with varying degrees of generality, and then allowing patterns to use both words and their classes. Examples of word classes are POS tags and generalizations over POS tags such as Noun, Active Verb or Passive Verb. The entity type can also be used, if the word is part of a known named entity, as well as the type of the chunk containing the word, when chunking information is available. Content words such as nouns and verbs can also be related to their synsets via WordNet. Patterns then will consist of sparse subsequences of words, POS tags, general POS (GPOS) tags, entity and chunk types, or WordNet synsets. For example, 'Noun of P 1 by P 2 ' is an FB pattern based on words and general POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Subsequence Kernels for Relation Extraction</head><p>We are going to show how to compute the relation kernel described in the previous section in two steps. First, in Section 4.1 we present a generalization of the subsequence kernel from <ref type="bibr" target="#b6">[7]</ref>. This new kernel works with patterns construed as mixtures of words and word classes. Based on this generalized subsequence kernel, in Section 4.2 we formally define and show the efficient computation of the relation kernel used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Generalized Subsequence Kernel</head><p>Let Σ 1 , Σ 2 , ..., Σ k be some disjoint feature spaces. Following the example in Section 3, Σ 1 could be the set of words, Σ 2 the set of POS tags, etc. Let Σ × = Σ 1 × Σ 2 × ... × Σ k be the set of all possible feature vectors, where a feature vector would be associated with each position in a sentence. Given two feature vectors x, y ∈ Σ × , let c(x, y) denote the number of common features between x and y. The next notation follows that introduced in <ref type="bibr" target="#b6">[7]</ref>. Thus, let s, t be two sequences over the finite set Σ × , and let |s| denote the length of s = s 1 ...s |s| . The sequence s[i : j] is the contiguous subsequence s i ...s j of s. Let i = (i 1 , ..., i |i| ) be a sequence of |i| indices in s, in ascending order. We define the length l(i) of the index sequence i in s as i |i|i 1 + 1. Similarly, j is a sequence of |j| indices in t.</p><p>Let Σ ∪ = Σ 1 ∪ Σ 2 ∪ ... ∪ Σ k be the set of all possible features. We say that the sequence u ∈ Σ * ∪ is a (sparse) subsequence of s if there is a sequence of |u| indices i such that u k ∈ s i k , for all k = 1, ..., |u|. Equivalently, we write u ≺ s[i] as a shorthand for the component-wise '∈' relationship between u and s[i].</p><p>Finally, let K n (s, t, λ) (Equation <ref type="formula" target="#formula_0">1</ref>) be the number of weighted sparse subsequences u of length n common to s and t (i.e. u ≺ s[i], u ≺ t[j]), where the weight of u is λ l(i)+l(j) , for some λ ≤ 1.</p><formula xml:id="formula_0">K n (s, t, λ) = u∈Σ n ∪ i:u≺s[i] j:u≺t[j] λ l(i)+l(j)<label>(1)</label></formula><p>Because for two fixed index sequences i and j, both of length n, the size of the set</p><formula xml:id="formula_1">{u ∈ Σ n ∪ |u ≺ s[i], u ≺ t[j]} is n k=1 c(s i k , t j k )</formula><p>, then we can rewrite K n (s, t, λ) as in Equation <ref type="formula" target="#formula_2">2</ref>:</p><formula xml:id="formula_2">K n (s, t, λ) = i:|i|=n j:|j|=n n k=1 c(s i k , t j k )λ l(i)+l(j)<label>(2)</label></formula><p>We use λ as a decaying factor that penalizes longer subsequences. For sparse subsequences, this means that wider gaps will be penalized more, which is exactly the desired behavior for our patterns. Through them, we try to capture head-modifier dependencies that are important for relation extraction; for lack of reliable dependency information, the larger the word gap is between two words, the less confident we are in the existence of a headmodifier relationship between them.</p><p>To enable an efficient computation of K n , we use the auxiliary function K</p><p>′ n with a similar definition as K n , the only difference being that it counts the length from the beginning of the particular subsequence u to the end of the strings s and t, as illustrated in Equation <ref type="formula" target="#formula_3">3</ref>:</p><formula xml:id="formula_3">K ′ n (s, t, λ) = u∈Σ n ∪ i:u≺s[i] j:u≺t[j] λ |s|+|t|-i1-j1+2<label>(3)</label></formula><p>An equivalent formula for K ′ n (s, t, λ) is obtained by changing the exponent of λ from Equation 2 to |s| + |t|i 1j 1 + 2.</p><p>Based on all definitions above, K n can be computed in O(kn|s||t|) time, by modifying the recursive computation from <ref type="bibr" target="#b6">[7]</ref> with the new factor c(x, y), as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In this figure, the sequence sx is the result of appending x to s (with ty defined in a similar way). To avoid clutter, the parameter λ is not shown in the argument list of K and K ′ , unless it is instantiated to a specific constant. </p><formula xml:id="formula_4">K ′ 0 (s, t) = 1, f or all s, t K ′′ i (sx, ty) = λK ′′ i (sx, t) + λ 2 K ′ i-1 (s, t) • c(x, y) K ′ i (sx, t) = λK ′ i (s, t) + K ′′ i (sx, t) Kn(sx, t) = Kn(s, t) + j λ 2 K ′ n-1 (s, t[1 : j -1]) • c(x, t[j])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computing the Relation Kernel</head><p>As described in Section 2, the input consists of a set of sentences, where each sentence contains exactly two entities (protein names in the case of interaction extraction). In Figure <ref type="figure">2</ref> we show the segments that will be used for computing the relation kernel between two example sentences s and t. In sentence s for instance, x 1 and x 2 are the two entities, s f is the sentence segment before x 1 , s b is the segment between x 1 and x 2 , and s a is the sentence segment after x 2 . For convenience, we also include the auxiliary segment s The relation kernel computes the number of common patterns between two sentences s and t, where the set of patterns is restricted to the three types introduced in Section 3. Therefore, the kernel rK(s, t) is expressed as the sum of three sub-kernels: f bK(s, t) counting the rK(s, t) = f bK(s, t) + bK(s, t) + baK(s, t) bKi(s, t) = Ki(s b , t b , 1) number of common fore-between patterns, bK(s, t) for between patterns, and baK(s, t) for between-after patterns, as in Figure <ref type="figure" target="#fig_2">3</ref>.</p><formula xml:id="formula_5">• c(x1, y1) • c(x2, y2) • λ l(s ′ b )+l(t ′ b ) f bK(s, t) = i,j bKi(s, t) • K ′ j (s f , t f ), 1 ≤ i, 1 ≤ j, i + j &lt; fbmax bK(s, t) = i bKi(s, t), 1 ≤ i ≤ bmax baK(s, t) = i,j bKi(s, t) • K ′ j (s - a , t - a ), 1 ≤ i, 1 ≤ j, i + j &lt; bamax</formula><p>All three sub-kernels include in their computation the counting of common subsequences between s ′ b and t ′ b . In order to speed up the computation, all these common counts can be calculated separately in bK i , which is defined as the number of common subsequences of length i between s ′ b and t ′ b , anchored at x 1 /x 2 and y 1 /y 2 respectively (i.e. constrained to start at x 1 /y 1 and to end at x 2 /y 2 ). Then f bK simply counts the number of subsequences that match j positions before the first entity and i positions between the entities, constrained to have length less than a constant f b max . To obtain a similar formula for baK we simply use the reversed (mirror) version of segments s a and t a (e.g. s - a and t - a ). In Section 3 we observed that all three subsequence patterns use at most 4 words to express a relation, therefore we set constants f b max , b max and ba max to 4. Kernels K and K ′ are computed using the procedure described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>The relation kernel (ERK) is evaluated on the task of extracting relations from two corpora with different types of narrative, which are described in more detail in the following sections. In both cases, we assume that the entities and their labels are known. All preprocessing steps -sentence segmentation, tokenization, POS tagging and chunking -were performed using the OpenNLP<ref type="foot" target="#foot_0">1</ref> package. If a sentence contains n entities (n ≥ 2), it is replicated into n 2 sentences, each containing only two entities. If the two entities are known to be in a relationship, then the replicated sentence is added to the set of corresponding positive sentences, otherwise it is added to the set of negative sentences. During testing, a sentence having n entities (n ≥ 2) is again replicated into n 2 sentences in a similar way. The relation kernel is used in conjunction with SVM learning in order to find a decision hyperplane that best separates the positive examples from negative examples. We modified the LibSVM<ref type="foot" target="#foot_1">2</ref> package by plugging in the kernel described above. In all experiments, the decay factor λ is set to 0.75. The performance is measured using precision (percentage of correctly extracted relations out of total extracted) and recall (percentage of correctly extracted relations out of total number of relations annotated in the corpus). When PR curves are reported, the precision and recall are computed using output from 10-fold cross-validation. The graph points are obtained by varying a threshold on the minimum acceptable extraction confidence, based on the probability estimates from LibSVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interaction Extraction from AImed</head><p>We did comparative experiments on the AImed corpus, which has been previously used for training the protein interaction extraction systems in <ref type="bibr" target="#b5">[6]</ref>. It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4084 protein references and around 1000 tagged interactions in this dataset.</p><p>We compare the following three systems on the task of retrieving protein interactions from AImed (assuming gold standard proteins):</p><p>• [Manual]: We report the performance of the rule-based system of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• [ELCS]:</head><p>We report the 10-fold cross-validated results from <ref type="bibr" target="#b5">[6]</ref> as a PR graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• [ERK]:</head><p>Based on the same splits as ELCS, we compute the corresponding precisionrecall graph. In order to have a fair comparison with the other two systems, which use only lexical information, we do not use any word classes here.</p><p>The results, summarized in Figure <ref type="figure" target="#fig_3">4</ref>(a), show that the relation kernel outperforms both ELCS and the manually written rules. To evaluate the impact that the three types of patterns have on performance, we compare ERK with an ablated system (ERK-A) that uses all possible patterns, constrained only to be anchored on the two entity names. As can be seen in Figure <ref type="figure" target="#fig_3">4</ref>(b), the three patterns (FB, B, BA) do lead to a significant increase in performance, especially for higher recall levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relation Extraction from ACE</head><p>To evaluate how well this relation kernel ports to other types of narrative, we applied it to the problem of extracting top-level relations from the ACE corpus <ref type="bibr" target="#b1">[2]</ref>, the version used for the September 2002 evaluation. The training part of this dataset consists of 422 documents, with a separate set of 97 documents allocated for testing. This version of the ACE corpus contains three types of annotations: coreference, named entities and relations. There are five types of entities -PERSON, ORGANIZATION, FACILITY, LOCATION, and GEO-POLITICAL ENTITY -which can participate in five general, top-level relations: ROLE, PART, LOCATED, NEAR, and SOCIAL. A recent approach to extracting relations is described in <ref type="bibr" target="#b8">[9]</ref>. The authors use a generalized version of the tree kernel from <ref type="bibr" target="#b9">[10]</ref>  We trained our relation kernel, under the first scenario, to recognize the same 5 top-level relation types. While for interaction extraction we used only the lexicalized version of the kernel, here we utilize more features, corresponding to the following feature spaces: Σ 1 is the word vocabulary, Σ 2 is the set of POS tags, Σ 3 is the set of generic POS tags, and Σ 4 contains the 5 entity types. We also used chunking information as follows: all (sparse) subsequences were created exclusively from the chunk heads, where a head is defined as the last word in a chunk. The same criterion was used for computing the length of a subsequence -all words other than head words were ignored. This is based on the observation that in general words other than the chunk head do not contribute to establishing a relationship between two entities outside of that chunk. One exception is when both entities in the example sentence are contained in the same chunk. This happens very often due to nounnoun ('U.S. troops') or adjective-noun ('Serbian general') compounds. In these cases, we let one chunk contribute both entity heads. Also, an important difference from the interaction extraction case is that often the two entities in a relation do not have any words separating them, as for example in noun-noun compounds. None of the three patterns from Section 3 capture this type of dependency, therefore we introduced a fourth type of pattern, the modifier pattern M. This pattern consists of a sequence of length two formed from the head words (or their word classes) of the two entities. Correspondingly, we updated the relation kernel from Figure <ref type="figure" target="#fig_2">3</ref> with a new kernel term mK, as illustrated in Equation <ref type="formula" target="#formula_6">4</ref>.</p><formula xml:id="formula_6">rK(s, t) = f bK(s, t) + bK(s, t) + baK(s, t) + mK(s, t)<label>(4)</label></formula><p>The sub-kernel mK corresponds to a product of counts, as shown in Equation <ref type="formula" target="#formula_7">5</ref>.</p><formula xml:id="formula_7">mK(s, t) = c(x1, y1) • c(x2, y2) • λ 2+2<label>(5)</label></formula><p>We present in Table <ref type="table" target="#tab_1">1</ref> the results of using our updated relation kernel to extract relations from ACE, under the first scenario. We also show the results presented in <ref type="bibr" target="#b8">[9]</ref> for their best performing kernel K4 (a sum between a bag-of-words kernel and the dependency kernel) under both scenarios. Even though it uses less sophisticated syntactic and semantic information, ERK in S1 significantly outperforms the dependency kernel. Also, ERK already performs a few percentage points better than K4 in S2. Therefore we expect to get an even more significant increase in performance by training our relation kernel in the same cascaded fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In <ref type="bibr" target="#b9">[10]</ref>, a tree kernel is defined over shallow parse representations of text, together with an efficient algorithm for computing it. Experiments on extracting PERSON-AFFILIATION and ORGANIZATION-LOCATION relations from 200 news articles show the advantage of using this new type of tree kernels over three feature-based algorithms. The same kernel was slightly generalized in <ref type="bibr" target="#b8">[9]</ref> and applied on dependency tree representations of sentences, with dependency trees being created from head-modifier relationships extracted from syntactic parse trees. Experimental results show a clear win of the dependency tree kernel over a bag-of-words kernel. However, in a bag-of-words approach the word order is completely lost. For relation extraction, word order is important, and our experimental results support this claim -all subsequence patterns used in our approach retain the order between words.</p><p>The tree kernels used in the two methods above are opaque in the sense that the semantics of the dimensions in the corresponding Hilbert space is not obvious. For subsequence kernels, the semantics is known by definition: each subsequence pattern corresponds to a dimension in the Hilbert space. This enabled us to easily restrict the types of patterns counted by the kernel to the three types that we deemed relevant for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a new relation extraction method based on a generalization of subsequence kernels. When evaluated on a protein interaction dataset, the new method showed better performance than two previous rule-based systems. After a small modification, the same kernel was evaluated on the task of extracting top-level relations from the ACE corpus, showing better performance when compared with a recent dependency tree kernel.</p><p>An experiment that we expect to lead to better performance was already suggested in Section 5.2 -using the relation kernel in a cascaded fashion, in order to improve the low recall caused by the highly unbalanced data distribution. Another performance gain may come from setting the factor λ to a more appropriate value based on a development dataset.</p><p>Currently, the method assumes the named entities are known. A natural extension is to integrate named entity recognition with relation extraction. Recent research <ref type="bibr" target="#b10">[11]</ref> indicates that a global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Computation of subsequence kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>′ b = x 1 Figure 2 :</head><label>12</label><figDesc>Figure 2: Sentence segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Computation of relation kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PR curves for interaction extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This is the classic setting: one multi-class SVM is learned to discriminate among the 5 top-level classes, plus one more class for the no-relation cases.-[S2] One binary SVM is trained for relation detection, meaning that all positive relation instances are combined into one class. The thresholded output of this binary classifier is used as training data for a second multi-class SVM, trained for relation classification.</figDesc><table><row><cell>to</cell></row><row><cell>compute a kernel over relation examples, where a relation example consists of the smallest</cell></row><row><cell>dependency tree containing the two entities of the relation. Precision and recall values are</cell></row><row><cell>reported for the task of extracting the 5 top-level relations in the ACE corpus under two</cell></row><row><cell>different scenarios:</cell></row><row><cell>-[S1]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Extraction Performance on ACE.</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F-measure</cell></row><row><cell cols="2">(S1) ERK 73.9</cell><cell>35.2</cell><cell>47.7</cell></row><row><cell>(S1) K4</cell><cell>70.3</cell><cell>26.3</cell><cell>38.0</cell></row><row><cell>(S2) K4</cell><cell>67.1</cell><cell>35.0</cell><cell>45.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>URL: http://opennlp.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>URL:http://www.csie.ntu.edu.tw/˜cjlin/libsvm/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>This work was supported by grants IIS-0117308 and IIS-0325116 from the NSF. We would like to thank Rohit J. Kate and the anonymous reviewers for helpful observations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="http://cs.nyu.edu/cs/faculty/grishman/muc6.html" />
		<title level="m">Message Understanding Conference 6</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ACE -Automatic Content Extraction</title>
		<author>
			<persName><surname>Nist</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/speech/tests/ace" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can bibliographic pointers for known biological data be found automatically? protein interactions as a case study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative and Functional Genomics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The frame-based module of the Suiseki information extraction system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representing sentence structure in hidden Markov models for information extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-2001)</title>
		<meeting>the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-2001)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1273" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparative experiments on learning information extractors for proteins and their interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine (special issue on Summarization and Information Extraction from Medical Documents)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="155" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Annual Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
