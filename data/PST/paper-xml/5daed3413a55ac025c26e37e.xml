<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Symbolic Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-22">October 22, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ying</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>2,4</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weilin</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiadong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Symbolic Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-22">October 22, 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.08892v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpretability is crucial for machine learning in many scenarios such as quantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a classic interpretable machine learning method by bridging X and Y using mathematical expressions composed of some basic functions. However, the search space of all possible expressions grows exponentially with the length of the expression, making it infeasible for enumeration. Genetic programming (GP) has been traditionally and commonly used in SR to search for the optimal solution, but it suffers from several limitations, e.g. the difficulty in incorporating prior knowledge in GP; overly-complicated output expression and reduced interpretability etc.</p><p>To address these issues, we propose a new method to fit SR under a Bayesian framework. Firstly, Bayesian model can naturally incorporate prior knowledge (e.g., preference of basis functions, operators and raw features) to improve the efficiency of fitting SR. Secondly, to improve interpretability of expressions in SR, we aim to capture concise but informative signals. To this end, we assume the expected signal has an additive structure, i.e., a linear combination of several concise expressions, of which complexity is controlled by a well-designed prior distribution. In our setup, each expression is characterized by a symbolic tree, and therefore the proposed SR model could be solved by sampling symbolic trees from the posterior distribution using an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared with GP, the proposed BSR(Bayesian Symbolic Regression) method doesn't need to keep an updated "genome pool" and so it saves computer memory dramatically.</p><p>Numerical experiments show that, compared with GP, the solutions of BSR are closer to the ground truth and the expressions are more concise. Meanwhile we find the solution of BSR is robust to hyper-parameter specifications such as the number of trees in the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Symbolic regression is a special regression model which assembles different mathematical expressions to discover the association between the response variable and the predictors, with applications studied in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, etc. Without a pre-specified model structure, it is challenging to fit symbolic regression, which requires to search for the optimal solution in a large space of mathematical expressions and estimate the corresponding parameters simultaneously.</p><p>Traditionally, symbolic regression is solved by combinatorial optimization methods like Genetic Programming (GP) that evolves over generations, see <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref>, etc. However, GP suffers from high computational complexity and overly complicated output expressions, and the solution is sensitive to the initial value, see <ref type="bibr" target="#b19">[20]</ref>. Some modifications of the original GP algorithm have been proposed to address those problems including <ref type="bibr" target="#b0">[1]</ref> which incorporates statistical information of generations, <ref type="bibr" target="#b22">[23]</ref> which deterministically builds higher-level expressions from 'elite' building blocks, <ref type="bibr" target="#b15">[16]</ref> which employs a hybrid of GP and deterministic methods, <ref type="bibr" target="#b25">[26]</ref> which exploits the Pareto front of GP to balance accuracy and simplicity, <ref type="bibr" target="#b21">[22]</ref> uses a divide and conquer strategy to decompose the search space and reduce the model complexity, and <ref type="bibr" target="#b18">[19]</ref> which proposes a local optimization method to control the complexity of symbolic regression.</p><p>Although some efforts have been made to improve GP, its intrinsic disadvantages still remain unsolved. Some research work explores SR estimation methods other than GP. For example, <ref type="bibr" target="#b11">[12]</ref> which introduces a new data structure called Interaction-Transformation to constrain the search space and simplify the output symbolic expression, <ref type="bibr" target="#b22">[23]</ref> which uses pathwise regularized learning to rapidly prune a huge set of candidate basis functions down to compact models, <ref type="bibr" target="#b2">[3]</ref> assumes regression models are spanned by a number of elite bases selected and updated by their proposed algorithm, <ref type="bibr" target="#b1">[2]</ref> introduces a neuro-encoded expression programming with recurrent neural networks to improve smoothness and stability of the search space, <ref type="bibr" target="#b20">[21]</ref>which introduces an expression generating neural network and proposes an Monte Carlo tree search algorithm to produce expressions that match given leading powers.</p><p>In this work, we consider to fit symbolic regression under a Bayesian framework, which can naturally incorporate prior knowledge, can improve model interpretability and can potentially simplify the structure and find prominent components of complicated signals. The key idea is to represent each mathematical expression as a symbolic tree, where each child node denotes one input value and the parent node denotes the output value of applying the mathematical operator to all the input values from its child nodes. To control model complexity, the response variable y is assumed to be a linear combination of multiple parent nodes whose descendant nodes (or leaf nodes) are the predictor x. We develop a prior model for the tree structures and assign informative priors to the associated parameters. Markov chain Monte Carlo (MCMC) methods are employed to simulate the posterior distributions of the underlying tree structures which correspond to a combination of multiple mathematical expressions.</p><p>The paper is organized as follows. First, we present our Bayesian symbolic regression model by introducing the tree representation of mathematical expressions. Then we develop an MCMC-based posterior computation algorithm for the proposed model. Finally, we demonstrate the superiority of the proposed method compared to existing alternatives via numerical experiments.</p><p>In the following parts, we will refer to our symbolic regression method based on Bayesian framework as Bayesian Symbolic Regression or BSR in exchange.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Symbolic Regression with Linearly-Mixed Tree Representations</head><p>Denote by x = (x 1 , . . . , x d ) ∈ R d the predictor variables and by y ∈ R the response variable. We consider a symbolic regression model:</p><formula xml:id="formula_0">y = g(x) + ,</formula><p>where g(•) is a function represented by a combination of mathematical expressions taking predictors x as the input variable. Specifically, the mathematical operators such as +, ×, . . ., and arithmetic functions like exp(•), cos(•), . . ., can be in the search space of mathematical expressions. For example, g(x) = x 1 + 2 cos(x 2 ) + exp(x 3 ) + 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of Basic Operators</head><p>All possible mathematical expressions are combinations of elements in a set of basic functions. The choice of basic operators is a building block of our tree representation, see <ref type="bibr" target="#b24">[25]</ref>. In this paper, we adopt the commonly-used operators +, ×, exp(), inv(x) = 1/x, neg(x) = −x and linear transformation lt(x) = ax + b with parameters (a, b) ∈ R 2 . They are able to express − and ÷ with symmetric binary operators. In practice, the basic operators can be specified by users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Expressions to Trees</head><p>The mathematical expression can be equivalently represented by a tree denoted by T , with non-terminal nodes indicating operations and terminal nodes indicating the selected features. T is a binary tree but not necessarily a complete tree.</p><p>Specifically, a non-terminal node has one child node if it is assigned a unary operator, and two if assigned a binary operator. For example, a non-terminal node with operator + represents the operation that the values of its two child nodes are added up. For a non-terminal unary operator, for example exp(), it means taking exponential of the value of its child node. Note that some operators may also be associated with parameters, like linear transformation lt(x) = ax + b with parameters (a, b) ∈ R 2 . We collect these parameters in a vector Θ.</p><p>On the other hand, each terminal node η specified by i k ∈ M represents a particular feature x i k of the data vector. Here M is the vector including features of all terminal nodes. For a tree of depth d, we start from the terminal nodes by performing the operations indicated by their parents, then go to their parents and perform upper-level operations accordingly. We obtain the output at the root node. For example, the tree in Figure <ref type="figure">1</ref> represents g(x) = cos(x 1 + x 2 ), which consists of two terminal nodes 1, 2 and two non-terminal nodes cos, +.</p><p>In short, the tree structure T is the set of nodes T = (η 1 , . . . , η t ), corresponding to operators with zero to two child nodes. Some operators involve parameters aggregated in Θ. From predictor x, terminal nodes select features specified by M = (i 1 , . . . , i p ), where i k indicates adopting x i k of vector x as the input of the corresponding node η k . The specification of T , Θ and M represents an equivalent tree for a mathematical expression g(•; T, M, Θ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Priors on Tree Representations</head><p>Under a Bayesian modeling framework, it is critical to specify appropriate priors for parameters, as it has the flexibility to incorporate prior knowledge to facilitate more accurate posterior inferences. In our model, we are interested in making inferences on the tree structure T , the parameter Θ and the selected feature indices M .</p><p>To ensure the model interpretability, we aim to control the size of tree representations, or equivalently, the complexity of mathematical expressions. The default prior of operators and features are uniform distributions, indicating no preference for any particular operator or feature. They can be user-specified weight vectors to pose preferences.</p><p>For a single tree, we adopt prior distributions on T , M and Θ in a similar fashion as those for Bayesian regression tree models in <ref type="bibr" target="#b6">[7]</ref> as follows. Of note, although the prior models are similar, our model and tree interpretations are completely different from the Bayesian regression tree model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior of Tree Structure T</head><p>We specify the prior p(T ) by assigning the probabilities to each event in the process of constructing a specific tree. The prior construction starts from the root node.</p><p>A node is randomly assigned a particular operator according to the prior. The operator indicates whether it extends to one child node, or split into two child nodes, or function as a terminal node. Starting from the root, such growth performs recursively on newly-generated nodes until all nodes are assigned operators or terminated.</p><p>Specifically, for a node with depth d η , i.e. the number of nodes passed from it to the node, with probability p 1 (η, T ) = α(1 + d η ) −β . It is a non-terminal node, which means it has descendants. Here α, β are prefixed parameters that guides the general sizes of trees in practice. The prior also includes a user-specified basic operator set and a corresponding weight vector indicating the probabilities of adopting each operator for a newly-grown node. For example, we specify the operator set (operator) as Ops= (exp(), lt(), inv(), neg(), +, ×) where lt(x) = ax + b, inv(x) = 1/x, neg(x) = −x, and the uniform weight vector w op = (1/6, 1/6, 1/6, 1/6, 1/6, 1/6). Such default choice shows no preference for any particular operator.</p><p>With probability p 1 (η, T ), the node η is assigned an operator according to w op if it is non-terminal and grows its one or two child nodes. Then its child nodes grow recursively. Otherwise it is a terminal node and assigned some feature in a way specified later. The construction of a tree is completed if all nodes are assigned or terminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior of Terminal Nodes M</head><p>When a node is terminated, it is assigned a feature of x according to the prior of features as input of the expression. The number and locations of terminal nodes are decided by structure of T . Conditioned on T , the specific feature that one terminal node takes is randomly generated with probabilities indicated by weight vector w f t . The default choice is uniform among all features, i.e., w f t = (1/d, . . . , 1/d). It can also be userspecified to highlight some preferred features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior of lt() Parameters</head><p>An important operator we adopt here is linear transformation lt(x) = ax + b associated with linear parameters (a, b) ∈ R 2 . lt() includes scalings and well enriches the set of potential expressions. Such operation is discussed in <ref type="bibr" target="#b16">[17]</ref> and proved to improve the fitting. Pairs of linear parameters (a, b) are assembled in Θ and are considered independent.</p><p>Let L(T ) be the set of lt() nodes in T , and each node η is associated with parameters (a η , b η ), then the prior of Θ is</p><formula xml:id="formula_1">p(Θ | T ) = η∈L(T ) p(aη, bη),</formula><p>where a η 's, b η 's are independent and</p><formula xml:id="formula_2">aη ∼ N (1, σ 2 a ), bη ∼ N (0, σ 2 b ).</formula><p>This indicates that the prior of the linear transformation is a Gaussian and centered around identity function. The prior of σ Θ = (σ a , σ b ) is conjugate prior of normal distribution, which is</p><formula xml:id="formula_3">σ 2 a ∼ IG(νa/2, νaλa/2), σ 2 b ∼ IG(ν b /2, ν b λ b /2),</formula><p>where ν a , λ a , ν b , λ b are pre-specified hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Find the Signal: Linear Mixture of Simpler Trees</head><p>Many popular machine learning techniques, such as neural networks, can approximate functions very well, but they are difficult to interpret. A widely celebrated advantage of symbolic regression is its interpretability and good performance of approximating functions. The model fitting of symbolic regression usually results in relatively simple mathematical expressions, it is straightforward to understand the relationship between the predictors x and the response variable y. However, if symbolic regression produces too complicated expressions, the interpretation of the model fitting becomes challenging: there exists a tradeoff between simplicity and accuracy. To highlight the superiority of symbolic regression in interpretability over other methods, we aim at finding the most prominent and concise signals. If the features are strong and expressive, we assume that the expression should not involve too many features, and the transformation should not be too complicated.</p><p>Moreover, the real-world signal may be a combination of simple signals, where only a small amount of simpler ones play a significant role. A simpler idea has its roots in <ref type="bibr" target="#b17">[18]</ref>, where the output is appropriately scaled. SR has also been addressed with methods related to generalized linear models, summarized in <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this sense, we model the final output y to be centered at some linear combination of relatively simple expressions</p><formula xml:id="formula_4">y = β0 + k i=1 βi • g(x; Ti, Mi, Θi) + , ∼ N (0, σ 2 )</formula><p>where k is a pre-specified number of simple components, g(x; T i , M i , Θ i ) is a relatively simple expression represented by a symbolic tree, and β i is the linear coefficient for the i-th expression. The coefficients β i , i = 0, . . . , k is obtained by OLS linear regression using intercept and g(</p><formula xml:id="formula_5">•; T i , M i , Θ i ), i = 1, . . . , k. Let {(T i , M i , Θ i )} k i=1</formula><p>denote the series of tuples (T i , M i , Θ i ), i = 1, . . . , k. Let OLS() denote the OLS fitting result, then a simpler form is</p><formula xml:id="formula_6">y = OLS x, {(Ti, Mi, Θi)} k i=1 + , ∼ N (0, σ 2 )</formula><p>where the prior of the noise scale is the conjugate inverse gamma distribution</p><formula xml:id="formula_7">σ 2 ∼ IG(ν/2, νλ/2)</formula><p>where ν and λ are pre-specified parameters. Additionally let</p><formula xml:id="formula_8">(T, M, Θ) = {(T i , M i , Θ i )} k i=1 , the joint likelihood is p(y, (T, M, Θ), σ, σΘ | x) =p(y | OLS x, T, M, Θ , σ 2 )p(M, T )p(Θ | T, σ 2 Θ )p(σ 2 Θ )p(σ 2 ) =p(y | OLS x, T, M, Θ , σ 2 )p(σ 2 ) × k i=1 p(Mi | Ti)p(Ti)p(Θi | Ti, σ 2 Θ )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Posterior Computation</head><p>We employ the Metropolis-Hastings (MH) algorithm proposed in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b14">[15]</ref> to make posterior inferences on the proposed model. Note that (T, M, Θ) represents the set of k trees {T i , M i , Θ i } k i=1 , and (T s , M s , Θ s ) denotes the set of k trees that the MH algorithm accepts at the s-th iteration.</p><p>With a pre-specified number of trees k, our method modifies the structure of the i-th tree by sampling from proposal q(• | •), and accepts the new structure with probability α, which can be calculated according to MH algorithm. Otherwise the i-th tree stays at its original form. The k trees are updated sequentially, so to illustrate, we first show how a single tree is modified at each time.</p><p>The sampling of a new tree consists of three parts. The first is the structure specified by T and M , which is discrete. In this subsection, T and M stand for a single tree. The second part is Θ aggregating parameters of all lt() nodes. The dimensionality of Θ may change with (T, M ) since the number of linear transformation nodes vary among different tree structures. Therefore we use the reversible jump MCMC (RJM-CMC) algorithm proposed by <ref type="bibr" target="#b12">[13]</ref> to solve the trans-dimensional sampling problem. For simplicity, denote by S = (T, M ) the structure parameters. The last part is the sampling of noise variance σ 2 from an inverse gamma prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Transition Kernel</head><p>We first specify how the sampling algorithm jumps from a tree structure to a new one. Inspired by <ref type="bibr" target="#b6">[7]</ref> and considering the nature of calculation trees, we design the following seven reversible actions. The probabilities from S = (T, M ) to new structure S * = (T * , M * ) is denoted as the proposal q(S * | S).</p><p>• Stay: If the expression involves n l ≥ 0 lt() operators, with probability p 0 = n l /4(n l + 3), the structure S = (T, M ) stays unchanged, and ordinary MH step follows to sample new linear parameters.</p><p>• Grow: Uniformly pick a terminal node and activate it. A sub-tree is then generated iteratively, where each time a node is randomly terminated or assigned an operator according to the prior until all nodes are terminated or assigned.</p><p>To regularize the complexity of the expression, the proposal grows with lower probability when the tree depth and amount of nodes are large. The probability of Grow is p g = 1−p0 3</p><p>• min 1, 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nnt+2</head><p>,where N nt is the number of nonterminal nodes.</p><p>• Prune: Uniformly pick a non-terminal node and turn it into a terminal node by discarding its descendants. Then randomly choose a feature of x to the newly pruned node.</p><p>We set the probability of Prune as p p = 1−p0 3 − p g such that Grow and Prune share one-third of the probability that the structure does not Stay.</p><p>• Delete: Uniformly pick a candidate node and delete it. Specifically, the candidate should be non-terminal. Also, if it is a root node, it needs to have at least one non-terminal child node to avoid leaving a terminal node as the root node. If the picked candidate is unary, then we just let its child replace it. If it is binary but not root, we uniformly select one of its children to replace it. If the picked candidate is binary and the root, we uniformly select one of its non-terminal children to replace it.</p><p>We set the probability of Delete as</p><formula xml:id="formula_9">p d = 1−p0 3 • Nc Nc+3</formula><p>, where N c is the number of aforementioned candidates.</p><p>• Insert: Uniformly pick a node and insert a node between it and its parent. The weight of nodes assigned is w op . If the inserted node is binary, the picked node is set as left child of the new node, and the new right child is generated according to the prior.</p><p>The probability of Insert is set as</p><formula xml:id="formula_10">p i = 1−p0 3</formula><p>− p d such that Delete and Insert share one-third of the probability that the structure does not Stay.</p><p>• ReassignOperator: Uniformly pick a non-terminal node, and assign a new operator according to w op .</p><p>If the node changes from unary to binary, its original child is taken as the left child, and we grow a new sub-tree as right child. If the node changes from binary to unary, we preserve the left sub-tree (this is to make the transition reversible).</p><p>• ReassignFeature: Uniformly pick a terminal node and assign another feature with weight w f t .</p><p>The probability of ReassignOperator and ReassignFeature is set as</p><formula xml:id="formula_11">p ro = p rf = 1−p0 6</formula><p>Note that the generation of the 'tree' is top-down, creating sub-trees from nodes. However, the calculation is bottom-up, corresponding to transforming the original features and combine different sources of information.</p><p>The above discrepancy can be alleviated by our design of proposal. Grow and Prune creates and deletes sub-trees in a top-down way, which corresponds to changing a "block", or a higher level feature represented by the sub-tree in the expression. On the other hand, Delete and Insert modify the higher-level structure by changing the way such "blocks" combine and interact in a bottom-up way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jump between Spaces of Parameters</head><p>Another issue of proposing new structure S * is that the number of linear transformation nodes may change. Therefore the dimensionality of Θ may be different and RJMCMC (reversible jump Markov Chain Monte Carlo) proposed in <ref type="bibr" target="#b12">[13]</ref> settles the problem well.</p><p>After we generate S * from S, there are three situations.</p><p>• No Change. When the new structure does not change the number of lt() nodes, the dimensionality of parameters does not change. In this case, it is sufficient to use ordinary MH step. Here the set of lt() nodes may change, but the sampling of new parameters is i.i.d., so we are satisfied with the MH step.</p><p>• Expansion. When the number of lt() nodes increases, the dimensionality of Θ, denoted by p Θ , increases. We may simultaneously lose some original lt() nodes and have more new ones. But due to the i.i.d. nature of parameters we only consider the number of all lt() nodes.</p><p>Denote the new parameter as Θ * . According to RJMCMC, we sample auxiliary variables</p><formula xml:id="formula_12">U = (u Θ , u n ) where dim(u Θ ) = dim(Θ), dim(u n ) + dim(Θ) = dim(Θ * ).</formula><p>The hyper-parameters U σ = (σ 2 a , σ 2 b ) are independently sampled from the inverse gamma prior, then each element of u Θ and u n is independently sampled from N (1, σ 2 a ) or N (0, σ 2 b ) accordingly. The new parameter Θ * along with new auxiliary variable U * is obtained by</p><formula xml:id="formula_13">(U * , Θ * , σ * Θ ) = je(Θ, U, Uσ) = je(Θ, uΘ, un, Uσ) = Θ − uΘ 2 , Θ + uΘ 2 , un, Uσ ,</formula><p>where</p><formula xml:id="formula_14">U * = Θ − uΘ 2 , Θ * = ( Θ + uΘ 2 , un), σ * Θ = Uσ.</formula><p>Then we discard U * and get Θ * , σ * Θ .</p><p>• Shrinkage. Θ shrinks when the number of lt() nodes decreases. Similar to the Expansion case, we may lose some lt() nodes and also have new ones (especially in the ReassignOperator transition), but only the dimensionality is of interest. Assume that the original parameter is Θ = (Θ 0 , Θ d ) where Θ d corresponds to the parameters of nodes to be dropped. Denote the new parameter as Θ * .</p><p>Firstly, U σ = (σ 2 a , σ 2 b ) are sampled independently from the inverse gamma prior. The new parameter candidate is then obtained by first sampling U , whose elements are independently sampled from N (0, σ 2 a ) and N (0, σ 2 b ), respectively, with dim(U ) = dim(Θ 0 ). Then the new candidate Θ * as well as the corresponding auxiliary variable U * is obtained by</p><formula xml:id="formula_15">(σ * Θ , Θ * , U * ) = js(Uσ, U, Θ) = js(Uσ, U, Θ0, Θ d ) = (Uσ, Θ0 + U, Θ0 − U, Θ d ),</formula><p>where</p><formula xml:id="formula_16">σ * Θ = Uσ, Θ * = Θ0 + U, U * = (Θ0 − U, Θ d ).</formula><p>Then we just discard U * and get U * , σ * Θ .</p><p>For simplicity, we denote the two transformation j e , j s as j S,S * , indicating that this is a transformation from parameters of S to those of S * . The auxiliary variables are denoted as U and U * respectively. Note that dim(Θ) + dim(U ) = dim(Θ * ) + dim(U * ) in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accepting New Candidates</head><p>Return to the K-tree case. We sequentially update the K trees in a way similar to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref>. Suppose we start from tree (T</p><formula xml:id="formula_17">(t) j , M (t) j , Θ (t)</formula><p>j ), that is, the j-th tree of the t-th accepted model, and that the newly proposed structure is</p><formula xml:id="formula_18">(T * j , M * j , Θ * j ). Denote (T (t) , M (t) , Θ (t) ) = {(T (t) i , M (t) i , Θ (t) i )} k i=1 , (T * , M * , Θ * ) = {(T * i , M * i , Θ * i )} k i=1 ,</formula><p>where</p><formula xml:id="formula_19">(T * i , M * i , Θ * i ) = (T (t) i , M (t) i , Θ<label>(t)</label></formula><p>i ) for i = j. Also let S * = (T * , M * ), S (t) = (T (t) , M (t) ). And (σ * ) 2 is the newly-sampled version of (σ (t) ) 2 . For simplicity, let</p><formula xml:id="formula_20">Σ (t) = (σ (t) ) 2 , σ (t) Θ and Σ * = (σ * ) 2 , σ * Θ . If dim(Θ (t) i ) = dim(Θ * ), the ordinary MH step gives the acceptance rate R = f (y | OLS(x, S * , Θ * ), Σ * )f (S * )q(S (t) | S * ) f (y | OLS(x, S (t) , Θ (t) ), Σ (t) )f (S (t) )q(S * | S (t) ) . (<label>1</label></formula><formula xml:id="formula_21">)</formula><p>If dim(Θ (t) i ) = dim(Θ * ), the RJMCMC method gives the acceptance rate</p><formula xml:id="formula_22">R = f (y | OLS(x, S * , Θ * ), Σ * )f (Θ * | S * )q(S (t) | S * ) f (y | OLS(x, S (t) , Θ (t) ), Σ (t) )f (Θ (t) | S (t) )q(S * | S (t) ) • f (S * )p(Σ * )h(U * | Θ * , S * , S (t) ) f (S (t) )p(Σ (t) )h(U (t) | Θ (t) , S (t) , S * ) • ∂j S (t) ,S * (Θ (t) , U (t) ) ∂(Θ (t) , U (t) )<label>(2)</label></formula><p>In each case, we accept the new candidate with probability α = min{1, R} with R in Equation ( <ref type="formula" target="#formula_20">1</ref>) or <ref type="bibr" target="#b1">(2)</ref>. If the new candidate is accepted, we next update the (j + 1)th tree starting from (T (t+1) , M (t+1) , Θ (t+1) ) = (T * , M * , Θ * ) and Σ (t+1) = Σ * . Otherwise we update the (j + 1)-th tree starting at (T (t) , M (t) , Θ (t) ) with Σ (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-codes of the Algorithm</head><p>We sum up the algorithm as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 pseudo-codes of MCMC-based Symbolic Regression for linearly-mixed signals</head><p>Input: Datapoints x1, . . . , xn, labels y = (y1, . . . , yn); number of components K, number of acceptance N ; transition kernel (proposal) q(• | •), prior distributions p(T, M, Θ), likelihood function f (y | OLS(S, Θ, x)); Output: A chain of accepted models (T (t) , M (t) , Θ (t) );</p><p>1: From prior p(T, M, Θ), generate independently K tree models (structures and parameters) (T</p><p>i , M</p><p>i , Θ</p><p>i ), i = 1, . . . , K; 2: Calculate linear regression coefficients β (1) from datapoints xi, labels yi and models (T</p><formula xml:id="formula_26">(1) i , M<label>(1)</label></formula><p>i , Θ</p><p>i ), i = 1, . . . , n using OLS; 3: Number of accepted models m = 1; 4: while m &lt; N do 5:</p><formula xml:id="formula_28">for i = 1 → K do 6: Propose S * i = (T * i , M * i ) by sampling S * i | S (m) i ∼ q(•; S (m) i ); 7: if dim(Θ * i ) = dim(Θ<label>(m) i ) then 8: Sample U</label></formula><formula xml:id="formula_29">(m) i ∼ h(U (m) i | Θ (m) i , S<label>(m) i , S * i ); 9:</label></formula><p>Obtain</p><formula xml:id="formula_30">(U * i , Θ * i ) = j S (m) i ,S * i (Θ<label>(m) i , U (m) i ); 10:</label></formula><p>Calculate linear regression coefficients β * from datapoints xi, labels yi and models (T * , M * , Θ * ) using OLS; Calculate coefficients β * from xi, yi, i = 1, . . . , n and models (T (m) , M (m) , Θ (m) ) using OLS; if u &lt; α then 20: </p><formula xml:id="formula_31">for j = 1 → K do 21: if j = i then 22: S (m+1) j ← S * j , Θ<label>(m+1)</label></formula><formula xml:id="formula_32">S (m+1) j ← S (m) j , Θ<label>(m+1) j ← Θ (m) j ; 25:</label></formula><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>end for 27:</p><formula xml:id="formula_33">β (m+1) ← β * ; 28: m ← m + 1; 29: end if 30:</formula><p>end for 31: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>The experimental results of GP and BSR methods are presented here. Firstly, we compare their generalization and domain adaptation ability by comparing RMSEs on both training and testing data. Secondly, we compare the complexity of the mathematical expressions generated by these two methods. Finally, we examine the robustness of the proposed BSR method by testing whether the estimated model is sensitive to the parameter K, which is the number of trees used in the linear regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Designs Benchmark Problems</head><p>We set up a benchmark mathematical expression sets with six tasks presented from Equations (3) to <ref type="bibr" target="#b7">(8)</ref>. These task formulas borrow the ideas from <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b3">[4]</ref>, and are used to evaluate the proposed model. The following task is to estimate a specific BSR model on each of the formulas. As we have mentioned, these problems have been widely used to test other symbolic regression methods, including those based on GP, and so it is convenient for us to compare BSR with GP directly. </p><formula xml:id="formula_34">f1(x0, x1) = 2.5x 4 0 − 1.3x 3 0 + 0.5x 2 1 − 1.7x1<label>(3)</label></formula><formula xml:id="formula_35">f2(x0, x1) = 8x 2 0 + 8x 3 1 − 15 (4) f3(x0, x1) = 0.2x 3 0 + 0.5x 3 1 − 1.2x1 − 0.5x0<label>(5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on simulated data. The experiment designed and used in <ref type="bibr" target="#b4">[5]</ref> is adopted here. For each task (corresponding to an expression), we have one training data set and three testing data sets. The training set consists of 100 samples. For each sample, its predictor variables are generated independently from uniform distributions in the interval [−3, 3], and the response variable is generated by the corresponding expression above. We consider three different testing sets, all with size of 30. Following the same way to generate training set, the three testing sets are generated within intervals [−3, 3], [−6, 6] and <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Settings</head><p>The detailed settings of the two methods are introduced as follows. Note that the GP algorithm consists of two nested iterations, the inner loop for population and the outer loop for generation. Therefore, the number of trees generated by GP is counted as</p><formula xml:id="formula_37">N g × N p</formula><p>, where N g is the number of generations and N p is the population size. We set N g = 200 and N p = 100 in this study, and therefore a total of 200,000 trees are generated. Meanwhile, the number of trees generated by BSR is set as 100,000. In addition, we specify two additive components of BSR are used for all tasks in our study.</p><p>The basis function pool is set as {+, −, ×, ÷, sin, cos, exp, x 2 , x 3 } for both BSR and GP method. In order to see the stability of their performances, we run the two methods in each task for 50 times independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy and Generalization Abilities</head><p>We use root mean square error (RMSE) on training set to measure how good the model fits the data, and use RMSE on test set to examine the generalization ability. The performance of two methods in all task are summarized in Table <ref type="table" target="#tab_1">1</ref>. This table records the mean and standard deviation of RMSEs over 50 simulation replications for each task. It turns out that BSR outperforms GP in most tasks, except the task defined in equation <ref type="bibr" target="#b7">(8)</ref>. A plausible reason is that the structure of function ( <ref type="formula" target="#formula_28">8</ref>) is far from linear structure, which is one of the key assumptions of BSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of Expressions</head><p>One of the most important aim we propose BSR is to improve interpretability of symbolic regression model by restricting the symbolic expression in a concise and readable form. Specifically, we introduce an additive symbolic tree structure for BSR model.</p><p>To check if BSR has advantage in interpretability, we summarize the complexity of the output from BSR and GP in Table <ref type="table" target="#tab_2">2</ref>, where the numbers are the means and standard deviations of the output tree sizes (number of nodes in each tree) out of 50 replications.</p><p>According to Table <ref type="table" target="#tab_2">2</ref>, the number of nodes on trees generated by BSR is significantly less that those generated by GP, leading to more concise and readable expressions. Table <ref type="table" target="#tab_3">3</ref> lists some typical expressions output from BSR and GP. It turns out that expressions estimated by BSR are generally closer to the ground truth and they are shorter and more comprehensible. The simulation study here verifies that, in favourable scenarios, BSR reaches its aim and shows its advantage in both prediction accuracy and interpretability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to the Number of Components K</head><p>The number of additive components K is an important hyper-parameter in BSR model and it is interesting to study if the optimal expression selected by BSR is sensitive to the choice of K. To check this, we summarize the average RMSEs on testing set [−3, 3] out of 50 replications in Table <ref type="table" target="#tab_4">4</ref>.</p><p>From the results we see that the RMSEs of these tasks tend to be smaller as K grows, but the improvement of performance is not significant when K is too large. To our surprise, experiments show that even if K is set to be smaller than what it should be (ground truth), BSR can automatically find an approximately equivalent additive component structure in some single trees. On the other hand, when K is significantly larger than what it should be, BSR automatically "discards" the redundant trees by estimating the non-informative trees as insignificant components, making them similar   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Research</head><p>This paper proposes a new symbolic regression method based on Bayesian statistics framework. Compared with traditional SR methods such as GP, the proposed method exhibits its advantage in several aspects, including better model interpretability, more executable prior incorporate way and more cost-effective memory use etc.</p><p>In the future, we continue this research and further improve and develop better method for symbolic regression. For example, we will study new MCMC algorithms to improve the search and sampling efficiency; we will study a dynamic empirical bayes method to optimize hyper-parameters in BSR; we will also research how to extend the proposed algorithm in a paralleled form in order to improve the computational efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 :</head><label>21</label><figDesc>Figure 1: Tree representation of cos(x 1 + x 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) f4(x0, x1) = 1.5 exp(x0) + 5 cos(x1) (6) f5(x0, x1) = 6.0 sin(x0) cos(x1) (7) f6(x0, x1) = 1.35x0x1 + 5.5 sin((x0 − 1)(x1 − 1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>+ 0.81)) − (((sin((0.80x0) 2 ) − cos(x1)6 ) + sin((0.80x0) 2 )) + cos(x1))) 80 ) + cos(x1)) + ((sin((0.71x0)2 ) −((sin(((0.71x0)) 2 ) − 0.77)) 2 ) + 1.0)) + x1))+(0.76 + x1))) + ((( cos((0.90x0)) * (exp(0.187) + cos((x 2 0 cos(0.75)))))) +(x1 − 0.77) 3 + exp(x1 − 0.38)(x1 − 0.38) BSR y = (−0.02) + (−1.38)[−7.56x 2 0 + 2.85] +(8.00)[−0.30x 2 0 + x 3 1 − 1.38] f3 Truth f3 = 0.2x 3 0 + 0.5x 3 1 − 1.2x1 − 0.5x0 GP y = (4x1 − sin(1.32x1) − 0.69 −(sin(sin(1.32x1)/0.50)/0.76)) − sin(x0) − sin(sin(sin((cos(x1) + x1)))) BSR y = (0.04) + (−0.30)[−0.67x 3 0 + 4.27] +(−0.21)[−2.45x 3 1 + 2.45x0 + x1 − 0.93] f4 Truth f4 = 1.5 exp(x0) + 5 cos(x1) GP y = (((((exp(cos(x0)) + 0.59 + x0) +exp(x0)) − cos(exp(cos(x1)))) − cos(exp(cos(sin(x1)x0)))) −x 2 1 + x 2 0 ) BSR y = (−0.01) + (0.28)[17.74 cos(x1) + 0.45] +(0.24)[6.26exp(x0) − 0.47] f5 Truth f5 = 6.0 sin(x0) cos(x1) GP y = 0.77exp(exp(sin(sin(cos(0.73x0))))) * x0 cos(x1) BSR y = (−7.06 * 10 −9 ) + (6.00)[sin(x0) cos(x1)] +(2.66 * 10 −9 )[sin( 0.34 sin 2 (x 1 ) −0.93exp(x0 + x1) − 0.95)] f6 Truth f6 = 1.35x0x1 + 5.5 sin((x0 − 1)(x1 − 1)) GP y = ((((((x1 sin(x0) + x1x0 − sin( −x 0 0.36 ) − sin((x0 + x1))) − sin((x0x1) 2 )) + sin( x 1 0.36 )) − sin((x1 sin(x0)) 2 )) − sin( −x 0 0.36 )) − sin(x1 sin(x0) + x1x0)) − sin(x1 sin(x0) + x1x0) BSR y = (−0.19) + (−0.85)[1.69x0x1 + 1.19] +(7.00 * 10 −3 )[exp(sin(exp(exp(exp(x1) +(1.37x1 − 1.01) 3 ) 3 )))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>j</figDesc><table><row><cell>23:</cell><cell>else</cell></row><row><cell>24:</cell><cell></cell></row><row><cell></cell><cell>← Θ  *  j ;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>RMSEs of Both Methods</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RMSEs(mean ± std)</cell></row><row><cell>Task</cell><cell>Dataset</cell><cell>BSR</cell><cell>GP</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>2.00 ± 3.87</cell><cell>2.71 ± 2.43</cell></row><row><cell>f 1</cell><cell>test[-3,3] test[-6,6]</cell><cell>2.04 ± 3.27 92.09 ± 258.54</cell><cell>4.25 ± 4.59 116.29 ± 97.59</cell></row><row><cell></cell><cell>test[3,6]</cell><cell cols="2">118.53 ± 311.57 203.31 ± 168.34</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>7.30 ± 10.19</cell><cell>3.56 ± 5.79</cell></row><row><cell>f 2</cell><cell>test[-3,3] test[-6,6]</cell><cell>6.84 ± 10.10 95.33 ± 145.31</cell><cell>2.92 ± 4.41 121.41 ± 126.19</cell></row><row><cell></cell><cell>test[3,6]</cell><cell cols="2">128.27 ± 221.73 174.01 ± 173.71</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>0.19 ± 0.16</cell><cell>0.63 ± 0.33</cell></row><row><cell>f 3</cell><cell>test[-3,3] test[-6,6]</cell><cell>0.21 ± 0.20 9.38 ± 9.08</cell><cell>0.60 ± 0.35 28.97 ± 20.68</cell></row><row><cell></cell><cell>test[3,6]</cell><cell>15.19 ± 32.24</cell><cell>34.08 ± 25.41</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>0.14 ± 0.56</cell><cell>0.72 ± 1.01</cell></row><row><cell>f 4</cell><cell>test[-3,3] test[-6,6]</cell><cell>0.16 ± 0.62 6.96 ± 19.44</cell><cell>0.84 ± 1.12 24.62 ± 29.66</cell></row><row><cell></cell><cell>test[3,6]</cell><cell>12.06 ± 38.27</cell><cell>31.74 ± 36.77</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>0.68 ± 1.14</cell><cell>0.78 ± 0.96</cell></row><row><cell>f 5</cell><cell>test[-3,3] test[-6,6]</cell><cell>0.66 ± 1.13 1.09 ± 2.39</cell><cell>0.72 ± 0.83 1.58 ± 1.55</cell></row><row><cell></cell><cell>test[3,6]</cell><cell>1.41 ± 3.57</cell><cell>4.49 ± 5.07</cell></row><row><cell></cell><cell>train[-3,3]</cell><cell>3.99 ± 0.71</cell><cell>3.17 ± 0.79</cell></row><row><cell>f 6</cell><cell>test[-3,3] test[-6,6]</cell><cell>4.63 ± 0.62 12.22 ± 8.46</cell><cell>3.70 ± 0.93 5.13 ± 1.91</cell></row><row><cell></cell><cell>test[3,6]</cell><cell>14.44 ± 10.39</cell><cell>11.09 ± 12.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Complexity of Expressions</figDesc><table><row><cell></cell><cell cols="2">Number of Nodes(mean ± std)</cell></row><row><cell>Task</cell><cell>BSR</cell><cell>GP</cell></row><row><cell>f 1</cell><cell>22.16 ± 7.44</cell><cell>40.85 ± 21.34</cell></row><row><cell>f 2</cell><cell>12.25 ± 11.41</cell><cell>54.51 ± 38.89</cell></row><row><cell>f 3</cell><cell>27.23 ± 10.61</cell><cell>22.88 ± 8.62</cell></row><row><cell>f 4</cell><cell>13.64 ± 12.50</cell><cell>22.80 ± 8.82</cell></row><row><cell>f 5</cell><cell>31.28 ± 9.13</cell><cell>19.80 ± 10.28</cell></row><row><cell>f 6</cell><cell>20.08 ± 4.78</cell><cell>21.18 ± 25.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Typical Expressions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>RMSEs for different K</figDesc><table><row><cell></cell><cell cols="2">RMSE(mean ± std)</cell><cell></cell></row><row><cell>Task</cell><cell>K=2</cell><cell>K=4</cell><cell>K=8</cell></row><row><cell>f 1</cell><cell>2.04 ± 3.27</cell><cell cols="2">2.86 ± 5.04 0.64 ± 2.46</cell></row><row><cell>f 2</cell><cell cols="2">6.84 ± 10.10 0.02 ± 0.03</cell><cell>0.03 ± 0.1</cell></row><row><cell>f 3</cell><cell>0.21 ± 0.20</cell><cell cols="2">0.06 ± 0.03 0.03 ± 0.02</cell></row><row><cell>f 4</cell><cell>0.16 ± 0.62</cell><cell cols="2">0.03 ± 0.06 0.01 ± 0.01</cell></row><row><cell>f 5</cell><cell>0.66 ± 1.13</cell><cell cols="2">0.29 ± 0.80 0.42 ± 0.94</cell></row><row><cell>f 6</cell><cell>4.63 ± 0.62</cell><cell cols="2">4.00 ± 0.34 5.28 ± 4.38</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical genetic programming for symbolic regression</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Amir Haeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mehdi Ebadzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluigi</forename><surname>Folino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="447" to="469" />
			<date type="published" when="2017-11">November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A novel continuous representation of genetic programmings using recurrent neural networks for symbolic regression</title>
		<author>
			<persName><forename type="first">Aftab</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Orchard</surname></persName>
		</author>
		<idno>CoRR, abs/1904.03368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Elite bases regression: A realtime algorithm for symbolic regression</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changtong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR, abs/1704.07313</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving generalisation of genetic programming for symbolic regression with structural risk minimisation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalisation and domain adaptation in gp with gradient descent for symbolic regression</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE congress on evolutionary computation (CEC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature selection to improve generalization of genetic programming for high-dimensional symbolic regression</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="792" to="806" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian cart model search</title>
		<author>
			<persName><forename type="first">Hugh</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">443</biblScope>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian additive regression trees</title>
		<author>
			<persName><forename type="first">Hugh</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010">03 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical modeling using symbolic regression via postfix genetic programming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vipul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">K</forename><surname>Dabhi</surname></persName>
		</author>
		<author>
			<persName><surname>Vij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Image Information Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Method for the identification of explicit polynomial formulae for the friction in turbulent pipe flow</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Savic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydroinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Symbolic and numerical regression: experiments and applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Savic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="117" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Recent Advances in Soft Computing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A greedy search tree heuristic for symbolic regression</title>
		<author>
			<persName><forename type="first">Fabrcio</forename><surname>Olivetti De Frana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reversible jump markov chain monte carlo computation and bayesian model determination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="711" to="732" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian backfitting (with comments and a rejoinder by the authors</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="223" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving genetic programming based symbolic regression using deterministic machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Icke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bongard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Congress on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="1763" to="1770" />
			<date type="published" when="2013-06">2013. June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving symbolic regression with interval arithmetic and linear scaling</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Keijzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic Programming</title>
				<editor>
			<persName><forename type="first">Conor</forename><surname>Ryan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Terence</forename><surname>Soule</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Keijzer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edward</forename><surname>Tsang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Riccardo</forename><surname>Poli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ernesto</forename><surname>Costa</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="70" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaled symbolic regression</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Keijzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming and Evolvable Machines</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="269" />
			<date type="published" when="2004-09">Sep 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Local optimization and complexity control for symbolic regression</title>
		<author>
			<persName><surname>Michael [verfasserin] Kommenda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Accuracy in Symbolic Regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Korns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="129" to="151" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural-guided symbolic regression with semantic prior</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno>CoRR, abs/1901.07714</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A divide and conquer method for symbolic regression</title>
		<author>
			<persName><forename type="first">Changtong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2017-05">05 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Trent</forename><surname>Mcconaghy</surname></persName>
		</author>
		<title level="m">FFX: Fast, Scalable, Deterministic Symbolic Regression Technology</title>
				<meeting><address><addrLine>New York, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="235" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusta</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the effect of function set to the generalisation of symbolic regression models</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Agapitos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pareto-Front Exploitation in Symbolic Regression</title>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Kotanchek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="283" to="299" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster genetic programming based on local gradient search of numeric leaf values</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation</title>
				<meeting>the 3rd Annual Conference on Genetic and Evolutionary Computation</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Model-based problem solving through symbolic regression via pareto genetic programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vladislavleva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Vladislavleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hertog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="349" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Genetic programming: an introduction and survey of applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference On Genetic Algorithms In Engineering Systems: Innovations And Applications</title>
				<imprint>
			<date type="published" when="1997-09">Sep. 1997</date>
			<biblScope unit="page" from="314" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Symbolic regression algorithms with built-in linear regression</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Zegklitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Posík</surname></persName>
		</author>
		<idno>CoRR, abs/1701.03641</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
