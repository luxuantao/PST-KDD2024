<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trellis-Coded Vector Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Min</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Washington State University</orgName>
								<address>
									<postCode>99164-2752</postCode>
									<settlement>Pullman</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<postCode>85721</postCode>
									<settlement>Tucson</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Wisconsin-Platteville</orgName>
								<address>
									<postCode>53818-3099</postCode>
									<settlement>Platteville</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trellis-Coded Vector Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A887025B37D949A62E96280A3905910</idno>
					<note type="submission">received September 7, 1988; revised March 1, 1991.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Trellis coding</term>
					<term>vector quantization</term>
					<term>source coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Trellis-coded quantization is generalized to allow a vector reproduction alphabet. Three encoding structures are described, several encoder design rules are presented, and two design algorithms are developed. It is shown that for a stationary, ergodic vector source, if the optimized trellis-coded vector quantization reproduction process is jointly stationary and ergodic with the source, then the quantization noise is zero-mean and of variance equal to the difference between the source variance and the variance of the reproduction sequence. Several examples illustrate the encoder design procedure and performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>RELLIS-CODED quantization is a form of trellis T coding that labels the trellis branches with subsets of reproduction symbols. The approach was motivated by trellis-coded modulation <ref type="bibr">[l]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and made use of the alphabet-constrained rate-distortion theory <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr">[5]</ref>. The design of trellis-coded scalar quantization systems for memoryless and Gauss-Markov sources and for predictive coding of speech is described in <ref type="bibr">[61, [71.</ref> In the present paper, we focus on vector reproduction symbols and trellis coded vector quantization (TCVQ).</p><p>In intuitive terms, TCVQ may be viewed as soft-decision quantization, but based on several lower-rate quantizers, whose cumulative number of codewords exceeds that allowed by the encoding rate. The novel feature of TCVQ is the partitioning of an expanded set of vector quantization symbols into subsets and the labeling of the trellis branches with these subsets. The trellis code folds the individual vector (or scalar) quantizers (subsets) into a "larger-dimensional" quantizer, with better performance. Labeling the trellis branches with properly formed subsets (instead of individual reproduction symbols) leads to a considerable reduction in encoding complexity, compared to traditional trellis encoding.</p><p>One significant innovation to the generalized Lloyd algorithm vector quantizer [SI is finite state vector quantization (FSVQ) <ref type="bibr">[9]</ref>, <ref type="bibr">[lo]</ref>. As the name implies, FSVQ is based on a finite state machine, and uses state-dependent codebooks that can take advantage of time dependence between a codeword and the next source vector. For sources with memory, FSVQ can provide significant improvement over standard VQ <ref type="bibr">[9]</ref>, <ref type="bibr">[lo]</ref>, however, for memoryless sources FSVQ offers no improvement over VQ. FSVQ has the disadvantage that it can be catastrophically affected by channel errors <ref type="bibr">[lo]</ref>.</p><p>In <ref type="bibr">[lo]</ref>, it is suggested that FSVQ can be generalized to allow a Viterbi encoding <ref type="bibr">[ l l</ref> ] search of the finite state machine trellis. In <ref type="bibr" target="#b5">[12]</ref>, a vector trellis encoding system is described that uses the M-L algorithm <ref type="bibr" target="#b6">[13]</ref> to search the FSVQ trellis. TCVQ should be regarded as a special case of the general trellis-searched FSVQ structure. The distinctive features of the TCVQ approach are the use of codebook partitioning and the labeling of trellis branches with subsets. In addition, with feedback-free implementations of the convolutional encoder that defines the trellis and branch labels <ref type="bibr">[l]</ref>, TCVQ is robust to channel errors and is naturally compatible with trellis-coded modulation Although trellis-searched FSVQ can outperform VQ for the encoding of memoryless sources, the main emphasis of <ref type="bibr" target="#b5">[12]</ref>, as well as of [9], <ref type="bibr">[lo]</ref>, is on the encoding of sources with memory without requiring an explicit predictor, such as is used in vector predictive coding [151. An encoder that uses state-dependent codebooks may be thought of as having implicit nonlinear prediction. TCVQ can also be used in this manner, and we provide an example of such a TCVQ design. Recent results <ref type="bibr" target="#b9">[16]</ref>, [61 seem to indicate, however, that incorporating the trellis search inside a predictive coding structure will provide encoding performance superior to FSVQ. That is, the infinite-state predictive (differential) encoding structure seems to be inherently superior to the finite-state structure, at least for the encoding of the Gauss-Markov sources considered in <ref type="bibr" target="#b9">[16]</ref> and <ref type="bibr">[6]</ref>. Hence, our approach is more strongly motivated by our wish to incorporate TCVQ in a trellis-searched vector predictive encoding structure, than by FSVQ.</p><p>In a predictive coding structure, it is often reasonable to model the prediction error as a memoryless source. With a suitable predictor, such a model may be justifiable for large encoding rates, although for low encoding rates the memoryless model may not be valid <ref type="bibr">[19, p.</ref> 2791. In the ~4 1 . 0018-9448/91$01.00 01991 IEEE vector case, the corresponding prediction error model is a memoryless vector source; that is, there may be dependence between the components within a vector, but consecutive vectors are independent. Even if the prediction error has memory, it is fairly common to ignore any memory in designing the quantizer. For example, in predictive VQ <ref type="bibr" target="#b8">[15]</ref>, the codebook is designed using the vector prediction error as the training sequence for the generalized Lloyd algorithm [8]. As another example, the predictive TCQ encoders for Gauss-Markov sources in [6] used codebooks designed for the memoryless Gaussian source. (Notable exceptions, however, are adaptive scalar quantizers, such as the Jayant quantizer [ 191, which take advantage of a certain type of memory in the speech prediction error of differential pulse code modulation.) We believe that the memoryless vector source model is useful for the design of codebooks for a variety of source coding systems, such as vector predictive and code-excited linear predictive speech coders. The memoryless vector source model also arises in subband image coding (the high frequency subbands display very little correlation) and in image transform coding (if the vectors are formed from like transform coefficients of spatially separated blocks). While TCVQ is by no means limited to memoryless sources, the emphasis of this paper is on the design of TCVQ for such sources. The designs can then be incorporated into trellis-searched predictive, transform, or subband structures to encode sources with memory.</p><p>The main purpose of the present paper is to provide a design methodology for TCVQ, focusing on three encoding structures. The first structure is based on an arbitrary VQ (such as a codebook designed with the generalized Lloyd algorithm, or one constrained to have as codewords the points in a lattice). The second structure is based on a two step differential encoding scheme and labels the trellis branches with codeword subsets from two codebooks. The third structure also features a two step encoding procedure, but the second step uses trellis-coded quantization with lower-dimensional symbols assigned to the trellis branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A question of both practical and theoretical interest is,</head><p>what is the minimum size of reproduction alphabet necessary for near-optimum encoding performance? FSVQ, as well as traditional trellis encoders, tend to have a large number of branches entering and leaving each trellis state, and allow for a unique codeword (or set of codewords) to be assigned to each branch. This high connectivity and large super codebook cause large complexity in an optimum (Viterbi algorithm) search for the minimum distortion path through the trellis. Following the description of the TCVQ structures, we address the question of codebook size by generalizing the alphabet-constrained rate-distortion theory <ref type="bibr" target="#b2">[3]</ref>-[61. This theory provides a straightforward (albeit, computationally intensive) technique for estimating the minimum codebook size required for near-optimum encoding of a memoryless source.</p><p>The design of TCVQ coders is then considered, with partition and branch labeling rules presented for memory-less vector sources. We view important features of the TCVQ design to be the selection of the initial codebook, the partitioning of this codebook into subsets, and the labeling of the trellis branches with these subsets. Once the initial codebook, subsets, and trellis labeling are selected, a codebook optimization algorithm can be used to (locally) optimize the codebook subject to the TCVQ structure. Ad hoc design rules are described to guide the initial TCVQ design, however, an optimization algorithm is used to construct the final codebook. The main purpose of the design rules is to provide "good" initial conditions for an optimization problem that may have many local optima. Training sequence-based algorithms are presented for designing the reproduction alphabets for two of the TCVQ structures. One of the algorithms is basically an application of the Stewart, Gray, and Linde trellis design algorithm <ref type="bibr" target="#b13">[20]</ref>, to the TCVQ structure. It is shown that the optimized TCVQ coder has a "conditional mean" property, which closely parallels a property of the generalized Lloyd algorithm VQ. Finally, several examples are provided that describe the TCVQ construction and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">CODER STRUCTURES AND COMPLEXITY</head><p>Let x ( n ) be a sequence of L-dimensional vector source symbols and i ( n ) a sequence of (vector) reproduction symbols. We assume the distortion measure, d(x, 21, is mean-squared error (mse), however, most of the development can be generalized to other measures of encoding fidelity. In this section several TCVQ encoding structures are defined and their encoding complexity is summarized. Later sections provide examples of TCVQ design and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structures</head><p>Let R be the encoding rate (in bits/dimension), L the source vector dimension, I a positive integer such that L / 1 is an integer, and M a positive integer. The product RL is assumed to be an integer, so that a nominal VQ would have 2RL codewords and an encoding rate of RL bits/vector. Consider an N-state trellis with 2 M branches entering and leaving each trellis state. If the branches are labeled with subsets of I-dimensional reproduction symbols, then M / 1 bits/dimension are required to uniquely specify a sequence of branches (a path) through the trellis and L / I consecutive branches in the trellis correspond to one source vector of dimension L. (We use interchangeably the terms reproduction vector, reproduction symbol, and codeword.) For a given trellis, branch labeling, set of reproduction symbols, and single letter distortion measure, the optimum encoding of a sequence of source vectors is accomplished by finding the minimum distortion sequence of reproduction symbols corresponding to a legitimate path through the trellis. The Viterbi algorithm [ll] is an efficient technique for trellis encoding, although suboptimum techniques can also be used <ref type="bibr" target="#b6">[13]</ref>. The re-mainder of the paper will assume that the Viterbi algorithm is used for encoding.</p><p>We consider three distinct ways to incorporate vector quantization with TCQ.</p><p>Structure 1: For R &gt; 0, R 2 0, M 2 1, L = 1, and RL and RL integers satisfying RL 2 M , form a codebook, say 9, of 2(R+R)L vector codewords. We refer to R as the (super) "codebook expansion factor" (in bits per dimension) since the codebook 9 has 2 R L times as many codewords as would a nominal rate-R VQ. Let K = RL + M and partition the codebook, according to rules to be discussed later, into 2 K subsets, denoted as S,, S, . . * , S 2 ~.</p><p>Each subset has exactly 2 R L -M vectors.</p><p>An N-state trellis is constructed with 2 M branches entering and leaving each state. Each branch is labeled with one of the subsets, S,. If each subset is to be assigned to at least one branch, then there must be N 2 2 R L trellis states. The choice of trellis and branch labeling are important aspects of the design, and are discussed in more detail later. The trellises and branch labelings of Ungerboeck [l] are particularly useful for M = 1 or 2. Given an initial state in the trellis, M bits/vector specify a unique sequence of branches, and thus a unique sequence of subsets. The remaining RL-M bits/vector specify a unique codeword within each subset, so that the transmission rate is RL bits/vector. The encoding is accomplished in two steps.</p><p>Step 1) For each input vector, x, find the closest codeword and corresponding distortion, d,, in each subset S,.</p><p>Step 2) Let the branch metric for a branch labeled with subset S, be the distortion found in Step 11, and use the Viterbi algorithm to find the minimum distortion path through the trellis. Selection of the minimum distortion pair, yi, zj, is generally computionally' intensive. One useful (suboptimum) alternative is to choose yi E S, based solely on the input, x, and then select zj â‚¬ C j based only on the difference vector, e = xyi.</p><p>It takes M bits/vector-to specify a path through the trellis, and R'L -K and RL -J bits/vector, respectively, to specify codewords in Si and Cj. If we wish to encode at an average rate of R bits/sample (RL an integer), then the rate constraint is</p><formula xml:id="formula_0">RL = ( R I + R ) L + M -K -J .</formula><p>(1)</p><p>The codebooks 9 and 8 must be designed so that the encoder yields small distortion. Selecting the trellis and the branch labeling is also part of the design problem.</p><p>However, if we require that all combinations Si, C j appear as labels for at least one branch in the trellis, then</p><formula xml:id="formula_1">N 2 2 K + J / 2 M , which, from (11, implies N 2(R'+R-R)L</formula><p>This exponential growth in the number of trellis states, which also occurs with Structure 1, is a significant practical limitation. Structure 3: As a modification to the format of Structure 2, let K = 0, R 2 J / L , and with R 2 M/l, select 1 s L so that JI/L and L / 1 are integers. The codebook 4 has 2R'L codewords and we assume that it is formed RL bits/vector are required to specify the sequence of using the generalized Lloyd algorithm [81. Since K = 0 , codewords corresponding to this minimum distortion path. quence of vectors and then released, or may be released after a suitable delay, at which point the survivor forced to merge. the is searched Only Once for the best of its Voronoi region, so that the normalized mse for the Voronoi region is the same as the conditional variance of tional variance as 02(y), and observe that a ( y ) can be be estimated from the training data used to design the VQ, of size 2R'L and partition it into 2 K subsets, S,; . ., S Z K , procedure is diagrammed in Fig. <ref type="figure" target="#fig_0">1</ref>, where the vector each 2 R ' L -K codewords. Next, form a codebook, 8, of e = ( X -Y ) / d Y &gt; is encoded with the TCVQ, as follows. size 2 R L and partition it into 2 J subsets, C,; . .,C,J, each Rather than labeling the trellis to necessarily have of ~R L -J codewords. ~i ~~l l ~, form an N-state trellis with branch symbols corresponding to L-dimensional vector 2~ branches entering and leaving each state. Each branch codewords, label the trellis to have I-dimensional branch is labeled with a pair S,,C,, i = 1;. . , 2 K ; j = 1;. .,2J. symbols, with 2 M branches entering and leaving each The encoding is accomplished via a two-step procedure. trellis state. It then takes L / I consecutive branches in the trellis to yield a codeword for e and hence M L / l Step 1) For each branch labeled with the subset pair bits/vector to specify a path through the trellis. The 2 J S,,C,, find the minimum distortion pair of subsets, C,; . ., C,J, (each containing codewords of length codewords, y, E S, and z, E C,, and the result-L) take on the special form of the L/l-fold Cartesian ing distortion d,,, = d ( x , y, + 2,).</p><p>products, as allowed ,by the trellis transitions, of-2 J 1 / L</p><p>Step 2 ) Use the Viterbi algorithm to find the minimum subsets, say C,; . ., C,J//L, each consisting of 2 R 1 -J ' / L distortion path through the trellis.</p><p>I-dimensional codewords. (It is also possible to have L / 1</p><p>These codeword bits may be collected for an entire se-reproduction vector7 Y * Each codeword Y is the centroid paths of the Viterbi algorithm search have merged or are the input vector, X, for the region-We denote this condi-Structure 2: F~~ R &gt; 0, let RIL, K , k, J , and nonnegative integers and let I = L. Form a codebook, J, and then stored for each Y . The (suboptimum) encoding distinct sets of subsets fj, each set corresponding to one of the L / 1 family of branches that encode e.) It takes M / I bits/dimension to specify a path through the trellis, R' bits/dimension to specify a codeword i n -9 , and R -J / L bits/dimension to specify a point in C j , so that the rate constraint is ML 1</p><formula xml:id="formula_2">RL = ( R' + Ei) L + --J .</formula><p>The trellis is constructed to have N states with 2M branches entering and leavinj each state. The branches are labeled with the subsets C,. Encoding is accomplished by finding the closest codeword and corresponding distoftion to that portion of e corresponding to each subset C,, and then using the Viterbi algorithm to determine the minimum distortion path through the trellis. For the mse distortion measure, the ith branch of the sequence of L / l branches corresponding to a codeword for e then has the branch metric 11 <ref type="figure">d ,</ref><ref type="figure">( e ,</ref><ref type="figure">e ^) =</ref> ( e m -i m ) 2 .</p><p>If each of the subsets is to be assigned to at least one branch in the trellis, then the number of trellis states must satisfy (from ( <ref type="formula">2</ref>))</p><formula xml:id="formula_3">m = 1 + ( I -1)1</formula><p>The encoding algorithm is summarized by the following</p><p>Step 1) For each x, find the closest y E 9.</p><p>Step 2) Form e = (xy ) / a ( y ) .</p><p>Step 3)Trellis encode e as e^, with branch metric three steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d,(e, e^&gt;.</head><p>The reproduction vector is formed as P = y + d y ) k .</p><p>One motivation for Structure 3 comes from the selection 1 = 1 and the observation that (at least for large rate) within each Voronoi region the probability density is roughly constant. Trellis coded scalar quantization [61 is an excellent coder for a uniform source with very modest complexity. In addition, if 1 = 1 then R can be made large without requiring the extremely large trellises that would be necessary when using large L with Structure 2.</p><p>An alternative motivation for Structure 3 is the relative simplicity of encoding with certain lattices <ref type="bibr" target="#b8">[15]</ref>, so that the codebook d could be selected as a subset of the points in a lattice. Even if I is selected as large as L, the fast lattice encoding may yield an overall coder that is much less complex than a full search VQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Complexity</head><p>The Viterbi algorithm (mse) encoding complexity of each TCVQ structuce can be easily evaluated. In Structure 1, there are 2RL+M subsets, each of 2RL-M codewords. The first step in the encoding is to find the closest codeword in each subset to the source vector. For unstructured codebooks, such a full search requires L2RL-M additions, L2RL-M multiplies, and 2RL-M -1 two-way comparisons, per subset. Next, each trellis state has 2M entering branches, with branch metrics the respective subset squared error. There are 2 additions and 2 -1 two-way compares to determine the "survivor" path at each trellis state. The total complexity is then</p><formula xml:id="formula_4">L ~( ~+ R ) ~ multiplies, L2(R+R)L + N 2 M additions, 2RL+M(2RL-M -1) + N(2M -1) two-way comparisons (3)</formula><p>per L-dimensional source symbol. (The complexity of a symbol release rule is neglected in this analysis.)</p><p>The TCVQ performance generally improves (for a fixed R ) with an increase in L or N. Based on experimental evaluation of TCVQ encoding performance of a given source, it is possible to model the variation in mse as a function of L and N. Thus, for a given level of complexity, "optimum" values of L and N can be selected for a given application.</p><p>The full search encoding complexity of C-dimensional VQ is roughly</p><formula xml:id="formula_5">L I ~R L ' multiplies, L'2R L' additions, 2RL' -1 two-way comparisons.</formula><p>( 4) Hence, for the same dimension of encoding symbols, TCVQ Structure 1 is more complex than VQ, but also better performing.</p><p>With Structure 2, there are 2R'L-K.2RL-J = 2RL-M pairs of codewords for each distinct pair SI,C,. An exhaustive mse distortion computation would then require multiplies, 2L2RL-M additions and 2RL-M -1 comparisons per pair, although suboptimum searches can be much simpler. Since there are 2 branches per trellis state, for an N state trellis there are at most N 2 M pairs S,,C, to be considered, and hence a maximum computational burden of</p><formula xml:id="formula_6">L ~R L -M NL2RL multiplies, N(2L2RL + 2 M ) additions, ~( 2 ~~ -1) two-way comparisons</formula><p>( 5 ) per L-dimensional source vector.</p><p>The complexity of Structure 3 is easily found to be</p><formula xml:id="formula_7">~( 2 ~' ~ + 2R' + 1) multiplies, L L ( 2R'L + 2R' + 1) + -N2M additions,<label>1</label></formula><p>two-way comparisons, (6) per L-dimensional source vector.</p><p>Comparing the full search VQ complexity of (4) to (3), (9, and (6) reveals that, for L' = L , the TCVQ Structure 1 and 2 complexities are larger, but the Structure 3 complexity may be significantly smaller. Since a TCVQ Structure 1 system will generally perform better than a full search VQ if L'= L , it is also possible that for equal distortion systems (and hence, L &lt; L'), the TCVQ Structure 1 and 2 coders may actually be less complex than full search VQ.</p><p>Tree-searched VQ <ref type="bibr">[26]</ref> is a generally suboptimum VQ implementation that sacrifices a (typically) small increase in distortion and approximately a doubling of the required memory, for a large decrease in encoding complexity. A binary tree-searched VQ has an encoding complexity of approximately 2 m t 2 multiplies, 2 E 2 additions, RL' two-way comparisons, (7) per L-dimensional source vector.</p><p>Naturally, the TCVQ encoders can also be implemented with a binary tree search of the subsets. The resulting complexity (for Structure 1) is then (for RL &gt; M )</p><formula xml:id="formula_8">about ~L ( R L -~) 2 ~~+ ~ multiplies, 2 L ( R L -M ) 2 R L + M + N 2 M additions, (RL -~) 2 " + ~ + ~( 2 ~</formula><p>-1) two-way comparisons, per L-dimensional source vector. This complexity still grows exponentially in the product RL, so there is strong practical incentive for keeping the TCVQ symbol dimension small and for selecting the codebook expansion factor to be the minimum that can provide most of the possible performance improvement over standard vector quantization. A theoretical basis for selecting R is the subject of the next section.</p><p>(8) 111. ANALYSIS There are a number of questions that can be asked of the TCVQ structures, perhaps the most important being how large does the expanded codebook need to be, and what is the possible performance that can be achieved? The answers to these questions are of practical importance since the implementation complexity grows with the size of the codebook. We provide partial answers to these questions by deriving an alphabet-constrained distortionrate function <ref type="bibr">[31-</ref>[61 that lower bounds the achievable mse of TCVQ. Our derivation of this alphabet-constrained distortion-rate function implicitly treats the vector source as memoryless. More precisely, it is developed for the Lth-order distribution function of the source. There may be dependence between the elements within a source vector, but the sequence of source vectors is treated as independent and identically distributed. Although such a model is unrealistic for many sources (e.g., speech and imagery), it does match the typical use of straightforward vector quantization, where correlation between source</p><p>The alphabet-constrained distortion-rate function describes how much improvement over VQ is possible by expanding the (vector) reproduction alphabet. We assume an L-dimensional source vector and distinguish between three cases: 1) the TCVQ is designed based on the Lth-order distribution function; 2 ) the TCVQ is designed in the spirit of FSVQ and uses the nonlinear prediction inherent in the state-dependent codebooks; 3) a predictive TCVQ encoder is designed based on the Lth-order distribution of the prediction error. The analysis of the rate distortion properties of the second case is an open problem which we do not address.</p><p>Let X denote either the source vector in Case 1 or the yediction error in Case 3. The vector X is encoded as <ref type="figure">,</ref><ref type="figure">b,</ref><ref type="figure">},</ref><ref type="figure">where ( b ,</ref><ref type="figure">;</ref><ref type="figure"></ref> . ., 6,) is the J-ary reproduction alphabet. We wish to compute the Lth-order distortion-rate function for X , subject to the constraint of a fixed J-ary alphabet. This alphabet-constrained distortion-rate function is denoted as D J R ) , and is clearly a function of the reproduction alphabet. Unfortunately, we know of no way to compute D,(R) directly, and so adopt the artifice of prequantization to derive an approximating alphabet-constrained distortion-rate function.</p><formula xml:id="formula_9">X E { b , ; . .</formula><p>Let X be prepuantized as U E { a , , . . ., a K } , and then U is encoded as X E { b , ; . ., bJ). Denoting the quantization noise of the prequantizer as Q = U -X , the overall mse (per dimension) is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L --E ( [ U -i ] ' e ) .</head><p>L (9)</p><p>Define f ( q l a k , bj) as the conditional d:nsity for the quantization noise given that U = a k and X = bj. Then, recognizing that f(qlakibj) = f ( q l a k ) for the Lth-order densities, the last term in (9) may be manipulated to yield</p><formula xml:id="formula_10">K = P( a k ) [ a k -E { i l a k } ]</formula><p>TE{Qluk}.</p><p>(</p><formula xml:id="formula_11">) k = l<label>10</label></formula><p>If the prequantizer is a conditional-mean vector quantizer then E{Qla,} = 0 for all k = 1; . ., K , and hence,</p><formula xml:id="formula_12">E( ; l l x -2 1 1 2 ) = E( i l l x -Ull2) + E( i I l U -2,l').<label>(11)</label></formula><p>Let D J R ) be the distortion-rate function for the disvectors is not exploited. crete vector source U and reproduction alphabet</p><formula xml:id="formula_13">3 ' 0 0 1 ' I CLUST. VQ A &amp; 2.00 - .- v, C e, E .- 'D &gt; Y ._ n v 1.00 - LT MSE (dB)</formula><p>Alphabet-constrained rate-distortion functions for the memory-Fig. <ref type="figure" target="#fig_4">2</ref>.</p><p>less Laplacian source ( L = 2).</p><p>{ b , . . . , b,). In principle, this can be determined using the  If the prequantization is sufficiently "fine" for a range of encoding rates of interest, then E{llX -V1I2/L) is negligible compared to D,(R), and D,(R) is an indication of whether or not the reproduction alphabet { b , ; . ., b,) is sufficient for achieving near-optimum encoding performance. For a given encoding rate, R , and performance indicator E &gt; 0, we generally seek (for sufficiently fine prequantization) a reproduction alphabet with the small-</p><formula xml:id="formula_14">est J such that D,(R)-D,(R) &lt; E .</formula><p>Examples of the alphabet-constrained rate-distortion functions are shown in Figs. <ref type="figure" target="#fig_4">2</ref><ref type="figure">3</ref><ref type="figure">4</ref><ref type="figure">5</ref>. In each case the dimension is L = 2, and the prequantization is with an appropriate generalized Lloyd algorithm VQ [SI, and is sufficiently fine so that the prequantization distortion is negligible for the distortion range of interest. The reproduction codebooks were also formed by using the generalized Lloyd algorithm. Figs. <ref type="figure" target="#fig_4">2</ref> and<ref type="figure">3</ref> are for the memoryless Laplacian and Gaussian sources, respectively, and show that by doubling the number of codewords in two dimen--20.00 -15.00</p><p>-10.00</p><p>-5.00 0.00</p><formula xml:id="formula_15">MSE (dB)</formula><p>Alphabet-constrained rate-distortion functions for the memory-Fig. <ref type="figure">3</ref>.</p><p>less Gaussian source ( L = 2).</p><p>sions (i.e., R = 0.5) significant reduction in mse is possible at any encoding rate. Using the notation introduced to describe TCVQ Structure 1, an encoder that operates at a rate of R bits/&lt;imension has a super codebook of size 2(R+R)L, where R is the (per dimension) codebook expansion factor. From Fig. <ref type="figure">3</ref>, it is fair to conclude that R 2 1 bit/dimension is adequate to near-optimally encode the memoryless Gaussian source at low bits rates, although at high encoding rates a slightly la_rger value of R may be required. From Fig. <ref type="figure" target="#fig_4">2</ref>, a value R 2 1.5 bits/dimension is required for near-optimum encoding of the Laplacian source. Fig. <ref type="figure">4</ref> shows the alphabet-constrained rate-distortion curves for a first-order Gauss-Markov source (with correlation coefficient p = 0.9) and L = 2, based on the second-order distribution function for [ x(n), x(n -1)IT. The alphabet-constrained rate-distortion curves are lower bounded by the second-order rate-distortion function. This, in turn, is far from the rate-distortion function for the source <ref type="bibr" target="#b22">[29]</ref>, demonstrating that there is much to be gained by effectively using the memory in the (vector) source.</p><p>Fig. <ref type="figure">5</ref> shows the alphabet-constrained rate-distortion functions for the ideal prediction error of the first-order Gauss-Markov source. That is, the Gauss-Markov source is defined as</p><formula xml:id="formula_16">x ( n ) = px( n -1) + w( n),</formula><p>so that where w ( n ) is a memoryless, zero-mean, unit-variance Fig. <ref type="figure">4</ref>. Alphabet-constrained rate-distortion functions for the 1st order Gauss-Markov source with correlation coefficient p = 0.9 ( L = 2).</p><p>3 00</p><p>adding in the prediction gain), at high encoding rates the unconstrained R ( D ) curve in Fig. <ref type="figure">5</ref> coincides with the unconstrained R ( D ) curve in Fig. <ref type="figure">4</ref>. The alphabet-constrained rate-distortion curves are similarly related, leading to the conclusion that it is possible to achieve near optimum (in a rate-distortion sense) performance with the</p><formula xml:id="formula_17">* CLUST VQ A g 2 00 E K v D 2 1 00 E -</formula><p>constrained alphabet in the predictive encoding structure. IV. CODER DESIGN FOR MEMORYLESS VECTOR SOURCES Gaussian sequence. The ideal prediction error is then Also shown in Fig. <ref type="figure">5</ref> is the rate-distortion function for the ideal prediction error. Fig. <ref type="figure">5</ref> illustrates that, for the ideal prediction error sequence, a super codebook size of about 2(R+1)L is sufficient for achieving near optimum (in a rate-distortion sense) encoding performance. Fig. <ref type="figure">5</ref> can be compared to Fig. <ref type="figure">4</ref> at high encoding rates by noting that nearly all of the prediction gain <ref type="bibr" target="#b12">[19]</ref> will be realized by a predictive coder. With this adjustment (i.e.,</p><p>The design of a TCVQ coder consists of several (interrelated) steps. These steps include selection of a trellis, selection of a codebook, partitioning of the codebook into subsets, the assignment of subsets to the trellis branches, and the optimization of the codebook for the choice of trellis, partition, and branch labeling. We have not done an exhaustive search for good trellises in TCVQ. However, it was determined in trellis coded scalar quantization [6] that the trellises and branch labeling introduced by Ungerboeck for trellis coded modulation <ref type="bibr">[l]</ref>, <ref type="bibr" target="#b1">[2]</ref> (but, with a feedback-free encoding circuit) were a good choice. Hence, we expect that for memoryless (vector) sources there is little advantage to using other trellises. For sources with memory we do not know if other trellises are superior.</p><p>Consistent with the analysis in the previous section, we assume that the vector source is memoryless, although there can be dependence between the components of a single vector. The choice of a particular encoding structure forces codebook optimization to be constrained (to the assumed structure). For example, if the Structure 3 were chosen with the codebook C to consist of points in a lattice, then naturally the design algorithm must be constrained to maintain the lattice codewords. We consider separately the design of Structure 1 and Structure 3 TCVQ coders, and establish a useful "conditional mean" property of the trellis encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure 1 Design</head><p>We assume the TCVQ Structure 1 and a given initial codebook, 4, and adopt the following ad hoc general rules to guide the subset construction and trellis branch labeling.</p><p>Rule 1) The subsets should be constructed along the lines of Ungerboeck's method of set partitioning <ref type="bibr">[l]</ref>, so that with each level of partitioning the Euclidian distance between the points in a subset is increasing (nondecreasing).</p><p>Rule 2) The union of subsets corresponding to branches emanating from or entering a trellis state should be a reasonable quantizer for the source. Rule 3) The subsets should appear as trellis branch labels with a relative frequency approximately equal to their probability. Rule 4) The symbols in a subset should be (roughly) equally likely.</p><p>Rule 1 is important for low-complexity encoders. Since trellis coding is a technique for soft-decision quantization, following Rule 1 allows a "good hard decision'' encoding within each subset. This greatly reduces the trellis size and number of branches per trellis state that must be used to achieve most of the possible reduction in distortion. The trellis search then chooses between the contending subsets. Rule 2 is justified by noting that a natural design goal is to have each (long enough) sequence of trellis states equally likely (for otherwise the entropy is smaller than the encoding rate). Since the (vector) source is assumed memoryless, the next input (vector) is independent of any given trellis state. Hence, for each trellis state, the union of the subsets labeling the branches leaving the state should define a "reasonable" codebook to quantize the input. The term reasonable is somewhat vague, but implies a codebook that spans the dynamic range of the input vector. In a formal sense, if a unique subset could be assigned to each trellis branch, one would like the union of subsets leaving each trellis state to form a codebook that is optimum for the conditional (on the state) density of the input, subject to the delayed decision  The design Rules 1-3 can be readily incorporated into a TCVQ design procedure. Most of our numerical experience with TCVQ has focused on using Rules 1-3 to define the structure, and then optimizing the codebook.</p><p>Based on estimates of the codeword probabilities, the design can often be improved by using Rule 4, in conjunction with Rules 1-3, to repartition the codebook and relabel the trellis. The value of the design rules is illustrated by the following simple example.</p><p>Example: Consider the trellis-coded scalar quantization .</p><p>(L = 1) of a memoryless uniform source using Structure 1, with R = 2 , R = l , and M = l . The Lloyd-Max rate-3 uniform scalar quantizer output levels are selected as the initial codebook 4. The codebook is partitioned into 4 subsets, in three distinct ways, for use with the four-state trellis, as shown in Fig. <ref type="figure">6</ref>. The partition labeled "TCQ" in Fig. <ref type="figure">6</ref> follows the design rules (and was used in [61). The partition labeled "Case 1" follows Rule 1 for the first level of partitioning only, while "Case 2" corresponds to a partitioning exactly opposite to that indicated by Rule 1. Each case follows Rules 2-4 to the extent possible for the respective subsets. Monte Carlo simulations of the trelIis coders' performance for the trellis in Fig. <ref type="figure">6</ref> and each respective codebook partition are summarized in Table <ref type="table" target="#tab_0">I</ref>. The more closely design Rule 1 is followed, the better the perfor-mance. The Case 2 results are particularly poor, and are much worse than scalar quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Also shown in Table I are the performance results</head><p>obtained if the codebook is optimized (using the method in [6]), but subject to the initial partitioning. The Case 1 and 2 optimized codebooks yield equivalent coders, and demonstrate that local minima exist in the optimization. That is, neither Case 1 or 2 initial partitions result, upon codebook optimization, in the "TCQ" design. An optimization algorithm that yields a locally optimum codebook (such as [6], [81, <ref type="bibr">[201,</ref> or the algorithms to be discussed later) can improve the trellis coder's performance, but generally cannot make up for a poor choice of partitioning or trellis branch labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Structure 1 Optimization Algorithm</head><p>Once an initial reproduction alphabet, subsets, trellis, and branch labeling have been selected, it is desirable to optimize the reproduction alphabet, subject to the trellis structure. A numerical algorithm based on a training set of source samples can be constructed. Given a TCVQ system (trellis, reproduction symbols, partition, and branch labeling) and a fixed training sequence to encode, the average distortion incurred by the encoding can be thought of as a function of the output alphabet. For the reproduction alphabet, 9, of 1 1 9 1 1 codewords, the encoding distortion is a function of the 11/11 symbols in the reproduction alphabet, and so maps %LLi l ~' i l to 3, where 3 is one-dimensional Euclidean space. In this framework, optimization of 9 can be carried out by any numerical algorithm that solves for a vector in !XL11911 that minimizes a scalar function of llSll vector variables.</p><p>In [6] a quasi-Newton method was used to optimize the one-dimensional reproduction alphabet. The computational complexity of the quasi-Newton method grows rapidly with the size of the alphabet, so the technique is not efficient for large, especially multidimensional, alphabets. Alternatively, the generalized Lloyd algorithm can be used to (locally) optimize the reproduction alphabet, as discussed by Stewart, Gray, and Linde [201. We briefly review this approach in the TCVQ context, and then establish a useful "conditional mean" property of the optimized TCVQ encoder.</p><p>Consider the Viterbi encoding of a training sequence.</p><p>The optimum path through the trellis (for an initial 9) is determined, and for each reproduction symbol y, that occurs in the encoded sequence, define the set B, as the set of all training vectors that were encoded as y l , i.e., B, = { x: x is encoded as yi} .</p><p>This procedure partitions the training sequence into cells, Bi, but unlike the generalized Lloyd algorithm for vector quantizer design [8], it is not required that each x E Bi is necessarily best represented, on an individual vector distortion basis, by the symbol yi. Instead the partition is with respect to minimum distortion encoding of the source sequence subject to the TCVQ structure (that is, for the minimum distortion path through the trellis). After all training vectors have been assigned to the appropriate partition cell, the reproduction symbols are updated in the usual way by calculating the minimum distortion output symbol for each cell. For the mse distortion measure, this implies that the updated reproduction symbol for cell Bi is given by the centroid, where 11B, 11 denotes the number of training vectors in cell Now, since the TCVQ structure (trellis and branch labeling) is unchanged, the path through the trellis that led to the partitioning is still a valid one for the encoding of the training sequence. Further, since the reproduction symbols for each partition cell were optimized by (131, encoding the training set with the previous path through the trellis, but with the updated reproduction symbols, must provide a smaller (at least, not larger) mse. The best encoded sequence based on the updated reproduction alphabet can then be found by again encoding the training sequence using the Viterbi algorithm. The optimization algorithm is summarized as follows.</p><p>Algorithm for Structure 1 Optimization: Given a training sequence, 3, a trellis and branch labeling, an initial reproduction alphabet, Y1, with elements y:, i = 1,2; . ., 1 1 9 1 1 1 , partitioned into subsets, and a m e distortion measure d ( x , y), select E &gt; 0 as a suitable convergence threshold, set p(0) = CO, j = 1, and at iteration j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B, .</head><p>Encode the training sequence using the Viterbi algorithm and the TCVQ structure with alphabet 9'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denote the resulting distortion as</head><p>If (p( j -1) -p( j ) ) / p ( j ) &lt; E , then stop and the final codebook is 9'. Otherwise continue.</p><p>For the encoded sequence, y ( n ) , found in 1), partition the training sequence into cells B:, so that x ( n ) E B:, if and only if x ( n ) was encoded as y,'.</p><p>Update the reproduction alphabet as 9'+ ' according to Set j to j + 1 and return to Step 1).</p><p>Since the distortion computed in Step 1 of the algorithm is a nonincreasing function of the iteration (and the distortion is bounded from below by zero), the algorithm eventually converges to a locally optimum reproduction alphabet for the TCVQ structure. The initial reproduction alphabet can be formed in a variety of ways, perhaps the easiest of which is to use the generalized Lloyd algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Properties of the TCVQ Coder</head><p>The TCVQ systems described have time-invariant decoders and fixed codebooks, but the encoding can be done in several ways. If the TCVQ encoding results in joint stationarity and ergodicity of the (vector) source and reproduction sequences, then several properties of the coder can be derived. In particular, if the source and reproduction processes are jointly stationary and ergodic, then we conclude from (13) that for a long enough training sequence, for each i = 1,2; . ., IlSll,</p><formula xml:id="formula_18">1 yi = -c x 5 E [ xIx encoded as yi] . (14) IlBill x E B,</formula><p>Define the encoding noise sequence as q ( n ) = x ( n ) -y(n). Equation (14) further implies that the encoding noise is zero-mean, that E[qTyl=O, and that the mse is the difference between the source variance and the reproduction sequence variance. We summarize these properties in the following theorem.</p><p>Theorem: Let x ( n ) be a stationary and ergodic vector source, and let the TCVQ Structure 1 encoder have a codebook designed using the Structure 1 optimization algorithm. Denote the reproduction sequence as y ( n ) and the encoding noise as q ( n ) = x ( n ) -y(n). If the vector source and reproduction processes are jointly stationary and ergodic, and the training (design) sequence is long enough so that (14) holds, then</p><formula xml:id="formula_19">1 L mse = -E{ llql12) = var ( x ) -var ( y ) ,<label>( 16)</label></formula><p>and</p><formula xml:id="formula_20">E [ q T Y ] = 0, where 1 L var(x) = -c var(x,), L 1 -1 1 L v a r ( y ) --</formula><p>var(y,), and llql12=qTq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>r = l</head><p>The proof of the theorem is similar to known derivations for vector quantization, but replaces ensemble averages with time averages. The proof and some comments are given in the Appendix. We note that it is a generally difficult and open problem to establish that the input and reproduction processes are jointly ergodic or stationary for a trellis encoding. Some of the technical difficulties include the possible ergodic modes introduced by artificially blocking a scalar process into L-tuples [30], and the different symbol release strategies that an encoder can use (such as block trellis coding or incremental trellis coding [21], or the periodic block symbol release discussed later in this paper). If the vector source is stationary and ergodic, and if an incremental (i.e., one vector symbol at a time) time-invariant symbol release strategy is used, then the sliding block code is time-invariant and the source and reproduction processes will be jointly stationary and ergodic. (This follows directly from the results in [31] and [32].) We also note that, with the same caveats, the theorem holds for trellis encoders designed using the Stewart, Gray, and Linde algorithm [20].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Structure 3 Design</head><p>Trellis-coded vector quantization Structure 3 is diagrammed in Fig. <ref type="figure">l</ref>, and is a two-step encoding process.</p><p>For each source vector, x, the codebook 4 is searched once to yield a codeword y with scaling parameter a ( y ) .</p><p>The scaled difference vector e = (xy ) / a ( y ) is then TCVQ encoded to yield e^. The reproduction vector is f = y + a(y)e^. The principal advantage of Structure 3 is its simplicity; the codebook 9 can be selected to have relatively few codewords, and the TCVQ encoding of e can use low-dimensional reproduction symbols. It is desirable to optimize the codebook, 4, the parameters a ( y ) for y E 9, and the trellis codebook 8, and this may be accomplished as follows.</p><p>Let X be a training sequence of source vectors. Assume that codebook 4 is specified, that a, = a ( y , ) is known for each y l E 4, i = 1,. . ., 1 1 4 1 1 , and let G = {a,). For each x, E X , let e, <ref type="figure">= (x,</ref><ref type="figure">&gt;&gt;/a(y(x,</ref>)), where y(x) is the minimum mse reproduction vector in 4. The partition of X induced by y(x) has cells B, = {x E 9 ? y(x) = y , } . Finally, denote the set 8 ={e,}. Now, for fixed codebooks 4 and C, the set 8 can be regarded as a training sequence for design of the TCVQ encoder with codebook 8. Following the procedure in the previous section, the generalized Lloyd algorithm (or another technique, as in [6]) can be used to (locally) optimize 8. Assume the codebook d is so optimized, and next consider the (subsequent) optimization of 4 and 2.</p><p>We note that with 8, 4 and G fixed, the sequence of TCVQ output vectors, e^, m,ay be thought of as a function of the training set X , say &amp;'(T). That is, the nth TCVQ reproduction vector corresponds to the nth source vector, and we denote this as e^(n) = e^(x(n)). Further, regardless of which y and a ( y ) are used with each x to produce e, the sequence e^(n) is a valid path through the TCVQ trellis. Hence, with 8 fixed, the sequence of two-tuples, ( x ( n &gt; , e^(n&gt;), n = 1,. . ., IIXII, can be thought of as a training sequence for the optimization of 4 and 2. The overall mse for encoding the training sequence is then Since a ( y i ) = a,, the mse becomes The necessary conditions for selecting optimum yj, ai are found by setting dmse/dyj = 0 and dmse/dq = 0, and are Substituting <ref type="bibr" target="#b12">(19)</ref> into <ref type="bibr" target="#b13">(20)</ref>, and rewriting <ref type="bibr" target="#b12">(19)</ref>  Bi, defined previously. The resulting codewords, yi, are generally no longer the centroids of the respective partition cells, so it is desirable to iteratively optimize 9, C, and Bi. We can redefine the partition cells to minimize (18) (for fixed 9 and 2 ) as Bi = ( x E x: 1 1 xy;a;;( x) 112 &lt; 1 1 xy, -a;-;( x) 112 ~j z i ) . (25)</p><p>Then, iterating between (25) and ( <ref type="formula">21</ref>)-( <ref type="formula">24</ref>) yields a nonincreasing sequence of distortions in <ref type="bibr" target="#b11">(18)</ref>, so that a locally optimum codebook and partition is eventually reached. Unfortunately, the encoder defined by this proc:dure is not practical (and, in fact, is noncausal, because &amp;(Z) is used to determine the partition cells Bi for the codebook 4.</p><p>To remedy this situation we adopt the realization constraint that Bi be a nearest neighbor partition cell, i.e., Bi = (x E 9:</p><formula xml:id="formula_21">IIx -yj1I2 &lt; IIx -yj1I2 V j f i ) . (26)</formula><p>Although this choice of partition cell does not necessarily lead to smaller distortion, the codebooks 9 , C , and the partition cells, B;, can often be improved by iteratively using, respectively, ( <ref type="formula">21</ref>)-( <ref type="formula">24</ref>) and (26) until the resulting distortion in (18) no longer decreases. This process is summarized by the following algorithm.</p><p>Algorithm for Structure 3 Optimization: Let k = 1,2, . . . denote the step in the iteration, let 9 ' and Bo be initial codebooks for y and e^, respectively, let CO be a set of positive scalars, let d o =a, let E &gt; 0 be a convergence threshold, and set k = 1. 1) Encode X using Y k -' , C k -' , and generate g k .</p><p>2) Iteratively optimize the codebook Bk based on the training set gk using the {istortion measure <ref type="bibr" target="#b11">(18)</ref>.</p><p>Call the encoded :equence g k . <ref type="bibr" target="#b2">3</ref>) Use Z, Bk, and E k to iteratively optimize J k , Z k , using ( <ref type="formula">21</ref>)-( <ref type="formula">24</ref>), and Bi using (26). Our numerical experience with this design algorithm has been that the noncausal encoder yielded (for the training sequence) mse only marginally smaller (a few tenths of a dB) than the causal encoder using the partition in (26). The simulation results in the next section summarize the performance of a few Structure 3 designs, and several Structure 1 designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Memoryless Laplacian Source</head><p>Consider the design of a TCVQ Structure 1 encoder for a zero-mean, unit-variance, memoryless Laplacian soyce and the parameter values L = 2, M = 1, and R = R = 1 bit/dimension. A 2(R+R)L = 16 codeword VQ was designed using the generalized Lloyd algorithm [81. Using the design Rules 1 and 4, the resulting codewords were partitioned into 2RL+M = 8 subsets, as shown in Fig. <ref type="figure" target="#fig_7">7</ref>, for use with 8-and 16-state trellises <ref type="bibr">[1]</ref>. The labeling of the trellis branches for the 8-state trellis is also shown in Fig. <ref type="figure" target="#fig_7">7</ref>, and follows the design Rules 2 and 3. Application of the Structure 1 optimization algorithm (using a training sequence of 10 000 vectors) yielded the codewords shown in Fig. <ref type="figure" target="#fig_7">7</ref>. A similar procedure was followed to design the subset codewords for a 16-state trellis. In each design the final codebooks were constrained to have 180" symmetry (with the consequence of a significant reduction in the encoding complexity, compared to the unconstrained case).</p><p>The performance of the 8-and 16-state TCVQ designs is summarized in Table <ref type="table" target="#tab_1">I1</ref> (based on test sequences of 100000 vectors, encoded in blocks of 1000), along with the performance of VQ <ref type="bibr" target="#b15">[22]</ref>, PVQ <ref type="bibr" target="#b16">[23]</ref>, TCQ [6], and entropy coded scalar quantization <ref type="bibr" target="#b17">[24]</ref>. An improvement in SNR of nearly 0.7 dB is realized by using two-dimensional codewords, compared to the scalar codewords in the TCQ system. However, entropy coded scalar quantization is still a more effective encoding scheme for this source.</p><p>Structure 1 TCVQ encoders were also designed for encoding rates of 1 and 2 bits per dimension, an %state trellis, and vector dimensions of L = 1, 2, and 4, under the constraint that the codewords be points in the c_ubic lattice. The codebook expansion parameter was R = 1 sa S, bit/dimension for L = 1 and 2, and R = 0.5 for L = 4. For L = 1, the codewords were a scaled version of the odd integers, with the scale parameter adjusted in the design process to minimize the distortion. For L = 2, the codewords were lattice points on concentric pyramids <ref type="bibr" target="#b16">[23]</ref>. For L = 4, the codebook was selected in two steps. First, an  initial super codebook was designed using the generalized Lloyd algorithm. Then, using a suitabIe scale factor (0.36 for R = 1, 0.25 for R = 2), the nearest point in the (scaled) lattice was selected as a codeword in the constrained codebook. The codebook was partitioned into subsets according to design Rule 1, and the subsets assigned as the trellis branch labels as in Fig. <ref type="figure" target="#fig_7">7</ref>. With the lattice-constrained codewords and the trellis labeling fixed, the scale parameter was then iteratively adjusted to minimize the encoding distortion.</p><formula xml:id="formula_22">R = l R = 2 VQ ( L = 1) [22] VQ ( L = 2) VQ ( L = 3 ) VQ ( L = 4) VQ ( L = 5) VQ ( L = 6) PVQ ( L = 32)_[23] TCQ ( L = 1, R = 1, N = 8) [5] TCQ ( L = 1, R = 1, N = 16) [5] TCQ ( L = I, R = 2, N = 16) [5] T C V Q ( L = 2 , R = l , N = 8 ) TCVQ ( L = 2, R = 1, N = 16) TCVQ (lattice-constrained codebook) ( L = l , R = l , N = 8 ) ( L = 4, R = 0.</formula><p>The performance of the lattice constrained TCVQ encoders is summarized in Table <ref type="table" target="#tab_1">I1</ref> (for test sequences, different from the design sequences, of 100 000 vectors). The improvement due to increasing the vector dimension is evident. Compared to the performance of the TCVQ coder with unconstrained codebook, it is evident that the lattice constraint causes a significant drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gauss -Markov Sources</head><p>Consider the P th-order Gauss-Markov source given by</p><formula xml:id="formula_23">P x ( n &gt; = C a , x ( n -i &gt; + w ( n &gt; , i = l</formula><p>where w ( n ) is a zero-mean, white Gaussian sequence of variance a:. Initially we are interested in the first-order source, with P = 1 and a, = p = 0.9. The generalized Lloyd algorithm VQ encoding results for this source are reported in <ref type="bibr">[251, [261.</ref> Following the format of TCVQ Structure 1, let R = 1, R = 1, L = 2, and M = 1: We choose as the initial TCVQ codebook, 9, the 2(R+R)L = 16 codewords shown in Fig. <ref type="figure">8</ref>, and designed by using the generalized Lloyd algorithm. The codewords are partitioned according to Rule 1 into the TCVQ performance to Fig. <ref type="figure">4</ref>, it is evident that most of the performance promised by the alphabet-constrained rate-distortion theory (but, based on the second-order distribution) is achieved.</p><p>A modified partitioning and branch labeling of the initial codebook 9 is shown in Fig. <ref type="figure">9</ref>. This partition follows Rule 1 to only one level, but takes advantage of the correlation between source vectors and the relative frequency of the codewords (Rule 31, and is similar to the histogram method in FSVQ <ref type="bibr" target="#b5">[12]</ref>. Upon optimization of the codebook, this TCVQ design yielded the improved performance shown in Table <ref type="table">111</ref>.</p><p>The modified TCVQ performance is significantly better than equivalent complexity full-search VQ, but not quite as good as the FSVQ [9], <ref type="bibr">[lo]</ref>, which uses a larger super codebook. Unlike FSVQ, as long as a feedback-free implementation of the trellis encoder is used, the effect of a single transmission error can last for at most four TCVQ receiver output vectors.</p><p>The performance of several TCVQ Structure 3 designs is also listed in Table <ref type="table">111</ref>. This performance is slightly inferior to the Structure 1 coders (for identical rate and vector dimension), but the Structure 3 coders have a complexity advantage.</p><p>Predictive trellis waveform coding <ref type="bibr" target="#b9">[16]</ref> and predictive TCQ [6] provide better performance than FSVQ. Predictive TCVQ coders were designed for first-and secondorder Gauss-Markov sources, subject to-the constraint of cubic lattice codewords. For L = 1 ( R = 11, the super codebooks consistep of scaled version: of the odd integers. For L = 2 ( R = 1) and L = 4 (R=0.5), the super codebooks were selected in a two-step process. First, the generalized Lloyd algorithm was used to design an unconstrained codebook of size 2(R+R)L, based on a training sequence of ideal prediction error vectors. For example, with L = 4 and the first-order Gauss-Markov source, the ideal prediction error vector is given by where w is a vector of zero-mean, unit-variance, independent Gaussian random variables. Second, an appropriate scale parameter for the lattice (between 0.1 and 1.0) was selected, and the lattice-constrained codebook was formed as the closest 2(R+R)L lattice points to the unconstrained codewords. The codewords were partitioned into four subsets for L = 1, and eight subsets for L = 2,4. In the latter cases the trellis wa_s labeled as in Fig. <ref type="figure">8</ref>. The codebooks for L = 2, R = R = 1 are shown in Fig. <ref type="figure" target="#fig_8">10</ref>.</p><p>The predictive TCVQ performance with the latticeconstrained codebooks was evaluated by encoding test sequences of 100 000 vectors from, respectively, a first-order Gauss-Markov process (a, = p = 0.91, and a second-order Gauss-Markov process with McDonald's coefficients for speech <ref type="bibr" target="#b20">[27]</ref> (a, = 1.515, a2 = -0.752). This performance is listed in Table <ref type="table" target="#tab_0">IV</ref>. Even though the codewords are constrained, (particularly with L = 4) the predictive TCVQ coder provides significant improvement over predictive TCQ <ref type="bibr">[6]</ref> with the same size trellis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Encoding Delay</head><p>Consider the Structure 1 TCVQ encoding of the firstorder Gauss-Markov source ( p = 0.9) as in Fig. <ref type="figure">9</ref>, with R = 1, L = 2, the 8-state trellis, and the modified partitioning and branch labeling that takes advantage of the correlation between codewords. Practical applications often require trellis symbol release rules that avoid excessive encoding delay. The performance of one such symbol release rule is now examined.</p><p>Let K , 2 0 and K , 2 1 denote, respectively, the depth of the trellis search at which a hard decision is made, and the number of branch symbols released. The symbol release algorithm is the following: At time n = K , + K,, i = 1,2, * * . , the minimum survivor metric is determined, and the corresponding survivor path is traced back K , + K , branches, and K , branch symbols are released (corresponding to times n -K , -K,, . . . , n -K , -1). Denote the survivor state at time n -K , as s*(n -K,) for this minimum survivor path at time n. Next, each survivor at time n is traced back to time n -K,. If the resulting state is s*(n -K,), then no change is made. If the resulting state is not s*(n -K,), then that survivor metric at time n is set to 03. This effectively purges all survivors that are not consistent with the released symbols. Although the symbol release rule is suboptimum, it is (relatively) easy to implement.</p><p>Fig. <ref type="figure" target="#fig_0">11</ref> shows simulation results (TCVQ encoding of 5000 source vectors) of the effect of the symbol release rule parameters on the encoding signal-to-noise ratio (SNR). These results are consistent with the general "rule of thumb" for the decoding of (channel) convolutional codes that suggests a symbol release depth of about five times the number of memory elements (510g2 N , for an N-state trellis) in the convolutional encoder <ref type="bibr" target="#b21">[28]</ref>. For the case considered, as long as K , 2 10 and K , = K,, there is only a slight loss in performance. Since the survivor "path traceback" must only be performed every K , source symbols, one would generally like to select K , as large as permissible in each application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper has generalized TCQ to allow vector codebooks and branch labels. Codebook design algorithms were presented for TCVQ Structures 1 and 3. For Structure 1, the codebook optimization algorithm is essentially the same as the algorithm of Stewart, Gray, and Linde <ref type="bibr" target="#b13">[20]</ref>, although the TCVQ formulation makes the resulting optimum vector quantizers.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Encoder for TCVQ Structure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Blahut algorithm for discrete sources [181. However, the exponential growth with rate and dimension of the reproduction alphabet can make the computation of D,(R) a formidable task.The alphabet-constrained distortion-rate function is upper bounded as D,( R ) I E -1IX -U1I2 + Du( R ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This function is useful in the following way. The source has a distortion-rate function, say D*(R), as well as an Lth-order version, say D,(R), and clearly D*( R ) I DL( R ) I D,( R ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figs. 4 and 3 - 2 0</head><label>432</label><figDesc>Figs. 4 and 5 correspond, respectively, to the Cases 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 TCQ</head><label>2</label><figDesc>Fig. 6. Codebook, partition, and trellis of the example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>424) can be used to solve for the (locally) optimum codebooks 9, C, given the partition cells</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4)</head><label></label><figDesc>Encode X using Y k , C k , and Bk, and call the distortion d k . If ( d k -' -d k ) / d k &lt; E , then stop with final codebooks B= d k , 9 = Y k , 2 = C k . Otherwise, replace k by k + 1 and return to Step 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Initial codebook and partition for memoryless Laplacian source. (b) Trellis and labeling for Laplacian source. (c) Optimized codebook for Laplacian source.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Lattice codebooks for prediction error. (a) 1st-order Gauss-Markov source. (bf 2nd-order Gauss-Markov sourceTABLE IV ( U , = 1.515, u2 = -0.752) GAUSS-MARKOV SOURCES, AT ENCODING RATES OF 1 AND 2 BITS/DIMENSION A COMPARISON OF THE MSE ENCODING PERFORMANCE OF VARIOUS CODERS FOR THE 1ST-ORDER (p = 0.9) AND 2ND-ORDER</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">MEAN-SQUARE ERRORS FOR THE DIFFERENT PARTITIONS IN THE</cell></row><row><cell></cell><cell>EXAMPLE</cell><cell></cell></row><row><cell>~</cell><cell>~</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Distortion-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rate</cell></row><row><cell></cell><cell cols="3">Codebook TCQ Case 1 Case 2 Lloyd-Max Bound [191</cell></row><row><cell></cell><cell>Lloyd-Max 0.0565 0.0617 0.131 Optimized 0.0540 0.0561 0.0561</cell><cell>0.0625 -</cell><cell>0.0477</cell></row></table><note><p>trellis search. For a memoryless vector source, the density is independent of the trellis state, and the method of set partitioning yields subsets whose unions are of size 2RL, and are good (though not, generally, minimum mse) codebooks for the source. (After the subset construction and trellis branch labeling, the codewords are (locally) optimized, subject to the TCVQ structure.) Rule 3 is borrowed from the technique of stochastically populating a trellis with a finite reproduction alphabet, but restricted to the trellis states at a single time. Rule 4 is justified since violating it implies that the conditional entropy of the subset is less than the bit rate used to encode it.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I1 A</head><label>I1</label><figDesc>COMPARISON OF THE MSE ENCODING PERFORMANCE OF VARIOUS CODERS FOR THE MEMORYLESS LAPLACIAN SOURCE AT ENCODING RATES OF 1 AND 2 BITS/DIMENSION</figDesc><table><row><cell>SNR (dB)</cell></row><row><cell>Coder</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank Prof. R. Gray for several helpful comments and the reviewers for their comments and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported, in part, by the National Science Foundation under Grants MIP-8619888 and NCR-8821764. This work was presented in part at the Beijing International Workshop on Information Theory, Beijing, China, July 1988.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, S? 2 d L + M = 8 subsets, of two codewords each, as also shown in Fig. <ref type="figure">8</ref>, and the subsets are assigned as branch labels to an 8-state trellis. It takes 1 bit/vector to specify a sequence of branches through the trellis and 1 bit/vector to specify the codeword within the subset assigned to the branch, so the encoding rate is 2 bits/vector or 1 bit/dimension, as required.   The theorem may also be proved by an alternative argument. The convergence in <ref type="bibr" target="#b7">(14)</ref> implies that we can consider the TCVQ mapping as inducing a conditional probability assignment on the ,source, as P(y,lx), and likewise, the conditional density p(xl y;). It follows that and hence that E[ql = 0. The other properties ( <ref type="formula">16</ref>)- <ref type="bibr" target="#b10">(17)</ref> follow directly. A similar argument can be used to prove analogous properties for the generalized Lloyd algorithm VQ [8]; however, in that case the mapping from x to y is deterministic, and the conditional probability distribution of y given x is an indicator function.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Channel coding with multilevel/phase signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ungerboeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1982-01">Jan. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trellis-coded modulation with redundant signal sets, Parts I and 11</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5" to="21" />
			<date type="published" when="1987-02">Feb. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal encoding of discrete-time continuous-amplitude memoryless sources with finite output alphabets</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Finamore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="348" to="359" />
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An algorithm for the design of labeled-transition finite-state vector quantizers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">0</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="83" to="89" />
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE (Invited Paper)</title>
		<meeting>IEEE (Invited Paper)</meeting>
		<imprint>
			<date type="published" when="1973-03">Mar. 1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simulation of vector trellis encoding systems</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="214" to="218" />
			<date type="published" when="1986-03">Mar. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential encoding algorithms: A survey and cost analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="1984-02">Feb. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trellis-Coded quantization/ modulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Marcellin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="172" to="176" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vector predictive coding of speech at 16 kbits/s</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The design of predictive trellis waveform coders using the generalized Lloyd algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ayanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1073" to="1080" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast quantizing and decoding algorithms for lattice quantizers and codes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="227" to="232" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computation of channel capacity and rate-distortion functions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Blahut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1972-07">July 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<title level="m">Digital Coding of Waueforms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The design of trellis waveform coders</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Time-invariant trellis encoding of ergodic discretetime sources with a fidelity criterion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="71" to="83" />
			<date type="published" when="1977-01">Jan. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vector quantizer design for memoryless Gaussian, gamma, and Laplacian sources</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dicharry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1065" to="1069" />
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A pyramid vector quantizer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="568" to="583" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimum quantizer performance for a class of non-Gaussian memoryless sources</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Modestino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="485" to="497" />
			<date type="published" when="1984-05">May 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vector quantizers and predictive quantizers for Gauss-Markov sources</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="1982-02">Feb. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mag</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Signal-to-quantization noise ratio and idle channel performance of DPCM systems with particular application to voice signals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1123" to="1151" />
			<date type="published" when="1966-09">Sept. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Error Control Coding: Fundamentals and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Costello</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">Rate Distortion Theory</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
		<title level="m">Information Theory and Reliable Communication</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>ch. 9</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><surname>Probability</surname></persName>
		</author>
		<author>
			<persName><surname>Reading</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Addison-Wesley Publishing Co</publisher>
			<biblScope unit="page">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sliding-block source coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="357" to="368" />
			<date type="published" when="1975-07">July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
