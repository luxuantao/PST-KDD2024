<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge-Directed Single-Image Super-Resolution via Adaptive Gradient Magnitude Self-Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
							<email>lfwang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of National Laboratory of Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<email>smxiang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of National Laboratory of Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
							<email>gfmeng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of National Laboratory of Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaiyu</forename><surname>Wu</surname></persName>
							<email>hywu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of National Laboratory of Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of National Laboratory of Pattern Recognition</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Bei-jing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge-Directed Single-Image Super-Resolution via Adaptive Gradient Magnitude Self-Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AB1F713AA23377044175AA5351E8E47</idno>
					<idno type="DOI">10.1109/TCSVT.2013.2240915</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Edge-directed</term>
					<term>gradient magnitude transformation</term>
					<term>super-resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super-resolution from a single image plays an important role in many computer vision systems. However, it is still a challenging task, especially in preserving local edge structures. To construct high-resolution images while preserving the sharp edges, an effective edge-directed super-resolution method is presented in this paper. An adaptive self-interpolation algorithm is first proposed to estimate a sharp high-resolution gradient field directly from the input low-resolution image. The obtained highresolution gradient is then regarded as a gradient constraint or an edge-preserving constraint to reconstruct the high-resolution image. Extensive results have shown both qualitatively and quantitatively that the proposed method can produce convincing superresolution images containing complex and sharp features, as compared with the other state-of-the-art super-resolution algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>S UPER-RESOLUTION is a fundamental task for various computer vision applications, such as object detection and recognition, video compression, and communication. Generally, the goal of image super-resolution methods is to recover a high-resolution (HR) image from one or a sequence of lowresolution (LR) images <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b19">[20]</ref>. In many real-world applications, only one LR image is available. Hence, super-resolution from a single-input LR image is practically in great need. Unfortunately, the problem of single-image super-resolution is under-constrained, which makes classical methods, such as interpolation or reconstruction-based methods, often bring undesired artifacts in the HR image, especially along the salient edges. To overcome this limitation or to preserve local edge structures in the HR image, it is effective to introduce an additional edge constraint, which is the main issue that we will address in this paper.</p><p>The problem of single-image super-resolution has received much attention from both computer graphic and computer vision communities. Generally, approaches addressing this problem can be grouped into four categories <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b11">[12]</ref>, i.e., the interpolation-based, learning-based, reconstruction-based, and edge-directed methods.</p><p>The interpolation-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref> are widely used for producing zoom-in images because of its simplicity. Unfortunately, these solutions tend to produce visual artifacts, such as ringing, aliasing, blocking, and blurring, especially on salient edges. A survey of these methods and their shortcomings is given in <ref type="bibr" target="#b18">[19]</ref> in detail.</p><p>The learning-based or example-based super-resolution methods are first introduced in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and <ref type="bibr" target="#b22">[23]</ref>, and extended later by others, such as <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b23">[24]</ref>. In these methods, the correspondences between LR and HR image patches are first learned from a database of LR and HR image pairs, and then applied to a new LR image to recover its HR version. Improved performances compared with traditional super-resolution methods have been reported in these papers. However, the learning-based methods rely largely on the quality of the prior used, that is, the similarity between the training set and the test set. Furthermore, the computational cost of these methods is expensive due to a large training set being used.</p><p>The reconstruction-based methods <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> emphasize the reconstruction constraint, which requires that the downsampling version of the target HR image should be close to the LR image. However, the main problem is that the reconstructed edges are sometimes too sharp and look unnatural. Moreover, these methods often introduce undesired artifacts, such as ringing, in the HR image, especially along salient edges.</p><p>The sophisticated methods based on the edge models (edgedirected) have been proposed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr">[28]</ref>, and <ref type="bibr" target="#b28">[29]</ref>. These methods estimate the target HR image by enforcing some edge knowledge, such as the smooth edge <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr">[28]</ref> or the gradient profile prior (GPP) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The enforced edge knowledge is able to produce sharp edges with minimal jaggy or ringing artifacts. Unfortunately, these two edge models have their own drawbacks. First, the smooth edge knowledge is a smoothness constraint. Hence, some small scale information cannot be well recovered in the HR image. That is, some image details, especially on the weak edges, may be hard to recover in the HR image. Second, in <ref type="bibr" target="#b28">[29]</ref>, the super-resolution algorithm needs to locate edge pixels. Therefore, their results partially rely on the edge pixel location accuracies. The gradient profile prior is learned from an image dataset, so the image super-resolution result of <ref type="bibr" target="#b28">[29]</ref> Fig. <ref type="figure">1</ref>. Overview of our super-resolution algorithm. First, a HR gradient is estimated by adaptive gradient magnitude self-interpolation. Then, the HR image is recovered by the reconstruction based super-resolution framework (see Sections II and VI). In the dashed box, we present the details about HR gradient estimation. The gradient magnitude is first used to compute a displacement field (refer to Section V), which is, thereafter, adopted to sharpen the gradient magnitude by an interpolating method (refer to Section IV).</p><p>relies on the utilization of training dataset to some extent. Third, the two types of methods are under the edge-directed reconstruction framework. In this framework, an iteration method is utilized to solve the objective functional. Hence, a good initialization is necessary on both reducing the iteration times and recovering the HR image. However, these methods often adopt a bicubic up-sampled image, which is blurred in the edges as the initialization.</p><p>Motivated by the previous work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, we propose an edge-directed super-resolution approach based on a novel adaptive gradient magnitude self-interpolation. The mainflow of our algorithm is shown in Fig. <ref type="figure">1</ref>. We estimate a sharp HR gradient field by an adaptive self-interpolation algorithm. In our self-interpolation algorithm, the displacement field is calculated from the gradient magnitude of the bicubic upsampled image. It is worth noting that this displacement field is also used to interpolate the bicubic up-sampled image as an initialization. The estimated HR gradient is used as a gradient constraint or a edge-preserving constraint to recover the HR image. The main advantages or details of our algorithm are summarized as follows.</p><p>1) The proposed super-resolution method introduces an additional edge constraint to reduce the undesired artifacts brought by the traditional algorithm. Hence, compared with the classical interpolation and reconstruction-based methods, our algorithm can effectively get rid of some visual artifacts, such as ringing, aliasing, and blurring. 2) Different from the smoothness edge methods, such as the soft edge smoothness proposed in <ref type="bibr" target="#b10">[11]</ref>, our selfinterpolation sharpening method does not belong to the smoothness constraint. Hence, both small and large scales can be better recovered in our HR image, as compared with the smoothness edge methods. 3) Compared with the gradient magnitude sharpening method proposed in <ref type="bibr" target="#b28">[29]</ref>, our method has two advantages.</p><p>The gradient sharpening algorithm used in <ref type="bibr" target="#b28">[29]</ref> depends on the gradient profile prior learnt from an image dataset. However, our adaptive self-interpolation algorithm relies on the displacement field, which is estimated from the input LR image directly. That is, our method does not rely on the using of training datasets. Furthermore, in <ref type="bibr" target="#b28">[29]</ref>, it needs to locate the edge-pixels before sharpening the gradient, while our method interpolates the whole image domain directly to obtain the sharp gradient. Hence, our method is more general.</p><p>The computational cost of our method is lower. GPP edge-directed <ref type="bibr" target="#b28">[29]</ref> and our method both have two timeconsuming procedures, i.e., the gradient sharpening and the reconstruction. In the gradient sharpening, the experimental results show that our method is three times faster than <ref type="bibr" target="#b28">[29]</ref>. In the reconstruction, the displacement field estimated by our method can also be used to obtain a sharp image as initialization. Comparing the bicubic up-sampled image used in <ref type="bibr" target="#b28">[29]</ref>, our sharp initialization image is more close to the desired image. Hence, less iterations are needed in our method. In practice, 30 iterations are enough for our method, while 100 iterations are used in <ref type="bibr" target="#b28">[29]</ref>. A smaller iteration makes the computational cost lower. The remainder of this paper is organized as follows. A brief introduction of an edge-directed single-image super-resolution framework is presented in Section II. The main motivation on HR gradient field estimation is presented in Section III. The gradient sharpening method is presented in Section IV. The adaptive gradient magnitude self-interpolation algorithm, especially the displacement field estimation, is described in Section V. The HR image is reconstructed by optimization of an objective energy functional, which is presented in Section VI. Section VII gives the experimental results. Conclusions and future work are given in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Edge-Directed Single-Image Super-Resolution</head><p>Within the reconstruction framework, the goal of edgedirected image super-resolution is to estimate the HR image I h based on two inputs, i.e., the LR image I l and the HR gradient field ∇I h . The estimation of I h can be formulated as the minimization of the following energy functional:</p><formula xml:id="formula_0">I h = arg min I h E(I h |I l , ∇I h ) = arg min I h E d (I h |I l ) + αE g (∇I h | ∇I h )<label>(1)</label></formula><p>where E d (.) and E g (.) are the down-sampling energy and the gradient energy, respectively, and α is the weighting constant to balance these two energies. The gradient field ∇I is denoted as ∇I = (∂ x I, ∂ y I) = G • θ, where G = (∂ x I) 2 + (∂ y I) 2 is the gradient magnitude and θ = arctan (∂ y I/∂ x I) is the gradient direction. To obtain ∂ x I and ∂ y I, the image I is convolved by the discrete gradient operators [-1/2 0 1/2] and [-1/2 0 1/2] T , respectively. The down-sampling energy (or constraint) is represented by the difference between the LR image I l and the down-sampled version of the HR image</p><formula xml:id="formula_1">I h E d (I h |I l ) = [I h ⊗ g] ↓(β) -I l 2</formula><p>where ⊗ is the convolution operation, [.] ↓(β) is the downsampling operation with factor β, and g is the blurry kernel. The gradient constraint requires the gradient of the HR image ∇I h should be close to the input gradient</p><formula xml:id="formula_2">∇I h E d (∇I h | ∇I h ) = ∇I h -∇I h 2 .</formula><p>Combining with (1), the edge-directed image super-resolution estimates the HR image I h by minimizing the following objective energy functional:</p><formula xml:id="formula_3">I h = arg min I h [I h ⊗ g] ↓(β) -I l 2 + α ∇I h -∇I h 2 . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>From ( <ref type="formula" target="#formula_3">2</ref>), we see that the estimation of I h mainly relies on two aspects, namely, the blurry kernel g and the HR gradient field ∇I h . In this paper, we focus on the second aspect, i.e., the estimation of the HR gradient field ∇I h . The blurry kernel is assumed to be a 2-D Gaussian kernel, and its standard variance is related to the up-sampling factors, i.e., it is set to 1.2, 1.5, and 1.8 for the up-sampling factors of 2, 3, and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Motivation of Gradient Sharpening</head><p>The estimation accuracy of the HR gradient field has both theoretical and practical importance on the edge-directed single-image super-resolution problem. As shown in <ref type="bibr" target="#b1">(2)</ref>, without a gradient constraint, that is, only using the first term, the super-resolution problem is severely under-constrained. Adding a gradient constraint can make this problem easy to solve (determined or even overdetermined). It is worth noting that the reconstruction-based method, such as back-projection <ref type="bibr" target="#b24">[25]</ref>, only considers the down-sampling energy, ignoring the gradient constraint. Theoretically, the back-projection algorithm is convergent. That is, the output HR image obtained by back-projection converges to a desired image that satisfies that the reconstruction error (or the down-sampling energy) is close to zero. However, this algorithm often brings undesired artifacts in the HR image, especially along salient edges. Hence, it is important to introduce additional edge constraints to the traditional reconstruction-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Gradient Sharpening</head><p>In the actual super-resolution problem, we only have the input LR image, but do not have the true HR gradient field. Hence, a more wise and effective method is to estimate the (f) Gradient magnitude sharpened by GPP <ref type="bibr" target="#b20">[21]</ref>. The red curves in (f) and (g) are the original gradient magnitudes, while the blue curves are the sharpened gradient magnitudes. (g) Gradient magnitude sharpened by our method. The green arrows in (g) are the displacement field, while the pink curve is the gradient of the original gradient profile (red curve).</p><p>∇I h from the LR image, such as the gradient of the bicubic up-sampled LR image, i.e., ∇I u h , where</p><formula xml:id="formula_5">I u h = [I l ] ↑(β) in which [.] ↑(β)</formula><p>is the (bicubic) up-sampling operation and β is the down-sampling factor. In practical, the bicubic up-sampled image I u h will be blurred in the image edge, which makes the corresponding gradient magnitude blurry [refer to Fig. <ref type="figure" target="#fig_0">2(b),</ref><ref type="figure">(c)</ref>]. Hence, it is infeasible to directly use ∇I u h . Fortunately, we can adopt its transformation version to represent ∇I h . That is, ∇I h is calculated as follows:</p><formula xml:id="formula_6">∇I h = Tran(∇I u h )<label>(3)</label></formula><p>where Tran(.) is a transformation function. We denote the HR gradient field ∇I h as ∇I h = G h • θ h . Generally, to obtain the sharp gradient ∇I h , we only need to sharpen its magnitude G h , while the direction θ h can be estimated by the bicubic up-sampled version, i.e., θ h = θ u h , where θ u h is the gradient direction of ∇I u h . Hence, (3) is simplified as</p><formula xml:id="formula_7">G h = Tran(G u h )<label>( 4 )</label></formula><p>where G u h is the gradient magnitudes of ∇I u h . Accordingly, the finally sharpened gradient field ∇I h is obtained by</p><formula xml:id="formula_8">∇I h = G h • θ u h .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scaling Sharpening Overview</head><p>In <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b28">[29]</ref>, they define the transformation function as follows ( is the image domain):</p><formula xml:id="formula_9">G h (x) = Tran(G u h ) = r(x)G u h (x) x ∈<label>(6)</label></formula><p>where ratio r is learnt from natural images. Yan et al. <ref type="bibr" target="#b29">[30]</ref> improve the ratio calculation by using the Laplacian image. As interpreted in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and <ref type="bibr" target="#b29">[30]</ref>, the main purpose of multiplying ratio r is to sharpen the gradient magnitude, especially the gradient profile [see Fig. <ref type="figure" target="#fig_0">2(d</ref>) and (e)]. For example, in Fig. <ref type="figure" target="#fig_0">2</ref>(f), the ratio r along the gradient profile should satisfy</p><formula xml:id="formula_10">r(x Y ) ≥ 1 x Y ∈ [y 2 y 3 ] 0 ≤ r(x Y ) &lt; 1 x Y ∈ [y 1 y 2 ) ∪ (y 3 y 4 ]<label>(7)</label></formula><p>where x Y is the 1-D coordinate value along the y-axis. The ratio r proposed in <ref type="bibr" target="#b28">[29]</ref> used to zoom the gradient magnitude; thereby, we call this gradient sharpening method scaling sharpening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Self-Interpolation Sharpening</head><p>We propose a self-interpolation sharpening to sharpen the gradient magnitude. We use an interpolation operator instead of a zoom operator to make the gradient magnitude sharp. That is, the sharp gradient magnitude G h is obtained thought the interpolation function</p><formula xml:id="formula_11">G h = Tran(G u h ) = Intp(G u h , u)<label>(8)</label></formula><p>where u is the displacement field and Intp(.) is the interpolation function defined as follows:</p><formula xml:id="formula_12">G h (x) = Intp(G u h (x), u(x)) = G u h (x -u(x)) x ∈ . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>Fig. <ref type="figure" target="#fig_0">2</ref>(g) gives a 1-D interpolation example. As shown in this subfigure, the green arrows illustrate the displacement field u.</p><p>The interpolated gradient magnitude (see the blue curve) is much sharper than the original gradient magnitude (see the red curve), after performing interpolating operation.</p><p>On the other hand, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>(g), to make G u h sharp by the interpolation method, the displacement field u should satisfy that <ref type="bibr" target="#b9">(10)</ref> where y o is the local peak point of the gradient profile. Fortunately, the gradient of original gradient magnitude, i.e., ∇G u h , just satisfies <ref type="bibr" target="#b9">(10)</ref>, that is</p><formula xml:id="formula_14">⎧ ⎪ ⎨ ⎪ ⎩ u(x Y ) &gt; 0 x Y &lt; y o u(x Y ) = 0 x Y = y o u(x Y ) &lt; 0 x Y &gt; y o</formula><formula xml:id="formula_15">⎧ ⎪ ⎨ ⎪ ⎩ ∇G u h (x Y ) &gt; 0 x Y &lt; y o ∇G u h (x Y ) = 0 x Y = y o ∇G u h (x Y ) &lt; 0 x Y &gt; y o . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>The pink curve of Fig. <ref type="figure" target="#fig_0">2</ref>(g) shows ∇G u h . From this subfigure, we conclude that the sign of the displacement field u is the same as the sign of ∇G u h . Thus, we can efficiently estimate the displacement field u from ∇G u h , directly. Based on the above analysis, the estimation of the displacement field u only relies on the original gradient magnitude. Thereby, we call our gradient sharpening method selfinterpolation sharpening. More information about gradient magnitude self-interpolation will be presented in Section V in detail.</p><p>In the following, we use a 1-D case to justify that our selfinterpolation-based method can sharpen the gradient magnitude (the gradient profile is 1-D). Let f (x) be a continuous and monotonic function defined in (a, b); the sharpness of f (x) in (a, b) is denoted as</p><formula xml:id="formula_17">ω (a,b) (f (x)) = b a f (x)dx |f (b) -f (a)| . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>The sharpness ω (a,b) (f (x)) is interpreted as the mean width of f (x) [refer to the rectangle width of Fig. <ref type="figure" target="#fig_2">3</ref> 1) If f (x) and g(x) are both monotonically increasing, and</p><formula xml:id="formula_19">f (x) ≥ g(x), ∀x ∈ (a, b), then f (x) is smoother than g(x), i.e., ω (a,b) (f (x)) ≥ ω (a,b) (g(x)</formula><p>). 2) If f (x) and g(x) are both monotonically decreasing, and</p><formula xml:id="formula_20">f (x) ≥ g(x), ∀x ∈ (a, b) then f (x) is smoother than g(x), i.e., ω (a,b) (f (x)) ≥ ω (a,b) (g(x)</formula><p>). Definition 1 gives a criterion to compare the sharpness of two functions. The intuitive comparisons are illustrated in Fig. <ref type="figure" target="#fig_10">3(b</ref>) and (c) in detail.</p><p>Lemma 1: f (x) is a continuous and monotonically increasing function defined in (a, b). The derivative f (x) is a bounded and continuous function, satisfying</p><formula xml:id="formula_21">f (a) = f (b) = 0. Defining g(x) = f (x -ηf (x))</formula><p>, where η &gt; 0 and 1ηf (x) ≥ 0, we get that</p><formula xml:id="formula_22">ω (a,b) (f (x)) ≥ ω (a,b) (g(x)).</formula><p>For a monotonically decreasing function f (x), we can get a similarity result.</p><p>Proof: First, according to f (a) = f (b) = 0, we get that</p><formula xml:id="formula_23">g(a) = g(a + f (a)) = f (a), g(b) = g(b + f (b)) = f (b). f (x)</formula><p>is bounded and continuous; thus, g(x) = f (xηf (x)) is also a continuous function. Let t(x) = xηf (x). Thereby, t (x) = 1ηf (x) ≥ 0. Hence, ∀a ≤ x 1 ≤ x 2 ≤ b. We get that g(x 1 ) = f (t(x 1 )) ≤ f (t(x 2 )) = g(x 2 ). To sum up, g(x) is a continuous and monotonically increasing function.</p><p>Second, f (x) is a continuous and monotonically increasing function; thus, f (x) ≥ 0, which makes x ≥ x -ηf (x). Hence,</p><formula xml:id="formula_24">g(x) = f (x -ηf (x)) ≤ f (x), ∀x ∈ (a, b).</formula><p>Combining with Definition 1, we get</p><formula xml:id="formula_25">ω (a,b) (f (x)) ≥ ω (a,b) (g(x)).</formula><p>Comparing <ref type="bibr" target="#b8">(9)</ref> with Lemma 1, ηf (x) can be regarded as a displacement field in the interpolation function. It is worth noting that ηf (x) is obtained from its gradient. Hence, for a continuous and monotonically function, self-interpolation can sharpen the gradient theoretically. In practice, the gradient profile function can be assumed to be composed of some continuous and monotonically functions. For example, the gradient profile can be approximated with a Gaussian function. In such a case, the definition domain of f (x) can be divided into two parts, i.e., (-∞, 0) and (0, ∞). It is easy to check that in each definition domain, f (x) satisfies Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Adaptive Gradient Magnitude</head><p>Self-Interpolation As interpreted in Section III, the key problem of our selfinterpolation sharpening is the estimation of displacement field u. The input gradient magnitude G u h is 2-D. Therefore, the corresponding displacement field u should also have two directions, namely, u = (u x , u y ), where u x and u y are the xand y-direction displacement fields, respectively. Similarly to the definition used for the gradient field, the displacement field u is also denoted by the dot product of its magnitude M and direction φ, given by</p><formula xml:id="formula_26">u = (u x , u y ) = M • φ. (<label>13</label></formula><formula xml:id="formula_27">)</formula><p>In the following, we first present the estimation of direction φ and magnitude M, and then summarize the main steps of our gradient sharpening algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Direction φ Estimation</head><p>As mentioned in Section III, in the 1-D case, the sign of the displacement field just equals that of the original gradient magnitude [refer to <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref> for details]. It is easy to extent the 1-D case to the 2-D case by replacing the sign with the angle between two directions. Hence, the direction φ can be computed as follows:</p><formula xml:id="formula_28">φ = arctan ∂ y G u h ∂ x G u h . (<label>14</label></formula><formula xml:id="formula_29">)</formula><p>In practice, the direction calculated by ( <ref type="formula" target="#formula_28">14</ref>) is a little sensitive to the noise. Hence, the direction is further smoothed by Gaussian kernel g, namely, φ = φ ⊗g. During implementation, the standard variance of g is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimation of the Magnitude M</head><p>From Fig. <ref type="figure" target="#fig_0">2</ref>(g), in a gradient profile, the magnitude of displacement (length of green arrows) gradually becomes larger, as the distance between the current coordinate value and the peak point becomes larger, that is</p><formula xml:id="formula_30">|y i -y o | &gt; |y j -y o | ⇒ M(y i ) &gt; M(y j ) (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>where y o is the local peak point of original gradient magnitude.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>(g), the gradient magnitude of G u h satisfies that (see the pink curve)</p><formula xml:id="formula_32">⎧ ⎨ ⎩ |y i -y o | &lt; |y j -y o | ⇒ |∇G u h (y i )| &gt; |∇G u h (y j )|, y 1 ≤ y i , y j &lt; y 2 |y i -y o | &gt; |y j -y o | ⇒ |∇G u h (y i )| &gt; |∇G u h (y j )|, y 2 ≤ y i , y j ≤ y 3 |y i -y o | &lt; |y j -y o | ⇒ |∇G u h (y i )| &gt; |∇G u h (y j )|, y 3 &lt; y i , y j ≤ y 4 . (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>From ( <ref type="formula" target="#formula_30">15</ref>) and ( <ref type="formula" target="#formula_32">16</ref>), we see that |∇G u h | and M share the same condition when y 2 ≤ y i , y j ≤ y 3 . Furthermore, the error between M(y) and |∇G u h (y)| becomes larger as the coordinate y becomes closer to the two edge points y 1 or y 4 . Fortunately, around y 1 or y 4 , the change of gradient </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>magnitude of G u</head><p>h is very small. That is, the interpolation result obtained by ( <ref type="formula" target="#formula_12">9</ref>) almost does not change even the estimation of displacement field (either magnitude or direction), and has obvious errors around y 1 or y 4 . Hence, |∇G u h | can be adopted as the approximate estimation of the magnitude M. In the 2-D case, M is calculated as follows:</p><formula xml:id="formula_34">M = κ (∂ x G u h ) 2 + (∂ y G u h ) 2 (<label>17</label></formula><formula xml:id="formula_35">)</formula><p>where κ is a scaling factor. The choice of κ is mainly related to the up-sampling factors and the characters of the input LR image. The κ are experimentally set to ≈ 0.6, ≈ 0.8, and ≈ 1 for the up-sampling factors of 2, 3, and 4.</p><p>As shown in <ref type="bibr" target="#b16">(17)</ref>, the parameter κ governs the scaling value globally. In practical, this global method cannot deal with the multiple scales case. Fig. <ref type="figure" target="#fig_3">4</ref> gives an example of multiple scales gradient magnitude sharpening. As shown in this figure, the original gradient magnitude (red curve) around y 1 is sharper than that around y 2 . Hence, to sharpen the gradient, the displacement magnitudes around y 1 should be smaller than those around y 2 . However, the gradient of original gradient magnitude |∇G u h | (see the pink curve) just provides the opposite result, i.e., |∇G u h | around y 1 is larger than those around y 2 . To solve multiple scales case, we should adaptively calculate the scaling factor κ(x) according to the local scale of gradient profile</p><formula xml:id="formula_36">κ(x) = σ(x)κ x ∈ (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>where σ(x) is the local scale located x. The corresponding magnitude M is calculated as follows:</p><formula xml:id="formula_38">M(x) = κ(x) (∂ x G u h ) 2 + (∂ y G u h ) 2 x ∈ . (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>In this paper, the local scale σ is obtained by two substeps: 1) ridge points (all points on ridge lines) detection, and local scale estimation of these ridge points, and 2) local scale value filling.</p><p>In the first step, we perform the ridge lines detection and width estimation algorithm proposed in <ref type="bibr" target="#b30">[31]</ref> on the gradient magnitude to obtain the ridge points p(x) and their corresponding width w(x), given by {p(x), w(x)|x ∈ X } where X ⊂ is a point position set of all ridge points. The local scales of these ridge points are calculated by</p><formula xml:id="formula_40">σ(x) = max ξ l , min ξ h , w(x) med(w)</formula><p>x ∈ X (20)  where ξ l and ξ h are the low and high bounds, respectively, and med(w) gives the median width of all ridge points. During implementation, ξ l and ξ h are set ξ l = 0.5 and ξ h = 2.</p><p>In the second step, we fill the rest points by the nearest neighboring filling methods, namely</p><formula xml:id="formula_41">σ(y) = σ(x), y ∈ -X (<label>21</label></formula><formula xml:id="formula_42">)</formula><p>where p(x) is the nearest neighbor of p(y) in the set X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithm</head><p>To sum up, the main procedures of our adaptive gradient sharpening method are summarized in Algorithm 1. Fig. <ref type="figure">5</ref> gives an example of our super-resolution result. From this figure, the adaptively self-interpolated gradient magnitude [see Fig. <ref type="figure">5(f)</ref>] is significantly sharper than the original one [see Fig. <ref type="figure">5(e)</ref>], especially on the salient edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Optimization and Initial HR Image Setting</head><p>The objective energy functional of ( <ref type="formula" target="#formula_3">2</ref>) is minimized by the standard gradient descent by solving the gradient flow equation given by</p><formula xml:id="formula_43">∂I h ∂t = - ∂E(I h |I l , ∇I h ) ∂I h = - ∂ [I h ⊗ g] ↓(β) -I l 2 + α ∇I h -∇I h 2 ∂I h = -[[I h ⊗ g] ↓(β) -I l ] ↑(1/β) ⊗ g + α div(∇I h ) -div( ∇I h ) (<label>22</label></formula><formula xml:id="formula_44">)</formula><p>where div(∇•) = ∂ 2 ∂x 2 + ∂ 2 ∂y 2 is the Laplacian operator. In our implementation, we use the following iterative scheme to optimize <ref type="bibr" target="#b21">(22)</ref>, that is</p><formula xml:id="formula_45">I t+τ h = I t h -τ [[I h ⊗ g] ↓(β) -I l ] ↑(1/β) ⊗ g- α div(∇I h ) -div( ∇I h ) (<label>23</label></formula><formula xml:id="formula_46">)</formula><p>where τ is time step. During implementation, the time step τ is set to 0.1. The parameter α is used to trade off between the two constraints, i.e., an image domain constraint and a gradient domain constraint. A smaller α will produce better image color and contrast, yet with ringing or jaggy artifacts along edges.</p><p>On the contrary, a larger α will result in sharp edges with little artifacts. To balance the image color recovery and artifacts removal, the parameter α is experimentally set to 0.1.</p><p>As described above, an iteration method is used to solve <ref type="bibr" target="#b21">(22)</ref>; thereby, the super-resolution result partially depends on the initial HR image. Generally, the bicubic up-sampled image I u h is regarded as the initial HR image I init h , namely, I init h = I u h . However, the up-sampled image I u h is blurred in the image edges, which causes that the super-resolution algorithm needs many iterations to obtain the sharp edge, for example, 100 iterations reported in <ref type="bibr" target="#b28">[29]</ref>. Hence, the computational cost will be high. To avoid this, we use the interpolated version as the initialization given by</p><formula xml:id="formula_47">I init h = Intp(I u h , u)<label>(24)</label></formula><p>where Intp(.) is defined in <ref type="bibr" target="#b8">(9)</ref>. Fig. <ref type="figure">5</ref> gives an example. As shown in Fig. <ref type="figure">5</ref>(c), the initialization obtained by our method is much more sharper than the bicubic [see Fig. <ref type="figure">5(b)</ref>]. In our method, 30 iterations are enough to produce sharp and clear HR results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Experimental Results</head><p>Extensive experiments have been conducted to evaluate our method in comparison with several state-of-the-arts, which cover the four categories of single-image super-resolution, i.e., interpolation-based, reconstruction-based, learning-based, and edge-directed methods. Note that, for a color image, it is first transformed from RGB to YIQ. Then, the Y channel (intensity) is up-sampled by our algorithm. The I and Q chromatic channels, characterized by low-frequency information, are interpolated by the bicubic method. Finally, the three channels are combined to form the final super-resolution result. Both qualitative and quantitative methods are utilized to evaluate our method. For quantitative evaluation, we use RMS, ERMS <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and SSIM <ref type="bibr" target="#b31">[32]</ref> to measure super-resolution results. A good super-resolution result should provide small RMS and ERMS, and large SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of the Proposed Method</head><p>In this subsection, we evaluate the initialization and key parameter of our method. For initialization, we compare our method with the GPP edge-directed method <ref type="bibr" target="#b28">[29]</ref>.   1) Initialization Evaluation: Fig. <ref type="figure" target="#fig_5">6</ref> first presents a synthetical comparative result. As shown in this figure, our initialization image is sharper than that of GPP edge-directed. For further comparison, we test two methods on 300 images, which are obtained from the Berkeley website <ref type="bibr" target="#b32">[33]</ref>. The original image is used as the ground truth. The LR image  is obtained by down sampling its original version. The quantitative evaluations are shown in Table <ref type="table" target="#tab_0">I</ref>. From this table, our results are better than those of GPP edge-directed, especially when the up-sampling factor is higher. Fig. <ref type="figure" target="#fig_7">7</ref> presents two examples from the 300 images. From this figure, we see that our initializations are sharper.</p><p>2) Parameter α Evaluation: We test our algorithm with different α on 300 image obtained from Berkeley database. The ground truth and LR images are obtained in the same way as given in Section VII-A1. The quantitative results are shown in Table <ref type="table" target="#tab_1">II</ref>. The visual results are illustrated in Fig. <ref type="figure" target="#fig_8">8</ref>. From Table <ref type="table" target="#tab_1">II</ref> and Fig. <ref type="figure" target="#fig_8">8</ref>, we see that our results are very stable under different choices of α. Moreover, when α = 0.1, the ERMS and SSIM are the best. In other experiments, the parameter α is set 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons of Other Edge-Directed Methods</head><p>In this subsection, we compare our method with three edgedirected methods, i.e., the GPP edge-directed <ref type="bibr" target="#b28">[29]</ref>,<ref type="foot" target="#foot_0">1</ref> the softcut edge-directed <ref type="bibr" target="#b10">[11]</ref>, <ref type="foot" target="#foot_1">2</ref> and the Laplacian edge-directed <ref type="bibr" target="#b29">[30]</ref>     1) Comparison With GPP <ref type="bibr" target="#b28">[29]</ref>: Fig. <ref type="figure" target="#fig_11">9</ref> illustrates the comparison of our method with the GPP edge-directed and Laplacian edge-directed methods in detail. As shown in this figure, the edges are significantly blurred by the back-projection and Laplacian edge-directed methods (see closeups). The ringing artifacts along the image edges of our method are less than those of the GPP edge-directed method. Particularly, this figure also shows that the gradient magnitude of our method is sharper than those of other methods, especially along the edges (see closeups).</p><p>2) Comparison With Soft-Cut <ref type="bibr" target="#b10">[11]</ref>: Figs. 10 and 11 present two comparisons of our approach with the soft-cut edge-directed and Laplacian edge-directed methods. As with the results in Fig. <ref type="figure" target="#fig_11">9</ref>, the image edges are blurred by the back-projection and Laplacian edge-directed methods. Moreover, these results also have obviously ringing artifacts along edges (refer to close-ups). In the soft-cut edge-directed superresolution results, some salient edges are blocked and look unnatural. For example, as shown in Fig. <ref type="figure" target="#fig_13">11(c</ref>), the blocking  artifacts are obvious along both sides of the black dapple of the butterfly. Compared with these results, our approach recovers the super-resolution results better, especially on salient edges.</p><p>3) Comparison With GPP <ref type="bibr" target="#b28">[29]</ref> and Soft-Cut <ref type="bibr" target="#b10">[11]</ref>: Figs. 12 and 13 present two more super-resolution examples by comparing with GPP, soft-cut, and Laplacian edge-directed methods. As shown in these two figures, the back-projection and Laplacian edge-directed algorithms produce serious ringing artifacts, such as the edges around nose in Fig. <ref type="figure" target="#fig_14">12</ref>. The salient edges in the soft-cut results are very sharp, but the small scale edges are not well recovered, for example, the flecks on the face in Fig. <ref type="figure" target="#fig_14">12</ref>, and the texture areas on the hat in Fig. <ref type="figure" target="#fig_15">13</ref>. The main reason is that in the soft-cut edge-directed method, the alpha channel in a small scale area is very hard to estimate. Hence, the gradient field obtained by this method is very smooth, which further causes the loss of details. From the second rows of these two figures, the gradient magnitudes of GPP edge-directed are less sharper than those obtained through our method and the soft-cut method. In particular, both the figures show that the gradient magnitudes of our method are  more similar to the ground truth than those of GPP and softcut methods. This indicates that the gradient fields obtained through our method are more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons With Learning and De-convolution Based Methods</head><p>In this subsection, we compare the proposed method with four methods, i.e., Yang et al.'s <ref type="bibr" target="#b6">[7]</ref>, <ref type="foot" target="#foot_3">4</ref> Shan et al.'s <ref type="bibr" target="#b33">[34]</ref> (de-convolution based), <ref type="foot" target="#foot_4">5</ref>  aspects, i.e., super-resolution results and computational cost. In our method, the iteration times are equally set 30. As can be seen from Figs. 14 and 15, the results of Yang et al.'s are sharper in appearance than those by the bicubic (see close-ups for detail comparisons). However, the high-frequency artifacts are also introduced from the training samples, such as the ringing artifacts around the pipelines. In Shan et al.'s <ref type="bibr" target="#b33">[34]</ref> results, the salient edges are smoother than those of our method [see the comparison of red patches in Fig. <ref type="figure" target="#fig_16">14(c</ref>), (d) as an example]. The speed of our method is faster. For example, in Fig. <ref type="figure" target="#fig_16">14</ref>, the running time of our method is about 20 s.   To sum up, these comparisons show that our method is able to generate sharper edges while producing less artifacts than the learning-based super-resolution methods. Meanwhile, the computational cost of our method is lower. Furthermore, as a general method on the image gradient, our gradient sharpening algorithm can also be applied to other image processing tasks, such as image enhancement and image de-focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison on Dataset</head><p>Table <ref type="table" target="#tab_1">III</ref> gives the numeric comparisons of our method with the bicubic, back-projection <ref type="bibr" target="#b24">[25]</ref>, Laplacian edge-directed <ref type="bibr" target="#b29">[30]</ref>, and Yang et al.'s <ref type="bibr" target="#b6">[7]</ref> methods. The tested images are obtained from the Berkeley website <ref type="bibr" target="#b32">[33]</ref>. Each image has the corresponding edge mask. Three up-sampling scales are adopted to make the comparison more thoroughly. The original image is used as the ground truth. The LR image is obtained by down sampling its original version. The EMS, ERMS, and SSIM error metrics are used to quantitatively measure the super-resolution results. We calculate the average errors of all tested images. As shown in the Table III, our method presents lower EMS and ERMS and higher SSIM, as compared with others, especially when the up-sampling scale is larger. Furthermore, the computational cost of our method is significantly</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of sharpening gradient magnitude. (a) Input LR image. (b) Bicubic up-sampled image (3X magnification). (c) Gradient maps (normalized and inverted magnitudes) of (b). (d) Closeups of the red rectangular region. Note that the X-O-Y plane is the image plane, and the z-axis represents the gradient magnitude. The green arrows illustrate a 1-D path passing through the edge-pixel (see the red point). (e) Gradient profile is the curve of gradient magnitude along the green arrows of (d). (f) Gradient magnitude sharpened by GPP<ref type="bibr" target="#b20">[21]</ref>. The red curves in (f) and (g) are the original gradient magnitudes, while the blue curves are the sharpened gradient magnitudes. (g) Gradient magnitude sharpened by our method. The green arrows in (g) are the displacement field, while the pink curve is the gradient of the original gradient profile (red curve).</figDesc><graphic coords="3,312.73,53.40,249.12,131.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a)]. The sharper the function f (x), the smaller the ω (a,b) (f (x)). Definition 1: f (x) and g(x) are both continuous and monotonic functions defined in (a, b), satisfying f (a) = g(a) and f (b) = g(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Sharpness ω (a,b) (f (x)) is defined as the mean width of f (x), or the rectangle width (green line). (b), (c) Intuitive comparisons of two functions.</figDesc><graphic coords="5,51.72,53.86,244.80,72.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Example of multiple scales gradient magnitude sharpening. The red curve is the original gradient magnitude, the blue curve is the sharpened gradient magnitude, and the pink curve is the gradient of original gradient magnitude.</figDesc><graphic coords="5,347.73,54.30,178.56,89.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Algorithm 1 :</head><label>51</label><figDesc>Fig. 5. Super-resolution example (4X magnification). (a) Input LR image. (b) Bicubic. (c) Initialization of our method. (d) Our result. (e) Gradient maps (normalized and inverted magnitudes) of (b). (f) Our adaptively selfinterpolated gradient maps.</figDesc><graphic coords="6,51.00,54.23,251.04,116.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Initialization comparison on synthetical image (4X magnification). (a) Bicubic initialization of GPP. (b) Our initialization. (c) Comparison of the edge sharpness. To compare sharpness, for each image, we select the pixel values in the 140th row.</figDesc><graphic coords="7,311.23,154.48,251.52,199.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Initialization comparison on two real images (4X magnification). (a) Bicubic initialization of GPP. (b) Our initialization.</figDesc><graphic coords="7,48.72,421.06,251.04,207.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Effect of parameter α on the super-resolution results. (a) Original image. (b)-(h) are the up-sampled results (3X) with α = 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Super-resolution comparison (3X magnification) of other edgedirected methods. (a) Back-projection. (b) Laplacian edge-directed [30]. (c) GPP edge-directed [29]. (d) Our result. The second row presents the gradient magnitude (normalized and inverted magnitude) detail of up-sampled images. The third row illustrates the closeups. Please refer to the electronic version for a better comparison.</figDesc><graphic coords="8,314.02,53.72,251.04,144.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Super-resolution comparison (3X magnification) of other edgedirected methods. (a) Back-projection. (b) Laplacian edge-directed [30]. (c) Soft-cut edge-directed [11]. (d) Our result. The bottom row illustrates the closeups of the corresponding results.</figDesc><graphic coords="8,314.02,271.04,251.04,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Super-resolution comparison (3X magnification) of other edgedirected methods. (a) Back-projection. (b) Laplacian edge-directed [30]. (c) Soft-cut edge-directed [11]. (d) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Super-resolution comparison (4X magnification) of other edgedirected methods. (a) Back-projection. (b) Laplacian edge-directed [30]. (c) GPP edge-directed [29]. (d) Soft-cut edge-directed [11]. (e) Our result. (f) Ground truth. The second row presents the gradient magnitude (normalized and inverted magnitude) detail of up-sampled images.</figDesc><graphic coords="8,51.00,253.82,251.04,79.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Super-resolution comparison (3X magnification) of other edgedirected methods. (a) Back-projection. (b) Laplacian edge-directed [30]. (c) GPP edge-directed [29]. (d) Soft-cut edge-directed [11]. (e) Our result. (f) Ground truth. The second row presents the gradient magnitude (normalized and inverted magnitude) detail of up-sampled images.</figDesc><graphic coords="8,51.00,393.57,251.04,66.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Super-resolution comparison (4X magnification) of learning based methods. (a) Bicubic. (b) Yang et al.'s [7]. (c) Shan et al.'s [34]. (d) Our result. The running time of our method is about 20 s. Yang et al.'s needs 80 s.</figDesc><graphic coords="9,48.22,182.93,251.52,82.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Super-resolution comparison (4X magnification) of learning based methods. (a) Bicubic. (b) Yang et al.'s [7]. (c) Shan et al.'s [34]. (d) Our result. The running time of our method is about 14 s. Yang et al.'s needs 50 s.</figDesc><graphic coords="9,311.23,177.52,251.52,92.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Super-resolution comparison (4X magnification) of learning based methods. (a) Bicubic. (b) Glasner et al.'s [6]. (c) Freedman et al.'s [24]. (d) Our result. The bottom rows illustrate the close-ups of the corresponding results.</figDesc><graphic coords="9,311.73,53.72,250.56,72.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Super-resolution comparison (3X magnification) of learning based methods. (a) Bicubic. (b) Glasner et al.'s [6]. (c) Freedman et al.'s [24]. (d) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,51.00,299.94,251.04,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>Initialization Comparison on 300 Images Obtained From</figDesc><table><row><cell></cell><cell cols="3">Berkeley Database</cell></row><row><cell></cell><cell>Method</cell><cell>GPP</cell><cell>Our Method</cell></row><row><cell cols="2">Mag.&amp; Criteria</cell><cell></cell></row><row><cell></cell><cell>RMS</cell><cell>13.48</cell><cell>13.60</cell></row><row><cell>X3</cell><cell cols="2">ERMS 21.49</cell><cell>21.00</cell></row><row><cell></cell><cell>SSIM</cell><cell>0.738</cell><cell>0.739</cell></row><row><cell></cell><cell>RMS</cell><cell>15.51</cell><cell>15.40</cell></row><row><cell>X4</cell><cell cols="2">ERMS 24.97</cell><cell>23.91</cell></row><row><cell></cell><cell>SSIM</cell><cell>0.665</cell><cell>0.670</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc>Effect of Parameter α on 300 Images Super-Resolution Results Parameter α = 0.05 α = 0.075 α = 0.1 α = O.125 = 0.15 α = 0.2 α = 0.25</figDesc><table><row><cell>Criteria</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMS</cell><cell>12.84</cell><cell>12.84</cell><cell>12.86</cell><cell>12.84</cell><cell>12.85 12.85 12.85</cell></row><row><cell>ERMS</cell><cell>18.76</cell><cell>18.78</cell><cell>18.67</cell><cell>18.82</cell><cell>18.84 18.89 18.94</cell></row><row><cell>SSIM</cell><cell>0.767</cell><cell>0.767</cell><cell>0.768</cell><cell>0.767</cell><cell>0.766 0.766 0.765</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Glasner et al.'s [6], 6 and Freedman et al.'s. [24]. In the first example, we compare our method with Yang et al.'s and Shan et al.'s in two</figDesc><table /><note><p><p><p><p><p><p>7  </p>1) Comparison With Yang et al.'s</p><ref type="bibr" target="#b6">[7]</ref> </p>and Shan et al.'s</p><ref type="bibr" target="#b33">[34]</ref></p>:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Yang et al.'s needs 80 s. It is worth noting that we run all algorithms on PC with CPU as a processor. Compared with Yang et al.'s and Shan et al.'s methods, our edge-directed method not only recovers the HR image with less visual artifacts, but also takes a lower computational cost.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The results are obtained from yuwing.kaist.ac.kr/projects/superresolution/ index.htm. In this website, Tai et al.<ref type="bibr" target="#b4">[5]</ref> provide the code about GPP implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The results are available at vision.eecs.northwestern.edu/research/IP/SR/ index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We implement the Laplacian edge-directed super-resolution algorithm according to their paper<ref type="bibr" target="#b29">[30]</ref>, and some parameters are adjusted according to the input image.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Code available at www.ifp.illinois.edu/∼jyang29/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The executable code is downloaded from www.cse.cuhk.edu.hk/∼leojia/ projects/upsampling/index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The results are available at www.wisdom.weizmann.ac.il/∼vision/ SingleImageSR.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>The results are available at www.cs.huji.ac.il/∼raananf/projects/lss upscale/ index.html.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program of China, under Grant 2012CB316304, and by the National Natural Science Foundation of China under Grants 61175025, 61005036, and 61272331. This paper was recommended by Associate Editor W</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We compared our algorithm with the bi-cubic, back-projection, Yang et al.'s <ref type="bibr" target="#b6">[7]</ref>, Laplacian edge-directed <ref type="bibr" target="#b29">[30]</ref>, and GPP edge-directed <ref type="bibr" target="#b28">[29]</ref> algorithms by three error measurements, i.e., EMS, ERMS, and SSIM. The CPU time rows present the running time (s). (Bold: best, underline: second best).</p><p>lower than that of Yang et al.'s method. Note that the iteration times for back-projection, Laplacian edge-directed, and our method are equally set to 30. We select two images, i.e., Star Fish and Lady, as examples for detailed comparison. The visual comparison results are presented in Fig. <ref type="figure">18</ref>, while the quantitative comparison results are shown in Table <ref type="table">IV</ref>. The quantitative results of GPP are obtained from their paper <ref type="bibr" target="#b28">[29]</ref>. As shown in this table, our method is better than the others. Moreover, as shown in Fig. <ref type="figure">18</ref>, our method recovers sharper HR images. As reported in <ref type="bibr" target="#b28">[29]</ref>, the gradient sharpening times for these two images are 0.829 and 0.862 s, respectively. However, in our method, it only needs 0.266 and 0.269 s to sharpen these gradient. Hence, the gradient sharpening time of our method is greatly lower (three times) that those of <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. Conclusion and Future Work</head><p>In this paper, an edge-directed single-image super-resolution algorithm was proposed by using a new adaptive gradient magnitude self-interpolation. The extensive experimental results demonstrated that the proposed gradient constraint, which is represented by the estimated HR gradient, can preserve image details or sharp edges, while suppressing the ringing, blocking, and blurring artifacts, especially along salient edges.</p><p>In the future, we want to speed up the proposed superresolution algorithm. It is also interesting to incorporate the learning-based models to enhance the super-resolution results. In addition to the super-resolution problem, the proposed gradient constraint can also be effectively applied to other computer graphic and computer vision problems, such as image de-blurring, de-noising, and enhancement.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="817" to="834" />
			<date type="published" when="1999-09">Sep. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing the non-local-means to super-resolution reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-constrained hallucination for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super resolution using edge prior and single image detail synthesis</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2400" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Markov random field model-based edgedirected image interpolation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1121" to="1128" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image interpolation by adaptive 2-D autoregressive modeling and soft-decision estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="896" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft edge smoothness prior for alpha channel super resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptually-inspired and edgedirected color image super-resolution</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="page" from="1948" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image up-sampling using total-variation regularization with a new observation model</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun.-Jul. 2004</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image hallucination with primal sketch priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002-04">Mar.-Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">New edge-directed interpolation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image interpolation and resampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thvenaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Medical Imaging, Processing and Analysis</title>
		<meeting><address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press, Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="393" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cubic splines for image interpolation and digital filtering</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="508" to="517" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Example-based learning for singleimage SR and JPEG artifact removal</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Max Planck&apos;Insitut fur biologische Kybernetik</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<date type="published" when="2008">2008</date>
			<pubPlace>Tübingen, Germany Tech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local selfexamples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motion analysis for image enhancement: Resolution, occlusion and transparency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Representation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="324" to="335" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002-09">Sep. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fundamental limits of reconstruction-based superresolution algorithms under local translation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image magnification using level set reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwartzwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="333" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient profile prior and its applications in image super-resolution and enhancement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1529" to="1542" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast gradient-aware upsampling for cartoon video</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Anal. Signal Process</title>
		<meeting>Int. Conf. Image Anal. Signal ess</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="636" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An unbiased detector of curvilinear structures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="125" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error measurement to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2001-07">Jul. 2001</date>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">He is currently an Associate Professor with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences. His current research interests include image processing, pattern recognition, and machine learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<idno>pp. 153:1-153:7</idno>
	</analytic>
	<monogr>
		<title level="m">He is currently an Assistant Professor with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</title>
		<title level="s">Lingfeng Wang received the B.S. degree in computer science from Wuhan University</title>
		<meeting><address><addrLine>Wuhan, China; Beijing, China; Chongqing, China; Beijing, China; Wuhan, China; Beijing; Fremont, CA; Tianjin, China; Xi&apos;an, China; Beijing, China; Beijing, China; Beijing; Beijing; Beijing, China; Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Shanghai Institute of Optics and Fine Mechanics</publisher>
			<date type="published" when="1990">2008. 2007. 2004. 1996 to 2001. 2006. 2002. 2009. 2000 and 2003. 2008. 2008 to 2011. 1990</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology ; Tsinghua University ; Beijing University of Aeronautics and Astronautics ; School of Electronics Engineering and Computer Science, Peking University</orgName>
		</respStmt>
	</monogr>
	<note>Chinese Academy of Sciences. and the Ph.D. degree in pattern recognition and intelligent systems from the Institute of Automation, Chinese Academy of Sciences, in 2000. He is currently a Professor with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences. His current research interests include computer vision, image processing. computer graphics, and remote sensing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
