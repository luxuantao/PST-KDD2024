<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Doom or Deliciousness: Challenges and Opportunities for Visualization in the Age of Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">V</forename><surname>Schetinger</surname></persName>
							<idno type="ORCID">0000-0002-8116-794X</idno>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Di</forename><surname>Bartolomeo</surname></persName>
							<idno type="ORCID">0000-0001-9517-3526</idno>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
							<idno type="ORCID">0000-0000-0000-0000</idno>
							<affiliation key="aff1">
								<orgName type="department">ETH AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Mcnutt</surname></persName>
							<idno type="ORCID">0000-0001-8255-4258</idno>
							<affiliation key="aff2">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Miller</surname></persName>
							<idno type="ORCID">0000-0002-6281-2173</idno>
							<affiliation key="aff3">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<addrLine>6 Hugging Face</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Passos</surname></persName>
							<idno type="ORCID">0000-0000-0000-0000</idno>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Adams</surname></persName>
							<idno type="ORCID">0000-0002-7826-3500</idno>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Wien</surname></persName>
						</author>
						<title level="a" type="main">Doom or Deliciousness: Challenges and Opportunities for Visualization in the Age of Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Different types of data visualization (and the prompts used to create them) as imagined a text-to-image generative models. While these examples are delicious (in their graphical intrigue and exploration of style), they risk doom by potentially creating misplaced trust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models are becoming increasingly prominent and experience an intensified use in many domains. There has been an astonishing technological development among such tools, such as the text-to-image generation of DALL-E 2 [Ope22a], Stable Diffusion [RBL * 22], or MidJourney <ref type="bibr" target="#b37">[Mot22]</ref>, as well as the textual synthesis found in tools like , Copilot [Git22], or Galactica [TKC * 22]. These systems take in a (typically textual) prompt and create an entity (such as an image or text) drawn from their learned representations of training data. These tools have been hailed for their rapid development and high-quality results.</p><p>Yet, this evolution has not been without friction. The automated creation of images and text raises questions about the role of human agency in the production of meaning in a graphical context, creating tensions in professional <ref type="bibr" target="#b42">[Plu22]</ref>, legal <ref type="bibr" target="#b58">[Vin22]</ref>, and artistic <ref type="bibr" target="#b43">[Roo22]</ref> contexts. For instance, the term Bach Faucet <ref type="bibr" target="#b9">[Com22]</ref> captures the paradox that the existence of a machine that can produce arbitrarily many high-quality instances of an artistic style reduces the value of that artistic style-inspired by an early success in generative art that was able to automatically produce sonatas in the style of Bach <ref type="bibr" target="#b10">[Cop04]</ref>. As with other model types, generative models replicate biases present in their training data, which might yield racist imagery <ref type="bibr" target="#b40">[Ope22c]</ref>.</p><p>Despite these challenges, these models offer the substantial potential to improve a wide variety of visualization workflows, such as by increasing the speed of production, facilitating creativity, and enabling expression. However, there has been little investigation into the role these generative models might play in visualization. Wood <ref type="bibr" target="#b65">[Woo22]</ref> called for their use as a way to break away from the "walled garden" of structured visualization, while others have explored using the visualization generation as means to explore the possibility space of highly stylized visualizations [For22, SFPM * 22] or validating visualization usage <ref type="bibr" target="#b66">[WTC22]</ref>. We believe that their wider spread utilization is on the nearby horizon.</p><p>This work seeks to circumvent the dilemmas found in other domains' interaction with generative models by trying to understand this landscape before it arrives. In support of this goal, we seek to answer:</p><p>What challenges and opportunities might we expect to find as use of generative models becomes commonplace in visual design and analytical workflows?</p><p>We answer this question by seeking the concerns, opinions, and predictions of domain experts (N = 21) from visualization, machinelearning, art, and art history in a semi-structured interview study (Sec. 3). Through this study, we elicited beliefs about the risks and opportunities posed by the generative models. Our interview study principally focuses on image-based generative models (IGMs); however, there are a wide variety of other generative models.</p><p>We analyze these findings by locating challenges and opportunities available at each stage of a standard visualization pipeline (Sec. 4). We find that participants believed that the use of generativity in visualization was promising. For instance, it may allow us to capture ephemeral aspects of visualization (such as emotion), or to create artistically rendered visualizations, or to increase the speed with which visualization designers can rapidly prototype their scientific and graphical communications. Such models also offer ample risk for visualization. For instance, there was substantial concern over the proclivity of such tools to amplify bias and to parrot components of their training data (in such a manner that was not respectful of copyright). We extend these concerns and highlight how future work might ameliorate such issues, as well as take advantage of the opportunities highlighted. This study and its analysis forms the core of our contribution. Many examples of our sketching of these concepts can be found in our online ideation gallery: http://visxgen.art/. In additon, our supplemental material is available on osf.io.</p><p>In conducting this study, we look to provide a forward-looking foundation of how these models might be used-both to guide future tool construction, but also to steer design of the models themselves. Per Bender and Koller <ref type="bibr" target="#b4">[BK20]</ref>, this work aims at highlighting the right hills to climb and valleys to watch out for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>From the creation of Generative Adversarial Networks (GANs) in 2014 <ref type="bibr" target="#b36">[MO14]</ref> to new text-to-image tools in 2022, such as DALL-E 2 [RDN * 22], Imagen [SCS * 22], Stable Diffusion 2.0 [RBL * 22], and MidJourney <ref type="bibr" target="#b37">[Mot22]</ref>, the development of image-based generative models (IGMs) has been progressing at breakneck speed, quickly entering many domains. We posit that it is reasonable, too, to expect the widespread adoption of generative technologies in visualization creation within the coming decade. Our work is situated among prior studies on generative models, both in using visualization to understand generative models ("Vis4Gen") and using generative models to enhance the overall process of visualization ("Gen4Vis").</p><p>Vis4Gen While the applications of generative technologies to improve or enhance visualization (Gen4Vis) are as yet in early development, there has already been significant exploration of how visualization can aid artificial intelligence reasoning (Vis4Gen). Within the visualization community, the rapid increase in papers on this topic demonstrate that there is significant interest in developing visualization tools for interpretability and explainability. Some examples of tools in this realm include Lundberg et al.'s SHAP visualization package <ref type="bibr" target="#b31">[LL17]</ref> for viewing the Shapley values computed from feature partial dependence plots (PDPs); Hohman et al.'s Summit tool <ref type="bibr" target="#b25">[HPRC20]</ref> for viewing neuron activation and influence in image classification models; Amershi et al. <ref type="bibr">'</ref>  <ref type="bibr" target="#b38">[MQB19]</ref> which supports exploration of decision-tree approximations for more complex models -just to name a small sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gen4Vis</head><p>The use of generative tools in the development of data visualizations has been of growing interest recently. Often, in traditional visualization pipelines, a designer has to either be proficient in a programming language, or translate their ideas into tool-specific operations [SSL * 22], which makes for a steep learning curve. A number of techniques have been developed to simplify the process, using either visual interfaces, or, more recently, natural language <ref type="bibr" target="#b50">[SLJL10]</ref> interfaces, which allow users to produce visualizations by simply typing or speaking their questions or requests. Three recent surveys are of particular relevance to our exploration how machine learning is being applied to the data visualization process [WCWQ22,WWS * 22,WH22].</p><p>Wang et al. focused on dividing applications of machine learning for visualization into three visualization process categories: data, visualization, and user <ref type="bibr" target="#b59">[WCWQ22]</ref>. The survey presents these elements as modular components of a prospective visualization process pipeline, which has informed our thinking in Sec. 4. Data transformations such as dimensionality reduction could be applied prior to visualization [WFC * 18]. Machine learning might be applied to a dataset to automatically generate Vega-Lite visualization specifications <ref type="bibr" target="#b12">[DD18]</ref> or to suggest the automatic stylistic transfer of graph drawing from one example layout to another [WJW * 20]. User-centered interventions might include user profiling, such as predicting a user's next click <ref type="bibr" target="#b39">[OGW19]</ref> or anticipating their visual attention across an infographic [BKO * 17, WCWQ22].</p><p>We use this framing in an axis of our interview study (Fig. <ref type="figure">2</ref>), from IGMs, like all ML-models, are susceptible to bias based on their training data. For instance, a prompt asking for a doctor might only produce images of white men <ref type="bibr" target="#b40">[Ope22c]</ref>.</p><p>Fig. <ref type="figure" target="#fig_0">3B</ref> Untrustworthy results Charts generated by probabilistic models may not accurately represent the input data or may manipulate it in subtle or confusing ways Fig. <ref type="figure" target="#fig_0">3B</ref> Replicated Private data Models are well known to parrot <ref type="bibr" target="#b3">[BGMMS21]</ref> their training data, and these models run a similar risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3B Rip off existing vis style</head><p>Just as IGMs are criticized for replicating artists' styles <ref type="bibr" target="#b21">[Hei22]</ref>, these might try to parrot well known graphics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3B Replacing vis designers</head><p>Sufficiently advanced models might be able to replace human visualization designers Fig. <ref type="figure" target="#fig_0">3B</ref> "Gen supports VIS" to "VIS supports Gen" and seed this visualization with prompts from the above related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology: Semi-Structured Interview Study</head><p>To better understand the challenges and risks that may arise through the integration of generative models into visualization workflows, we conducted a semi-structured interview study that sought to elicit forecasts, opinions, fears, hopes, and concerns arising from experts from various domains. Given the rapid development cycles of generative models, our goal in this study is to prospect the space around visualization (rather than specifically identify the next trends)-work which we believed would be best aided by those with expertise in this or related domains.</p><p>Study Participants. We conducted interviews with 21 participants, which lasted an average of one hour. Interviews were conducted remotely over Zoom. Participants were drawn from a convenience sample assembled based on their work in relevant domains. In particular, we consulted experts with backgrounds in art or art history (N = 5), machine learning (N = 2), and visualization or HCI (N = 14). Given</p><p>Figure <ref type="figure">2</ref>: Participants were invited to locate potential usages of IGMs for visualization in a space that sought to elicit how much a concept (such as "rapid iterative prototyping") was artistic or supported visualization vs. supported generativity (per Sec. 2).</p><p>their central relevance to the topic, most participants' primary background was in visualization, however, most lacked experience with generative tools, and so we supplemented their opinions with those of experts drawn from other fields. We included a participant if they had sufficient expertise in their relevant domain-as demonstrated by holding or pursuing a post-graduate degree or a substantial history of working in their field. We conducted two pilot interviews prior to the study. However, the results were close enough to include them in our eventual analysis. See appendix for details of self-reported backgrounds. Participants were not compensated.</p><p>Study Procedure. Interviews consisted of three phases, which, sequentially, sought to elicit participant self-identification (Fig. <ref type="figure">8</ref>), opportunities that they foresaw, and risks posed. The discussion was focused through a shared Miro board (see appendix for examples), in which participants were invited to create post-its about ideas and place them on several axes denoting different views (Fig. <ref type="figure">2</ref>, 3). In addition to predefined prompts (which we list in Table <ref type="table" target="#tab_1">1</ref>), participants were invited to create their own and thereby ruminate on their concerns and aspirations for these tools. These discussions were interleaved with situating prompts that reviewed various functionalities possessed by current tools, such as in-painting <ref type="bibr" target="#b70">[YQS20]</ref> and out-painting <ref type="bibr" target="#b68">[XLC20]</ref>, to help situate their thinking among current   <ref type="table" target="#tab_1">1</ref>) and selfdefined suggestions onto these axes as a means of ideation. Each quadrant was given a provocative name based on its meaning to help the understanding of the task.</p><p>technologies. We utilized this shared creative space to invite thinking aloud and ideation, which may have been more limited in a more restricted environment. Through these discussions, participants rated various potential concerns and usages of IGMs on either four axes (purpose, objectivity, excitement, and feasibility) or two axes (concern and likelihood), depending of if we were discussing a potential or a usage, respectively. We focused on these axes (and therein aspects) and set of initial topics (Table <ref type="table" target="#tab_1">1</ref>) because they might elicit thoughtful discussion of fears and hopes for these technologies.</p><p>Result Coding. Interviews were automatically transcribed, and then were coded by two of the authors. Results and themes were then iteratively discussed among the rest of the team in order to form our discussion, which we present in the next section. Given the speculative nature of our work, we sought to locate our findings around a generalizable model, and so we modified a standard visualization pipeline in support of the resultant themes. We augmented this analysis by quantizing the placement of each usage or concern and normalizing it on a unary axis, as shown in Fig. <ref type="figure" target="#fig_1">4</ref>. The full results can be found in the appendix to this paper, with aggregates presented in Figure <ref type="figure">7</ref>. We analyze our interview findings into challenges and opportunities. Per our analysis methodology, described in the previous section, we locate our results within an adapted standard visualization pipeline model <ref type="bibr" target="#b35">[MKC20,</ref><ref type="bibr" target="#b7">Chi00]</ref>, which we show in Fig. <ref type="figure">5</ref>. This pipeline consists of four transformations: condensing the world into data by Data-fying it (Sec. 4.1), Transforming (Sec. 4.2) that data into something usable, changing that processed data into an image by Visualizing (Sec. 4.3), and then using or Interacting (Sec. 4.4) with that rendered image. While other workflow organizations (such as KDD model [FPSS96]) might be used, we select this one both to be generalizable but also to be in dialog with other works on applying AI-tools to visualization [WWS * 22]. Generative models entail opportunities for useful intervention at each stage of this pipeline.</p><p>We further organize our observations following a modification of the SWOT-strategic management [BEM * 21], which enumerates the Strengths and Weaknesses arising from inside an organization, as well as the Opportunities and Threats from outside. This approach is often used as part of the decision-making process for businesses or other organizations. We adapt this analysis by partitioning by whether the opportunities and risks arise from the visualization design side (producer) or the application domain side (consumer). In doing so, we seek to highlight the speculative decision of whether to integrate or implement a particular feature and thereby explore the possibilities available at each stage.</p><p>We next traverse our pipeline and highlight the potential that generative models hold for each stage, using the above color scheme to denote risks and opportunities. These considerations over-emphasize the visualization component of this pipeline (likely motivated by our context as trying to identify opportunities for visualization), however we note that the other stages also offer ample opportunities for generative intervention. To support these discussions, we show participant ratings of various potential usages and concerns, such as in Fig. <ref type="figure" target="#fig_1">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data-fying</head><p>Data is not a natural resource, and so its entry into a visualization workflow begins by conceptualizing what that data can be <ref type="bibr" target="#b35">[MKC20]</ref>. This process can be time-consuming and difficult, as data modeling is a notoriously challenging <ref type="bibr" target="#b44">[RS19]</ref>. A generative model might aid this process by helping the user identify what counts as data (or just by defining the data workspace subsequent analysis will occur in), such as by preparing a SQL query, suggesting a data model, finding a relevant data set, or otherwise injecting its own knowledge [HHS * 22]. For instance, a natural language prompt such as "get data from earnings of the fourth quarter relative to our competitors" offers ample ambiguity and opportunity for the model to exert agency, such as in identifying who the competitors are, how to compute earnings data, which business units to include, and so on. Huang et al. employ this approach for flow visualization using NLP <ref type="bibr" target="#b27">[HXHT22]</ref>. These moments of agency invite risk (e.g., the model might provide bad results or exhibit biases) [GSS * 22], but they also invite opportunity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amplifying Biases likelihood concern</head><p>Consider creating a subset of data (Fig. <ref type="figure">5 left</ref>). In such a task, the user asks the model to create a working subset of the data, such as by generating an SQL query for a large database. Such an automation might reduce the burden of designing a potentially difficult query and thereby speed up the analysis or design process. In addition, this might also allow for the system to synthesize or create data not present in the original dataset, but that might be inferred, such as emotions, or other aspects that the model might synthesize form the dataset <ref type="bibr" target="#b65">[Woo22]</ref>. Such an exchange of agency carries with it potential for harm; for instance, it invisibly surfaces the models' biases. If an idea is out of the scope of the training data, or if a type of person is biased against, that bias will be embedded into the data, possibly without labeling-an error which can cascade down the pipeline, affecting every subsequent data interaction. Reciprocally, the use of a model to construct data might also manifest the biases of the user, such as through confirmation bias. Participants believed that amplifying biases was an inevitable consequence of using these models. For instance, "when you model data using a normal distribution, and then you sample from this normal distribution, you will always have bias because things that sit closer to the average will have a higher probability of being selected" (P21).</p><p>Similarly, there were concerns about the model parroting its training dataset (Fig. <ref type="figure" target="#fig_1">4</ref> right). Some of these stemmed from anxieties about using material in ways not permitted by the licenses of the data (an issue which is being examined in by the American court system <ref type="bibr" target="#b58">[Vin22]</ref>), and the inability to track down the sources used for the generation. For instance, P6 observed that "If you are doing a chart, and you want a simple icon graphic to represent certain subjects, that would be the most common use case, but I have issues my wor-  <ref type="table" target="#tab_1">1</ref>.</p><p>A variety of other tasks might be fulfilled at this stage. P1 described the potential of automatically reverse engineering data from an image. This work has been explored previously <ref type="bibr" target="#b41">[PH17]</ref>. However, using a generative model might allow better extraction of data (not based on a predefined schema), but it may also introduce new, more difficult to address or identify forms of bias. Finally, generative models are trained with large datasets (Fig. <ref type="figure" target="#fig_1">4</ref> left), and their results can greatly vary depending on this training. Curating input data can help reduce biases, personalize models, and increase the general quality of results. The complexity latent to this task type could be allayed by a generative model, which might be able to act as an assistant, such as Hynes et al.'s data linter <ref type="bibr" target="#b26">[HST17]</ref>, which could provide more dynamic or situational suggestions than heuristically-motivated training-data analysis tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transforming</head><p>Once the data is established, it needs to be transformed into a form that might be pliable for visualization. This process can involve wrangling, processing, introducing additional models (such as through regression or other advanced analytics), or countless additional approaches. Within each of these interleaved steps and stages, there are opportunities for a generative model to exert agency. For instance, prompts like "remove outliers" or "find missing data" presupposes particular structures of the data. While those might sometimes be unambiguous (for instance, a monthly calendar missing data from the weekends has clearly defined gaps), in other cases, it might be less clear (for instance, removing outliers might assume a particular model of the data distribution). Consider cleaning data (Fig. <ref type="figure">5</ref> center-left); this stage might require removing duplicates or outliers, and converging on consistent naming-among a host of other sub-activities. These repetitive processes prove to be something that can largely be automated <ref type="bibr" target="#b30">[KW19]</ref>, and so a generative model might usefully intervene by automatically creating transformations. Tools like Copilot [Git22] can already effectively assist in such a process when there is a humanin-the-loop to help guide and curate the synthesis. Just as in the previous stage of our pipeline, yielding agency to a model runs the risk of introducing the model's biases into the generated content, which P9 and P17 particularly worried about. In addition, it may also risk outof-domain schemas which, noted P21, may cause the suggestions to be irrelevant or incorrect. Further, the potential frictionlessness of this process may cause the user to overly trust the synthesized results, a huge issue for many participants (P11, P12, P17, P19), which may lead to difficulties in identifying errors down the line. In addition to acting as a tool to automated transform creation, generative models might also be used to evaluate whether or not a transformation was done well or if any steps were missing, in something akin to a spell checker for data analysis, which might be analogous to Wu et al.'s <ref type="bibr" target="#b66">[WTC22]</ref> use of a generative model to develop a chart analyzerbased for Vega-Lite charts. While current LLMs do not consider data as part of their synthesis process future models might do so to ensure that suggestions are appropriate to the data and task to be pursued.</p><p>In addition, to the general task of cleaning data, generative models might be used at each incremental step of the cleaning pipeline, such as in creating regressions or identifying missing data. P11, for instance, was interested in augmenting financial datasets for research purposes: "getting data of financial transactions can be very hard, which makes it hard to test our techniques". These pose similar benefits (like increasing the speed to accomplish a task or showing an unknown functionality) and risks (furthering user and model biases) as in the general case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualizing</head><p>The most prominent stage in our pipeline is the visualization stage. Here, the modeled data is mapped to a visual encoding which will  subsequently be presented to the user. As our work is centered around visualization, participants observed the most risk in this domain, but also the most opportunities. For instance, some participants (P4, P6, P10,P12) highlighted the potential for harm in found graphics that do not handle sensitive content carefully (which is aligned to McNutt et al.'s <ref type="bibr" target="#b33">[MHK21]</ref> descriptions therein). Most common models contain some form of safety system against pornography, gore, and deepfakes, but they are not perfect and can sometimes be circumvented by resourceful users. Yet, the same free-form nature that can be potentially harmful also offers a deep well of potentially unfamiliar ideas and suggestions that might unset settled ideas.</p><p>One such example is the beautification of visualization designs, which while mostly aesthetic, could also be functional. From the selection of proper colors based on semantics [EAKM * 22, HYC * 22] to more extreme visual transformations, it was considered Prettyfication feasibility excitement objectivity v4g-g4v by participants to be mostly "subjective" (P15). However it was recognized that, as with Coelho's [CM20] "Infomages", they do change the impact and interpretation of a visualization. For example, P17 commented on the value of beautification by considering the "entertaining value of generative models for engaging users" (P17). Works like BeauVis <ref type="bibr" target="#b24">[HIDI22]</ref> could be used to orient the training of models for "aesthetic amplification". Participants expressed enthusiasm towards this aspect, but also doubts on its feasibility. Most of the concerns related to lacking trust into the result of the model -indeed, P8 explained, there would be no way of knowing if the change in design would affect the data represented in the resulting visualization, or that the change would be appropriate assuming the process is done through a model that has no awareness of the data and the context of the user. These issues are further discussed in Sec. 5.1.</p><p>Moodboard creation through an application where visualization supports the use of generative models was summarized as a "Pin-terest in the generative space" (P13). Many creative tasks begin with constructing moodboards, or a collage of images and text for inspiration and ideation. Generative models, aided by visualization and interactive techniques to navigate and query the latent space could become an infinite source of inspiration. Most participants believed developing such moodboards to be easy, because besides Pinterest, there have been other examples of intelligent moodboards [KTBL * 20]. P21 pointed out that pulling images from the latent space is still a very GPU-intensive task, which limits this application. An example of how such an application could look like is seen in Fig. <ref type="figure" target="#fig_3">6b</ref>. Ideally, generative models could be used to suggest types of charts that have fitting qualities to represent given data. The tag presented to participants used the example of Tableau's chart recommendation system. Thus, this item was believed to be very feasible, as most people recognized that it already exists in some forms in commercial software. Participants were not very excited about chart recommendation systems, but some participants included as custom tags some additional ideas adjacent to chart recommendation. Examples are: "Exploring the parameter space for design solutions to a task" (P5), ""Semantic" chart recommendation" (P5), "Figuring out how to visualize multidimensional data that is hard to represent" (P8), and "Search for novel visualization types" (P8). Rapid iterative prototyping is the process of rapidly changing parameters while designing or developing a prototype to test different ideas and quickly iterate over them. It was placed exactly in the middle of the plane, as it is a process that could be used both for supporting VIS and supporting the development of generative models, and both for exploring a design space (thus a subjective process) and more objective, pragmatic purposes. Participants were excited about being able to use generative models for rapid prototyping, and they believed it to be feasible. P5 suggested the IGMs could be used to "generate variations of a design", rather than starting from scratch each time. A similar idea was expressed by P19 and P10, in terms of "exploring the parameter space for design solutions to a task" and "exploring potentialities of media". P1 and P21 made a connection between generative models and NLP-based interfaces for information visualization. Similarly, P14 thought that it might be used as a "search tool", which is similar to how users of Copilot perceive that tool [SGN * 22]. P12, coming from HCI, noted that "prototyping entails different things and processes depending on the field", and was not confident that interaction or complex engineering solutions could be prototyped this way. We sketch this idea, and some of the highlighted opportunities here, in Fig. <ref type="figure" target="#fig_3">6</ref> Chart Embellishment feasibility excitement objectivity v4g-g4v</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rapid Iterative Prototyping</head><p>The concept of chart embellishment was presented to the participants as "generating parts of a visualization that don't really require data"-which participants (P8) also referred to as "chart junk". We did not limit the concept to exclusively chart junk. For instance, the use of glyphs in visualization [SM14, YSD * 22], sometimes do not require data. Indeed, it is easy to see how IGMs could be used to generate iconic or symbolic encodings <ref type="bibr" target="#b65">[Woo22]</ref>, such as Chernoff faces <ref type="bibr" target="#b6">[Che73]</ref>, and support anthropographics <ref type="bibr" target="#b34">[MJAD20]</ref>. At least two interviewees (P12 and P17) thought that this was the most probable "inevitable corporate application" to come soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rip Off Existing Style likelihood concern</head><p>Design styles being ripped off was seen as inevitable, but opinions about how concerning it would be, were split. Some participants (P1, P8, P15) declared this wasn't a worry for them, and saw the issue as more of an advantage for them, while others cared deeply (P4, P6). The inevitability of it was often accompanied by a discussion on how this is already happening in the context of art and illustration. P17 commented that models are already doing this with illustrators and artists, and we should expect this for visualization too. In generative models, this concept is often called style transfer [SLC * 22].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Interacting</head><p>The last pipeline stage comprises the user interactions with data visualizations, or more generally, how they might use the visualization. Such interactions are iterated and interleaved amongst other stages of the pipeline: seeing one chart may suggest constructing or another, or may necessitate a return to the transformation stage to better clean an aspect of the data revealed to be incorrect.</p><p>An illustrative application of this potential is the (Fig. <ref type="figure">5</ref> right) of visual design to a users tastes, tasks, and abilities. For instance, a model might act as an adaptor and automatically transform rendered visualization into more accessible versions that are tailored to a users particular needs, such as color-blindness or low visualization literacy (Fig. <ref type="figure" target="#fig_3">6c</ref>). P10 thought that being able to express your own "personal aesthetics", such as by "(training on your own data)" would be useful. Graphics also might be personalized to how a user prefers to consume information. For instance, a manager might only like to see big number displays, while a analyst might like to see richer data displays. However this also runs the risk that seeing only such display might accidentally cause hallucinations based on properties of the chart form <ref type="bibr" target="#b35">[MKC20]</ref>. Such personalization also might be dynamic and make "changes based on tracking user behavior and attention" (P5). While such modifications could help to obtain new perspectives, there is a risk that users could lose track of the overview or sense of agency <ref type="bibr" target="#b20">[Hee19]</ref> of the presented information.</p><p>Despite the value of these interactions, participants were concerned about both malicious and involuntary misleading information generated through the models. They cited the ability for these models to generate believable misinformation (P1, P19, P21) and mentioned deepfakes (P7, P13) and fake news (P10). P17 warned that this could also be involuntary, mentioning the possibility of "irresponsible use of a black box", or just for the users of these models to be mislead by their own results. P14 reiterated the dangers of using a black box without fully understanding it, saying that "everything that is robust is obvious and everything that is obvious is robust", referencing Da's <ref type="bibr" target="#b11">[Da19]</ref> argument against the utility of computational literary studies, in which she dismisses them as domineering counting exercises that do little to interact with the core content of texts-an important failure mode to avoid in the design of future systems.</p><p>Finally, a variety of additional interactions and applications were suggested for these models. For instance, P20 suggested that a "content aware brush, painting enhancement" interaction might be useful to get alternatives for a particular part of a chart. These models could be used to generate guidance, such as in in the form of explanations. These might take the form of automatic summaries for charts (such as in the manner of Kanthara et al. [KLL * 22]), or parts of charts, such as in a Tableau's Explain data feature <ref type="bibr" target="#b55">[Tab22]</ref>-the advantage of generative model is that results could be iteratively tuned and adjusted to taste. Like other places in the pipeline where user preference can drive usage, this can lead to expressions of bias or what might be called an AI-filter bubble, wherein the the model reflects and amplifies the user's biases in a negative feedback loop. Further, high levels of intrusive automatic guidance could be distracting during the interpretation process if the user does not require additional information. Similarly if that guidance is delivered in a way that does not reflect the users agency or previous decisions then it runs the risk of being perceived as impolite <ref type="bibr" target="#b63">[Whi05]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Despite some skepticism and critique clouding the introduction of generative models to new domains generally, participants in our interviews expressed tempered optimism and cautious excitement about the possibilities for applying these technologies to visualization. Participants were asked to think beyond current technical constraints, and even the most conservative visualization experts conceded the usefulness and inevitability of these technologies, for aesthetic, artistic, and disseminative purposes. A common thread throughout our discussions was a sense of excitement: the huge technological leap to text-to-image models make a compelling case for the potential of visualization in the age of generative models. We echo this sentiment and look forward to dawning age of rich accessible illustrative visu-V. Schetinger, S. Di Bartolomeo, M. El-Assady, A. McNutt, M. Miller, J. P. A. Passos, J. L. Adams / Doom or Deliciousness alization adapted to taste and to task. Yet not every aspect of these models yields a delicious conclusion, which we discuss by considering cross-cutting concerns and challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Concerns</head><p>Our participants' enthusiasm of about these technologies is matched in their concerns. Participants mostly expressed that they were concerned about these technologies (Fig. <ref type="figure">7c</ref>), but that they were, perhaps unfortunately, inevitable. Only P18, a visualization expert who is "much more excited than worried about these technologies", considered most of the risks discussed "not worth a thought". Participants typically had strongly-held concerns, but there was not a general consensus of what was most worrying. Some seemed to view generative models as a form of Trojan horse, allowing the issues such as amplification of bias, leaking of private data, deep fakes, and so on, to creep in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unreliable Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>likelihood concern</head><p>Unreliable Results Among these issues, the unreliability of results was one of the aspects of greatest concerning aspects. Most worries stemmed from the untraceability or unexplainability of the sources and suggestions. For instance, such issues can make it difficult to demonstrate that an image is original, which may incur possible copyright or ownership issues. To wit, P6 claimed that the risk of copyright-violation would prevent them generating even minor sections of a visualization. While such issues may have a technical solution (such as is promised in Amazon's yet to be released Code Whisperer <ref type="bibr" target="#b1">[Ama22]</ref>), navigating the ethical elements beyond the legal ones remains a thorny task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing VIS Designers likelihood concern</head><p>Replacing Visualization Designers Despite our own assumptions to the contrary, participants were generally unconcerned about the premise that generative models would replace designers of visualization. Similarly, participants were generally not worried about designs being stolen, which we found surprising given the comparable concerns in artistic <ref type="bibr" target="#b58">[Vin22]</ref> and coding communities <ref type="bibr" target="#b43">[Roo22]</ref>. P6 also observed that they do not believe they will be replaced per se, but for the expectations to change and for goalposts to be moved.</p><p>No participants questioned whether "can computers be creative?" Our participants may not view visualizations as artistic artifacts and so such issues were irrelevant to our discussion, or perhaps the lack of a consensus on what just defines creativity <ref type="bibr" target="#b14">[FMBD18]</ref>, makes the matter of AI creativity murky. Two practicing artists, P13 and P20, agreed that mastering generative tools will become essential skills in their field, but espoused differing levels of optimism. While P20 was enthusiastic for achieving an edge in his trade, P13 is "worried about the raising entry level for newcomers in an already cruel business". P20 observed that tasks like illustrating land cards in Magic: The Gathering would likely be among the the first jobs to be consumed by these technologies, as generative models can already create astonishing landscape paintings faster and more easily than experienced illustrators. P9, P10, and P16-are also involved in art, but are not practicing artists-offered curiosity and excitement about this new media, as well as ethical concerns (such as use of publicly available art as training data without consent). The public perception of models that generate visualizations remains to be seen, but based on these initial reactions, we suggest that it may be less severe than the reception to generative art <ref type="bibr" target="#b43">[Roo22]</ref>. Yet, the negatively perceived practices that led to the current generation of text-to-image models may have 'poisoned the well' for some groups. For instance, the visualization community on the vis.social Mastodon social media instance has specifically disallowed AI-generated images that use text-to-image models such as MidJourney and Stable Diffusion, due to questions of provenance <ref type="bibr" target="#b22">[Hen22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Challenges</head><p>We next describe several high-level challenges that we identified through out analysis.</p><p>Data-Constrained Generation. The inclusion of constraints is a prominent technical gap in diffusion models. For instance, most popular models (such as Midjourney <ref type="bibr" target="#b37">[Mot22]</ref>) cannot deal with prompts involving quantities (such as "five dogs and two cats"), have trouble with text, and objects such as hands (Fig. <ref type="figure" target="#fig_3">6c</ref>). Thus the substantially more constrained task of creating data visualizations is far off at the moment. Constraining the output of generative processes to data is not a new concept [Soc17, KM20], but it can be tricky to apply to the powered-by-noise diffusion process of which most text-to-image models are currently based. Yet, even once such techniques are possible, it might be long a time until such images can be reliable trustedat least according to in the views of our participants. Some skeptical participants emphasized this limitation, arguing that generative models have no place in objective visualization (Fig. <ref type="figure">2</ref>, top-right quadrant). This is aligned with how, despite the promise of recommendation engines to create elaborate images, analysts have primarily preferred to focus on simple charts with clear takeaways [BLF * 22]. Still, we observed ample optimism in other interviewees, which suggest that when such a technology is manifest, it may be well received.</p><p>Beyond exploring how to better navigate and convey trust, there is ample room for improvement in generative models, such as from a performance perspective. For instance, the current resolution for generated images is still quite low, as higher resolutions require massive amounts of GPU memory. Simply by increasing the maximum resolution, we allow for higher frequencies to be rendered, which are essential for text and data visualization. We suggest that such issues indicate the value of investigating models that create vector, rather than raster, images, as they may be less constrained by such technical limitations.</p><p>Inheritance of AI Worries. By incorporating generative models into the visualization pipeline, we inherit many problems faced by the AI and ML communities. For instance, as we observed throughout Sec. 4, the issue of bias is a pressing concern. P16 observed that "bias is a human problem, not a machine problem", so our approach to addressing these issues can not rely exclusively on technical solutions. Further, the current trend towards increasingly large models has meant that only companies with large resources are able to develop these models from scratch. Several participants (e.g. P3, P12, P14) discussed the issue of centralization and democratization of these technologies, noting that the current situation places these firms in control of the data that can be leaked, allows them to exercise authority on what counts as bias, and potentially lead to censorship. Figure <ref type="figure">7</ref>: Aggregated participant responses. In (a) we see high interest in artistic applications of generativity to visualization creation. In (b) we see that the majority of concepts were considered as feasible, and participants were mostly excited about them. In (c), similarly, the top-right quadrant is dominant, indicating that the risks discussed were generally seen as concerning, and likely to happen.</p><p>Harmful Rationalization. The easier it is to create personalized, easily digestible, compelling, memorable, and beautiful visualizations, the harder it will be to guard ourselves from their influence. Misinformation becomes a bigger risk even without ill-intent <ref type="bibr" target="#b33">[MHK21]</ref>, as a convincing visualization might lead a person to become overconfident over the knowledge gained from it <ref type="bibr" target="#b46">[SEA22]</ref>. Their domineering form can make readers less questioning about conclusions <ref type="bibr" target="#b32">[MCC20]</ref>.</p><p>For instance, Galactica [TKC * 22], a generative large language model meant to facilitate scientific inquiry, often produced incorrect or biased answers to questions, but presented in an authoritative manner <ref type="bibr" target="#b19">[Hea22]</ref>.</p><p>Of course, when there exists actual ill-intent in the use of these models the possible visualizations that can be generated from them become even more dangerous. Many participants (P1, P7, P9, P10, P13, P21) worried about deep fakes and fake news, and that visualization powered by generative models has the potential to be a "super fake news machine" (P13). Yet, one does not even need to use fake information, only to have an automated tool capable of choosing convenient half-truths to support a position, querying the needed data and then creating beautiful and convincing visualizations that can be easily spread through unregulated social media channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Limitations</head><p>As with any study, ours has limitations. We focused on the ideation process in our interviews to elicit ideas and opinions from our participants, which by is an inherently speculative task. This conjectural nature likely influenced participants response, which would have been different if we had provided demos to experiment with. For instance, in the evaluating of concern versus likelihood some participants reported not being concerned about something precisely because they thought it was unlikely.</p><p>Our study participants may not be representative of a wider or different population. As we focused on gathering the perspective from visualization researchers and practitioners, the other groups (artists, art historians, etc) were under-represented in our sample. Some participants lacked familiarity with generative models and so had difficulty in discussing the default concepts.</p><p>Development of tools in this space continues to happen at a breakneck speed. During the course of our study, a number of notable models were released-such as MidJourney V4 <ref type="bibr" target="#b37">[Mot22]</ref>, Stable Diffusion 2.0 [RBL * 22], and Galactica [TKC * 22]. While the developments in these models are important, their behaviour is not fundamentally different from the prior models, and so we believe that our findings will continue to apply as these models are further iterated upon. Our interviews only focused on IGMs, rather than the full scope of generative models. We chose to focus on image generation because of its functional similarity to the human act of producing data visualizations, and for its visually engaging artifacts, which we hoped would provoke more inspired participant responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Conclusion</head><p>In this work, we sought to chart the challenges and opportunities latent in the oncoming interweaving of data visualization and generative models, particularly those focused on text-to-image generation. We did so through a semi-structured interview study with experts from a variety of domains, and identified a number of areas of risk (such as the potential for harm that models trained on countless images hold) as well as of areas of potentially ample reward (such as enabling easily accessible illustrative visualizations).</p><p>Yet, we are never really prepared for the effects new technologies can have in ourselves, in society. McLuhan observed that "We look at the present through a rearview mirror. We march backwards into the future" <ref type="bibr" target="#b13">[FM67]</ref>. However, we should not be afraid, because "there is absolutely no inevitability as long as there is a willingness to contemplate what is happening". Through this work, we strove to contemplate what is happening, gathering the participants experiences and opinions-their collective rearview mirrors-and then projecting them into the future. In this paper, we have tried to peer out from the walled garden of visualization <ref type="bibr" target="#b65">[Woo22]</ref> and chart this land beyond the trees: its risks, challenges, and opportunities. Should visualization researchers eat from the low-hanging fruit of generative models, leave the garden, and address the consequences? Figure <ref type="figure">8</ref>: Aggregate participant ratings for self identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Additional Details</head><p>In this appendix we include additional information that we were unable to fit in the main body of the paper. These cover further details about our participants, the full structure of the interview, and the data we obtained through the interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant backgrounds</head><p>As part of our on-boarding, participants were asked to self-identify between Practice??Research and Artistic??Technical by shaping a circle over a plane, which we aggregate in Fig. <ref type="figure">8</ref>. The resulting aggregation shows a variety of specialists and generalists over our covered spectra. This population includes 21 participants: 11 visualization researchers or practitioners (4 predocs, 4 postdocs, 3 researchers/designers, and 1 professor), the majority with over 5 years of experience and some with over 10 years, 3 artists each with over 8 of years experience, 2 machine learning researchers, 2 HCI postdocs, and 2 art historians, one of which is also a philosopher and researches automated image-making. Fig. <ref type="figure">8</ref> shows a distribution of the aggregated self identifications modeled as points. However, as can be observed in the miro interviews (end pages), each person had the freedom to shape a circle around this spectrum as desired, and participants varied widely in their choices of size and positioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interview Format</head><p>The interviews were done over a miro board, and each interview was split into three phases: onboarding (introduction, self-identification), exploring opportunities (stages 1a, 1b), and understanding risks (stage 2). The two main forms of interaction were adding post-its with ideas to the board, and then ordering them relatively to the axis to answer questions, which are done while vocalizing thoughts and talking to the interviewer. Adding a post-it or splitting an existing one represents an act of ideation, and establishing relative positions are acts of comparative evaluation. The steps of the interview are organized so that subjects alternate between ideation and evaluation.</p><p>During the onboarding, the participants were presented with examples of experimental visualizations made with generative technologies by the authors, briefly discussed them, and then were asked to self-identify by shaping a circle over an axis (Fig. Fig. <ref type="figure">9</ref>, top-left). This served to introduce the ideas that were to be discussed and to Figure <ref type="figure">9</ref>: A blank copy of the study instrument. Each participant was walked through each of the cells shown here, and add or sort sticky notes on the axes. The positioning of the sticky notes was then coded according to the axes, assigning each one a value between -1 and 1, yielding Fig. <ref type="figure">7</ref> make participants accustomed to Miro and the interview format.</p><p>The opportunities and risks phases contain an initial population of concepts, which serves two purposes: to help the interviewee orient themselves in the proposed dimensions, allowing for easier ideation, and to force them to evaluate core ideas curated by the authors. Originally, the spaces were empty to avoid priming, but the pilot interviews showed us that eliciting novelty requires a lot of creativity and effort in the part of the interviewee, which could easily blank. By having a set of relevant concepts in place we not only inspired subjects, but also forced them to give us some input in the subsequent evaluation phase by ordering them in the case they could not think of new ideas to add. The relevant stages for collecting insights were 1a, 1b, and 2, each with its own plane and conceptual goal.</p><p>Stage 1a starts with participants being shown a populated space of concepts through the dimensions of purpose (vertical) vs. objectivity (horizontal). Purpose is defined between two polarities: generative models supporting visualization at the top, and visualization supporting generative models at the bottom. After an explanation and discussion of these ideas, participants are asked then to add their own, and the totality of ideas at the board will be transferred to the next stage (1b).</p><p>Stage 1b sought to measure the relative excitement (vertical axis) vs the assumed feasibility (horizontal axis) of the ideas by spatially ordering them around. This was meant to gauge the participant's interest and belief that these applications could be useful and realistically integrated in the visualization process, or support it. Each quadrant was given a name based on its meaning to help the understanding of the task. At the end of this stage, participants were also asked to indicate which one of the applications offered the biggest risk to them by overlaying a red dot on top of it. The top-right quadrant is specially important to us, indicating the most probable opportunities: ideas which participants are excited about and appear feasible, and this is why we named it "delicious low-hanging fruits".</p><p>Stage 2 was intended to investigate the participants' fears and concerns about AI motifs applied to visualization, which would imply risks applications based on generative models. After being presented with a list of common "AI Worries", and some predefined risks for visualization, and then discussing them, the participant would be asked to add more ideas to the list. Then, once again, all ideas on the board would be moved to a plane, at this stage defined by concern (vertical axis) vs. likelihood (horizontal axis), where they had to be ordered comparatively. In this stage the quadrants are named relatively to the type of danger they represent: a concerning, but unlikely scenario is a "doomsday prophecy", but concerning and inevitable risks could be "like watching a train wreck", and tackling them should be treated as serious challenges.</p><p>The full artifacts of our interviews are included in the next pages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head><p>Most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI used in a VIS context</head><note type="other">Style</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head><p>Most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head><p>Most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head><p>Mercator projection everywhere ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data rights / copyright violation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private data being replicated</head><p>Vis design style being ripped off</p><p>Replacing artists / stealing jobs from humans</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacing visualization designers</head><p>Trusting the accuracy of the results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2b</head><note type="other">Most</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Worries Worries applying AI to VIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2a</head><p>Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</p><p>Replicating or amplifying biases that already exist </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Participants were invited to place both predefined potential usages and concerns for IGMs in visualization (Table1) and selfdefined suggestions onto these axes as a means of ideation. Each quadrant was given a provocative name based on its meaning to help the understanding of the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Participant ratings for a potential usage (visualizing training data as part of model construction) and a concern (private data replication). Ticks indicate individual responses, while circles indicate averages. Throughout, the blue boxes show potential usages, and brown show concerns 4. Analysis: Help and Harm in the Visualization Pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Interface of a visualization design system that lets users select different marks and channels to generate pretty visuals, and could also be used for defining smart, generative brushes. (b) A generative moodboard where an artist ideates visual moods for a garden, selecting between drawings and pictures of plants, and different color palettes. (c) A set of instructions for how to wash your hands. Two of the biggest difficulties for text-toimage models (at the time of writing) are shown: drawing hands and rendering meaningful text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An illustration of possible manifestations of Rapid Iterative Prototyping with a generative model. We simulated a simple design sprint, starting with a vague idea of what we wanted, and collaboratively refined our prompts to obtain the above proofs of concept, each of them being a realistic visualization-related use case. Strengths (aesthetics) and weaknesses (adhering to data) of the model can be observed. Additional example applications can be found at: http://visxgen.art/.</figDesc><graphic url="image-8.png" coords="6,49.09,81.96,150.63,150.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>V. Schetinger, S. Di Bartolomeo, M. El-Assady, A. McNutt, M. Miller, J. P. A. Passos, J. L. Adams / Doom or Deliciousness Concerns expressed in Fig.3B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>What would it look like to apply these AI motifs to the visualization design process? position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? or uncertainty of the data being represented (consider "sketchiness" on a continuous scale) thinking beyond 1:1 mappings to consider compositionality, complexity widening the scope of the design process; supporting deeper human understanding rather than replacing human intelligence things that are regarded as 'pretty' now might not be later as the ai aesthetic novelty wears off nuance: gen supports *understanding* (as a designer, as interrogating the usefulness of vis) --is the visual the right way to address these questions? vis is the means to something rather than the ends in itself clear benefit commercially but not as exciting re: what are the rules, what makes good design? bigger challenges might not be feasibility of tech, but acceptance of the tech in this field (visualization) could be a diminished version of their work; watering down their contribution/style</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research. Prior work has tended to focus on feature measures of the artefact, such as symmetry, complexity and coherence.Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? characterize different aspects of the same graph. Finding a "good" layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and v? echoing personal biases / lack of confrontation Generate parts of visualization that don't really require data input (e.g. icons) a chart look handdrawn Interpolating missing data Creating immersive analytics for XR on-the-fly Making sonification sound actually nice/pleasant Extrapolating future data Clustering/embedding Making sonification sound actually nice/pleasant Grouping similar chartsBrainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? Inevitable doomsday prophecy like watching a train wreck not worth a thought just the way it goes Generate parts of visualization that don't really require data input (e.g. icons) injection (the model can reduce personal responsibility) -rhetorical tool plagiarizing text-to-image as search and hybridization tool for an image corpus everything that is robust is obvious and everything that is obvious is robust mood/image targeting: https://dl.ac m.org/doi/abs/10.1145 /3170427.3188398 bias washing/injection (the model can reduce personal responsibility) -rhetorical tool plagiarizing text-to-image as search and hybridization tool for an image corpus mood/image targeting: https://dl.ac m.org/doi/abs/10.1145/3170427that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? , S. Di Bartolomeo, M. El-Assady, A. McNutt, M. Miller, J. P. A. Passos, J. L. Adams / Doom or Deliciousness ? 2023 The Author(s) Computer Graphics Forum ? 2023 The Eurographics Association and John Wiley &amp; Sons Ltd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>FeasibleFeasible</head><label></label><figDesc>position these ideas in relation to their feasibility and your personal interest in them? -VIS Make a chart prettier (e.g. make it look hand drawn) risks (red) or opportunities (green) in terms of feasibility for integration into the vis design processMove ideas from the box below onto the design space (feel free to add additional ideas as you see fit) (feel free to leave items in the box if you're not sure) Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What are some AI 'motifs' (different general things we can do with ML) that you've seen or are excited about? What would it look like to apply these AI motifs to the visualization design process. Can you think of more?Stage 6What are some AI worries from these generative technologies? Which are more concerning?Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)How would you position these ideas in relation to their feasibility and your personal interest in them? that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process? a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply) How would you position these ideas in relation to their feasibility and your personal interest in them? position these ideas in relation to their likelihood and your concerns about them? , S. Di Bartolomeo, M. El-Assady, A. McNutt, M. Miller, J. P. A. Passos, J. L. Adams / Doom or Deliciousness</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>s ModelTracker [ACD * 15] for monitoring missclassifications in image classifiers; Gomez et al.'s ViCE interactive counterfactual explorer [GHYB20]; the Embedding Projector from Smilkov et al.'s at Google Brain [STN * 16] for exploring and perturbing embeddings such as UMAP, tSNE, and PCA; Squares Multi-Class Classifier Explorer by Donghao et al. [RAL * 17] giving a new perspective to the multi-class confusion matrix; and surrogate model explainer RuleMatrix by Ming et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A listing of the predefined potential usages and concerns that participants were asked to place on various axes. model to improve the appearance of a visualization, as in style transfer [SLC * 22]. Fig.2, 3A Embellishment Automatically introduce visual elements not bound to data. E.g. legends, annotations, decorations, or chart junk. Fig. 2, 3A Chart recommendation Generate charts based on the semantics, types, or shape of the data.</figDesc><table><row><cell>Prompt</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Schetinger, S. Di Bartolomeo, M. El-Assady, A. McNutt, M. Miller, J. P. A. Passos, J. L. Adams / Doom or Deliciousness There are opportunities to apply generative models throughout the visualization pipeline. Here, we highlight some of the help and harm that use of generative models can do for particular example tasks at each stage of our broadly organized pipeline.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Data-fying</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transforming</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Visualizing</cell><cell>Interacting</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Capture aspects of the world as data</cell><cell cols="5">Modify or wrangle data into a vis-ready format</cell><cell cols="6">Create an image (such as a chart) from the data</cell><cell>The reader uses the chart to modify the display</cell></row><row><cell cols="2">Example: Auto-select a subset of the data</cell><cell>vis design app. domain</cell><cell>help Ease hard mental work of design data form Capture previously difficult to quantify data, e.g. emotion</cell><cell>harm Select Incorrect data Reinforce user biases Confuse Visualization designers with aggregation</cell><cell cols="2">Example: Data Cleaning</cell><cell>vis design app. domain</cell><cell>help Automate tedous or repetitive cleaning steps "Spell check" the transformation process</cell><cell>harm Out-of-domain data may be cleaned improperly Invite unearned trust Frictionlessness invites inattention</cell><cell cols="2">Example: Auto-create a Chart</cell><cell>vis design app. domain</cell><cell>help Shake up settled beliefs Novel color mapping Speed up process Find helpful information Stylization Chart Analysis</cell><cell cols="2">harm Wrong encoding Non-optimal perceptual design Rip off exisiting chart style No sociotechnical contextualization Inappropriate Style</cell><cell>Example: Personalization</cell><cell>vis design app. domain</cell><cell>help Update views in real-time based on IoT devices like fitness trackers Fight user biases Adapt to users needs (such as color blindness) or preferences</cell><cell>harm Lose sense of overview in data Increase presence of models biases Reduce user sense of control Irrelevant to domain desgin</cell></row><row><cell>Further</cell><cell cols="2">Examples:</cell><cell>Dataset curation Reverse engineer Creation or Synthesis data from image</cell><cell>Visualize model</cell><cell>Further</cell><cell cols="2">Examples:</cell><cell cols="2">Regression missing data Identify Clean data</cell><cell>Further</cell><cell cols="2">Examples:</cell><cell cols="2">Semantic chart recomendation (e.g. based on domain) Rapid prototyping Explore design space</cell><cell>Create embellishments</cell><cell>Further</cell><cell>Examples:</cell><cell>Personalization Guidance</cell><cell>Select new views (e.g. filter, highlight, transform) Summarization</cell></row><row><cell cols="10">Figure 5: ries about copyright that comes along with it. If I generate an icon, I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">don't know if there are any ramifications of me using it in my paper,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">and that might get me in trouble. So I would be uncomfortable. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">idea is good, but I would be uncomfortable utilizing it because I'd be</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">worried that I'm ripping somebody off.". Such fears are difficult to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">address in a text based medium, but it would be exceptionally quar-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">relsome to try to check if a particular image has been derived from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">another, even when using modern reverse image look up tools. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">recent tool Have I been trained? [Spa22] tries to address the issue by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">offering a lookup function into the LAION dataset, used to train Sta-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">ble Diffusion [RBL  *  22]. A review of the search "data visualization"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">on this database returns a number of graphics with identifiable author-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">ship, as well as graphics that violate common mores of visualization,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">e.g. 3D bar charts showing occlusion, distorted iconography, distract-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">ing gradients, and cluttered interfaces -all concerns listed in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>V.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Research Practice Artistic Technical</head><label></label><figDesc>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</figDesc><table><row><cell></cell><cell>ML</cell></row><row><cell>Art</cell><cell>researcher</cell></row><row><cell>Historian</cell><cell>at a</cell></row><row><cell></cell><cell>university</cell></row><row><cell></cell><cell>ML</cell></row><row><cell>Painter</cell><cell>engineer</cell></row><row><cell></cell><cell>at amazon</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Costly Far</head><label></label><figDesc>How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc><table><row><cell>"prettyfication of designs"</cell><cell>wildest dreams</cell><cell>delicious low hanging fruits</cell></row><row><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feasible</cell></row><row><cell>rapid iterative</cell><cell></cell><cell>Practical</cell></row><row><cell>prototyping</cell><cell></cell><cell>Soon</cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>moodboards for</cell><cell></cell><cell></cell></row><row><cell>creation</cell><cell></cell><cell></cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>dataset for</cell><cell></cell><cell></cell></row><row><cell>training model</cell><cell>useless trinket</cell><cell>inevitable corporate application</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>supports VIS VIS supports Gen Artistic/Subjective/ Creativity/Beauty Scientific/ Objective/ Utilitarian Stage 1a</head><label></label><figDesc>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>How would you position these ideas in relation to their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feasibility and your personal interest in them?</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>"tracking"-based (e.g. user behavior, biometrics, etc) adapting for subjective experience</cell><cell>generate variations on a design</cell><cell>attention/comprenehion -based (e.g. through eye-tracking) adapting for objective understanding</cell><cell>attention/comprehenesion -based (e.g. through eye-tracking) adapting for "semantic" tableau chart recommendation objective understanding</cell><cell>rapid (engineering) iterative prototyping</cell><cell>"tracking"-based (e.g. user behavior, biometrics, etc) adapting for subjective experience "prettyfication of designs" visualizing dataset for training model visualizing moodboards for design creation generate variations on a task rapid (artistic) exploring the iterative parameter space for design solutions to a prototyping</cell><cell>Data rights / copyright violation</cell><cell>Mercator projection everywhere ? Private data being replicated Vis design style being ripped off</cell><cell>doomsday prophecy</cell><cell>Most Concerning Private data being replicated</cell><cell>like watching a train wreck Amplifying biases (Mercator projection everywhere ?) Trusting the accuracy of the results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Replacing artists / stealing</cell><cell>Replacing visualization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generate parts of</cell><cell>jobs from humans</cell><cell>designers</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization that don't really require data</cell><cell></cell><cell>Unlikely</cell><cell>Vis design style being ripped off</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>input (e.g. icons)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trusting the accuracy of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Replacing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(visualization)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>designers</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>generate</cell><cell>exploring the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>variations on a design</cell><cell>parameter space for design solutions to a task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>not worth a</cell><cell>just the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>thought</cell><cell>way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>attention/comprenehio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Least Concerning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>n-based (e.g. through</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>eye-tracking) adapting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Self-Identification</cell><cell></cell><cell cols="2">AI used in a VIS context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">What would it look like to apply these AI motifs to the</cell><cell>Brainstorm some ideas that could fit on these axes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization design process?</cell><cell>(either generative processes that support visualization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>design, or visualizations that support generative work)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>AI Motifs and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Generative Technologies</cell><cell>Vis Applications</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Art Historian</cell><cell>Research</cell><cell>ML university researcher at a</cell><cell>Style Transfer</cell><cell>Making a chart look hand-drawn Interpolating missing data</cell><cell>Gen Generate parts of visualization that don't really require data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Inpainting/Outpainting</cell><cell></cell><cell>input (e.g. icons)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Extrapolating future data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Artistic</cell><cell></cell><cell>Technical</cell><cell>3D Model from 2D Image</cell><cell>Creating immersive analytics for XR on-the-fly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Generative music</cell><cell>Making sonification Making sonification sound actually sound actually nice/pleasant nice/pleasant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ML</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Painter</cell><cell></cell><cell>engineer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Practice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Far</head><label></label><figDesc>How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc><table><row><cell>"prettyfication of designs"</cell><cell>wildest dreams</cell><cell>delicious low hanging fruits</cell></row><row><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feasible</cell></row><row><cell>rapid iterative</cell><cell></cell><cell>Practical</cell></row><row><cell>prototyping</cell><cell></cell><cell>Soon</cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>moodboards for</cell><cell></cell><cell></cell></row><row><cell>creation</cell><cell></cell><cell></cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>dataset for</cell><cell></cell><cell></cell></row><row><cell>training model</cell><cell>useless trinket</cell><cell>inevitable corporate application</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Costly Far</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>How would you position these ideas in relation to their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feasibility and your personal interest in them?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>help catch</cell><cell>Mercator projection everywhere ?</cell><cell>Most Concerning</cell><cell>Trusting the accuracy of the results</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"prettyfication of designs"</cell><cell>visualizing moodboards for creation mistakes in rapid coding (not explainable AI -understanding where the model is pulling information from (some risks are associated) iterative prototyping -for design generative?)</cell><cell>Generate parts of visualization that don't really require data input (e.g. icons) visualizing dataset for</cell><cell>Data rights / copyright violation</cell><cell>Private data being replicated Vis design style being ripped off</cell><cell>doomsday prophecy</cell><cell>amplifying biases (Mercator projection everywhere ?) Vis design style being ripped off</cell><cell>like watching a train wreck Private data being replicated</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>training model</cell><cell>example: characters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>I think just the</cell><cell>thta are color</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">rapid iterative tableau chart prototyping -for deciding vis recommendation type</cell><cell>Replacing artists / stealing jobs from humans</cell><cell>Replacing visualization designers</cell><cell>amount of output is going to change!</cell><cell>variation of the same model could now be full models instead two variables:</cell><cell>Unlikely</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trusting the accuracy of the results</cell><cell>amount of work and expectation -we don't need</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>people writing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tedious code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>rapid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>explainable AI -understanding where the model is pulling information from (some risks are associated)</cell><cell>generating visualizations</cell><cell>iterative prototyping -for coding</cell><cell>it's going to move the goalpost and not replace jobs</cell><cell>not worth a Replacing visualization designers</cell><cell>just the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>thought</cell><cell>way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Least Concerning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 1a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AI Motifs and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generative Technologies</cell><cell>Vis Applications</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ML</cell><cell>Style Transfer</cell><cell>Making a chart look hand-drawn</cell><cell>"prettyfication of designs"</cell><cell>Gen supports VIS</cell><cell></cell><cell>wildest dreams</cell><cell></cell><cell>delicious low hanging fruits</cell></row><row><cell>Art Historian</cell><cell>researcher at a</cell><cell></cell><cell>Interpolating missing data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>university</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Inpainting/Outpainting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extrapolating future data</cell><cell></cell><cell></cell><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3D Model from 2D Image</cell><cell>Creating immersive analytics for XR on-the-fly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generative music</cell><cell>nice/pleasant nice/pleasant sound actually sound actually Making sonification Making sonification</cell><cell>Artistic/Subjective/ Creativity/Beauty</cell><cell>rapid iterative prototyping</cell><cell>Scientific/ Objective/ Utilitarian</cell><cell></cell><cell></cell><cell>Feasible Practical Soon</cell></row><row><cell>Painter</cell><cell>ML engineer at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell><cell>visualizing moodboards for creation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset for</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VIS supports Gen</cell><cell>training model</cell><cell>useless trinket</cell><cell></cell><cell>inevitable corporate application</cell></row></table><note><p><p><p><p><p><p><p>Generate parts of visualization that don't really require data input (e.g. icons)</p>AI used in a VIS context</p>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process?</p>Self-Identification</p>Research Practice Artistic Technical</p>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</p>How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Most Concerning Least Concerning Unlikely</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>doomsday</cell><cell>like watching</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>prophecy</cell><cell>amplifying biases</cell><cell>a train wreck</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Private data</cell><cell>(Mercator projection everywhere ?)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>being replicated</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trusting the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>"prettyfication of designs"</cell><cell>moodboards for visualizing</cell><cell>accuracy of the results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>creation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generate parts of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization that don't really require data input (e.g. icons)</cell><cell>Vis design style being ripped off</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Replacing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>designers</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rapid iterative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>prototyping</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tableau chart recommendation</cell><cell>visualizing dataset for training model</cell><cell>not worth a thought</cell><cell>just the way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 1a</cell></row><row><cell></cell><cell></cell><cell>AI Motifs and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generative Technologies</cell><cell>Vis Applications</cell></row><row><cell></cell><cell></cell><cell>Style Transfer</cell><cell>Making a chart look hand-drawn</cell><cell>Gen supports VIS</cell></row><row><cell></cell><cell>ML</cell><cell></cell><cell></cell></row><row><cell>Art Historian</cell><cell>researcher at a</cell><cell></cell><cell>Interpolating missing data</cell></row><row><cell></cell><cell>university</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Inpainting/Outpainting</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extrapolating future data</cell></row><row><cell></cell><cell></cell><cell>3D Model from 2D Image</cell><cell>Creating immersive analytics for XR on-the-fly</cell></row><row><cell></cell><cell></cell><cell>Generative music</cell><cell>Making sonification Making sonification sound actually sound actually nice/pleasant nice/pleasant</cell><cell>Artistic/Subjective/ Creativity/Beauty</cell><cell>Scientific/ Objective/ Utilitarian</cell></row><row><cell></cell><cell>ML</cell><cell></cell><cell></cell></row><row><cell>Painter</cell><cell>engineer</cell><cell></cell><cell></cell></row><row><cell></cell><cell>at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VIS supports Gen</cell></row></table><note><p><p><p><p><p><p><p>How would you position these ideas in relation to their feasibility and your personal interest in them?</p>Generate parts of visualization that don't really require data input (e.g. icons)</p>AI used in a VIS context</p>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process?</p>Self-Identification</p>Research Practice Artistic Technical</p>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Costly Far</head><label></label><figDesc></figDesc><table><row><cell>"prettyfication of designs"</cell><cell>wildest dreams</cell><cell>delicious low hanging fruits</cell></row><row><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feasible</cell></row><row><cell>rapid iterative</cell><cell></cell><cell>Practical</cell></row><row><cell>prototyping</cell><cell></cell><cell>Soon</cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>moodboards for</cell><cell></cell><cell></cell></row><row><cell>creation</cell><cell></cell><cell></cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>dataset for</cell><cell></cell><cell></cell></row><row><cell>training model</cell><cell>useless trinket</cell><cell>inevitable corporate application</cell></row></table><note><p>How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Most Concerning Least Concerning Unlikely</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>deepfakes</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>"recommender"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"people cannot read data"</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>deepfakes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>prettification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of designs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>make</cell><cell></cell><cell>search for novel</cell><cell>figure out how to visualize</cell><cell></cell><cell>doomsday prophecy</cell><cell></cell><cell>amplifying biases like watching a train wreck (Mercator projection everywhere ?)</cell><cell></cell></row><row><cell></cell><cell>chart</cell><cell></cell><cell>visualization types</cell><cell>multidimensional data that I don't</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>junk</cell><cell>search for novel visualization types</cell><cell>figure out how to visualize multidimensional know how to data that I don't</cell><cell>know how to represent chart make</cell><cell>rapid iterative visualizing moodboards for creation prototyping</cell><cell>Generate parts of visualization that don't really require data input (e.g. icons) "prettyfication of designs"</cell><cell>Private data being replicated</cell><cell>Trusting the accuracy of the results</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell>represent</cell><cell>junk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualizing</cell><cell></cell><cell>Replacing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset for training model</cell><cell>tableau chart</cell><cell>visualization designers</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>recommendation</cell><cell></cell><cell>Vis design style</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>being ripped off</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>not worth a</cell><cell></cell><cell>just the</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>thought</cell><cell></cell><cell>way it goes</cell><cell></cell></row></table><note><p>How would you position these ideas in relation to their feasibility and your personal interest in them?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Gen supports VIS VIS supports Gen Artistic/Subjective/ Creativity/Beauty Scientific/ Objective/ Utilitarian Stage 1a</head><label></label><figDesc></figDesc><table><row><cell>Generate parts of</cell></row><row><cell>visualization that don't</cell></row><row><cell>really require data</cell></row><row><cell>input (e.g. icons)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Gen supports VIS VIS supports Gen Artistic/Subjective/ Creativity/Beauty Scientific/ Objective/ Utilitarian Stage 1a</head><label></label><figDesc>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process?Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</figDesc><table><row><cell>Self-Identification</cell><cell></cell><cell cols="2">AI used in a VIS context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AI Motifs and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generative Technologies</cell><cell>Vis Applications</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Research</cell><cell></cell><cell>Style Transfer</cell><cell>Making a chart look hand-drawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Art Historian</cell><cell>ML university researcher at a</cell><cell></cell><cell>Interpolating missing data</cell><cell>Generate parts of visualization that don't really require data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Inpainting/Outpainting</cell><cell></cell><cell>input (e.g. icons)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extrapolating future data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Artistic</cell><cell>Technical</cell><cell>3D Model from 2D Image</cell><cell>Creating immersive analytics for XR on-the-fly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generative music</cell><cell>Making sonification Making sonification sound actually sound actually nice/pleasant nice/pleasant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ML</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Painter</cell><cell>engineer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Practice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>How would you position these ideas in relation to their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feasibility and your personal interest in them?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mercator projection everywhere ?</cell><cell></cell><cell>Most Concerning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vis-to-text (generating explanation from visualization)</cell><cell>"scribbles to art"</cell><cell>"prettyfication of designs"</cell><cell>Data rights / copyright</cell><cell>Private data being</cell><cell>doomsday prophecy</cell><cell>like watching a train wreck</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualizing moodboards for</cell><cell>violation</cell><cell>replicated</cell><cell></cell><cell>Private data being replicated</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>rapid iterative prototyping</cell><cell>generate a first visualization idea</cell><cell>creation</cell><cell></cell><cell>Vis design style being ripped off</cell><cell></cell><cell>how the results are really accurate</cell><cell>Vis design style</cell><cell>amplifying biases (Mercator projection everywhere ?)</cell></row><row><cell>21</cell><cell></cell><cell></cell><cell></cell><cell>generate a first visualization idea</cell><cell>vis-to-text (generating explanation from visualization)</cell><cell>visualizing dataset for training model</cell><cell></cell><cell>Generate parts of visualization that don't really require data input (e.g. icons)</cell><cell>Replacing artists / stealing jobs from humans</cell><cell>Replacing visualization designers Trusting the accuracy of the results</cell><cell>Unlikely</cell><cell>Replacing visualization designers</cell><cell>being ripped off</cell><cell>active misinformation (malicious usage)</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>people trusting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tableau chart recommendation</cell><cell></cell><cell></cell><cell></cell><cell>not worth a thought</cell><cell>the accuracy of the results</cell><cell>just the way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Least Concerning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Costly Far</head><label></label><figDesc>How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.</figDesc><table><row><cell>"prettyfication of designs"</cell><cell>wildest dreams</cell><cell>delicious low hanging fruits</cell></row><row><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feasible</cell></row><row><cell>rapid iterative</cell><cell></cell><cell>Practical</cell></row><row><cell>prototyping</cell><cell></cell><cell>Soon</cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>moodboards for</cell><cell></cell><cell></cell></row><row><cell>creation</cell><cell></cell><cell></cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>dataset for</cell><cell></cell><cell></cell></row><row><cell>training model</cell><cell>useless trinket</cell><cell>inevitable corporate application</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Gen supports VIS VIS supports Gen Artistic/Subjective/ Creativity/Beauty Scientific/ Objective/ Utilitarian Stage 1a</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2b</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">How would you position these ideas in relation to their</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">feasibility and your personal interest in them?</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generate parts of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization that don't</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>really require data input (e.g. icons)</cell><cell></cell><cell></cell><cell>Mercator projection everywhere ?</cell><cell></cell><cell>Most Concerning</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>rapid iterative prototyping</cell><cell>information design</cell><cell></cell><cell>Data rights / copyright</cell><cell>Private data being</cell><cell>doomsday prophecy</cell><cell></cell><cell>misleading information gain</cell><cell>amplifying biases (Mercator projection everywhere ?)</cell><cell>like watching a train wreck</cell></row><row><cell>17</cell><cell>engaging novice users entertaining</cell><cell>onboarding to visualizations information design</cell><cell>tableau chart recommendation onboarding to visualizations</cell><cell>engaging novice users entertaining</cell><cell>visualizing moodboards for creation</cell><cell>violation Replacing artists / stealing jobs from humans</cell><cell>replicated Vis design style being ripped off Replacing visualization designers</cell><cell>Replacing visualization designers</cell><cell>Trusting the accuracy of the results</cell><cell>irresponsible use of black boxes</cell><cell>Private data being replicated Vis design style being ripped off focus too much on the aesthetics</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unlikely</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell>"prettyfication of designs"</cell><cell></cell><cell></cell><cell></cell><cell>Trusting the accuracy of the results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>visualizing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>dataset for training model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>not worth a thought</cell><cell></cell><cell></cell><cell></cell><cell>just the way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Least Concerning</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Generate parts of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>visualization that don't</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>really require data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>input (e.g. icons)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>AI used in a VIS context</head><label></label><figDesc>Brainstorm some ideas that could fit on these axes (either generative processes that support visualization design, or visualizations that support generative work) What would it look like to apply these AI motifs to the visualization design process?</figDesc><table><row><cell>Self-Identification</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AI Motifs and</cell></row><row><cell></cell><cell></cell><cell>Generative Technologies</cell><cell>Vis Applications</cell></row><row><cell></cell><cell></cell><cell>Style Transfer</cell><cell>Making a chart look hand-drawn</cell></row><row><cell></cell><cell>ML</cell><cell></cell></row><row><cell>Art Historian</cell><cell>researcher at a</cell><cell></cell><cell>Interpolating missing data</cell></row><row><cell></cell><cell>university</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Inpainting/Outpainting</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extrapolating future data</cell></row><row><cell></cell><cell></cell><cell>3D Model from 2D Image</cell><cell>Creating immersive analytics for XR on-the-fly</cell></row><row><cell></cell><cell></cell><cell>Generative music</cell><cell>Making sonification Making sonification sound actually sound actually nice/pleasant nice/pleasant</cell></row><row><cell></cell><cell>ML</cell><cell></cell></row><row><cell>Painter</cell><cell>engineer</cell><cell></cell></row><row><cell></cell><cell>at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell></row></table><note><p><p><p><p>Research</p>Practice</p>Artistic Technical</p>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Stage 1b Excited Interested Not excited Not Interested Unfeasible Costly Far</head><label></label><figDesc>How would you position these ideas in relation their feasibility and your personal interest them? Additionally, chose one that you find particularly risky, dangerous, problematic and mark with the red spot.Here how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</figDesc><table><row><cell></cell><cell></cell><cell>Stage 2a</cell></row><row><cell></cell><cell></cell><cell>Replicating or amplifying</cell></row><row><cell></cell><cell></cell><cell>biases that already exist</cell></row><row><cell>"prettyfication of designs"</cell><cell>dreams</cell><cell>delicious low hanging fruits</cell></row><row><cell>tableau chart</cell><cell></cell><cell></cell></row><row><cell>recommendation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feasible</cell></row><row><cell>rapid iterative</cell><cell></cell><cell>Practical</cell></row><row><cell>prototyping</cell><cell></cell><cell>Soon</cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>moodboards for</cell><cell></cell><cell></cell></row><row><cell>creation</cell><cell></cell><cell></cell></row><row><cell>visualizing</cell><cell></cell><cell></cell></row><row><cell>dataset</cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>useless trinket</cell><cell>inevitable corporate application</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>AI Worries Worries applying AI to VIS</head><label></label><figDesc>Professional identity is a high-dimensional form of data, and yet we are asking you to place yourself on just two axes... Please break/annotate/augment as you see fit (e.g. stretching a bubble across the whole length of one axis if both ends apply)How would you position these ideas in relation to their feasibility and your personal interest in them? Additionally, chose one that you find particularly risky, dangerous, our problematic and mark with the red spot.Here is how some classic AI worries could translate to VIS. Can you think of other worries or risks coming from AI or generative technologies when applied to VIS?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Stage 1a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gen supports VIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Generate parts of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">visualization that don't</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">really require data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">input (e.g. icons)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Artistic/Subjective/ Creativity/Beauty</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scientific/ Objective/ Utilitarian</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VIS supports Gen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Self-Identification</cell><cell></cell><cell cols="2">AI used in a VIS context</cell><cell></cell><cell cols="2">Stage 1a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Stage 1b</cell><cell>Stage 2a</cell><cell>Stage 2b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">What would it look like to apply these AI motifs to the</cell><cell cols="4">Brainstorm some ideas that could fit on these axes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>How would you position these ideas in relation to their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>visualization design process?</cell><cell cols="4">(either generative processes that support visualization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feasibility and your personal interest in them?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">design, or visualizations that support generative work)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AI Motifs and Generative Technologies</cell><cell>Vis Applications</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AI Worries</cell><cell>Worries applying AI to VIS</cell></row><row><cell></cell><cell>Historian Art</cell><cell>Research</cell><cell>researcher university at a ML</cell><cell>Style Transfer Inpainting/Outpainting</cell><cell>Making a chart look hand-drawn Interpolating missing data</cell><cell>"prettyfication of designs"</cell><cell cols="2">Gen supports VIS Generate parts of visualization that don't really require data input (e.g. icons)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>wildest dreams</cell><cell></cell><cell></cell><cell>Excited Interested</cell><cell>delicious low hanging fruits</cell><cell>violation Data rights / copyright Replicating or amplifying biases that already exist</cell><cell>replicated Private data being Mercator projection everywhere ?</cell><cell>doomsday prophecy</cell><cell>Most Concerning</cell><cell>like watching a train wreck</cell></row><row><cell>16</cell><cell>Artistic</cell><cell></cell><cell>Technical</cell><cell>3D Model from 2D Image Generative music</cell><cell>Creating immersive analytics for XR on-the-fly nice/pleasant nice/pleasant sound actually sound actually Making sonification Extrapolating future data Making sonification</cell><cell>Artistic/Subjective/ Creativity/Beauty</cell><cell cols="2">rapid iterative prototyping mind-mapping and ideation</cell><cell></cell><cell></cell><cell>tableau chart recommendation</cell><cell>Scientific/ Objective/ Utilitarian</cell><cell></cell><cell>Unfeasible Costly Far</cell><cell></cell><cell></cell><cell>Feasible Practical Soon</cell><cell>Replacing artists / stealing jobs from humans</cell><cell>the results Trusting the accuracy of Vis design style being ripped off Replacing visualization designers</cell><cell>Unlikely</cell><cell>Trusting the amplifying biases (Mercator projection everywhere ?) Private data being replicated</cell><cell>Inevitable</cell><cell>bias is a human problem</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>accuracy of the</cell></row><row><cell></cell><cell>Painter</cell><cell>Practice</cell><cell>ML engineer at amazon</cell><cell>Clustering/embedding</cell><cell>Grouping similar charts</cell><cell>visualizing moodboards for creation</cell><cell></cell><cell></cell><cell cols="2">visualizing</cell><cell></cell><cell>dada = pivot</cell><cell>disruptiveness, queerness, transgression, revolutionary</cell><cell>francis picabia</cell><cell></cell><cell></cell><cell>results Replacing visualization designers</cell><cell>Vis design style being ripped off</cell><cell>there's an opportunity that creativity as a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">dataset for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>notion is challenged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VIS supports Gen</cell><cell cols="2">training model</cell><cell></cell><cell></cell><cell>metaphysics transdiscipl</cell><cell>useless trinket</cell><cell></cell><cell></cell><cell>Not excited Not Interested</cell><cell>inevitable corporate application</cell><cell>not worth a thought</cell><cell>Least Concerning</cell><cell>just the way it goes</cell><cell>Andreas Reckwitz,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>inarity</cell><cell></cell><cell></cell><cell></cell><cell>Kreativit?tsdispos</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>exclusivity,</cell><cell></cell><cell></cell><cell></cell><cell>itv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>original</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"prettyfication of designs"</cell><cell>Generate parts of visualization that don't really require data input (e.g. icons)</cell><cell cols="2">tableau chart recommendation</cell><cell>rapid iterative prototyping</cell><cell>visualizing moodboards for creation</cell><cell>visualizing dataset for training model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>generative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>instead of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>creative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage 2b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>related: Setlur</cell><cell></cell><cell>related:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and Mackinlay</cell><cell></cell><cell>MetaGlyph (VIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(CHI 2014)</cell><cell></cell><cell>2022)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>generating "marks"</cell><cell>related: Supporting Expressive and Faithful</cell><cell></cell><cell>"design</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mercator projection everywhere ?</cell><cell>Most Concerning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>color / shape palettes reflecting intended affect</cell><cell>that have semantic connect with the data</cell><cell cols="2">Pictorial Visualization Design with Visual Style Transfer (VIS 2022)</cell><cell>guideline" style-transfer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">anthropographics meaningfulness</cell><cell>generating "marks" that have semantic connect with the data</cell><cell>design provenance</cell><cell>Data rights / copyright violation</cell><cell>Private data being replicated</cell><cell>doomsday prophecy</cell><cell>like watching a train wreck</cell></row><row><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Related: Liu et al's OPAL (UIST 2022) for generating news illustrations generative image + vis Related: Coelho and Mueller's Infomages juxtapositions that convey a narrative / reflect an intended affect vis as image prior? see Meta's Make-a-Scene (ECCV 2022)</cell><cell>anthropographics meaningfulness</cell><cell>"winnow" DIATOMS related: anthropographics in medical risk communication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">training your own dataset</cell><cell>Generate parts of visualization that don't really require data input (e.g. icons) vis generation from caption UX for latent color / shape palettes reflecting intended affect space exploration generative image + vis rapid iterative prototyping juxtapositions that convey a narrative / Scene (ECCV 2022) reflect an intended affect vis as image prior? see Meta's Make-a-</cell><cell>Replacing artists / stealing jobs from humans</cell><cell>Vis design style being ripped off Replacing visualization designers</cell><cell>amplifying biases (Mercator projection everywhere ?)</cell><cell>Private data being replicated</cell><cell>Inevitable</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paper</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"prettyfication</cell><cell>visualizing</cell><cell>Trusting the accuracy of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">caption generation from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">hypothetical data set representation</cell><cell>of designs"</cell><cell>moodboards for creation</cell><cell>the results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>exploration space UX for latent</cell><cell>Cartography Latent Space</cell><cell>vis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sketching</cell><cell>dataset for training model visualizing</cell><cell>transfer guideline" style-"design</cell><cell>Replacing visualization designers</cell><cell>Vis design style</cell><cell>Trusting the accuracy of the results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>being ripped off</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>not worth a</cell><cell>just the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">training your</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>thought</cell><cell>way it goes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">own dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Least Concerning</cell></row></table><note><p><p>Unlikely</p>How would you position these ideas in relation to their feasibility and your personal interest in them?</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>? 2023 The Author(s) Computer Graphics Forum ? 2023 The Eurographics Association and John Wiley &amp; Sons Ltd.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeltracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Human Factors in Computing Systems (CHI 2015)</title>
		<meeting>of the Conf. on Human Factors in Computing Systems (CHI 2015)</meeting>
		<imprint>
			<publisher>ACM -Association for Computing Machinery</publisher>
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://aws.amazon.com/codewhisperer/features/,2022.Accessed8/16/22" />
		<title level="m">AMAZON: Amazon codewhisperer features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Swot analysis applications: An integrative literature review</title>
		<author>
			<persName><forename type="first">]</forename><surname>Bem * 21</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Benzaghta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elwalda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Business Insights</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><surname>Mcmillan-Major A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Conf. on Fairness, Accountability, and Transparency</title>
		<meeting>of the ACM Conf. on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Climbing towards nlu: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donovan</forename><forename type="middle">P</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno>CoRR abs/1708.02660</idno>
		<ptr target="http://arxiv.org/abs/17" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 58th annual meeting of the association for computational linguistics</title>
		<meeting>of the 58th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2020. 2017</date>
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
	<note>Learning visual importance for graphic designs and data visualizations</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recommendations for visualization recommendations: Exploring preferences and priorities in public health</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02660.2[BLF*22</idno>
	</analytic>
	<monogr>
		<title level="m">CHI Conf. on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of faces to represent points in kdimensional space graphically</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2284077.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A taxonomy of visualization techniques using the data state reference model</title>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">E H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization 2000</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="75" />
		</imprint>
	</monogr>
	<note>INFOVIS 2000. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infomages: Embedding data into thematic images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="593" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Compton</surname></persName>
		</author>
		<ptr target="https://web.archive.org/web/20221124001133/https://twitter.com/GalaxyKate/status/1583907942834716672.1" />
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
			<publisher>Tweet</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Virtual music: computer synthesis of musical style</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The computational case against computational literary studies</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical inquiry</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="601" to="639" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data2vis: Automatic generation of data visualizations using sequence to sequence recurrent neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dibia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName><surname>Kehlbeck R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><surname>Sevastjanova R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sperrle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1804.03126</idno>
		<ptr target="https://arxiv.org/abs/1804.03126" />
	</analytic>
	<monogr>
		<title level="m">Semantic Color Mapping: A Pipeline for Assigning Meaningful Colors to Text. 4th IEEE Workshop on Visualization Guidelines in Research, Design, and Education</title>
		<imprint>
			<date type="published" when="2018">2018. 2022</date>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The medium is the massage</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcluhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Random House</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Twenty years of creativity research in human-computer interaction: Current state and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frich</surname></persName>
		</author>
		<author>
			<persName><surname>Mose Biskjaer M</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dalsgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2018 Designing Interactive Systems Conf</title>
		<meeting>of the 2018 Designing Interactive Systems Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1235" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The kdd process for extracting useful knowledge from volumes of data</title>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">J</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<ptr target="https://nightingaledvs.com/i-asked-an-artificial-intelligence-to-draw" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1996">June 2022. 1996</date>
			<pubPlace>Nightingale</pubPlace>
		</imprint>
	</monogr>
	<note>I Asked an Artificial Intelligence to Draw a Chart</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vice: visual counterfactual explanations for machine learning models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3377325.3377536</idno>
		<idno>doi:10.1145/3377325.337753</idno>
		<ptr target="https://doi.org/10.1145/3377325.3377536" />
	</analytic>
	<monogr>
		<title level="m">IUI &apos;20: 25th International Conference on Intelligent User Interfaces</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Spano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Tintarev</surname></persName>
		</editor>
		<meeting><address><addrLine>Cagliari, Italy; Patern? F., Oliver N., Conati C</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">March 17-20, 2020 (2020</date>
			<biblScope unit="page" from="531" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<idno>11/28/22</idno>
		<ptr target="https://github.com/features/copilot" />
		<title level="m">GITHUB: Github copilot</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209456</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9903602/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>GSS * 22</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why Meta&apos;s latest large language model survived only three days online</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Heaven</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/.9" />
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Agency plus automation: Designing artificial intelligence into interactive systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>1844-1850. 7</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences</title>
		<meeting>of the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">This artist is dominating ai-generated art. and he&apos;s not happy about it</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkil?</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/.3" />
		<imprint>
			<date type="published" when="2022-09">Sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Henry</surname></persName>
		</author>
		<idno>12/1/22. 8</idno>
		<ptr target="https://vis.social/about" />
		<title level="m">vis.social server rules</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence At Scale</title>
		<author>
			<persName><forename type="middle">N</forename><surname>Hhs ; Hoque M</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209466</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9904017/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Dachselt R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209390</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9903341/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934659</idno>
		<ptr target="https://fredhohman.com/summit/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The data linter: Lightweight, automated sanity checking for ml data sets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS MLSys Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-Supervised Color-Concept Association via Image Colorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><surname>Hu R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">V</forename><surname>Kaick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanthara</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">T K</forename><surname>Leong R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209481</idno>
		<idno type="arXiv">arXiv:2203.06486</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9904484/" />
	</analytic>
	<monogr>
		<title level="m">Chart-to-text: A large-scale benchmark for chart summarization</title>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2022</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE Transactions on Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep generative model for graph layout</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934396</idno>
		<idno>doi:10.110</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="665" to="675" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagesense: An intelligent collaborative ideation tool to support diverse human-computer partnerships</title>
		<author>
			<persName><forename type="first">/</forename><surname>Tvcg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934396</idno>
		<idno>2934396. 8 [KTBL * 20</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. of the ACM on human-computer interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note>CSCW</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Alphaclean: Automatic generation of data cleaning pipelines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11827</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee S.-I. ; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf.2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Divining insights: Visual analytics through cartomancy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conf. on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visualization for villainy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koenig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>alt.vis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Showing data about people: A design space of anthropographics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Surfacing visualization mirages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2020 CHI Conf. on human factors in computing systems</title>
		<meeting>of the 2020 CHI Conf. on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Motion</surname></persName>
		</author>
		<idno>17/11/22</idno>
		<ptr target="https://www.midjourney.com/.1,2" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Midjourney</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rulematrix: Visualizing and understanding classifiers with rules</title>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864812</idno>
		<idno>doi:10.1109/TVCG.2018.2864812. 2</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2018.2864812" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Follow the clicks: Learning and anticipating mouse interactions during exploratory data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett R</surname></persName>
		</author>
		<author>
			<persName><surname>Wan R</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13670</idno>
		<ptr target="https://doi.org/10.1111/cgf.13670" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reducing bias and improving safety in dall?e 2</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/.2" />
		<imprint>
			<date type="published" when="2022-09">2022. 9/5/22</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reverse-engineering visualizations: Recovering visual encodings from chart images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Plunkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Rombach R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Om-Mer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><forename type="middle">A</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename><forename type="middle">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.06125</idno>
		<idno>doi:10.485 50/ARXIV.2204.06125. 2</idno>
		<ptr target="https://arxiv.org/abs/2204.06125" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-06">August 2022. 2017. June 2022. 2022</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
	<note>Hierarchical text-conditional image generation with clip latents</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An A.I.-Generated Picture Won an Art Prize. Artists Aren&apos;t Happy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Roose</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html.1" />
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward a taxonomy of modeling difficulties: A multi-modal study on individual modeling processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strecker</surname></persName>
		</author>
		<ptr target="https://aisel.aisnet.org/icis2019/learning_environ/learning_environ/12.4" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th International Conf. on Information Systems, ICIS 2019</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Krcmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Fedorowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Boh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Leimeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wattal</surname></persName>
		</editor>
		<meeting>of the 40th International Conf. on Information Systems, ICIS 2019<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">December 15-18, 2019 (2019</date>
		</imprint>
	</monogr>
	<note>Association for Information Systems</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><forename type="middle">W</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Den-Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S G</forename><surname>Lopes R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.11487</idno>
		<ptr target="https://arxiv.org/abs/2205.11487" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>SCS * 22</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Beware the rationalization trap! when language model explainability diverges from our mental models of language</title>
		<author>
			<persName><surname>Sevastjanova R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06897</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">I learn to diffuse, or data alchemy 101: a mnemonic manifesto</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sfpm * 22 ; Schetinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Filipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>P?rez-Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">O</forename><surname>De Oliveira R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>alt.vis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">What is it like to program with artificial intelligence?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sgn * 22] Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Negreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poelitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ragavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Programming Interest Group</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022">2022</date>
			<publisher>PPIG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supporting Expressive and Faithful Pictorial Visualization Design With Visual Style Transfer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
			<affiliation>
				<orgName type="collaboration">SLC * 22</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">SLC * 22</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">SLC * 22</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">SLC * 22</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
			<affiliation>
				<orgName type="collaboration">SLC * 22</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3209486</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9903511/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Articulate: A semiautomated model for translating natural language queries into meaningful visualizations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Graphics</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kr?ger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Olivier</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automatic generation of semantic icon encodings for visualizations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGCHI Conf. on Human Factors in Computing Systems</title>
		<meeting>of the SIGCHI Conf. on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data as a creative constraint</title>
		<author>
			<persName><forename type="first">E</forename><surname>Socolofsky</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=fIy7rkYyyhI.8" />
	</analytic>
	<monogr>
		<title level="j">OpenVisConf</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Have i been trained?</title>
		<author>
			<persName><surname>Spawning</surname></persName>
		</author>
		<idno>12/1/22</idno>
		<ptr target="https://haveibeentrained.com/.5" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Embedding projector: Interactive visualization and interpretation of embeddings</title>
		<author>
			<persName><forename type="first">;</forename><surname>Ssl * 22</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3148007</idno>
		<idno type="arXiv">arXiv:1611.05469.2</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2022. 2016</date>
		</imprint>
	</monogr>
	<note>Towards natural language interfaces for data visualization: A survey</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Get started with explain data</title>
		<idno>12/1/22</idno>
		<ptr target="https://help.tableau.com/current/pro/desktop/en-us/explain_data_basics.htm.7" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>TABLEAU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><surname>Tkc * 22] Taylor R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saravia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><surname>Stojnic R</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09085</idno>
		<title level="m">Galactica: A large language model for science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Schetinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Di Bartolomeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Adams</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data.1,4,8" />
		<title level="m">The scary truth about AI copyright is nobody knows what will happen next. Verge</title>
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey on ML4VIS: applying machine learning advances to data visualization</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3106142</idno>
		<idno>doi:10.1109/TVCG.202</idno>
		<ptr target="https" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="5134" to="5153" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TVCG.2021.3106142</idno>
		<idno>3106142. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A perception-driven approach to supervised dimensionality reduction for visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Fu C.-W</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedl-Mair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2701829</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1828" to="1840" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Dl4scivis: A state-of-the-art survey on deep learning for scientific visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.06504</idno>
		<ptr target="https://arxiv.org/abs/2204.06504" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Polite computing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Whitworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deepdrawing: A deep learning approach to graph drawing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934798</idno>
		<idno>doi:10.1109/TVCG.2019.29 34798. 2</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2019.2934798" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="676" to="686" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>WJW * 20</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Beyond the walled garden: A visual essay in five chapters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>alt.vis (2022). 2, 4, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conf. on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">AI4VIS: survey on artificial intelligence approaches for data visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3099002</idno>
		<idno>doi:10.1109/TVCG.2021.3099002</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2021.3099002" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>WWS * 22</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Image outpainting: Hallucinating beyond the image</title>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3024861</idno>
		<idno>doi:10.110</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3024861" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="173576" to="173583" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">/</forename><surname>Access</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3024861</idno>
		<idno>3024861. 3</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Metaglyph: Automatic generation of metaphoric glyph-based visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/6951.3[YSD*22" />
	</analytic>
	<monogr>
		<title level="m">AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020. 2022</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Learning to incorporate structure knowledge for image inpainting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
