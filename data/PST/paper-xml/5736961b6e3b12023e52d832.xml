<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">D</forename><surname>Shen</surname></persName>
							<email>dgshen@med.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain and Cognitive Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<postCode>02841</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">06FF7F859FC27509FB46162AA2BE0530</idno>
					<idno type="DOI">10.1109/TMI.2015.2508280</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods. Index Terms-MR prostate segmentation, stacked sparse auto-encoder (SSAE), sparse patch matching, deformable model I. INTRODUCTION rostate cancer is the second leading cause of cancer death in American men, behind only lung cancer [1]. As a main imaging modality for clinical inspection of prostate, Magnetic Resonance (MR) imaging provides better soft tissue contrast than ultrasound in a non-invasive way, and has the emerging role in prostate cancer diagnosis and treatment [2, 3]. The accurate localization of the prostate is an important step for Manuscript received May 4, 2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>assisting the diagnosis and treatment, such as guiding biopsy procedure <ref type="bibr" target="#b0">[2]</ref> and radiation therapy <ref type="bibr" target="#b1">[3]</ref>. However, the manual segmentation of the prostate is tedious and time-consuming, and also suffers from intra-and inter-observer variability. Therefore, developing automatic and reliable segmentation methods for MR prostate is clinically desirable and an important task.</p><p>However, accurate prostate localization in MR images is difficult due to the following two main challenges. First, the appearance patterns vary a lot around the prostate boundary across patients. As we can see from Fig. <ref type="figure" target="#fig_1">1 (a)</ref>, the image contrasts at different prostate regions, i.e., the anterior, central and posterior regions, change both across different subjects and within each subject. Fig. <ref type="figure" target="#fig_1">1 (b)</ref> gives the intensity distributions of prostate and background voxels around the prostate boundary, respectively. As shown in the figure, the intensity distributions highly vary across different patients and do not often follow the Gaussian distribution.</p><p>To evaluate the shape difference in our dataset, we adopt the PCA analysis by mapping each high-dimensional shape vector onto a space spanned by the first three principal components. Note that the shape vector is formed by the concatenation of all vertex coordinates, and then linearly aligned to the mean shape before PCA analysis. Fig <ref type="figure" target="#fig_2">2</ref> shows the distribution of 66 prostate shapes, which also indicates the inter-patient shape variation among the shape repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Recently, most studies in T2-weighted MR prostate segmentation focus on two types of methods: multi-atlas-based <ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref> and deformable-model-based <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9]</ref> segmentation methods. Multi-atlas-based methods are widely used in medical imaging <ref type="bibr" target="#b8">[10]</ref><ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref>. Most research focuses on the design of sophisticated atlas selection or label fusion method. Yan et al <ref type="bibr" target="#b3">[5]</ref> proposed a label image constrained atlas selection and label fusion method for prostate MR segmentation. During the atlas selection, label images are used to constrain the manifold projection of intensity images, which can relieve the misleading projection due to other anatomical structures. Ou et al <ref type="bibr" target="#b5">[7]</ref> proposed an iterative multi-atlas label fusion method by gradually improving the registration based on the prostate vicinity between the target and atlas images. For deformable-model-based methods, Toth <ref type="bibr" target="#b6">[8]</ref> proposed to incorporate different features in the context of AAMs (Active Appearance Models). Besides, with the adoption of the level set, the issue of landmark correspondence can be avoided.</p><p>But both types of these methods require careful feature  engineering to achieve good performance. The multi-atlas based methods require good features for identifying correspondences between a new testing image and each atlas image <ref type="bibr" target="#b11">[13]</ref>, while the deformable model relies on discriminative features for separating the target object (e.g., the prostate) from the background <ref type="bibr" target="#b12">[14]</ref>. Traditionally, intensity patch is often used as features for the above two methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>. However, due to the inhomogeneity of MR images, the simple intensity features often fail in segmentation of MR images with different contrasts and illuminations. To overcome this problem, recent MR prostate segmentation methods started to use features that are specifically designed for vision tasks, such as gradient <ref type="bibr" target="#b15">[17]</ref>, Haar-like wavelets <ref type="bibr" target="#b16">[18]</ref>, Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b17">[19]</ref>, SIFT <ref type="bibr" target="#b18">[20]</ref>, Local Binary Patterns (LBP) <ref type="bibr" target="#b19">[21]</ref>, and variance adaptive SIFT <ref type="bibr" target="#b12">[14]</ref>.</p><p>Compared to simple intensity features, these vision-based features show better invariance to illumination, and also provide some invariance to small rotation. In <ref type="bibr" target="#b20">[22]</ref>, authors showed that better prostate segmentations could be obtained by using the combination of these features. One major limitation of the aforementioned handcrafted features is incapable of adapting to data at hand. That means the representation power and effectiveness of these features could vary across different kinds of image data. To deal with this limitation, the learning based feature representation methods <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref> are developed to extract latent information, which can be adapted to the data at hand. As one important type of feature learning methods, deep learning recently becomes a hot topic in machine learning <ref type="bibr" target="#b21">[23]</ref>, computer vision <ref type="bibr" target="#b23">[25]</ref>, and many other research fields including medical image analysis <ref type="bibr" target="#b24">[26]</ref>. Compared with handcrafted features, which need expert knowledge for careful design and also lack sufficient generalization power to different domains, deep learning is able to automatically learn effective feature hierarchies from the data. Therefore, it draws an increasing interest in the research communities. For example, Vincent et al. <ref type="bibr" target="#b25">[27]</ref> showed that the features learned by deep belief network and the stacked denoising auto-encoder beat the state-of-the-art handcrafted features for the digit classification problem in the MINST dataset. Farabet et al. <ref type="bibr" target="#b26">[28]</ref> proposed to use convolutional network to produce feature representation, which is more powerful in the application of scene labeling than the engineered features, and also achieved the state-of-the-art performance. In the field of medical image analysis, Shin et al. <ref type="bibr" target="#b27">[29]</ref> applied the stacked auto-encoders to organ identification in MR images, which shows the potential of deep learning method for application to medical images. In summary, compared with handcrafted features, deep learning has the following advantages: (1) Instead of designing effective features for a new task by trial and error, deep learning largely saves researchers' time by automating this process. Also, it is capable to exploit the complex feature patterns, which the manual feature engineering is not good at. (2) Unlike the handcrafted features, which are usually shallow in representation due to the difficulty of designing abstract high-level features, deep learning is able to learn the feature hierarchy in a layer-by-layer manner, by first learning the low-level features and then recursively building more comprehensive high-level features based on the previously learned low-level features. (3) When unsupervised pre-training is combined with supervised fine-tuning, the deep-learned features can be optimized for a certain task, such as segmentation, thus boosting the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Contribution</head><p>Motivated by the above factors, we propose to learn the hierarchical feature representation from MR prostate images by deep feature learning. These learned features are further integrated in a sparse patch matching framework to find the corresponding patches in the atlas images for label propagation. Finally, a deformable model is adopted to segment the prostate by combining the shape prior with the prostate likelihood map derived from sparse patch matching. The main contribution of our method lies in threefold:</p><p>â€¢ Instead of using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE) <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31]</ref>, which includes an unsupervised pre-training step and also a task-related fine-tuning step.</p><p>â€¢ By using deep-learned features for measuring inter-patch  similarity, a sparse patch matching method is proposed for finding the corresponding patches in the atlas images and then transferring their prostate labels from atlases to the new prostate image.</p><p>â€¢ A deformable model is adopted to further enforce a sparse shape constraint during segmentation, which aims to cope with the large variation existing in prostate shape space.</p><p>The proposed method has been extensively evaluated on the T2-weighted MR prostate image dataset, which contains 66 3D images. The manual prostate segmentations are provided by a radiation oncologist for the evaluation purpose. Experimental results show that the sparse patch matching with deep-learned features achieve better segmentation accuracy than using the handcrafted features, as well as the simple intensity features. Besides, compared to other state-of-the-art prostate segmentation methods, our method obtains competitive segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Brief Outline of Our method</head><p>The proposed MR prostate segmentation framework is composed of two levels (Fig. <ref type="figure" target="#fig_3">3</ref>). The first level (two upper panels of Fig. <ref type="figure" target="#fig_3">3</ref>) learns the deep feature representation and then applies sparse patch matching with the deep-learned features for deriving the prostate likelihood map. Based on the produced likelihood map, the second level (lower panel of Fig. <ref type="figure" target="#fig_3">3</ref>) consists of a deformable model by enforcing the shape prior during the evolution of prostate segmentation.</p><p>The rest of the paper is organized as follows. In Section II, we present the stacked sparse auto-encoder for feature learning and the sparse patch matching framework for deriving the prostate likelihood map in the first level. Section III elaborates both the deformable model and the sparse shape model in the second level. Section IV evaluates the proposed segmentation method on the T2-weighted prostate MR dataset. Finally, conclusive remarks are presented in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FIRST LEVEL: LEARNING DEEP FEATURE REPRESENTATION</head><p>AND SPARSE PATCH MATCHING The goal of this level is to learn a latent feature representation for MR prostate images, and then use them to infer a likelihood map of prostate gland for a new image. To achieve this goal, two main stages (i.e., learning stage and testing stage) are conducted as illustrated in the two upper panels of Fig. <ref type="figure" target="#fig_3">3</ref>. First, in the learning stage, the intrinsic feature hierarchy from MR prostate image patches is learned by using a deep learning framework, namely the stacked sparse auto-encoder (SSAE). Then, in the testing stage, each image patch from both atlas and target images is first represented by the features learned from the SSAE network. Then, these features are integrated into a sparse patch matching method for estimating the prostate likelihood map by transferring the label information from atlas images to the target image.</p><p>The organization of this section is as follows. In Section II.A, we first investigate the limitation of handcrafted features in MR prostate segmentation, and give our motivation of adopting deep learning features. Afterwards, we introduce the feature learning method in Section II.B, and the sparse patch matching in Section II.C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Limitation of Handcrafted Features in MR Prostate Segmentation</head><p>Since our sparse patch matching method belongs to multi-atlas based segmentation methods, in the following, we will illustrate the importance of features in such context. As briefly mentioned in the Introduction, good features in multi-atlas based segmentation should identify the correct correspondences between the target image and the atlas images. In computer vision, various handcrafted features, such as Haar features <ref type="bibr" target="#b16">[18]</ref>, HOG features <ref type="bibr" target="#b18">[20]</ref> and Local Binary Patterns <ref type="bibr" target="#b19">[21]</ref>, have been proposed in different applications, with promising results such as in object detection of natural images. However, these features are not suitable for MR prostate images, as they are not invariant to both the inhomogeneity of MR images and the appearance variations of prostate gland.</p><p>To describe and compare the effectiveness of different features for identifying correspondences in two images, Fig. <ref type="figure" target="#fig_4">4</ref> shows a typical example by computing the similarity maps between one point (shown as red cross in Fig. <ref type="figure" target="#fig_4">4(a)</ref>) in the target image (Fig. <ref type="figure" target="#fig_4">4(a)</ref>) and all points in an aligned atlas image (Fig. <ref type="figure" target="#fig_4">4(b)</ref>). The white contours in (a) and (b) show the prostate boundaries, and the black dashed cross in Fig. <ref type="figure" target="#fig_4">4 (b)</ref> indicates the correct correspondence of the red-cross target point in the atlas image. The effectiveness of features can be reflected by the similarity map. If features are distinctive for correspondence detection, the similarity computed by using these features would be high for correct correspondences and low for incorrect correspondences. Fig. <ref type="figure" target="#fig_4">4(c-f</ref>) shows the similarity maps computed using different handcrafted features, such as intensity patch features, Haar features, HOG features and LBP features, respectively. It is clear that none of these features could capture correct correspondence, as the similarity between the corresponding voxels indicated by the red crosses is low, compared to that of nearby voxels. This shows that the existing handcrafted features are insufficient in multi-atlas based segmentation for the MR prostate.</p><p>To relieve the limitation of handcrafted features, it is necessary to learn discriminant features adaptive to MR prostate images. To demonstrate the effectiveness of deep learning features, Fig. <ref type="figure" target="#fig_4">4</ref> (g) and (h) provide the similarity maps computed using the two kinds of deep learning features obtained by our proposed unsupervised and supervised stacked sparse auto-encoder (SSAE), respectively. Compared to similarity maps of handcrafted features, it is clear that the correct correspondence can be better identified with the deep learning features, especially for the supervised SSAE. In the following section, we will elaborate how these features could be adaptively learned from MR prostate images by SSAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stacked Sparse Auto-Encoder (SSAE) for Learning the Latent Feature Representation</head><p>As illustrated in the previous section, it is necessary to learn the feature representation adaptive to the data, thus alleviating the need of labor-intensive feature engineering. To achieve this purpose, we introduce stacked sparse auto-encoder (SSAE) as a way to learn the latent feature representation from a collection of training prostate image patches. Stacked sparse auto-encoder is a deep learning architecture, which consists of basic feature learning layers, i.e., sparse auto-encoders (SAE). It is built by layer-wise stacking of sparse auto-encoders (Fig. <ref type="figure" target="#fig_8">7</ref>). In the following paragraphs, we first introduce the auto-encoder as a basic feature learning algorithm. Then, we explain sparse auto-encoder, which imposes sparsity constraint for learning the robust shallow feature representations. Finally, we elaborate how to learn deep feature hierarchy by stacking multiple sparse auto-encoders layer-wisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Basic Auto-Encoder</head><p>Serving as the fundamental component for SSAE, the basic auto-encoder (AE) trains a feed-forward non-linear neural network, which contains three layers, i.e., input layer, hidden layer, and output layer, as illustrated in Fig. <ref type="figure">5</ref>. Each layer is represented by a number of nodes. Blue nodes on the left and right sides of Fig. <ref type="figure">5</ref> indicate the input and output layers, respectively, and green nodes indicate the hidden layer. Nodes in the two neighboring layers are fully connected, which means that each node in the previous layer can contribute to any node in the next layer. Basically, AE consists of two steps, namely encoding and decoding. In the encoding step, AE encodes the input vector into a concise representation through connections between input and hidden layers. In the decoding step, AE tries to reconstruct the input vector from the encoded feature representation in the hidden layer. The goal of AE is to find a concise representation of input data, which could be used for the purpose of best reconstruction. Since we are interested in the representation of image patches, in this application the input to AE is an image patch, which is concatenated as a vector. In the training stage, given a set of training patches ğ‘¿ = {ğ’™ ğ‘– âˆˆ â„ ğ¿ , ğ‘– = 1, â€¦ , ğ‘}, where ğ‘ and ğ¿ are the number and the dimension of training patches, respectively, AE automatically learns the weights of all connections in the network by minimizing the reconstruction error in Eq. (1).</p><formula xml:id="formula_0">argmin ğ‘¾,ğ’ƒ,ğ‘¾ Ì‚,ğ’ƒ Ì‚âˆ‘ â€–ğ’™ ğ‘– -(ğ‘¾ Ì‚(ğœ(ğ‘¾ğ’™ ğ‘– + ğ’ƒ)) + ğ’ƒ Ì‚)â€– 2 2 ğ‘ ğ‘–=1 (1)</formula><p>where ğ‘¾, ğ’ƒ, ğ‘¾ Ì‚, ğ’ƒ Ì‚ are the parameters in the AE network, and ğœ(ğ’‚) = (1 + exp(-ğ’‚)) -1 . Given an input vector ğ’™ ğ‘– , AE first encodes it into the concise representation ğ’‰ ğ‘– = ğœ(ğ‘¾ğ’™ ğ‘– + ğ’ƒ), where ğ’‰ ğ‘– is the responses of ğ’™ ğ‘– at the hidden nodes, and the dimension of ğ’‰ equals to the number of nodes in the hidden layer. In the next step, AE tries to decode the original input from the encoded representation, i.e., with ğ‘¾ Ì‚ğ’‰ğ‘– + ğ’ƒ Ì‚. To learn effective features for the input training patches, AE requires that the dimension of the hidden layer is less than that of the input layer. Otherwise, the minimization of Eq. (1) would lead to trivial solutions, e.g., identity transformation. Studies <ref type="bibr" target="#b30">[32]</ref> have also shown that the basic AE learns very similar features as PCA.</p><p>Once the weights {ğ‘¾, ğ’ƒ, ğ‘¾ Ì‚, ğ’ƒ Ì‚} have been learned through the training patches, in the testing stage AE could efficiently obtain a concise feature representation for a new image patch ğ’™ new by a forward passing step, i.e., ğ’‰ new = ğœ(ğ‘¾ğ’™ new + ğ’ƒ).   where ğ›¿ is a parameter to balance between reconstruction and sparsity terms, and ğ‘€ is the number of hidden nodes. ğ¾ğ¿(ğœŒ|ğœŒ ğ‘— ) is the Kullback-Leibler divergence between two Bernoulli distributions with probability ğœŒ and ğœŒ ğ‘— . As we can see, the sparsity term is minimized only when ğœŒ ğ‘— is close to ğœŒ for every hidden node ğ‘—. Since ğœŒ is set to be a small constant, minimizing Eq. ( <ref type="formula">2</ref>) could lead to the sparse responses of hidden nodes, hence the sparsity of learned feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Stacked Sparse Auto-Encoder</head><p>By using SAE, we can learn the low-level features (such as Gabor-like features as shown in Fig. <ref type="figure" target="#fig_7">6</ref>) from the original data (MR image patches). However, low-level features are not enough due to large appearance variations of the MR prostate. It is necessary to learn abstract high-level features, which could also be invariant to the inhomogeneity of MR images. Motivated by the human perception, which constitutes a deep network to describe concepts in a hierarchical way using multiple levels of abstraction, we recursively apply SAE to learn more abstract/high-level features based on the features learned from the low-level. This multi-layer SAE model is referred to as a stacked sparse auto-encoder (SSAE), which stacks multiple SAEs on top of each other for building deep hierarchies.</p><p>Fig. <ref type="figure" target="#fig_8">7</ref> shows a typical SSAE with ğ‘… stacked SAEs. Let ğ‘¾ (ğ‘Ÿ) , ğ’ƒ (ğ‘Ÿ) , ğ‘¾ Ì‚(ğ‘Ÿ) and ğ’ƒ Ì‚(ğ‘Ÿ) denote the connection weights and intercepts between the input layer and hidden layer, and between the hidden layer and output layer in the ğ‘Ÿ-th SAE, respectively. In the encoding part of the SSAE, the input vector ğ’™ ğ‘– is first encoded by the first SAE for obtaining the low-level representation ğ’‰ ğ‘–</p><p>(1) , i.e., ğ’‰ ğ‘– (1) = ğœ(ğ‘¾ (1) ğ’™ ğ‘– + ğ’ƒ (1) ) .</p><p>Then, the low-level representation ğ’‰ ğ‘– (1) of the first SAE is considered as the input vector to the next SAE, which encodes it into higher level representation ğ’‰ ğ‘– (2) , i.e., ğ’‰ ğ‘– (2) = ğœ(ğ‘¾ (2) ğ’‰ ğ‘– (1) + ğ’ƒ (2) ). Generally, the ğ‘Ÿ -th level representation ğ’‰ ğ‘– (ğ‘Ÿ) can be obtained by a recursive encoding procedure ğ’‰ ğ‘– (ğ‘Ÿ) = ğœ(ğ‘¾ (ğ‘Ÿ) ğ’‰ ğ‘– (ğ‘Ÿ-1) + ğ’ƒ (ğ‘Ÿ) ) with ğ’‰ ğ‘– (0) = ğ’™ ğ‘– . Similarly, the decoding step of SSAE recursively reconstructs the input of each SAE. In this example, SSAE first reconstructs the low-level representation ğ’‰ Ì‚ğ‘– (ğ‘Ÿ-1) from the high-level representation ğ’‰ Ì‚ğ‘– (ğ‘Ÿ) , i.e., ğ’‰ Ì‚ğ‘– (ğ‘Ÿ-1) = ğ‘¾ Ì‚(ğ‘Ÿ) ğ’‰ Ì‚ğ‘– (ğ‘Ÿ) + ğ’ƒ Ì‚(ğ‘Ÿ) with ğ’‰ Ì‚ğ‘– (ğ‘…) = ğ’‰ ğ‘– (ğ‘…) for ğ‘Ÿ = ğ‘…, â€¦ ,2 . Then, using the reconstructed low-level representation ğ’‰ Ì‚ğ‘– (1) , the original input vector could be estimated, i.e., ğ’™ Ì‚ğ‘– = ğ‘¾ Ì‚(1) ğ’‰ Ì‚ğ‘– (1) + ğ’ƒ Ì‚(1) .</p><p>After stacking multiple SAEs together by feeding the output layer from the low-level SAE as the input layer of a high-level SAE, SSAE is able to extract more useful and general high-level features. In the optimization of SSAE, this deep architecture is first pre-trained in an unsupervised layer-wise manner and then fine-tuned by back propagation. Since the aforementioned SSAE network is trained based only on the original image patches, without using the supervised label information, it is denoted as the unsupervised SSAE. Fig. <ref type="figure" target="#fig_10">8</ref> shows some typical prostate image patches and their reconstructions by the unsupervised SSAE with ğ‘… = 4.  However, since the unsupervised SSAE trains the whole network on the unlabeled data, the high-level features learned from unsupervised SSAE are only data-adaptive, that is, not necessarily discriminative to separate prostate and background voxels. To make the learned feature representation discriminative <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>, the supervised fine-tuning is often adopted by stacking another classification output layer on the top of the encoding part of the SSAE, as shown in red dashed box of Fig. <ref type="figure" target="#fig_11">9</ref>. This top layer is used to predict the label likelihood of the input data ğ’™ ğ‘– by using the features learned from the most high-level representation ğ’‰ ğ‘– (ğ‘…) . The number of nodes in the classification output layer equals to the number of labels (i.e., "1" denotes prostate, and "0" denotes background).</p><p>Using the optimized parameters from the pre-training of SSAE as initialization, the entire neural network (Fig. <ref type="figure" target="#fig_11">9</ref>) can be further fine-tuned by back-propagation to maximize the classification performance. This tuning step is referred to as the supervised fine-tuning, in contrast with the unsupervised fine-tuning mentioned before. Accordingly, the entire deep network is referred to as the supervised SSAE. Fig. <ref type="figure" target="#fig_12">10</ref> gives a visual illustration of typical feature representations of the first and second hidden layers learned by a four-layer supervised SSAE based on the visualization method in <ref type="bibr" target="#b33">[35]</ref>. Here, Figs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sparse Patch Matching with the Deep Learning Features</head><p>Before sparse patch matching, all atlas images are registered to the target image. This registration includes two steps. First, linear registration is applied for initial alignment, with the guidance from the landmarks automatically-detected around the prostate region <ref type="bibr" target="#b34">[36]</ref>. Then, the free-form deformation (FFD) <ref type="bibr" target="#b35">[37]</ref> is further adopted to the linearly aligned images for deformable registration.</p><p>After learning the SSAE networks (either in unsupervised or supervised manner), each new image patch in the testing stage can be encoded as a high-level feature vector (i.e., the last hidden layer of the SSAE). These features can be fed into a segmentation framework for labeling voxels as either prostate or background. As one of the popular segmentation frameworks, multi-atlas based segmentation demonstrates its effectiveness on dealing with image variations in different applications <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39]</ref>. However, traditionally the multi-atlas based segmentation adopts only the intensity or handcrafted features for measuring the similarity between different local patches, or computing the weights of different patches during label propagation. Since MR prostate images exhibit large structural and appearance variations, we propose to incorporate the deep learning features, instead of the conventional handcrafted features, into the multi-atlas segmentation   framework. As the features extracted by the deep learning methods are usually more robust and discriminative, the performance of multi-atlas segmentation can be improved at the same time. Fig. <ref type="figure" target="#fig_14">11</ref> gives the general description of our multi-atlas based method, called sparse patch matching. In this method, instead of computing pair-wise intensity similarity as a matching weight, we propose to select only a small number of similar atlas patches by sparse representation, which is more robust to outliers. In the following, we give the detailed for our sparse patch matching method. In order to estimate the prostate likelihood map of a target image ğ¼ ğ‘  , we first align all the atlas images {ğ¼ ğ‘ , ğ‘ = 1, â€¦ , ğ‘ƒ} and their label maps {ğº ğ‘ , ğ‘ = 1, â€¦ , ğ‘ƒ} onto the target image ğ¼ ğ‘  . Then, to determine the prostate likelihood of a particular voxel ğ‘£ in the target image ğ¼ ğ‘  , we first extract the image patch centered at voxel ğ‘£ from the target image, and then all image patches within a certain searching neighborhood â„•(ğ‘£) across all the aligned atlas images.</p><p>Next, the deep learned feature representations for those extracted intensity patches are obtained through the encoding procedure of the learned SSAE as introduced in Section II. (3) According to Eq. ( <ref type="formula">3</ref>), it is easy to see that the robustness and accuracy of prostate likelihood estimation depends on how well the weighting vector ğœ¶ ğ‘£ is determined. In the literature, different weight estimation methods have been proposed <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b39">41]</ref>. Most multi-atlas based segmentation methods directly compute ğœ¶ ğ‘£ as the pair-wise similarity between intensity patches, such as using the Euclidean distance. In our method, we compute the weighting vector ğœ¶ ğ‘£ different from the previous methods in respect to the following two aspects. First, instead of using the intensity or handcrafted features, the high-level features are learned from the deep learning architecture. Second, with the help of recently proposed sparse representation method <ref type="bibr" target="#b2">[4]</ref>, we enforce sparsity constraint upon the weighting vector ğœ¶ ğ‘£ . In this way, we seek for the best representation of the target patch using a limited set of similar atlas patches. Mathematically, the optimization of ğœ¶ ğ‘£ can be formulated as the sparse representation problem below:</p><formula xml:id="formula_1">ğœ¶ ğ‘£ = arg min ğœ¶ ğ‘£ 1 2 â€–ğ’‡ ğ‘  (ğ‘£) -ğ‘¨ ğ‘£ ğœ¶ ğ‘£ â€– 2 2 + ğœ‚â€–ğœ¶ ğ‘£ â€– 1 ğ‘ . ğ‘¡. ğœ¶ ğ‘£ â‰¥ 0 (4)</formula><p>The first term is the data fitting term, which measures the difference between the target feature vector ğ’‡ ğ‘  (ğ‘£) and the linearly combined feature representation ğ‘¨ ğ‘£ ğœ¶ ğ‘£ from all atlas image patches. The second term is the sparsity term, which attributes to the sparsity property of the weighting vector ğœ¶ ğ‘£ . ğœ‚ controls the strength of sparsity constraint on the weighting vector ğœ¶ ğ‘£ . If ğœ‚ is larger, the number of non-zero elements in ğœ¶ ğ‘£ will be smaller. In this way, only a few patches in patch dictionary ğ‘¨ ğ‘£ will be selected to reconstruct the target features ğ’‡ ğ‘  (ğ‘£) in a non-parameter fashion, thus reducing the risk of including those misleading atlas patches in the likelihood estimation.</p><p>Based on the derived weighting vector ğœ¶ ğ‘£ , the prostate likelihood ğ‘„ ğ‘  (ğ‘£) for a target point ğ‘£ can be estimated by Eq. ( <ref type="formula">4</ref>). Since the weighting vector ğœ¶ ğ‘£ is sparse, the prostate likelihood ğ‘„ ğ‘  (ğ‘£) is finally determined by the linear combination of labels corresponding to atlas patches with non-zero elements in vector ğœ¶ ğ‘£ . After estimating the prostate likelihood for all voxels in the target image ğ¼ ğ‘  , a likelihood map ğ‘„ ğ‘  is generated, which can be used to robustly locate the prostate region (as shown in Fig. <ref type="figure" target="#fig_14">11</ref>). Usually, a simple thresholding or level set method <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43]</ref> can be applied to binarize the likelihood map for segmentation. However, since each voxel in the target image is independently estimated in the multi-atlas segmentation method, the final segmentation could be weird as no shape prior is considered. In order to robustly and accurately estimate the final prostate region from the prostate likelihood map, it is necessary to take into account the prostate shape prior during the segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SECOND LEVEL: DATA AND SHAPE DRIVEN DEFORMABLE MODEL</head><p>The main purpose of this section is to segment the prostate region based on the prostate likelihood map estimated in the previous section. The likelihood map can be used in two aspects for deformable model construction. First, the initialization of deformable model can be easily built by thresholding the likelihood map. In this way, the limitation of model initialization problem in the traditional deformable segmentation can be naturally relieved. Second, the likelihood map can be used as the appearance force to drive the evolution of deformable model. Besides, in order to deal with the large inter-patient shape variation, we propose to use sparse shape prior for deformable model regularization. In the following paragraphs, we first introduce the sparse shape composition as a non-parametric shape modeling method. Then, we present the optimization of our deformable model by jointly considering both shape and appearance information. Finally, the proposed deformable segmentation method is summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shape Prior by Sparse Shape Composition</head><p>Here, our deformable model is represented by a 3D surface, which is composed of ğ¾ vertices {ğ’… ğ‘˜ |ğ‘˜ = 1, â€¦ , ğ¾} . After concatenating these ğ¾ vertices {ğ’… ğ‘˜ |ğ‘˜ = 1, â€¦ , ğ¾} into a vector ğ’…, each deformable model can be represented as a shape vector with length of 3 â€¢ ğ¾. Let ğ‘« denotes a large shape dictionary that includes prostate shape vectors of all training subjects. Each column of shape dictionary ğ‘« corresponds to the shape vector of one subject. The shape dictionary can be used as a shape prior to constrain the deformable model in a learned shape space. Instead of assuming the Gaussian distribution of shapes and then simply using PCA for shape modeling as in the Active Shape Model <ref type="bibr" target="#b42">[44]</ref>, we adopt a recently proposed method, named sparse shape composition <ref type="bibr" target="#b43">[45]</ref>, for shape modeling. In the sparse shape composition, the shapes are sparsely represented by shape instances in the shape dictionary without the need of Gaussian assumption. Specifically, given a new shape vector ğ’… and shape dictionary ğ‘« , sparse shape composition method reconstructs shape vector ğ’… as the sparse representation of shape dictionary ğ‘« by minimizing the following objective function:</p><p>(ğœº, ğœ“) = arg min ğœº,ğœ“ â€–ğœ“(ğ’…) -ğ‘«ğœºâ€– 2 2 + ğœ‡â€–ğœºâ€– 1 <ref type="bibr" target="#b3">(5)</ref> where ğœ“(ğ’…) denotes the target shape ğ’… that is affine aligned onto the mean shape space of shape dictionary ğ‘«. ğœº indicates the sparse coefficient for the linear shape combination. Once (ğœº, ğœ“) are estimated by Eq. ( <ref type="formula">5</ref>), the regularized shape can be computed by ğœ“ -1 (ğ‘«ğœº) , where ğœ“ -1 is the inverse affine transform of ğœ“.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization of Deformable Model Method</head><p>For each target image, the segmentation task is formulated as the deformable model optimization problem. During the optimization procedure, each vertex of deformable model ğ‘‘ ğ‘˜ is driven iteratively by the information from both prostate likelihood map and shape model until converged at the prostate boundaries. Mathematically, the evolution of the deformable model can be formulated as the minimization of an energy function, which contains a data energy ğ¸ data and a shape energy ğ¸ shape as in Eq. ( <ref type="formula" target="#formula_2">6</ref>):</p><formula xml:id="formula_2">ğ¸ = ğ¸ data + ğœ†ğ¸ shape<label>(6)</label></formula><p>The data term ğ¸ data is used to attract the 3D surface towards the object boundary based on the likelihood map. Specifically, each vertex ğ’… ğ‘˜ is driven by the force related to the gradient vector of prostate likelihood map. Denote âˆ‡ğ‘„ âƒ—âƒ—âƒ—âƒ—âƒ— ğ‘  (ğ’… ğ‘˜ ) as the gradient vector at vertex ğ’… ğ‘˜ in the prostate likelihood map, and ğ‘› âƒ— ğ‘  (ğ’… ğ‘˜ ) as the normal vector on the vertex ğ’… ğ‘˜ of surface. When vertex ğ’… ğ‘˜ deforms exactly to the prostate boundary and also its normal direction aligns with the gradient direction of prostate boundary, the local matching term âŒ©âˆ‡ğ‘„ âƒ—âƒ—âƒ—âƒ—âƒ— ğ‘  (ğ’… ğ‘˜ ), ğ‘› âƒ— ğ‘  (ğ’… ğ‘˜ )âŒª is maximized. In this case, we formulate to minimize the data energy ğ¸ data as: ğ¸ data = -âˆ‘ âŒ©âˆ‡ğ‘„ âƒ—âƒ—âƒ—âƒ—âƒ— ğ‘  (ğ’… ğ‘˜ ), ğ‘› âƒ— ğ‘  (ğ’… ğ‘˜ )âŒª ğ‘˜ <ref type="bibr" target="#b5">(7)</ref> Since all the vertices on the deformable model are jointly evolved during the deformation, the matching of the deformable model with prostate boundary will be robust to possible incorrect likelihood on some vertices, as well as inconsistency between neighboring vertices.</p><p>The shape term ğ¸ shape is used to encode the geometric property of prostate shape based on the estimated coefficient ğœº and the transformation ğœ‘ in Eq. ( <ref type="formula">5</ref>). Specifically, the shape term is formulated as below:</p><formula xml:id="formula_3">ğ¸ shape = â€–ğ’… -ğœ“ -1 (ğ‘«ğœº)â€– 2 2 + ğ›½ âˆ‘ â€–ğ’… ğ‘˜ - âˆ‘ ğ’… ğ‘˜ ğ‘‘ ğ‘— âˆˆâ„•(ğ’… ğ‘˜ ) âˆ‘ ğŸ ğ‘‘ ğ‘— âˆˆâ„•(ğ’… ğ‘˜ ) â€– 2 2 ğ‘˜<label>(8)</label></formula><p>where the first term constrains the deformed shape ğ’… to be close to the regularized shape ğœ“ -1 (ğ‘«ğœ€) by the sparse shape composition, and the second term imposes the smoothness constraint on shape, which prevents large deviations between each vertex ğ’… ğ‘˜ and the center of its neighboring vertices ğ‘‘ ğ‘— âˆˆ â„•(ğ’… ğ‘˜ ). By combining Eq. ( <ref type="formula">7</ref>) and Eq. ( <ref type="formula" target="#formula_3">8</ref>) into Eq. ( <ref type="formula" target="#formula_2">6</ref>), the vertices on the deformable model are iteratively driven towards the prostate boundary while constraining the shape in a non-parametric shape space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Summary of the Proposed Deformable Segmentation Method</head><p>Generally, the overall optimization of deformable model can be summarized as an expectation-maximization (EM) algorithm, which minimizes the data energy and shape energy alternatively. Given the target image ğ¼ ğ‘  , the initial deformable surface ğ’… 0 is estimated by solving the sparse learning problem in Eq. ( <ref type="formula">5</ref>). Then the "M" step and "E" step are alternatively executed as follows. Based on the likelihood map generated from stacked sparse auto-encoder and sparse patch matching, in the "M" step, the deformable model ğ’… ğ‘¡ is first evolved to minimize the data energy function ğ¸ data (Eq. ( <ref type="formula">7</ref>)). Then, in the "E" step, the parameters (ğœº ğ‘¡ , ğœ“ ğ‘¡ ) is estimated for the shape refinement by solving Eq. ( <ref type="formula">5</ref>), and the deformable model ğ’… ğ‘¡ is further refined by minimizing the shape energy (Eq. ( <ref type="formula" target="#formula_3">8</ref>)) with the computed parameters (ğœº ğ‘¡ , ğœ“ ğ‘¡ ) . After ğ‘‡ iterations of the above EM step, the output shape ğ’… ğ‘‡ is converted to a binary label map ğº ğ‘  , which gives the final segmentation result of the target image ğ¼ ğ‘  .</p><p>0278-0062 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Materials and Parameter Settings</head><p>We evaluate our method on the dataset, which includes 66 T2-weighted MR images from the University of Chicago Hospital. The images are acquired with 1.5T magnetic field strength from different patients under different MR image scanners (34 images from Philips Medical Systems and 32 images from GE Medical Systems). Under this situation, the difficulty for the segmentation task increases since both shape and appearance differences are large. In Fig. <ref type="figure" target="#fig_18">12</ref>, images (b) and (e) were acquired from a GE MRI scanner, while the other three were acquired from a Philips MRI scanner. As shown in Fig. <ref type="figure" target="#fig_18">12</ref>, image (c) was obtained without the endorectal coil. It has different prostate shape with other four images acquired with the endorectal coil. Besides, the prostate appearance suffers from the inhomogeneity (as in (b) and (d)) and noises (as in (d) and (e)), which further produce large variations. The image dimension and spacing are different from image to image. For example, the image dimension varies from 256 Ã— 256 Ã— 28 to 512 Ã— 512 Ã— 30 . The image spacing varies from 0.49 Ã— 0.49 Ã— 3 mm to 0.56 Ã— 0.56 Ã— 5 mm. The manual delineation of the prostate in each image is provided by a clinical expert as the ground truth for quantitative evaluation. As the preprocessing of the dataset, the bias field correction <ref type="bibr" target="#b44">[46]</ref> and histogram matching <ref type="bibr" target="#b45">[47]</ref> are applied to each image successively. We adopted the two-fold cross-validation. Specifically, in each case, the images of one fold are used for training the models, while the images of other fold are used for testing the performance.</p><p>The parameters for deep feature learning are listed below. The patch size is 15 Ã— 15 Ã— 9. The number of layers in SSAE framework is 4. The number of nodes in each layer of SSAE is 800, 400, 200, 100, respectively. Thus, the deep learning features have the dimensionality of 100. The target activation ğœŒ for the hidden units is 0.15. The sparsity penalty ğ›½ is 0.1. The Deep Learning Toolbox [48] is used for training our SSAE framework.</p><p>The searching neighborhood â„•(ğ‘£) is defined as the 7 Ã— 7 Ã— 7 neighborhood centered at voxel ğ‘£. For sparse patch matching, the parameter ğœ‚ in Eq. ( <ref type="formula">4</ref>), which controls the sparsity constraint, is 0.001. For the final deformable model segmentation, the parameter ğœ† in Eq. ( <ref type="formula" target="#formula_2">6</ref>), which weights the shape energy during deformation, is 0.5, and the parameter ğœ‡ in Eq. ( <ref type="formula">5</ref>) is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>Given the ground-truth segmentation ğ‘† and the automatic segmentation ğ¹, the segmentation performance is evaluated by four metrics: Dice ratio, precision, Hausdorff distance, and average surface distance. Dice ratio and precision measure the overlap between two segmentations. Hausdorff distance and average surface distance measures the boundary distance between two surfaces of segmentation. Detailed definitions are given as Table <ref type="table" target="#tab_1">I</ref>, where ğ‘‰ indicates the volume size, ğ‘’ ğ‘† and ğ‘’ ğ¹ are the surfaces for the ground-truth and automatic segmentations, respectively, and dist(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ) denotes the Euclidean distance between vertices ğ‘‘ ğ‘– and ğ‘‘ ğ‘— .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Results 1) Evaluation of the Performance of First Level</head><p>Inspired by paper <ref type="bibr" target="#b46">[49]</ref>, we plot the PCA-projected features to show the effectiveness of different features in separating voxels from different classes (e.g., the prostate class and the non-prostate class). After mapping each feature vector to the subspace spanned by the first three principal components, the effective features would 1) cluster the voxels with the same class label as close as possible and 2) separate the voxels with different class labels as far as possible. First, we demonstrate the discrimination power of our deep learning features in Fig. <ref type="figure" target="#fig_17">13</ref>, by visualizing the PCA-projected feature distributions of different feature representations, i.e., intensity patch (Fig. <ref type="figure" target="#fig_17">13</ref> (a)), handcrafted (Fig. <ref type="figure" target="#fig_17">13 (b</ref>)), features learned by unsupervised SSAE (Fig. <ref type="figure" target="#fig_17">13 (c</ref>)), and features learned by supervised SSAE (Fig. <ref type="figure" target="#fig_17">13 (d)</ref>). Specifically, for the case of handcrafted features, we include three commonly used features, i.e., Haar <ref type="bibr" target="#b47">[50]</ref>, HoG <ref type="bibr" target="#b45">[47]</ref> and LBP <ref type="bibr" target="#b19">[21]</ref>. The same patch size is used for computing all features under comparison. It can be seen that the deep learning features from supervised SSAE have better clustering results in the projected feature space, and thus better discriminative power than other two predefined features (i.e., intensity, and handcrafted), as well as deep learning features by unsupervised SSAE. The superior performance of supervised SSAE over the unsupervised SSAE indicates the necessity of utilizing label information to improve the discrimination power of learned features.</p><p>Next, we evaluate the segmentation accuracy of different feature representations in the context of sparse patch matching. Table <ref type="table" target="#tab_2">II</ref> lists the quantitative results (Dice ratio, precision, Hausdorff distance, and average surface distance) for all feature representations. The p-values (computed with paired t-test at 5% significance level), comparing the supervised SSAE with all other methods, are provided below each quantitative result. It can be observed that our supervised SSAE method significantly outperforms all the intensity and handcrafted feature methods. According to the paired t-test at 5% significance level, both our proposed method (unsupervised and supervised SSAE) outperformed the rest of competing method, but the supervised SSAE is not statistically superior to the unsupervised SSAE.</p><p>Fig. <ref type="figure" target="#fig_20">14</ref> further shows the typical likelihood maps estimated by four different feature representations for three different patients. It can be observed that the features learned from supervised SAE can better capture the prostate boundary, especially on the anterior and right posterior parts of the prostate. Fig. <ref type="figure" target="#fig_19">15</ref> shows some typical segmentation results obtained by the sparse label matching method with four different feature representations, respectively. Similarly, the proposed method (i.e., supervised SSAE) achieves the best   segmentation, especially on the anterior parts of the prostate, which demonstrates the effectiveness of our proposed method. Moreover, Fig. <ref type="figure" target="#fig_21">16</ref> gives the typical prostate segmentation results of different patients produced by four different feature representations, respectively. 3D visualization of the segmentation result has been added below each segmentation result shown in 2D. For each 3D visualization, the red surface indicates automatic segmentation results with different features, such as intensity, handcrafted, unsupervised SSAE and supervised SSAE, respectively. The transparent grey surfaces indicate the ground-truth segmentations. Our proposed supervised SSAE method improves the segmentation accuracy on both the anterior and posterior parts of the prostates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Evaluation of the Performance of Second Level</head><p>In this section, we further evaluate our deformable model to show its effectiveness. The comparison methods contain three different deformable model based methods. The first one is the conventional Active Shape Model (ASM). The second one uses intensity features for multi-atlas label fusion, and then finalizes the segmentation by adopting a deformable model on the produced likelihood map, similar to our proposed method. The second method follows the same procedure as the first one except using the handcrafted features, such as Haar, HOG, and LBP, instead of intensity patch for multi-atlas label fusion. Table <ref type="table" target="#tab_3">III</ref> shows the segmentation results of intensity, handcrafted and supervised SSAE with/without deformable model and the p-value (with paired t-test at 5% significance level) between the supervised SSAE with deformable model and all other methods. According to the paired t-test at 5% significance level on Dice ratio, our proposed deformable model is statistically the best among all the competing methods. Specifically, our proposed supervised SAE outperforms the ASM, the intensity based deformable model, and the handcrafted based deformable model by 10.7%, 2.1% and 1.6%, respectively. Besides, it can be seen that, after adopting the second level of deformable segmentation, the segmentation accuracy can be further improved for all the comparing methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion 1) The Issues of Deep Learning Method</head><p>The key advantage of deep learning methods is learning good features automatically from data and avoiding using the manually-designed feature extractors, which often require high engineering skills. SAE differs from AE and PCA in the aspect that it imposes sparsity on the mapped features (i.e., responses of hidden nodes), thus avoiding the problem of trivial solutions when the dimensionality of hidden features is more than that of the input features. By stacking SAE together, SSAE is able to learn the hierarchical feature representation, similar to other deep learning models. Besides, we use unsupervised initialization in the pre-training stage, which prevents the later supervised training from falling into the bad local minimum. This also contributes to good performance of our method.</p><p>However, the issue of small dataset vs. large number of variables arises during the training of the deep network. To prevent the potential issue raised from the limited number of training samples, two strategies are adopted in our method. First, we pre-trained the deep network in a layer-by-layer manner <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b48">51]</ref>, which can learn a hierarchy of feature representation one layer a time. Specifically, in the training of each layer, the features learned from previous layer are feed into the next layer. The first three layers consist of 320,000, 80,000, 20,000 parameters, respectively. In our experiment, totally 396,000 training samples were used, which should be sufficient for this lay-wise pre-training step. Second, in the fine-tuning stage, the entire deep network is refined only by several iterations, thus better preventing the overfitting issue.</p><p>To further relieve the possible overfitting issues, we can also use the idea of transfer learning <ref type="bibr" target="#b49">[52]</ref><ref type="bibr" target="#b50">[53]</ref><ref type="bibr" target="#b51">[54]</ref><ref type="bibr" target="#b52">[55]</ref>. Specifically, in the unsupervised pre-training step, we can borrow MR images from other body parts (e.g., heart) to initialize our deep network, thus capturing more general MR image appearance. We believe that this initialization could benefit the fine-tuning step and thus overcome the small sample problem. Note that similar strategies have been widely used in the field of computer vision and machine learning <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b48">[51]</ref><ref type="bibr" target="#b49">[52]</ref><ref type="bibr" target="#b50">[53]</ref><ref type="bibr" target="#b51">[54]</ref><ref type="bibr" target="#b52">[55]</ref><ref type="bibr" target="#b53">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) The Issue of Deformable Segmentation Method</head><p>According to Eq. ( <ref type="formula">7</ref>), the data term of deformable model is driven by the gradient of the prostate likelihood map. One potential issue may happen if evolving the deformable model according to this data term. That is, the gradient will be zero if the initial shape is a bit away from the boundary. We proposed two strategies to address this potential issue. First, we obtained the initial prostate shape by thresholding the probability map, which makes the initialization not far away from the boundary. Second, the deformation is regularized by the shape model. Thus, even some model vertices cannot find the boundary in the capture range, the shape model can still pull them towards the boundary as long as other vertices have been deformed to the boundary. That is, shape regularization makes all vertices deform as a whole, thus addressing the capture range issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) The Computational Time</head><p>For our algorithm, the computational time mainly contains three parts: 1) registration part; 2) multi-atlas label fusion part; 3) deformable segmentation part. For registration, the run-time for each affince and non-linear registration is about 20 seconds and four minutes, respectively. For multi-atlas label fusion, the computational time is about 45 minutes, which is the major computational cost of our method. This mainly is due to the individual labeling for a large amount of voxels in each subject image. Currently, we implement multi-atlas label fusion in MATLAB, which is time-consuming for the loop job of sequentially labeling each voxel. For improving the efficiency of our algorithm, one possible solution is to implement the whole label fusion step by using the C++ language. In this way, we expect the computational time to be reduced to 10 minutes for the entire label fusion part. As for the deformable model segmentation part, the computational time is less than one minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present an automatic segmentation algorithm for T2 MR prostate images. To address the challenges of making the feature representation robust to large appearance variations of the prostate, we propose to extract the deep learning features by the SSAE framework. Then, the learned features are used under sparse patch matching framework to estimate the prostate likelihood map of the target image. To further relieve the impact of large shape variation in the prostate shape repository, a deformable model is driven toward the prostate boundary under the guidance from the estimated prostate likelihood map and sparse shape prior. The proposed method is extensively evaluated on the data set containing 66 prostate MR images. By comparing with several state-of-the-art MR prostate segmentation methods, our method demonstrates the superior performance regarding to the segmentation accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2508280, IEEE Transactions on Medical Imaging &gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Typical T2-weighted prostate MR images. Red contours indicate the prostate glands delineated manually by an expert. (b) Intensity distributions of prostate and background voxels around the prostate boundary of (a). (c) The 3D illustrations of prostate surfaces corresponding to each image in (a).</figDesc><graphic coords="2,55.95,51.20,235.90,181.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The prostate shape distribution obtained from the PCA analysis.</figDesc><graphic coords="2,339.45,44.30,199.50,147.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The schematic description of our proposed segmentation framework.</figDesc><graphic coords="3,53.23,50.15,245.61,214.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The similarity maps computed between a reference voxel (red cross) in the target image (a) and all voxels in the atlas image (b) by the four handcrafted feature representations, i.e., intensity (c), Haar (d), HOG (e) and LBP (f), as well as the two deep learning feature representations, namely unsupervised SSAE (g) and the supervised SSAE (h). White contours indicate the prostate boundaries, and the black dashed crosses indicate the ground-truth point in (b), which is corresponding to the red cross in (a).</figDesc><graphic coords="4,48.00,57.50,248.36,140.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 . 2 )</head><label>52</label><figDesc>Fig. 5. Construction of the basic AE.</figDesc><graphic coords="4,345.90,54.35,189.55,149.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The low-level feature representation learned from the SAE. Here, we reshape each row in ğ‘¾ into the size of image patch, and only visualize its first slice as an image filter.</figDesc><graphic coords="5,323.85,57.20,226.80,136.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Construction of the unsupervised SSAE with ğ‘… stacked SAEs.</figDesc><graphic coords="5,313.05,249.25,248.35,113.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>10  (a)  and (b) show the visualization of 60 units obtained from the first and second hidden layers under unsupervised pre-training (with unlabeled image patches) and supervised fine-tuning (with labeled image patches), respectively. It can be seen that higher hidden layer tends to be more affected by the classification layer we introduced.After learning all the parameters {ğ‘¾ (ğ‘Ÿ) , ğ‘¾ Ì‚(ğ‘Ÿ) , ğ’ƒ(ğ‘Ÿ) , ğ’ƒ Ì‚(ğ‘Ÿ) } of SSAE (ğ‘Ÿ = 1, â€¦ , ğ‘…), where ğ‘… denotes the number of stacked SAEs, the high-level representations of a new image patch ğ’™ new can be efficiently obtained by a recursive forwarding pass, i.e., ğ’‰ new ğ‘Ÿ = ğœ(ğ‘¾ (ğ‘Ÿ) ğ’‰ new (ğ‘Ÿ-1) + ğ’ƒ(ğ‘Ÿ) ) with ğ’‰ new 0 = ğ’™ new for ğ‘Ÿ = 1, â€¦ , ğ‘…. The final high-level representation ğ’‰ new ğ‘… will be used as features to guide the sparse patch matching (Section II.C), and propagate labels from atlas images to the target image for estimating the prostate likelihood map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Typical prostate image patches (a) and their reconstructions (b) by using the unsupervised SSAE with four stacked SAEs.</figDesc><graphic coords="6,49.05,55.20,248.40,108.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Construction of the supervised SSAE with a classification layer, which fine-tunes the SSAE with respect to the task of voxel-wise classification between prostate (label = 1) and background (label = 0).</figDesc><graphic coords="6,56.25,201.50,233.58,148.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Visualization of typical feature representations of the first hidden layer (first row) and second hidden layer (second row) for the unsupervised pre-training (a) and supervised fine-tuning (b), respectively.</figDesc><graphic coords="6,315.10,49.55,248.40,204.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>B. Denote ğ’‡ ğ‘  (ğ‘£) as the deep learning features for the target image patch at point ğ‘£, and denote ğ‘¨ ğ‘£ as the feature matrix resulted from column-wise combination of deep learning features of atlas patches, i.e., ğ‘¨ ğ‘£ = [ğ’‡ ğ‘ (ğ‘¢)|ğ‘ = 1, â€¦ , ğ‘ƒ; ğ‘¢ âˆˆ â„•(ğ‘£)] . To estimate the prostate likelihood ğ‘„ ğ‘  (ğ‘£) for voxel ğ‘£ in the target image ğ¼ ğ‘  , we linearly combine the label of each voxel ğ‘¢ âˆˆ â„•(ğ‘£) from each atlas image ğ¼ ğ‘ with a weighting vector ğœ¶ ğ‘£ = [ğ›¼ ğ‘£ (ğ‘, ğ‘¢)] ğ‘¢=1,â€¦,|â„•(ğ‘£)|;ğ‘=1,â€¦,ğ‘ƒ as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The schematic description of sparse patch matching</figDesc><graphic coords="7,48.75,61.10,244.78,176.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2508280, IEEE Transactions on Medical Imaging &gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>)ğ‘†:</head><label></label><figDesc>ground truth segmentation; ğ¹: automatic segmentation; ğ‘‰: volume size; ğ‘’ ğ‘† : surfaces of ground-truth segmentation; ğ‘’ ğ¹ : surface of automatic segmentation; ğ‘‘ ğ‘– : vertices on the surfaces ğ‘’ ğ‘† ; ğ‘‘ ğ‘— : vertices on the surfaces ğ‘’ ğ¹ ; dist(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ) : Euclidean distance between vertices ğ‘‘ ğ‘– and ğ‘‘ ğ‘—</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Distributions of voxel samples by using four types of features: (a) intensity, (b) handcrafted, (c) unsupervised SSAE, and (d) supervised SSAE. Red crosses and green circles denote prostate and non-prostate voxel samples, respectively.</figDesc><graphic coords="9,313.35,292.30,250.33,218.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Five typical T2-weighted MR prostate images acquired from different scanners, showing large variations of both prostate appearance and shape, especially for the cases with or without using the endorectal coils.</figDesc><graphic coords="9,47.50,52.50,256.15,52.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Typical prostate segmentation results of the same patients produced by four different feature representations: (a) intensity, (b) handcrafted, (c) unsupervised SSAE, and (d) supervised SSAE. Three rows show the results for three different slices of the same patient, respectively. Red contours indicate the manual ground-truth segmentations, and yellow contours indicate the automatic segmentations.</figDesc><graphic coords="10,321.85,224.25,239.19,148.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. (a) Typical slices of T2 MR images with manual segmentations. The likelihood maps produced by sparse patch matching with four feature (b) intensity patch, (c) handcrafted, (d) unsupervised SSAE, and (e) supervised SSAE. Red contours indicate the manual ground-truth segmentations.</figDesc><graphic coords="10,46.90,224.55,259.00,139.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Typical prostate segmentation results of three different patients produced by four different feature representations: (a) intensity, (b) handcrafted, (c) unsupervised SSAE, and (d) supervised SSAE. Three odd rows show the results for three different patients, respectively. Red contours indicate the manual ground-truth segmentations, and yellow contours indicate the automatic segmentations. Three even rows show the 3D visualization of the segmentation results corresponding to the images above. For each 3D visualization, the red surfaces indicate the automatic segmentation results using different features, such as intensity, handcrafted, unsupervised SSAE and supervised SSAE, respectively. The transparent grey surfaces indicate the ground-truth segmentations.</figDesc><graphic coords="11,313.30,55.95,253.95,316.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Deformable MR Prostate Segmentation via DeepFeature Learning and Sparse Patch Matching Yanrong Guo â€  , Yaozong Gao â€  , Dinggang Shen</figDesc><table /><note><p>* , Senior Member, IEEE P 0278-0062 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DEFINITION</head><label>I</label><figDesc>OF EVALUATION MEASUREMENT Hausdorff distance max(ğ»(ğ‘’ ğ‘† , ğ‘’ ğ¹ ), ğ»(ğ‘’ ğ‘† , ğ‘’ ğ¹ )), ğ»(ğ‘’ ğ‘† , ğ‘’ ğ¹ ) = max ğ‘‘ ğ‘– âˆˆğ‘’ ğ‘† {min ğ‘‘ ğ‘— âˆˆğ‘’ ğ¹ dist(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— )}</figDesc><table><row><cell>Dice ratio</cell><cell></cell><cell></cell><cell></cell><cell cols="2">2 â€¢ ğ‘‰(ğ‘† âˆ© ğ¹) ğ‘‰(ğ‘†) + ğ‘‰(ğ¹)</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘‰(ğ‘† âˆ© ğ¹) ğ‘‰(ğ¹)</cell></row><row><cell>Average surface</cell><cell>1 2</cell><cell cols="2">âˆ‘ (</cell><cell cols="2">ğ‘‘ ğ‘– âˆˆğ‘’ ğ‘†</cell><cell>min ğ‘‘ ğ‘— âˆˆğ‘’ ğ¹ dist(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ) âˆ‘ 1 ğ‘‘ ğ‘– âˆˆğ‘’ ğ‘†</cell></row><row><cell>distance</cell><cell cols="2">+</cell><cell cols="2">âˆ‘ ğ‘‘ ğ‘— âˆˆğ‘’ ğ¹</cell><cell>min ğ‘‘ ğ‘– âˆˆğ‘’ ğ‘† dist(ğ‘‘ ğ‘— , ğ‘‘ ğ‘– ) âˆ‘ 1 ğ‘‘ ğ‘— âˆˆğ‘’ ğ¹</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MEAN</head><label>II</label><figDesc>AND STANDARD DEVIATION OF QUANTITATIVE RESULTS FOR MR PROSTATE SEGMENTATION WITH DIFFERENT FEATURE REPRESENTATIONS. BEST PERFORMANCE IS INDICATED BY BOLD FONT.</figDesc><table><row><cell>Method</cell><cell>Intensity</cell><cell>Haar</cell><cell>HOG</cell><cell>LBP</cell><cell cols="2">Handcraft Unsupervised SSAE</cell><cell>Supervised SSAE</cell></row><row><cell>Dice (%)</cell><cell>85.3Â±6.2 (1.1e-04)</cell><cell>85.6Â±4.9 (2.2e-05)</cell><cell>85.7Â±4.9 (6.1e-05)</cell><cell>85.5Â±4.3 (5.5e-05)</cell><cell>85.9Â±4.5 (7.0e-06)</cell><cell>86.7Â±4.4 (2.1e-01)</cell><cell>87.1Â±4.2 (NA)</cell></row><row><cell>Precision (%)</cell><cell>85.1Â±8.3 (1.9e-03)</cell><cell>85.9Â±8.5 (3.63e-02)</cell><cell>85.3Â±8.7 (4.5e-03)</cell><cell>83.7Â±7.7 (1.3e-06)</cell><cell>87.3Â±7.4 (7.0e-01)</cell><cell>87.3Â±7.3 (7.3e-01)</cell><cell>87.1Â±7.3 (NA)</cell></row><row><cell>Hausdorff</cell><cell>8.68Â±4.24 (1.8e-01)</cell><cell>8.50Â±2.86 (1.9e-01)</cell><cell>8.51Â±2.69 (1.6e-01)</cell><cell>8.59Â±2.38 (1.1e-01)</cell><cell>8.55Â±2.91 (1.5e-01)</cell><cell>8.65Â±2.69 (6.3e-02)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III MEAN</head><label>III</label><figDesc>AND STANDARD DEVIATION OF QUANTITATIVE RESULTS FOR THE SEGMENTATIONS OBTAINED BY SUPERVISED SSAE WITH/WITHOUT USING DEFORMABLE MODEL. BEST PERFORMANCE IS INDICATED BY BOLD FONT. * DENOTES THE STATISTICALLY BEST PERFORMANCE AMONG ALL THE METHODS WITH/WITHOUT DEFORMABLE MODEL (ACCORDING TO THE PAIRED T-TEST AT 5% SIGNIFICANT LEVEL). This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2508280, IEEE Transactions on Medical Imaging &gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 12</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Intensity</cell><cell cols="2">Handcrafted</cell><cell cols="2">Supervised SSAE</cell></row><row><cell>Method</cell><cell>ASM</cell><cell>w/o Deformable</cell><cell>w/ Deformable</cell><cell>w/o Deformable</cell><cell>w/ Deformable</cell><cell>w/o Deformable</cell><cell>w/ Deformable</cell></row><row><cell></cell><cell></cell><cell>model</cell><cell>model</cell><cell>model</cell><cell>model</cell><cell>model</cell><cell>model*</cell></row><row><cell>Dice (%)</cell><cell>78.4Â±9.7 (2.7e-11)</cell><cell>85.3Â±6.2 (3.0e-06)</cell><cell>86.0Â±4.3 (7.3e-10)</cell><cell>85.9Â±4.5 (7.7e-08)</cell><cell>86.4Â±4.4 (5.1e-06)</cell><cell>87.1Â±4.2 (2.6e-03)</cell><cell>87.8Â±4.0* (NA)</cell></row><row><cell>Precision (%)</cell><cell>71.9Â±13.8 (1.3e-21)</cell><cell>85.1Â±8.3 (8.0e-17)</cell><cell>89.3Â±7.4 (4.5e-07)</cell><cell>87.3Â±7.4 (1.4e-11)</cell><cell>92.3Â±7.3 (7.7e-02)</cell><cell>87.1Â±7.3 (5.7e-20)</cell><cell>91.6Â±6.5 (NA)</cell></row><row><cell>Hausdorff</cell><cell>11.50Â±5.48 (8.5e-07)</cell><cell>8.68Â±4.24 (3.6e-04)</cell><cell>7.72Â±2.90 (2.5e-02)</cell><cell>8.55Â±2.91 (3.8e-05)</cell><cell>7.97Â±2.92 (1.2e-04)</cell><cell>8.12Â±2.89 (7.4e-03)</cell><cell>7.43Â±2.82* (NA)</cell></row><row><cell>ASD</cell><cell>(9.4e-10) 3.12Â±1.71</cell><cell>(6.4e-04) 1.87Â±0.93</cell><cell>( 2.0e-07) 1.78Â±0.55</cell><cell>(2.0e-05) 1.77Â±0.54</cell><cell>(1.3e-03) 1.71Â±0.50</cell><cell>(1.6e-02) 1.66Â±0.49</cell><cell>1.59Â±0.51*</cell></row></table><note><p>(NA) 0278-0062 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>&gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by NIH grant CA140413.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MR-guided biopsy of the prostate: an overview of techniques and a systematic review</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pondman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>FÃ¼tterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Schultze Kool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Witjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hambrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Urology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="517" to="527" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The role of preoperative endorectal magnetic resonance imaging in the decision regarding whether to preserve or resect neurovascular bundles during radical retropubic prostatectomy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hricak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Coakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Reuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="2655" to="2663" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic prostate MR image segmentation with sparse label propagation and domain-specific manifold regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yousuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Karademir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pohl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>ZÃ¶llei</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7917</biblScope>
			<biblScope unit="page" from="511" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label Image Constrained Multiatlas Selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Choyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1158" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-atlas Based Image Selection with Label Image Constraint</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications (ICMLA), 2012 11th International Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="311" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-Atlas Segmentation of the Prostate: A Zooming Process with Robust Registration and Atlas Selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PROMISE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multifeature landmark-free active appearance models: application to prostate MRI segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1638" to="1650" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate prostate volume estimation using multifeature active shape models on T2-weighted MRI</title>
		<author>
			<persName><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Genega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Lenkinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic radiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="745" to="754" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A generative model for image segmentation based on label fusion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T T</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1714" to="1729" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic clustering of white matter fibers in brain diffusion MRI with an application to genetics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>De Zubicaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mcmahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="75" to="90" />
			<date type="published" when="2014">10/15/ 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated multi-atlas labeling of the fornix and its integrity in alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="140" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identification of infants at high-risk for autism spectrum disorder using multiparameter multiscale white matter connectivity networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="4880" to="4896" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prostate Segmentation in MR Images Using Discriminant Boundary Features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Choyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="479" to="488" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label fusion in atlas-based segmentation using a selective and iterative method for performance level estimation (SIMPLE)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">A</forename><surname>Van Der Heide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N T J</forename><surname>Kotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Vulpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2000" to="2008" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Patient specific prostate segmentation in 3-D magnetic resonance images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kai-Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raniga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Greer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1955" to="1964" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boundary detection in medical images using edge following algorithm based on intensity gradient and texture gradient features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Somkantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Theera-Umpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auephanwiriyakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="567" to="573" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition,Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>PietikÃ¤ Inen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse Patch-Based Label Propagation for Accurate Prostate Localization in CT Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="419" to="434" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivative-based search Methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="968" to="982" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="575" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hoo-Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural decision forests for semantic image labelling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">`</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning-based feature representation for AD/MCI classification</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2013</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="583" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Active scheduling of organ detection and segmentation in whole-body medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2008</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Axel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>SzÃ©</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="page" from="313" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast free-form deformation using graphics processing units</title>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="278" to="284" />
			<date type="published" when="2010">6// 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">STEPS: similarity and truth estimation for propagated segmentations and its application to hippocampal segmentation and brain parcelation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keihaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="671" to="684" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="903" to="921" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-local statistical label fusion for multi-atlas segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Asman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="194" to="208" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An automatic multi-atlas segmentation of the prostate in transrectal ultrasound images using pairwise atlas shape similarity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nouranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Spadinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salcudean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2013</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic hippocampus segmentation of 7.0 Tesla MR images by combining multiple atlases and auto-context models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="335" to="345" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deformable segmentation via sparse representation and dictionary learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1385" to="1396" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A nonparametric method for automatic correction of intensity nonuniformity in MRI data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Zijdenbos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prostate segmentation by sparse representation based classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="6372" to="6387" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sinno Jialin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>A Survey on Transfer Learning</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transfer Learning Improves Supervised Image Segmentation Across Imaging Protocols</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Opbroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Vernooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1018" to="1030" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-time ultrasound transducer localization in fluoroscopy images by transfer learning from synthetic training data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mountney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ionasec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1320" to="1328" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transfer learning method using multi-prediction deep Boltzmann machines for a small scale dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kozuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision Applications (MVA), 2015 14th IAPR International Conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="110" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facilitating Image Search With a Scalable and Compact Semantic Mapping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1561" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
