<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Webly-supervised Fine-grained Visual Categorization via Deep Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Webly-supervised Fine-grained Visual Categorization via Deep Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12268FFB6AFCBA6FA18275DEFAFD4171</idno>
					<idno type="DOI">10.1109/TPAMI.2016.2637331</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2637331, IEEE Transactions on Pattern Analysis and Machine Intelligence SUBMITTED TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2637331, IEEE Transactions on Pattern Analysis and Machine Intelligence SUBMITTED TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 HEAD CNN BODY CNN BBOX CNN</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained visual categorization</term>
					<term>part-based model</term>
					<term>domain adaptation</term>
					<term>webly-supervised learning</term>
					<term>semi-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning visual representations from web data has recently attracted attention for object recognition. Previous studies have mainly focused on overcoming label noise and data bias and have shown promising results by learning directly from web data. However, we argue that it might be better to transfer knowledge from existing human labeling resources to improve performance at nearly no additional cost. In this paper, we propose a new semi-supervised method for learning via web data. Our method has the unique design of exploiting strong supervision, i.e., in addition to standard image-level labels, our method also utilizes detailed annotations including object bounding boxes and part landmarks. By transferring as much knowledge as possible from existing strongly supervised datasets to weakly supervised web images, our method can benefit from sophisticated object recognition algorithms and overcome several typical problems found in webly-supervised learning. We consider the problem of fine-grained visual categorization, in which existing training resources are scarce, as our main research objective. Comprehensive experimentation and extensive analysis demonstrate encouraging performance of the proposed approach, which, at the same time, delivers a new pipeline for fine-grained visual categorization that is likely to be highly effective for real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE increasing popularity of deep learning methods has made it apparent that data volume and handling are critical issues in object recognition. Although manual annotation of large-scale datasets <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref> has been usefully applied in various applications <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b40">[41]</ref>, the timeconsuming human labeling effort required argues against using this approach as a scalable solution to further improve recognition performance. Therefore, there is increasing interest in directly learning visual representations from web data <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>Web-scale learning has recently gained traction for its ability to model "everything about anything" <ref type="bibr" target="#b14">[15]</ref>, especially concepts not covered in the pre-defined vocabulary of existing datasets. However, with respect to specific tasks, webly-supervised methods usually struggle to compete with contemporary methods that employ extensive human supervision <ref type="bibr" target="#b8">[9]</ref>.</p><p>We argue that one crucial reason for this performance gap is due to the inadequate use of related knowledge available in existing manually labeled datasets. Due to the scalability issue, the object recognition algorithms adopted in webly-supervised scenarios tend to be rather simple. Instead, data mining algorithms such as bootstrapping <ref type="bibr" target="#b11">[12]</ref> and query expansion <ref type="bibr" target="#b53">[54]</ref> are often used to reduce semantic bias and data noise in web data. Therefore, to boost performance, it might be reasonable to extract powerful perceptual representations from existing manually labeled datasets using sophisticated object recognition algorithms.</p><p>• Z. Xu is with Shanghai Jiao Tong University and University of Technology Sydney. E-mail: xz3030@sjtu.edu.cn • S. Huang and D. Tao are with University of Technology Sydney.</p><p>• Y. Zhang is with Shanghai Jiao Tong University.</p><p>Then, this knowledge could be exploited to facilitate weblysupervised learning via some kind of knowledge transfer procedure.</p><p>Following this principle, our solution is to transfer as much as knowledge possible from existing manually labeled datasets to guide web-scale learning and then optionally transfer back the learned representations to further improve performance using the original datasets. Implemented as a semi-supervised framework, we adopt a unique design that utilizes stronger annotations on individual images in the labeled dataset but at a much smaller scale. We propose that this is more reasonable than the standard label collection process; compared to computer algorithms, humans are better at performing explicit labeling tasks than repetitive ones that aim to improve scalability.</p><p>The proposed strategy has several advantages. First, the semi-supervised method acts as a bridge between the webly-supervised paradigm (which increases scalability) and the counterpart object recognition methods (which introduce more powerful feature representations). By exploiting knowledge using more sophisticated object recognition algorithms on stronger supervision, each web image carries more explicit knowledge and, therefore, greater information. Second, as extensively discussed elsewhere, the main problem with using existing datasets to initialize weblysupervised learning is that they introduce data bias when constructing manually-labeled datasets <ref type="bibr" target="#b39">[40]</ref>. The advantage of employing strong supervision is that the additional annotations such as object part definitions or interpretable attributes, introduce a more reliable source of transferred knowledge. These definitions are generally more closely related to common knowledge but are inheritably shared between different categories. As a standalone measure, they Fig. <ref type="figure">1</ref>. The proposed semi-supervised method via web data. A strongly supervised dataset is introduced to "teach" web images how to learn properly. make it possible for the selected web images to be simultaneously diverse and precise.</p><p>We investigate fine-grained visual categorization (FGVC) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b59">[60]</ref>, a visual task starving for extensive training data, to demonstrate the effectiveness of this strategy. Given the goal of classifying objects at the subordinate level, data collection in FGVC problems is naturally difficult because expert knowledge and detailed part-level annotations are needed. As a result, existing datasets <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b55">[56]</ref> are usually relatively small in scale but rich in annotations, properties well suited to the proposed semi-supervised strategy. In particular, as shown in Fig. <ref type="figure">1</ref>, we propose a warm-start FGVC scheme using web data, which learns robust deep convolutional feature representations and employs detailed object part annotations in a unified framework while overcoming the lack of training data with the help of weakly supervised web images. Experimental results reveal a significant improvement over state-of-the-art methods on the FGVC CUB-200-2011 benchmark <ref type="bibr" target="#b49">[50]</ref>, highlighting the benefits of the proposed strategy.</p><p>Our contributions are as follows. 1) We propose a novel semi-supervised strategy for web-scale learning that exploits knowledge from existing datasets with strong supervision and show that it significantly improves standalone weblysupervised methods through effective knowledge transfer.</p><p>2) We implement this strategy in the context of FGVC, which, in combination with a sophisticated strongly supervised algorithm, delivers state-of-the-art performance on benchmark datasets. 3) We propose a new, strongly supervised fine-grained recognition algorithm that extends part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> but with a significantly different feature training strategy. 4) We present in-depth experimental results and analysis of the proposed method including extensive ablation studies and investigations of the effect of different CNN architectures, web image scales, and multiple forms of knowledge transfer. 5) Several practical issues with respect to generalizing the proposed method to various applications are discussed, including the ability to learn with limited numbers of training samples and dealing with novel categories. 6) The proposed method is organized as an end-to-end FGVC pipeline containing strongly supervised data collection, algorithm design, and web-scale learning, which is highly applicable to real-world systems.</p><p>The remainder of the paper is organized as follows. Related works are reviewed in Section 2. We detail the strongly supervised fine-grained categorization algorithm in Section 3. The proposed semi-supervised method is described in Section 4. Detailed performance studies and analyses are presented in Section 5, and we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Why Fine-Grained Visual Categorization?</head><p>We consider FGVC in our analysis of the proposed semisupervised and webly-supervised learning algorithm, in part because of the effectiveness of part-based methods and lack of strongly supervised training data for FGVC, as discussed above. Furthermore, we find that the properties of web images obtained for FGVC are noticeably different from those in generic object recognition, leading to some interesting discoveries when conducting webly-supervised learning.</p><p>Noise Distribution. In general, web data noise can be classified into two categories: label flip noise, where a sample is mislabeled from category A into category B, and outlier noise, in which irrelevant data are retrieved. In practice, most of the noise in webly-supervised FGVC falls into the outlier category as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. The remaining label flip noise, however, needs to be accounted for by experts using some kind of active learning approach.</p><p>Data bias. It is argued that webly-supervised learning suffers from visible domain differences between the target domain of test images and the source domain of web images. For example, the top images returned from the Google image search engine usually contain a single object in the center with a clean background and a canonical viewpoint, which can be significantly different from standard useruploaded images. Nevertheless, due to the specificity of FGVC tasks, the associated domain bias is usually much lower since the names of fine-grained categories seldom represent abstract concepts. Meanwhile, test images for FGVC usually also follow similar object-in-the-center priors (Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2(b)).</head><p>Data scale. The supply of web data is often believed to be nearly endless. However, this may not always hold true for all subordinate categories. For example, the bird category "Crested Auklet" only has 243 results on Flickr, which makes every image important for classifier learning.</p><p>These findings suggest that, for webly-supervised FGVC problems, the focus should be on removing outlier noise and better exploiting each training sample. Data bias plays a relatively minor role in system performance. We have, therefore, designed our method taking these assumptions into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Webly-supervised learning. There are two main branches of webly-supervised learning. The first is learning visual concepts directly from the web, which exploits the fact that it is possible to find resources on nearly every single concept on the internet <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Methods belonging to this category usually collect a large image pool from image search engines and then perform a filtering operation to remove noise  and discover visual concepts <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b50">[51]</ref>. The second branch attempts to boost existing object recognition task performance using web resources <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Mostly implemented as semi-supervised frameworks, they can be modeled as "growing" approaches <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b53">[54]</ref> that first generate a small group of labeled seed images and then enlarge the dataset from these seeds via some kind of iterative "self training" <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Our method naturally falls into the second category.</p><p>Fine-grained visual categorization. Of the various strategies used in FGVC, two have proven to be the most effective. Specifically, as stated by Rosch et al. <ref type="bibr" target="#b42">[43]</ref>, object parts principally define basic-level categories, whereas the unique properties of these parts distinguishe subordinate-level categories. This encourages the use of part-based algorithms that rely on localizing object parts and assigning detailed attributes to them. Various methods have been used to define object parts including unsupervised patch discovery <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b56">[57]</ref>, human-in-the-loop methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and direct reliance on strongly supervised datasets with part-level annotations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b58">[59]</ref>.</p><p>The second strategy is to introduce more discriminative feature representations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b59">[60]</ref>, a particularly attractive approach given the recent success of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref> in visual recognition. By employing deep feature CNN extractors pre-trained on large datasets (such as ImageNet <ref type="bibr" target="#b12">[13]</ref>) and domain-specific fine-tuning approaches, considerable improvements can be made in a wide range of image classification and detection tasks including fine-grained categorization <ref type="bibr" target="#b40">[41]</ref>.</p><p>The latest developments in FGVC have been in learning with less supervision <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b52">[53]</ref>; that is, only requiring object-level annotations or even image-level labels during training. Various studies have shown that weakly supervised methods are comparable to contemporary strongly supervised routines on benchmark datasets. However, it is still unclear whether these methods are generalizable to noisy web data.</p><p>Krause et al. <ref type="bibr" target="#b28">[29]</ref> also explored the idea of weblysupervised fine-grained recognition and reported surprisingly good results by directly training CNNs on large and noisy web data. In contrast, our method represents a way of improving the benefit gained by introducing each web image by transferring knowledge from existing strongly supervised datasets, which is likely to be complementary to <ref type="bibr" target="#b28">[29]</ref>. Domain adaptation. Our method is also related to the strategy of training with auxiliary data sources such as in domain adaptation <ref type="bibr" target="#b22">[23]</ref> for heterogeneous data sources and incremental learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b53">[54]</ref> for homogeneous ones. Some widely used object recognition approaches, such as the use of pre-trained CNNs <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b40">[41]</ref> and category-independent object proposals <ref type="bibr" target="#b47">[48]</ref>, can also be regarded as special cases of this strategy.</p><p>Our method is strongly motivated by Hoffman et al. <ref type="bibr" target="#b22">[23]</ref>, who exploited joint training over both weak and strong labels for detector learning. Here, our unique contribution is the employment of strongly supervised data that provides extensive and detailed information to guide the learning process on weakly supervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KNOWLEDGE EXTRACTION FROM STRONGLY SUPERVISED DATASETS</head><p>The proposed semi-supervised method contains: (i) an initialization step to extract robust perceptual representations from strongly supervised datasets, and (ii) a model updating step that uses noisy web data via effective knowledge transfer (Fig. <ref type="figure" target="#fig_2">3</ref>). In this section, we first introduce the strongly supervised algorithm.</p><p>We employ part-based methods for FGVC (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>), which have shown considerable success in the literature, and adopt deep CNNs as the feature representation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Whilst our method is agnostic to the specific form taken by part annotations and CNN architectures, here we study the same problem statement as part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> but make several significant modifications in our implementation to better deal with the lack of training data in strongly supervised datasets.</p><p>The resulting domain-specific knowledge acquired from strong supervision is given in multiple forms including deep convolutional feature representations, precise part detectors, and robust object classifiers. They are further employed to provide a reliable initialization for the whole semisupervised framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We start with a strongly supervised dataset S, in which ground-truth bounding box annotations are provided not only for entire objects p 0 but also for a set of n semantic parts {p 1 , p 2 , ..., p n }. Assume that there are K fine-grained categories in the dataset. Selective search <ref type="bibr" target="#b47">[48]</ref> is used to extract category-independent object proposals. Typically, 1000-2000 region proposals are generated per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Representations</head><p>The core idea behind part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> is to customize deep convolutional features for each object part so that the resulting feature representations carry specific information that distinguishes objects from the part level. To do so, in the original implementation <ref type="bibr" target="#b58">[59]</ref>, a CNN model was trained on ground-truth crops of the whole object bounding box and each of the part bounding boxes. However, since the scale of the strongly supervised dataset is supposed to be relatively small, it is argued that training CNNs only on ground-truth crops will easily overfit the learning objective and thus compromise the results.</p><p>To overcome this problem, we augment the positive set by adding object proposals with high intersection-overunion (IoU) over the ground-truth bounding boxes, also known as a data jittering approach. Specifically, for each of the object parts p i (or the whole object p 0 ), our goal is to train part-specific deep convolutional features φ (i) (x) on the extracted region proposals.</p><p>Starting with a CNN pre-trained on ImageNet <ref type="bibr" target="#b29">[30]</ref>, we replace the CNN's ImageNet-specific 1000-way classification layer with a randomly initialized (K + 1)-way layer that accounts for all the fine-grained categories and also a background class. Object proposals with IoU ≥ 0.5 over the ground-truth bounding boxes are treated as positive examples for that box's class, while the others are regarded as the background. For each object proposal, the tight bounding box is dilated by m pixels (we use m = 16) to introduce contextual information, and all the pixels in the dilated region are warped into a fixed size of 227 × 227 pixels. The warped regions are then used as the input to fine-tune the network by stochastic gradient descent (SGD), starting at a learning rate of 0.001. As a result, the learned CNNs (we call them part-CNNs) carry specific domainspecific knowledge of fine-grained categorization but do not clobber the initialization through large-scale ImageNet pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object Part Detectors</head><p>For the case of testing with no available part-level annotations, the algorithm should be able to locate object parts automatically. Therefore, based on the fine-tuned CNNs, we further train a linear SVM with binary outputs to obtain the part detector for each part. Only ground-truth boxes are used as positive samples to improve detection. In our implementation, we train SVMs beyond features extracted from the fc7 layer of the CNNs and adopt a standard hard negative mining method <ref type="bibr" target="#b17">[18]</ref> to fit the training data into memory. We also adopt bounding box regression <ref type="bibr" target="#b20">[21]</ref> to further regularize the detected regions.</p><p>Denote {v 0 , v 1 , ..., v n } as the weights of detectors for whole object p 0 and n parts p i | n i=1 . For a region proposal x, the corresponding detector scores {d 0 , d 1 , ..., d n } are computed as</p><formula xml:id="formula_0">d i (x) = σ(v T i φ (i) (x)),<label>(1)</label></formula><p>where σ(•) is the sigmoid function and φ (i) (x) is the descriptor at location x according to the i-th part-CNN. It is worth noting a significant difference in Section 3.2 and 3.3's goals. The first aims to produce discriminative feature representations to classify fine-grained categories; therefore, training is conducted with fine-grained output labels, with a "soft" IoU threshold that introduces more training samples to prevent overfitting. Conversely, the latter step focuses on obtaining accurate object detection results. Therefore, when training a specific part detector, only the ground-truth regions of this part are used as positive samples. Meanwhile, to introduce robustness to factors including various object poses, scales, and occlusions, we want the part detector to be shared among all subordinate categories and thus ignore the object category labels in this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fine-grained Classifiers</head><p>The next step is to integrate the part-specific feature representations via part detection results and use them to train fine-grained classifiers. We consider a simple box constraint to ensure that the object parts do not fall outside the root bounding box. Although more complicated geometric constraints can be adopted <ref type="bibr" target="#b58">[59]</ref>, in reality we find that they only play a minor role in detection due the strength and robustness of our part detectors.</p><p>For an image I, let X = {x 0 , x 1 , ..., x p } be the predicted locations (bounding boxes) of an object and its parts, which are given during training but are unknown for both weakly supervised images and test images. The final feature representation is then denoted as</p><formula xml:id="formula_1">Φ(x) = [φ (0) (x 0 ), ..., φ (n) (x n )],</formula><p>where φ (i) (x i ) is the feature representation for part p i as the output of the fc7 layer of the i-th part-CNN. In addition, a one-versus-all linear SVM is trained for each fine-grained category. The classification score for an image I being class k is then calculated as:</p><formula xml:id="formula_2">s(I; k) = n i=0 w (i) k T φ (i) (x i ),<label>(2)</label></formula><p>where w</p><formula xml:id="formula_3">(i)</formula><p>k is the classifier weight for class k on features extracted from the i-th object part.</p><p>In summary, given n object parts, a root, and K finegrained categories to be classified, the initialization step obtains:</p><p>• n + 1 independently fine-tuned part-CNNs with (K + 1)-way classification layers as the initialized feature extractors. We use the fc7 layer to obtain a 4096-dimensional feature vector φ (i) for each part p i .</p><p>• n+1 part (or object) detectors. Each part (or root) p i is associated with a detector d i based on the respective CNN feature extractor φ (i) .</p><p>• K(n + 1) sets of classification model weights, with each w</p><formula xml:id="formula_4">(i) k ∈ R 4096×1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KNOWLEDGE TRANSFER TO WEAKLY SUPER-VISED DATASETS</head><p>The second part of the proposed method is a model updating step that transfers learned knowledge from the smaller, strongly supervised dataset S to a larger collection of images acquired from the web (termed W). Images in W are directly collected from the internet without any human labeling effort. This has two important consequences: 1) the web dataset can contain many more images than the strongly supervised dataset, say N W and N S , which largely scale the learning algorithm; and 2) the web dataset is weakly supervised; images are only associated with image-level labels, which might also be incorrect due to label noise and outliers.</p><p>Motivated by these discoveries, the proposed model updating step employs the results from Section 3 for initialization to introduce part-based domain knowledge then re-trains CNN features and object classifiers at a much larger scale. A carefully designed noise removal strategy is utilized for robustness. We also introduce an iterative strategy to augment the training set with web images and gradually update the model parameters, exemplifying the idea of selfpaced learning <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Re-training</head><p>Given initialized models learned from the strongly supervised dataset, the proposed model re-training scheme updates both feature representations φ and model parameters w. For this, a joint optimization algorithm is conducted on the combination of strongly supervised dataset S and weakly labeled data W. The overall objective function is defined as a sum of sub-problems on each object part:</p><formula xml:id="formula_5">min w,φ n p=0 L(w (p) , φ (p) ),</formula><p>where</p><formula xml:id="formula_6">L(w (p) , φ (p) ) = λ • Ω(w (p) ) + 1 N W I∈W q (p) I • l(y I , max xp∈X I w (p) y I T φ (p) (x p )) + 1 N S I∈S l(y I , w (p) y I T φ (p) (x p )).<label>(3)</label></formula><p>The objective function contains three terms: a traditional l2-norm regularizer and two terms capturing the loss on weakly supervised images in W and strongly supervised images in S, respectively. Here, w (p) k denotes classifier weights of the k-th category. For each image I, a softmax loss l is computed based on the ground-truth label y I ∈ [1, ..., K] and the predicted result given feature representation φ (p) (•) and part location x p . For the web images, we introduce a multi-instance formulation in which X I is the set of candidate bounding boxes and q (p) I denotes an indicator of whether the detected region of the p-th part in weakly-supervised image I is selected to augment the training set (detailed in Section 4.2).</p><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the re-training scheme starts from an initialization on the strongly supervised dataset, followed by object part detection in weak images, CNN feature retraining, and object classifier training.</p><p>Part Discovery. The first form of knowledge transfer originates from the object part detectors. Since images in the web dataset are only associated with image-level labels, they are inherently weakly supervised and carry less information. Using the detectors trained on the strongly supervised dataset S, we can introduce part-level "supervision" to web images by automatic part discovery.</p><p>After obtaining detection scores for all the parts, we adopt the box constraint restriction to introduce geometric relationships between object parts. The detected locations X * = {x 0 , x 1 , ..., x n } are given as:</p><formula xml:id="formula_7">X * = arg max X n i=1 c x0 (x i ) n i=0 d i (x i ),<label>(4)</label></formula><p>where c x (y) = 1, if region y falls outside x by at most 10 pixels 0, otherwise Feature Updating. A key motivation of the proposed method is to generate more powerful deep convolutional feature representations using a larger scale of training data. Given the discovered part patches from the weakly supervised dataset, we obtain an augmented dataset to further refine-tune part-CNNs. The same CNN architecture as Section 3.3 is employed, once again with the (K + 1)-way fc8 layer randomly initialized and the previous layers' filter weights kept fixed. All region proposals with IoU ≥ 0.5 over the detected part bounding boxes are cropped, dilated, warped and then fed into the CNN architecture as the input.</p><p>Classifier Updating. Having updated the feature representations and detected part locations on weakly supervised images, the model parameters w are jointly retrained on the strong and weak datasets to obtain the final object classifiers. Inspired by <ref type="bibr" target="#b22">[23]</ref>, we define a multi-instance learning (MIL) formulation <ref type="bibr" target="#b54">[55]</ref> that includes bags defined on both types of images. Specifically, for each image in the web dataset, the top 10 locations of the root bounding box are detected, each of which is regarded as an instance in MIL. The objective function ( <ref type="formula" target="#formula_6">3</ref>) is rewritten as:</p><formula xml:id="formula_8">L(w) = λΩ(w) + 1 N S I∈S l(y I , w T y I Φ(x)) + 1 N W I∈W l(y I , max x∈X I w T y I Ψ(x)),<label>(5)</label></formula><p>where w = [w (0) , ..., w (n) ] denotes the joint model classifier;</p><formula xml:id="formula_9">Φ(x) = [φ (0) (x 0 ), ..., φ (n) (x n )] is the part-based R-CNN</formula><p>feature representation for a strongly supervised image; and</p><formula xml:id="formula_10">Ψ(x) = [q (0) I φ (0) (x 0 ), ..., q (n) I φ (n) (x n )]</formula><p>is the feature representation for a weakly supervised image, in which a part filter p is set to a zero vector if the indicator q (p) is zero. The objective can be solved by standard MIL methods with only slight modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-paced Learning and Noise Removal</head><p>Self training approaches such as bootstrapping <ref type="bibr" target="#b41">[42]</ref> and curriculum learning <ref type="bibr" target="#b1">[2]</ref> are particularly useful when there is considerable uncertainty in the training data, such as in webly-supervised learning problems. Here we adopt self-paced learning <ref type="bibr" target="#b30">[31]</ref>, which incrementally improves the discriminative power of the model by processing the samples in a meaningful order. The underlying principle of self-paced learning is to learn easy samples first and then gradually introduce more complex samples to the learning algorithm. As a result, the algorithm gradually "grows up" and becomes more robust to challenge.</p><p>In the context of generating part patches from weakly supervised images, the easiness criterion can be defined in two ways: (i) a sample should be selected if we are confident that the detected localization is correct; or (ii) a sample should be selected if it is easy to predict its true label. We argue that, in our task, adopting either of these strategies alone is unlikely to be optimal. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, images that are correctly classified do not always generate valid part patches due to occlusion effects and the absence of a particular object part. On the other hand, there is no clear boundary to perfectly separate "good" from "poor" detections with respect to detection scores.</p><p>Therefore, we propose a "two threshold" strategy that combines two types of transferred knowledge -detection scores and classification results -to select "easy" part patches. The basic idea is to flexibly adjust the threshold of "good" detections by setting a loose condition on the correctly classified images and requiring harsher terms for misclassified images. Specifically, suppose we are at iteration t in selfpaced learning, the criterion of whether a part patch x is selected to augment the training set is determined as an indicator q (i)</p><formula xml:id="formula_11">I = I(d i (x) &gt; λ (t)</formula><p>), where</p><formula xml:id="formula_12">λ (t) = λ pos , if ỹI (t) = y I λ neg , if ỹI (t) = y I ,<label>(6)</label></formula><p>Here, y I is the label of image I and ỹI is the predicted label obtained by part-based R-CNN classifiers. We set two thresholds for detection scores d i (x), where λ pos &lt; λ neg .</p><p>At each iteration, this strategy renews the definition of "easy" samples via the updated feature representations and model parameters (note that the classification results ỹI (t) are changed). This exemplifies the idea of self-paced learning. Note that this design also takes the label noise in web images into consideration. Samples with low classification and detection scores are much more likely to be outliers and are dropped by the proposed strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Novel Category Analysis</head><p>The proposed method can also be applied to cases in which the web dataset contains novel categories that are not listed in the strongly supervised dataset. In particular, the classifier updating function ( <ref type="formula" target="#formula_8">5</ref>) is modified to:</p><formula xml:id="formula_13">L(w) = λΩ(w) + 1 N W I∈W l(y I , max x∈X I w T y I Ψ(x)).<label>(7)</label></formula><p>At first glance, <ref type="bibr" target="#b6">(7)</ref> appears to be similar to the standard multi-instance learning function for weakly supervised problems. However, the proposed method is different in three important aspects:</p><p>1) We introduce the definition of multiple object parts via knowledge transfer. On the contrary, it is not straightforward to obtain a set of object parts as different "positive" classes in multi-instance learning.</p><p>2) We provide robust identification of noisy images via knowledge transfer. This cannot happen in weblysupervised learning methods without employing existing human knowledge.</p><p>3) The CNN features learned for categorizing novel categories are initialized by a related classification task of similar categories (mainly as different subordinate categories) thus providing additional information that may facilitate the new learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND ANALYSIS</head><p>To demonstrate the effectiveness of the proposed method from several different perspectives, our experiments are organized as follows. We first present the results of our approach applied to a benchmark dataset and compared to state-of-the-art methods. This is followed by a detailed ablation study to demonstrate the impact of each step and different forms of transferred knowledge. We then present several practical considerations for generalizing the proposed method to various real-world applications. Finally, we discuss the mechanisms underlying the proposed method through a visualization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Results</head><p>The first set of experiments were conducted on the widely used FGVC benchmark, the Caltech-UCSD Birds dataset (CUB-200-2011) <ref type="bibr" target="#b49">[50]</ref>. We first tested a naïve implementation of the proposed method using a smaller CNN architecture and only one self-paced learning iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Dataset and Implementation Details</head><p>The CUB-200-2011 dataset contains 11,788 images of 200 types of bird, with roughly 30 images per category used for training. The dataset is strongly supervised, i.e., images are associated with annotations including image-level labels, object bounding boxes, and part landmarks. Following the protocol in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, we exploited the location annotations of two semantic parts -head and body -along with whole object bounding boxes for the part-based models.</p><p>A weakly supervised dataset was then collected from the web. Images were obtained from Flickr 1 using image searches with the names of the 200 bird species as queries. The top 100 images were downloaded for each category. The image list was sorted by upload time to ensure no overlap between the acquired images and the test images in the dataset. No further manual filtering process was conducted on the web dataset. The downloaded images only possessed image-level labels, which were not always correct due to the ambiguity of the query words and label noise.</p><p>The open-source package Caffe <ref type="bibr" target="#b24">[25]</ref> was used to extract deep features and fine-tune part-CNNs from AlexNet <ref type="bibr" target="#b29">[30]</ref>. The last fully connected layer fc7 in the CNN architecture was used to train detectors and object classifiers. Our code for training detectors and classifiers was modified based on <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b58">[59]</ref>, respectively.</p><p>For noise removal, the two thresholds introduced in Section 4.2 were defined as λ pos = σ di (neg) and λ neg = σ di (pos), where di (•) is the average detection score of part patches over correctly or incorrectly classified images, with σ empirically set to 0.5. Different σs were tested, but performance was similar across a reasonably wide range. The resulting threshold λ pos is guaranteed to be lower than λ neg because successfully detected part patches always contribute to classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Detection Results and Analysis of the Discovered Part Patches</head><p>One of the key assumptions in our method is that the detectors learned from strongly supervised data can accurately  locate object part patches in weakly supervised web images. Therefore, our analysis started with the evaluation of detection results and studying the discovered part patches. Quantitative detection results were measured in terms of the "percentage of correctly localized parts" (PCP) on the test set. A part patch was marked as correctly localized if the predicted bounding box had IoU ≥ 0.5 with the ground-truth. The learned part detectors were reasonable, achieving greater than 70% PCP for all parts (Table <ref type="table" target="#tab_0">1</ref>). The improvement over part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> is due to the additional negative mining process and from assigning the background as the (K + 1)-th category for fine-tuning part-CNNs (as specified in <ref type="bibr" target="#b19">[20]</ref>). Examples of detected patches from the web dataset are shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Classification Results</head><p>A comparison of the accuracies of the proposed method and state-of-the-art methods on CUB-200-2011 is shown in Table <ref type="table" target="#tab_1">2</ref>. Unlike most other literature on this dataset, we consider it more realistic to consider the birds' bounding boxes as unknown during testing. In this challenging setting, we achieved an accuracy of 84.6% with AlexNet, outperforming the state-of-the-art. It is worth noting that our strongly supervised method without web images already outperformed part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> by 5%, demonstrating the importance of the proposed design in feature training. Although our method requires additional training data by collecting weakly supervised images from the web, this data acquisition process is easy to implement and requires no additional human labeling effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies and Scalability</head><p>Since our method incorporates multiple steps to boost classification performance, we next analyzed the impact of each step through detailed comparison with the baselines shown </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Train Anno. Accuracy(%)</head><p>Constellation <ref type="bibr" target="#b44">[45]</ref> n/a 68.5 Attention <ref type="bibr" target="#b52">[53]</ref> n/a 69.7 Weak-Sup <ref type="bibr" target="#b62">[63]</ref> n/a 75.0 Fused <ref type="bibr" target="#b61">[62]</ref> n/a 76.0 Bilinear CNN <ref type="bibr" target="#b34">[35]</ref> n/a 78.1 Deep Filter <ref type="bibr" target="#b60">[61]</ref> n/a 80.3 Without part <ref type="bibr" target="#b27">[28]</ref> bbox 73.7 Part R-CNN <ref type="bibr" target="#b58">[59]</ref> bbox+parts 73.9 PoseNorm CNN <ref type="bibr" target="#b6">[7]</ref> bbox+parts 75.7 SPDA-CNN <ref type="bibr" target="#b57">[58]</ref> bbox+parts   <ref type="table" target="#tab_3">3</ref>. We also studied the impact of more advanced settings including deeper CNN models, more web images, and more self-paced iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Feature Perspective</head><p>Our first comparisons revealed that improving feature representations by fine-tuning CNNs on domain-specific data significantly contributed to classification accuracy. Directly exploiting an ImageNet pre-trained CNN as the feature extractor achieved 68% accuracy. Fine-tuning part-CNNs on the bird training set improved this result by a large margin to 79%. Further, by augmenting the part patches by performing part discovery on the weak dataset and re-finetuning the CNNs, we further improved the classification accuracy to 81%. These results show that the larger amount of training data improves the discriminative power of the learned CNN representation. Denoising on web images achieved a 2% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Model Perspective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It is argued that employing additional training data can boost classification results by increasing data diversity in</head><p>the training examples even without using CNN features. We studied this factor by re-training classifiers on the augmented dataset and comparing the results to those trained with strongly supervised training data alone. When the feature representations were fixed (as in traditional features such as SIFT), the performance improvement was trivial (∼ 1%) compared to re-fine-tuning the CNN features. This reveals an interesting phenomenon: feature representation plays a greater role in fine-grained object recognition than model training. When the multi-instance learning step in weakly supervised web images was not performed, the performance dropped slightly by ∼ 1%. The proposed method of training classifiers on the re-fine-tuned part-CNN features finally delivered 84.6% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Localization Accuracy</head><p>The part localization accuracy also had a profound impact on the final classification results. Although our detectors were reasonably accurate for detecting object parts, an average 3% difference remained between classification results using predicted bounding boxes and the oracle method, which casts an upper classification performance bound by employing ground-truth part annotations during both training and testing. It is worth noting that our final classification result of 84.6% after introducing weakly supervised samples exceeded even the upper bound accuracy of 82.1% when using strongly supervised training data alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Are Deeper Models Helpful?</head><p>It is well known that the CNN model architecture has a large impact on object recognition performance. We investigated this issue by testing our method using the VGG16 architecture <ref type="bibr" target="#b45">[46]</ref> and compared the results with AlexNet. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, using a deeper model (VGG16) was better than using shallower models (AlexNet), as expected.</p><p>In particular, the VGG model was more effective for part detection; the performance boost in the classification accuracy was relatively smaller. Due to the effectiveness of deeper CNN architectures, results in subsequent sections are reported using VGG16 models unless otherwise stated. Meanwhile, we performed control experiments using only the top-1 detection results on web images (without the multi-instance learning method in the final classification step) to accelerate learning and to study the effects of other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Are More Web Examples Helpful?</head><p>Data scale is a crucial factor in webly-supervised learning. We investigated this impact by incrementally increasing the number of web images used for each category from 100 to 500. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, in general, the classification accuracy improved steadily with the use of more training images. Note that the proposed semi-supervised method consistently outperformed the baseline, in which only image-level 0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Red lines show results for the proposed method; blue lines indicate the baseline webly-supervised method using only image labels and web images. We also show results using only the strongly supervised dataset as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Effect of self-paced learning at multiple iterations. The total number of samples selected at each iteration is shown together with the accuracy, where "k" denotes thousands.</p><p># of iteration 0 1 2 3 # of samples 6k 18k 20k 23k Accuracy 81.08 85.67 86.12 86.33 labels were used to train CNN models at the web scale, with the margin between the two methods finally approaching a nearly constant number. This phenomenon indicates that the improvement provided by additional part-level knowledge is consistent and significant regardless of the number of web images used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Is Self-paced Learning Helpful?</head><p>Self-paced learning is supposed to improve performance by processing the samples in a meaningful order. To evaluate this strategy, we used the same web dataset with 100 images per category but conducted multiple iterations of self-paced learning to analyze the impact on performance. At each iteration, more samples were added to the training set by re-defining the criterion of "easy" samples, leading to a solid improvement in classification accuracy. Specifically, as shown in Table <ref type="table">4</ref>, the accuracy of iteration #3 was nearly identical to that of training with more data (200 images per category; see Fig. <ref type="figure" target="#fig_6">7</ref>), demonstrating that self-paced learning can further increase the value of each weakly supervised image, especially when the number of web images available for each fine-grained category is limited. As a comparison, by explicitly tuning parameter λ in Eqn. <ref type="bibr" target="#b5">(6)</ref>, the best result we can achieve was 85.9% using the same number of web images. However, since more iterations naturally incur additional computational cost, one should determine the number of iterations empirically according to the specific application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Forms of Knowledge Transfer</head><p>Recall that one of our key assumptions is that by exploiting detailed annotations from existing datasets, stronger partlevel knowledge can be extracted and transferred to weakly  supervised web images, leading to better performance compared to traditional webly-supervised methods. We therefore conducted a set of experiments to study how different forms of knowledge transfer contribute to performance. The comparison with respect to classification accuracy is shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">No Knowledge Transfer</head><p>The simplest way to conduct webly-supervised learning is to train CNN models using images and labels acquired directly from the web without any manual labeling. Since this provides no additional annotations such as object bounding boxes or part landmarks, we first considered the most straightforward method by fine-tuning CNNs on imagelevel labels. Despite its simplicity, this approach has been the main one used to date <ref type="bibr" target="#b28">[29]</ref>, mainly considering the scale of web data. In our experiment, this approach delivered 71.72% accuracy, about ten percent lower than our partbased algorithm trained on the strongly supervised dataset. Due to the weakly supervised nature of web images and the difficulty of fine-grained recognition tasks, the profit of introducing each web image is much lower than images with rich annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Image-level Knowledge Transfer</head><p>Image-level labels provide the basic way to transfer knowledge from strongly supervised datasets. We investigated this strategy by training perceptual representations on the combination of strongly supervised data and weakly supervised web images. Again, CNNs were fine-tuned on imagelevel labels. The accuracy was 80.12%, about 9% higher than using web images alone and comparable to our baseline strongly supervised algorithm. When more web images were employed (500 images per category), the performance boost provided by adding human labeled training data was smaller (86.29% vs. 83.93%). However, this was still a simple but effective way of improving the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Part-level Knowledge Transfer</head><p>The proposed semi-supervised method transfers part-level knowledge to web data and thus delivers additional performance improvements. In particular, our method achieved 85.67% accuracy by re-fine-tuning deep convolutional features and training fine-grained classifiers using the augmented dataset described in Section 4, a 5% improvement ). This interesting phenomenon indicates that by introducing part-level domain knowledge from existing datasets, each web image is 5-times more valuable than its original when the proposed knowledge transfer strategy is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Generalizability</head><p>Having shown that the proposed method is highly effective on FGVC benchmarks, we further discuss the generalizability of the proposed method in real-world applications. We examine three scenarios: 1) only a very restricted number of strongly supervised samples are introduced for initialization; 2) novel categories are seen in the web images; and 3) applications other than animal recognition are explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Few Training Samples</head><p>One of the advantages of the proposed method is that it can improve the results of webly-supervised learning using a relatively small set of strongly supervised images. Therefore, it is intuitive to ask: how many strongly supervised training samples must be labeled to produce significant improvements?</p><p>To study this problem, we selected a subset of the CUB-200-2011 training set containing only five images per category, resulting in a total of 1000 training images. Considering the difficulty of fine-grained recognition, training on this dataset alone was not expected to be effective. In practice, using our strongly supervised FGVC method, the classification accuracy was only 37.9%.</p><p>However, even in this extreme case, the associated strong supervision can also be effectively transferred to weakly supervised web images. Specifically, we again trained object part detectors as described in Section 3, but only using training examples in this subset. Although the subset contained a very restricted number of images in each subordinate category, this was still enough data to train object part detectors, since the definition of object parts is inheritably shared between categories. As shown in Table <ref type="table" target="#tab_5">5</ref>, compared to the classification results, the performance of learned part detectors was much closer to that trained on the whole CUB training set. By utilizing the discovered object and part patches, our method achieved an accuracy of 81.05% on the augmented dataset, significantly higher than training on strongly supervised data alone (37.88%), web images alone (71.72%), or the augmented dataset without part-level knowledge transfer (74.20%).</p><p>These results show that a small amount of explicitly labeled data can significantly facilitate webly-supervised learning through effective part-level knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Novel Categories</head><p>Fine-grained categories usually include an extraordinarily wide range of subordinate concepts. In practice, it is nearly impossible to enumerate all of the subordinate categories without omissions, especially considering that the concept list may be actively growing, e.g., with the release of new models by a car manufacturer. Therefore, a practical problem for fine-grained object recognition is how to generalize the learned perceptual representations to new categories.</p><p>We therefore investigated the possibility of transferring knowledge to unseen categories using the proposed method. In particular, we employed the knowledge learned from CUB-200-2011 to facilitate the bird species prediction task on the larger NAbirds dataset <ref type="bibr" target="#b48">[49]</ref>. The NAbirds dataset covers most bird species in North America, resulting in a finer scale of 555 categories. Although the dataset provides additional annotations including part annotations and bounding boxes, we only used the image-level labels in our experiments and borrowed the part-level information from the CUB dataset through knowledge transfer.</p><p>The detectors trained on CUB-200-2011 were again used to discover object parts in NAbirds. Based on the discovered part regions, we re-fine-tuned the deep convolutional feature representations by replacing the fc8 layer with a randomly initialized 556-way classification layer aware of the background. This fine-tuning process was initialized by the CNN learned using the CUB-200-2011 dataset; thus, the learned features carried additional information from the previous learning task.</p><p>Classification accuracies are shown in Table <ref type="table" target="#tab_6">6</ref>. Using no annotations during both training and testing, the proposed method easily outperformed the baseline method of finetuning CNN models without object parts. Our method also outperformed <ref type="bibr" target="#b48">[49]</ref>, in which the given part annotations were extensively leveraged, and the results of Simon et al. <ref type="bibr" target="#b44">[45]</ref>, who proposed an unsupervised part discovery method for fine-grained recognition. The part-level annotations in the smaller CUB dataset are clearly generalizable to different but related classification tasks and facilitate recognition using the proposed method. Experimental results including detection, classification, and noise removal using the three bird datasets are illustrated in Fig. <ref type="figure" target="#fig_8">9</ref>.</p><p>We also explored the applicability of this strategy on another FGVC benchmark -the Stanford Dogs dataset <ref type="bibr" target="#b26">[27]</ref>. The dataset contains 120 dog species, with 100 images per  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Clothing Attribute Prediction</head><p>To fully test generalizability, we next evaluated the proposed algorithm in other applications. We investigated clothing attribute prediction, the goal being to recognize fine-grained attributes such as V-shape and polo collar.</p><p>A web-scale clothing dataset <ref type="bibr" target="#b23">[24]</ref> harvested from several online shopping websites was used. Clothing attributes were parsed from surrounding texts of images followed by manually pruning. We selected two attributes (collar shape and clothing pattern) that were naturally related to human body parts (neck and chest, respectively) to evaluate our algorithm. Attributes containing few samples and the ambiguous "other" category were removed, resulting in about 200, 000 images for training and 40, 000 for testing.</p><p>Since the clothing dataset contains no part-level annotations, we took advantage of human pose estimation techniques <ref type="bibr" target="#b19">[20]</ref> and transferred the learned body part detectors to the clothing dataset. As shown in Table <ref type="table" target="#tab_7">7</ref>, our method was slightly better than baseline of training classifiers on image-level labels, demonstrating the benefit of introducing additional part-level knowledge. The performance boost was smaller than that seen in animal species recognition, mostly because: 1) the images in the clothing dataset have cleaner backgrounds and contained center-aligned objects; and 2) certain differences exist between the source domain of human pose estimation and the target domain of part detection on clothing images, making the knowledge transfer less effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Model Visualization</head><p>Finally, we provide a more intuitive and qualitative description of how our method works in practice. The fine-grained image classification procedure using the proposed method is shown in Fig. <ref type="figure" target="#fig_9">10</ref>. Given a test image (a) belonging to Green Kingfisher, detectors were first used to localize the object and its semantic parts, detailed in (b) and (c). As We solved this problem by introducing an auxiliary dataset of weakly supervised images collected from the web to augment the training data. As shown in (e), the new feature representations obtained by re-fine-tuning part-CNNs on the augmented training set improvef the discriminative power in this case, especially for the bird's head, even when only images in the strongly supervised dataset are employed to train the object classifiers. Naturally, inserting weakly supervised images into the training set also contributed to the classification process. Nearest neighbors shown in (f) indicate that there are a larger number of images similar to the test image in the web dataset, making the classification result more convincing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we describe a simple but effective method for webly-supervised learning that performs knowledge transfer from existing datasets containing detailed annotations. The proposed method achieves two desired properties in a unified framework, namely scalability (by employing largescale web images) and expertise (by introducing knowledge from sophisticated strongly supervised object recognition algorithms). The result is excellent recognition accuracy.</p><p>The extensive experiments and analysis reveal a number of important findings. <ref type="bibr" target="#b0">(1)</ref> The strong supervision available in existing datasets can be exploited to "teach" the weblysupervised learning procedure. (2) Even a small number of strongly supervised training samples significantly boosts performance compared to using only images from the web.</p><p>(3) The learned representations from strong supervision can easily be adapted to novel but related categories. (4) Weblysupervised learning is particularly effective for fine-grained visual categorization where existing datasets are usually small in scale but rich in annotations.</p><p>The proposed method also provides a general pipeline that is potentially useful in real-world fine-grained recognition systems. This pipeline has three steps. First, a reliable, strongly supervised dataset should be collected and annotated with detailed information such as part landmarks and attributes. Based on this dataset, the second step extracts domain-specific knowledge using effective strongly supervised recognition algorithms, which acts as a mentor for webly-supervised learning. Finally, for scalability, a large set of related images is collected from the web and the extracted knowledge is transferred to these web images. As a result, excellent recognition can be achieved with only a little human labeling effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Comparison of training images and web images for category "Cardinal". Notice that most of the data noise appears as outliers. Google vs Flickr for query "BIRD" Google vs Flickr for query "LAYSAN_ALBATROSS" (b) Data bias between Google and Flickr. Clearly fine-grained objects have lower bias than generic objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Demonstration of the properties of web images for fine-grained object recognition.</figDesc><graphic coords="3,181.31,49.54,103.13,76.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flowchart of the proposed algorithm. The employed strongly supervised algorithm for initialization is shown in the right, which provides multiple forms of domain knowledge to facilitate webly supervised learning procedure. The learning process could be conducted several iterations via self-paced learning.</figDesc><graphic coords="4,257.17,150.09,100.13,105.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Detection results on weakly supervised images. Green frames indicate the detected bounding box for the part "body". Image labels in the top two rows are correctly classified; the bottom two rows show cases in which classification has failed. Beyond the classification results, part patches in rows 1 and 3 are associated with high detection scores, while rows 2 and 4 have low detection scores. We propose to use both the classification and detection scores to select valid part patches and augment the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of detected part patches from web images selected as valid training patches. From top to bottom: whole object, head, body. The leftmost five columns show top-scoring detections, while the right two columns show patches with the lowest detection scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Demonstration of the impact of different CNN architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Number of web images per category used for training vs. classification accuracy.Red lines show results for the proposed method; blue lines indicate the baseline webly-supervised method using only image labels and web images. We also show results using only the strongly supervised dataset as reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Classification accuracy using different forms of knowledge transfer in multiple experimental settings. "Strong+Weak(100)" means training models using the original CUB-200-2011 dataset and a web dataset with 100 images per category. "Weak", "image", and "part" indicate the three different forms of knowledge transfer detailed in the main text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Visualization of detection and classification results on the CUB-200-2011 test set (rows 1-3), web images (rows 4-6), and the NAbirds dataset (rows 6-9). The last row in each group shows examples with inaccurate detection results.For classification results, green marks indicate successful cases, while red ones denote misclassifications. Object bounding boxes together with "head" and "body" are shown by rectangles in green, red, and blue, respectively. Specifically, part detections with low scores are not visualized. Note that some web images are regarded as noise by our method (blue signs). Viewing digitally with zoom is recommended.</figDesc><graphic coords="11,173.79,591.82,104.19,69.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Visualization of classification using the proposed method with a root and two parts: head and body. (a) Test image with a ground-truth label of 80. (b) Activation map for the three detectors. (c) Located part bounding boxes. The top 6 nearest neighbors for the detected parts from the training images are shown in (d)-(f). The original part-based R-CNN method using training data only misclassifies the test image into class 81, as shown in (d). Green boxes demonstrate the image patches of label 80, and red boxes for label 81. After re-fine-tuning part-CNNs with the augmented training set, the new feature representations guarantee that the test image is correctly classified. (e) Nearest neighbors from the strongly supervised training set only using the new feature representations. (f) Results after putting weakly supervised images into the training set. Yellow boxes indicate images in the weakly supervised dataset with label 80. (g) and (h) show typical training images from class 80 (Green Kingfisher ) and 81 (Pied Kingfisher ), respectively.</figDesc><graphic coords="13,391.03,165.20,59.85,59.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Part localization accuracy in terms of PCP on the CUB-200-2011 dataset.</figDesc><table><row><cell></cell><cell>BBox</cell><cell>Head</cell><cell>Body</cell></row><row><cell>Strong DPM [1]</cell><cell>-</cell><cell cols="2">37.44% 47.08%</cell></row><row><cell>Part R-CNN [59]</cell><cell>-</cell><cell cols="2">61.94% 70.16%</cell></row><row><cell>Ours</cell><cell cols="3">92.84% 70.89% 75.79%</cell></row></table><note><p>1. www.flickr.com</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Comparisons of methods on the CUB-200-2011 dataset. For fair comparison, only methods that use no annotation during testing are listed. For all methods, we report the results using the same CNN architecture (AlexNet) where possible.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 CUB</head><label>3</label><figDesc>-200-2011 ablation study to investigate different fine-tuning, classifier, detector, and denoising strategies.</figDesc><table><row><cell>Part Localization</cell><cell cols="2">Predict BBox</cell><cell>Oracle</cell></row><row><cell>feature\classifier</cell><cell cols="2">Train Train+Weak</cell><cell>Train</cell></row><row><cell>w/o ft</cell><cell>68.58</cell><cell>71.19</cell><cell>74.14</cell></row><row><cell>ft on train</cell><cell>78.56</cell><cell>79.89</cell><cell>82.12</cell></row><row><cell>ft on train/weak</cell><cell>81.17</cell><cell>82.16</cell><cell>85.07</cell></row><row><cell cols="2">ft on train/weak-dn 83.24</cell><cell>84.59</cell><cell>86.57</cell></row><row><cell>in Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Demonstration of the impact of less strongly supervised images. Controlled experiments were conducted to produce these results without performing MIL in the final classifier learning step.</figDesc><table><row><cell cols="2">dataset (# of images)</cell><cell>detection (PCP)</cell><cell></cell><cell></cell><cell cols="2">classification (Accuracy)</cell><cell></cell></row><row><cell>strong</cell><cell>weak</cell><cell>bbox head body</cell><cell>strong</cell><cell>strong</cell><cell>weak</cell><cell cols="2">strong+weak strong+weak</cell></row><row><cell cols="2">(per category)</cell><cell>(fast R-CNN)</cell><cell cols="3">(image) (part) (image)</cell><cell>(image)</cell><cell>(part)</cell></row><row><cell>30</cell><cell>100</cell><cell>95.60 86.97 84.16</cell><cell>73.32</cell><cell>81.08</cell><cell>71.72</cell><cell>80.12</cell><cell>85.67</cell></row><row><cell>5</cell><cell>100</cell><cell>89.77 74.65 64.38</cell><cell>36.83</cell><cell>37.88</cell><cell>71.72</cell><cell>74.20</cell><cell>81.05</cell></row><row><cell cols="4">over the method using only image-level labels on the same</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">training samples. In comparison, by training CNNs with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">image labels only, 500 web images per category were needed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">to achieve comparable results (86.29%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Comparison of classification accuracy on the NAbirds Dataset. Part-level information is transferred from the CUB-200-2011 dataset in our method.</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell>Test</cell><cell>Accuracy(%)</cell></row><row><cell>No parts</cell><cell>n/a</cell><cell>n/a</cell><cell>63.9</cell></row><row><cell>Horn et al. [49]</cell><cell cols="2">Parts Parts</cell><cell>75.0</cell></row><row><cell>Simon et al. [45]</cell><cell>n/a</cell><cell>n/a</cell><cell>76.3</cell></row><row><cell>Our Method (ImageNet ft.)</cell><cell>n/a</cell><cell>n/a</cell><cell>77.4</cell></row><row><cell>Our Method (CUB ft.)</cell><cell>n/a</cell><cell>n/a</cell><cell>79.0</cell></row><row><cell cols="4">class used for training. As this dataset contains no detailed</cell></row><row><cell cols="4">annotations, we utilized part-level annotations from the</cell></row><row><cell cols="4">smaller Oxford-Pets dataset [39] (altogether 37 species of</cell></row><row><cell cols="4">cats and dogs) to train object detectors and the part "head"</cell></row><row><cell cols="4">and transferred the learned part-level representations to</cell></row><row><cell cols="4">the Stanford Dogs dataset. Compared to the baseline of</cell></row><row><cell cols="4">training classifiers directly on image-level labels, the pro-</cell></row><row><cell cols="4">posed method again delivered a solid improvement in clas-</cell></row><row><cell cols="4">sification accuracy (from 75.4% to 78.2%) without human</cell></row><row><cell>labeling effort.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Comparison of classification accuracy on the clothing dataset. Mean accuracy (mA) and total accuracy are reported for this experiment. ), the strongly-supervised FGVC algorithm for initialization misclassified the image into a close subcategory Pied Kingfisher. Closer inspection reveals that the bird in the test image indeed belonged to a rare occurring subclass in the category in which black and white spots decorate the chest. Unfortunately, the strongly supervised dataset did not include sufficient training data for this subclass.</figDesc><table><row><cell>Method</cell><cell cols="2">clothing pattern mA(%) Acc(%)</cell><cell cols="2">collar shape mA(%) Acc(%)</cell></row><row><cell>Baseline</cell><cell>74.0</cell><cell>76.2</cell><cell>58.5</cell><cell>74.2</cell></row><row><cell>Our method</cell><cell>76.5</cell><cell>78.0</cell><cell>60.4</cell><cell>75.0</cell></row><row><cell>shown in (d</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is partially supported by the High Technology Research and Development Program of China 2015AA015801, NSFC 61221001, STCSM 12DZ2272600, and the 111 Project B07022, and the Australian Research Council Projects DP-140102164, FT-130101457, and LE-140100061.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection using stronglysupervised deformable part models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Strong supervision from weak annotation: Interactive training of deformable part models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting visual knowledge from web data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards scalable dataset construction: An active learning approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained crowdsourcing for fine-grained recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A visual category filter for google images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conceptmap: Mining noisy web data for concept learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Golge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<idno>arX- iv:1511.02251</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06789</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimol: automatic online picture collection via incremental model learning</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attributes make sense on segmented objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Finegrained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Weaklyand semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dataset issues in object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward category-level object recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Basic objects in natural categories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boyes-Braem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Harvesting image databases from the web</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1611" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in finegrained dataset collection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011 dataset. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Annotating images by mining image search results</title>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Error-driven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Architectural style classification using multinomial latent logistic regression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised template learning for fine-grained object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Part-based rcnns for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deformable part descriptors for fine-grained recognition and attribute prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fused one-vs-all features with semantic alignments for fine-grained visual categorization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Weakly supervised fine-grained image categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.04943</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
