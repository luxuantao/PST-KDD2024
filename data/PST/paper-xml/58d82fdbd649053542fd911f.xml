<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inertial Proximal Alternating Linearized Minimization (iPALM) for Nonconvex and Nonsmooth Problems *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-11-15">electronically November 15, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Pock</surname></persName>
							<email>pock@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computer Graphics and Vision</orgName>
								<orgName type="department" key="dep2">Digital Safety &amp; Security Department</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AIT Austrian Institute of Technology GmbH</orgName>
								<address>
									<postCode>1220</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shoham</forename><surname>Sabach</surname></persName>
							<email>ssabach@ie.technion.ac.il</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering and Management</orgName>
								<orgName type="institution">Technion-Israel Institute of Technology</orgName>
								<address>
									<postCode>3200003</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inertial Proximal Alternating Linearized Minimization (iPALM) for Nonconvex and Nonsmooth Problems *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-15">electronically November 15, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">BEC2953AAA8EC99DDC3EEB1878D53609</idno>
					<idno type="DOI">10.1137/16M1064064</idno>
					<note type="submission">Received by the editors March 3, 2016; accepted for publication (in revised form) August 1, 2016;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>alternating minimization</term>
					<term>blind image deconvolution</term>
					<term>block coordinate descent</term>
					<term>heavy ball method</term>
					<term>Kurdyka-Lojasiewicz property</term>
					<term>nonconvex and nonsmooth minimization</term>
					<term>sparse nonnegative matrix factorization</term>
					<term>dictionary learning AMS subject classifications. 90C26</term>
					<term>90C30</term>
					<term>49M37</term>
					<term>65K10</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study nonconvex and nonsmooth optimization problems with semialgebraic data, where the variables vector is split into several blocks of variables. The problem consists of one smooth function of the entire variables vector and the sum of nonsmooth functions for each block separately. We analyze an inertial version of the proximal alternating linearized minimization algorithm and prove its global convergence to a critical point of the objective function at hand. We illustrate our theoretical findings by presenting numerical experiments on blind image deconvolution, on sparse nonnegative matrix factorization and on dictionary learning, which demonstrate the viability and effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. In the last decades advances in convex optimization have significantly influenced scientific fields such as image processing and machine learning, which are dominated by computational approaches. However, it is also known that the framework of convexity is often too restrictive to provide good models for many practical problems. Several basic problems such as blind image deconvolution are inherently nonconvex, and hence there is a vital interest in the development of efficient and simple algorithms for tackling nonconvex optimization problems.</p><p>A large part of the optimization community has also been devoted to the development of general purpose solvers <ref type="bibr" target="#b23">[23]</ref>, but in the era of big data, such algorithms often come to their limits since they cannot efficiently exploit the structure of the problem at hand. One notable exception is the general purpose limited quasi-Newton method <ref type="bibr" target="#b18">[18]</ref>, which has been published more than 25 years ago but still remains a competitive method.</p><p>A promising approach to tackle nonconvex problems is to consider a very rich class of problems which share certain structure that allows the development of efficient algorithms.</p><p>One such class of nonconvex optimization problems is given by the sum of three functions:</p><p>(1.1) min</p><formula xml:id="formula_0">x=(x 1 ,x 2 ) F (x) := f 1 (x 1 ) + f 2 (x 2 ) + H (x) ,</formula><p>where f 1 and f 2 are assumed to be general nonsmooth and nonconvex functions with efficiently computable proximal mappings (see the exact definition in the next section) and H is a smooth coupling function which is required to have only partial Lipschitz continuous gradients ∇ x 1 H and ∇ x 2 H. (It should be noted that ∇ x H might not be a Lipschitz continuous.) Many practical problems frequently used in the machine learning and image processing communities fall into this class of problems. Let us briefly mention two classical examples.</p><p>(Another example will be discussed in section 5.)</p><p>The first example is nonnegative matrix factorization (NMF) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b15">15]</ref>. Given a nonnegative data matrix A ∈ R m×n + and an integer r &gt; 0, the idea is to approximate the matrix A by a product of again nonnegative matrices BC, where B ∈ R m×r + and C ∈ R r×n + . It should be noted that the dimension r is usually much smaller than min{m, n}. Clearly this problem is very difficult to solve, and hence several algorithms have been developed (see, for example, <ref type="bibr" target="#b33">[33]</ref>). One possibility to solve this problem is by finding a solution for the nonnegative least squares model given by <ref type="bibr">(1.2)</ref> min</p><formula xml:id="formula_1">B,C 1 2 A -BC 2 F , s.t. B ≥ 0, C ≥ 0,</formula><p>where the nonnegativity constraint is understood pointwise and • F denotes the classical Frobenius norm. The NMF has important applications in image processing (face recognition) and bioinformatics (clustering of gene expressions). Observe that the gradient of the objective function is not Lipschitz continuous, but it is partially Lipschitz continuous, which enables the application of alternating minimization based methods (see <ref type="bibr" target="#b10">[10]</ref>). Additionally, it is popular to impose sparsity constraints on one or both of the unknowns, e.g., C 0 ≤ c, to promote sparsity in the representation. See <ref type="bibr" target="#b10">[10]</ref> for the first globally convergent algorithm for solving the sparse NMF problem. As we will see, the complicated sparse NMF can be also simply handled by our proposed algorithm, which seems to produce better performances (see section 5).</p><p>The second example we would like to mention is the important but ever challenging problem of blind image deconvolution (BID) <ref type="bibr" target="#b16">[16]</ref>. Let A ∈ [0, 1] M ×N be the observed blurred image of size M × N , and let B ∈ [0, 1] M ×N be the unknown sharp image of the same size. Furthermore, let K ∈ ∆ mn denote a small unknown blur kernel (point spread function) of size m × n, where ∆ mn denotes the mn-dimensional standard unit simplex. We further assume that the observed blurred image has been formed by the following linear image formation model:</p><formula xml:id="formula_2">A = B * K + E,</formula><p>where * denotes a two-dimensional (2D) discrete convolution operation and E denotes a small additive Gaussian noise. A typical variational formulation of the blind deconvolution problem is given by <ref type="bibr">(1.3)</ref> min</p><formula xml:id="formula_3">U,K R (U ) + 1 2 B * K -A 2 F , s.t. 0 ≤ U ≤ 1, K ∈ ∆ mn .</formula><p>In the above variational model, R is an image regularization term, typically a function, that imposes sparsity on the image gradient and hence favors sharp images over blurred images.</p><p>We will come back to both examples in section 5, where we will show how the proposed algorithm can be applied to efficiently solve these problems.</p><p>In <ref type="bibr" target="#b10">[10]</ref>, the authors proposed the proximal alternating linearized minimization method (PALM) that efficiently exploits the structure of problem <ref type="bibr">(1.1)</ref>. PALM can be understood as a blockwise application of the well-known proximal forward-backward algorithm <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b11">11]</ref> in the nonconvex setting. In the case that the objective function F satisfies the so-called Kurdyka-Lojasiewicz (KL) property (the exact definition will be given in section 3), the whole sequence of the algorithm is guaranteed to converge to a critical point of the problem.</p><p>In this paper, we propose an inertial version of the PALM algorithm and show convergence of the whole sequence in case the objective function F satisfy the KL property. The inertial term is motivated by the heavy ball method of Polyak <ref type="bibr" target="#b29">[29]</ref>, which in its most simple version applied to minimizing a smooth function f and can be written as the iterative scheme</p><formula xml:id="formula_4">x k+1 = x k -τ ∇f x k + β x k -x k-1 ,</formula><p>where β and τ are suitable parameters that ensure convergence of the algorithm. The heavy ball method differs from the usual gradient method by the additional inertial term β(x k -x k-1 ), which adds part of the old direction to the new direction of the algorithm. Therefore for β = 0, we completely recover the classical algorithm of unconstrained optimization, the gradient method. The heavy ball method can be motivated from basically three view points.</p><p>First, the heavy ball method can be seen as an explicit finite differences discretization of the heavy ball with friction dynamical system (see <ref type="bibr" target="#b1">[2]</ref>):</p><formula xml:id="formula_5">ẍ (t) + c ẋ (t) + g (x (t)) = 0,</formula><p>where x(t) is a time continuous trajectory, ẍ(t) is the acceleration, c ẋ(t) for c &gt; 0 is the friction (damping), which is proportional to the velocity ẋ(t), and g(x(t)) is an external gravitational field. In the case that g = ∇f the trajectory x(t) is running down the "energy landscape" described by the objective function f until a critical point (∇f = 0) is reached. Due to the presence of the inertial term, it can also overcome spurious critical points of f , e.g., saddle points.</p><p>Second, the heavy ball method can be seen as a special case of the so-called multistep algorithms where each step of the algorithm is given as a linear combination of all previously computed gradients <ref type="bibr" target="#b12">[12]</ref>, that is, an algorithm of the following form:</p><formula xml:id="formula_6">x k+1 = x k - k i=0 α i ∇f x i .</formula><p>Let us note that in the case that the objective function f is quadratic, the parameters α i can be chosen in a way such that the objective function is minimized at each step. This approach eventually leads to the conjugate gradient (CG) method, pointing out a close relationship to inertial-based methods.</p><p>Third, accelerated gradient methods, as pioneered by Nesterov (see <ref type="bibr" target="#b21">[21]</ref> for an overview), are based on a variant of the heavy ball method that uses the extrapolated point (based on the inertial force) also for evaluating the gradient in the current step. It turns out that, in the convex setting, these methods improve the worst convergence rate from O(1/k) to O(1/k 2 ), while leaving the computational complexity of each step basically the same.</p><p>In <ref type="bibr" target="#b35">[35]</ref>, the heavy ball method has been analyzed for the first time in the setting of nonconvex problems. It is shown that the heavy ball method is attracted by the connected components of critical points. The proof is based on considering a suitable Lyapunov function that allows one to consider the two-step algorithm as a one-step algorithm. As we will see later, our convergence proof is also based on rewriting the algorithm as a one-step method.</p><p>In <ref type="bibr" target="#b24">[24]</ref>, the authors developed an inertial proximal gradient algorithm (iPiano). The algorithm falls into the class of forward-backward splitting algorithms <ref type="bibr" target="#b11">[11]</ref>, as it performs an explicit forward (steepest descent) step with respect to the smooth (nonconvex) function followed by a (proximal) backward step with respect to the nonsmooth (convex) function. Motivated by the heavy ball algorithm mentioned before, the iPiano algorithm makes use of an inertial force which empirically shows to improve the convergence speed of the algorithm. A related method based on general Bregman proximal-like distance functions has been recently proposed in <ref type="bibr" target="#b14">[14]</ref>.</p><p>Very recently, a randomized proximal linearization method has been proposed in <ref type="bibr" target="#b34">[34]</ref>. The method is closely related to our proposed algorithm but convergence is proved only in the case that the function values are strictly decreasing. This is true only if the inertial force is set to be zero or the algorithm is restarted whenever the function values are not decreasing. In this paper, however, we overcome this major drawback and prove convergence of the algorithm without any assumption on the monotonicity of the objective function.</p><p>The remainder of the paper is organized as follows. In section 2 we give an exact definition of the problem and the proposed algorithm. In section 3 we state a few technical results that will be necessary for the convergence analysis, which will be presented in section 4. In section 5 we present some numerical results and analyze the practical performance of the algorithm in dependence of its inertial parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem formulation and algorithm.</head><p>In this paper we follow <ref type="bibr" target="#b10">[10]</ref> and consider the broad class of nonconvex and nonsmooth problems of the following form:</p><formula xml:id="formula_7">(2.1) minimize F (x) := f 1 (x 1 ) + f 2 (x 2 ) + H (x) over all x = (x 1 , x 2 ) ∈ R n 1 × R n 2 ,</formula><p>where f 1 and f 2 are extended valued (i.e., giving the possibility of imposing constraints separately on the blocks x 1 and x 2 ) and H is a smooth coupling function. (See below for more precise assumptions on the involved functions.) We would like to stress from the beginning that even though all the discussions and results of this paper derived for two blocks of variables x 1 and x 2 , they hold true for any finite number of blocks. This choice was done only for the sake of simplicity of the presentation of the algorithm and the convergence results. As we discussed in the introduction, the proposed algorithm can be viewed either as a block version of the recent iPiano algorithm <ref type="bibr" target="#b24">[24]</ref> or as an inertial based version of the recent PALM algorithm <ref type="bibr" target="#b10">[10]</ref>. Before presenting the algorithm it will be convenient to recall the definition of the Moreau proximal mapping <ref type="bibr" target="#b20">[20]</ref>. Given a proper and lower semicontinuous function σ : R d → (-∞, ∞], the proximal mapping associated with σ is defined by</p><formula xml:id="formula_8">(2.2) prox σ t (p) := argmin σ (q) + t 2 q -p 2 : q ∈ R d , (t &gt; 0) .</formula><p>Following <ref type="bibr" target="#b10">[10]</ref>, we take the following as our blanket assumption. (i) f 1 : R n 1 → (-∞, ∞] and f 2 : R n 2 → (-∞, ∞] are proper and lower semicontinuous functions such that inf</p><formula xml:id="formula_9">R n 1 f 1 &gt; -∞ and inf R n 2 f 2 &gt; -∞. (ii) H : R n 1 × R n 2 → R is differentiable and inf R n 1 ×R n 2 F &gt; -∞. (iii) For any fixed x 2 the function x 1 → H(x 1 , x 2 ) is C 1,1 L 1 (x 2 )</formula><p>, namely, the partial gradient</p><formula xml:id="formula_10">∇ x 1 H(x 1 , x 2 ) is globally Lipschitz with moduli L 1 (x 2 ), that is, ∇ x 1 H(u, x 2 ) -∇ x 1 H(v, x 2 ) ≤ L 1 (x 2 ) u -v ∀ u, v ∈ R n 1 .</formula><p>Likewise, for any fixed x 1 the function</p><formula xml:id="formula_11">x 2 → H(x 1 , x 2 ) is assumed to be C 1,1 L 2 (x 1 ) . (iv) For i = 1, 2 there exists λ - i , λ + i &gt; 0 such that inf {L 1 (x 2 ) : x 2 ∈ B 2 } ≥ λ - 1 and inf {L 2 (x 1 ) : x 1 ∈ B 1 } ≥ λ - 2 , (2.3) sup {L 1 (x 2 ) : x 2 ∈ B 2 } ≤ λ + 1 and sup {L 2 (x 1 ) : x 1 ∈ B 1 } ≤ λ + 2 (2.4) for any compact set B i ⊆ R n i , i = 1, 2. (v) ∇H is Lipschitz continuous on bounded subsets of R n 1 × R n 2 . In other words, for each bounded subset B 1 × B 2 of R n 1 × R n 2 there exists M &gt; 0 such that (∇ x 1 H (x 1 , x 2 ) -∇ x 1 H (y 1 , y 2 ) , ∇ x 2 H (x 1 , x 2 ) -∇ x 2 H (y 1 , y 2 )) ≤ M (x 1 -y 1 , x 2 -y 2 ) .</formula><p>We propose now the inertial proximal alternating linearized minimization (iPALM) algorithm.</p><p>iPALM: Inertial Proximal Alternating Linearized Minimization 1. Initialization: start with any ( The parameters τ k 1 and τ k 2 , k ∈ N, are discussed in section 4, but for now we can say that they are proportional to the respective partial Lipschitz moduli of H. The larger the partial Lipschitz moduli the smaller the step-size, and hence the slower the algorithm. As we shall see below, the partial Lipschitz moduli L 1 (x 2 ) and L 2 (x 1 ) are explicitly available for the examples mentioned at the introduction. However, note that if these are unknown, or still too difficult to compute, then a backtracking scheme <ref type="bibr" target="#b6">[6]</ref> can be incorporated and the convergence results developed below remain true; for simplicity of exposition we omit the details.</p><formula xml:id="formula_12">x 0 1 , x 0 2 ) ∈ R n 1 × R n 2 . 2. For each k = 1, 2, . . . generate a sequence {(x k 1 , x k 2 )} k∈N as follows: 2.1 Take α k 1 , β k 1 ∈ [0, 1] and τ k 1 &gt; 0. Compute y k 1 = x k 1 + α k 1 x k 1 -x k-1 1 , (2.5) z k 1 = x k 1 + β k 1 x k 1 -x k-1 1 , (2.6) x k+1 1 ∈ prox f 1 τ k 1 y k 1 - 1 τ k 1 ∇ x 1 H z k 1 , x k 2 . (2.7) 2.2 Take α k 2 , β k 2 ∈ [0, 1] and τ k 2 &gt; 0. Compute y k 2 = x k 2 + α k 2 x k 2 -x k-1 2 , (2.8) z k 2 = x k 2 + β k 2 x k 2 -x k-1 2 , (2.9) x k+1 2 ∈ prox f 2 τ k 2 y k 2 - 1 τ k 2 ∇ x 2 H x k+1 1 , z k 2 . (<label>2</label></formula><p>In section 5, we will show that the involved functions of the nonnegative matrix factorization model (see <ref type="bibr">(1.2)</ref>) and of the blind image deconvulation model (see <ref type="bibr">(1.3</ref>)) do satisfy Assumption A. For the general setting we point out the following remarks about Assumption A.</p><p>(i) The first item of Assumption A is very general, and most of the interesting constraints (via their indicator functions) or regularizing functions fulfill these requirements. (ii) Items (ii)-(v) of Assumption A are beneficially exploited to build the proposed iPALM algorithm. These requirements do not guarantee that the gradient of H is globally Lipschitz (which is the case in our mentioned applications). The fact that ∇H is not globally Lipschitz reduces the potential of applying the iPiano and PFB methods in concrete applications and therefore highly motivated us to study their block counterparts (PALM in <ref type="bibr" target="#b10">[10]</ref> and iPALM in this paper). (iii) Another advantage of algorithms that exploit block structures inherent in the model at hand is the fact that they achieve better numerical performance (see section 5) by taking step-sizes which are optimized to each separated block of variables. (iv) Item (v) of Assumption A holds true, for example, when H is C 2 . In this case the inequalities in (2.4) could be obtained if the sequence, which generated by the algorithm, is bounded. The iPALM algorithm generalizes few known algorithms for different values of the inertial parameters α k i and β k i , k ∈ N, and i = 1, 2. For example, when α k i = β k i = 0, k ∈ N, we recover the PALM algorithm of <ref type="bibr" target="#b10">[10]</ref>, which is a block version of the classical proximal forward-backward (PFB) algorithm. When there is only one block of variables, for instance only i = 1, we get the iPiano algorithm <ref type="bibr" target="#b24">[24]</ref>, which is recovered exactly only when β k 1 = 0, k ∈ N. It should be also noted that in <ref type="bibr" target="#b24">[24]</ref>, the authors additionally assume that the function f 1 is convex (an assumption that is not needed in this work). The iPiano algorithm by itself generalizes two classical and known algorithms: one is the heavy ball method <ref type="bibr" target="#b30">[30]</ref> (when f 1 ≡ 0) and again the PFB method (when α k 1 = 0, k ∈ N).</p><p>3. Mathematical preliminaries and proof methodology. Throughout this paper we are using standard notation and definitions of nonsmooth analysis which can be found in any classical book; see, for instance, <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b19">19]</ref>. We recall here some notation and technical results. Let σ : R d → (-∞, ∞] be a proper and lower semicontinuous function. Since we are dealing with nonconvex and nonsmooth functions that can have the value ∞, we use the notion of limiting subdifferential (or simply subdifferential), see <ref type="bibr" target="#b19">[19]</ref>, which is denoted by ∂σ. In what follows, we are interested in finding critical points of the objective function F defined in (2.1). Critical points are those points for which the corresponding subdifferential contains the zero Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php vector 0. The set of critical points of σ is denoted by crit σ, that is, crit σ = {u ∈ dom σ : 0 ∈ ∂σ (u)} .</p><p>An important property of the subdifferential is recorded in the following remark (see <ref type="bibr" target="#b31">[31]</ref>).</p><p>Remark 3.1. Let {(u k , q k )} k∈N be a sequence in graph (∂σ) that converges to (u, q) as k → ∞. By the definition of ∂σ(u), if σ(u k ) converges to σ(u) as k → ∞, then (u, q) ∈ graph (∂σ).</p><p>The convergence analysis of iPALM is based on the proof methodology which was developed in <ref type="bibr" target="#b5">[5]</ref> and more recently extended and simplified in <ref type="bibr" target="#b10">[10]</ref>. The main part of the suggested methodology relies on the fact that the objective function of the problem at hand satisfies the KL property. Before stating the KL property we will need the definition of the following class of desingularizing functions. For η ∈ (0, ∞] define</p><formula xml:id="formula_13">(3.1) Φ η ≡      ϕ ∈ C [[0, η) , R + ] such that      ϕ (0) = 0 ϕ ∈ C 1 on (0, η) ϕ (s) &gt; 0 ∀s ∈ (0, η)           .</formula><p>The function σ is said to have the KL property at u ∈ dom ∂σ if there exist η ∈ (0, ∞], a neighborhood U of u, and a function ϕ ∈ Φ η , such that, for all</p><formula xml:id="formula_14">u ∈ U ∩ [σ(u) &lt; σ(u) &lt; σ(u) + η],</formula><p>the following inequality holds:</p><formula xml:id="formula_15">(3.2) ϕ (σ (u) -σ (u)) dist (0, ∂σ (u)) ≥ 1,</formula><p>where for any subset S ⊂ R d and any point x ∈ R d dist (x, S) := inf { y -x : y ∈ S} .</p><p>When S = ∅, we have that dist(x, S) = ∞ for all x. If σ satisfies property (3.2) at each point of dom ∂σ, then σ is called a KL function.</p><p>The convergence analysis presented in the following section is based on the uniformized KL property which was established in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr">Lemma 6,</ref><ref type="bibr">p. 478</ref>].</p><p>Lemma 3.1. Let Ω be a compact set and let σ : R d → (-∞, ∞] be a proper and lower semicontinuous function. Assume that σ is constant on Ω and satisfies the KL property at each point of Ω. Then, there exist ε &gt; 0, η &gt; 0, and ϕ ∈ Φ η such that for all u in Ω and all u in the intersection</p><formula xml:id="formula_16">(3.3) u ∈ R d : dist (u, Ω) &lt; ε ∩ [σ (u) &lt; σ (u) &lt; σ (u) + η] ,</formula><p>one has</p><formula xml:id="formula_17">(3.4) ϕ (σ (u) -σ (u)) dist (0, ∂σ (u)) ≥ 1.</formula><p>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>We refer the reader to <ref type="bibr" target="#b9">[9]</ref> for an in-depth study of the class of KL functions. For the important relation between semialgebraic and KL functions see <ref type="bibr" target="#b8">[8]</ref>. In <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b10">10]</ref>, the interested reader can find a catalog of functions which are very common in many applications and satisfy the KL property.</p><p>Before concluding the mathematical preliminaries part we would like to mention a few important properties of the proximal map (defined in (2.2)). The following result can be found in <ref type="bibr" target="#b31">[31]</ref>.</p><p>Proposition 3.1. Let σ : R d → (-∞, ∞] be a proper and lower semicontinuous function with inf R d σ &gt; -∞. Then, for every t ∈ (0, ∞) the set prox tσ (u) is nonempty and compact.</p><p>It follows immediately from the definition that proxσ is a multivalued map when σ is nonconvex. The multivalued projection onto a nonempty and closed set C is recovered when σ = δ C , which is the indicator function of C that is defined to be zero on C and ∞ outside.</p><p>The main computational effort of iPALM involves a proximal mapping step of a proper and lower semicontinuous but nonconvex function. The following property will be essential in the forthcoming convergence analysis and is a slight modification of [10, Lemma 2, p. 471].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.2 (proximal inequality).</head><p>Let h : R d → R be a continuously differentiable function with gradient ∇h assumed L h -Lipschitz continuous and let σ : R d → (-∞, ∞] be a proper and lower semicontinuous function with inf R d σ &gt; -∞. Then, for any v, w ∈ dom σ and any u + ∈ R d defined by</p><formula xml:id="formula_18">(3.5) u + ∈ prox σ t v - 1 t ∇h (w) , t &gt; 0,</formula><p>we have, for any u ∈ dom σ and any s &gt; 0,</p><formula xml:id="formula_19">(3.6) g u + ≤ g (u) + L h + s 2 u + -u 2 + t 2 u -v 2 - t 2 u + -v 2 + L 2 h 2s u -w 2 ,</formula><p>where g := h + σ.</p><p>Proof. First, it follows immediately from Proposition 3.1 that u + is well-defined. By the definition of the proximal mapping (see (2.2)) we get that</p><formula xml:id="formula_20">u + ∈ argmin ξ∈R d ξ -v, ∇h (w) + t 2 ξ -v 2 + σ (ξ) ,</formula><p>and hence in particular, by taking ξ = u, we obtain Invoking first the descent lemma (see <ref type="bibr" target="#b7">[7]</ref>) for h, and using (3.7), yields</p><formula xml:id="formula_21">u + -v, ∇h (w) + t 2 u + -v 2 + σ u + ≤ u -v, ∇h (w) + t 2 u -v 2 + σ (u) . Thus (3.7) σ u + ≤ u -u + , ∇h (w) + t 2 u -v 2 - t 2 u + -v 2 + σ (u) .</formula><formula xml:id="formula_22">h u + + σ u + ≤ h (u) + u + -u, ∇h (u) + L h 2 u + -u 2 + u -u + , ∇h (w) + t 2 u -v 2 - t 2 u + -v 2 + σ (u) = h (u) + σ (u) + u + -u, ∇h (u) -∇h (w) + L h 2 u + -u 2 + t 2 u -v 2 - t 2 u + -v 2 .</formula><p>Now, using the fact that p, q ≤ (s/2) p 2 + (1/2s) q 2 for any two vectors p, q ∈ R d and every s &gt; 0 yields</p><formula xml:id="formula_23">u + -u, ∇h (u) -∇h (w) ≤ s 2 u + -u 2 + 1 2s ∇h (u) -∇h (w) 2 ≤ s 2 u + -u 2 + L h 2 2s u -w 2 ,</formula><p>where we have used the fact that ∇h is L h -Lipschitz continuous. Thus, combining the last two inequalities proves that (3.6) holds.</p><p>Remark 3.2. It should be noted that if the nonsmooth function σ is also known to be convex, then we can derive the following tighter upper bound (cf. (3.6))</p><formula xml:id="formula_24">(3.8) g u + ≤ g (u) + L h + s -t 2 u + -u 2 + t 2 u -v 2 - t 2 u + -v 2 + L 2 h 2s u -w 2 .</formula><p>3.1. Convergence proof methodology. In this section we briefly summarize (cf. Theorem 3.1 below) the methodology recently proposed in <ref type="bibr" target="#b10">[10]</ref>, which provides the key elements to obtain an abstract convergence result that can be applied to any algorithm and will be applied here to prove convergence of iPALM. Let u k k∈N be a sequence in R d which was generated from a starting point u 0 by a generic algorithm A. The set of all limit points of u k k∈N is denoted by ω(u 0 ) and defined by</p><formula xml:id="formula_25">u ∈ R d : ∃ an increasing sequence of integers {k l } l∈N such that u k l → u as l → ∞ . Theorem 3.1. Let Ψ : R d → (-∞, ∞]</formula><p>be a proper, lower semicontinuous and semialgebraic function with inf Ψ &gt; -∞. Assume that u k k∈N is a bounded sequence generated by a generic algorithm A from a starting point u 0 , for which the following three conditions hold true for any k ∈ N.</p><p>(C1) There exists a positive scalar ρ 1 such that </p><formula xml:id="formula_26">ρ 1 u k+1 -u k 2 ≤ Ψ u k -Ψ u k+1 ∀ k = 0,</formula><formula xml:id="formula_27">w k ≤ ρ 2 u k -u k-1 ∀ k = 0, 1, . . . . (C1) Each limit point in the set ω(u 0 ) is a critical point of Ψ, that is, ω(u 0 ) ⊂ crit Ψ.</formula><p>Then, the sequence u k k∈N converges to a critical point u * of Ψ. 4. Convergence analysis of iPALM. Our aim in this section is to prove that the sequence {(x k 1 , x k 2 )} k∈N which is generated by iPALM converges to a critical point of the objective function F defined in (2.1). To this end we will follow the proof methodology described above in Theorem 3.1. In the case of iPALM, similarly to the iPiano algorithm (see <ref type="bibr" target="#b24">[24]</ref>), it is not possible to prove that condition (C1) hold true for the sequence {(x k 1 , x k 2 )} k∈N and the function F , namely, this is not a descent algorithm with respect to F . Therefore, we first show that conditions (C1), (C2), and (C3) hold true for an auxiliary sequence and auxiliary function (see the details below). Then, based on these properties we will show that the original sequence converges to a critical point of the original function F .</p><p>We first introduce the following notation that simplifies the coming expositions. For any k ∈ N, we define</p><formula xml:id="formula_28">(4.1) ∆ k 1 = 1 2 x k 1 -x k-1 1 2 , ∆ k 2 = 1 2 x k 2 -x k-1 2 2</formula><p>, and</p><formula xml:id="formula_29">∆ k = 1 2 x k -x k-1 2 ;</formula><p>it is clear that, using this notation, we have that</p><formula xml:id="formula_30">∆ k = ∆ k 1 + ∆ k 2 for all k ∈ N.</formula><p>Using this notation we can easily show few basic relations of the sequences {x k i } k∈N , {y k i } k∈N , and</p><formula xml:id="formula_31">{z k i } k∈N for i = 1, 2, generated by iPALM. Proposition 4.1. Let {(x k 1 , x k 2 )</formula><p>} k∈N be a sequence generated by iPALM. Then, for any k ∈ N and i = 1, 2, we have</p><formula xml:id="formula_32">(i) x k i -y k i 2 = 2(α k i ) 2 ∆ k i ; (ii) x k i -z k i 2 = 2(β k i ) 2 ∆ k i ; (iii) x k+1 i -y k i 2 ≥ 2(1 -α k i )∆ k+1 i + 2α k i (α k i -1)∆ k i .</formula><p>Proof. The first two items follow immediately from the facts that</p><formula xml:id="formula_33">x k i -y k i = α k i (x k-1 i -x k i ) and x k i -z k i = β k i (x k-1 i -x k i ) for i = 1, 2 (see steps (2.5</formula><p>), (2.6), (2.8), and (2.9)). The last item follows from the argument</p><formula xml:id="formula_34">x k+1 i -y k i 2 = x k+1 i -x k i -α k i x k i -x k-1 i 2 = 2∆ k+1 i -2α k i x k+1 i -x k i , x k i -x k-1 i + 2 α k i 2 ∆ k i ≥ 2 1 -α k i ∆ k+1 i + 2α k i α k i -1 ∆ k i , (4.2)</formula><p>where we have used the fact that</p><formula xml:id="formula_35">2 x k+1 i -x k i , x k i -x k-1 i ≤ x k+1 i -x k i 2 + x k i -x k-1 i 2 = 2∆ k+1 i + 2∆ k i ,</formula><p>which follows from the Cauchy-Schwartz and Young inequalities. This proves item (iii). 2 )} k∈N be a sequence generated by iPALM; then for all k ∈ N, we have that</p><formula xml:id="formula_36">F x k+1 ≤ F x k + 1 s k 1 L 1 (x k 2 ) 2 β k 1 2 + s k 1 τ k 1 α k 1 ∆ k 1 + 1 s k 2 L 2 (x k+1 1 ) 2 β k 2 2 + s k 2 τ k 2 α k 2 ∆ k 2 + L 1 (x k 2 ) + s k 1 -τ k 1 1 -α k 1 ∆ k+1 1 + L 2 (x k+1 1 ) + s k 2 -τ k 2 1 -α k 2 ∆ k+1 2 ,</formula><p>where s k 1 &gt; 0 and s k 2 &gt; 0 are arbitrarily chosen for all k ∈ N. Proof. Fix k ≥ 1. Under our Assumption A(ii), the function x 1 → H(x 1 , x 2 ) (x 2 is fixed) is differentiable and has a Lipschitz continuous gradient with moduli L 1 (x 2 ). Using the iterative step (2.7), applying Lemma 3.2 for h(</p><formula xml:id="formula_37">•) := H(•, x k 2 ), σ := f 1 , and t := τ k 1 with the points u = x k 1 , u + = x k+1 1 , v = y k 1 , and w = z k 1 yields that H x k+1 1 , x k 2 + f 1 x k+1 1 ≤ H x k 1 , x k 2 + f 1 x k 1 + L 1 (x k 2 ) + s k 1 2 x k+1 1 -x k 1 2 + τ k 1 2 x k 1 -y k 1 2 - τ k 1 2 x k+1 1 -y k 1 2 + L 1 (x k 2 ) 2 2s k 1 x k 1 -z k 1 2 ≤ H x k 1 , x k 2 + f 1 x k 1 + L 1 (x k 2 ) + s k 1 ∆ k+1 1 + τ k 1 α k 1 2 ∆ k 1 -τ k 1 1 -α k 1 ∆ k+1 1 + α k 1 α k 1 -1 ∆ k 1 + L 1 (x k 2 ) 2 β k 1 2 s k 1 ∆ k 1 = H x k 1 , x k 2 + f 1 x k 1 + L 1 (x k 2 ) + s k 1 -τ k 1 1 -α k 1 ∆ k+1 1 + 1 s k 1 L 1 (x k 2 ) 2 β k 1 2 + s k 1 τ k 1 α k 1 ∆ k 1 , (4.3)</formula><p>where the second inequality follows from Proposition 4.1. Repeating all the arguments above on the iterative step (2.10) yields the following:</p><formula xml:id="formula_38">H x k+1 1 , x k+1 2 + f 2 x k+1 2 ≤ H x k+1 1 , x k 2 + f 2 x k 2 + L 2 (x k+1 1 ) + s k 2 -τ k 2 1 -α k 2 ∆ k+1 2 + 1 s k 2 L 2 (x k+1 1 ) 2 β k 2 2 + s k 2 τ k 2 α k 2 ∆ k 2 . (4.4)</formula><p>By adding (4.3) and (4.4) we get</p><formula xml:id="formula_39">F x k+1 ≤ F x k + 1 s k 1 L 1 (x k 2 ) 2 β k 1 2 + s k 1 τ k 1 α k 1 ∆ k 1 + 1 s k 2 L 2 (x k+1 1 ) 2 β k 2 2 + s k 2 τ k 2 α k 2 ∆ k 2 + L 1 (x k 2 ) + s k 1 -τ k 1 1 -α k 1 ∆ k+1 1 + L 2 (x k+1 1 ) + s k 2 -τ k 2 1 -α k 2 ∆ k+1 2 .</formula><p>This proves the desired result. Before we proceed and for the sake of the simplicity of our developments we would like to chose the parameters s k 1 and s k 2 for all k ∈ N. The best choice can be derived by minimizing the right-hand side of (3.6) with respect to s. Simple computations yield that the minimizer should be</p><formula xml:id="formula_40">s = L h u -w u + -u ,</formula><p>where u, u + , w, and L h are all in terms of Lemma 3.2. In Proposition 4.2 we have used Lemma 3.2 with the choices u = x k 1 , u + = x k+1 1 , and w = z k 1 . Thus</p><formula xml:id="formula_41">s k 1 = L 1 (x k 2 ) x k 1 -z k 1 x k+1 1 -x k 1 = L 1 (x k 2 )β k 1 x k 1 -x k-1 1 x k+1 1 -x k 1 ,</formula><p>where the last equality follows from step (2.6). Thus, from now on, we will use the following parameters:</p><p>(4.5)</p><formula xml:id="formula_42">s k 1 = L 1 (x k 2 )β k 1 and s k 2 = L 2 (x k+1 1 )β k 2 ∀ k ∈ N.</formula><p>An immediate consequence of this choice of parameters which combined with Proposition 4.2 is recorded now.</p><formula xml:id="formula_43">Corollary 4.1. Suppose that Assumption A holds. Let {(x k 1 , x k 2 )</formula><p>} k∈N be a sequence generated by iPALM, and then for all k ∈ N, we have that</p><formula xml:id="formula_44">F x k+1 ≤ F x k + L 1 (x k 2 )β k 1 + τ k 1 α k 1 ∆ k 1 + L 2 (x k+1 1 )β k 2 + τ k 2 α k 2 ∆ k 2 + 1 + β k 1 L 1 (x k 2 ) -τ k 1 1 -α k 1 ∆ k+1 1 + 1 + β k 2 L 2 (x k+1 1 ) -τ k 2 1 -α k 2 ∆ k+1 2 .</formula><p>Similarly to iPiano, the iPALM algorithm generates a sequence which does not ensure that the function values decrease between two successive elements of the sequence. Thus we can not obtain condition (C1) of Theorem 3.1. Following <ref type="bibr" target="#b24">[24]</ref> we construct an auxiliary function which enjoys the property of yielding a sequence of decreasing function values. Let Ψ : R n 1 ×n 2 × R n 1 ×n 2 → (-∞, ∞] be the auxiliary function which is defined as follows:</p><formula xml:id="formula_45">(4.6) Ψ δ 1 ,δ 2 (u) := F (u 1 ) + δ 1 2 u 11 -u 21 2 + δ 2 2 u 12 -u 22 2 ,</formula><p>where</p><formula xml:id="formula_46">δ 1 , δ 2 &gt; 0, u 1 = (u 11 , u 12 ) ∈ R n 1 × R n 2 , u 2 = (u 21 , u 22 ) ∈ R n 1 × R n 2 , and u = (u 1 , u 2 ). Let {(x k 1 , x k 2 )</formula><p>} k∈N be a sequence generated by iPALM and denote, for all k ∈ N,</p><formula xml:id="formula_47">u k 1 = (x k 1 , x k 2 ), u k 2 = (x k-1 1 , x k-1</formula><p>2 ), and</p><formula xml:id="formula_48">u k = (u k 1 , u k 2 )</formula><p>. We will prove now that the sequence u k k∈N and the function Ψ defined above satisfy conditions (C1), (C2), and (C3) of Theorem 3.1. We begin with proving condition (C1). To this end we will show that there are choices of δ 1 &gt; 0 and δ 2 &gt; 0, such that there exists ρ 1 &gt; 0 which satisfies It is easy to check that using the notation defined in (4.1), we have, for all k ∈ N, that</p><formula xml:id="formula_49">ρ 1 u k+1 -u k 2 ≤ Ψ u k -Ψ u k+1 .</formula><formula xml:id="formula_50">Ψ u k = F x k + δ 1 2 x k 1 -x k-1 1 2 + δ 2 2 x k 2 -x k-1 2 2 = F x k + δ 1 ∆ k 1 + δ 2 ∆ k 2 .</formula><p>In order to prove that the sequence {Ψ(u k )} k∈N decreases we will need the following technical result. (We provide the proof in Appendix 7.)</p><p>Lemma 4.1. Consider the functions g : R 5 + → R and h : R 5 + → R defined as</p><formula xml:id="formula_51">g (α, β, δ, τ, L) = τ (1 -α) -(1 + β) L -δ, h (α, β, δ, τ, L) = δ -τ α -Lβ.</formula><p>Let ε &gt; 0 and ᾱ &gt; 0 be two real numbers for which 0 ≤ α ≤ ᾱ &lt; 0.5(1 -ε). Assume, in addition, that 0 ≤ L ≤ λ for some λ &gt; 0 and 0 ≤ β ≤ β with β &gt; 0. If</p><formula xml:id="formula_52">δ * = ᾱ + β 1 -ε -2 ᾱ λ, (4.7) τ * = (1 + ε) δ * + (1 + β) L 1 -α , (4.8) then g(α, β, δ * , τ * , L) = εδ * and h(α, β, δ * , τ * , L) ≥ εδ * .</formula><p>Based on the mentioned lemma we will set, from now on, the parameters τ k 1 and τ k 2 for all k ∈ N, as follows: (4.9)</p><formula xml:id="formula_53">τ k 1 = (1 + ε) δ k 1 + 1 + β k 1 L 1 (x k 2 ) 1 -α k 1 and τ k 2 = (1 + ε) δ k 2 + 1 + β k 2 L 2 (x k+1 1 ) 1 -α k 2 .</formula><p>Remark 4.1. If we additionally know that f i , i = 1, 2, is convex, then a tighter bound can be used in Proposition 3.2 as described in Remark 3.8. Using this tight bound will improve the possible parameter τ k i that can be used (cf. (4.9)). Indeed, in the convex case, (4.7) and (4.8) are given by</p><formula xml:id="formula_54">δ * = ᾱ + 2 β 2 (1 -ε -ᾱ) λ, (4.10) τ * = (1 + ε) δ * + (1 + β) L 2 -α . (4.11)</formula><p>Thus, the parameters τ k i , i = 1, 2, can be taken in the convex case as follows:</p><formula xml:id="formula_55">τ k 1 = (1 + ε) δ k 1 + 1 + β k 1 L 1 (x k 2 ) 2 -α k 1 and τ k 2 = (1 + ε) δ k 2 + 1 + β k 2 L 2 (x k+1 1 ) 2 -α k 2 .</formula><p>This means that in the convex case, we can take smaller τ k i , i = 1, 2, which means a larger step-size in the algorithm. On top of that, in the case that f i , i = 1, 2, is convex, it should be noted that a careful analysis shows that in this case the parameters α k i , i = 1, 2, can be in the interval [0, 1) and not [0, 0.5) as stated in Lemma 4.1. (See also Assumption B below.)</p><p>In order to prove condition C1 and according to Lemma 4.1, we will need to restrict the possible values of the parameters α k i and β k i , i = 1, 2, for all k ∈ N. The following assumption is essential for our analysis.</p><p>Assumption B. Let ε &gt; 0 be an arbitrary small number. For all k ∈ N and i = 1, 2, there exist 0 &lt; ᾱi &lt; (1/2)(1 -ε) such that 0 ≤ α k i ≤ ᾱi . In addition, 0 ≤ β k i ≤ βi for some βi &gt; 0. Remark 4.2. It should be noted that using Assumption B, we obtain that τ k 1 ≤ τ + 1 , where</p><formula xml:id="formula_56">τ + 1 = (1 + ε) δ 1 + 1 + β1 λ + 1 1 -ᾱ1 ,</formula><p>where δ 1 is given in (4.7). Similar arguments show that</p><formula xml:id="formula_57">τ k 2 ≤ τ + 2 := (1 + ε) δ 2 + 1 + β2 λ + 2 1 -ᾱ2 .</formula><p>Now we will prove a descent property of {Ψ(u k )} k∈N .</p><p>Proposition 4.3. Let x k k∈N be a sequence generated by iPALM which is assumed to be bounded. Suppose that Assumptions A and B hold true. Then, for all k ∈ N and ε &gt; 0, we have</p><formula xml:id="formula_58">ρ 1 u k+1 -u k 2 ≤ Ψ u k -Ψ u k+1 ,</formula><p>where</p><formula xml:id="formula_59">u k = (x k , x k-1 ), k ∈ N, and ρ 1 = (ε/2) min{δ 1 , δ 2 } with (4.12) δ 1 = ᾱ1 + β1 1 -ε -2ᾱ 1 λ + 1 and δ 2 = ᾱ2 + β2 1 -ε -2ᾱ 2 λ + 2 .</formula><p>Proof. From the definition of Ψ (see (4.6)) and Corollary 4.1 we obtain that</p><formula xml:id="formula_60">Ψ u k -Ψ u k+1 = F x k + δ 1 ∆ k 1 + δ 2 ∆ k 2 -F x k+1 -δ 1 ∆ k+1 1 -δ 2 ∆ k+1 2 ≥ τ k 1 1 -α k 1 -1 + β k 1 L 1 (x k 2 ) -δ 1 ∆ k+1 1 + δ 1 -τ k 1 α k 1 -L 1 (x k 2 )β k 1 ∆ k 1 + τ k 2 1 -α k 2 -1 + β k 2 L 2 (x k+1 1 ) -δ 2 ∆ k+1 2 + δ 2 -τ k 2 α k 2 -L 2 (x k+1 1 )β k 2 ∆ k 2 = a k 1 ∆ k+1 1 + b k 1 ∆ k 1 + a k 2 ∆ k+1 2 + b k 2 ∆ k 2 ,<label>where</label></formula><formula xml:id="formula_61">a k 1 := τ k 1 1 -α k 1 -1 + β k 1 L 1 (x k 2 ) -δ 1 and b k 1 := δ 1 -τ k 1 α k 1 -L 1 (x k 2 )β k 1 , a k 2 := τ k 2 1 -α k 2 -1 + β k 2 L 2 (x k+1 1 ) -δ 2 and b k 2 := δ 2 -τ k 2 α k 2 -L 2 (x k+1 1 )β k 2 .</formula><p>Let ε &gt; 0 be an arbitrary. Using (4.9) and (4.12) with the notation of Lemma 4.1 we immediately see that</p><formula xml:id="formula_62">a k 1 = g 1 (α k 1 , β k 1 , δ 1 , τ k 1 , L 1 (x k 2 )) and a k 2 = g 2 (α k 2 , β k 2 , δ 2 , τ k 2 , L 2 (x k+1<label>1</label></formula><p>)).</p><p>From Assumptions A and B we get that the requirements of Lemma 4.1 are fulfilled, which means that Lemma 4.1 can be applied. Thus a k 1 = εδ 1 and a k 2 = εδ 2 . Using again the notions of Lemma 4.1, we have that b</p><formula xml:id="formula_63">k 1 = h 1 (α k 1 , β k 1 , δ 1 , τ k 1 , L 1 (x k 2 )) and b k 2 = h 2 (α k 2 , β k 2 , δ 2 , τ k 2 , L 2 (x k+1<label>1</label></formula><p>)). Thus we obtain from Lemma 4.1 that b k 1 ≥ εδ 1 and b k 2 ≥ εδ 2 . Hence, for ρ 1 = (ε/2) min{δ 1 , δ 2 }, we have</p><formula xml:id="formula_64">Ψ u k -Ψ u k+1 ≥ a k 1 ∆ k+1 1 + b k 1 ∆ k 1 + a k 2 ∆ k+1 2 + b k 2 ∆ k 2 ≥ εδ 1 ∆ k+1 1 + ∆ k 1 + εδ 1 ∆ k+1 2 + ∆ k 2 ≥ ρ 1 ∆ k+1 1 + ∆ k 1 + ρ 1 ∆ k+1 2 + ∆ k 2 = ρ 1 u k+1 -u k 2 ,</formula><p>where the last equality follows from (4.1). This completes the proof. Now, we will prove that condition (C2) of Theorem 3.1 holds true for the sequence u k k∈N and the function Ψ.</p><p>Proposition 4.4. Let x k k∈N be a sequence generated by iPALM which is assumed to be bounded. Suppose that Assumptions A and B hold true. Assume that u k = (x k , x k-1 ), k ∈ N. Then, there exists a positive scalar ρ 2 such that for some w k ∈ ∂Ψ(u k ) we have</p><formula xml:id="formula_65">w k ≤ ρ 2 u k -u k-1 .</formula><p>Proof. Let k ≥ 2. By the definition of Ψ (see (4.6)) we have that</p><formula xml:id="formula_66">∂Ψ u k = ∂ x 1 F x k + δ 1 x k 1 -x k-1 1 , ∂ x 2 F x k + δ 2 x k 2 -x k-1 2 , δ 1 x k-1 1 -x k 1 , δ 2 x k-1 2 -x k 2 .</formula><p>By the definition of F (see (2.1)) and [10, Proposition 1, p. 465] we get that (4.13)</p><formula xml:id="formula_67">∂F x k = ∂f 1 x k 1 + ∇ x 1 H x k 1 , x k 2 , ∂f 2 x k 2 + ∇ x 2 H x k 1 , x k 2 .</formula><p>From the definition of the proximal mapping (see (2.2)) and the iterative step (2.7) we have</p><formula xml:id="formula_68">x k 1 ∈ argmin x 1 ∈R n 1 x 1 -y k-1 1 , ∇ x 1 H z k-1 1 , x k-1 2 + τ k-1 1 2 x 1 -y k-1 1 2 + f 1 (x 1 ) .</formula><p>Writing down the optimality condition yields </p><formula xml:id="formula_69">∇ x 1 H z k-1 1 , x k-1 2 + τ k-1 1 x k 1 -y k-1 1 + ξ k 1 = 0, where ξ k 1 ∈ ∂f 1 (x k 1 ). Hence ∇ x 1 H z k-1 1 , x k-1 2 + ξ k 1 = τ k-1 1 y k-1 1 -x k 1 = τ k-1 1 x k-1 1 -x k 1 + α k-1 1 x k-1 1 -x k-</formula><formula xml:id="formula_70">:= ∇ x 1 H x k 1 , x k 2 -∇ x 1 H z k-1 1 , x k-1 2 + τ k-1 1 x k-1 1 -x k 1 + α k-1 1 x k-1 1 -x k-2 1 ,</formula><p>we obtain from (4.13) that v k 1 ∈ ∂ x 1 F (x k ). Similarly, from the iterative step (2.10), by defining</p><formula xml:id="formula_71">(4.15) v k 2 := ∇ x 2 H x k 1 , x k 2 -∇ x 2 H x k 1 , z k-1 2 + τ k-1 2 x k-1 2 -x k 2 + α k-1 2 x k-1 2 -x k-2 2 ,</formula><p>we have that v k 2 ∈ ∂ x 2 F (x k ). Thus, for</p><formula xml:id="formula_72">w k := v k 1 + δ 1 x k 1 -x k-1 1 , v k 2 + δ 2 x k 2 -x k-1 2 , δ 1 x k 1 -x k-1 1 , δ 2 x k 2 -x k-1 2 ,</formula><p>we obtain that (4.16)</p><formula xml:id="formula_73">w k ≤ v k 1 + v k 2 + 2δ 1 x k 1 -x k-1 1 + 2δ 2 x k 2 -x k-1 2 .</formula><p>This means that we have to bound from above the norms of v k 1 and v k 2 . Since ∇H is Lipschitz continuous on bounded subsets of R n 1 × R n 2 (see Assumption A(v)) and since we assumed that x k k∈N is bounded, there exists M &gt; 0 such that</p><formula xml:id="formula_74">v k 1 ≤ τ k-1 1 x k-1 1 -x k 1 + α k-1 1 x k-1 1 -x k-2 1 + ∇ x 1 H x k 1 , x k 2 -∇ x 1 H z k-1 1 , x k-1 2 ≤ τ k-1 1 x k-1 1 -x k 1 + τ k-1 1 α k-1 1 x k-1 1 -x k-2 1 + M x k -z k-1 1 , x k-1 2 ≤ τ + 1 x k-1 1 -x k 1 + x k-1 1 -x k-2 1 + M x k 1 -x k-1 1 -β k-1 1 x k-1 1 -x k-2 1 , x k 2 -x k-1 2 , = τ + 1 x k-1 1 -x k 1 + x k-1 1 -x k-2 1 + M x k -x k-1 -β k-1 1 x k-1 1 -x k-2 1 , 0 ≤ τ + 1 + M x k -x k-1 + x k-1 -x k-2 ,</formula><p>where the third inequality follows from (2.6), the fact the sequence {τ k 1 } k∈N is bounded from above by τ + 1 (see Remark 4.2), and α k 1 , β k 1 ≤ 1 for all k ∈ N. On the other hand, from the Lipschitz continuity of ∇ x 2 H(x 1 , •) (see Assumption A(iii)), we have that </p><formula xml:id="formula_75">v k 2 ≤ τ k-1 2 x k-1 2 -x k 2 + α k-1 2 x k-1 2 -x k-2 2 + ∇ x 2 H x k 1 , x k 2 -∇ x 2 H x k 1 , z k-1 2 ≤ τ k-1 2 x k-1 2 -x k 2 + τ k-1 2 α k-1 2 x k-1 2 -x k-2 2 + L 1 (x k 1 ) x k 2 -z k-1 2 ≤ τ + 2 x k-1 2 -x k 2 + x k-1 2 -x k-2 2 + λ + 2 x k 2 -x k-1 2 -β k-1 2 x k-1 2 -x k-2 2 ≤ τ + 2 + λ + 2 x k-1 2 -x k 2 + x k-1 2 -x k-2 2 ≤ τ + 2 + λ + 2 x k -x k-1 + x k-1 -x k-2 ,</formula><formula xml:id="formula_76">w k ≤ v k 1 + v k 2 + 2δ 1 x k 1 -x k-1 1 + 2δ 2 x k 2 -x k-1 2 ≤ v k 1 + v k 2 + 2 (δ 1 + δ 2 ) x k -x k-1 ≤ τ + 1 + M + τ + 2 + λ + 2 x k -x k-1 + x k-1 -x k-2 + 2 (δ 1 + δ 2 ) u k -u k-1 ≤ √ 2 τ + 1 + M + τ + 2 + λ + 2 + 2 (δ 1 + δ 2 ) u k -u k-1 ,</formula><p>where the second inequality follows from the fact that</p><formula xml:id="formula_77">x k i -x k-1 i ≤ x k -x k-1 for i = 1, 2,</formula><p>the third inequality follows from the fact that</p><formula xml:id="formula_78">x k -x k-1 ≤ u k -u k-1</formula><p>, and the last inequality follows from the fact that</p><formula xml:id="formula_79">x k -x k-1 + x k-1 -x k-2 ≤ √ 2 u k -u k-1 . This completes the proof with ρ 2 = √ 2(τ + 1 + M + τ + 2 + λ + 2 ) + 2(δ 1 + δ 2 )</formula><p>. So far we have proved that the sequence u k k∈N and the function Ψ (see (4.6)) satisfy conditions (C1) and (C2) of Theorem 3.1. Now, in order to get that u k k∈N converges to a critical point of Ψ, it remains to prove that condition (C3) holds true. Proposition 4.5. Let x k k∈N be a sequence generated by iPALM which is assumed to be bounded. Suppose that Assumptions A and B hold true. Assume that u k = (x k , x k-1 ), k ∈ N. Then, each limit point in the set ω(u 0 ) is a critical point of Ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Since u k</head><p>k∈N is assumed to be bounded, the set ω(u 0 ) is nonempty. Thus there exists u * = (x * 1 , x * 2 , x1 , x2 ) which is a limit point of {u k l } l∈N , which is a subsequence of u k k∈N . We will prove that u * is a critical point of Ψ (see (4.6)). From condition (C2), for some w k ∈ ∂Ψ(u k ), we have that</p><formula xml:id="formula_80">w k ≤ ρ 2 u k -u k-1 .</formula><p>From Proposition 4.3, it follow that for any N ∈ N, we have (4.17)</p><formula xml:id="formula_81">ρ 1 N k=0 u k+1 -u k 2 ≤ Ψ u 0 -Ψ u N +1 .</formula><p>Since F is bounded from below (see Assumption A(ii)) and the fact that Ψ(•) ≥ F (•) we obtain that Ψ is also bounded from below. Thus, letting N → ∞ in (4.17) yields that </p><formula xml:id="formula_82">(4.18) ∞ k=0 u k+1 -u k 2 &lt; ∞,</formula><formula xml:id="formula_83">f 1 x k 1 ≥ f 1 (x * 1 ) and lim inf k→∞ f 2 x k 2 ≥ f 2 (x * 2 ) .</formula><p>From the iterative step (2.7), we have, for all integer k, that</p><formula xml:id="formula_84">x k+1 1 ∈ argmin x 1 ∈R n 1 x 1 -y k 1 , ∇ x 1 H z k 1 , x k 2 + τ k 1 2 x 1 -x k 1 2 + f 1 (x 1 ) .</formula><p>Thus letting x 1 = x * 1 in the above, we get</p><formula xml:id="formula_85">x k+1 1 -y k 1 , ∇ x 1 H z k 1 , x k 2 + τ k 1 2 x k+1 1 -x k 1 2 + f 1 x k+1 1 ≤ x * 1 -y k 1 , ∇ x 1 H z k 1 , x k 2 + τ k 1 2 x * 1 -x k 1 2 + f 1 (x * 1 )</formula><p>.</p><p>Choosing k = k l -1 and letting k goes to infinity, we obtain lim sup</p><formula xml:id="formula_86">l→∞ f 1 x k l 1 ≤ lim sup l→∞ x * 1 -x k l 1 , ∇ x 1 H z k l -1 1 , x k l -1 2 + τ k l -1 1 2 x * 1 -x k l -1 1 2 + f 1 (x * 1 ) , (4.21)</formula><p>where we have used the facts that both sequences x k k∈N (and therefore z k k∈N ) and {τ k 1 } k∈N (see Remark 4.2) are bounded, ∇H continuous and that the distance between two successive iterates tends to zero (see <ref type="bibr">(4.19)</ref>). For that very reason we also have that x k l 1 → x * 1 as l → ∞, and hence (4.21) reduces to lim sup l→∞ f 1 (x k l 1 ) ≤ f 1 (x * 1 ). Thus, in view of (4.20), f 1 (x k l 1 ) tends to f 1 (x </p><formula xml:id="formula_87">Ψ u k l = lim l→∞ f 1 x k l 1 + f 2 x k l 2 + H x k l + δ 1 2 x k l 1 -x k l -1 1 2 + δ 2 2 x k l 2 -x k l -1 2 2 = f 1 (x * 1 ) + f 2 (x * 2 ) + H (x * 1 , x * 2 ) = F (x * 1 , x * 2 ) (4.22) = Ψ (u * ) .</formula><p>Now, the closedness property of ∂Ψ (see Remark 3.1) implies that 0 ∈ ∂Ψ(u * ), which proves that u * is a critical point of Ψ. This proves condition (C3). Now using the convergence proof methodology of <ref type="bibr" target="#b10">[10]</ref>, which is summarized in Theorem 3.1, we can obtain the following result. Corollary 4.2. Let x k k∈N be a sequence generated by iPALM which is assumed to be bounded. Suppose that Assumptions A and B hold true. Assume that</p><formula xml:id="formula_88">u k = (x k , x k-1 ), k ∈ N.</formula><p>If F is a KL function, then the sequence u k k∈N converges to a critical point u * of Ψ.</p><p>Proof. The proof follows immediately from Theorem 3.1 since Proposition 4.3 proves that condition (C1) holds true, Proposition 4.4 proves that condition (C2) holds true, and condition (C3) was proved in Proposition 4.5. It is also clear that if F is a KL function, then obviously Ψ is a KL function since we just add two quadratic functions.</p><p>To conclude the convergence theory of iPALM we have to show that the sequence x k k∈N which is generated by iPALM converges to a critical point of F (see (2.1)).  </p><formula xml:id="formula_89">0 ∈ ∂F (x * ). Since u * is a critical point of Ψ, it means that 0 ∈ ∂Ψ(u * ). Thus 0 ∈ (∂ x 1 F (u * 1 ) + δ 1 (u * 11 -u * 12 ) , ∂ x 2 F (u * 2 ) + δ 2 (u * 21 -u * 22 ) , δ 1 (u * 11 -u * 12 ) , δ 2 (u * 21 -u * 22 )) , which means that 0 ∈ (∂ x 1 F (u * 1 ) , ∂ x 2 F (u * 2 )) = ∂F (x * ) .</formula><p>This proves that x * is a critical point of F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical results.</head><p>In this section we consider several important applications in image processing and machine learning to illustrate the numerical performance of the proposed iPALM method. All algorithms have been implemented in MATLAB R2013a and executed on a server with Xeon(R) E5-2680 v2 at 2.80GHz CPU and running Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Nonnegative matrix factorization.</head><p>In our first example we consider the problem of using the NMF to decompose a set of facial images into a number of sparse basis faces, such that each face of the database can be approximated by using a small number of those parts. We use the ORL database <ref type="bibr" target="#b32">[32]</ref> that consists of 400 normalized facial images. In order to enforce sparsity in the basis faces, we additionally consider an 0 sparsity constraint (see, for example, <ref type="bibr" target="#b27">[27]</ref>). The sparse NMF problem to be solved is given by (5.1) min</p><formula xml:id="formula_90">B,C 1 2 A -BC 2 : B, C ≥ 0, b i 0 ≤ s, i = 1, 2, . . . , r ,</formula><p>where A ∈ R m×n is the data matrix, organized in a way that each column of the matrix A corresponds to one face of size m = 64 × 64 pixels. In total, the matrix holds n = 400 faces. (See Figure <ref type="figure" target="#fig_2">1</ref> for a visualization of the data matrix A.) The matrix B ∈ R m×r holds the r basis vectors b i ∈ R m×1 , where r corresponds to the number of sparse basis faces. The sparsity constraint applied to each basis face requires that the number of nonzero elements in each column vector b i , i = 1, 2, . . . , r should have less or equal to s nonzero elements. Finally, the matrix C ∈ R r×n corresponds to the coefficient vectors. The application of the proposed iPALM algorithm to this problem is straightforward. The first block of variables corresponds to the matrix B and the second block of variables corresponds to the matrix C. Hence, the smooth coupling function of both blocks is given by</p><formula xml:id="formula_91">H (B, C) = 1 2 A -BC 2 .</formula><p>The block-gradients and respective block-Lipschitz constants are easily computed via</p><formula xml:id="formula_92">∇ B H (B, C) = (BC -A) C T , L 1 (C) = CC T 2 , ∇ C H (B, C) = B T (BC -A) , L 2 (B) = B T B 2 .</formula><p>The nonsmooth function for the first block, f 1 (B), is given by the nonnegativity constraint B ≥ 0 and the 0 sparsity constraint applied to each column of the matrix B, that is,</p><formula xml:id="formula_93">f 1 (B) = 0, B ≥ 0, b i 0 ≤ s, i = 1, 2, . . . , r, ∞, else.</formula><p>Although this function is an indicator function of a nonconvex set, it is shown in <ref type="bibr" target="#b10">[10]</ref> that its proximal mapping can be computed very efficiently (in fact in linear time) via</p><formula xml:id="formula_94">B = prox f 1 B ⇔ b i = T s b+ i , i = 1, 2, . . . , r,</formula><p>where b+ i = max{ bi , 0} denotes an elementwise truncation at zero and the operator T s ( b+ i ) corresponds to first sorting the values of b+ i , keeping the s largest values and setting the remaining m -s values to zero.</p><p>The nonsmooth function corresponding to the second block is simply the indicator function of the nonnegativity constraint of C, that is, and its proximal mapping is trivially given by</p><formula xml:id="formula_95">f 2 (C) = 0, C ≥ 0, ∞, else,</formula><formula xml:id="formula_96">C = prox f 2 Ĉ = Ĉ+ ,</formula><p>which is again an elementwise truncation at zero. In our numerical example we set r = 25, that is, we seek for 25 sparse basis images. Figure <ref type="figure">2</ref> shows the results of the basis faces when running the iPALM algorithm for different sparsity settings. One can see that for smaller values of s, the algorithm leads to more compact representations. This might improve the generalization capabilities of the representation.</p><p>In order to investigate the properties of iPALM on the inertial parameters, we run the algorithm for a specific sparsity setting (s = 33%) using different constant settings of α i and β i for i = 1, 2. From our convergence theory (see Proposition 4.3) it follows that the parameters δ i , i = 1, 2, have to be chosen as constants. However, in practice we shall use a varying parameter δ k i , i = 1, 2, and assume that the parameters will become constant after a certain number of iterations. Observe that the nonsmooth function of the first block is nonconvex ( 0 constraint), while the nonsmooth function of the second block is convex (nonnegativity constraint), which will affect the rules to compute the parameters (see Remark 4.1).</p><p>We compute the parameters τ k i , i = 1, 2, by invoking (4.7) and (4.8), where we practically choose ε = 0. Hence, for the first, completely nonconvex block, we have</p><formula xml:id="formula_97">δ k 1 = α k 1 + β k 1 1 -2α k 1 L 1 (x k 2 ), τ k 1 = δ k 1 + 1 + β k 1 L 1 (x k 2 ) 1 -α k 1 ⇒ τ k 1 = 1 + 2β k 1 1 -2α k 1 L 1 (x k 2 ),</formula><p>from which it directly follows that α ∈ [0, 0.5).</p><p>For the second block, where the nonsmooth function is convex, we follow Remark 4.1 and invoke (4.10) and (4.11) to obtain</p><formula xml:id="formula_98">δ k 2 = α k 2 + 2β k 2 2 1 -2α k 2 L 2 (x k+1 1 ), τ k 2 = δ k 2 + 1 + β k 2 L 2 (x k+1 1 ) 2 -α k 2 ⇒ τ k 2 = 1 + 2β k 2 2 1 -α k 2 L 2 (x k+1 1 )</formula><p>.</p><p>Table <ref type="table">1</ref> Values of the objective function of the sparse NMF problem after K iterations, using different settings for the inertial parameters αi and βi, i = 1, 2, and using computation of the exact Lipschitz constant. Table <ref type="table">2</ref> Values of the objective function of the sparse NMF problem after K iterations, using different settings for the inertial parameters αi and βi, i = 1, 2, and using backtracking to estimate the Lipschitz constant. Comparing this parameter to the parameter of the first block we see that now α ∈ [0, 1) and the value of τ is smaller by a factor of 2. Hence, convexity in the nonsmooth function allows for twice larger steps.</p><formula xml:id="formula_99">K = 100 K = 500 K = 1000 K = 5000 time (s) α1,2 = β1,2 = 0.</formula><formula xml:id="formula_100">K = 100 K = 500 K = 1000 K = 5000 time (s) α1,2 = β1,2 = 0.</formula><p>In Tables <ref type="table">1</ref> and<ref type="table">2</ref>, we report the performance of the iPALM algorithm for different settings of the inertial parameters α k i and β k i , i = 1, 2, and k ∈ N. Since we are solving a nonconvex problem, we report the values of the objective function after a certain number of iterations (100, 500, 1000, and 5000). As already mentioned, setting α i = β i = 0, i = 1, 2, reverts the proposed iPALM algorithm to the PALM algorithm <ref type="bibr" target="#b10">[10]</ref>. In order to estimate the (local) Lipschitz constants L 1 (x k 2 ) and L 2 (x k+1 1 ) we computed the exact values of the Lipschitz constants by computing the largest eigenvalues of C k (C k ) T and (B k ) T B k , respectively. The results are given in Table <ref type="table">1</ref>. Furthermore, we also implemented a standard backtracking procedure, see, for example, <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b24">24]</ref>, which makes use of the descent lemma in order to estimate the value of the (local) Lipschitz constant. In terms of iterations of iPALM, the backtracking procedure generally leads to a better overall performance, but each iteration also takes more time compared to the exact computation of the Lipschitz constants. The results based on backtracking are shown in Table <ref type="table">2</ref>.</p><p>We tried the following settings for the inertial parameters.</p><p>• Equal. Here we used the same inertial parameters for both the first and the second block. In case all parameters are set to be zero, we recover PALM. We observe that the Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php use of the inertial parameters can speed up the convergence, but for too large inertial parameters we also observe not as good results as in the case that we use PALM, i.e., no inertial is used. Since the problem is highly nonconvex the final value of the objective function can be missleading since it could correspond to a bad stationary point. • Double. Since the nonsmooth function of the second block is convex, we can take twice larger inertial parameters. We observe an additional speedup by taking twice larger inertial parameters. Again, the same phenomena occur here; too large inertial parameters yields inferior performances of the algorithm. • Dynamic. We also report the performance of the algorithm in case we choose dynamic inertial parameters similar to accelerated methods in smooth convex optimization <ref type="bibr" target="#b21">[21]</ref>.</p><p>We use</p><formula xml:id="formula_101">α k i = β k i = (k -1)/(k + 2), i = 1, 2</formula><p>, and we set the parameters</p><formula xml:id="formula_102">τ k 1 = L 1 (x k 2 ) and τ k 2 = L 2 (x k+1<label>1</label></formula><p>). One can see that this setting outperforms the other settings by a large margin. Although our current convergence analysis does not support this setting, it shows the great potential of using inertial algorithms when tackling nonconvex optimization problems. The investigation of the convergence properties in this setting will be subject to future research. Finally, we also compare the proposed algorithm to the iPiano algorithm <ref type="bibr" target="#b24">[24]</ref>, which is similar to the proposed iPALM algorithm but does not make use of the block structure. Note that in theory, iPiano is not applicable since the gradient of the overall problem is not Lipschitz continuous. However, in practice it turns out that using a backtracking procedure to determine the Lipschitz constant of the gradient is working, and hence we show comparisons. In the iPiano algorithm, the step-size parameter τ (α in terms of <ref type="bibr" target="#b24">[24]</ref>) was set as</p><formula xml:id="formula_103">τ ≤ 1 -2β L ,</formula><p>from which it follows that the inertial parameter β can be chosen in the interval [0, 0.5). We used β = 0.4 since it gives the best results in our tests. Observe that the performance of iPALM is much better compared to the performance of iPiano. In terms of CPU time, one iteration of iPiano is clearly slower than one iteration of iPALM using the exact computation of the Lipschitz constant, because the backtracking procedure is computationally more demanding than the computation of the largest eigenvalues of C k (C k ) T and (B k ) T B k . Comparing the iPiano algorithm to the version of iPALM which uses backtracking, one iteration of iPiano is faster since iPALM needs to backtrack the Lipschitz constants for both of the two blocks.</p><p>5.2. Blind image deconvolution. In our second example, we consider the well-studied (yet challenging) problem of BID. Given a blurry and possibly noisy image f ∈ R M of M = m 1 ×m 2 pixels, the task is to recover both a sharp image u of the same size and the unknown point spread function b ∈ R N , which is a small 2D blur kernel of size N = n 1 × n 2 pixels. We shall assume that the blur kernel is normalized, that is, b ∈ ∆ N , where ∆ N denotes the standard unit simplex defined by </p><formula xml:id="formula_104">(5.2) ∆ N = b ∈ R N : b i ≥ 0, i = 1,</formula><formula xml:id="formula_105">U M = u ∈ R M : u i ∈ [0, 1] , i = 1, 2, . . . , M .</formula><p>We consider here a classical blind image deconvolution model (see, for example, <ref type="bibr" target="#b28">[28]</ref>) defined by <ref type="bibr">(5.4</ref>) min</p><formula xml:id="formula_106">u,b    8 p=1 φ (∇ p u) + λ 2 u * m 1 ,m 2 b -f 2 : u ∈ U M , b ∈ ∆ N    .</formula><p>The first term is a regularization term which favors sharp images and the second term is a data fitting term that ensures that the recovered solution approximates the given blurry image. The parameter λ &gt; 0 is used to balance between regularization and data fitting. The linear operators ∇ p are finite differences approximation to directional image gradients, which in implicit notation are given by</p><formula xml:id="formula_107">(∇ 1 u) i,j = u i+1,j -u i,j , (∇ u) i,j = u i,j+1 -u i,j , (∇ 3 u) i,j = u i+1,j+1 -u i,j √ 2 , (∇ 4 u) i,j = u i+1,j-1 -u i,j √ 2 (∇ 5 u) i,j = u i+2,j+1 -u i,j √ 5 , (∇ 6 u) i,j = u i+2,j-1 -u i,j √ 5 , (∇ 7 u) i,j = u i+1,j+2 -u i,j √ 5 , (∇ 8 u) i,j = u i-1,j+2 -u i,j √ 5</formula><p>for 1 ≤ i ≤ m 1 and 1 ≤ j ≤ m 2 . We assume natural boundary conditions, that is, (∇ p ) i,j = 0, whenever the operator references a pixel location that lies outside the domain. The operation u * m 1 ,m 2 b denotes the usual 2D modulo-m 1 , m 2 discrete circular convolution operation defined by (and interpreting the image u and the blur kernel b as 2D arrays)</p><formula xml:id="formula_108">(5.5) (u * m 1 ,m 2 b) i,j = n 1 k=0 n 2 l=0 b k,l u (i-k) mod m 1 ,(j-l) mod m 2 , 1 ≤ i ≤ m 1 , 1 ≤ j ≤ m 2 .</formula><p>For ease of notation, we will rewrite the 2D discrete convolutions as the matrix vector products of the form</p><formula xml:id="formula_109">(5.6) v = u * m 1 ,m 2 b ⇔ v = K (b) u ⇔ v = K (u) b,</formula><p>where K(b) ∈ R M ×M is a sparse matrix, each row holds the values of the blur kernel b, and K(u) ∈ R M ×N is a dense matrix, where each column is given by a circularly shifted version of the image u. Finally, the function φ(•) is a differentiable robust error function that promotes sparsity in its argument. For a vector x ∈ R M , the function φ is defined as</p><formula xml:id="formula_110">(5.7) φ (x) = M i=1 log 1 + θx 2 i ,</formula><p>where θ &gt; 0 is a parameter.  the function promotes sparsity in the edges of the image. Hence, we can expect that sharp images result in smaller values of the objective function than blurry images. For images that can be well described by piecewise constant functions (see, for example, the books image in Figure <ref type="figure">3</ref>), such a sparsity promoting function might be well suited to favor sharp images, but we would like to stress that this function could be a bad choice for textured images, since a sharp image usually has much stronger edges than the blurry image. This often leads to the problem that the trivial solution (b being the identity kernel and u = f ) has a lower energy compared to the true solution.</p><p>In order to apply the iPALM algorithm, we identify the function</p><formula xml:id="formula_111">(5.8) H b) = 8 p=1 φ (∇ p u) + λ 2 u * m 1 ,m 2 b -f 2 ,</formula><p>which is smooth with block Lipschitz continuous gradients given by</p><formula xml:id="formula_112">∇ u H (u, b) = 2θ 8 p=1 ∇ T p vec (∇ p u) i,j 1 + θ (∇ p u) 2 i,j m 1 ,m 2 i,j=1 + λK T (b) (K (b) u -f ) , ∇ b H (u, b) = λK T (u) (K (u) b -f ) ,</formula><p>where the operation vec(•) denotes the formation of a vector from the values passed to its argument. The nonsmooth function of the first block is given by (5.9)</p><formula xml:id="formula_113">f 1 (u) = 0, u ∈ U M , ∞, else,</formula><p>and the proximal map with respect to f 1 is computed as</p><formula xml:id="formula_114">(5.10) u = prox f 1 (û) ⇔ u i,j = max {0, min(1, û i,j )} .</formula><p>The nonsmooth function of the second block is given by the indicator function of the unit simplex constraint, that is,</p><formula xml:id="formula_115">(5.11) f 2 (b) = 0, b ∈ ∆ N , ∞, else.</formula><p>In order to compute the proximal map with respect to f 2 , we use the algorithm proposed in <ref type="bibr" target="#b13">[13]</ref>, which computes the projection onto the unit simplex in O(N log N ) time.</p><p>We applied the BID problem to the books image of size m 1 × m 2 = 495 × 323 pixels (see Figure <ref type="figure">3</ref>). We set λ = 10 6 , θ = 10 4 and generated the blurry image by convolving it with a s-shaped blur kernel of size n 1 × n 2 = 31 × 31 pixels. Since the nonsmooth functions of both blocks are convex, we can set the parameters as (compare this to the first example) (5.12)</p><formula xml:id="formula_116">τ k 1 = 1 + 2β k 1 2 1 -α k 1 L 1 (x k 2 ) and τ k 2 = 1 + 2β k 2 2 1 -α k 2 L 2 (x k+1 1 ).</formula><p>To determine the values of the (local) Lipschitz constants, we used again a backtracking scheme <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b24">24]</ref>. In order to avoid the trivial solution (that is, u = f and b being the identity kernel) we took smaller descent steps in the blur kernel which was realized by multiplying τ k 2 , k ∈ N, by a factor of c = 5. Note that this form of "preconditioning" does not violate any step-size restrictions as we can always take larger values for τ than the value computed in (5.12). Table <ref type="table" target="#tab_15">3</ref> shows an evaluation of the iPALM algorithm using different settings of the inertial parameters α i and β i for i = 1, 2. First, we observe that the use of inertial parameters lead to higher values of the objective function after a smaller number of iterations. However, for a larger number of iterations the use of inertial forces leads to significantly lower values. Again, the use of dynamic inertial parameters together with the parameter τ k</p><formula xml:id="formula_117">1 = L 1 (x k 2 ) and τ k 2 = L 2 (x<label>k+1</label></formula><p>1 ) leads to the best overall performance. In Figure <ref type="figure">3</ref> we show the results of the blind deconvolution problem. One can see that the quality of the recovered image as well as the recovered blur kernel is much better using inertial forces. Note that the recovered blur kernel is very close to the true blur kernel but the recovered image appears slightly more piecewise constant than the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Convolutional LASSO.</head><p>In our third experiment we address the problem of sparse approximation of an image using dictionary learning. Here, we consider the convolutional LASSO model <ref type="bibr" target="#b36">[36]</ref>, which is an interesting variant of the well-known patch-based LASSO model <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b0">1]</ref> for sparse approximations. The convolutional model inherently models the transnational invariance of images, which can be considered as an advantage over the usual patch-based model which treats every patch independently.</p><p>The idea of the convolutional LASSO model is to learn a set of small convolution filters d j ∈ R l×l , j = 1, 2, . . . , p, such that a given image f ∈ R m×n can be written as f ≈ p j=1 d j * m,n v j , where * m,n denotes again the 2D modulo m, n discrete circular convolution operation and v j ∈ R m×n are the corresponding coefficient images which are assumed to be sparse. In order to make the convolution filters capture the high-frequency information in the image, we fix the first filter d 1 to be a Gaussian (low pass) filter g with standard deviation σ l and we set the corresponding coefficient image v j equal to the initial image f . Furthermore, we assume that the remaining filters d j , j = 2, 3, . . . , p have zero mean as well as a 2 -norm less or equal to 1. In order to impose a sparsity prior on the coefficient images v j we make use of the 1 -norm. The corresponding objective function is hence given by min where the parameter λ &gt; 0 is used to control the degree of sparsity. It is easy to see that the convolutional LASSO model nicely fits to the class of problems that can be solved using the proposed iPALM algorithm. We leave the details to the interested reader. In order to compute the (local) Lipschitz constants we again made use of a backtracking procedure and the parameters τ k i , i = 1, 2, were computed using (5.12). We applied the convolutional LASSO problem to the Barbara image of size 512×512 pixels, which is shown in Figure <ref type="figure">4</ref>. The Barbara image contains a lot of stripe-like texture and hence we expect that the learned convolution filters will contain these characteristic structures. In our experiment, we learned a dictionary made of 81 filter kernels, each of size 9 × 9 pixels. The regularization parameter λ was set to λ = 0.2. From Figure <ref type="figure">4</ref> it can be seen that the learned convolution filters indeed contain stripe-like structures of different orientations but also other filters that are necessary to represent the other structures in the image. Table <ref type="table" target="#tab_16">4</ref> summarizes the performance of the iPALM algorithm for different settings of the inertial parameters. From the results, one can see that larger settings of the inertial parameters lead to a consistent improvement of the convergence speed. Again, using a dynamic choice of the inertial parameters clearly outperforms the other settings.</p><p>For illustration purposes we finally applied the learned dictionary to denoise a noisy variant of the same Barbara image. The noisy image has been generated by adding zero-mean Gaussian noise with standard deviation σ = 0.1 to the original image. For denoising we use the previously learned dictionary d and minimizing the convolutional LASSO problem only with respect to the coefficient images v. Note that this is a convex problem and hence it can be efficiently minimized using for example the FISTA algorithm <ref type="bibr" target="#b6">[6]</ref>. The denoised image is again shown in Figure <ref type="figure">4</ref>. Observe that the stripe-like texture is very well preserved in the denoised image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion.</head><p>In this paper we proposed iPALM which an inertial variant of the PALM method proposed in <ref type="bibr" target="#b10">[10]</ref> for solving a broad class of nonconvex and nonsmoooth optimization problems consisting of block-separable nonsmooth, nonconvex functions with easy to compute proximal mappings and a smooth coupling function with block-Lipschitz continuous gradients. We studied the convergence properties of the algorithm and provide bounds on the inertial and step-size parameters that ensure convergence of the algorithm to a critical point of the problem at hand. In particular, we showed that in case the objective function satisfies the KL property, we can obtain a finite length property of the generated sequence of iterates. In several numerical experiments we show the advantages of the proposed algorithm to minimize a number of well-studied problems and image processing and machine learning. In our experiments we found that choosing the inertial and step-size parameters dynamically, as pioneered by Nesterov <ref type="bibr" target="#b22">[22]</ref>, leads to a significant performance boost, both in terms of convergence speed and convergence to a "better" critical point of the problem. Our current convergence theory does not support this choice of parameters, but developing a more general convergence theory will be interesting and a subject for future research. Thus, we only remain to show that</p><formula xml:id="formula_118">(1 -ε) δ * - (1 + ε) δ * α + L (α + β) 1 -α ≥ 0.</formula><p>Indeed, simple manipulations yields the following equivalent inequality:</p><formula xml:id="formula_119">(7.3) (1 -ε -2α) δ * ≥ L (α + β) .</formula><p>Using now (7.1) and the facts that α ≤ ᾱ, β ≤ β and 0 &lt; L ≤ λ we obtain that (7.3) holds true. This completes the proof of the lemma.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Downloaded 11 /</head><label>11</label><figDesc>16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. ORL database which includs 400 faces which we used in our NMF example.</figDesc><graphic coords="20,72.00,102.34,442.85,110.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Downloaded 11 /Figure 2 .</head><label>112</label><figDesc>Figure 2. 25 basis faces using different sparsity settings. A sparsity of s = 25% means that each basis face contains only 25% nonzero pixels. Clearly, stronger sparsity leads to a more compact representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 k+2Figure 3 .</head><label>13</label><figDesc>Figure 3. Results of blind deconvolution using K = 5000 iterations and different settings of the inertial parameters. The result without inertial terms (i.e., αi = βi = 0 for i = 1, 2) is significantly worse compared to the result using inertial terms. The best results are obtained using the dynamic choice of the inertial parameters.</figDesc><graphic coords="25,72.00,331.71,132.87,203.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>d j * m,n v j -f 2 2 ,Figure 4 .</head><label>24</label><figDesc>Figure 4. Results of dictionary learning using the convolutional LASSO model. Observe that the learned dictionary very well captures the stripe-like texture structures of the Barbara image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 .Lemma 7 . 1 .</head><label>771</label><figDesc>Appendix A: Proof of Lemma 4.1. We first recall the result that should be proved. Consider the functions g : R 5 + → R and h : R 5 + → R defined asg (α, β, δ, τ, L) = τ (1 -α) -(1 + β) L -δ, h (α, β, δ, τ, L) = δ -τ α -Lβ.Let ε &gt; 0 and ᾱ &gt; 0 be two real numbers for which 0 ≤ α ≤ ᾱ &lt; 0.5(1 -ε). Assume, in addition, that 0 ≤ L ≤ λ for some λ &gt; 0 and 0≤ β ≤ β with β &gt; 0. If δ then g(α, β, δ * , τ * , L) = εδ * and h(α, β, δ * , τ * , L) ≥ εδ * .Proof. From the definition of g we immediately obtain thatg (α, β, δ * , τ * , L) = τ * (1 -α) -(1 + β) L -δ * = εδ * ,which proves the first desired result. We next simplify h(α, β, δ * , τ * , L) -εδ * as follows:h (α, β, δ * , τ * , L) -εδ * = (1 -ε) δ * -τ * α -Lβ = (1 -ε) δ * -(1 + ε) δ * + L (1 + β) 1 -α α -Lβ = (1 -ε) δ * -(1 + ε) δ * α + L (1 + β) α + Lβ (1 -α) 1 -α = (1 -ε) δ * -(1 + ε) δ * α + L (α + β) 1 -α .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Now we prove the following property of the sequence {(x k 1 , x k 2 )} k∈N generated by iPALM. Proposition 4.2. Suppose that Assumption A holds. Let {(x k 1 , x k</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where the third and fourth inequalities follow from (2.9), the fact that the sequence {τ k 2 } k∈N is bounded from above by τ + 2 (see Remark 4.2), and β k 2 ≤ 1 for all k ∈ N. Summing up these estimations, we get from (4.16) that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpThis fact together with condition (C1) implies that w k → 0 as k → ∞. Thus, in order to use the closedness property of ∂Ψ (see Remark 3.1) it remains to show that {Ψ(u k )} k∈N converges to Ψ(u * ). Since f 1 and f 2 are lower semicontinuous (see Assumption A(i)), we</figDesc><table><row><cell>obtain that</cell><cell></cell><cell></cell></row><row><cell>(4.20)</cell><cell>lim inf k→∞</cell><cell></cell></row><row><cell cols="2">which means that</cell><cell></cell></row><row><cell>(4.19)</cell><cell>lim k→∞</cell><cell>u k+1 -u k = 0.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>4.1 (convergence of iPALM). Let x k k∈N be a sequence generated by iPALM which is assumed to be bounded. Suppose that Assumptions A and B hold true. If F is a KL function, then the sequence x k k∈N converges to a critical point x * of F . Proof. From Corollary 4.2 we have that the sequence u k k∈N converges to a critical point u * = (u * 11 , u * 12 , u * 21 , u * 22 ) of Ψ. Therefore, obviously also the sequence x k k∈N converges. Let x * be the limit point of x k k∈N . Hence u * 1 = (u * 11 , u * 12 ) = x * and u * 2 = (u * 21 , u * 22 ) = x * . (See 4.6.) We will prove that x * is a critical point of F (see (2.1)), that is, we have to show that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>2, . . . , N, Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpFurthermore, we shall assume that the pixel intensities of the unknown sharp image u are normalized to the interval [0, 1], that is, u ∈ U M , where(5.3)   </figDesc><table><row><cell>N</cell></row><row><cell>b i = 1 .</cell></row><row><cell>i=1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Here, since the argument of the function are image gradients, Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 3</head><label>3</label><figDesc>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Values of the objective function of the BID problem, after K iterations and using different settings for the inertial parameters αi and βi for i = 1, 2.</figDesc><table><row><cell></cell><cell>K = 100</cell><cell>K = 500</cell><cell>K = 1000</cell><cell>K = 5000</cell><cell>time (s)</cell></row><row><cell>α1,2 = β1,2 = 0.0</cell><cell cols="4">2969668.92 1177462.72 1031575.57 847268.70</cell><cell>1882.63</cell></row><row><cell>α1,2 = β1,2 = 0.4</cell><cell cols="4">5335748.90 1402080.44 1160510.16 719295.30</cell><cell>1895.61</cell></row><row><cell>α1,2 = β1,2 = 0.8</cell><cell cols="4">5950073.38 1921105.31 1447739.06 780109.56</cell><cell>1888.25</cell></row><row><cell cols="2">α k 1,2 = β k 1,2 = (k -1)/(k + 2) 2014059.03</cell><cell>978234.23</cell><cell>683694.72</cell><cell>678090.51</cell><cell>1867.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 4</head><label>4</label><figDesc>Values of the objective function for the convolutional LASSO model using different settings of the inertial parameters.</figDesc><table><row><cell></cell><cell cols="4">K = 100 K = 200 K = 500 K = 1000</cell><cell>time (s)</cell></row><row><cell>α1,2 = β1,2 = 0.0</cell><cell>336.13</cell><cell>328.21</cell><cell>322.91</cell><cell>321.12</cell><cell>3274.97</cell></row><row><cell>α1,2 = β1,2 = 0.4</cell><cell>329.20</cell><cell>324.62</cell><cell>321.51</cell><cell>319.85</cell><cell>3185.04</cell></row><row><cell>α1,2 = β1,2 = 0.8</cell><cell>325.19</cell><cell>321.38</cell><cell>319.79</cell><cell>319.54</cell><cell>3137.09</cell></row><row><cell>α k 1,2 = β k 1,2 = (k -1)/(k + 2)</cell><cell>323.23</cell><cell>319.88</cell><cell>318.64</cell><cell>318.44</cell><cell>3325.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 11/16/16 to 139.80.123.51. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. http://www.siam.org/journals/siims/9-4/M106406.html</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of the first author was supported by the Austrian science fund (FWF) under the project EANOI, I1148, and the ERC starting grant HOMOVIS, 640156.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-Svd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An inertial proximal method for maximal monotone operators via discretization of a nonlinear oscillator with damping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Set-Valued Anal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 11/16/16 to 139.80.123.51</idno>
		<ptr target="http://www.siam.org/journals/ojsa.phpCopyright©bySIAM" />
		<imprint/>
	</monogr>
	<note>Unauthorized reproduction of this article is prohibited</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the convergence of the proximal algorithm for nonsmooth functions involving analytic features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="5" to="16" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Lojasiewicz inequality</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Redont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soubeyran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="438" to="457" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convergence of descent methods for semi-algebraic and tame problems: Proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="91" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<title level="m">Parallel and Distributed Computation</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1205" to="1223" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizations of Lojasiewicz inequalities: Subgradient flows, talweg, convexity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mazet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="3319" to="3363" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proximal alternating linearized minimization for nonconvex and nonsmooth problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Ser. A</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="459" to="494" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Signal recovery by proximal forward-backward splitting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Wajs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1168" to="1200" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance of first-order methods for smooth convex minimization: A novel approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="451" to="482" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient projections onto the l 1-ball for learning in high dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An inertial Tseng&apos;s type proximal algorithm for nonsmooth and nonconvex optimization problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Bot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Csetnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Patter Recognition</title>
		<meeting>Computer Vision and Patter Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Splitting algorithms for the sum of two nonlinear operators</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Lions</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="964" to="979" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational Analysis and Generalized Differentiation: I</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Mordukhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Grundlehren Math. Wiss.</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Proximitéet dualité dans un espace Hilbertien</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Soc. Math. France</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="299" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introductory Lectures on Convex Optimization, Appl. Optim</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="issue">1/k 2</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Numerical Optimization</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inertial proximal algorithm for nonconvex optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1388" to="1419" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: A nonnegative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<title level="m">Sparse nonnegative matrix factorization with 0-constraints, Neurocomputing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Blind deconvolution via lower-bounded logarithmic image priors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diethelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR)</title>
		<meeting>the International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Comput. Math. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Comput. Math. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-B</forename><surname>Wets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Variational Analysis, Grundlehren Math. Wiss</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<date type="published" when="1998">1998</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Parameterisation of a stochastic model for human face identification, in WACV</title>
		<author>
			<persName><forename type="first">F</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized nonnegative matrix approximations with Bregman divergences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Globally Convergent Algorithm for Nonconvex Optimization Based on Block Coordinate Update</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<idno>arXiv:1410:1386</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report, preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Heavy-ball method in nonconvex optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zavriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Kostyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Model</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="336" to="341" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
