<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-12">12 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
							<email>sitao.luan@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
							<email>chenqing.hua@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
							<email>qincheng.lu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
							<email>jiaqi.zhu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
							<email>mingde.zhao@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
							<email>shuyuan.zhang@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
							<email>chang@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-12">12 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.05641v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful 1 for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of both graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity channels in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 10 realworld node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Networks (NNs) <ref type="bibr" target="#b20">[21]</ref> have revolutionized many machine learning areas, including image recognition <ref type="bibr" target="#b19">[20]</ref>, speech recognition <ref type="bibr" target="#b12">[13]</ref> and natural language processing <ref type="bibr" target="#b1">[2]</ref>, etc.One major strength is their capacity and effectiveness of learning latent representation from Euclidean data. Recently, the focus has been put on its applications on non-Euclidean data <ref type="bibr" target="#b4">[5]</ref>, e.g., relational data or graphs. Combining graph signal processing and convolutional neural networks <ref type="bibr" target="#b21">[22]</ref>, numerous Graph Neural Networks (GNNs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> have been proposed which empirically outperform traditional neural networks on graph-based machine learning tasks, e.g., node classification, graph classification, link prediction and graph generation, etc.GNNs are built on the homophily assumption <ref type="bibr" target="#b30">[31]</ref>, i.e., connected nodes tend to share similar attributes with each other <ref type="bibr" target="#b13">[14]</ref>, which offers additional information besides node features. Such relational inductive bias <ref type="bibr" target="#b2">[3]</ref> is believed to be a key factor leading to GNNs' superior performance over NNs' in many tasks.</p><p>Nevertheless, growing evidence shows that GNNs do not always gain advantages over traditional NNs when dealing with relational data. In some cases, even simple Multi-Layer Perceptrons (MLPs) can outperform GNNs by a large margin <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref>. An important reason for the performance degradation is believed to be the heterophily problem, i.e., connected nodes tend to have different labels which makes the homophily assumption fail. Heterophily challenge has received attention recently and there are increasing number of models being put forward to address this problem <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Contributions In this paper, we first demonstrate that not all heterophilous graphs are harmful for aggregation-based GNNs and the existing metrics of homophily are insufficient to decide whether the aggregation operation will make nodes less distinguishable or not. By constructing a similarity matrix from backpropagation analysis, we derive new metrics to depict how much GNNs are influenced by the graph structure and node features. We show the advantage of our metrics over the existing metrics by comparing the ability of characterizing the performance of two baseline GNNs on synthetic graphs of different levels of homophily. From the similarity matrix, we find that diversification operation is able to address some harmful heterophily cases, and based on which we propose Adaptive Channel Mixing (ACM) GNN framework. The experiments on the synthetic datasets, ablation studies and real-world datasets consistently show that the baseline GNNs augmented by ACM framework are able to obtain significant performance boost on node classification tasks on heterophilous graphs.</p><p>The rest of this paper is mainly organized as follows: In section 2, we introduce the notation and the background knowledge. In section 3, we conduct node-wise analysis on heterophily, derive new homophily metrics based on a similarity matrix and conduct experiments to show their advantages over the existing homophily metrics. In section 4, we demonstrate the capability of diversification operation on addressing some cases of harmful heterophily and propose the ACM-GNN framework to adaptively utilize the information from different filterbank channels to address heterophily problem. In section 5, we discuss the related works and clarify the differences to our method. In section 6, we provide empirical evaluations on ACM framework, including ablation study and tests on 10 real-world node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We will introduce the related notation and background knowledge in this section. We use bold fonts for vectors (e.g., v). Suppose we have an undirected connected graph G = (V, E, A), where V is the node set with |V| = N ; E is the edge set without self-loop; A ‚àà R N √óN is the symmetric adjacency matrix with A i,j = 1 iff e ij ‚àà E, otherwise A i,j = 0. We use D to denote the diagonal degree matrix of G, i.e., D i,i = d i = j A i,j and use N i to denote the neighborhood set of node i, i.e., N i = {j : e ij ‚àà E}. A graph signal is a vector x ‚àà R N defined on V, where x i is defined on the node i. We also have a feature matrix X ‚àà R N √óF , whose columns are graph signals and whose i-th row X i,: is a feature vector of node i. We use Z ‚àà R N √óC to denote the label encoding matrix, whose i-th row Z i,: is the one-hot encoding of the label of node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Laplacian, Affinity Matrix and Their Variants</head><p>The (combinatorial) graph Laplacian is defined as L = D ‚àí A, which is Symmetric Positive Semi-Definite (SPSD) <ref type="bibr" target="#b7">[8]</ref>. Its eigendecomposition gives L = U ŒõU T , where the columns u i of U ‚àà R N √óN are orthonormal eigenvectors, namely the graph Fourier basis, Œõ = diag(Œª 1 , . . . , Œª N ) with Œª 1 ‚â§ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚â§ Œª N , and these eigenvalues are also called frequencies. The graph Fourier transform of the graph signal x is defined as T , where u T i x is the component of x in the direction of u i .</p><formula xml:id="formula_0">x F = U ‚àí1 x = U T x = [u T 1 x, . . . , u T N x]</formula><p>In additional to L, some variants are also commonly used, e.g., the symmetric normalized Laplacian L sym = D ‚àí1/2 LD ‚àí1/2 = I ‚àí D ‚àí1/2 AD ‚àí1/2 and the random walk normalized Laplacian L rw = D ‚àí1 L = I ‚àí D ‚àí1 A. The affinity (transition) matrices can be derived from the Laplacians, e.g., A rw = I ‚àí L rw = D ‚àí1 A, A sym = I ‚àí L sym = D ‚àí1/2 AD ‚àí1/2 and are considered to be low-pass filters <ref type="bibr" target="#b29">[30]</ref>. Their eigenvalues satisfy Œª</p><formula xml:id="formula_1">i (A rw ) = Œª i (A sym ) = 1 ‚àí Œª i (L sym ) = 1 ‚àí Œª i (L rw ) ‚àà (‚àí1, 1].</formula><p>Applying the renormalization trick <ref type="bibr" target="#b17">[18]</ref> to affinity and Laplacian matrices respectively leads to √Çsym = D‚àí1/2 √É D‚àí1/2 and Lsym = I ‚àí √Çsym , where √É ‚â° A + I and D ‚â° D + I. The renormalized affinity matrix essentially adds a self-loop to each node in the graph, and is widely used in Graph Convolutional Network (GCN) <ref type="bibr" target="#b17">[18]</ref> as follows,</p><formula xml:id="formula_2">Y = softmax( √Çsym ReLU( √Çsym XW 0 ) W 1 )<label>(1)</label></formula><p>where W 0 ‚àà R F √óF1 and W 1 ‚àà R F1√óO are learnable parameter matrices. GCN can be trained by minimizing the following cross entropy loss</p><formula xml:id="formula_3">L = ‚àítrace(Z T log Y )<label>(2)</label></formula><p>where log(‚Ä¢) is a component-wise logarithm operation. The random walk renormalized matrix √Çrw = D‚àí1 √É, which shares the same eigenvalues as √Çsym , can also be applied in GCN. The corresponding Laplacian is defined as Lrw = I ‚àí √Çrw . √Çrw is essentially a random walk matrix and behaves as a mean aggregator that is applied in spatial-based GNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. To bridge the spectral and spatial methods, we use √Çrw in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metrics of Homophily</head><p>The metrics of homophily are defined by considering different relations between node labels and graph structures defined by adjacency matrix. There are three commonly used homophily metrics: edge homophily <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref>, node homophily <ref type="bibr" target="#b32">[33]</ref>, and class homophily <ref type="bibr" target="#b23">[24]</ref> <ref type="foot" target="#foot_1">2</ref> defined as follows:</p><formula xml:id="formula_4">H edge (G) = {e uv | e uv ‚àà E, Z u,: = Z v,: } |E| , H node (G) = 1 |V| v‚ààV {u | u ‚àà N v , Z u,: = Z v,: } d v , H class (G) = 1 C ‚àí 1 C k=1 h k ‚àí {v | Z v,k = 1} N + , h k = v‚ààV {u | Z v,k = 1, u ‚àà N v , Z u,: = Z v,: } v‚àà{v|Z v,k =1} d v<label>(</label></formula><p>3) where [a] + = max(a, 0); h k is the class-wise homophily metric <ref type="bibr" target="#b23">[24]</ref>. They are all in the range of [0, 1] and a value close to 1 corresponds to strong homophily while a value close to 0 indicates strong heterophily. H edge (G) measures the proportion of edges that connect two nodes in the same class; H node (G) evaluates the average proportion of edge-label consistency of all nodes; H class (G) tries to avoid the sensitivity to imbalanced class, which can cause H edge misleadingly large. The above definitions are all based on the graph-label consistency and imply that the inconsistency will cause harmful effect to the performance of GNNs. With this in mind, we will show a counter example to illustrate the insufficiency of the above metrics and propose new metrics in the following section. Metrics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis of Heterophily</head><formula xml:id="formula_5">H !"#! (G) = 0 H $%"! (G) = 0 H &amp;'()) (G) = 0 ùêª agg (G) = 1 ùêª agg * (G) = 1</formula><p>Class 1</p><p>Class 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Example of harmless heterophily</head><p>Heterophily is believed to be harmful for message-passing based GNNs <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7]</ref> because intuitively features of nodes in different classes will be falsely mixed and this will lead nodes indistinguishable <ref type="bibr" target="#b39">[40]</ref>. Nevertheless, it is not always the case, e.g., the bipartite graph shown in Figure <ref type="figure" target="#fig_5">1</ref> is highly heterophilous according to the homophily metrics in (3), but after mean aggregation, the nodes in classes 1 and 2 only exchange colors and are still distinguishable. Authors in <ref type="bibr" target="#b6">[7]</ref>  To analyze to what extent the graph structure can affect the output of a GNN, we first simplify the GCN by removing its nonlinearity as <ref type="bibr" target="#b35">[36]</ref>. Let √Ç ‚àà R N √óN denote a general aggregation operator. Then, equation ( <ref type="formula" target="#formula_2">1</ref>) can be simplified as,</p><formula xml:id="formula_6">Y = softmax( √ÇXW ) = softmax(Y )<label>(4)</label></formula><p>After each gradient decent step ‚àÜW = Œ≥ dL dW , where Œ≥ is the learning rate, the update of Y will be (see Appendix B for derivation),</p><formula xml:id="formula_7">‚àÜY = √ÇX‚àÜW = Œ≥ √ÇX dL dW ‚àù √ÇX dL dW = √ÇXX T √ÇT (Z ‚àí Y ) = S( √Ç, X)(Z ‚àí Y ) (5)</formula><p>where S( √Ç, X) ‚â° √ÇX( √ÇX) T is a post-aggregation node similarity matrix, Z ‚àí Y is the prediction error matrix. The update direction of node i is essentially a weighted sum of the prediction error, i.e., ‚àÜ(Y ) i,:</p><formula xml:id="formula_8">= j‚ààV S( √Ç, X) i,j (Z ‚àí Y ) j,: .</formula><p>To study the effect of heterophily, we first define the aggregation similarity score as follows.</p><p>Definition 1. Aggregation similarity score</p><formula xml:id="formula_9">S agg S( √Ç, X) = v Mean u {S( √Ç, X) v,u |Z u,: = Z v,: } ‚â• Mean u {S( √Ç, X) v,u |Z u,: = Z v,: } |V| (6)</formula><p>where Mean u ({‚Ä¢}) takes the average over u of a given multiset of values or variables.</p><p>S agg (S( √Ç, X)) measures the proportion of nodes v ‚àà V that will put relatively larger similarity weights on nodes in the same class than in other classes after aggregation. It is easy to see that S agg (S( √Ç, X)) ‚àà [0, 1]. But in practice, we observe that in most datasets, we will have S agg (S( √Ç, X)) ‚â• 0.5. Based on this observation, we rescale <ref type="bibr" target="#b5">(6)</ref> to the following modified aggregation similarity for practical usage,</p><formula xml:id="formula_10">S M agg S( √Ç, X) = 2S agg S( √Ç, X) ‚àí 1 +<label>(7)</label></formula><p>In order to measure the consistency between labels and graph structures without considering node features and make a fair comparison with the existing homophily metrics in (3), we define the graph (G) aggregation ( √Ç) homophily and its modified version as</p><formula xml:id="formula_11">H agg (G) = S agg S( √Ç, Z) , H M agg (G) = S M agg S( √Ç, Z)<label>(8)</label></formula><p>In practice, we will only check H agg (G) when H M agg (G) = 0. As Figure <ref type="figure" target="#fig_5">1</ref> shows, when √Ç = √Çrw , H agg (G) = H M agg (G) = 1. Thus, this new metric reflects the fact that nodes in classes 1 and 2 are still highly distinguishable after aggregation, while other metrics mentioned before fail to capture the information and misleadingly give value 0. This shows the advantage of H agg (G) and H M agg (G) by additionally considering information from aggregation operator √Ç and the similarity matrix.</p><p>To comprehensively compare H M agg (G) with the metrics in (3) in terms of how they reveal the influence of graph structure on the GNN performance, we generate synthetic graphs and evaluate SGC <ref type="bibr" target="#b35">[36]</ref> and GCN <ref type="bibr" target="#b17">[18]</ref> on them in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation and Comparison on Synthetic Graphs</head><p>Data Generation &amp; Experimental Setup We first generate 280 graphs in total with 28 edge homophily levels varied from 0.005 to 0.95, each corresponding to 10 graphs. For every generated graph, we have 5 classes with 400 nodes in each class. For each node, we randomly generate 2 intra-class edges and [ 2 h ‚àí 2] inter-class edges (see the details about the data generation process in appendix C). The features of nodes in each class are sampled from node features in the corresponding class of the base dataset. Nodes are randomly split into 60%/20%/20% for train/validation/test. We train 1-hop SGC (sgc-1) <ref type="bibr" target="#b35">[36]</ref> and GCN <ref type="bibr" target="#b17">[18]</ref> on synthetic data (see appendix A.1 for hyperparameter searching range). For each value of H edge (G), we take the average test accuracy and standard deviation of runs over 10 generated graphs. For each generated graph, we also calculate its H node (G), H class (G) and H M agg (G). Model performance with respect to different homophily values are shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Homophily Metrics</head><p>The performance of SGC-1 and GCN are expected to be monotonically increasing with a proper and informative homophily metric. However, Figure <ref type="figure" target="#fig_2">2</ref>  show that the performance curves under H edge (G), H node (G) and H class (G) are U -shaped<ref type="foot" target="#foot_2">3</ref> , while Figure <ref type="figure" target="#fig_2">2</ref>(d) reveals a nearly monotonic curve with a little perturbation around 1. This indicates that H M agg (G) can describe how the graph structure affects the performance of SGC-1 and GCN more appropriately and adequately than the existing metrics.</p><p>In addition, we notice that in Figure <ref type="figure" target="#fig_2">2</ref>(a), both SGC-1 and GCN get the worst performance on all datasets when H edge (G) is around somewhere between 0.1 and 0.2. This interesting phenomenon can be explained by the following theorem based on the similarity matrix which can verify the usefulness of H M agg (G). </p><formula xml:id="formula_12">g(h) ‚â° E S( √Ç, Z) v,u1 ‚àí E S( √Ç, Z) v,u2 = (C ‚àí 1)(hd + 1) ‚àí (1 ‚àí h)d (C ‚àí 1)(d + 1) 2<label>(9)</label></formula><p>and the minimum of g(h) is reached at</p><formula xml:id="formula_13">h = d + 1 ‚àí C Cd = d intra /h + 1 ‚àí C C(d intra /h) ‚áí h = d intra Cd intra + C ‚àí 1</formula><p>where d intra = dh, which is the expected number of neighbors of a node that have the same label as the node.</p><p>The value of g(h) in ( <ref type="formula" target="#formula_12">9</ref>) is the expected differences of the similarity values between nodes in the same class as v and nodes in other classes. g(h) is strongly related to the definition of aggregation homophily and its minimum potentially implies the worst value of H agg (G). In the synthetic experiments, we have d intra = 2, C = 5 and the minimum of g(h) is reached at h = 1/7 ‚âà 0.14, which corresponds to the lowest point in the performance curve in Figure <ref type="figure" target="#fig_2">2(a)</ref>. In other words, the h where SGC-1 and GCN perform worst is where g(h) gets the smallest value, instead of the point with the smallest edge homophily value h = 0. This again reveals the advantage of H agg (G) over H edge (G) by taking use of the similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Channel Mixing (ACM) Framework</head><p>Besides the new homophily metrics, in this section, we will also figure out how diversification operation (high-pass filter) is potentially capable to address some cases of harmful heterophily based on the similarity matrix proposed in equation <ref type="bibr" target="#b4">(5)</ref>. From the analysis, we argue that low-pass filter and high-pass filter should be combined together for feature extraction, which lead us to the filterbank method in subsection 4.2. We generalize filterbank method and propose ACM framework in subsection 4.3.   We first consider the example shown in Figure <ref type="figure" target="#fig_4">3</ref>. From S( √Ç, X), nodes 1,3 assign relatively large positive weights to nodes in class 2 after aggregation, which will make node 1,3 hard to be distinguished from nodes in class 2. Despite the fact, we can still distinguish between nodes 1,3 and 4,5,6,7 by considering their neighborhood difference: nodes 1,3 are different from most of their neighbors while nodes 4,5,6,7 are similar to most of their neighbors. This indicates, in some cases, although some nodes become similar after aggregation, they are still distinguishable via their surrounding dissimilarities. This leads us to use diversification operation, i.e., high-pass (HP) filter I ‚àí √Ç [10] (will be introduced in the next subsection) to extract the information of neighborhood differences and address harmful heterophily. As S(I ‚àí √Ç, X) in Figure <ref type="figure" target="#fig_4">3</ref> shows, nodes 1,3 will assign negative weights to nodes 4,5,6,7 after diversification operation, i.e., nodes 1,3 treat nodes 4,5,6,7 as negative samples and will move away from them during backpropagation. Base on this example, we first propose diversification distinguishability as follows to measures the proportion of nodes that diversification operation is potentially helpful for, Definition 2. Diversification Distinguishability (DD) based on S(I ‚àí √Ç, X).</p><p>Given S(I ‚àí √Ç, X), a node v is diversification distinguishable if the following two conditions are satisfied at the same time,</p><formula xml:id="formula_14">1. Mean u {S(I ‚àí √Ç, X) v,u |u ‚àà V ‚àß Z u,: = Z v,: } ‚â• 0; 2. Mean u {S(I ‚àí √Ç, X) v,u |u ‚àà V ‚àß Z u,: = Z v,: } ‚â§ 0<label>(10)</label></formula><p>Then, graph diversification distinguishability value is defined as</p><formula xml:id="formula_15">DD √Ç,X (G) = 1 |V| {v|v is diversification distinguishable}<label>(11)</label></formula><p>We can see that DD √Ç,X (G) ‚àà [0, 1] . The effectiveness of diversification operation can be proved for binary classification problems under certain conditions based on definition 2, leading us to:</p><p>Theorem 2. (See Appendix E for proof). Suppose X = Z, √Ç = √Çrw . Then, for a binary classification problem, i.e., C = 2, all nodes are diversification distinguishable, i.e., DD √Ç,Z (G) = 1.</p><p>Theorem 2 theoretically demonstrates the importance of diversification operation to extract highfrequency information of graph signal <ref type="bibr" target="#b9">[10]</ref>. Combined with aggregation operation, which is a low-pass filter <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>, we can get a filterbank which uses both aggregation and diversification operations to distinctively extract the low-and high-frequency information from graph signals. We will introduce filterbank in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Filterbank in Spectral and Spatial Forms</head><p>Filterbank For the graph signal x defined on G, a 2-channel linear (analysis) filterbank <ref type="bibr" target="#b9">[10]</ref> <ref type="foot" target="#foot_3">4</ref> includes a pair of low-pass(LP) and high-pass(HP) filters H LP , H HP , where H LP and H HP retain the low-frequency and high-frequency content of x, respectively.</p><p>Most existing GNNs are under uni-channel filtering architecture <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15]</ref> with either H LP or H HP channel that only partially preserves the input information. Unlike the uni-channel architecture, filterbanks with H LP + H HP = I will not lose any information of the input signal, i.e., perfect reconstruction property <ref type="bibr" target="#b9">[10]</ref>.</p><p>Generally, the Laplacian matrices (L sym , L rw , Lsym , Lrw ) can be regarded as HP filters <ref type="bibr" target="#b9">[10]</ref> and affinity matrices (A sym , A rw , √Çsym , √Çrw ) can be treated as LP filters <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, MLPs can be considered as owing a special identity filterbank with matrix I that satisfies</p><formula xml:id="formula_16">H LP + H HP = I + 0 = I.</formula><p>Filterbank in Spatial Form Filterbank methods can also be extended to spatial GNNs. Formally, on the node level, left multiplying H LP and H HP on x performs as aggregation and diversification operations, respectively. For example, suppose H LP = √Ç and H HP = I ‚àí √Ç, then for node i we have</p><formula xml:id="formula_17">(H LP x) i = j‚àà{Ni‚à™i} √Çi,j x j , (H HP x) i = x i ‚àí j‚àà{Ni‚à™i} √Çi,j x j (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where √Çi,j is the connection weight between two nodes. To leverage HP and identity channels in GNNs, we propose the Adaptive Channel Mixing (ACM) architecture in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaptive Channel Mixing(ACM) GNN Framework</head><p>ACM framework can be applied to lots of baseline GNNs and in this subsection, we use GCN as an example and introduce ACM framework in matrix form. We use H LP and H HP to represent general LP and HP filters. The ACM framework includes 3 steps as follows,</p><p>Step 1. Feature Extraction for Each Channel:</p><formula xml:id="formula_19">Option 1: H l L = ReLU H LP H l‚àí1 W l‚àí1 L , H l H = ReLU H HP H l‚àí1 W l‚àí1 H , H l I = ReLU IH l‚àí1 W l‚àí1 I ; Option 2: H l L = H LP ReLU H l‚àí1 W l‚àí1 L , H l H = H HP ReLU H l‚àí1 W l‚àí1 H , H l I = I ReLU H l‚àí1 W l‚àí1 I ; W l‚àí1 L , W l‚àí1 H , W l‚àí1 I ‚àà R F l‚àí1 √óF l ; Step 2. Feature-based Weight Learning Œ±l L = œÉ H l L W l L , Œ±l H = œÉ H l H W l H , Œ±l I = œÉ H l I W l I , W l‚àí1 L , W l‚àí1 H , W l‚àí1 I ‚àà R F l √ó1 Œ± l L , Œ± l H , Œ± l I = Softmax Œ±l L , Œ±l H , Œ±l I W l Mix /T, , W l Mix ‚àà R 3√ó3 , T ‚àà R is the temperature; Step 3. Node-wise Channel Mixing: H l = diag(Œ± l L )H l L + diag(Œ± l H )H l H + diag(Œ± l I )H l I . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>The framework with option 1 in step 1 is ACM framework and with option 2 is ACMII framework. ACM(II)-GCN first implement distinct feature extractions for 3 channels, respectively. After processed by a set of filterbanks, 3 filtered components H l L , H l H , H l I are obtained. Different nodes may have different needs for the information in the 3 channels, e.g., in Figure <ref type="figure" target="#fig_4">3</ref>, nodes 1,3 demand highfrequency information while node 2 only needs low-frequency information. To adaptively exploit information from different channels, ACM(II)-GCN learns row-wise (node-wise) feature-conditioned weights to combine the 3 channels. ACM(II) can be easily plugged into spatial GNNs by replacing H LP and H HP by aggregation and diversification operations as shown in <ref type="bibr" target="#b11">(12)</ref>. See Appendix F for a detailed discussion of model comparison on synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity Number of learnable parameters in layer</head><formula xml:id="formula_21">l of ACM(II)-GCN is 3F l‚àí1 (F l + 1) + 9, while it is F l‚àí1 F l in GCN. The computation of step 1-3 takes N F l (8 + 6F l‚àí1 ) + 2F l (nnz(H LP ) + nnz(H HP )) + 18N flops, while GCN layer takes 2N F l‚àí1 F l + 2F l (nnz(H LP ))</formula><p>flops, where nnz(‚Ä¢) is the number of non-zero elements. A detailed comparison on running time is conducted in section 6.1.</p><p>Limitations Diversification operation does not work well in all harmful heterophily cases. For example, consider an imbalanced dataset where several small clusters with distinctive labels are densely connected to a large cluster. In this case, the surrounding differences of nodes in small clusters are similar, i.e., the neighborhood differences are mainly from their connection to the same large cluster, and this possibly makes diversification operation fail to discriminate them. See a more detailed demonstration and discussion in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prior Work</head><p>GNNs on Addressing Heterophily We discuss relevant work of GNNs on addressing heterophily challenge in this part. <ref type="bibr" target="#b0">[1]</ref> acknowledges the difficulty of learning on graphs with weak homophily and propose MixHop to extract features from multi-hop neighborhood to get more information. Geom-GCN <ref type="bibr" target="#b32">[33]</ref> precomputes unsupervised node embeddings and uses graph structure defined by geometric relationships in the embedding space to define the bi-level aggregation process. <ref type="bibr" target="#b15">[16]</ref> proposes measurements based on feature smoothness and label smoothness that are potentially helpful to guide GNNs on dealing with heterophilous graphs. H 2 GCN <ref type="bibr" target="#b39">[40]</ref> combines 3 key designs to address heterophily: (1) ego-and neighbor-embedding separation; (2) higher-order neighborhoods;</p><p>(3) combination of intermediate representations. CPGNN <ref type="bibr" target="#b38">[39]</ref> models label correlations by the compatibility matrix, which is beneficial for heterophily settings, and propagates a prior belief estimation into GNNs by the compatibility matrix. FBGNN <ref type="bibr" target="#b28">[29]</ref> first proposes to use filterbank to address heterophily problem, but it does not fully explain the insights behind HP filters and does not contain identity channel and node-wise channel mixing mechanism. FAGCN <ref type="bibr" target="#b3">[4]</ref> learns edge-level aggregation weights as GAT <ref type="bibr" target="#b34">[35]</ref> but allows the weights to be negative which enables the network to capture the high-frequency components in graph signals. GPRGNN <ref type="bibr" target="#b6">[7]</ref> uses learnable weights that can be both positive and negative for feature propagation, it allows GRPGNN to adapt heterophily structure of graph and is able to handle both high-and low-frequency parts of the graph signals.</p><p>GNNs with Filterbanks Previously, there are geometric scattering networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> that apply filterbanks to address over-smoothing <ref type="bibr" target="#b22">[23]</ref> problem. The scattering construction captures different channels of variation from node features or labels. In geometric learning and graph signal processing, the band-pass filtering operations extract geometric information beyond smooth signals, thus it is believed that filterbanks can alleviate over-smoothing in GNNs. In ACM framework, we aim to design a framework with the help of filterbanks to adaptively utilize different channels to address the challenge of learning on heterophilous graph. We deal with different problem from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on Real-World Datasets</head><p>In this section, we evaluate ACM(II) framework on real-world datasets. We first conduct ablation studies in subsection 6.1 to validate different components. Then, we compare with the state-of-the-arts (SOTA) models in subsection 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study &amp; Efficiency</head><p>We investigate the effectiveness and efficiency of adding HP, identity channels and the adaptive mixing mechanism in ACM(II) framework by ablation study. Specifically, we apply the above components to SGC-1 and GCN separately, run 10 times on each dataset with 60%/20%/20% random splits for train/validation/test used in <ref type="bibr" target="#b6">[7]</ref> and report the average test accuracy as well as the standard deviation. We also record the average running time per epoch (in milliseconds) to compare the efficiency. We set the temperature T in (13) to be 3. (See Appendix A for hyperparameter searching range.)</p><p>From the results we can see that on most datasets, the additional HP and identity channels are helpful, even on strong homophily datasets, such as Cora, CiteSeer and PubMed. The adaptive mixing mechanism also shows its advantage over the method that directly adds the three channels together. This illustrates the necessity of learning to customize the channel usage adaptively for different nodes. As for efficiency, we can see that the running time is approximately doubled under ACM(II) framework than the original model. Table <ref type="table">1</ref>: Ablation study on 9 real-world datasets <ref type="bibr" target="#b32">[33]</ref>. Cell with means the component is applied to the baseline model. The best test results are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with State-of-the-art Models</head><p>Datasets &amp; Experimental Setup In this section, we implement SGC <ref type="bibr" target="#b35">[36]</ref> with 1 hop and 2 hops (SGC-1, SGC-2), GCNII <ref type="bibr" target="#b5">[6]</ref>, GCNII* <ref type="bibr" target="#b5">[6]</ref>, GCN <ref type="bibr" target="#b17">[18]</ref> and snowball networks with 2 and 3 layers (snowball-2, snowball-3) and apply them in ACM or ACMII framework (see details in appendix A.5): we use √Çrw as LP filter and the corresponding HP filter can be derived from <ref type="bibr" target="#b11">(12)</ref>. We compare them with several baselines and SOTA models: MLP with 2 layers (MLP-2), GAT <ref type="bibr" target="#b34">[35]</ref>, APPNP <ref type="bibr" target="#b18">[19]</ref>, GPRGNN <ref type="bibr" target="#b6">[7]</ref>, H 2 GCN <ref type="bibr" target="#b39">[40]</ref>, MixHop <ref type="bibr" target="#b0">[1]</ref>, GCN+JK <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref>, GAT+JK <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref>, FAGCN <ref type="bibr" target="#b3">[4]</ref> GraphSAGE <ref type="bibr" target="#b14">[15]</ref> and Geom-GCN <ref type="bibr" target="#b32">[33]</ref>. Besides the 9 benchmark datasets used in <ref type="bibr" target="#b32">[33]</ref>, we further test the above models on a new benchmark dataset, Deezer-Europe, that is proposed in <ref type="bibr" target="#b23">[24]</ref>. We test these models 10 times on Cornell, Wisconsin, Texas, Film, Chameleon, Squirrel, Cora, Citeseer and Pubmed following the same early stopping strategy, the same random data splitting method <ref type="foot" target="#foot_4">5</ref> and Adam <ref type="bibr" target="#b16">[17]</ref> optimizer as used in GPRGNN <ref type="bibr" target="#b6">[7]</ref>. For Deezer-Europe, we test the above models 5 times with the same early stopping strategy, the same fixed splits and AdamW <ref type="bibr" target="#b26">[27]</ref> used in <ref type="bibr" target="#b23">[24]</ref>. The details of hyperparameter searching range and the optimal hyperparameters are reported in appendix A.3 and A.4.</p><p>The main results of this set of experiments with statistics of datasets are summarized in  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref> and results " ‚Ä† " are from <ref type="bibr" target="#b32">[33]</ref>. NA means the reported results are not available and OOM means out of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>The similarity matrix and the new metrics defined in this paper mainly capture the linear relations of the aggregated node features. But this might be insufficient sometimes when nonlinearity information in feature vectors are important for classification. In the future, similarity matrix that is able to capture nonlinear relations between node features can be proposed to define new homophily metrics.</p><p>As the limitation raised in section 4.3, filterbank method cannot properly handle all cases of harmful heterophily. In the future, we need to explore and address more challenging heterophilous graph with GNNs.  {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} --ACM-SGC-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} -GCN-LP+HP {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 GCN-LP+Identity {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 ACM-GCN-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters &amp; Details of The Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Optimal Hyperparameters for Baselines and ACM(II)-GNNs on Real-world Tasks</head><p>See table 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Details of the Implementation of ACM and ACMII Frameworks</head><p>In ACM(II) framework, we first use dropout operation over the input data. The implementation of ACM(II)-GCN and ACM(II)-snowball is straightforward, but SGC-1, SGC-2, GCNII and GCNII* are not able to be applied under ACM(II) framework and we will make an explanation as follows.</p><p>‚Ä¢ SGC-1 and SGC-2: SGC does not contain nonlinearity, so the option 1 and option 2 in step 1 is the same for ACM-SGC and ACMII-SGC. Thus, we only implement ACM-SGC. ‚Ä¢ GCNII and GCNII*: GCCII:</p><formula xml:id="formula_22">H ( +1) = œÉ (1 ‚àí Œ± ) √ÇH ( ) + Œ± H (0) (1 ‚àí Œ≤ ) I n + Œ≤ W ( ) GCCII*: H ( +1) = œÉ (1 ‚àí Œ± ) √ÇH ( ) (1 ‚àí Œ≤ ) I n + Œ≤ W ( ) 1 + +Œ± H (0) (1 ‚àí Œ≤ ) I n + Œ≤ W ( )<label>2</label></formula><p>Without major modification, GCNII and GCNII* are hard to be put into ACMII framework. In ACMII frameworks, before apply the operator √Ç, we first implement a nonlinear feature operation over H . But in GCNII and GCNII*, before multiplying W (or W 1 , W 2 ) to extract features, we need to add another term including H (0) , which are not filtered by √Ç. This is incompatible with ACMII framework, thus, we did not implement GCNII and GCNII* in ACMII framework.</p><p>The open source code will be released soon. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Derivation in Matrix Form</head><p>In output layer, we have</p><formula xml:id="formula_23">Y = softmax( √ÇXW ) ‚â° softmax(Y ) = exp(Y )1 C 1 T C ‚àí1 exp(Y ) &gt; 0 L = ‚àítrace(Z T log Y )</formula><p>where 1 C ‚àà R C√ó1 , (‚Ä¢) ‚àí1 is point-wise inverse function and each element of Y is positive. Then</p><formula xml:id="formula_24">dL = ‚àítrace Z T ((Y ) ‚àí1 dY ) = ‚àítrace Z T (softmax(Y )) ‚àí1 d softmax(Y ) Note that d softmax(Y ) = ‚àí exp(Y )1 C 1 T C ‚àí2 [(exp(Y ) dY )1 C 1 T C ] exp(Y ) + exp(Y )1 C 1 T C ‚àí1 (exp(Y ) dY ) = ‚àí softmax(Y ) exp(Y )1 C 1 T C ‚àí1 [(exp(Y ) dY )1 C 1 T C ] + softmax(Y ) dY = softmax(Y ) ‚àí exp(Y )1 C 1 T C ‚àí1 (exp(Y ) dY )1 C 1 T C + dY Then, dL = ‚àí trace Z T (softmax(Y )) ‚àí1 softmax(Y ) ‚àí exp(Y )1 C 1 T C ‚àí1 (exp(Y ) dY )1 C 1 T C + dY = ‚àí trace Z T ‚àí exp(Y )1 C 1 T C ‚àí1 (exp(Y ) dY )1 C 1 T C + dY = trace Z exp(Y )1 C 1 T C ‚àí1 1 C 1 T C T [exp(Y ) dY ] ‚àí Z T dY = trace exp(Y ) Z exp(Y )1 C 1 T C ‚àí1 1 C 1 T C T dY ‚àí Z T dY = trace exp(Y ) exp(Y )1 C 1 T C ‚àí1 T dY ‚àí Z T dY = trace (softmax(Y ) ‚àí Z) T dY</formula><p>where the 4-th equation holds due to Z exp </p><formula xml:id="formula_25">(Y )1 C 1 T C ‚àí1 1 C 1 T C = exp(Y )1 C 1 T C ‚àí1 . Thus,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Component-wise Derivation</head><p>Denote X = XW . We rewrite L as follows:</p><formula xml:id="formula_27">L = ‚àítrace Z T log (exp(Y )1 C 1 T C ) ‚àí1 exp(Y ) = ‚àítrace Z T ‚àí log(exp(Y )1 C 1 T C ) + Y = ‚àítrace Z T Y + trace Z T log exp(Y )1 C 1 T C = ‚àítrace Z T √ÇXW + trace Z T log exp(Y )1 C 1 T C = ‚àítrace Z T √ÇXW + trace 1 T C log (exp(Y )1 C ) = ‚àí N i=1 j‚ààNi</formula><p>√Çi,j Z i,:</p><formula xml:id="formula_28">XT j: + N i=1 log Ô£´ Ô£≠ C c=1 exp( j‚ààNi √Çi,j Xj,c ) Ô£∂ Ô£∏ = ‚àí N i=1 log Ô£´ Ô£≠ exp Ô£´ Ô£≠ C c=1 j‚ààNi √Çi,j Z i,c Xj,c Ô£∂ Ô£∏ Ô£∂ Ô£∏ + N i=1 log Ô£´ Ô£≠ C c=1 exp Ô£´ Ô£≠ j‚ààNi √Çi,j Xj,c Ô£∂ Ô£∏ Ô£∂ Ô£∏ = ‚àí N i=1 log exp C c=1 j‚ààNi √Çi,j Z i,c Xj,c C c=1 exp( j‚ààNi √Çi,j Xj,c ) Note that C c=1</formula><p>Z j,c = 1 for any j. Consider the derivation of L over Xj ,c :</p><formula xml:id="formula_29">dL d Xj ,c = ‚àí N i=1 C c=1 exp( j‚ààNi √Çi,j Xj,c ) exp C c=1 j‚ààNi √Çi,j Z i,c Xj,c √ó Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ √Çi,j Z i,c exp C c=1 j‚ààNi √Çi,j Z i,c Xj,c C c=1 exp( j‚ààNi √Çi,j Xj,c ) C c=1 exp( j‚ààNi √Çi,j Xj,c ) 2 ‚àí √Çi,j exp C c=1 j‚ààNi √Çi,j Z i,c Xj,c exp( j‚ààNi √Çi,j Xj,c ) C c=1 exp( j‚ààNi √Çi,j Xj,c ) 2 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ = ‚àí N i=1 Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ √Çi,j Z i,c C c=1 exp( j‚ààNi √Çi,j Xj,c ) ‚àí √Çi,j exp( j‚ààNi √Çi,j Xj,c ) C c=1 exp( j‚ààNi √Çi,j Xj,c ) Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ = ‚àí N i=1 Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ √Çi,j C c=1,c =c (Z i,c ) exp( j‚ààNi √Çi,j Xj,c ) + (Z i,c ‚àí 1) exp( j‚ààNi √Çi,j Xj,c ) C c=1 exp( j‚ààNi √Çi,j Xj,c ) Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ = ‚àí N i=1 √Çi,j Z i,c P (Y i = c ) + (Z i,c ‚àí 1) P (Y i = c ) = ‚àí N i=1 √Çi,j Z i,c ‚àí P (Y i = c )</formula><p>Writing the above in matrix form, we have</p><formula xml:id="formula_30">dL d X = √Ç(Z ‚àí Y ), dL d W = X T √ÇT (Z ‚àí Y ), ‚àÜY ‚àù √ÇXX T √ÇT (Z ‚àí Y )<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Synthetic Experiments</head><p>In our synthetic experiments, we generate graphs with edge homophily levels h ‚àà 0.95 : 0.05 : 0.05 and h ‚àà 0.05 : 0.005 : 0.005. We explore the interval [0.05, 0.005] with a more fine-grained scale 0.005 because we empirically find that the performance of GNNs is sensitive in more this area. For a given h, we generate intra-class edges from numpy.random.multinomial(2, numpy.ones(399)/399, size=1)[0] (does not include self-loop) and inter-class edges from numpy.random.multinomial(int(2/h -2), numpy.ones(1600)/1600, size=1)[0].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 1</head><p>Proof. According to the given assumptions, for node v, we have √Çv,k = 1 d+1 , the expected number of intra-class edges is dh (here the self-loop edge introduced by √Ç is not counted based on the definition of edge homophily and data generation process) and inter-class edges is (1 ‚àí h)d. Suppose there are C ‚â• 2 classes. Consider matrix √ÇZ,</p><formula xml:id="formula_31">Then, we have E ( √ÇZ) v,c = E k‚ààV √Çv,k 1 {Z k,: =e T c } = k‚ààV E 1 {Z k,: =e T c } d+1</formula><p>, where 1 is the indicator function.</p><p>When v is in class c, we have k‚ààV</p><formula xml:id="formula_32">E 1 {Z k,: =e T c } d+1 = hd+1 d+1 (hd + 1 = hd intra-class edges + 1 self-loop introduced by √Ç). When v is not in class c, we have k‚ààV E 1 {Z k,: =e T c } d+1 = (1‚àíh)d (C‚àí1)(d+1) ((1 ‚àí h)d inter-class edges uniformly distributed in the other C ‚àí 1 classes).</formula><p>For nodes v, u, we have ( √ÇZ) v,: , ( √ÇZ) u,: ‚àà R C and since elements in √Çv,k and √Çu,k are independently generated for all k, k ‚àà V, we have</p><formula xml:id="formula_33">E ( √ÇZ) v,c ( √ÇZ) u,c = E ( k‚ààV √Çv,k 1 {Z k,: =e T c } )( k ‚ààV √Çu,k 1 {Z k ,: =e T c } ) = E ( k‚ààV √Çv,k 1 {Z k,: =e T c } ) E ( k ‚ààV √Çu,k 1 {Z k ,: =e T c } ) Thus, E S( √Ç, Z) v,u = E &lt; ( √ÇZ) v,: , ( √ÇZ) u,: &gt; = c E ( k‚ààV √Çv,k 1 {Z k,: =e T c } ) E ( k ‚ààV √Çu,k 1 {Z k ,: =e T c } ) = Ô£± Ô£≤ Ô£≥ hd+1 d+1 2 + ((1‚àíh)d) 2 (C‚àí1)(d+1) 2 , u, v are in the same class 2(hd+1)(1‚àíh)d (C‚àí1)(d+1) 2 + (C‚àí2)(1‚àíh) 2 d 2 (C‚àí1) 2<label>(</label></formula><p>d+1) 2 , u, v are in different classes For nodes u 1 , u 2 , and v, where Z u1,: = Z v,: and Z u2,: = Z v,: ,</p><formula xml:id="formula_34">g(h) ‚â° E S( √Ç, Z) v,u1 ‚àí E S( √Ç, Z) v,u2<label>(16)</label></formula><formula xml:id="formula_35">= (C ‚àí 1) 2 (hd + 1) 2 + (C ‚àí 1) [(1 ‚àí h)d] 2 ‚àí (C ‚àí 1) (2(hd + 1)(1 ‚àí h)d) ‚àí (C ‚àí 2) [(1 ‚àí h)d] 2 (C ‚àí 1) 2 (d + 1) 2 = (C ‚àí 1)(hd + 1) ‚àí (1 ‚àí h)d (C ‚àí 1)(d + 1)<label>2</label></formula><p>Setting g(h) = 0, we obtain the optimal h: h</p><formula xml:id="formula_36">= d + 1 ‚àí C Cd<label>(17)</label></formula><p>For the data generation process in the synthetic experiments, we fix d intra , then d = d intra /h, which is a function of h. We change d in ( <ref type="formula" target="#formula_36">17</ref>) to d intra /h, leading to</p><formula xml:id="formula_37">h = d intra /h + 1 ‚àí C Cd intra /h<label>(18)</label></formula><p>It is easy to observe that h satisfying (18) still makes g(h) = 0, when d in g(h) is replaced by d intra /h. From ( <ref type="formula" target="#formula_37">18</ref>) we obtain the optimal h in terms of d intra : = Z v,: } Since RV is symmetrically distributed and under the conditions in theorem 1, its expectation is E[RV ] = g(h) as showed in <ref type="bibr" target="#b15">(16)</ref>. Since the minimum of g(h) is 0 and RV is symmetrically distributed, we have P(RV ‚â• 0) ‚â• 0.5 and this can explain why H agg (G) is always greater than 0.5 in many real-world tasks. ACM-GNN models on all synthetic datasets and plot the mean test accuracy with standard deviation on each dataset. From Figure <ref type="figure">4</ref> and Figure <ref type="figure">5</ref>, we can see that on each H M agg (G) level, ACM-GNNs will not underperform GNNs and graph-agnostic models. But when H M agg (G) is small, GNNs will be outperformed by graph-agnostic models by a large margin. This demonstrate the advantage of the ACM framework.</p><formula xml:id="formula_38">h = d intra Cd intra + C ‚àí 1 D.1 An extension of Theorem 1 S agg S( √Ç, Z) = v Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } ‚â• Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } |V| = v‚ààV 1 Meanu {S( √Ç,Z)v,</formula><p>G Discussion of the Limitations of Diversification Operation   From the black box area of S(I ‚àí √Ç, X) in the example in Figure <ref type="figure" target="#fig_9">6</ref> we can see that nodes in class 1 and 4 assign non-negative weights to each other; nodes in class 2 and 3 assign non-negative weights to each other as well. This is because the surrounding differences of class 1 are similar as class 4, so are class 2 and 3. In real-world applications, when nodes in several small clusters connect to a large cluster, the surrounding differences of the nodes in the small clusters will become similar. In such case, HP filter are not able to distinguish the nodes from different small clusters.</p><p>H The Similarity, Homophily and DD √Ç,X (G) Metrics and Their Estimations Additional Metrics There are three key factors that influence the performance of GNNs in realworld tasks: labels, features and graph structure. The (modified) aggregation homophily tries to investigate how the graph structure will influence the performance with labels and features being fixed. And their correlation is verified through the synthetic experiments.</p><p>Besides graph-label consistency, we need to consider feature-label consistency and aggregatedfeature-label consistency as well. With aggregation similarity score of the features S agg (S(I, X))</p><p>and aggregated features S agg S( √Ç, X) listed in appendix H, our methods open up a new perspective on analyzing and comparing the performance of graph-agnostic models and graph-aware models in real-world tasks. Here are 2 examples.</p><p>Example 1: GCN (graph-aware model) underperforms MLP-2 (graph-agnostic model) on Cornell, Wisconsin, Texas, Film. Based on the aggregation homophily, the graph structure is not the main cause of the performance degradation. And from Table <ref type="table" target="#tab_10">6</ref> we can see that the S agg S( √Ç, X)</p><p>for the above 4 datasets are lower than their corresponding S agg (S(I, X)), which implies that it is the aggregated-feature-label inconsistency that causes the performance degradation.</p><p>Example 2: According to the widely used analysis based on node or edge homophily, the graph structure of Chameleon, and Squirrel are heterophilous and bad for GNNs. But in practice, GCN outperforms MLP-2 on those 2 datasets which means the additional graph information is helpful for node classification instead of being harmful. Traditional homophily metrics fail to explain such phenomenon but our method can give an explanation from different angles: For Chameleon, its modified aggregation homophily is not low and its S agg S( √Ç, X) is higher than its S agg (S(I, X))</p><p>Table <ref type="table">9</ref>: Experimental results on fixed splits provided by <ref type="bibr" target="#b32">[33]</ref>: average test accuracy ¬± standard deviation on 9 real-world benchmark datasets. The best results are highlighted. Results of Geom-GCN, H 2 GCN and GPRGNN are from <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25]</ref>; results on the rest models are run by ourselves and the hyperparameter searching range is the same as table 5.</p><p>J A Detailed Explanation of the Differences Between ACM(II)-GNNs and Several SOTA Models</p><p>‚Ä¢ Difference with GPRGNN <ref type="bibr" target="#b6">[7]</ref>: To explain the difference between channel mixing mechanism and the learning mechanism in GPRGNN, we first rewrite GPRGNN as</p><formula xml:id="formula_39">Z = K k=0 Œ≥ k H (k) = K k=0 Œ≥ k IH (k) = K k=0</formula><p>diag(Œ≥ k , Œ≥ k , . . . , Œ≥ k )H (k) . The node-wise channel mixing mechanism in GPRGNN form is Z = K k=0 diag(Œ≥ 1 k , Œ≥ 2 k , . . . , Œ≥ N k )H (k) , where N is the number of nodes and Œ≥ i k , i = 1, . . . , N are learnable parametric mixing weights. ‚Ä¢ Difference with FAGCN <ref type="bibr" target="#b3">[4]</ref>: instead of using a fixed √Ç, FAGCN learns a new filter √Ç based on √Ç. And √Ç can be decomposed as √Ç = √Ç 1 ‚àí √Ç 2 , where √Ç 1 and ‚àí √Ç 2 represents positive and negative edge (propagation) information respectively. In our paper, we are not discussing the advantages of using the learned filter √Ç over the fixed filter √Ç, we are comparing the models with and without channel mixing mechanism. We believe the empirical results on real-world tasks in table 2 and table 9 is the best way to compare the models with fixed filter and node-wise channel mixing and the models with learned filter but without channel mixing</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a)(b)(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of baseline performance under different homophily metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>in inter-class block for node 1,3.Positive weights in intra-class blocks, Non-negative weights in cross-class blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of how HP filter addresses harmful heterophily</figDesc><graphic url="image-208.png" coords="6,371.86,91.61,127.88,56.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] In the Appendix G, we discussion cases that high-pass filter cannot tackle. (c) Did you discuss any potential negative societal impacts of your work? [No] It is in Section 8, we have not come up with significant social negative impact. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] In Section 3&amp;4, we mainly define a new homophily metric and it is followed by two theorems. (b) Did you include complete proofs of all theoretical results? [Yes] In Appendix B&amp;D&amp; E, we justify the new metric and two theorems. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The settings are provided in details and the source code is submitted in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In Section 6, we specify model details. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We include average test accuracy of times of running with standard deviation. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We include hardware details in Appendix, which is not computationally expensive. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] In Section 6, we specify the datasets with their data split sources in footnotes. (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] None included. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [No] None included. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] None included. (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No] None included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>dW</head><label></label><figDesc>we have dL dY = softmax(Y ) ‚àí Z = Y ‚àí Z For Y and W , we have dY = √ÇXdW and dL = trace dL dY To get dL dW we have, dL dW = X T √ÇT dL dY = X T √ÇT (Y ‚àí Z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>v‚ààV 1 MeanuP</head><label>1</label><figDesc>u|Zu,:=Zv,:} ‚â•Meanu {S( √Ç,Z)v,u|Zu,: =Zv,:}|V|Then,E S agg S( √Ç, Z) = E Ô£´ Ô£≠ {S( √Ç,Z)v,u|Zu,:=Zv,:} ‚â•Meanu {S( √Ç,Z)v,u|Zu,: =Zv,:} Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } ‚â• Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } |V| = P Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } ‚àí Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } ‚â• 0 Consider the random variable RV = Mean u {S( √Ç, Z) v,u |Z u,: = Z v,: } ‚àí Mean u {S( √Ç, Z) v,u |Z u,:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>' XX T (I ‚àí ùê¥ ' )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of the case (the area in black box) that HP filter does not work well for harmful heterophily</figDesc><graphic url="image-424.png" coords="24,330.27,200.90,168.11,77.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Theorem 1. (See Appendix D for proof). Suppose there are C classes in the graph G and G is a d-regular graph (each node has d neighbors). Given d, edges for each node are i.i.d.generated, such that each edge of any node has probability h to connect with nodes in the same class and probability 1 ‚àí h to connect with nodes in different classes. Let the aggregation operator √Ç = √Çrw . Then, for nodes v, u 1 and u 2 , where Z u1,: = Z v,: and Z u2,: = Z v,: , we have</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ùê¥ % ùëãùëã ! ùê¥ %</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">ùêº ‚àí ùê¥ % XX T (I ‚àí ùê¥ % )</cell><cell></cell></row><row><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.50</cell><cell>.00</cell><cell>.67</cell><cell>-.20</cell><cell>-.25</cell><cell>-.20</cell><cell>-.25</cell></row><row><cell>.50 .50</cell><cell>1.0 .33</cell><cell>.33 .56</cell><cell>.20 .60</cell><cell>.25 .58</cell><cell>.20 .60</cell><cell>.25 .58</cell><cell>.00 .67</cell><cell>.00 .00</cell><cell>.00 .89</cell><cell>.00 -.27</cell><cell>.00 -.33</cell><cell>.00 -.27</cell><cell>.00 -.33</cell></row><row><cell>.50 .50 .50</cell><cell>.20 .25 .25</cell><cell>.60 .58 .60</cell><cell>.68 .65 .68</cell><cell>.65 .63 .65</cell><cell>.68 .65 .68</cell><cell>.65 .63 .65</cell><cell>-.20 -.25 -.20</cell><cell>.00 .00 .00</cell><cell>-.27 -.33 -.27</cell><cell>.08 .10 .08</cell><cell>.10 .13 .10</cell><cell>.08 .10 .08</cell><cell>.10 .13 .10</cell></row><row><cell>.50</cell><cell>.25</cell><cell>.58</cell><cell>.65</cell><cell>.63</cell><cell>.65</cell><cell>.63</cell><cell>-.25</cell><cell>.00</cell><cell>-.33</cell><cell>.10</cell><cell>.13</cell><cell>.10</cell><cell>.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ablation Study on Different Components in ACM-SGC and ACM-GCN (%) Std Acc ¬± Std Acc ¬± Std Acc ¬± Std Acc ¬± Std Acc ¬± Std Acc ¬± Std Acc ¬± Std Acc ¬± Std SGC-1 w/ 70.98 ¬± 8.39 70.38 ¬± 2.85 83.28 ¬± 5.43 25.26 ¬± 1.18 64.86 ¬± 1.81 47.62 ¬± 1.27 85.12 ¬± 1.64 79.66 ¬± 0.75 85.5 ¬± 0.76 12.89 83.28 ¬± 5.81 91.88 ¬± 1.61 90.98 ¬± 2.46 36.76 ¬± 1.01 65.27 ¬± 1.9 47.27 ¬± 1.37 86.8 ¬± 1.08 80.98 ¬± 1.68 87.21 ¬± 0.42 10.44 93.93 ¬± 3.6 95.25 ¬± 1.84 93.93 ¬± 2.54 38.38 ¬± 1.13 63.83 ¬± 2.07 46.79 ¬± 0.75 86.73 ¬± 1.28 80.57 ¬± 0.99 87.8 ¬± 0.58 9.44 88.2 ¬± 4.39 93.5 ¬± 2.95 92.95 ¬± 2.94 37.19 ¬± 0.87 62.82 ¬± 1.84 44.94 ¬± 0.93 85.22 ¬± 1.35 80.75 ¬± 1.68 88.11 ¬± 0.21 11.00 93.77 ¬± 1.91 93.25 ¬± 2.92 93.61 ¬± 1.55 39.33 ¬± 1.25 63.68 ¬± 1.62 46.4 ¬± 1.13 86.63 ¬± 1.13 80.96 ¬± 0.93 87.75 ¬± 0.88 10.00 ACM-GCN w/ 82.46 ¬± 3.11 75.5 ¬± 2.92 83.11 ¬± 3.2 35.51 ¬± 0.99 64.18 ¬± 2.62 44.76 ¬± 1.39 87.78 ¬± 0.96 81.39 ¬± 1.23 88.9 ¬± 0.32 11.44 82.13 ¬± 2.59 86.62 ¬± 4.61 89.19 ¬± 3.04 38.06 ¬± 1.35 69.21 ¬± 1.68 57.2 ¬± 1.01 88.93 ¬± 1.55 81.96 ¬± 0.91 90.01 ¬± 0.8 7.22 94.26 ¬± 2.23 96.13 ¬± 2.2 94.1 ¬± 2.95 41.51 ¬± 0.99 67.44 ¬± 2.14 53.97 ¬± 1.39 88.95 ¬± 0.9 81.72 ¬± 1.22 90.88 ¬± 0.55 4.44 91.64 ¬± 2 95.37 ¬± 3.31 95.25 ¬± 2.37 40.47 ¬± 1.49 68.93 ¬± 2.04 54.78 ¬± 1.27 89.13 ¬± 1.77 81.96 ¬± 2.03 91.01 ¬± 0.7 3.11 94.75 ¬± 2.62 96.75 ¬± 1.6 95.08 ¬± 3.2 41.62 ¬± 1.15 69.04 ¬± 1.74 58.02 ¬± 1.86 88.95 ¬± 1.3 81.80 ¬± 1.26 90.69 ¬± 0.53 2.78 ACMII-GCN w/ 82.46 ¬± 3.03 91.00 ¬± 1.75 90.33 ¬± 2.69 38.39 ¬± 0.75 67.59 ¬± 2.14 53.67 ¬± 1.71 89.13 ¬± 1.14 81.75 ¬± 0.85 89.87 ¬± 0.39 7.44 94.26 ¬± 2.57 96.00 ¬± 2.15 94.26 ¬± 2.96 40.96 ¬± 1.2 66.35 ¬± 1.76 50.78 ¬± 2.07 89.06 ¬± 1.07 81.86 ¬± 1.22 90.71 ¬± 0.67 4.67 91.48 ¬± 1.43 96.25 ¬± 2.09 93.77 ¬± 2.91 40.27 ¬± 1.07 66.52 ¬± 2.65 52.9 ¬± 1.64 88.83 ¬± 1.16 81.54 ¬± 0.95 90.6 ¬± 0.47 6.67 95.9 ¬± 1.83 96.62 ¬± 2.44 95.25 ¬± 3.15 41.84 ¬± 1.15 68.38 ¬± 1.36 54.53 ¬± 2.09 89.00 ¬± 0.72 81.79 ¬± 0.95 90.74 ¬± 0.5 2.78</figDesc><table><row><cell>Baseline</cell><cell>Model Components</cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell>Models</cell><cell cols="6">LP HP Identity Mixing Acc ¬± Comparison of Average Running Time Per Epoch(ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2.53</cell><cell>2.83</cell><cell>2.5</cell><cell>3.18</cell><cell>3.48</cell><cell>4.65</cell><cell>3.47</cell><cell>3.43</cell><cell>4.04</cell></row><row><cell></cell><cell></cell><cell>4.01</cell><cell>4.57</cell><cell>4.24</cell><cell>4.55</cell><cell>4.76</cell><cell>5.09</cell><cell>5.39</cell><cell>4.69</cell><cell>4.75</cell></row><row><cell>SGC-1 w/</cell><cell></cell><cell>3.88 3.31</cell><cell>4.01 3.49</cell><cell>4.04 3.18</cell><cell>4.43 3.7</cell><cell>4.06 3.53</cell><cell>4.5 4.83</cell><cell>4.38 3.92</cell><cell>3.82 3.87</cell><cell>4.16 4.24</cell></row><row><cell></cell><cell></cell><cell>5.53</cell><cell>5.96</cell><cell>5.43</cell><cell>5.21</cell><cell>5.41</cell><cell>6.96</cell><cell>6</cell><cell>5.9</cell><cell>6.04</cell></row><row><cell></cell><cell></cell><cell>3.67</cell><cell>3.74</cell><cell>3.59</cell><cell>4.86</cell><cell>4.96</cell><cell>6.41</cell><cell>4.24</cell><cell>4.18</cell><cell>5.08</cell></row><row><cell></cell><cell></cell><cell>6.63</cell><cell>8.06</cell><cell>7.89</cell><cell>8.11</cell><cell>7.8</cell><cell>9.39</cell><cell>7.82</cell><cell>7.38</cell><cell>8.74</cell></row><row><cell>ACM-GCN w/</cell><cell></cell><cell>5.73 5.16</cell><cell>5.91 5.25</cell><cell>5.93 5.2</cell><cell>6.86 5.93</cell><cell>6.35 5.64</cell><cell>7.15 8.02</cell><cell>7.34 5.73</cell><cell>6.65 5.65</cell><cell>6.8 6.16</cell></row><row><cell></cell><cell></cell><cell>8.25</cell><cell>8.11</cell><cell>7.89</cell><cell>7.97</cell><cell>8.41</cell><cell>11.9</cell><cell>8.84</cell><cell>8.38</cell><cell>8.63</cell></row><row><cell></cell><cell></cell><cell>6.62</cell><cell>7.35</cell><cell>7.39</cell><cell>7.62</cell><cell>7.33</cell><cell>9.69</cell><cell>7.49</cell><cell>7.58</cell><cell>7.97</cell></row><row><cell></cell><cell></cell><cell>6.3</cell><cell>6.05</cell><cell>6.26</cell><cell>6.87</cell><cell>6.44</cell><cell>6.5</cell><cell>6.14</cell><cell>7.21</cell><cell>6.6</cell></row><row><cell>ACMII-GCN w/</cell><cell></cell><cell>5.24</cell><cell>5.27</cell><cell>5.46</cell><cell>5.72</cell><cell>5.65</cell><cell>7.87</cell><cell>5.48</cell><cell>5.65</cell><cell>6.33</cell></row><row><cell></cell><cell></cell><cell>7.59</cell><cell>8.28</cell><cell>8.06</cell><cell>8.85</cell><cell>8</cell><cell>10</cell><cell>8.27</cell><cell>8.5</cell><cell>8.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 ,</head><label>2</label><figDesc>where we report the mean accuracy and standard deviation. We can see that after applied in ACM(II) framework, the performance of baseline models are boosted on almost all tasks. Especially, ACMII-GCN performs the best in terms of average rank(3.40)  across all datasets and ACM(II)-GNNs achieve SOTA performance on 8 out of 10 datasets. Overall, It suggests that ACM(II) framework can help GNNs to generalize better on node classification tasks on heterophilous graphs. ¬± 0.70 93.87 ¬± 3.33 92.26 ¬± 0.71 38.58 ¬± 0.25 46.72 ¬± 0.46 31.28 ¬± 0.27 66.55 ¬± 0.72 76.44 ¬± 0.30 76.25 ¬± 0.28 86.43 ¬± 0.13 18.60 GAT* 76.00 ¬± 1.01 71.01 ¬± 4.66 78.87 ¬± 0.86 35.98 ¬± 0.23 63.9 ¬± 0.46 42.72 ¬± 0.33 61.09 ¬± 0.77 76.70 ¬± 0.42 67.20 ¬± 0.46 83.28 ¬± 0.12 21.40 APPNP* 91.80 ¬± 0.63 92.00 ¬± 3.59 91.18 ¬± 0.70 38.86 ¬± 0.24 51.91 ¬± 0.56 34.77 ¬± 0.34 67.21 ¬± 0.56 79.41 ¬± 0.38 68.59 ¬± 0.30 85.02 ¬± 0.09 18.00 GPRGNN* 91.36 ¬± 0.70 93.75 ¬± 2.37 92.92 ¬± 0.61 39.30 ¬± 0.27 67.48 ¬± 0.40 49.93 ¬± 0.53 66.90 ¬± 0.50 79.51pm 0.36 67.63 ¬± 0.38 85.07 ¬± 0.09 14.40 H2GCN 86.23 ¬± 4.71 87.5 ¬± 1.77 85.90 ¬± 3.53 38.85 ¬± 1.17 52.30 ¬± 0.48 30.39 ¬± 1.22 67.22 ¬± 0.90 87.52 ¬± 0.61 79.97 ¬± 0.69 87.78 ¬± 0.28 17.00 MixHop 60.33 ¬± 28.53 77.25 ¬± 7.80 76.39 ¬± 7.66 33.13 ¬± 2.40 36.28 ¬± 10.22 24.55 ¬± 2.60 66.80 ¬± 0.58 65.65 ¬± 11.31 49.52 ¬± 13.35 87.04 ¬± 4.10 23.50 GCN+JK 66.56 ¬± 13.82 62.50 ¬± 15.75 80.66 ¬± 1.91 32.72 ¬± 2.62 64.68 ¬± 2.85 53.40 ¬± 1.90 60.99 ¬± 0.14 86.90 ¬± 1.51 73.77 ¬± 1.85 90.09 ¬± 0.68 18.80 GAT+JK 74.43 ¬± 10.24 69.50 ¬± 3.12 75.41 ¬± 7.18 35.41 ¬± 0.97 68.14 ¬± 1.18 52.28 ¬± 3.61 59.66 ¬± 0.92 89.52 ¬± 0.43 74.49 ¬± 2.76 89.15 ¬± 0.87 16.70 FAGCN 88.03 ¬± 5.6 89.75 ¬± 6.37 88.85 ¬± 4.39 31.59 ¬± 1.37 49.47 ¬± 2.84 42.24 ¬± 1.2 66.86 p, 0.53 88.85 ¬± 1.36 82.37 ¬± 1.46 89.98 ¬± 0.54 14.10 GraphSAGE 71.41 ¬± 1.24 64.85 ¬± 5.14 79.03 ¬± 1.20 36.37 ¬± 0.21 62.15 ¬± 0.42 41.26 ¬± 0.26 OOM 86.58 ¬± 0.26 78.24 ¬± 0.30 86.85 ¬± 0.11 20.89 ¬± 8.39 70.38 ¬± 2.85 83.28 ¬± 5.43 25.26 ¬± 1.18 64.86 ¬± 1.81 47.62 ¬± 1.27 59.73 ¬± 0.12 85.12 ¬± 1.64 79.66 ¬± 0.75 85.5 ¬± 0.76 20.10 SGC-2 72.62 ¬± 9.92 74.75 ¬± 2.89 81.31 ¬± 3.3 28.81 ¬± 1.11 62.67 ¬± 2.41 41.25 ¬± 1.4 61.56 ¬± 0.51 85.48 ¬± 1.48 80.75 ¬± 1.15 85.36 ¬± 0.52 20.70 GCNII 89.18 ¬± 3.96 83.25 ¬± 2.69 82.46 ¬± 4.58 40.82 ¬± 1.79 60.35 ¬± 2.7 38.81 ¬± 1.97 66.38 ¬± 0.45 88.98 ¬± 1.33 81.58 ¬± 1.3 89.8 ¬± 0.3 14.80 GCNII* 90.49 ¬± 4.45 89.12 ¬± 3.06 88.52 ¬± 3.02 41.54 ¬± 0.99 62.8 ¬± 2.87 38.31 ¬± 1.3 66.42 ¬± 0.56 88.93 ¬± 1.37 81.83 ¬± 1.78 89.98 ¬± 0.52 12.30 GCN 82.46 ¬± 3.11 75.5 ¬± 2.92 83.11 ¬± 3.2 35.51 ¬± 0.99 64.18 ¬± 2.62 44.76 ¬± 1.39 62.23 ¬± 0.53 87.78 ¬± 0.96 81.39 ¬± 1.23 88.9 ¬± 0.32 16.30 Snowball-2 82.62 ¬± 2.34 74.88 ¬± 3.42 83.11 ¬± 3.2 35.97 ¬± 0.66 64.99 ¬± 2.39 47.88 ¬± 1.23 OOM 88.64 ¬± 1.15 81.53 ¬± 1.71 89.04 ¬± 0.49 15.22 Snowball-3 82.95 ¬± 2.1 69.5 ¬± 5.01 83.11 ¬± 3.2 36.00 ¬± 1.36 65.49 ¬± 1.64 48.25 ¬± 0.94 OOM 89.33 ¬± 1.3 80.93 ¬± 1.32 88.8 ¬± 0.82 14.78 ACM-SGC-1 93.77 ¬± 1.91 93.25 ¬± 2.92 93.61 ¬± 1.55 39.33 ¬± 1.25 63.68 ¬± 1.62 46.4 ¬± 1.13 66.67 ¬± 0.56 86.63 ¬± 1.13 80.96 ¬± 0.93 87.75 ¬± 0.88 12.60 ACM-SGC-2 93.77 ¬± 2.17 94.00 ¬± 2.61 93.44 ¬± 2.54 40.13 ¬± 1.21 60.48 ¬± 1.55 40.91 ¬± 1.39 66.53 ¬± 0.57 87.64 ¬± 0.99 80.93 ¬± 1.16 88.79 ¬± 0.5 13.40 ACM-GCNII 92.62 ¬± 3.13 94.63 ¬± 2.96 92.46 ¬± 1.97 41.37 ¬± 1.37 58.73 ¬± 2.52 40.9 ¬± 1.58 66.39 ¬± 0.56 89.1 ¬± 1.61 82.28 ¬± 1.12 90.12 ¬± 0.4 10.40 ACM-GCNII* 93.44 ¬± 2.74 94.37 ¬± 2.81 93.28 ¬± 2.79 41.27 ¬± 1.24 61.66 ¬± 2.29 38.32 ¬± 1.5 66.6 ¬± 0.57 89.00 ¬± 1.35 81.69 ¬± 1.25 90.18 0.51 10.10 ACM-GCN 94.75 ¬± 3.8 95.75 ¬± 2.03 94.92 ¬± 2.88 41.62 ¬± 1.15 69.04 ¬± 1.74 58.02 ¬± 1.86 67.01 ¬± 0.38 88.62 ¬± 1.22 81.68 ¬± 0.97 90.66 ¬± 0.47 4.80 ACM-Snowball-2 95.08 ¬± 3.11 96.38 ¬± 2.59 95.74 ¬± 2.22 41.4 ¬± 1.23 68.51 ¬± 1.7 55.97 ¬± 2.03 OOM 88.83 ¬± 1.49 81.58 ¬± 1.23 90.81 ¬± 0.52 4.44 ACM-Snowball-3 94.26 ¬± 2.57 96.62 ¬± 1.86 94.75 ¬± 2.41 41.27 ¬± 0.8 68.4 ¬± 2.05 55.73 ¬± 2.39 OOM 89.59 ¬± 1.58 81.32 ¬± 0.97 91.44 ¬± 0.59 4.44 ACMII-GCN 95.9 ¬± 1.83 96.62 ¬± 2.44 95.08 ¬± 2.07 41.84 ¬± 1.15 68.38 ¬± 1.36 54.53 ¬± 2.09 67.15 ¬± 0.41 89.00 ¬± 0.72 81.79 ¬± 0.95 90.74 ¬± 0.5 3.40 ACMII-Snowball-2 95.25 ¬± 1.55 96.63 ¬± 2.24 95.25 ¬± 1.55 41.1 ¬± 0.75 67.83 ¬± 2.63 53.48 ¬± 0.6 OOM 88.95 ¬± 1.04 82.07 ¬± 1.04 90.56 ¬± 0.39 4.78 ACMII-Snowball-3 93.61 ¬± 2.79 97.00 ¬± 2.63 94.75 ¬± 3.09 40.31 ¬± 1.6 67.53 ¬± 2.83 52.31 ¬± 1.57 OOM 89.36 ¬± 1.26 81.56 ¬± 1.15 91.31 ¬± 0.6 5.89</figDesc><table><row><cell></cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Deezer-Europe</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>#nodes</cell><cell>183</cell><cell>251</cell><cell>183</cell><cell>7,600</cell><cell>2,277</cell><cell>5,201</cell><cell>28,281</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell></row><row><cell>#edges</cell><cell>295</cell><cell>499</cell><cell>309</cell><cell>33,544</cell><cell>36,101</cell><cell>217,073</cell><cell>92,752</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell></row><row><cell>#features</cell><cell>1,703</cell><cell>1,703</cell><cell>1,703</cell><cell>931</cell><cell>2,325</cell><cell>2,089</cell><cell>31,241</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell></row><row><cell>#classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>H_edge</cell><cell>0.5669</cell><cell>0.4480</cell><cell>0.4106</cell><cell>0.3750</cell><cell>0.2795</cell><cell>0.2416</cell><cell>0.5251</cell><cell>0.8100</cell><cell>0.7362</cell><cell>0.8024</cell></row><row><cell>H_node</cell><cell>0.3855</cell><cell>0.1498</cell><cell>0.0968</cell><cell>0.2210</cell><cell>0.2470</cell><cell>0.2156</cell><cell>0.5299</cell><cell>0.8252</cell><cell>0.7175</cell><cell>0.7924</cell></row><row><cell>H_class</cell><cell>0.0468</cell><cell>0.0941</cell><cell>0.0013</cell><cell>0.0110</cell><cell>0.0620</cell><cell>0.0254</cell><cell>0.0304</cell><cell>0.7657</cell><cell>0.6270</cell><cell>0.6641</cell></row><row><cell>Data Splits</cell><cell cols="10">60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 50%/25%/25% 60%/20%/20% 60%/20%/20% 60%/20%/20%</cell></row><row><cell>H_agg M(G)</cell><cell>0.8032</cell><cell>0.7768</cell><cell>0.694</cell><cell>0.6822</cell><cell>0.61</cell><cell>0.3566</cell><cell>0.5790</cell><cell>0.9904</cell><cell>0.9826</cell><cell>0.9432</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Test Accuracy (%) of State-of-the-art Models, Baseline GNN Models and ACM-GNN models</cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell cols="2">MLP-2* 91.30 Geom-GCN  ‚Ä† 60.81</cell><cell>64.12</cell><cell>67.57</cell><cell>31.63</cell><cell>60.9</cell><cell>38.14</cell><cell>NA</cell><cell>85.27</cell><cell>77.99</cell><cell>90.05</cell><cell>22.67</cell></row><row><cell>SGC-1</cell><cell>70.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Experimental results: average test accuracy ¬± standard deviation on 10 real-world benchmark datasets. The best results are highlighted. Results "*" are reported from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameter Searching Range for Synthetic Experiments A.2 Hyperparameters Searching Range for GNNs on Ablation Study</figDesc><table><row><cell cols="5">A.1 Hyperparameters Searching Range for GNNs on Synthetic Graphs</cell></row><row><cell></cell><cell cols="4">Hyperparameter Searching Range for Synthetic Experiments</cell></row><row><cell cols="2">Models\Hyperparameters</cell><cell>lr</cell><cell>weight_decay</cell><cell>dropout</cell><cell>hidden</cell></row><row><cell>MLP-1</cell><cell></cell><cell cols="2">0.05 {5e-5, 1e-4, 5e-4}</cell><cell>-</cell><cell>-</cell></row><row><cell>SGC-1</cell><cell></cell><cell cols="2">0.05 {5e-5, 1e-4, 5e-4}</cell><cell>-</cell><cell>-</cell></row><row><cell>ACM-SGC-1</cell><cell></cell><cell cols="4">0.05 {5e-5, 1e-4, 5e-4} { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}</cell><cell>-</cell></row><row><cell>MLP-2</cell><cell></cell><cell cols="4">0.05 {5e-5, 1e-4, 5e-4} { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}</cell><cell>64</cell></row><row><cell>GCN</cell><cell></cell><cell cols="4">0.05 {5e-5, 1e-4, 5e-4} { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}</cell><cell>64</cell></row><row><cell>ACM-GCN</cell><cell></cell><cell cols="4">0.05 {5e-5, 1e-4, 5e-4} { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9}</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameter Searching Range for Ablation Study</cell></row><row><cell>Models\Hyperparameters</cell><cell></cell><cell>lr</cell><cell>weight_decay</cell><cell></cell><cell>dropout</cell><cell>hidden</cell></row><row><cell>SGC-LP+HP</cell><cell cols="4">{0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}</cell><cell>-</cell><cell>-</cell></row><row><cell>SGC-LP+Identity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameter Searching Range for Ablation Study A.3 Hyperparameters Searching Range for GNNs on Real-world Datasets See</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table 5 .</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter Searching Range for Real-world Datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>lr</cell><cell>weight_decay</cell><cell>dropout</cell><cell>hidden</cell><cell>lambda</cell><cell>alpha_l</cell><cell>head</cell><cell>layers</cell><cell>JK type</cell></row><row><cell cols="2">H2GCN</cell><cell></cell><cell>0.01</cell><cell>0.001</cell><cell>{0, 0.5}</cell><cell>{8, 16, 32, 64}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{1, 2}</cell><cell>-</cell></row><row><cell cols="2">MixHop</cell><cell></cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>{8, 16, 32}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{2, 3}</cell><cell>-</cell></row><row><cell cols="2">GCN+JK</cell><cell></cell><cell>{0.1, 0.01,</cell><cell>0.001</cell><cell>0.5</cell><cell>{4, 8, 16, 32, 64}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>{max, cat}</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.001}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GAT+JK</cell><cell></cell><cell>{0.1, 0.01,</cell><cell>0.001</cell><cell>0.5</cell><cell>{4, 8, 12, 32}</cell><cell>-</cell><cell>-</cell><cell>{2,4,8}</cell><cell>2</cell><cell>{max, cat}</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.001}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GCNII, GCNII*</cell><cell></cell><cell>0.01</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>0.5</cell><cell>64</cell><cell>{0.5, 1, 1.5}</cell><cell cols="2">{0.1,0.2,0.3,0,4,0.5} -</cell><cell>{4, 8, 16, 32}</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for Deezer-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>{4, 8, 16,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32, 64} for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>others</cell></row><row><cell cols="3">Baselines: {SGC-1, SGC-</cell><cell>{0.002,</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{0, 0.1,</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2,</cell><cell cols="2">GCN, Snowball-2,</cell><cell>0.01,</cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Snowball-3,</cell><cell>FAGCN};</cell><cell>0.05} for</cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ACM-{SGC-1,</cell><cell>SGC-</cell><cell>Deezer-</cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2,</cell><cell cols="2">GCN, Snowball-2,</cell><cell>Europe</cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Snowball-3};</cell><cell>ACMII-</cell><cell>and {0.01,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">{SGC-1, SGC-2, GCN,</cell><cell>0.05, 0.1}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Snowball-2, Snowball-3}</cell><cell>for others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GraphSAGE</cell><cell></cell><cell>{0.01,0.05,</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{ 0, 0.1,</cell><cell>8 for Deezer-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.1}</cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell>Europe and 64</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell>for others</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ACM-{GCNII, GCNII*}</cell><cell>0.01</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{ 0, 0.1,</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{1,2,3,4}</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">A.6 Computing Resources</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">For all experiments on synthetic datasets and real-world datasets, we use NVidia V100 GPUs with</cell></row><row><cell cols="11">16/32GB GPU memory, 8-core CPU, 16G Memory. The software implementation is based on</cell></row><row><cell cols="5">PyTorch and PyTorch Geometric [11].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for baseline models</figDesc><table><row><cell>Hyperparameters for ACM-GNNs and ACMII-GNNs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for ACM-GNNs and ACMII-GNNs B Details of Gradient Calculation in<ref type="bibr" target="#b4">(5)</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>T</figDesc><table><row><cell>.50</cell><cell>.50</cell><cell>.25</cell><cell>-.50</cell><cell>-.25</cell><cell>.00</cell><cell>-.25</cell><cell>.25</cell><cell>.13</cell><cell>.00</cell></row><row><cell>.50</cell><cell>.67</cell><cell>.50</cell><cell>-.50</cell><cell>-.17</cell><cell>-.17</cell><cell>-.50</cell><cell>.17</cell><cell>.17</cell><cell>.11</cell></row><row><cell>.25</cell><cell>.50</cell><cell>.50</cell><cell>-.25</cell><cell>.00</cell><cell>-.25</cell><cell>-.50</cell><cell>.00</cell><cell>.13</cell><cell>.17</cell></row><row><cell>-.50</cell><cell>-.50</cell><cell>-.25</cell><cell>.50</cell><cell>.25</cell><cell>.00</cell><cell>.25</cell><cell>-.25</cell><cell>-.13</cell><cell>.00</cell></row><row><cell>-.25</cell><cell>-.17</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.25</cell><cell>.00</cell><cell>-.50</cell><cell>-.38</cell><cell>-.17</cell></row><row><cell>.00</cell><cell>-.17</cell><cell>-.25</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.25</cell><cell>-.25</cell><cell>-.38</cell><cell>-.33</cell></row><row><cell>-.25</cell><cell>-.50</cell><cell>-.50</cell><cell>.25</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.00</cell><cell>-.13</cell><cell>-.17</cell></row><row><cell>.25</cell><cell>.17</cell><cell>.00</cell><cell>-.25</cell><cell>-.50</cell><cell>-.25</cell><cell>.00</cell><cell>.50</cell><cell>.38</cell><cell>.17</cell></row><row><cell>.13</cell><cell>.17</cell><cell>.13</cell><cell>-.13</cell><cell>-.38</cell><cell>-.38</cell><cell>-.13</cell><cell>.38</cell><cell>.38</cell><cell>.25</cell></row><row><cell>.00</cell><cell>.11</cell><cell>.17</cell><cell>.00</cell><cell>-.17</cell><cell>-.33</cell><cell>-.17</cell><cell>.17</cell><cell>.25</cell><cell>.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In general, harmful heterophily means the heterophilous structure that will make a graph-aware model underperform its corresponding graph-agnostic model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The authors in<ref type="bibr" target="#b23">[24]</ref> did not name this homophily metric. We name it class homophily based on its definition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">A similar J-shaped curve is found in<ref type="bibr" target="#b39">[40]</ref>, though using different data generation processes. It does not mention the insufficiency of edge homophily.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">In graph signal processing, an additional synthesis filter<ref type="bibr" target="#b9">[10]</ref> is required to form the 2-channel filterbank. But synthesis filter is not needed in our framework, so we do not introduce it in our paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">See the open source code for splits in https://github.com/jianhao2016/GPRGNN/blob/ f4aaad6ca28c83d3121338a4c4fe5d162edfa9a2/src/utils.py#L16. See table 9 in appendix I for the performance comparison with several SOTA models on the fixed 48%/32%/20% splits provided by<ref type="bibr" target="#b32">[33]</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Theorem 2</head><p>Proof. Define W c v = ( √ÇZ) v,c . Then,</p><p>Note that</p><p>For any node v, let the class v belongs to be denoted by c v . For two nodes v, u, if Z v,: = Z u,: , we have</p><p>Then, from <ref type="bibr" target="#b18">(19)</ref> it follows that</p><p>If Z v,: = Z u,: , i.e., c v = c u , we have</p><p>Then, from <ref type="bibr" target="#b18">(19)</ref> it follows that</p><p>Apparently, the two conditions in <ref type="bibr" target="#b9">(10)</ref> are satisfied. Thus v is diversification distinguishable and DD √Ç,X (G) = 1. The theorem is proved.   In order to separate the effects of nonlinearity and graph structure, we compare sgc with 1 hop (sgc-1) with MLP-1 (linear model). For GCN which includes nonlinearity, we use MLP-2 as the graph-agnostic baseline model. We train the above GNN models, graph-agnostic baseline models and which means its graph-label consistency and aggregated-feature-label consistency help the graphaware model obtain the performance gain; for Squirrel, its modified aggregation homophily is low but its S agg S( √Ç, X) is higher than its S agg (S(I, X)) which means although its graph-label consistency is bad but the aggregated-feature-label consistency is the key factor to help the graph-aware model perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Model Comparison on Synthetic Graphs</head><p>We also need to point out that (modified) aggregation similarity score, S agg S( √Ç, X) and S agg (S(I, X)) are not deciding or threshold values because they do not consider the nonlinearity structure in the features. In practice, a low score does not tell us the GNN models will definitely perform bad. In most real-world applications, not all labels are available to calculate the dataset statistics. In this section, We randomly split the data into 60%/20%/20% for training/validation/test, and only use the training labels for the estimation of the statistics. We repeat each estimation for 10 times and report the mean with standard deviation. The results are shown in table <ref type="table">8</ref>.</p><p>Estimation The statistics we estimate are H agg (G), S agg S( √Ç, X) , S agg (S(I, X)) and DD √Ç,X (G) and are denoted as ƒ§agg (G), ≈úagg S( √Ç, X) , ≈úagg (S(I, X)) and DD √Ç,X (G). The two similarity scores S agg S( √Ç, X) and S agg (S(I, X)) measures the proportion of nodes, according to aggregated features and nodes features respectively, that will put larger weights on nodes in the same class than in other classes. The higher values of S agg S( √Ç, X) and S agg (S(I, X)) indicates the better quality of aggregated features and nodes features.</p><p>Analysis From the reported results we can see that the estimations are accurate and the errors are within the acceptable range, which means the proposed metrics and similarity scores can be accurately estimated with a subset of labels and this is important for real-world applications. Furthermore, we notice some interesting results, e.g., the performance of GNNs and MLP are bad on Squirrel and Film, and according to the aggregation homophily values, the graph structure of Film is not quite harmful compared to other datasets, but its features and aggregated features are much worse than others; the features and aggregated features of Squirrel are not too bad, buts its graph topology is more harmful than others. Combining the metrics defined in this paper together can help us separate different factors in graph structure and features and identify what might cause the performance degradation of GNNs.</p><p>I Experiments on Fixed Splits Provided by <ref type="bibr" target="#b32">[33]</ref> See table <ref type="table">9</ref> for the results and table 10 the optimal searched hyperparameters. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<title level="m">Beyond low-frequency information in graph convolutional networks</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>arXiv, abs/1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>arXiv, abs/1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graph structured data viewed through a fourier lens</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ekambaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric scattering for graph data analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2122" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arXiv, abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G√ºnnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno>arXiv, abs/1801.07606</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simple truncated svd based model for node classification on heterophilic graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12807</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14612</idno>
		<title level="m">Non-local graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08844</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scattering gcn: Overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08414</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv, abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H D</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Texas Film Chameleon Squirrel Cora CiteSeer PubMed Rank GPRGNN 78.11 ¬± 6</title>
		<author>
			<persName><forename type="first">Cornell</forename><surname>Wisconsin</surname></persName>
		</author>
		<idno>55 82.55 ¬± 6.23 81.35 ¬± 5.32 35.16 ¬± 0.9 62.59 ¬± 2.04 46.31 ¬± 2.46 87.95 ¬± 1.18 77.13 ¬± 1.67 87.54 ¬± 0.38 8.22</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
