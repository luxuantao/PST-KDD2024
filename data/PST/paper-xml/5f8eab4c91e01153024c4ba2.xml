<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manas</forename><surname>Gaur</surname></persName>
							<email>mgaur@sc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Keyur</forename><surname>Faldu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Sheth</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
							<email>kaushikr@email.sc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of South Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of South Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of South Carolina</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Manuscript submitted to IEEE Internet Computing, October 12, 2020;under review</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Graphs</term>
					<term>Knowledge Infusion</term>
					<term>Neuro-Symbolic AI</term>
					<term>Explainability</term>
					<term>Interpretability</term>
					<term>Black-Box Deep Learning</term>
					<term>Mental Healthcare</term>
					<term>Education Technology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. The DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, signal processing, and human-computer interactions. However, the Black-Box nature of DL models and their over-reliance on massive amounts of data condensed into labels and dense representations poses challenges for interpretability and explainability of the system. Furthermore, DLs have not yet been proven in their ability to effectively utilize relevant domain knowledge and experience critical to human understanding. This aspect is missing in early data-focused approaches and necessitated knowledge-infused learning and other strategies to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL methods using knowledge-infused learning, which is one of the strategies. We then discuss how this makes a fundamental difference in the interpretability and explainability of current approaches, and illustrate it with examples from natural language processing for healthcare and education applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>depression. I dread having to retread all this again because the clinic where I get my mental health addressed is closing down due to loss in business caused by the pandemic; Question: Does the person suffer from depression?; Response generated from a pre-trained seq2seq model: Yes (the correct answer is no).</p><p>In such scenarios, it is exceptionally challenging to probe the model's mechanism without the support of background knowledge <ref type="bibr" target="#b0">[1]</ref>. Although Neural Attention Models (NAM) <ref type="bibr" target="#b1">[2]</ref> are endowed with a certain degree of interpretability in visualizing parts of the sentence which are focused on answering the question, they cannot provide further explanations to answer the question in a human-understandable format. Furthermore, recent research has highlighted challenges concerning the model behavior in question answering domains, particularly towards queries with disjunctive clauses (questions that contain or) <ref type="bibr" target="#b2">[3]</ref>. We have observed unpredictable responses to questions like Are you feeling nervous or anxious or on edge? Is the feeling of restlessness due to stress or anxiety? Does an employee own a company or work for a company?</p><p>The ability to fine-tune pre-trained models trained in unsupervised settings to a downstream task has alleviated the need for an extensive labeled dataset. This introduces the challenge of explaining decisions using insights from limited labeled data. Research practices have often resorted to using frozen datasets, intending to surpass benchmark test-data-accuracy. As a result, DL models have outperformed human baselines on specific datasets in terms of 'test-data-accuracy' but still fail to generalize intuitive cases in real-world scenarios. Ribeiro et al. introduced CHECKLIST, a task agnostic framework to test the generalisability of NLP models, and found that these models are vulnerable to test data prepared with linguistically intuitive rules <ref type="bibr" target="#b3">[4]</ref>. The bottom line is that most state-of-theart DL approaches are not integrated with prior knowledge, a necessary condition for explaining the predictions, and interpreting the model mechanism for appropriate probing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY</head><p>The Black-Box nature of DL models needs to be addressed to foster trust among users and domain experts to be used with higher confidence. This can also facilitate broader assimilation in a variety of domains. In healthcare, clinicians routinely choose methods that allow them to understand how an outcome was derived compared to an objectively superior method that cannot be explained. In education, tracing students' learning outcomes with attribution to weak academic and behavioral areas is a better tool for teachers compared with the ability to predict only a student's performance. As a result, explainability and interpretability for the DL models have focused on recent key research areas.</p><p>Prior research has attempted to establish a generic definition of explainability, which is the ability to generate human-comprehensible explanations around the decision-making process <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>. In contrast, interpretability is the ability to discern the internal mechanisms of any module. Key reasons we need explainability and interpretability are to: (a) trace and verify the fine-grained supporting information in safetycritical systems (e.g., autonomous driving vehicles), (b) support evolving events and discern necessary context -as the underlying facts (data) may not be static (e.g. newer findings replace earlier findings), and metainformation such as time and space/location may be critical in understanding, interpreting, and explaining results (e.g., identifying emerging sub-events in natural disasters <ref type="bibr" target="#b7">[8]</ref>), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling <ref type="bibr" target="#b1">[2]</ref>[9]), (d) prevent prediction errors in unintuitive scenarios (e.g. adversarial examples <ref type="bibr" target="#b9">[10]</ref>, CHECKLIST <ref type="bibr" target="#b3">[4]</ref>), (e) make sure minor perturbations in inputs are handled in robust ways, (f) After enhancement of training data through knowledge. graph (KG), proactive detection of outliers is possible; (g) gather new knowledge insights to further enable research, and (h) ensure that acquired domain knowledge is getting leveraged for the decision making process. The enumerated list combines the need for "explainability and interpretability" as it applies to DL systems and enhancements obtained by integrating a knowledge graph-based approach. Addressing these concerns would foster confidence among domain experts and trust among end-users. The terms explainability and interpretability are often used interchangeably in the prior research without clear distinctions and the different roles that they play <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEFINING EXPLAINABILITY AND INTERPRETABILITY WITH KNOWLEDGE-INFUSION</head><p>Making explanations on the model behavior is subjective to the problem from the stakeholders. A set of privileged knowledge (e.g., domain expertise, advice specific to the situation) must be infused to comprehend the model outcomes and interpret its functioning. The methods for infusing knowledge in DL models occurs through a set of neuro-symbolic procedures that enrich the dataset with concepts and relationships from multiple KGs or ontologies, which assists end-users in decision making. For instance: Taxonomic Knowledge (e.g., Columbia Suicide Severity Risk Scale (C-SSRS)), Relational Knowledge (e.g., ConceptNet), Metaphorical Knowledge, and Behavioral Knowledge (e.g., LIWC) are necessary forms of external contextual information required to understand online conversations, mainly when the problem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) <ref type="bibr" target="#b10">[11]</ref>. Also, knowledge-infusion during the model learning using information-theoretic loss function (e.g., KL divergence <ref type="bibr" target="#b1">[2]</ref>) can check conceptual drifting at the representational level through weak-supervision from KG. Alternatively, the loss of information during learning can be supplemented by augmenting the abstract information from KG to layered representation in DL models through various mathematical operations (e.g., pointwise multiplication, concatenation). Fusing the relevant information from KG to hidden representations in DL allows quantitative and qualitative assessment of its functioning, which we define as knowledge-infused interpretability. One possible way to do knowledge infusion for discerning model behavior is through an architecture having layers of DL connected to KG through a function. Such a function can be a "knowledge-aware loss function" that computes the loss in information at each layer per epoch. Or a function can be "knowledge-aware propagation function," which computes the loss information and transfers missing information through mathematical operations. The computing of loss in information is performed by tracing the KG through the embedding of hidden layers and printing the concepts and relationships that the model is learning through pattern recognition. On the other hand, having a ground-truth subgraph of KG would help in calculating the distance between the concepts learned by hidden layers and actual concepts in the subgraph (all done at the embedding level), which can be used to modulate the hidden embeddings and thus leading to faster convergence with model interpretations. In achieving interpretability and explainability through knowledge-infusion, these two terminologies can be differentiated as Explainability would cater to why the prediction has been made. In contrast, interpretability would unfold the ability to understand patterns learned or knowledge acquired by the system. Any explainable system has to be interpretable, but the reverse is not valid. Explainability can be evaluated with its interpretability and completeness.</p><p>"Empirically, an explainable system would comprise of collectively exhaustive interpretable subsystems and orchestration among them. More often than not, explanations would be in natural language explaining the decision, while interpretations can be statistical or conceptual (using either generic or domain-specific KG <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>) in nature pertaining to its inner functioning."</p><p>Explanations can be thought of as answers to cascading "why questions" in the context of model prediction until there is no longer a need to ask another "why question". Explanations are expected to be faithful and plausible. Faithfulness defines how well the explanation correlates with the model's decision making. It is considered plausible when it has a human-understandable justification for the prediction <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref>. Such systems are considered to be potentially useful for real-world decision making in various domains, especially healthcare and education technology. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LIMITATIONS OF LEARNING USING DISTRIBUTIONAL SEMANTICS</head><p>The recent success of DL language models in NLP has been attributed to self-supervised objectives over a large volume of unlabeled data such as BERT, RoBERTa, and T5. These models learn distributional semantics about the text and relationships between phrases. It is also an active research area to probe whether these models learn linguistic knowledge like part of speech and dependency tree. Many probing experiments have given insights into linguistic patterns discovered by different layers and internal components like attention heads of neural networks. However, it is still an open question, how much semantic knowledge is learned by these models, specifically when the semantics are not expressed with statistically significant patterns in the data. This becomes evident when these models fail to learn facts around concepts.</p><p>Furthermore, there is a growing trend of fine-tuning the pre-trained models with limited labeled data. These have given success when (a) the distribution of the labeled dataset is similar to unlabeled data used for pre-training, (b) tasks are relatively straightforward like natural language entailment and span extractive question answering. However, real-world scenarios are often more complex, which poses the following challenges: (a) Fine-tuning such models for domain-specific tasks with limited labeled data may not be sufficient to learn domain knowledge as these data may not be able to capture domain knowledge. (b) Similarly, self-supervised training objectives over unlabeled data is not attempting to learn the domain knowledge required for real-world adoption. (c) Generating explanations and interpretations would be even more challenging as internal layers or mechanisms of distributional semantics would not represent specific domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. KNOWLEDGE GRAPH INFUSION FOR BETTER EXPLAINABILITY</head><p>Aforementioned concerns in distributional semantics calls for a specific need to infuse domain knowledge in deep neural models for better predictive performance and the descriptive performance of generating explanations and interpretations.</p><p>Infusing domain knowledge in DL models can be categorized into the shallow infusion, semi-deep infusion, and deep infusion <ref type="bibr" target="#b1">[2]</ref>. In shallow infusion, both the external information and method of knowledge infusion is shallow, i.e., Word2Vec or GloVE embeddings are reconstructed using domain knowledge as features (see Figure <ref type="figure">1</ref>  <ref type="bibr" target="#b14">[15]</ref>). On the other hand, deep infusion of knowledge is a paradigm that couples the latent representation learned by deep neural networks with the KGs exploiting the semantic relationships between entities <ref type="bibr" target="#b15">[16]</ref>.</p><p>Recently, there has been a flurry of research to experiment with different techniques to infuse knowledge graphs or domain rules to deep neural models for different needs. Mainly they can be categorized as follows: (a) Fusion of relevant external knowledge as an additional context in the query or as external knowledge-based features, (b) Infusing KG embedding to hidden layers of neural models for explainable decision making, (c) Infusing contextual representations from relevant subgraph or paths of KG through either concatenation or pooling or non-linear transformations, (d) Leveraging KG to alter attention mechanism and pre-training of DL model, and (e) Developing approaches that uses domain-specific KG using Integer Linear Programming for non-trivial natural language inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPLANATIONS AND INTERPRETATIONS FOR HEALTHCARE DOMAIN USE CASES</head><p>Consider an application domain of summarization, which is at the intersection of natural language understanding and DL. An extensive body of research has investigated summarizing news articles, meeting logs, and financial/legal contracts that are often templatized. On the other hand, patient-clinician conversations are open-ended as the clinician tries to reflect on the patient's responses. Consequently, the interview forms an inherent structure, that raises challenges in natural language understanding: (1) Anaphora-where sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user); <ref type="bibr" target="#b1">(2)</ref> The clinical conversation contains implicit references to healthcare conditions, developing sparsity in the clinically-relevant concepts. Such a problem scenario requires Fig. <ref type="figure">1</ref>. Illustration of shallow infusion wherein the Google BERT model is reconstructed after concepts from Drug Abuse Ontology (DAO) were appended to existing vocabulary to assess the negative media exposure during COVID-19. This weakly supervised approach with knowledge infusion is practical when it is difficult to annotate millions of news articles and procure labeled dataset for depression and drug abuse a model to capture the context of the conversation (see Figure <ref type="figure">2</ref>).</p><p>The summarization of clinical conversation requires meaningful responses to be associated with clinically-relevant questions while resolving anaphora problem. State-of-the-art language models (e.g., BERT) trained over the large-scale corpus of news articles fail to capture the question's context and assess the importance of a response from the clinician perspective. In addition, BERT-like models' fine-tuning is not helpful because updating the model parameter needs to be governed by a structure that can abstractly describe a clinical conversation with stratified knowledge (see Figure <ref type="figure">3</ref>). Hence, recently, Integer Linear Programming (ILP)-based summarization approaches have gained sufficient traction from the NLP community (https://www.jmir.org/preprint/20865). The optimization framework is interpretable as knowledge is incorporated as constraints. The outcome is explainable because optimization criteria reflect on the end-users requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPLANATIONS AND INTERPRETATIONS FOR</head><p>EDUCATION DOMAIN USE CASES Let's take an example of a use case in the education domain (see Figure <ref type="figure" target="#fig_0">4</ref>). We illustrate this figure with respect to two types of knowledge infusion -shallow and deep infusion. Shallow infusion covers the scenario of using knowledge at the start of the DL pipeline and deep infusion refers to using knowledge at every layer of the pipeline<ref type="foot" target="#foot_0">1</ref> . Student's score prediction is one of the key problems to assess the student's true potential for a goal. In the context of this problem, explanations of this prediction are really important as they reveal what governs the student score, both strengths, and weaknesses on the academic and behavioral aspects. These explanations can potentially induce remedial actions from the student and mentor.</p><p>The domain knowledge for student performance prediction can be thought of as (a) Academic knowledge graph, set of concepts, it's metadata and relations between concepts that plays an important role in tracing student's concepts mastery <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b15">[16]</ref>, and (b) Engagement and learning patterns from historical student's data <ref type="bibr" target="#b12">[13]</ref>. Students' historical knowledge state could be derived from this data <ref type="bibr" target="#b17">[18]</ref>.</p><p>The domain knowledge infused model would be able to explain student's performance by tracing the predicted score to weak concepts, furthermore, to the cause of these weak concepts. Infusing academic knowledge graphs and a student's historical knowledge state makes it possible to trace academic weakness until the deepest pre-requisite concept impacts student score <ref type="bibr" target="#b18">[19]</ref>. As shown in Figure <ref type="figure" target="#fig_0">4</ref>, the student has got an attempt wrong on a question of concept "Projectile Motion", using a knowledge graph and student's knowledge Fig. <ref type="figure">2</ref>. Overview of Knowledge-infused Abstractive Summarization (KiAS) for an interview snippet of a patient's responses to the question asked by Ellie (virtual interviewer). Phrases relevant to mental health are identified using the PHQ-9 lexicon. The contextual similarity between utterances is calculated through a retrofitted embedding model. The resulting summaries contain relevant questions and meaningful responses. KiAS strategy recognizes the semantic similarity between Anxiety disorder [SNOMEDCT ID: 197480006] and PTSD [SNOMEDCT ID: 47505003] as described in the SNOMED-CT hierarchy through an "is-a" relationship and recorded in the PHQ-9 lexicon. It associates responses of anxiety and PTSD interchangeably Fig. <ref type="figure">3</ref>. In an experiment to summarize a transcript of a 12 minute recorded conversation between a patient and clinician (https://bit.ly/patid313), ILP with PHQ-9 knowledge could align appropriate responses to the question much better than simple ILP based abstractive summarization and pre-trained BERT. KiAS generates summaries (7 sentences on an average) that capture informative questions and responses exchanged during long (58 sentences on an average), ambiguous, and sparse clinical diagnostic interviews state it could be traced deeper to the previous grade concept "Quadratic Equation".</p><p>Students' behavior is an equally important part of tracing learning outcomes <ref type="bibr" target="#b17">[18]</ref>. Assessments or diagnostic tests are also created using such behavioral profiling <ref type="bibr" target="#b16">[17]</ref>. Student's tendency to answer a question without comprehending or thinking through can lead to "Careless Mistakes", which could be the result of guessing the answer or expressing over-confidence. Similarly, the inability to leave a question unanswered to focus more on answering other questions could be termed as a lack of decisiveness. Which could lead to "Overtime Incorrects" attempts. Domain knowledge is needed to classify attempts to a "Careless Mistakes", or an "Overtime Incorrects" attempt. Figure <ref type="figure" target="#fig_0">4</ref> shows an example of generating explanations with student's behavioral traits like "Careless Mistakes", "Overtime Incorrects" by infusing domain knowledge into the model.</p><p>Interpretations around these explanations could be traced back to the student's attempts-stream as input, domain knowledge, and data lake to understand how these are derived by applying interpretability techniques mentioned in Table <ref type="table">1</ref>. Moreover, the impact of such explanations on student's performance prediction could also be derived and it could furnish as a foundation for recommendations and estimating learning outcomes upon student's action for the same <ref type="bibr" target="#b17">[18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION</head><p>In this article, we highlighted the need for explanations and interpretations for the domain adoption of AI models, particularly in healthcare and education technology. We overviewed existing statistical methods and metrics devised to quantitatively assess the explainability of the model and interpretability of its mechanisms. Existing frameworks categorized as post-hoc Interpretability, counterfactual explanations, and rule-based explanations fall short in providing answers to the following open questions: Can the model mine (varied) relationships from the existing text? Can the model reliably classify entities into known ontology? Can the model answer the question with trust and transparency? Is it possible to measure the model's "reasonability" and "meaningfulness" of the response to a question? How much context is needed for the model to provide a precise response? An emerging trend to fine-tune a pre-trained model on limited labeled data for a downstream task and the inability of distributional semantics learning to capture domain-specific knowledge pose limitations in addressing the above questions. We noticed the necessity of KG as an integral component in neuro-symbolic AI systems with capabilities to generate explainable outcomes and interpretability through tracing over KG. Future advances in this area would seek AI systems that can help stakeholders (e.g., instructors, public health experts) to conceptually understand the working of involved AI systems <ref type="bibr" target="#b19">[20]</ref>. There is also a pressing requirement for benchmark datasets which assess the quality of explanations derived from model outcomes and interpretability of the algorithms in achieving explanations <ref type="bibr">[4] [21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Interpretations and Explanations for Students Performance Prediction. Student's attempt stream in practice or test session is fed as "Inputs". Domain knowledge present in knowledge graphs and data lakes can be "shallow infused" in features, or "deep infused" in the model. Explanations and Interpretations are generated along with prediction of student score.</figDesc><graphic url="image-4.png" coords="6,48.96,56.07,514.08,405.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="4,48.96,56.07,514.09,321.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="5,48.96,56.07,514.09,241.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="5,48.96,366.77,514.11,155.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PRIOR</head><label>I</label><figDesc>APPROACHES AND METHODS TO DISCOVER INTERPRETABLE SUBSYSTEMS. THOUGH IT FORMS THE SUPERSET OF THE TAXONOMY OUTLINED IN GILPIN ET AL., THESE METHODS ARE NOT SUFFICIENT TO ACHIEVE EXPLAINABILITY AND INTERPRETABILITY OF THE LEVEL REQUIRED FOR DECISION MAKING IN HIGH STAKES PROBLEMS, INCLUDING MENTAL HEALTHCARE AND EDUCATION TECHNOLOGY.</figDesc><table><row><cell>Focus Area</cell><cell>Approach</cell><cell>Methods</cell><cell>Interpretable outcomes</cell></row><row><cell>Input Features</cell><cell>Proxy Functions</cell><cell>LIME, Gradients, Saliency maps, Smooth-</cell><cell>Linear Model Coefficients, Relative Fea-</cell></row><row><cell></cell><cell></cell><cell>Grad</cell><cell>tures Importance</cell></row><row><cell>Input Features</cell><cell>Occlusion or Perturbations</cell><cell>SHAP, Integrated Gradients, DeepLift,</cell><cell>Relative Features Importance</cell></row><row><cell></cell><cell></cell><cell>PDA, Activation Maximization</cell><cell></cell></row><row><cell>Input Features</cell><cell>Tracing Positive Contributions</cell><cell>LRP, Deconvolutions, Guided Backpropaga-</cell><cell>Relative Features Importance</cell></row><row><cell></cell><cell></cell><cell>tion</cell><cell></cell></row><row><cell>Representations</cell><cell>Projections</cell><cell>PCA, ICA, NMF, Conicity</cell><cell>Interpretable Vector Representations Space</cell></row><row><cell>Attentions</cell><cell>Transformations</cell><cell>Effective attention, attention flow</cell><cell>Interpretable Attentions Distribution</cell></row><row><cell>Neurons, Layers Con-</cell><cell>Transformations</cell><cell>Conductance, Layer Relevance Propagation,</cell><cell>Neurons, Layers Contribution</cell></row><row><cell>tribution</cell><cell></cell><cell>DIFFPOOL</cell><cell></cell></row><row><cell>Encoded Knowledge</cell><cell>Probes</cell><cell>Knowledge Probes, Diagnostic Classifiers,</cell><cell>Interpretable Vector Representations Space,</cell></row><row><cell>in Representations,</cell><cell></cell><cell>Auxiliary Tasks</cell><cell>Attentions Distribution, Relative Feature</cell></row><row><cell>Attentions, Layers</cell><cell></cell><cell></cell><cell>Importance</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://bit.ly/kidl2020</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">esnli: Natural language inference with natural language explanations</title>
		<author>
			<persName><forename type="first">O.-M</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9539" to="9549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shades of knowledgeinfused learning for enhancing deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kursuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wickramarachchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05969</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond accuracy: Behavioral testing of nlp models with checklist</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tiddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>L?cu?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IOS Press</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Oltramari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wickramarachchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04707</idno>
		<title level="m">Neuro-symbolic architectures for context understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised detection of sub-events in large scale disasters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arachie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anzaroot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-infused deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kursuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wickramarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM Conference on Hypertext and Social Media</title>
		<meeting>the 31st ACM Conference on Hypertext and Social Media</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="309" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graphs to empower humanity-inspired ai systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Shalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing crowd wisdom using explainable diversity inferred from social media</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bullemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minnery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive learning machine for score improvement and parts thereof</title>
		<author>
			<persName><forename type="first">K</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<idno>App. 15/708</idno>
		<imprint>
			<date type="published" when="2018">Mar. 29 2018</date>
			<biblScope unit="page">591</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">uS Patent</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nature inspired algorithms in remote sensing image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Depressive, drug abusive, or informative: Knowledge-aware study of news exposure during covid-19 outbreak</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alambo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15209</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Knowledge infused learning (kil): Towards deep incorporation of knowledge in deep learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kursuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00512</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto generation of diagnostic assessments and their quality evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhavala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avasthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 13th International Conference on Educational Data Mining</title>
		<meeting>The 13th International Conference on Educational Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="730" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A framework for predicting, interpreting, and improving learning outcomes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dhavala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avasthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02629</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep knowledge tracing with side information</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence in Education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
