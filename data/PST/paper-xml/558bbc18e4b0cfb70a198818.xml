<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hands in Action: Real-Time 3D Reconstruction of Hands in Interaction with Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Vision and Active Perception Lab, Centre for Autonomous Systems</orgName>
								<orgName type="institution">CSC-KTH</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hedvig</forename><surname>Kjellstr√∂m</surname></persName>
							<email>hedvig@kth.se</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Vision and Active Perception Lab, Centre for Autonomous Systems</orgName>
								<orgName type="institution">CSC-KTH</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danica</forename><surname>Kragic</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Vision and Active Perception Lab, Centre for Autonomous Systems</orgName>
								<orgName type="institution">CSC-KTH</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hands in Action: Real-Time 3D Reconstruction of Hands in Interaction with Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2475747328E8068AE5B555E571DE3BB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a method for vision based estimation of the pose of human hands in interaction with objects. Despite the fact that most robotics applications of human hand tracking involve grasping and manipulation of objects, the majority of methods in the literature assume a free hand, isolated from the surrounding environment. Our hand tracking method is non-parametric, performing a nearest neighbor search in a large database (100000 entries) of hand poses with and without grasped objects. The system operates in real time, it is robust to self occlusions, object occlusions and segmentation errors, and provides full hand pose reconstruction from markerless video. Temporal consistency in hand pose is taken into account, without explicitly tracking the hand in the high dimensional pose space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Articulated tracking and reconstruction of human hands has received an increased interest within the fields of computer vision, graphics and robotics <ref type="bibr" target="#b0">[1]</ref> and applications include learning from demonstration, rehabilitation, prosthesis development, human-computer interaction. Our goal is to equip robots with the capability of observing human hands in interaction with objects based solely on vision data, without markers.</p><p>Capturing hand articulation from video without markers is a challenging problem. A realistic articulated hand model has at least 28 degrees of freedom, making the state-space very large. The pose estimation suffers from self-similarity -fingers are hard to distinguish from each other -and a high degree of self-occlusion. Furthermore, hands move fast and non-linearly. Any method is thus computationally costly, making real-time implementation demanding. Although there are hand tracking systems developed for specific purposes such as sign recognition <ref type="bibr" target="#b0">[1]</ref>, full pose estimation remains an open problem, specially if real-time performance is required, as in virtually all robotics applications.</p><p>Hand pose estimation methods can largely be divided into two groups <ref type="bibr" target="#b0">[1]</ref>: A) model based tracking and B) single frame pose detection. Methods of type A) usually employ generative articulated models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Due to the high dimensionality of the human hand, they are facing challenges such as high computational complexity and singularities in the state space. They are thus generally unsuitable for robotics applications. Methods of type B) are usually nonparametric <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. They are computationally less demanding and more suited for a real-time system, but also more brittle and sensitive to image noise, since there is no averaging over time. In this paper we present a type B) non-parametric pose estimation method (Fig. <ref type="figure" target="#fig_0">1</ref>), which takes temporal consistency into account. The probabilistic framework of this method is described in Section II. The method is faster and better at recovering from temporary errors than type A) model-based tracking methods. In an earlier paper <ref type="bibr" target="#b6">[7]</ref> we also showed that the time continuity constraint makes the method more accurate and robust than other type B) single frame detection methods.</p><p>The method maintains a large database of (synthetic) hand images. Each database instance is labeled with 31 parameters describing the hand articulation and orientation of the hand with respect to the camera. The 31D hand configuration of a new (real) image can then be found using an approximate nearest neighbor approach, taking previous configurations into account. Section II describes the composition of the database. The hand image representation is described in Section IV and the nearest neighbor-based mapping is described in Section V.</p><p>In the majority of applications, the human hands are frequently in contact with objects. Despite this, researchers have up to now almost exclusively focused on estimating the pose of hands in isolation from the surrounding scene. A recent notable exception is <ref type="bibr" target="#b7">[8]</ref>, who describe a type A) model-based tracker that allows for objects in the hand. Our method is also able to reconstruct hands both with and without grasped objects. Reconstruction of a hand grasping an object is in many ways a much more challenging task than reconstruction of a free hand, since the grasped object generally occludes large parts of the hand. The method of <ref type="bibr" target="#b7">[8]</ref> allows for hand pose reconstruction despite the object occlusion.</p><p>On the other hand, knowledge about object shape gives important cues about the configuration of palm and fingers in contact with the object. Moreover, object shape and functionality give cues as to how this object is generally grasped. The relation between object shape and hand shape is however complex, and this information is hard to exploit in a type A) generative tracking model. In contrast to <ref type="bibr" target="#b7">[8]</ref>, our method is non-parametric, which means that complex objecthand shape dependencies can be implicitly represented by examples. Hand views in the database depicting grasping hands include occlusion from objects with a shape typical for this kind of grasp (Fig. <ref type="figure" target="#fig_0">1</ref>). The occlusion affects the appearance of a hand view, so that hands with similar objects in them will appear similarly. Since the underlying assumption is that appearance similarity implies similarity in hand pose, the object shape contributes to the hand pose estimation in our method. Thus, the main contribution of the paper is a robust nonparametric method for 3D hand reconstruction, operating in real-time, that also takes time continuity constraints into account. The method handles severe occlusions of the hand and also takes the object shape into account in 3D hand reconstruction. Experiments in Section VII also show that the method is robust to segmentation errors, a necessary requirement for the method to be applicable in a realistic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Probabilistic Framework</head><p>The following notation is used throughout the paper. In a specific time instant t, let x t be the articulated hand pose and y t the observation. Here, x t is a 28 dimensional vector of joint angles, and y t is a 512D histogram of oriented gradients (HOG) <ref type="bibr" target="#b8">[9]</ref>, see Section IV. The space spanned by x is hereafter called JOINT space, while the space spanned by y is called HOG space. We assume that p(x t ) is uniform over the JOINT space, and that the process is Markovian, i.e., x t depends on the previous pose x t-1 only.</p><p>As shown in <ref type="bibr" target="#b6">[7]</ref>, the view y t alone is not enough to nonambiguously estimate the articulated hand pose x t . Therefore, the pose x t-1 at the previous timestep is taken into account in the estimation. This corresponds to sequential estimation of p(x t |y t , x t-1 ), the hand pose given the observation and the previous state. The temporal regression problem is decomposed as p(x t |y t , x t-1 ) ‚àù p(x t |y t )p(x t |x t-1 ). As shown in Fig. <ref type="figure">2</ref>, the method takes as input a monocular image and segments the hand based on skin color segmentation (a). A HOG y t is then computed as described in Section IV (b).</p><p>The HOG y t is compared to a large database of hand views (c), returning a weighted set of nearest neighbors {(y i t , x i t , w i t )}, as described in Section V (d). Each neighbor view y i t from the database has an associated joint angle configuration x i t , which, weighted by w i t , constitute a sampled approximation of p(x t |y t ) (e).</p><p>The temporal consistency constraint p(x t |x t-1 ) is a parametric function of x t and x t-1 , as explained in Section VI (f). This term gives a higher probability to estimates where the hand has moved little over the last time step, thus giving priority to smooth motion estimates. The multiplication with p(x t |x t-1 ) is approximated by updating the database nearest neighbor weights to w * i t ‚àù w i t p(x i t |x t-1 ) (g). The expected hand pose value at time t is then estimated as xt = E(x t |x t-1 , y t ) ‚âà arg max x i t w * i t , i.e., the database pose with the highest weight (h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Database Composition</head><p>The hand pose x t could potentially be found by expressing p(x t |y 1 , x t-1 ) parametrically, and finding the maxima of this function using an optimization algorithm. However, this optimization problem is high dimensional and non-convex. To alleviate the dimensionality problem, and constrain the search to commonly observed hand poses, we use a nonparametric approach: we discretize the state space by creating a large database of hand poses with synthetic images.</p><p>The composition of the database is motivated by our research aim: understanding human interaction with objects. Our database has more than 10 5 images, consisting of 5 different timesteps of 33 object grasping actions observed from 648 different viewpoints. The grasp types are selected according to the taxonomy presented in <ref type="bibr" target="#b9">[10]</ref>. The graphics software Poser 7 is used to generate the synthetic hand views. The synthetic views in the database include basic object shapes that are usually involved in each kind of grasp (see Fig. <ref type="figure" target="#fig_1">3c</ref>). The objects are considered background (although colored black for visibility in the figures) and the hand parts occluded by the object do not provide any features to the image observation y t . This can be seen in Fig. <ref type="figure" target="#fig_1">3c</ref>, bottom, where there is a "hole" in the middle of the HOG. As mentioned in the Introduction, the object shape contributes to the hand pose estimation in our method, since the hand pose depends on the shape of the object, which in turn affects the HOG y t .</p><p>It can be argued that this method can only work if the object shape in the real action is the same as in the database. However, firstly, a particular kind of grasp is executed usually to similarly shaped objects and, secondly, the features used in our system (see Section IV) generalizes well over small variations in object shape. As described in Section II, p(x t |y t , x t-1 ) is modeled non-parametrically using {(y i t , x i t )}, a set of database nearest neighbors to y t in HOG space, weighted by their distance to y t in HOG space and x t-1 in JOINT space. The weighting is formalized in Sections V and VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Image Representation</head><p>The input to the method are monocular images of the type and quality shown in Figure <ref type="figure" target="#fig_1">3a</ref>. In these images, the hand is segmented using skin color thresholding in HSV space <ref type="bibr" target="#b10">[11]</ref> (Figure <ref type="figure" target="#fig_1">3b</ref>, top). From the segmented hand image a histogram of oriented gradients (HOG) <ref type="bibr" target="#b8">[9]</ref> is extracted (Figure <ref type="figure" target="#fig_1">3b</ref>, bottom). This is a rich representation of shape, with certain robustness towards segmentation errors and small differences in spatial location and proportions of the segmented hand. The image is partitioned into cells and a histogram of gradient orientation is computed for each cell.</p><p>The size of the cells and the granularity of the histograms affect the generalization capabilities of the feature. With smaller cells and detailed histograms, the feature is richer but less capable of generalize over small differences. For our purposes, 8 √ó 8 cells and histograms with 8 bins provide good generalization with a sufficient level of details. The observation y t equals the concatenation of the 8 √ó 8 histograms corresponding to each cell of the image. The dimensionality of y t is thus 8 √ó 8 √ó 8 = 512. A more detailed discussion on how different parameters of the HOG affect human detection can be found in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Non-Parametric Mapping</head><p>The probability density function p(x t |y t ) is approximated by indexing into the database of hand poses using the image</p><formula xml:id="formula_0">p(x t |x t-1 ) JOINT space Gaussian xt-1 (a) Gaussian weighting p(x t |x t-1 )</formula><p>JOINT space As an exact kNN search would put serious limitations on the size of the database, an approximate kNN search method, Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b11">[12]</ref> is employed. LSH is a method for efficient nearest neighbor ( NN) search. It is particularly suited for high dimensional data, since its online complexity does not depend explicitly on the set size or the dimensionality <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_1">x 3 t-1 x 2 t-1 x 1 t-1 KDE (b) KDE weighting</formula><p>Each retrieved NN y i t is given a weight w i t = N(y i t |y t , œÉ y ), drawn from a 512D Gaussian density centered in y t with standard deviation œÉ y . This gives higher weight to database NN that look similar to the observed hand.</p><p>In the database, each HOG y j is associated with a pose x j . The poses corresponding to the NN {y i t } can thus be retrieved. Together with the weights, they form the set {(x i t , w i t )} which is a sampled non-parametric approximation of the density p(x t |y t ).</p><p>The pose vector x is composed of the rotation matrix of the wrist wrt the camera and the sines of the joint angles of the hand (which takes values between [-œÄ 2 , œÄ 2 ]). Each component of x therefore lie in the domain [-1, 1], which makes scaling unnecessary. The advantage of using a rotation matrix to represent the wrist rotation is that rotation matrices can be compared in a Euclidean fashion, as opposed to Euler angles and quaternions. Euclidean comparison of poses is used in the temporal consistency modeling (Section VI) and the experimental evaluation (Section VII-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Temporal Consistency Modeling</head><p>As described in Section II, the temporal consistency constraint p(x t |x t-1 ) is modeled as a parametric function. It is used to reweight the sampled distribution {(x i t , w i t )}, approximating p(x t |y t ). We propose two ways to model the temporal consistency constraint, outlined in the two subsections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Hypothesis Gaussian Weighting</head><p>The simplest way of modeling temporal consistency is to assume that poses similar to the previous estimated pose xt-1 are more likely than poses that are very different from the previous one. Hence, p(x t |x t-1 ) = N(x t | xt-1 , œÉ x ), a 28D Gaussian density centered in xt-1 with standard deviation œÉ x . This approach was used in <ref type="bibr" target="#b6">[7]</ref> and is depicted in Figure <ref type="figure" target="#fig_2">4a</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple Hypothesis Kernel Density Estimation Weighting</head><p>A drawback of the single hypothesis approach is that all the "second best" nearest neighbor hypotheses at t -1 are thrown away before temporal propagation. A logical improvement is to consider the full weighted set of hypotheses {(x i t-1 , w * i t-1 )} instead of the most likely hypothesis xt-1 in the estimation of p(x t |x t-1 ). This is illustrated in Figure <ref type="figure" target="#fig_2">4b</ref>.</p><p>Following this idea, we use kernel density estimation (KDE) <ref type="bibr" target="#b12">[13]</ref> over the weighted set of poses of the previous frame {(x i t-1 , w * i t-1 )} to estimate p(x t |x t-1 ). The system can then recover from an erroneous estimation of x t-1 .</p><p>As shown in the experiments in Section VII, KDE leads to a more robust sequential estimation than Gaussian weighting in many cases. Furthermore, even though KDE increases the computational load with a factor corresponding to the number of nearest neighbors |{x t-1 }|, the computational load of computing the temporal consistency weights is negligible compared to, e.g., the database NN lookup. A drawback of KDE compared to Gaussian weighting is however the necessity of tuning more parameters, most importantly, the bandwidth of the kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Experiments</head><p>We first experimentally compare the two temporal consistency models detailed in Section VI, using synthetic sequences with hand pose ground truth. Then, the method is evaluated on real sequences featuring three different subjects and three object shapes. The sequences were captured at 10 frames/sec with a Point Grey Dragonfly camera with a resolution of 640√ó480 pixels. The method was implemented in C++ and runs at 10 frames/sec on one of the cores of a four core 2.66GHz Intel processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison of Temporal Consistency Models</head><p>The single hypothesis and multiple hypothesis temporal consistency models are first compared in terms of pose reconstruction accuracy. This quantitative analysis of our method is done with synthetic sequences, where the hand pose ground truth is available. To make experimental conditions as realistic as possible, none of the hand poses or the objects in the synthetic sequences are present in the database. Moreover, the poses are corrupted with a variable amounts of segmentation noise (see Fig. <ref type="figure" target="#fig_3">5</ref>), to simulate segmentation errors that occur with real sequences. The segmentation corruption is performed in the following way: The segmentation mask is first assigned as the full hand view (without noise). A fraction Œ± of the pixels in the segmentation mask are set to zero. The error is then propagated through an erosion followed by dilation. In each frame t, the error of the estimated hand pose xt relative to the ground truth pose x gt t is estimated as xtx gt t , the Euclidean distance in the pose space explained in Section V. Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the hand pose estimation of synthetic sequences 1 and 2 respectively, with segmentation corruption Œ± = 0.5%.</p><p>As shown in Fig. <ref type="figure">8</ref><ref type="figure">9</ref>, the multiple hypothesis temporal consistency model almost consistently gives a better accuracy. The effect is more visible with higher segmentation corruption levels Œ±. The reason for this is that the singleframe pose estimate p(x t |y t ) is more ambiguous for higher Œ±, which means that there is a higher uncertainty about which sample x i t is the best pose estimate at time t. With higher Œ± it is thus increasingly better to let all samples {(x i t-1 , w i t-1 )} influence the temporal model. It can als be seen that the pose estimation performance is largely unaffected by segmentation corruption levels up to Œ± = 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real Sequences with Subjects Not in Database</head><p>To show the performance of the method on real data, it was evaluated with sequences of the first author and two uninstructed persons (one man and one woman) grasping three different objects: A cup (with no equivalence in the database), a tennis ball (similar to a ball in the database), and a pair of pliers (with no equivalence in the database). The actions are not required to start from any specific pose. Naturally, the grasps in the sequences do not have exact correspondences in the database. Furthermore, the subjects' hands are of different sizes and shapes.</p><p>The multiple hypothesis temporal consistency modeling, shown above to be consistently better than the single hypothesis alternative, was used throughout the real image experiments. Fig. <ref type="figure" target="#fig_0">10</ref>, 11, and 12, show the result of pose estimation for the three subjects respectively.</p><p>One conclusion that can be drawn is that the method is robust to individual variations in hand shape and proportions. The hand model used to generate the database view is designed to be male. However, the method is successful in recovering the poses of the considerably more slender female hand (Fig. <ref type="bibr">"12)</ref>, as well as of the hand with a larger proportion of the lower arm uncovered (Fig. <ref type="figure" target="#fig_0">11</ref>); this affects skin segmentation, which in turn affects the HOG y t used for database lookup.</p><p>The results also show that the method generalizes over grasps and objects that are not exactly represented in the database. It should be taken into account that two of the subjects have no previous experience with the method or the database, and thus can be expected to grasp the objects in a natural way. The cup and the ball are well represented by other objects present in the database. However, the pliers pose a slightly larger challenge for the method. There are two possible reasons for this. Firstly, the layout of the pliers, with two separated legs, makes the occlusion of the hand appear differently than any example in the database. Secondly, the functionality of the pliers makes the subjects grasp it differently than other rod-like structures in the database. Fig. <ref type="figure" target="#fig_8">13</ref> shows the pose estimation of a sequence where large parts of the hand is occluded by the grasped object showing the method is robust to large object occlusion.</p><p>The pose estimation in Fig. <ref type="figure" target="#fig_9">14</ref> points to an avenue for improvement of the method. In our current temporal continuity approaches we assume that the most probable current pose is similar to the most probable previous pose. With this we are making an implicit assumption of static hand pose. However, this assumption is frequently violated; fast hand motions like the one shown at the end of the sequence in Figure <ref type="figure" target="#fig_9">14</ref> are not uncommon. With the assumption of being static in the temporal consistency model, all poses x i t selected by the NN sampling will be equally unlikely according to the temporal consistency model. Ambiguities in the HOG signature, e.g., between the front and back part of the hand, will then cause estimation errors as the one in the leftmost frame of Fig. <ref type="figure" target="#fig_9">14</ref>. This issue can be addressed by including a dynamic model of pose over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. Conclusions</head><p>A non-parametric method for 3D sequential pose estimation of hands in interaction with objects was presented. The contributions of this paper are the development of a method that not only handles severe occlusion from objects in the hand, but also takes the object shape into account in 3D hand reconstruction. In addition, the method is nonparametric and provides 3D hand reconstruction, operating in real-time, taking time continuity constraints into account.</p><p>Experiments showed that the method estimates hand pose in real time robustly against segmentation errors and large occlusion of the hand from objects. It was also shown that    Future work includes improving the motion model; currently, a static temporal model is implicitly assumed. This can be done in several ways, e.g., by learning lowdimensional models of hand motion from motion capture training data. Furthermore, we will enlarge the database to represent poses of differently shaped hands, grasping a wider range of objects under different illumination conditions. The approximate database lookup has a highly sub-linear time complexity, which allows for a significantly larger database with a moderate increase in computational load.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left) Original image and Right) Estimated pose.</figDesc><graphic coords="1,334.79,161.54,201.67,100.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Data representation.</figDesc><graphic coords="2,314.63,237.14,242.19,91.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two different methods for modeling temporal consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Artificial segmentation corruption Œ± added to synthetic sequences.</figDesc><graphic coords="4,60.65,53.96,65.59,109.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Pose error with increasing segmentation corruption in sequence 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Synthetic sequence 1. Top: original synthetic image. Middle: segmentation image with Œ± = 0.5%. Bottom: estimated pose. (The objects in the database are colored black for visibility here, but do not contribute to the HOGs.) Video at www.csc.kth.se/‚àºjrgn/VideosICRA2010/synthetic1.mp4</figDesc><graphic coords="5,53.87,194.36,53.31,100.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Real sequence 1 (male subject 1). Top: image with skin segmentation window highlighted. Bottom: estimated pose. (The objects in the database are colored black for visibility here, but do not contribute to the HOGs.) Video at www.csc.kth.se/‚àºjrgn/VideosICRA2010/real1.mp4</figDesc><graphic coords="6,53.87,149.66,53.31,56.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Real sequence 3 (female subject 3). Top: image with skin segmentation window highlighted. Bottom: estimated pose. (The objects in the database are colored black for visibility here, but do not contribute to the HOGs.) Video at www.csc.kth.se/‚àºjrgn/VideosICRA2010/real3.mp4</figDesc><graphic coords="6,77.09,341.12,63.36,66.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Real sequence 4 (male subject 1) with large hand occlusion. Top: image with skin segmentation window highlighted. Bottom: estimated pose. Video at www.csc.kth.se/‚àºjrgn/VideosICRA2010/real4.mp4</figDesc><graphic coords="6,143.45,341.12,63.42,66.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Real sequence 5 (male subject 1) with fast non-linear motion. Top: image with skin segmentation window highlighted. Bottom: estimated pose. Video at www.csc.kth.se/‚àºjrgn/VideosICRA2010/real5.mp4</figDesc><graphic coords="6,339.17,341.12,63.36,66.91" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2010" xml:id="foot_0"><p>IEEE International Conference on Robotics and Automation Anchorage Convention District May 3-8, 2010, Anchorage, Alaska, USA 978-1-4244-5040-4/10/$26.00 ¬©2010 IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by EU through the project PACO-PLUS, IST-FP6-IP-027657, and GRASP, IST-FP7-IP-215821 and Swedish Foundation for Strategic Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Filtering using a tree-based estimator</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D R</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thayananthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual hand tracking using non-parametric belief propagation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Generative Model Based Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-based hand tracking with texture, shading and self-occlusions</title>
		<author>
			<persName><forename type="first">M</forename><surname>De La Gorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating 3D hand pose from a cluttered image</title>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual recognition of grasps for human-to-robot mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellstr√∂m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragiƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular real-time 3D articulated hand pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellstr√∂m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragiƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking a hand manipulating an object</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comprehensive grasp taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pawlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmiedmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics, Science and Systems Conference: Workshop on Understanding the Human Hand for Advancing Robotic Manipulation</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real time tracking of multiple skin-colored objects with a possibly moving camera</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I A</forename><surname>Lourakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficiently matching sets of features with random histograms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic online tuning for fast gaussian summation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
