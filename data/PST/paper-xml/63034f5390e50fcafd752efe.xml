<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaV: A Meta-Verifier Approach to Task-Agnostic Model Fingerprinting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xudong</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country>China Yifan Yan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mi Zhang</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Min Yang</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MetaV: A Meta-Verifier Approach to Task-Agnostic Model Fingerprinting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539257</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Security and privacy â†’ Domain-specific security and privacy architectures;</term>
					<term>Computing methodologies â†’ Neural networks Fingerprinting</term>
					<term>Intellectual Property</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protecting the intellectual property (IP) of deep neural networks (DNN) becomes an urgent concern for IT corporations. For model piracy forensics, previous model fingerprinting schemes are commonly based on adversarial examples constructed for the owner's model as the fingerprint, and verify whether a suspect model is indeed pirated from the original model by matching the behavioral pattern on the fingerprint examples between one another. However, these methods heavily rely on the characteristics of classification tasks which inhibits their application to more general scenarios. To address this issue, we present MetaV, the first task-agnostic model fingerprinting framework which enables fingerprinting on a much wider range of DNNs independent from the downstream learning task, and exhibits strong robustness against a variety of ownership obfuscation techniques. Specifically, we generalize previous schemes into two critical design components in MetaV: the adaptive fingerprint and the meta-verifier, which are jointly optimized such that the meta-verifier learns to determine whether a suspect model is stolen based on the concatenated outputs of the suspect model on the adaptive fingerprint. As a key of being task-agnostic, the full process makes no assumption on the model internals in the ensemble only if they have the same input and output dimensions. Spanning classification, regression and generative modeling, extensive experimental results validate the substantially improved performance of MetaV over the state-of-the-art fingerprinting schemes and demonstrate the enhanced generality of MetaV for providing task-agnostic fingerprinting. For example, on fingerprinting ResNet-18 trained for skin cancer diagnosis, MetaV achieves simultaneously 100% true positives and 100% true negatives on a diverse test set of 70 suspect models, achieving an about 220% relative improvement in ARUC over the optimal baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the past decades, deep learning finds a wide application in a variety of mission-critical scenarios in the real world, including autonomous driving <ref type="bibr" target="#b5">[6]</ref>, finance <ref type="bibr" target="#b14">[15]</ref>, intelligent healthcare <ref type="bibr" target="#b10">[11]</ref>, and many more. In the ever-evolving trend of applying deep learning in IT industry, increasingly more high-ended computing power and massive amounts of well-annotated data are devoted to the construction of deep neural networks (DNN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>, which are later deployed as prediction APIs, i.e., Machine-Learning-as-a-Service (MLaaS), to provide intelligent service for profiting. Considering the substantial training costs, many IT corporations as the model owners become aware of the importance of protecting the confidentiality of those well-trained DNN, as an inseparable part of their intelligent property (IP). Threateningly, even with careful access control, an attacker can still pirate the working DNN behind an online intelligent service by conducting system <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> or algorithmic attacks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Orthogonal to the advances in protecting DNNs against model privacy <ref type="bibr" target="#b19">[20]</ref>, model watermarking and model fingerprinting are two fast-developing techniques for model piracy forensics. Applying model watermarking, the model owner embeds a secret into his/her owned model (i.e., the target model). Once the ownership of a DNN model is in doubt (i.e., the suspect model), a trusted third party verifies the existence of the exclusively known secret in the suspect model to determine the actual ownership. From Uchida et al. <ref type="bibr" target="#b35">[36]</ref>, previous works devise different types of secrets (e.g., a specific function or a specific parameter pattern) into various parts of a DNN, which we briefly survey in Section 2. However, because model watermarking unavoidably modifies the original parameters of a well-trained DNN for secret embedding, the otherwise optimal accuracy would be slightly degraded, causing an unacceptable trade-off for mission-critical tasks in healthcare and traffic <ref type="bibr" target="#b4">[5]</ref>.</p><p>Complemental to model watermarking, model fingerprinting is a passive forensic technique against model piracy, which in general tests whether certain fingerprint of the target model are present in a suspect model, which would help collect essential evidence of model piracy in the wild before filing a lawsuit. As a key difference from model watermarking, the fingerprint is innate but not embedded to the target model. In other words, no modifications on the target model are conducted during fingerprinting, which provably preserves the normal utility of well-trained DNNs. Thanks to this desirable characteristic, model fingerprinting arises as a booming direction in model protection from the last year, which attracts increasing research efforts from different backgrounds <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Following the fingerprinting framework in Cao et al. <ref type="bibr" target="#b4">[5]</ref>, previous schemes mostly focus on fingerprinting classifiers by constructing a special set of adversarial examples <ref type="bibr" target="#b32">[33]</ref>, i.e., normal examples added with human-imperceptible perturbations which cause misclassification of the target classifier, as the fingerprint, and verifying whether a suspect model is indeed stolen from the original model by matching the behavioral pattern, e.g., the predicted labels <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref> or probability vector similarity <ref type="bibr" target="#b23">[24]</ref>, on the fingerprint examples. Despite their pioneering contributions to model IP protection, existing schemes are however limited to fingerprinting DNNs for other important downstream tasks except for classification, mainly because they commonly rely on concepts like adversarial examples and classification boundary which have no direct counterparts in other typical learning tasks such as regression and generative modeling. With recent years witnessing the fast trend of distribution, deployment and redistribution of DNNs in nowadays deep learning ecosystem, how to conduct forensics on the improper reuse and illegal piracy for a more general set of DNNs poses an urgent open challenge to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Work</head><p>In this paper, we present a meta-verifier approach to task-agnostic model fingerprinting scheme (dubbed as MetaV ), which for the first time enables fingerprinting on a much wider range of DNNs independent from the downstream learning task, and by design implements the robustness against a variety of ownership obfuscation techniques possibly adopted by the adversary.</p><p>To realize task-agnostic model fingerprinting, we generalize the idea of using adversarial examples and the corresponding classification results for fingerprinting respectively into two critical design components in MetaV, i.e., the adaptive fingerprint and the meta-verifier. Concisely, the adaptive fingerprint is a set of trainable inputs to the suspect model, the concatenated outputs of the suspect model on which are classified by the meta-verifier to be True or False, where True implies the suspect model is indeed stolen (i.e., positive suspect model), and False implies the suspect model is independent from the original model (i.e., negative suspect model).</p><p>To implement the design principles above, the adaptive fingerprint and the meta-verifier are jointly optimized on an ensemble composed of the target model, the positive and the negative suspect models which are virtually generated by MetaV during the fingerprinting construction phase. Specifically, the positive suspect models in the ensemble are crafted by post-processing the target model with a number of popular ownership obfuscation techniques, e.g., compression <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, fine-tuning, partial retraining and distillation <ref type="bibr" target="#b15">[16]</ref>, while the irrelevant models are independently trained from scratch on similar learning tasks to the target model. As the full construction process of MetaV makes no assumption on the model internals or functions in the ensemble only if they have the same input and output dimensions, MetaV is therefore applicable independent of the downstream tasks for which the DNN is designed. Moreover, by permitting more types of obfuscation techniques in producing the stolen models for the model ensemble, our proposed MetaV is by construction robust against a diverse set of existing obfuscation techniques, with the potential to evolve along with future adversarial techniques.</p><p>In summary, we mainly make the following contributions:</p><p>â€¢ We present MetaV, the first task-agnostic fingerprinting framework with adaptive robustness against a variety of ownership obfuscation techniques, to substantially advance the cutting-edge model fingerprinting capability to a much broader set of DNNs for arbitrary downstream tasks.  <ref type="bibr" target="#b24">[25]</ref> and Zhao et al. <ref type="bibr" target="#b44">[45]</ref> independently propose to extract so-called conferrable adversarial examples from the ensemble of the target classifier and a set of locally trained suspect classifiers. These conferrable adversarial examples, which transfer much better to the positive suspect models than to negative ones, can be regarded as a unique link between the target model and the positive suspect models. Besides, Wang and Chang <ref type="bibr" target="#b36">[37]</ref> utilize the geometry characteristics inherited in the DeepFool algorithm to construct adversarial examples as the fingerprint <ref type="bibr" target="#b36">[37]</ref>, while Li et al. <ref type="bibr" target="#b23">[24]</ref> leverage the similarity between models in terms of the probability vectors on test inputs for piracy detection <ref type="bibr" target="#b23">[24]</ref>. More detailed surveys can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In this work, our proposed MetaV generalizes the aforementioned fingerprinting techniques to abstract the usage of adversarial examples and the classification results into the adaptive fingerprint and the trainable meta-verifiers, which is applicable to arbitrary DNN models in a task-agnostic way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Watermarking</head><p>Orthogonal to model fingerprinting, model watermarking embeds a watermark into the trained model before it is released, which potentially sacrifices the utility of the model. Previous works on model watermarking explore various types of watermarks such as secret bit strings <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>, generated serial numbers <ref type="bibr" target="#b39">[40]</ref> and unrelated or slightly modified sample sets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref>. These identifying codes are then encoded secretly into the least significant bit of the weight <ref type="bibr" target="#b35">[36]</ref>, the distribution of outputs at the intermediate <ref type="bibr" target="#b30">[31]</ref> or the full layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> 3 SECURITY SETTINGS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backgrounds &amp; Notions</head><p>Model fingerprinting is a multi-party security game among a model owner, a verifier and an attacker. Initially, a model owner devotes computing power and well-curated training data to build its own DNN ğ¹ : X â†’ Y for a certain downstream task, crowning the obtained model ğ¹ as an inseparable part of the model owner's IP. Following the nomenclature in Cao et al. <ref type="bibr" target="#b4">[5]</ref>, we refer to the model ğ¹ as the target model. In nowadays deep learning ecosystem, the model owner can deploy the target model at a third-party platform like Amazon AWS as a prediction API to gain monetary profits. However, the profits may also serve as incentives on the potential attacker to conduct model piracy via, e.g., software/hardware vulnerabilities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref>, social engineering and algorithmic attacks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref>. This essentially infringes the IP of the model owner.</p><p>As a rescue, the model owner can delegate a verifier, usually played by a trusted third party or the model owner him/herself, to provide model fingerprinting service for model piracy forensics. In general, model fingerprinting determines whether a suspect model F is pirated from the target model ğ¹ in the two stages:</p><p>â€¢ Fingerprint Construction. At the construction stage, a certain type of model fingerprint encoding the essential characteristics of the target model ğ¹ is constructed. â€¢ Fingerprint Verification. At the verification stage, the suspect model is attested via the prediction API (i.e., black-box access) to determine whether and with what confidence (i.e., matching rate) the fingerprint is also present in the suspect model F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Threat Model</head><p>We mainly consider the following threat model in this paper.</p><p>â€¢ Attacker's Capability. We assume the attacker would apply a variety of model post-processing techniques to obfuscate the ownership of the stolen model (detailed in the subsequent part) after he/she successfully steals the target model from an online prediction API. Such an obfuscated model is called a positive suspect model. Correspondingly, a suspect model independently trained by another honest model owner is called a negative suspect model. The attacker is assumed to answer any queries to his/her provided prediction API, as more queries served by the API bring more monetary profits. â€¢ Verifier's Capability. Following, e.g., Cao et al. <ref type="bibr" target="#b4">[5]</ref>, we assume the verifier has a white-box access to the target model while a black-box access to the suspect models via their prediction APIs.</p><p>To relax their assumptions, we do not assume the internal architecture or the type of downstream tasks of the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Techniques</head><p>Integrating existing ownership obfuscation techniques studied in previous works, we mainly cover the following classes of adversarial techniques which the attacker is likely to adopt.</p><p>â€¢ Model Compression: Compression-based obfuscation adopts weight (filter) pruning <ref type="bibr" target="#b22">[23]</ref> to remove a certain ratio of small weights (filters) in a DNN, which would largely preserve the utility of the obfuscated model on the learning task <ref type="bibr" target="#b12">[13]</ref> and inhibits heuristic-based fingerprinting from verifying the model owernship based on parameter comparison <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Fine-Tuning &amp; Partial Retraining:</head><p>To obfuscate the behavioral pattern of a DNN, attackers may resume the training of the stolen model on public data collected from a similar domain of the training data. Specifically, to fine-tune the last ğ¾ layers of a trained DNN, the parameters of the last ğ¾ layers are further updated according to the learning objective with the other layers fixed. In comparison, during the partial retraining, the parameters of the last ğ¾ layers are first randomly initialized before the training is resumed. Due to the non-convexity of deep learning <ref type="bibr" target="#b7">[8]</ref>, both the fine-tuned and partially-retrained models may fall into a different local optimum, preserve the original utility, but exhibit divergent prediction behaviors <ref type="bibr" target="#b36">[37]</ref>. â€¢ Model Distillation: Distillation-based obfuscation adopts the knowledge distillation strategies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> by viewing the stolen model or the corresponding prediction API as the teacher model, and a DNN of a different architecture as the student model <ref type="bibr" target="#b23">[24]</ref>. Via distillation, the learned knowledge in the target model is inherited by the student model, which exacerbates the obfuscation of model ownership due to the transformed model architecture and the correspondingly altered predictive behaviors <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR METHODOLOGY FOR TASK-AGNOSTIC FINGERPRINTING 4.1 Overview of MetaV</head><p>As a generalization of previous fingerprinting schemes, our proposed MetaV abstracts the usage of adversarial examples and the we then jointly optimize the adaptive fingerprint X ğ¹ and the meta-verifier V to satisfy: For both the target model or any suspect model in M + , the meta-verifier ğ‘‰ is trained to predict True, i.e., ğ‘ + &gt; ğ‘ âˆ’ , on the concatenated outputs of the model on examples in X ğ¹ , and vice versa for any suspect model in M âˆ’ . At the end of this stage, we obtain optimized adaptive fingerprint and the corresponding meta-verifier, i.e., X * ğ¹ and V * respectively. We call (X * ğ¹ , V * ) a fingerprinting pair. â€¢ Stage 3. (Fingerprint Verification in MetaV) Finally, with the optimized fingerprinting pair (X * ğ¹ , V * ), we verify whether and with what matching rate a suspect model F is a stolen version or an independently trained one by querying the prediction API with the fingerprint examples in X ğ¹ . The received prediction results are then concatenated and input to the meta-verifier to predict</p><formula xml:id="formula_0">( pâˆ’ , p+ ) = V ( F (ğ‘¥ 1 ğ¹ ) âŠ• . . . âŠ• F (ğ‘¥ ğ‘ ğ¹ )).</formula><p>When p+ is larger than a predefined threshold ğœŒ, the verification process outputs True to claim possible model piracy behind the tested prediction API, or other the process outputs False to assert the fidelity of the suspect model.</p><p>In the following sections, we elaborate on the detailed methodology for the first two stages of MetaV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Ensemble Preparation</head><p>To construct the adaptive fingerprint and the meta-verifier with simultaneously high robustness and uniqueness, MetaV is first required to collaborate with the model owner to prepare a diverse set of positive and negative suspect models. Intuitively, with a more representative set of positive suspect models, the obtained fingerprinting pair would stay robust against a wider range of ownership obfuscation techniques, resulting in higher true positives. Alternatively, more representative negative suspect models would lower the probability of the learned fingerprinting pair to be present in other irrelevant models, which therefore reduces the true negatives of MetaV. Specifically, the positive and negative suspect models are constructed as follows.</p><p>â€¢ Prepare Positive Suspect Models. We derive a representative set of positive suspect models by randomly applying one or more common ownership obfuscation techniques mentioned in Section 3 to the target model ğ¹ . The applied obfuscation techniques are recommended to cover a wider range of hyperparameter configurations for better robustness. For example, we apply weight and filter pruning to the target model with different pruning ratios. As a notation, we denote the full set of obfuscation techniques for preparing the positive suspect models as T . Therefore, we have M + := {ğ‘‡ â€¢ ğ¹ |ğ‘‡ âˆˆ T }. Section 5 presents the detailed composition of T in the evaluation settings part. â€¢ Prepare Negative Suspect Models. We recommend three complemental sources to collect a diverse set of negative suspect models for the fingerprint construction stage of MetaV. First, MetaV may request the model owner to train a moderate number of relatively small-scale DNNs on the same training dataset of the target model. For IP protection of the target model, it would be reasonable for the model owner to devote additional computing power to collaborate with a trusted third-party verifier. Second, MetaV can also download a number of pretrained models from online sources (e.g., PyTorch Hub) and fine-tune these models on the domain-relevant public data to serve as the negative suspect models. Moreover, MetaV may consider incorporate a proportion of irrelevant publicly available models into the set of negative suspect models to further enhance the uniqueness of the obtained fingerprinting pair. We denote the prepared negative suspect models as M âˆ’ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fingerprint Construction in MetaV</head><p>In this part, we detail the learning objective and the optimization algorithm for training the fingerprinting pair (X ğ¹ , V) on the model ensemble M âˆ’ âˆª {ğ¹ } âˆª M + prepared in the first stage. As Fig. <ref type="figure" target="#fig_0">1(b)</ref> shows, the learning objective of MetaV is viewed as a binary classification problem. To formulate, we introduce an additional label ğ‘  for each model, which takes value in {+, âˆ’} (literally, positive and negative respectively). Specifically, we label an arbitrary model ğ‘€ + âˆˆ M + âˆª {ğ¹ } as + and an arbitrary model ğ‘€ âˆ’ âˆˆ M âˆ’ as âˆ’.</p><p>To supervise the adaptive fingerprint and the meta-verifier with the labels, we solve the learning objective:</p><formula xml:id="formula_1">arg max X ğ¹ ,V log ğ‘ + (ğ¹ ) + 1 |M + | âˆ‘ ğ‘€ + âˆˆM + log ğ‘ + (ğ‘€ + ) + 1 |M âˆ’ | âˆ‘ ğ‘€ âˆ’ âˆˆM âˆ’ log ğ‘ âˆ’ (ğ‘€ âˆ’ ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">(ğ‘ âˆ’ (ğ‘€), ğ‘ + (ğ‘€)) = V (ğ‘€ (ğ‘¥ 1 ğ¹ ) âŠ• . . . âŠ• ğ‘€ (ğ‘¥ ğ‘ ğ¹ )</formula><p>), the prediction from the meta-verifier on the concatenated outputs of a model under test on the adaptive fingerprint. Intuitively, the learning objective above encourages the meta-verifier to output a ğ‘ + higher than ğ‘ âˆ’ when the model under test is a positive suspect model or the target model, and vice versa for a negative suspect model.</p><p>As the learning objective above is fully derivative w.r.t. the adaptive fingerprint X ğ¹ and the parameters of the meta-verifier, we leverage off-the-shelf non-convex optimizers (e.g., Adam <ref type="bibr" target="#b20">[21]</ref>) for gradient-based optimization. However, we notice it is resourceconsuming to conduct back-propagation over the whole model ensembles in each optimization step. As an alternative, we reformulate the batched learning objective in (1) as a stochastic objective with randomness in a tuple of (ğ‘€ âˆ’ , ğ¹, ğ‘€ + ) uniformly sampled from M âˆ’ , {ğ¹ } and M + in each iteration. Besides, we adopt the reparametrization trick in Carlini and Wagner <ref type="bibr" target="#b6">[7]</ref> to constrain the adaptive fingerprint in the problem space X. Taking X := [âˆ’1, 1] ğ‘‘ in , a common case in computer vision for example, Algorithm 1 presents the details of the optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION SETTINGS 5.1 Scenarios and Datasets</head><p>Table <ref type="table">2</ref> provides an overview on the three scenarios, i.e., skin cancer diagnosis (Yang et al. <ref type="bibr" target="#b41">[42]</ref>,classification), warfarin dose prediction (Whirl-Carrillo et al. <ref type="bibr" target="#b37">[38]</ref>, regression), and fashion generation (Xiao et al. <ref type="bibr" target="#b38">[39]</ref>, generative modeling), covered in the evaluation sections. The information of the datasets are concisely introduced below.</p><p>â€¢ Skin Cancer Diagnosis (abbrev. Skin). The first scenario covers the usage of deep convolutional neural network (CNN) for skin cancer diagnosis. According to <ref type="bibr" target="#b41">[42]</ref>, we train a ResNet-18 <ref type="bibr" target="#b13">[14]</ref> as the target model on DermaMNIST <ref type="bibr" target="#b41">[42]</ref>, which consists of 10005 multi-source dermatoscopic images of common pigmented skin lesions imaging dataset. The input size is originally 3 Ã— 28 Ã— 28, which is upsampled to be 3 Ã— 224 Ã— 224 to fit the input shape of a standard ResNet-18 architecture implemented in torchvision <ref type="bibr" target="#b0">[1]</ref>. The task is a 7-class classification task. â€¢ Warfarin Dose Prediction (abbrev. Warfarin). The second scenario covers the usage of FCN for warfarin dose prediction, which is a safety-critical regression task that helps predict the proper individualised warfarin dosing according to the demographic and physiological record of the patients (e.g., weight, age and genetics). We use the International Warfarin Pharmacogenetics Consortium (IWPC) dataset <ref type="bibr" target="#b37">[38]</ref>, which is a public dataset composed of 31-dimensional features of 6256 patients and is widely used for researches in automated warfarin dosing. According to Truda and Marais <ref type="bibr" target="#b34">[35]</ref>, we use a three-layer multi-layer perception (MLP) with ReLU as the target model, with its hidden layer composed of 100 neurons. As a notation, we denote the architecture as (31-100-1). The target model learns to predict the value of proper warfarin dosing, which is a non-negative real-valued scalar with its value in (0, 300.0]. â€¢ Fashion Generation (abbrev. Fashion) The final scenario covers the usage of FCN for generative modeling. We choose <ref type="bibr" target="#b38">[39]</ref>, which consists of 60000 images for articles of clothing of size 28 Ã— 28. We train a DCGAN-like architecture <ref type="bibr" target="#b27">[28]</ref> for generative modeling on this task. We solely view the generator as the target model, as a well-trained generator represents more the IP of the model owner because it can be directly used to generate realistic images without the aid of the discriminator. The detailed DCGAN architecture we use is demonstrated in Table <ref type="table" target="#tab_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fingerprinting Benchmarks</head><p>For each scenario, we construct a model benchmark composed of 140 positive/negative suspect models. We split the benchmark randomly by a ratio of 1 : 1 into two independent sets of suspect models for training and testing.</p><p>â€¢ Constructing Positive Suspect Models. Following Cao et al. <ref type="bibr" target="#b4">[5]</ref> and Lukas et al. <ref type="bibr" target="#b24">[25]</ref>, we apply a number of popular ownership obfuscation techniques with a variety of hyperparameter configurations on the target model to derive the positive suspect models:</p><p>(1) Compression: For weight pruning, we vary the ratio of pruned weights from 0.1 to 0.9 with a stride of 0.1. For filter pruning, we choose the ratio of pruned filters from 1/16 to 15/16 with a stride of 1/16. (2) Fine-Tuning &amp; Partial Retraining: We consider 4 types of obfuscation in this category, i.e., fine-tuning/retraining the last layer and fine-tuning/retraining all layers. For both types of retraining, the last one layer is first reset as a randomly initialized layer, after which the model is finetuned according to the configuration. We set the number of epochs for fine-tuning and retraining both as 10.</p><p>(3) Distillation: For each target model, we select 3-5 diverse models with different architectures as the student model. For the ResNet-18 classifier, we follow the classical distillation algorithm in Hinton et al. <ref type="bibr" target="#b15">[16]</ref> to prepare the student model. We do not consider other model distillation algorithms because most of them require the access to the internals of the target model (i.e., the teacher) for distillation, implausible for an attacker who pirates the model from the prediction API. For the multi-layer perception (MLP) as the regressor and the DCGAN <ref type="bibr" target="#b27">[28]</ref> as the generator, we implement the distillation algorithms in Clark et al. <ref type="bibr" target="#b8">[9]</ref> and Aguinaldo et al. <ref type="bibr" target="#b2">[3]</ref> respectively. For fine-tuning, partial retraining and distillation, we mutate the random seeds to produce multiple suspect models belonging to the corresponding category.</p><p>â€¢ Constructing Negative Suspect Models. To construct the negative suspect models, we use different random seeds to initialize models of different architectures. We then train the models from scratch respectively on the original training data, on the public data from a similar domain of the training set, and on other irrelevant dataset to obtain a diverse benchmark of negative suspect models. Table <ref type="table">3</ref> lists the composition of the suspect models for all the three scenarios. For convenience, we use the following abbreviation: fine-tuning the last layer (=FTLL), fine-tuning all layers (=FTAL), retraining the last layer (=RTAL), retraining all layers (=RTAL), weight-pruning (=WP), filter-pruning (=FP). For constructing distillation-based positive suspect models and independently trained negative suspect models, we implement 3-5 models of diverse architectures and incremental sizes for each of the three target models. For convenience, we index these models as S, M, L, XL, XLL. Specifically, these models are:</p><p>â€¢ Skin: S=SqueezeNet-1-0 <ref type="bibr" target="#b17">[18]</ref>; M=ResNet-18 (He et al. <ref type="bibr" target="#b13">[14]</ref>, the same as the target model); L=DenseNet-161 <ref type="bibr" target="#b16">[17]</ref>; XL=AlexNet <ref type="bibr" target="#b21">[22]</ref>; XXL=VGG-16 <ref type="bibr" target="#b31">[32]</ref>. â€¢ Warfarin: S=(31-100-1) (the same as the target model); M= <ref type="bibr">(31-</ref>100-100-1); L=(31-100-100-100-1). â€¢ Fashion: S=Architecture in Table <ref type="table" target="#tab_3">5</ref> with ğ‘  = 1; M=with ğ‘  = 2;</p><p>L=with ğ‘  = 3; XL=the same as the target model in Table <ref type="table" target="#tab_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>We cover 4 state-of-the-art fingerprinting schemes as the baselines for evaluating the effectiveness of MetaV under the classification settings. The baselines are respectively:</p><p>â€¢ IPGuard <ref type="bibr" target="#b4">[5]</ref>: IPGuard is one of the earliest fingerprinting schemes on classification models, which searches for a set of adversarial examples with a specified label near the decision boundary of the target model as the fingerprinting examples.</p><p>â€¢ ConferAE <ref type="bibr" target="#b24">[25]</ref>: ConferAE improves the design of IPGuard by further covering the distillation-based obfuscation techniques. Specifically, ConferAE constructs a set of adversarial examples on a prepared ensemble of positive/negative suspect models which may be implemented with different architecture from the target model. The generated adversarial examples are additionally required to be transferable from the target model to the positive suspect models but not to the negative ones.</p><p>â€¢ DeepFoolFP <ref type="bibr" target="#b36">[37]</ref>: DeepFoolFP literally leverages the DeepFool algorithm <ref type="bibr" target="#b25">[26]</ref> to generate adversarial examples as the model fingerprints, the motivation of which is to improve the efficiency of fingerprint construction. For verification, the above model fingerprinting schemes define the matching rate as the ratio of the fingerprinting examples which are correctly classified by the suspect model into the specified class.</p><p>â€¢ ModelDiff <ref type="bibr" target="#b23">[24]</ref>: ModelDiff is a very recent technique which is originally proposed to quantify the behavioral similarity between a pair of models. Specifically, the similarity is measured as the cosine similarity of two models' decision distance vector, each element of which is the distance of the logits between a clean test input and an adversarial example derived from the input. A suspect model is verified if its behavioral similarity with the target model is smaller than a threshold.</p><p>With no further specifications, the number of fingerprint examples, i.e., ğ‘ , is set as 100 for MetaV and the baselines by default. More details on the baselines are in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Metrics</head><p>Given a predefined threshold ğœ– âˆˆ (0, 1), MetaV and all the baseline methods recognize a suspect model as positive when the matching rate of fingerprint verification is higher than ğœŒ, or otherwise recognize the model as negative. In the evaluation, we following the evaluation protocol in <ref type="bibr" target="#b4">[5]</ref> which is composed of the metrics below:</p><p>(1) Robustness/Uniqueness (ğ‘…(ğœŒ)/ ğ‘ˆ (ğœŒ)): The robustness metric measures the proportion of positive suspect models also recognized as positive by the fingerprinting scheme, i.e., true positives.</p><p>(2) Uniqueness (ğ‘ˆ (ğœŒ)): The uniqueness metric measures the proportion of negative suspect models also recognized as negative by the fingerprinting scheme, i.e.,true negatives. (3) Area under the Robustness-Uniqueness Curves (ARUC):</p><p>ARUC measures the area of the intersecion region under the robustness and uniqueness when the threshold varies in (0, 1), i.e., âˆ« 1 0 min{ğ‘…(ğœŒ), ğ‘ˆ (ğœŒ)}ğ‘‘ğœŒ. A higher ARUC implies a more wider value range for the threshold to choose from to obtain simultaneously high robustness and uniqueness. ARUC is empirically calculated as the average min{ğ‘…(ğœŒ), ğ‘ˆ (ğœŒ)} on {0, 1/ğ¿, . . . , (ğ¿ âˆ’ 1)/ğ¿, 1} with ğ¿ = 100. For all the experiments, we run 5 repetitive experiments and report the average metric with the 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Other Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Hyperparameter Setups.</head><p>With no further specifications, we always set the number of fingerprint examples, i.e., ğ‘ , for MetaV and the baselines as 100 for fair comparisons. We set the learning rate in Algorithm 1 as 0.001 and the number of iteration as 1000. In all the three scenarios, we implement the meta-verifier V as a three-layer fully-connected neural network with the ReLU hidden layer size of 100.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS &amp; ANALYSIS 6.1 Comparison with Baselines</head><p>First, we compare the performance of MetaV with 4 state-of-theart model fingerprinting schemes specifically designed for classifiers. Fig. <ref type="figure" target="#fig_3">3</ref> reports the robustness (i.e., true positives) when the threshold ğœŒ is set to allow the uniqueness (i.e., true negatives) to reach 100% on the test set, along with the corresponding ARUC presented in Fig. <ref type="figure" target="#fig_2">2(a</ref>). As Fig. <ref type="figure" target="#fig_3">3</ref> shows, our proposed MetaV is the only method which simultaneously achieves 100% robustness and uniqueness in fingerprinting a stolen and adversarially obfuscated ResNet-18 classifier for skin cancer diagnosis. Besides, as we can see from Fig. <ref type="figure" target="#fig_2">2(a)</ref>, MetaV constructs the model fingerprint with the highest ARUC metric, i.e., 0.86 Â± 0.01, among all the tested fingerprinting schemes, which improves the optimal baseline IPGuard by 0.59 absolutely, i.e., a roughly 220% relative improvement. As we construct a more diverse benchmark of suspect models compared with previous works, the ARUC of IPGuard is not as high as the results reported in Cao et al. <ref type="bibr" target="#b4">[5]</ref>. Fig. <ref type="figure" target="#fig_2">2</ref>(b)-(f) show the robustness and uniqueness curves of each fingerprint schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Time Efficiency of MetaV</head><p>Next, we empirically study the learning behaviors and the time complexity of MetaV when constructing the fingerprint of a ResNet-18. As Fig. <ref type="figure" target="#fig_4">4</ref> shows, the ARUC and loss curves demonstrate the time efficiency of MetaV in fingerprint construction. In less than 200 seconds, MetaV stably constructs a fingerprinting pair which achieves an ARUC over 0.8. Besides, Table <ref type="table" target="#tab_1">1</ref> presents a tentative comparison on the time cost of each fingerprint scheme for constructing the corresponding model fingerprint to achieve the reported ARUC in Fig. <ref type="figure" target="#fig_2">2</ref> and for fingerprint verification in the same experimental environment detailed in the experimental setting part. As is shown, MetaV is similarly efficient compared with the state-of-the-art fingerprinting schemes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MetaV for Task-Independent Fingerprinting</head><p>Besides the substantial improvements in fingerprinting classifiers, more importantly, MetaV presents the first task-agnostic fingerprinting scheme which can be applied to more general application scenarios. To validate, we apply MetaV to fingerprint an MLP for regression (i.e., the Warfarin case) and a DCGAN for generative modeling (i.e., the Fashion case), which requires no modification on Algorithm 1, as MetaV is by design independent from either the internals or the functions of the target model. Fig. <ref type="figure" target="#fig_5">5</ref>(a)-(b) plot the curves of robustness and uniqueness of MetaV on Warfarin and Fashion when the threshold ğœŒ increases from 0 to 1, where the area of the shaded region is by definition the ARUC. As we can see, the robustness and the uniqueness remain 1 unless the threshold is very close to 0 or 1. This results in an over 0.98 ARUC for both the two scenarios which existing fingerprinting schemes can hardly handle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Number of Fingerprint Examples</head><p>We further study the influence of the number of fingerprint examples on the performance of MetaV and the baselines. Fig. <ref type="figure" target="#fig_7">6&amp;5(c)</ref> presents the ARUC curves when the number of fingerprint examples, i.e., ğ‘ , increases, on the classification and non-classification tasks respectively. As is shown, in all the three scenarios, the performance of MetaV increases stably when ğ‘ increases from 10 to 100. For example, when fingerprinting ResNet-18 on Skin, the ARUC of MetaV is about 1.2Ã— when ğ‘ is enlarged from 10 to 100. This is a desirable feature of MetaV as one would naturally expect a more accurate fingerprinting when more computing power is devoted to the construction of the model fingerprints. In comparison, the upward trend is unclear for all the baseline schemes. Similarly, enhanced performance is also observed on Warfarin and Fashion by about 3.2% and 17.0% respectively, which is noticeable considering the already high ARUC of MetaV when the number of fingerprint examples is 10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Size of Prepared Model Ensemble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present MetaV, the first task-agnostic model fingerprinting framework which: (a) substantially improves existing fingerprinting schemes on classification models in terms of fingerprint robustness and uniqueness, and, (b) more importantly, advances the capability of model piracy forensics to more general application scenarios. As it is a common challenge for any novel fingerprinting methods to be evaluated on large-scale datasets (e.g., for evalaution on ImageNet, one has to train over 100 suspect models on the dataset to construct the benchmark, which would incur over 50 days of computation on medium-end devices), we design our evaluation at the same scale of all our previous works by involving 32 Ã— 32 images only. It would be meaningful for future works to cooperate with the industry to evaluate MetaV on larger datasets. Meanwhile, although MetaV by design has no assumptions on the input, the architecture, or the output of the target model, considering the impossibility of exhausting all possible task types, we mainly choose the three representative scenarios in our paper for evaluation. Future works may consider deploy and evaluate the effectiveness of MetaV on other typical learning tasks such as feature extraction, information retrieval and ranking.     ğ‘Š ğ‘¡ +1 â† Opt ğ‘Š (â„“,ğ‘Š ğ‘¡ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Î˜ ğ‘¡ +1 â† Opt Î˜ (â„“, Î˜ ğ‘¡ ). 12: end for 13: For each ğ‘– âˆˆ [ğ‘ ], ğ‘¥ ğ‘–, * ğ¹ = tanh(ğ‘¤ ğ¿ ğ‘– ). 14: Î˜ * â† Î˜ ğ¿ . 15: Return: (X * ğ¹ , V (â€¢; Î˜ * ))</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The general pipeline of our proposed MetaV: (a) model ensemble preparation and (b) fingerprint construction.</figDesc><graphic url="image-1.png" coords="4,79.02,85.60,453.96,167.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5. 5 . 2</head><label>52</label><figDesc>Experimental Environment. All the defenses and experiments are implemented with PyTorch<ref type="bibr" target="#b26">[27]</ref>, an open-source software framework for numeric computation and deep learning. All our experiments are conducted on a Linux server running Ubuntu 16.04, one AMD Ryzen Threadripper 2990WX 32-core processor and 2 NVIDIA GTX RTX2080 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The ARUC of MetaV (Ours) and baselines on Skin. (b)-(f): Curves of robustness and uniqueness of MetaV on Warfarin and Fashion, where the ARUC is reported in the figure title.</figDesc><graphic url="image-2.png" coords="7,79.02,85.60,453.96,199.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The robustness when the threshold is set such that the uniqueness reaches 100%.</figDesc><graphic url="image-3.png" coords="7,60.43,449.02,226.98,110.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The learning curves of MetaV for constructing a fingerprinting pair of ResNet-18 (ğ‘ = 100), where the x-axis shows the wall-clock time.</figDesc><graphic url="image-4.png" coords="7,324.59,445.89,226.98,113.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a)-(b): Curves of robustness and uniqueness of MetaV on Warfarin and Fashion, with the ARUC reported in the figure title. The curves of ARUC (c) when the number of fingerprint examples increases and (d) when the model ensemble is enlarged on Warfarin and Fashion.</figDesc><graphic url="image-5.png" coords="8,53.80,85.60,504.40,90.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Finally</head><label></label><figDesc>, we provide quantitative results to analyze the impact of the model ensemble size on the performance of MetaV. We fix the number of fingerprint examples as 100 and randomly sample different ratios of positive/negative suspect models from the full model ensemble for training MetaV. Fig.7&amp;5(d)shows the ARUC curves on the three scenarios when the model ensemble size varies. As is shown, the ARUC of MetaV shows a steady upward trend when the model ensemble is enlarged, which conforms to our design principle that a more diverse set of crafted suspect models would help construct more unique and robust model fingerprints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The curves of ARUC when the number of fingerprint examples increases on Skin.</figDesc><graphic url="image-6.png" coords="8,324.59,428.72,226.98,111.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The curves of ARUC when the model ensemble is enlarged on Skin.</figDesc><graphic url="image-7.png" coords="9,60.43,85.60,226.98,112.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 : 4 : 7 :</head><label>147</label><figDesc>Input: A prepared model ensemble M âˆ’ âˆª {ğ¹ } âˆª M + , the number of adaptive fingerprints ğ‘ , the input/output dimension of the models ğ‘‘ in , ğ‘‘ out , the number of iterations ğ¿ and the learning rate ğœ†. 2: Output: The optimal fingerprinting pair (X * ğ¹ , V * ). 3: Initialize real-valued variables ğ‘Š 0 = (ğ‘¤ ğ‘– ) ğ‘ ğ‘–=1 . Initialize a meta-verifier V (â€¢; Î˜ 0 ) : Y ğ‘ Ã—ğ‘‘ out â†’ S 2 with parameters Î˜ 0 âŠ² We implement V as a fully-connected neural network with a softmax output. 5: Initialize Adam optimizers Opt ğ‘Š , Opt Î˜ of a learning rate ğœ†. 6: for ğ‘¡ in {0, . . . , ğ¿ âˆ’ 1} do Sample a tuple of models (ğ‘€ âˆ’ , ğ¹, ğ‘€ + ) from M âˆ’ , {ğ¹ } and M + respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8 :</head><label>8</label><figDesc>For each ğ‘– âˆˆ [ğ‘ ], calculate ğ‘¥ ğ‘– ğ¹ = tanh(ğ‘¤ ğ‘– ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>9 :â„“</head><label>9</label><figDesc>(ğ‘Š ğ‘¡ , Î˜ ğ‘¡ ) = log ğ‘ + (ğ‘€ + ) + log ğ‘ + (ğ¹ ) + log ğ‘ âˆ’ (ğ‘€ âˆ’ ) âŠ² (ğ‘ âˆ’ (ğ‘€), ğ‘ + (ğ‘€)) = V (ğ‘€ (ğ‘¥ 1 ğ¹ ) âŠ• . . . âŠ• ğ‘€ (ğ‘¥ ğ‘ ğ¹ ))10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Comparison of the time costs for fingerprint con- struction and verification (sec.).</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Construction Verification</cell></row><row><cell>MetaV</cell><cell>202+11</cell><cell>7.2+0.8</cell></row><row><cell>IPGuard</cell><cell>177+1</cell><cell>9.1+0.2</cell></row><row><cell>ModelDiff</cell><cell>123+1</cell><cell>13.6+0.5</cell></row><row><cell>DeepFoolFP</cell><cell>174+4</cell><cell>9.1+0.6</cell></row><row><cell>ConferAE</cell><cell>&gt;2860</cell><cell>10.0+0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 : The detailed architecture of DCGAN on Fashion, which is described by convention of PyTorch.</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>nn.ConvTranspose2d(100, 128, 4, 1, 0, bias=False)</cell></row><row><cell></cell><cell>nn.BatchNorm2d(128)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.ConvTranspose2d(128, 64, 3, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.BatchNorm2d(64)</cell></row><row><cell>Generator</cell><cell>nn.ReLU() nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.BatchNorm2d(32)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.ConvTranspose2d(32, 1, 4, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.Tanh()</cell></row><row><cell></cell><cell>nn.Conv2d(1, 32, 4, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.LeakyReLU(0.2)</cell></row><row><cell></cell><cell>nn.Conv2d(32, 64, 4, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.BatchNorm2d(64)</cell></row><row><cell>Discriminator</cell><cell>nn.LeakyReLU(0.2) nn.Conv2d(64, 128, 3, 2, 1, bias=False)</cell></row><row><cell></cell><cell>nn.BatchNorm2d(128)</cell></row><row><cell></cell><cell>nn.LeakyReLU(0.2)</cell></row><row><cell></cell><cell>nn.Conv2d(128, 1, 4, 1, 0, bias=False)</cell></row><row><cell></cell><cell>nn.Sigmoid()</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 : The detailed architecture of the student models for DCGAN on Fashion, which is described by convention of Py- Torch (ğ‘˜</head><label>5</label><figDesc>= 2 ğ‘  ). The algorithmic details of MetaV's fingerprint construction stage.</figDesc><table><row><cell></cell><cell>nn.Linear(100, 128)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.Linear(64ğ‘˜, 128ğ‘˜)</cell></row><row><cell>Generator</cell><cell>nn.ReLU() nn.Linear(128ğ‘˜, 256ğ‘˜)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.Linear(256ğ‘˜, 28x28)</cell></row><row><cell></cell><cell>nn.Sigmoid()</cell></row><row><cell></cell><cell>nn.Linear(28x28, 256ğ‘˜)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.Linear(256ğ‘˜, 128ğ‘˜)</cell></row><row><cell>Discriminator</cell><cell>nn.ReLU() nn.Linear(128ğ‘˜, 64ğ‘˜)</cell></row><row><cell></cell><cell>nn.ReLU()</cell></row><row><cell></cell><cell>nn.Linear(64ğ‘˜, 1)</cell></row><row><cell>Algorithm 1</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their insightful comments that helped improve the quality of the paper. This work was supported in part by the National Key Research and Development Program (2021YFB3101200), National Natural Science Foundation of China (61972099, U1736208, U1836210, U1836213, 62172104, 62172105, 61902374, 62102093, 62102091), Natural Science Foundation of Shanghai (19ZR1404800). Min Yang is a faculty of Shanghai Institute of Intelligent Electronics &amp; Systems, and Engineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, China. Mi Zhang and Min Yang are the corresponding authors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE TECHNICAL DETAILS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://pytorch.org/hub/" />
		<title level="m">PyTorch Hub</title>
				<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compressing GANs using Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Aguinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<idno>ArXiv abs/1902.00159</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Boenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Survey on Model Watermarking Neural Networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IPGuard: Protecting the Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiaoyu Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AsiaCCS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial Sensor Attack on LiDARbased Perception in Autonomous Driving</title>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards Evaluating the Robustness of Neural Networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Loss Surfaces of Multilayer Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>ChoromaÅ„ska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BAM! Born-Again Multi-Task Networks for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge Distillation: A Survey</title>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both Weights and Connections for Efficient Neural Network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Learning for Finance: Deep Portfolios. Econometric Modeling: Capital Markets -Portfolio Theory eJournal</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</title>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>ArXiv abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Hoyong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dohyun</forename><surname>Ryu</surname></persName>
		</author>
		<title level="m">Neural Network Stealing via Meltdown. ICOIN (2021)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="36" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PRADA: Protecting Against DNN Model Stealing Attacks</title>
		<author>
			<persName><forename type="first">Mika</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Szyller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EuroS&amp;P</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>ArXiv abs/1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pruning Filters for Efficient ConvNets</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ModelDiff: testing-based DNN similarity comparison for model reuse detection</title>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSTA</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Deep Neural Network Fingerprinting by Conferrable Adversarial Examples. ICLR</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adam Paszke</surname></persName>
		</author>
		<author>
			<persName><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>CoRR abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Protecting artificial intelligence IPs: a survey of watermarking and fingerprinting for machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palmieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stealing Machine Learning Models via Prediction APIs</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>TramÃ¨r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Warfarin dose estimation on multiple datasets with automated hyperparameter optimisation and a novel software framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Truda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Embedding Watermarks into Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Nagai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fingerprinting Deep Neural Networksa DeepFool Approach</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chip-Hong</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCAS</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Whirl-Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdonagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pharmacogenomics Knowledge for Personalized Medicine</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>Clinical Pharmacology &amp; Therapeutics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identity Bracelets&quot; for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures</title>
		<author>
			<persName><forename type="first">Mengjia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>USENIX Security</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples</title>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Protecting intellectual property of deep neural networks with watermarking</title>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongshu</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AsiaCCS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AFA: Adversarial fingerprinting authentication for deep neural networks</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
