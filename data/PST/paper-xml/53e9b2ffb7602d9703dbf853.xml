<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Discriminative Hidden Part Model for Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A 1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Discriminative Hidden Part Model for Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">635DE30B53D10871F58C9A3082829313</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random field (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a flexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs significantly better than directly applying hCRF on local patches alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recognizing human actions from videos is a task of obvious scientific and practical importance. In this paper, we consider the problem of recognizing human actions from video sequences on a frame-by-frame basis. We develop a discriminatively trained hidden part model to represent human actions. Our model is inspired by the hidden conditional random field (hCRF) model <ref type="bibr" target="#b15">[16]</ref> in object recognition.</p><p>In object recognition, there are three major representations: global template (rigid, e.g. <ref type="bibr" target="#b2">[3]</ref>, or deformable, e.g. <ref type="bibr" target="#b0">[1]</ref>), bag-of-words <ref type="bibr" target="#b17">[18]</ref>, and part-based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. All three representations have been shown to be effective on certain object recognition tasks. In particular, recent work <ref type="bibr" target="#b5">[6]</ref> has shown that part-based models outperform global templates and bag-of-words on challenging object recognition tasks.</p><p>A lot of the ideas used in object recognition can also be found in action recognition. For example, there is work <ref type="bibr" target="#b1">[2]</ref> that treats actions as space-time shapes and reduces the problem of action recognition to 3D object recognition. In action recognition, both global template <ref type="bibr" target="#b4">[5]</ref> and bag-of-words models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> have been shown to be effective on certain tasks. Although conceptually appealing and promising, the merit of part-based models has not yet been widely recognized in action recognition. The goal of this work is to address this gap.</p><p>Our work is partly inspired by a recent work in part-based event detection <ref type="bibr" target="#b9">[10]</ref>. In that work, template matching is combined with a pictorial structure model to detect and localize actions in crowded videos. One limitation of that work is that one has to manually specify the parts. Unlike Ke et al. <ref type="bibr" target="#b9">[10]</ref>, the parts in our model are initialized automatically. The major contribution of this work is that we combine the flexibility of part-based approaches with the global perspectives of large-scale template features in a discriminative model. We show that the combination of part-based and large-scale template features improves the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Model</head><p>The hidden conditional random field model <ref type="bibr" target="#b15">[16]</ref> was originally proposed for object recognition and has also been applied in sequence labeling <ref type="bibr" target="#b18">[19]</ref>. Objects are modeled as flexible constellations of parts conditioned on the appearances of local patches found by interest point operators.</p><p>The probability of the assignment of parts to local features is modeled by a conditional random field (CRF) <ref type="bibr" target="#b10">[11]</ref>. The advantage of the hCRF is that it relaxes the conditional independence assumption commonly used in the bag-of-words approaches of object recognition.</p><p>Similarly, local patches can also be used to distinguish actions. Figure. <ref type="bibr">4(a)</ref> shows some examples of human motion and the local patches that can be used to distinguish them. A bag-of-words representation can be used to model these local patches for action recognition. However, it suffers from the same restriction of conditional independence assumption that ignores the spatial structures of the parts. In this work, we use a variant of hCRF to model the constellation of these local patches in order to alleviate this restriction.</p><p>There are also some important differences between objects and actions. For objects, local patches could carry enough information for recognition. But for actions, we believe local patches are not sufficiently informative. In our approach, we modify the hCRF model to combine local patches and large-scale global features. The large-scale global features are represented by a root model that takes the frame as a whole. Another important difference with <ref type="bibr" target="#b15">[16]</ref> is that we use the learned root model to find discriminative local patches, rather than using a generic interest-point operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motion features</head><p>Our model is built upon the optical flow features in <ref type="bibr" target="#b4">[5]</ref>. This motion descriptor has been shown to perform reliably with noisy image sequences, and has been applied in various tasks, such as action classification, motion synthesis, etc.</p><p>To calculate the motion descriptor, we first need to track and stabilize the persons in a video sequence. Any reasonable tracking or human detection algorithm can be used, since the motion descriptor we use is very robust to jitters introduced by the tracking. Given a stabilized video sequence in which the person of interest appears in the center of the field of view, we compute the optical flow at each frame using the Lucas-Kanade <ref type="bibr" target="#b11">[12]</ref> algorithm. The optical flow vector field F is then split into two scalar fields F x and F y , corresponding to the x and y components of F . F x and F y are further half-wave rectified into four non-negative channels</p><formula xml:id="formula_0">F + x , F - x , F + y , F - y , so that F x = F + x -F - x</formula><p>and F y = F + y -F - y . These four non-negative channels are then blurred with a Gaussian kernel and normalized to obtain the final four channels F b +</p><p>x ,F b - x ,F b + y ,F b - y (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hidden conditional random field(hCRF)</head><p>Now we describe how we model a frame I in a video sequence. Let x be the motion feature of this frame, and y be the corresponding class label of this frame, ranging over a finite label alphabet Y. Our task is to learn a mapping from x to y. We assume each image I contains a set of salient patches {I 1 , I 2 , ..., I m }. we will describe how to find these salient patches in Sec. 3. Our training set consists of labeled images (x t , y t ) (as a notation convention, we use superscripts to index training images and subscripts to index patches) for t = 1, 2, ..., n, where y t âˆˆ Y and x t = (x t 1 , x t 2 ..., x t m ). x t i = x t (I t i ) is the feature vector extracted from the global motion feature x t at the location of the patch I t i . For each image I = {I 1 , I 2 , ..., I m }, we assume there exists a vector of hidden "part" variables h = {h 1 , h 2 , ..., h m }, where each h i takes values from a finite set H of possible parts. Intuitively, each h i assigns a part label to the patch I i , where i = 1, 2, ..., m. For example, for the action "waving-two-hands", these parts may be used to characterize the movement patterns of the left and right arms. The values of h are not observed in the training set, and will become the hidden variables of the model.</p><p>We assume there are certain constraints between some pairs of (h j , h k ). For example, in the case of "waving-two-hands", two patches h j and h k at the left hand might have the constraint that they tend to have the same part label, since both of them are characterized by the movement of the left hand. If we consider h i (i = 1, 2, ..., m) to be vertices in a graph G = (E, V ), the constraint between h j and h k is denoted by an edge (j, k) âˆˆ E. See Fig. <ref type="figure" target="#fig_1">2</ref> for an illustration of our model. Note that the graph structure can be different for different images. We will describe how to find the graph structure E in Sec. 3. Given the motion feature x of an image I, its corresponding class label y, and part labels h, a hidden conditional random field is defined as p(y, h|x; Î¸) = exp(Î¨(y,x,h;Î¸)) P Å·âˆˆY P Ä¥âˆˆH m exp(Î¨(Å·,x, Ä¥;Î¸)) , where Î¸ is the model parameter, and Î¨(y, h, x; Î¸) âˆˆ R is a potential function parameterized by Î¸. It follows that</p><formula xml:id="formula_1">p(y|x; Î¸) = hâˆˆH m p(y, h|x; Î¸) = hâˆˆH m exp(Î¨(y, h, x; Î¸)) Å·âˆˆY hâˆˆH m exp(Î¨(Å·, h, x; Î¸))<label>(1)</label></formula><p>We assume Î¨(y, h, x) is linear in the parameters Î¸ = {Î±, Î², Î³, Î·}:</p><formula xml:id="formula_2">Î¨(y, h, x; Î¸) = jâˆˆV Î± âŠ¤ â€¢Ï†(x j , h j )+ jâˆˆV Î² âŠ¤ â€¢Ï•(y, h j )+ (j,k)âˆˆE Î³ âŠ¤ â€¢Ïˆ(y, h j , h k )+Î· âŠ¤ â€¢Ï‰(y, x) (2)</formula><p>where Ï†(â€¢) and Ï•(â€¢) are feature vectors depending on unary h j 's, Ïˆ(â€¢) is a feature vector depending on pairs of (h j , h k ), Ï‰(â€¢) is a feature vector that does not depend on the values of hidden variables. The details of these feature vectors are described in the following.</p><p>Unary potential Î± âŠ¤ â€¢ Ï†(x j , h j ) : This potential function models the compatibility between x j and the part label h j , i.e., how likely the patch x j is labeled as part h j . It is parameterized as</p><formula xml:id="formula_3">Î± âŠ¤ â€¢ Ï†(x j , h j ) = câˆˆH Î± âŠ¤ c â€¢ 1 {hj =c} â€¢ [f a (x j ) f s (x j )]<label>(3)</label></formula><p>where we use [f a (x j ) f s (x j )] to denote the concatenation of two vectors f a (x j ) and f s (x j ). f a (x j ) is a feature vector describing the appearance of the patch x j . In our case, f a (x j ) is simply the concatenation of four channels of the motion features at patch x j , i.e., f a (</p><formula xml:id="formula_4">x j ) = [F b + x (x j ) F b - x (x j ) F b + y (x j ) F b - y (x j )]. f s (x j</formula><p>) is a feature vector describing the spatial location of the patch x j . We discretize the whole image locations into l bins, and f s (x j ) is a length l vector of all zeros with a single one for the bin occupied by x j . The parameter Î± c can be interpreted as the measurement of compatibility between feature vector [f a (x j ) f s (x j )] and the part label h j = c. The parameter Î± is simply the concatenation of Î± c for all c âˆˆ H.</p><p>Unary potential Î² âŠ¤ â€¢Ï•(y, h j ) : This potential function models the compatibility between class label y and part label h j , i.e., how likely an image with class label y contains a patch with part label h j . It is parameterized as</p><formula xml:id="formula_5">Î² âŠ¤ â€¢ Ï•(y, h j ) = aâˆˆY bâˆˆH Î² a,b â€¢ 1 {y=a} â€¢ 1 {hj =b} (4)</formula><p>where Î² a,b indicates the compatibility between y = a and h j = b.</p><p>Pairwise potential Î³ âŠ¤ â€¢ Ïˆ(y, h j , h k ): This pairwise potential function models the compatibility between class label y and a pair of part labels (h j , h k ), i.e., how likely an image with class label y contains a pair of patches with part labels h j and h k , where (j, k) âˆˆ E corresponds to an edge in the graph. It is parameterized as</p><formula xml:id="formula_6">Î³ âŠ¤ â€¢ Ïˆ(y, h j , h k ) = aâˆˆY bâˆˆH câˆˆH Î³ a,b,c â€¢ 1 {y=a} â€¢ 1 {hj =b} â€¢ 1 {h k =c}<label>(5)</label></formula><p>where Î³ a,b,c indicates the compatibility of y = a, h j = b and h k = c for the edge (j, k) âˆˆ E.</p><formula xml:id="formula_7">Root model Î· âŠ¤ â€¢ Ï‰(y, x):</formula><p>The root model is a potential function that models the compatibility of class label y and the large-scale global feature of the whole image. It is parameterized as</p><formula xml:id="formula_8">Î· âŠ¤ â€¢ Ï‰(y, x) = aâˆˆY Î· âŠ¤ a â€¢ 1 {y=a} â€¢ g(x)<label>(6)</label></formula><p>where g(x) is a feature vector describing the appearance of the whole image. In our case, g(x)</p><p>is the concatenation of all the four channels of the motion features in the image, i.e., g(</p><formula xml:id="formula_9">x) = [F b + x F b - x F b + y F b - y ]</formula><p>. Î· a can be interpreted as a root filter that measures the compatibility between the appearance of an image g(x) and a class label y = a. And Î· is simply the concatenation of Î· a for all a âˆˆ Y.</p><p>The parameterization of Î¨(y, h, x) is similar to that used in object recognition <ref type="bibr" target="#b15">[16]</ref>. But there are two important differences. First of all, our definition of the unary potential function Ï†(â€¢) encodes both appearance and spatial information of the patches. Secondly, we have a potential function Ï‰(â€¢) describing the large scale appearance of the whole image. The representation in Quattoni et al. <ref type="bibr" target="#b15">[16]</ref> only models local patches extracted from the image. This may be appropriate for object recognition. But for human action recognition, it is not clear that local patches can be sufficiently informative. We will demonstrate this experimentally in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning and Inference</head><p>The model parameters Î¸ are learned by maximizing the conditional log-likelihood on the training images:</p><formula xml:id="formula_10">Î¸ * = arg max Î¸ L(Î¸) = arg max Î¸ t log p(y t |x t ; Î¸) = arg max Î¸ t log h p(y t , h|x t ; Î¸)<label>(7)</label></formula><p>The objective function L(Î¸) in Quattoni et al. <ref type="bibr" target="#b15">[16]</ref> also has a regularization term -1 2Ïƒ 2 ||Î¸|| 2 . In our experiments, we find that the regularization does not seem to have much effect on the final results, so we will use the un-regularized version. Different from conditional random field (CRF) <ref type="bibr" target="#b10">[11]</ref>, the objective function L(Î¸) of hCRF is not concave, due to the hidden variables h. But we can still use gradient ascent to find Î¸ that is locally optimal. The gradient of the log-likelihood with respect to the t-th training image (x t , y t ) can be calculated as: </p><formula xml:id="formula_11">âˆ‚L t (Î¸) âˆ‚Î± = jâˆˆV E p(hj</formula><formula xml:id="formula_12">âˆ‚L t (Î¸) âˆ‚Î· = Ï‰(y t , x t ) -E p(y|x t ;Î¸) Ï‰(y, x t )<label>(8)</label></formula><p>Assuming the edges E form a tree, the expectations in Eq. 8 can be calculated in O(|Y||E||H| 2 ) time using belief propagation. Now we describe several details about how the above ideas are implemented.</p><p>Learning root filter Î·: Given a set of training images (x t , y t ), we firstly learn the root filter Î· by solving the following optimization problem:</p><formula xml:id="formula_13">Î· * = arg max Î· t log L(y t |x t ; Î·) = arg max Î· t log exp Î· âŠ¤ â€¢ Ï‰(y t , x t ) y exp (Î· âŠ¤ â€¢ Ï‰(y, x t ))<label>(9)</label></formula><p>In other words, Î· * is learned by only considering the feature vector Ï‰(â€¢). We then use Î· * as the starting point for Î· in the gradient ascent (Eq. 8). Other parameters Î±, Î², Î³ are initialized randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch initialization:</head><p>We use a simple heuristic similar to that used in <ref type="bibr" target="#b5">[6]</ref> to initialize ten salient patches on every training image from the root filter Î· * trained above. For each training image I with class label a, we apply the root filter Î· a on I, then select an rectangle region of size 5 Ã— 5 in the image that has the most positive energy. We zero out the weights in this region and repeat until ten patches are selected. Figure <ref type="figure" target="#fig_4">4(a)</ref> shows examples of the patches found in some images. The tree G = (V, E) is formed by running a minimum spanning tree algorithm over the ten patches.</p><p>Inference: During testing, we do not know the class label of a given test image, so we cannot use the patch initialization described above to initialize the patches, since we do not know which root filter to use. Instead, we run root filters from all the classes on a test image, then calculate the probabilities of all possible instantiations of patches under our learned model, and classify the image by picking the class label that gives the maximum of the these probabilities. In other words, for a testing image with motion descriptor x, we first obtain |Y| instances {x (1) , x (2) , ..., x (|Y|) }, where each x (k) is obtained by initializing the patches on x using the root filter Î· k . The final class label y * of x is obtained as y * = arg max y max{p(y|x (1) ; Î¸), p(y|x (2) ; Î¸), ..., p(y|x (|Y|) ; Î¸)} .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test our algorithm on two publicly available datasets that have been widely used in action recognition: Weizmann human action dataset <ref type="bibr" target="#b1">[2]</ref>, and KTH human motion dataset <ref type="bibr" target="#b16">[17]</ref>. Performance on these benchmarks is saturating -state-of-the-art approaches achieve near-perfect results. We show our method achieves results comparable to the state-of-the-art, and more importantly that our extended hCRF model significantly outperforms a direct application of the original hCRF model <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weizmann dataset:</head><p>The Weizmann human action dataset contains 83 video sequences showing nine different people, each performing nine different actions: running, walking, jumpingjack, jumping-forward-on-two-legs,jumping-in-place-on-two-legs, galloping-sideways, wavingtwo-hands, waving-one-hand, bending. We track and stabilize the figures using the background subtraction masks that come with this dataset.</p><p>We randomly choose videos of five subjects as training set, and the videos in the remaining four subjects as test set. We learn three hCRF models with different sizes of possible part labels, |H| = 6, 10, 20. Our model classifies every frame in a video sequence (i.e., per-frame classification), but 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.93 0.01 0.02 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.74 0.00 0.06 0.02 0.12 0.02 0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.72 0.06 0.17 0.00 0.00 0.00 0.01 0.07 0.00 0.02 0.73 0.17 0.00 0.00 0.00 0.00 0.01 0.00 0.05 0.06 0.88 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.75 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.  we can also obtain the class label for the whole video sequence by the majority voting of the labels of its frames (i.e., per-video classification). We show the confusion matrix with |H| = 10 for both per-frame and per-video classification in Fig. <ref type="figure" target="#fig_3">3</ref>. We compare our system to two baseline methods. The first baseline (root model) only uses the root filter Î· âŠ¤ â€¢ Ï‰(y, x), which is simply a discriminative version of Efros et al. <ref type="bibr" target="#b4">[5]</ref>. The second baseline (local hCRF) is a direct application of the original hCRF model <ref type="bibr" target="#b15">[16]</ref>. It is similar to our model, but without the root filter Î· âŠ¤ â€¢ Ï‰(y, x), i.e., local hCRF only uses the root filter to initialize the salient patches, but does not use it in the final model. The comparative results are shown in Table <ref type="table" target="#tab_1">1</ref>. Our approach significantly outperforms the two baseline methods. We also compare our results(with |H| = 10) with previous work in Table <ref type="table" target="#tab_3">2</ref>. Note <ref type="bibr" target="#b1">[2]</ref> classifies space-time cubes. It is not clear how it can be compared with other methods that classify frames or videos. Our result is significantly better than <ref type="bibr" target="#b12">[13]</ref>, and comparable to <ref type="bibr" target="#b7">[8]</ref>. Although we accept the fact that the comparison is not completely fair, since <ref type="bibr" target="#b12">[13]</ref> does not use any tracking or background subtraction.</p><p>We visualize the learned parts in Fig. <ref type="figure" target="#fig_4">4</ref>(a). Each patch is represented by a color that corresponds to the most likely part label of that patch. We also visualize the root filters applied on these images in Fig. <ref type="figure" target="#fig_4">4(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KTH dataset:</head><p>The KTH human motion dataset contains six types of human actions (walking, jogging, running, boxing, hand waving and hand clapping) performed several times by 25 subjects in four different scenarios: outdoors, outdoors with scale variation, outdoors with different clothes and indoors. We first run an automatic preprocessing step to track and stabilize the video sequences, so that all the figures appear in the center of the field of view.</p><p>We     shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The comparison with the two baseline algorithms is summarized in Table <ref type="table" target="#tab_6">3</ref>. Again, our approach outperforms the two baselines systems.</p><p>The comparison with other approaches is summarized in Table <ref type="table">4</ref>. We should emphasize that we do not attempt a direct comparison, since different methods listed in Table <ref type="table">4</ref> have all sorts of variations in their experiments (e.g., different split of training/test data, whether temporal smoothing is used, whether per-frame classification can be performed, whether tracking/background subtraction is used, whether the whole dataset is used etc.), which make it impossible to directly compare them. We provide the results only to show that our approach is comparable to the state-of-the-art.   <ref type="bibr" target="#b7">[8]</ref> 91.70 Nowozin et al. <ref type="bibr" target="#b14">[15]</ref> 87.04 Niebles et al. <ref type="bibr" target="#b13">[14]</ref> 81.50 DollÃ¡r et al. <ref type="bibr" target="#b3">[4]</ref> 81.17 Schuldt et al. <ref type="bibr" target="#b16">[17]</ref> 71.72 Ke et al. <ref type="bibr" target="#b8">[9]</ref> 62.96</p><p>Table <ref type="table">4</ref>: Comparison of per-video classification accuracy with previous approaches on KTH dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a discriminatively learned part model for human action recognition. Unlike previous work <ref type="bibr" target="#b9">[10]</ref>, our model does not require manual specification of the parts. Instead, the parts are initialized by a learned root filter. Our model combines both large-scale features used in global templates and local patch features used in bag-of-words models. Our experimental results show that our model is quite effective in recognizing actions. The results are comparable to the state-of-theart approaches. In particular, we show that the combination of large-scale features and local patch features performs significantly better than using either of them alone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Construction of the motion descriptor. (a) original image; (b) optical flow; (c) x and y components of optical flow vectors F x , F y ; (d) half-wave rectification of x and y components to obtain 4 separate channels F + x , F - x , F + y , F - y ; (e) final blurry motion descriptors F b + x , F b - x , F b + y , F b - y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the model. Each circle corresponds to a variable, and each square corresponds to a factor in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Confusion matrices of classification results on Weizmann dataset. Horizontal rows are ground truths, and vertical columns are predictions.method root model local hCRF our approach |H| = 6 |H| = 10 |H| = 20 |H| = 6 |H| = 10 |H| = 20 per-frame 0.7470 0.5722 0.6656 0.6383 0.8682 0.9029 0.8557 per-video 0.8889 0.5556 0.6944 0.6111 0.9167 0.9722 0.9444</figDesc><graphic coords="6,171.45,82.88,120.75,104.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Visualization of the learned parts. Patches are colored according to their most likely part labels. Each color corresponds to a part label. Some interesting observations can be made.For example, the part label represented by red seems to correspond to the "moving down" patterns mostly observed in the "bending" action. The part label represented by green seems to correspond to the motion patterns distinctive of "hand-waving" actions; (b) Visualization of root filters applied on these images. For each image with class label c, we apply the root filter Î· c . The results show the filter responses aggregated over four motion descriptor channels. Bright areas correspond to positive energies, i.e., areas that are discriminative for this class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Confusion matrices of classification results on KTH dataset. Horizontal rows are ground truths, and vertical columns are predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>|y t ,x t ;Î¸) Ï†(x t j , h j ) -E p(hj ,y|x t ;Î¸) Ï†(x t j , h j )</figDesc><table><row><cell>âˆ‚L t (Î¸) âˆ‚Î²</cell><cell>=</cell><cell>jâˆˆV</cell><cell cols="2">E p(hj |y t ,x t ;Î¸) Ï•(h j , y t ) -E p(hj ,y|x t ;Î¸) Ï•(h j , y)</cell></row><row><cell>âˆ‚L t (Î¸) âˆ‚Î³</cell><cell>=</cell><cell cols="2">(j,k)âˆˆE</cell><cell>E p(hj ,h</cell></row></table><note><p>k |y t ,x t ;Î¸) Ïˆ(y t , h j , h k ) -E p(hj ,h k ,y|x t ;Î¸) Ïˆ(y, h j , h k )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of two baseline systems with our approach on Weizmann dataset.</figDesc><table><row><cell></cell><cell>.7470</cell><cell>0.5722</cell><cell>0.6656</cell><cell>0.6383</cell><cell>0.8682</cell><cell>0.9029</cell><cell>0.8557</cell></row><row><cell>per-video</cell><cell>0.8889</cell><cell>0.5556</cell><cell>0.6944</cell><cell>0.6111</cell><cell>0.9167</cell><cell>0.9722</cell><cell>0.9444</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>split the videos roughly equally into training/test sets and randomly sample 10 frames from each video. The confusion matrices (with |H| = 10) for both per-frame and per-video classification are</figDesc><table><row><cell></cell><cell cols="3">per-frame(%) per-video(%) per-cube(%)</cell></row><row><cell>Our method</cell><cell>90.3</cell><cell>97.2</cell><cell>N/A</cell></row><row><cell>Jhuang et al. [8]</cell><cell>N/A</cell><cell>98.8</cell><cell>N/A</cell></row><row><cell>Niebles &amp; Fei-Fei [13]</cell><cell>55</cell><cell>72.8</cell><cell>N/A</cell></row><row><cell>Blank et al. [2]</cell><cell>N/A</cell><cell>N/A</cell><cell>99.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of classification accuracy with previous work on the Weizmann dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>|H| = 10 |H| = 20 |H| = 6 |H| = 10 |H| = 20</figDesc><table><row><cell cols="3">method |H| = 6 per-frame root model 0.5377 0.4749</cell><cell>local hCRF 0.4452</cell><cell>0.4282</cell><cell>0.6633</cell><cell>our approach 0.6698</cell><cell>0.6444</cell></row><row><cell>per-video</cell><cell>0.7339</cell><cell>0.5607</cell><cell>0.5814</cell><cell>0.5504</cell><cell>0.7855</cell><cell>0.8760</cell><cell>0.7512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of two baseline systems with our approach on KTH dataset.</figDesc><table><row><cell>methods</cell><cell>accuracy(%)</cell></row><row><cell>Our method</cell><cell>87.60</cell></row><row><cell>Jhuang et al.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using low distortion correspondence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histogram of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2003-01">January 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hierarchical model of shape and appearance for human action classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatialtemporal words</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative subsequence mining for action classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conditional random fields for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for gesture recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
