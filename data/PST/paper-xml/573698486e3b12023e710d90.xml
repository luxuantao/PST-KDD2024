<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Streaming Feature Selection in Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong.li@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>hu@cse.tamu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country>USA Yahoo</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huan.liu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Labs</orgName>
								<orgName type="institution">Yahoo! Inc</orgName>
								<address>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Streaming Feature Selection in Social Media</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D56C03A0568C3072FE2B9C34466CAED</idno>
					<idno type="DOI">10.1145/2806416.2806501</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.8 [DATABASE MANAGEMENT]: Database Applications-Data Mining Unsupervised Feature Selection</term>
					<term>Streaming Features</term>
					<term>Social Media Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The explosive growth of social media sites brings about massive amounts of high-dimensional data. Feature selection is effective in preparing high-dimensional data for data analytics. The characteristics of social media present novel challenges for feature selection. First, social media data is not fully structured and its features are usually not predefined, but are generated dynamically. For example, in Twitter, slang words (features) are created everyday and quickly become popular within a short period of time. It is hard to directly apply traditional batch-mode feature selection methods to find such features. Second, given the nature of social media, label information is costly to collect. It exacerbates the problem of feature selection without knowing feature relevance. On the other hand, opportunities are also unequivocally present with additional data sources; for example, link information is ubiquitous in social media and could be helpful in selecting relevant features. In this paper, we study a novel problem to conduct unsupervised streaming feature selection for social media data. We investigate how to exploit link information in streaming feature selection, resulting in a novel unsupervised streaming feature selection framework USFS. Experimental results on two real-world social media datasets show the effectiveness and efficiency of the proposed framework comparing with the state-of-the-art unsupervised feature selection algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The rapid growth and popularity of social media services such as Twitter<ref type="foot" target="#foot_0">1</ref> , Facebook<ref type="foot" target="#foot_1">2</ref> provide a platform for people to perform online social activities by sharing information and communicating with others. Massive amounts of highdimensional data (blogs, posts, images, etc.) are user generated and quickly disseminated. It is desirable and of great importance to reduce the dimensionality of social media data for many learning tasks due to the curse of dimensionality. One way to resolve this problem is feature selection <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20]</ref>, which aims to select a subset of relevant features for a compact and accurate representation.</p><p>Traditional feature selection assumes that all features are static and known in advance. However, this assumption is invalid in many real-world applications especially in social media which is imbued with high-velocity streaming features. In social media, features are generated dynamically, new features are sequentially added and the size of features is unknown in most cases. For example, Twitter produces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously being user generated. These slang words promptly grab users' attention and become popular in a short time. It is not practical to wait until all features are available before performing feature selection. Another example is that after earthquakes, topics (features) like "Nepal" emerge as hot topics in social media shortly, traditional batch-mode feature selection can hardly capture and select such features timely. Therefore, it could be more appealing to perform streaming feature selection (SFS) <ref type="bibr" target="#b37">[37]</ref> to rapidly adapt to the changes.</p><p>In SFS, the number of instances is considered to be constant while candidate features arrive one at a time, the task is to timely select a subset of relevant features from all features seen so far <ref type="bibr" target="#b37">[37]</ref>. Instead of searching for the whole feature space which is costly, SFS processes a new feature upon its arrival. A general framework of streaming feature selection is presented in Figure <ref type="figure" target="#fig_1">1</ref>. At each time step, a typical SFS algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore. The vast majority of existing streaming feature selection algorithms are supervised which utilize label information to guide feature selection process <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref>. However, in social media, it is easy to amass vast quantities of unlabeled data, while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data in social media, we propose to study unsupervised streaming feature selection. Unsupervised streaming feature selection is particularly difficult and challenging: (1) without any label information, it is difficult to assess the importance of features; and (2) features are usually not predefined but are generated dynamically, hence it cannot be carried out by directly applying traditional unsupervised feature selection algorithms.  On the other hand, link information is abundant in social media. As observed by homophily <ref type="bibr" target="#b21">[21]</ref> from social sciences, linked instances are likely to share similar features (or attributes). Therefore, as label information for supervised streaming feature selection, link information could provide helpful constraints to enable unsupervised streaming feature selection. However, linked social media data is inherently not independent and identically distributed (i.i.d.) while existing streaming feature selection are based on the data i.i.d. assumption, it is challenging to exploit link information for streaming feature selection.</p><p>In this work, we investigate: (1) how to exploit link information for feature selection; and (2) how to perform streaming feature selection in unsupervised scenarios. Our solutions to these two questions lead to a novel unsupervised streaming feature selection framework USFS. The main contributions of this paper are outlined as follows:</p><p>• Providing a principled approach to utilize link information to enable unsupervised streaming feature selection in social media;</p><p>• Proposing an unsupervised streaming feature selection framework, USFS, which exploits link and feature information simultaneously to select features dynamically and efficiently; and</p><p>• Evaluating the efficacy and efficiency of the proposed USFS framework on real-world social media datasets.</p><p>The rest of this paper is organized as follows. We formally define the problem of unsupervised streaming feature selection in Section 2. In Section 3, we introduce the proposed framework of unsupervised streaming feature selection. Empirical evaluation on real-world social media datasets is presented in Section 4 with discussion. In Section 5, we briefly review related work. The conclusion and future work are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM STATEMENT</head><p>We first summarize some notations used in this paper. We use bold uppercase characters to denote matrices, bold lowercase characters to denote vectors, normal lowercase characters to denote scalars. For an arbitrary matrix A ∈ R n×d , ai and a j mean the i-th row and j-the column of matrix A, respectively. Aij or a j i denotes the (i, j)-th entry of matrix A. A (t) denotes the matrix of A at time step t, (a (t) )i and (a (t) ) j represent i-th row and j-th column of matrix A (t) , respectively. T r(A) is the trace of matrix A if it is square, the Frobenius norm of the matrix A ∈ R n×d is defined as</p><formula xml:id="formula_0">||A||F = n i=1 d j=1 A 2 ij . Let U = {u1,</formula><p>u2, ..., un} denote a set of n linked data instances. We assume features are dynamically generated and one feature arrives at each time step, thus, at time step t, each linked data instance is associated with a set of t features F (t) = {f1, f2, ..., ft}. Then at the next time step t+1, each linked instance is tied with a new feature set F (t+1) = {f1, f2, ..., ft, ft+1}. The data representation at time step t and t + 1 can be represented as X (t) = [f1, f2, ..., ft] and X (t+1) = [f1, f2, ..., ft, ft+1], where f1, ..., ft, ft+1 are the feature vectors corresponding to features f1, ..., ft, ft+1. We denote the link information between instances in a matrix M ∈ R n×n , where Mij = 1 if ui and uj are linked, otherwise Mij = 0. The link information can either be a directed or an undirected graph. Note that in this paper, we do not consider the dynamics of link information and the reason is that link information does not change as fast as feature information; for example, the friend circles of most users are often stable once they are established.</p><p>With these notations, the task of unsupervised streaming feature selection in social media focuses on finding a subset of most relevant features S (t) ⊆ F (t) at each time step t to facilitate clustering by utilizing both the feature information F (t) and the link information M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UNSUPERVISED STREAMING FEATURE SELECTION IN SOCIAL MEDIA</head><p>The work flow of the proposed framework USFS is shown in Figure <ref type="figure" target="#fig_2">2</ref>. We can observe that it consists of three components. The first component shows the representation of data. We have a set of linked instances (for example u1, u2, ..., u5); for each linked instance, its features arrive through a streaming fashion, for example, u1, u2, ..., u5 are associated with features f1, f2..., ft at time step t; are associated with features f1, f2, ..., ft+i at time step t+i. The second component shows the process of the algorithm, we will first talk about how to model link information via extracting social latent factors and how to use them as a constraint through a regression model in Section 3.1. Then we will introduce how to model feature information to make it consistent with social latent factors in Section 3.2. At last, we will show how to efficiently test new feature and existing features in Section 3.3. After that, as shown in the third component, we obtain a subset of relevant features at each time step (for example, S (t) at time step t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Link Information</head><p>Social media users connect to each other due to different factors such as movie fans, football enthusiasts, colleagues and each factor should be related to certain features (or at-  tributes) of users <ref type="bibr" target="#b19">[19]</ref>. Therefore, extracting these factors from link information should be very useful to steer the unsupervised streaming feature selection. However, in most cases, these hidden factors are not explicitly available in social media data.</p><p>Uncovering hidden social factors has been extensively studied <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30]</ref>. In this work, we extract the social latent factors for each instance based on the mixed membership stochastic blockmodel <ref type="bibr" target="#b2">[2]</ref>. In the blockmodel, it assumes that there exists a number of latent factors, and these latent factors interact with each other with certain probabilities to form social relationships. More specially, each instance is associated with a k-dimensional latent factor vector πi ∈ R k , where πig denotes the probability of ui in factor g. This means that each instance can simultaneously be sided with multiple latent factors with different affiliation strength. For each instance, the indicator vector zi→j denotes the latent factor membership of ui when it links to uj and zi←j denotes the latent factor membership of ui when it is linked from uj. The interaction strength between different latent factors is encoded in a k × k stochastic matrix B, in which each element is between 0 and 1. Then the observed link information is generated according to the following process:</p><p>1. For each linked instance ui,</p><p>• Draw a k dimensional vector πi ∼ Dirichlet(θ).</p><p>2. For each pair of linked instance (i, j) ∈ U × U,</p><p>• Draw indicator vector zi→j ∼ Multinomial(πi).</p><p>• Draw indicator vector zi←j ∼ Multinomial(πj).</p><p>• Draw the relationship between ui and uj, Mi,j ∼ Bernoulli(zi→jBzi←j).</p><p>Motivated by <ref type="bibr" target="#b12">[12]</ref>, we use a scalable inference algorithm to obtain the social latent factors Π = [π1, π2, ..., πn] T ∈ R n×k for all n instances efficiently.</p><p>As we obtain the social latent factors for each linked instances, we take advantage of them as a constraint to perform feature selection through a regression model. We measure the importance of each feature by its ability to differentiate different social latent factors. At time step t, given each social latent factor π i (a column of Π) for all instances, we are able to find a subset of most relevant features by the following minimization problem: min</p><formula xml:id="formula_1">W (t) J (W (t) ) = 1 2 k i=1 ||X (t) (w (t) ) i -π i || 2 2 + α k i=1 (w (t) ) i 1 = 1 2 ||X (t) W (t) -Π|| 2 F + α k i=1 (w (t) ) i 1,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">X (t) ∈ R n×t , W (t)</formula><p>∈ R t×k is a mapping matrix which assigns each instance a k-dimensional social latent vector at time step t. Each column of W (t) , i.e., (w (t) ) i ∈ R t contains coefficients of t different features in approximating the i-th social latent vector of Π. α is a parameter which controls the trade-off between the loss function and the 1-norm. One main advantage of 1-norm regression (Lasso) <ref type="bibr" target="#b31">[31]</ref> is that it leads some coefficients of (w (t) ) i to be exact zero. This property makes it to be suitable for feature selection, as we can select features with corresponding non-zero coefficients.</p><p>In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications. Besides, features in social media usually have strong pairwise correlations, such as synonyms or antonyms words in text data. Lasso tends to randomly select features from a group and discards the others. Therefore, we employ the elastic net <ref type="bibr" target="#b38">[38]</ref> on the basis of Eq. ( <ref type="formula" target="#formula_1">1</ref>): min</p><formula xml:id="formula_3">W (t) J (W (t) ) = 1 2 ||X (t) W (t) -Π|| 2 F + α k i=1 (w (t) ) i 1 + β 2 ||W (t) || 2 F ,<label>(2)</label></formula><p>where the regularization term β 2 ||W (t) || 2 F controls the robustness of the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Feature Information</head><p>In Twitter, if two users post similar contents (features), they are more likely to share similar social latent factors, like hobbies, education background, etc. The similarity of social latent factors reflects the correlation of two linked in-stances in the feature space. In other words, social latent factors of two instances are more likely to be consistent when their feature similarity (like textual similarity) is high. To model the feature information, we first construct a graph G to represent the feature similarity between different data instances. The adjacency matrix A ∈ R n×n of the graph G at time step t is defined as:</p><formula xml:id="formula_4">A (t) ij = 1 if (x (t) )i ∈ Np((x (t) )j) or (x (t) )j ∈ Np((x (t) )i) 0 otherwise</formula><p>where (x (t) )i indicates the feature information of ui, Np((x (t) )i) represents p-nearest neighbors of (x (t) )i. Then feature information can be modeled by minimizing the following term:</p><formula xml:id="formula_5">1 2 n i=1 n j=1 A (t) ij ||(X (t) W (t) )i -(X (t) W (t) )j|| 2 2 = T r((X (t) W (t) ) T (D (t) -A (t) )(X (t) W (t) )) = T r((X (t) W (t) ) T L (t) (X (t) W (t) )),<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">D (t) ∈ R n×n is a diagonal matrix with D (t) ii = n j=1 A (t) ij , L (t) = D (t) -A (t)</formula><p>is the Laplacian matrix. Since the Laplacian matrix in Eq. ( <ref type="formula" target="#formula_5">3</ref>) is positive-semidefinite, Eq. ( <ref type="formula" target="#formula_5">3</ref>) can also be written as:</p><formula xml:id="formula_7">T r((X (t) W (t) ) T L (t) (X (t) W (t) )) = ||(X (t) W (t) ) T (L (t) ) 1 2 || 2 F .<label>(4)</label></formula><p>The optimization formulation, which integrates feature information, is defined as: min</p><formula xml:id="formula_8">W (t) J (W (t) ) = 1 2 ||X (t) W (t) -Π|| 2 F + α k i=1 (w (t) ) i 1 + β 2 ||W (t) || 2 F + γ 2 ||(X (t) W (t) ) T (L (t) ) 1 2 || 2 F ,<label>(5)</label></formula><p>where γ is the regularization parameter to balance link information and feature information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Streaming Feature Selection Framework</head><p>The objective function in Eq. ( <ref type="formula" target="#formula_8">5</ref>) at time step t is parameterized by a transformation matrix W (t) . It can be further decomposed into a series of k sub-problems which correspond to k social latent factors:</p><formula xml:id="formula_9">min (w (t) ) i J ((w (t) ) i ) = 1 2 ||X (t) (w (t) ) i -π i || 2 2 + α (w (t) ) i 1 + β 2 ||(w (t) ) i || 2 2 + γ 2 ||(X (t) (w (t) ) i ) T (L (t) ) 1 2 || 2 2 ,<label>(6)</label></formula><p>where i = 1, ..., k. By solving each sub-problem in Eq. ( <ref type="formula" target="#formula_9">6</ref>), we can select a subset of features at time t. Next we introduce how to efficiently perform feature selection when a new feature ft+1 is generated at a new time step t + 1. Following common steps of supervised streaming feature selection <ref type="bibr" target="#b33">[33]</ref>, the proposed framework will test: (1) whether we should select the new feature; and (2) whether we should discard some existing features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Testing New Features</head><p>It can be observed from Eq. ( <ref type="formula" target="#formula_9">6</ref>) that at time step t + 1, incorporating a new feature feature ft+1 involves adding a new non-zero weight value (w (t+1) ) i t+1 to the model, which incurs a penalty increasing α (w (t+1) ) i t+1 1 on the 1 regularization term. The addition of the new feature ft+1 reduces the overall objective function value in Eq. ( <ref type="formula" target="#formula_9">6</ref>) only when the overall reduction from the first, third, and forth term outweighs the increase of 1 penalty α (w (t) ) i t+1 1.</p><p>Motivated by <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>, we adopt a stagewise way to check newly arrived features. Let J ((w (t+1) ) i ) denotes the objective function of Eq. ( <ref type="formula" target="#formula_9">6</ref>) at time step t + 1:</p><formula xml:id="formula_10">min (w (t+1) ) i J ((w (t+1) ) i ) = 1 2 ||X (t+1) (w (t+1) ) i -π i || 2 2 + α (w (t+1) ) i 1 + β 2 ||(w (t+1) ) i || 2 2 + γ 2 ||(X (t+1) (w (t+1) ) i ) T (L (t+1) ) 1 2 || 2 2 ,<label>(7)</label></formula><p>then the derivative of J ((w (t+1) ) i ) with respect to (w (t+1) ) i t+1 is as follows:</p><formula xml:id="formula_11">∂J ((w (t+1) ) i ) ∂(w (t+1) ) i t+1 = [(X (t+1) ) T (X (t+1) (w (t+1) ) i -π i ) + β(w (t+1) ) i + γ(X (t+1) ) T L (t+1) X (t+1) (w (t+1) ) i ]t+1 + αsign((w (t+1) ) i t+1 ) = [(X (t+1) ) T (X (t+1) (w (t+1) ) i -π i ) + β(w (t+1) ) i + γ(X (t+1) ) T L (t+1) X (t+1) (w (t+1) ) i ]t+1 ± α.<label>(8)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_11">8</ref>), the derivative of 1-norm term α (w (t+1) ) i 1 w.r.t (w (t+1) ) i t+1 is not smooth. Here we discuss the sign of the derivative, i.e., sign(w (t+1) ) i t+1 . When the new feature ft+1 arrives, we first set its feature coefficient (w (t+1) ) i t+1 to be zero and add it to the model, if:</p><formula xml:id="formula_12">[(X (t+1) ) T (X (t+1) (w (t+1) ) i -π i ) + β(w (t+1) ) i + γ(X (t+1) ) T L (t+1) X (t+1) (w (t+1) ) i ]t+1 -α &gt; 0,<label>(9)</label></formula><p>it is easy to verify that:</p><formula xml:id="formula_13">∂J ((w (t+1) ) i ) ∂(w (t+1) ) i t+1 &gt; 0. (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>In order to reduce the objective function value J ((w (t+1) ) i ), we need to slightly reduce the value of (w (t+1) ) i t+1 to make it negative, and then the sign of (w (t+1) ) i t+1 will be negative. For the same reason, if: <ref type="bibr" target="#b11">(11)</ref> then:</p><formula xml:id="formula_15">[(X (t+1) ) T (X (t+1) (w (t+1) ) i -π i ) + β(w (t+1) ) i + γ(X (t+1) ) T L (t+1) X (t+1) (w (t+1) ) i ]t+1 + α &lt; 0,</formula><formula xml:id="formula_16">∂J ((w (t+1) ) i ) ∂(w (t+1) ) i t+1 &lt; 0, (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>the sign of (w (t+1) ) i t+1 will be positive. If both of previous conditions are not satisfied, it is impossible to reduce the objective function value J ((w (t+1) ) i ) by making (w (t+1) ) i t+1 as a small disturbance around 0. In other words, for the new feature ft+1, we need to check:</p><formula xml:id="formula_18">|[(X (t+1) ) T (X (t+1) (w (t+1) ) i -π i ) + β(w (t+1) ) i + γ(X (t+1) ) T L (t+1) X (t+1) (w (t+1) ) i ]t+1| &gt; α. (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>As the condition in Eq. ( <ref type="formula" target="#formula_18">13</ref>) is satisfied, it indicates that the addition of the new feature ft+1 will reduce the objective function value J ((w (t+1) ) i ), therefore the new feature is included in the model described in Eq. ( <ref type="formula" target="#formula_10">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Testing Existing Features</head><p>In social media, when new features are continuously being generated, they may take place of some existing features since new features can better reflect the interests of users, etc. Old features become outdated as a result, therefore, in the proposed unsupervised streaming feature selection framework USFS, we also investigate if it is necessary to remove any existing selected features.</p><p>After a new feature is accepted and added to the model, we optimize Eq. ( <ref type="formula" target="#formula_10">7</ref>) with respect to existing feature weights, the optimization may force some feature weights to be zero. If the feature weight obtains a zero value, it indicates that the existence of the feature is not likely to reduce the objective function value and the feature can be removed. Here we discuss how to solve the optimization problem in Eq. ( <ref type="formula" target="#formula_10">7</ref>). The objective function in Eq. ( <ref type="formula" target="#formula_10">7</ref>) is convex and the gradient with respect to (w (t+1) ) i t+1 can be easily obtained as Eq. ( <ref type="formula" target="#formula_11">8</ref>), then a global optimum solution can be achieved. We choose to use a Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasinewton method <ref type="bibr" target="#b3">[3]</ref> to solve the optimization problem. Unlike traditional Newton's method which requires the calculation of second derivatives (the Hessian), BFGS only needs the gradient of the objective function to be computed at each iteration. Therefore, it is more efficient than Newton's methods especially when Hessian evaluation is slow.</p><p>The minimization problem in Eq. ( <ref type="formula" target="#formula_10">7</ref>) can be generalized to the following form:</p><formula xml:id="formula_20">min f (x), x ∈ R n . (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>At each iteration, the optimal solution x is updated as:</p><formula xml:id="formula_22">xm+1 = xm -δmHmgm,<label>(15)</label></formula><p>where Hm = B -1 m , Bm is an approximation to the Hessian matrix (Bm ≈ [∇ 2 f (xm)]), gm = ∇f (xm) is the gradient and δm is the step size that can be determined by line search. Let the vectors sm and cm be:</p><formula xml:id="formula_23">sm = xm+1 -xm, cm = gm+1 -gm,<label>(16)</label></formula><p>the next Hessian approximation has to meet the secant equation:</p><formula xml:id="formula_24">Bm+1sm = cm. (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>By pre-multiplying the secant equation s T m at both sides, we obtain the curvature condition:</p><formula xml:id="formula_26">s T m Bm+1sm &gt;0 = s T m cm &gt; 0. (<label>18</label></formula><formula xml:id="formula_27">)</formula><p>If the curvature condition is satisfied, Bm+1 in the secant equation has at least one solution, which can be updated by the following way:</p><formula xml:id="formula_28">Bm+1 = Bm + cmc T m c T m sm - Bmsms T m Bm s T m Bmsm . (<label>19</label></formula><formula xml:id="formula_29">)</formula><p>Its inverse, i.e., Hm+1, can be updated efficiently by Sherman-Morrison formula <ref type="bibr" target="#b10">[10]</ref>:</p><formula xml:id="formula_30">Hm+1 = Hm- smc T m Hm + Hmcms T m s T m cm +(1+ c T m Hmcm s T m cm ) sms T m s T m cm . (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>With these, the BFGS algorithm to solve Eq. ( <ref type="formula" target="#formula_10">7</ref>) is illustrated in Algorithm 1.</p><p>Algorithm 1 BFGS to optimize Eq. ( <ref type="formula" target="#formula_10">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Feature Selection by USFS</head><p>By solving all k sub-problems at time step t+1, we obtain the sparse coefficient matrix W = [(w (t+1) ) 1 , ..., (w (t+1) ) k ]. Since we solve each sub-problem separately, the number of non-zero weights in each (w (t+1) ) i (i = 1, ..., k) is not necessarily to be the same. For each feature fj, if any of the k corresponding feature weight coefficients (w (t+1) ) i j (i = 1, 2, ..., k) is nonzero, the feature is included in the final model, otherwise the feature is not selected. If fj is selected, its feature score at time step t + 1 is defined as: F Score(j) (t+1) = max((w (t+1) ) 1 j , ..., (w (t+1) ) k j )</p><p>The selected features are then sorted according to their feature scores in a descending order, the higher the feature score, the more important the feature is. The pseudo code of the proposed unsupervised streaming feature selection algorithm for social media data is illustrated in Algorithm 2. It efficiently performs unsupervised feature selection when a new feature ft+1 arrives. In line 1, we obtain the social latent factor matrix Π using the link information M. The algorithm to check new feature and existing features is illustrated in lines 2-8. More specifically, for each sub-problem, we first check the gradient condition, this step decides whether we accept the new feature (line 3). If the condition is satisfied (line 4), the new feature is included in the model (line 5) and the model is re-optimized with respect to all existing feature weights (line 6). At last, when the new feature is included in the model, it updates the Laplacian matrix (line 10), calculates the feature scores, and updates the selected feature set (lines 11-12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Time Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Time Complexity for All Streaming Features</head><p>The mixed membership stochastic model to extract social latent factors has a time complexity of O(n 2 k 2 ). Assuming the total number of streaming features is t and the number of obtained features is s, the time complexity of updating Laplacian matrix is bounded by O(n 2 st). At each time step, we check the gradient condition in Eq. ( <ref type="formula" target="#formula_18">13</ref>). The time complexity upper bound of the gradient checking over all t time steps is O(n 2 kst). Since we optimize the model in Eq. ( <ref type="formula" target="#formula_10">7</ref>) when the new feature is accepted, the total time of optimization with BGFS is O(n 2 s 2 t) in the worst case when the selected s features are the latest arrived s features.</p><p>Overall, the total time complexity of the proposed USFS is O(n 2 k 2 ) + O(n 2 st) + O(n 2 kst) + O(n 2 s 2 t). Since k t and s t, the upper bound of the overall time complexity is O(n 2 s 2 t). However, it only provides an upper bound, in real-world applications, the time complexity could be much lower than this upper bound. We will empirically show the efficiency of the proposed framework in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Time Complexity for An Individual Feature</head><p>For the newly generated feature, suppose there are already s features in the model, if its previous feature is added in the model, the time complexity of gradient test is O(n 2 ks), otherwise the time complexity is only O(n). To test existing features via BFGS, the time complexity is O(n 2 s 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate the performance of the proposed framework USFS. In particular, we evaluate the following: 1) How is the quality of selected features by USFS compared with the state-of-the-art unsupervised feature selection algorithms? 2) How efficient is the proposed USFS framework? Before introducing the details of experiments, we first introduce the datasets and experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use two real-world social media datasets BlogCatalog and Flickr for experimental evaluation. Some statistics of the datasets are listed in Table <ref type="table" target="#tab_2">1</ref>.</p><p>BlogCatalog: BlogCatalog 3 is a social blog directory which manages bloggers and their blogs. Bloggers are asso- ciated with sets of tags, which provide feature information.</p><p>Users in blogcatalog follow each other which form the social link information. Bloggers can also register their blogs under predefined categories, which are used as ground truth for validation in our work.</p><p>Flickr: Flickr<ref type="foot" target="#foot_2">4</ref> is an image hosting and sharing website, the key features in flickr are tags, user can specify the list of tags to reflect their interests. Similar to blogcatalog, users in flickr can interact with others. Photos are organized under prespecified categories, which are used as the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Following a standard way to assess unsupervised feature selection <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35]</ref>, we use the clustering performance to evaluate the quality of selected features. Two commonly used clustering performance evaluation metrics, i.e., accuracy (ACC) and normalized mutual information (NMI) <ref type="bibr" target="#b4">[4]</ref> are used in this paper.</p><p>To the best of our knowledge, we are the first to study streaming feature selection in social media. To investigate the effectiveness and the efficiency of the proposed framework, we choose the following state-of-the-art unsupervised feature selection algorithms as baseline methods:</p><p>• LapScore: Laplacian score <ref type="bibr" target="#b16">[16]</ref> evaluates feature importance by its ability to preserve the local manifold structure of data.</p><p>• SPEC: Features are selected by spectral analysis <ref type="bibr" target="#b36">[36]</ref> and SPEC can be considered as an extension of Laplacian score method.</p><p>• NDFS: Nonnegative Discriminative Unsupervised Feature Selection <ref type="bibr" target="#b18">[18]</ref> which selects features via a joint nonnegative spectral analysis as well as a 2,1-norm regularization.</p><p>• LUFS: LUFS utilizes both content information and link information to perform feature selection in an unsupervised scenario <ref type="bibr" target="#b29">[29]</ref>.</p><p>For LapScore, NDFS and USFS, we follow previous work <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b16">16]</ref> to specify the number of neighborhood size to be 5 to construct the Laplacian matrix on the data instances. NDFS and LUFS have different regularization parameters, we set these regularization parameters according to the suggestions from the original papers. For USFS, we set the number of social latent factors as the number of clusters. There are three important regularization parameters α, β and γ in USFS. α controls the sparsity of the model, β is the parameter for elastic net which controls the robustness of the model, and γ balances the contribution of the link information and feature information. In the experiments, we empirically set α = 10, β = 0.1, γ = 0.1 and more details about the effects of these parameters on the proposed framework will be discussed in Section 4.5.</p><p>All experiments are conducted on a machine with 16GB RAM and Intel Core i7-4770 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quality of Selected Features</head><p>Following streaming feature selection settings that assume features arrive one at a time <ref type="bibr" target="#b37">[37]</ref>, we divide all features into 9 groups where we choose the first {20%, ..., 90%, 100%} as streaming features. In each group, we perform feature selection with traditional unsupervised feature selection algorithm as well as the proposed USFS algorithm. We record how many features USFS selects and specify the same number as that of selected features by traditional unsupervised feature selection algorithms for a fair comparison.</p><p>After obtaining the feature selection results, K-means clustering is performed based on the chosen features. We repeat the K-means algorithm 20 times and report average results because K-means may converge to local minima. The clustering results are evaluated by both accuracy (ACC) and normalized mutual information (NMI). The higher the ACC and NMI values are, the better feature selection performance is. The comparison results are shown in Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref> for BlogCatalog and Flickr, respectively. Note that the number in parentheses in the table indicates the number of selected features determined by USFS. We make the following observations:</p><p>• USFS tends to accept new features at the very beginning, then it becomes increasingly difficult for newly generated features to alternate previous decisions since existing features already provide us enough information. For example, no new features are accepted anymore after the number of selected features reach 275 and 670 in BlogCatalog and Flickr, respectively.</p><p>• USFS consistently outperforms all baseline methods on both datasets with significant performance gain in most cases. The reason is that traditional unsupervised feature selection algorithms are based on the i.i.d. assumption which is invalid in linked social media data. USFS takes advantage of link information to guide the unsupervised streaming feature selection. It can also be observed that when feature information is scarce (for example 20%), link information could better complement feature information for feature selection. We also perform pair-wise wilcoxon signed-rank test <ref type="bibr" target="#b7">[7]</ref> between USFS and other baseline methods on different proportions of streaming features and the test results show that USFS is significantly better (with both 0.01 and 0.05 significance level).</p><p>• For baseline methods, clustering performance gradually decreases when features are continuously generated. While for USFS, the clustering performance is relatively more stable when the proportion of streaming features varies from 20% to 100%. The number of selected features by USFS is also very stable, which varies from 236 to 275 in BlogCatalog and 562 to 670 in Flickr, respectively. It demonstrates the effectiveness of streaming feature selection, with a large amount of streaming features, we can only dynamically maintain a small set of relevant features without deteriorating the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficiency Performance Comparison</head><p>To evaluate the efficiency of the proposed USFS algorithm, we compare the running time of different methods in Figure <ref type="figure" target="#fig_5">3</ref>. As LapScore, SPEC, NDFS and LUFS are not designed for dealing with streaming features, we rerun the feature selection process at each time step. For both datasets, we set the cumulative running time threshold to be around 10 4 seconds since all methods except USFS take more than 50 hours. As can be observed, the proposed USFS algorithm is significantly faster than other baseline methods, the average processing time for each feature in BlogCatalog and Flickr is only 0.62 seconds and 1.37 seconds, respectively.</p><p>We also record the cumulative running time of USFS when the cumulative running time of other methods arrive the threshold (10 4 seconds), the results show that in BlogCatalog, USFS is 7×, 20×, 29×, 76× faster than LapScore, LUFS, NDFS, SPEC, respectively; in Flickr, USFS is 5×, 11×, 20×, 75× faster than LapScore, LUFS, NDFS, SPEC, respectively. The difference is becoming larger as the curve of USFS in Figure <ref type="figure" target="#fig_5">3</ref> is getting more smooth when streaming features continuously arrive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects of Parameters</head><p>As discussed in Section 3, USFS has four important parameters: the number of social latent factors k, and parameters α, β, and γ in Eq. ( <ref type="formula" target="#formula_9">6</ref>). To investigate the effects of these parameters, we vary one parameter each time and fix the other three to see how the parameter affects the feature selection performance in terms of clustering with different number of selected features. We only perform the parameter study on BlogCatalog dataset to save space since we have similar observations on the Flickr dataset.</p><p>First, we vary the number of social latent factors k from 5 to 10 while fix the other three parameters (α = 10, β = 0.1, γ = 0.1). The clustering performance in terms of Accuracy and NMI is illustrated in Figure <ref type="figure" target="#fig_6">4</ref>. The clustering performance is the best when the number of social latent factors is close to the number of clusters, which is 6 in BlogCatalog.</p><p>To assess the effect of parameter α which controls the model sparseness, we vary α as {0.001, 0.01, 0.1, 1, 10, 100, 1000} while fix k = 6, β = 0.1, γ = 0.1, performance variance between α and number of selected features is presented in Figure <ref type="figure" target="#fig_7">5</ref>. With the increase of α, the clustering performance rises rapidly and then keeps stable between the range of 10 to 1000. A high value of α indicates that it is not easy for new features to pass the gradient test in Eq. ( <ref type="formula" target="#formula_18">13</ref>), thus the accepted features are more relevant and meaningful.</p><p>We study the effect of parameter β which makes the model more robust. Similar to the setting of α, β is also in the range of {0.001, 0.01, 0.1, 1, 10, 100, 1000} and k = 6, α = 10, γ = 0.1. The results are shown in Figure <ref type="figure" target="#fig_8">6</ref>. We can see that clustering performance is much more sensitive to the number of selected features than to β. The performance is relatively higher when β is between 0.1 and 10.</p><p>At last, we evaluate the trade-off between link information and feature information by varying γ in {0.001, 0.01, 0.1, 1, 10, 100, 1000} while fix k = 6, α = 10, β = 0.1. The results are presented in Figure <ref type="figure" target="#fig_9">7</ref>. As shown in the figure, in most cases, the clustering performance first increases, reaches its peak and then it gradually decreases. The best performance achieves when γ is around 0.1. These observations suggest the importance of both link information and feature information in unsupervised streaming feature selection.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>In this section, we first briefly review some related work in traditional feature selection, then we introduce existing work on supervised streaming feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Traditional Feature Selection</head><p>Feature selection algorithms can be broadly grouped into three categories: filter methods, wrapper methods as well as embedded methods. Filter methods are independent of any learning algorithms and are thereby very efficient, they rely on some data characteristics such as distance, consistency, dependency, information, and correlation to measure the strength of each feature individually <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28]</ref>. Wrapper methods use the prediction power of a predefined learning algorithm to evaluate the quality of selected features. They are inevitably computational expensive since the search space grows exponentially with the number of features <ref type="bibr" target="#b9">[9]</ref>. Embedded methods is a tradeoff between these two models which combines feature selection and model con-    struction <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b23">23]</ref>. Therefore, they are usually comparably efficient to filters and are comparably accurate to wrappers.</p><p>According to the availability of labels, feature selection methods consist of supervised methods and unsupervised methods. Supervised methods take advantage of the discriminative information encoded in class labels to select the subset of features that are able to distinguish instances from different classes <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>. Since real-world data is usually unlabeled and collecting labeled data is particular expensive requiring both time and effort, unsupervised feature selection receives more and more attention recently. Due to the lack of label information, unsupervised feature selection algorithms exploit different criteria to define the relevance of features such as data similarity <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b36">36]</ref> and local discriminative information <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Streaming Feature Selection</head><p>Previous mentioned methods can only handle static dataset without considering the dynamic properties of features. Sometimes, it is difficult to obtain the whole feature space. Streaming feature selection, provides a solution to this problem. The first attempt to perform streaming feature selection is credited to Perkins and Theiler <ref type="bibr" target="#b27">[27]</ref>. They adopted a stagewise gradient descent technique to dynamically update selected feature set. Alpha-investing calculates the p-value of new feature in a regression model to determine if it should be included. In Alpha-investing <ref type="bibr" target="#b37">[37]</ref>, once a feature is added, it will never be discarded in the future. OSFS <ref type="bibr" target="#b33">[33]</ref> finds an optimal subset of features based on online feature relevance and feature redundancy analysis. At each time step, new candidate feature is accepted if it is strongly or weakly relevant to existing features; then existing but redundant features will be removed by redundancy analysis. In contrast to traditional streaming feature selection, Wang et al. <ref type="bibr" target="#b32">[32]</ref> studied the problem of group streaming feature selection in which features continuously arrive in groups, the setting is more realistic in real-world applications. Guo et al. <ref type="bibr" target="#b14">[14]</ref> proposed a node classification algorithm for streaming network. They consider the changes of network structure and node contents for node classification via feature selection techniques. All previous mentioned methods, focus on supervised scenarios, unsupervised streaming feature selection, however, is not well studied yet.</p><p>It should be noted that streaming feature selection differs from the task of feature selection/extraction on data streams, such as <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b34">34]</ref>, in which feature sets are predefined while data are generated dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>The prevalence of unlabeled, high-dimensional and highvelocity social media data presents new challenges for feature selection. Meanwhile, the unique characteristics of link information in social media bring about new opportunities as we might be able to leverage them to dynamically select relevant features efficiently. In this work, we study a novel problem, unsupervised streaming feature selection for social media data. In particular, we investigate how to take advantage of link information to perform unsupervised streaming feature selection. Also, we propose a stagewise algorithm to solve the optimization problem at each time step. Therefore, the model has the power to decide whether to add a newly arrived feature and whether to remove existing features promptly. Theoretical time complexity analysis and empirical experimental results on real-world social media datasets demonstrate that the proposed USFS framework works effectively and efficiently.</p><p>Future work can be focused on two aspects: First, in this work, we assume that the link information is relative stable compared with dynamic feature information. However, sometimes, the network structure is also continuously changing, how to perform streaming feature selection on dynamic network is a challenging problem and is unsolved yet. Second, in social media, we have features from multiple sources, like image features, text features, video features, etc. We will investigate how to fuse heterogeneous feature sources in the streaming feature selection task in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A framework of streaming feature selection. It consists of two phases: testing newly arrived features and testing existing features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A framework of unsupervised streaming feature selection in social media.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>)</head><label></label><figDesc>Input: Starting point x0, convergence threshold , initial inverse Hessian approximation H0 Output: Optimal solution x * 1: m ← 0 2: gm = ∇f (xm) 3: while ||gm|| &gt; do 4: Obtain a direction pm = -Hmgm 5: Compute xm+1 = xm + δmpm, where δm is chosen by line search to meet curvature condition 6: gm+1 = ∇f (xm+1) 7: sm = xm+1 -xm 8: cm = gm+1 -gm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The cumulative runtime on BlogCatalog dataset and Flickr dataset. The Y axes are in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of # social latent factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Effect of β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>…… Time: t Time: t+i Social Latent Factors Constraint Streaming Features …… Time: t Time: t+i …… Time: t Time: t+i …… Time: t Time: t+i …… Time: t Time: t+i Streaming Feature Selection … … … … … Time: t Time: t+i ① ② at time t+i at time t … … at time 1 … … ③ Modeling Link Information Modeling Feature Information Testing New feature Testing Existing features Streaming Features Streaming Features Streaming Features Streaming Features</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 Unsupervised streaming feature selection framework (USFS) Input: New feature ft+1 at time t + 1, feature weight matrix W (t) at previous time step t, link information M, parameters α, β, γ, number of social latent factors k, number of nearest neighbors p Output: Selected feature subset S (t+1) at time step t + 1</figDesc><table><row><cell cols="2">1: Obtain social latent factors Π from M</cell></row><row><cell cols="2">2: for each social latent factor π l (l = 1, ..., k) do</cell></row><row><cell>3:</cell><cell>compute gradient g for ft+1 according to Eq. (8)</cell></row><row><cell>4:</cell><cell>if abs(g) &gt; α then</cell></row><row><cell>5:</cell><cell>add feature ft+1 to the model</cell></row><row><cell>6:</cell><cell>optimize the model via BFGS in Algorithm 1</cell></row><row><cell>7:</cell><cell>end if</cell></row><row><cell cols="2">8: end for</cell></row><row><cell cols="2">9: if feature ft+1 is accepted then</cell></row><row><cell>10:</cell><cell>update Laplacian matrix L (t+1)</cell></row><row><cell>11:</cell><cell>obtain feature scores according to Eq. (21)</cell></row><row><cell>12:</cell><cell>sort features by scores and update S (t+1)</cell></row><row><cell cols="2">13: end if</cell></row><row><cell cols="2">14: return S (t+1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Detailed information of datasets.</figDesc><table><row><cell>3 http://www.blogcatalog.com/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Clustering results with different feature selection algorithms on BlogCatalog dataset.</figDesc><table><row><cell></cell><cell>40.65</cell><cell>39.61</cell><cell>40.57</cell><cell>40.61</cell><cell>40.67</cell><cell>40.67</cell><cell>40.78</cell><cell>40.84</cell><cell>40.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NMI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">20%(259) 30%(266) 40%(270) 50%(271) 60%(272) 70%(272) 80%(274) 90%(275) 100%(275)</cell></row><row><cell>LapScore</cell><cell>0.1451</cell><cell>0.0600</cell><cell>0.0474</cell><cell>0.0510</cell><cell>0.0507</cell><cell>0.0743</cell><cell>0.0675</cell><cell>0.0793</cell><cell>0.0652</cell></row><row><cell>SPEC</cell><cell>0.0606</cell><cell>0.0765</cell><cell>0.0397</cell><cell>0.0143</cell><cell>0.0051</cell><cell>0.0098</cell><cell>0.0032</cell><cell>0.0029</cell><cell>0.0019</cell></row><row><cell>NDFS</cell><cell>0.1475</cell><cell>0.1250</cell><cell>0.1193</cell><cell>0.1092</cell><cell>0.1234</cell><cell>0.1006</cell><cell>0.1125</cell><cell>0.1130</cell><cell>0.1150</cell></row><row><cell>LUFS</cell><cell>0.0674</cell><cell>0.0533</cell><cell>0.0465</cell><cell>0.0490</cell><cell>0.0462</cell><cell>0.0492</cell><cell>0.0462</cell><cell>0.0345</cell><cell>0.0287</cell></row><row><cell>USFS</cell><cell>0.2028</cell><cell>0.1861</cell><cell>0.2028</cell><cell>0.2026</cell><cell>0.2042</cell><cell>0.2042</cell><cell>0.2059</cell><cell>0.2072</cell><cell>0.2072</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">20%(645) 30%(666) 40%(670) 50%(670) 60%(670) 70%(670) 80%(670) 90%(670) 100%(670)</cell></row><row><cell>LapScore</cell><cell>25.06</cell><cell>19.30</cell><cell>21.27</cell><cell>17.52</cell><cell>15.27</cell><cell>13.58</cell><cell>13.53</cell><cell>12.73</cell><cell>12.07</cell></row><row><cell>SPEC</cell><cell>25.52</cell><cell>20.26</cell><cell>17.50</cell><cell>15.46</cell><cell>13.53</cell><cell>14.11</cell><cell>13.94</cell><cell>13.52</cell><cell>13.07</cell></row><row><cell>NDFS</cell><cell>22.30</cell><cell>29.50</cell><cell>26.79</cell><cell>25.29</cell><cell>25.64</cell><cell>28.01</cell><cell>25.97</cell><cell>29.08</cell><cell>20.45</cell></row><row><cell>LUFS</cell><cell>27.13</cell><cell>22.11</cell><cell>19.19</cell><cell>24.00</cell><cell>24.79</cell><cell>19.97</cell><cell>18.22</cell><cell>19.24</cell><cell>23.99</cell></row><row><cell>USFS</cell><cell>27.22</cell><cell>29.50</cell><cell>28.37</cell><cell>28.37</cell><cell>28.37</cell><cell>28.37</cell><cell>28.37</cell><cell>28.37</cell><cell>28.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NMI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">20%(645) 30%(666) 40%(670) 50%(670) 60%(670) 70%(670) 80%(670) 90%(670) 100%(670)</cell></row><row><cell>LapScore</cell><cell>0.1072</cell><cell>0.0629</cell><cell>0.0786</cell><cell>0.0521</cell><cell>0.0308</cell><cell>0.0143</cell><cell>0.0172</cell><cell>0.0100</cell><cell>0.0040</cell></row><row><cell>SPEC</cell><cell>0.0854</cell><cell>0.0546</cell><cell>0.0328</cell><cell>0.0245</cell><cell>0.0117</cell><cell>0.0152</cell><cell>0.0118</cell><cell>0.0109</cell><cell>0.0083</cell></row><row><cell>NDFS</cell><cell>0.0676</cell><cell>0.1260</cell><cell>0.1073</cell><cell>0.0876</cell><cell>0.0853</cell><cell>0.1207</cell><cell>0.1236</cell><cell>0.1152</cell><cell>0.0663</cell></row><row><cell>LUFS</cell><cell>0.1129</cell><cell>0.0958</cell><cell>0.0550</cell><cell>0.1015</cell><cell>0.1023</cell><cell>0.0602</cell><cell>0.0535</cell><cell>0.0524</cell><cell>0.0913</cell></row><row><cell>USFS</cell><cell>0.1235</cell><cell>0.1368</cell><cell>0.1262</cell><cell>0.1262</cell><cell>0.1262</cell><cell>0.1262</cell><cell>0.1262</cell><cell>0.1262</cell><cell>0.1262</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Clustering results with different feature selection algorithms on Flickr dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://twitter.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.facebook.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://www.flickr.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is, in part, supported by National Science Foundation (NSF) under grant number IIS-1217466.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for projected clustering of high dimensional data streams</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>VLDB Endowment</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="852" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-cluster data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gi-nmf: Group incremental non-negative matrix factorization on data streams</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1119" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lwi-svd: low-rank, windowed, incremental singular value decompositions on time-evolving data sets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="987" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimum redundancy feature selection from microarray gene expression data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of bioinformatics and computational biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="185" to="205" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature subset selection and order identification for unsupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix computations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>JHU Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text stream clustering algorithm based on adaptive feature selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1393" to="1399" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable inference of overlapping communities</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized fisher score for feature selection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Snoc: streaming network node classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using nonnegative spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1026" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting trusts among users of online communities: an epinions case study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<title level="m">Computational Methods of Feature Selection</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding and evaluating community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26113</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint l2, 1-norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Trace ratio criterion for feature selection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="671" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grafting: Fast, incremental feature selection by gradient descent in function space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1333" to="1356" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online feature selection using grafting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Theoretical and empirical analysis of relieff and rrelieff</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="23" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for linked social media data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="904" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online group feature selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1757" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online streaming feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1159" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Streaming sparse principal component analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Theorem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="494" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">1-norm regularized discriminative feature selection for unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1589" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spectral feature selection for supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1151" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Streaming feature selection using alpha-investing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
