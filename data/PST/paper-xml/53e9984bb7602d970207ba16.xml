<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Workload-Aware Anonymization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kristen</forename><surname>Lefevre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin -Madison</orgName>
								<address>
									<addrLine>1210 West Dayton St</addrLine>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin -Madison</orgName>
								<address>
									<addrLine>1210 West Dayton St</addrLine>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin -Madison</orgName>
								<address>
									<addrLine>1210 West Dayton St</addrLine>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<addrLine>701 First Ave</addrLine>
									<postCode>94089</postCode>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Workload-Aware Anonymization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">450C613D0D7C32E160027AB72A4F693A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications Algorithms</term>
					<term>Experimentation</term>
					<term>Security Privacy</term>
					<term>Anonymity</term>
					<term>Data Recoding</term>
					<term>Predictive Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protecting data privacy is an important problem in microdata distribution. Anonymization algorithms typically aim to protect individual privacy, with minimal impact on the quality of the resulting data. While the bulk of previous work has measured quality through one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used.</p><p>This paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads, consisting of one or more data mining tasks, as well as selection predicates. An extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>k-Anonymity <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> and l-diversity <ref type="bibr" target="#b18">[18]</ref> have been studied widely as mechanisms for preventing re-identification attacks in microdata release. Of course, subject to the given anonymity constraints, the data should remain as useful as possible. Unfortunately, there is often a tension between these two goals.</p><p>It is our position that the best way of measuring quality is based on the task for which the data will ultimately be used. This paper provides anonymization techniques that incorporate a target workload of selections and mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivating Example</head><p>Suppose that a trusted agency compiles a database of disease information for several million hospital patients. However, the agency is prohibited by law from distributing this data without taking precautions to ensure individual privacy. For example, the agency should take steps to guarantee that the released data does not reveal any individual's HIV status.</p><p>Alice is an external researcher who is directing two separate studies, each of which could benefit from using the data in the central database. As part of the first study, Alice wants to build a classification model that uses age, smoking history, and HIV status to predict life expectancy. In the second study, she would like to find combinations of variables that are useful for predicting elevated cholesterol and obesity in males over 40.</p><p>In this situation, it is desirable to distribute anonymized microdata to individuals like Alice (the data recipients). <ref type="foot" target="#foot_0">1</ref>One might consider a simpler protocol, in which Alice requests a specific model, constructed entirely by the agency. However, there are two downsides to this approach. First, the simple model-distribution protocol assumes that the tasks are fully-specified at the time of the initial request. However, in our example, Alice's second study involves an entire class of models, each constructed using a subset of the data (attributes and records). Indeed, workloads like this arise naturally in certain types of exploratory data analysis <ref type="bibr" target="#b9">[9]</ref>.</p><p>Also, the inference implications of releasing one or more models constructed on the agency's unmodified data are not well-understood. Each such model reveals something about the distributional characteristics of the agency's data, and in certain cases, the revealed information might constitute a breach of privacy. However, in the case of a single released view, there are well-defined notions of anonymity, and the best Alice can do is to approximate the distribution in the (sanitized) data she is given.</p><p>The work presented in this paper is motivated by this type of scenario, where the goal is to create a single view of the database that respects all given anonymity constraints, but that remains useful for carrying out the tasks in a target class of workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper Overview and Contributions</head><p>We begin by reviewing the problems of anonymity, classification, and regression in Section 2. Because previous defin-itions of anonymity with respect to a sensitive attribute (i.e., l-diversity <ref type="bibr" target="#b18">[18]</ref>) have assumed that the sensitive attribute is nominally-valued, we also propose a novel diversity requirement for numeric attributes.</p><p>Our first main contribution, described in Section 3, is a suite of algorithms for generating an anonymous data snapshot, while preserving the utility of the data with respect to a target class of workloads. While previous work has considered incorporating a single classifier (constructed over the entire released data set) <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>, we incorporate the following expressive workload characteristics:</p><p>• Classification &amp; Regression We incorporate models predicting both categorical and numeric attributes.</p><p>• Multiple Target Models Often, the data recipient will want to build separate models to predict multiple different attributes.</p><p>• Selection &amp; Projection Frequently, one or more of the mining tasks will involve only a subset of the data (e.g., males over 40). In this case, it is important to guarantee that this data can be precisely and accurately selected from the released snapshot. Similarly, it is important to guarantee that the data remains useful when only a subset of the released attributes is used for a particular task.</p><p>Our second main contribution is an extensive experimental evaluation, described in Section 4. The results show that our anonymization algorithms are often more effective than previous algorithms in producing high-quality data, as judged by a variety of workloads.</p><p>Much of the previous work on k-anonymity has measured data quality or optimality using simple measures based on equivalence class size or the total number of generalizations/suppressions <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. Not surprisingly, our experiments also show that one-size-fits-all measures are not necessarily indicative of quality with respect to a particular workload.</p><p>In order to assess the impact of anonymization on subsequent analysis techniques, we first had to address some additional problems. Because standard learning algorithms use point data for training, rather than the region data produced by multidimensional recoding, Section 4.2 proposes a pre-processing step for converting regions to points. Following pre-processing, standard learning algorithms can be applied without modification.</p><p>The paper concludes with discussions of related and future work in Sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>K-anonymity <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> and l-diversity <ref type="bibr" target="#b18">[18]</ref> were proposed to limit re-identification risk in microdata publishing. Consider a single relation T . In defining anonymity, each attribute in T is characterized by at most one of the following types:</p><p>• Unique Identifiers A unique identifier is any attribute that identifies individuals (e.g., SS#). Known identifiers are typically removed entirely from released microdata.</p><p>• Quasi-identifier (Q 1 , ..., Q d ) A quasi-identifier is a minimal set of attributes that can be joined with external information to re-identify individual records. We assume that a quasi-identifier is recognized based on knowledge of the domain.</p><p>• Sensitive attributes (S) An attribute is considered sensitive if an adversary should not be permitted to uniquely associate its value with a unique identifier. For example, the HIV Status field in released medical data would likely be considered sensitive. Previous work assumed a single, nominally-valued, sensitive attribute <ref type="bibr" target="#b18">[18]</ref>; we also propose an extension to a numeric sensitive attribute.</p><p>The k-anonymity requirement is quite simple. Intuitively, it stipulates that no individual record should be uniquely identifiable from a group of k on the basis of its quasiidentifier values. We will refer to each group of tuples in T with identical quasi-identifier values as an equivalence class.</p><formula xml:id="formula_0">K-Anonymity [22, 23] A table T is k-anonymous with re- spect to quasi-identifier set Q 1 , ..., Q d if every unique tuple q 1 , ..., q d in the (multiset) projection of T on Q 1 , ..., Q d occurs at least k times.</formula><p>l-Diversity <ref type="bibr" target="#b18">[18]</ref> provides a natural extension, incorporating a nominal sensitive attribute S. The l-diversity principle requires that each equivalence class (as defined by kanonymity) also contain at least l "well-represented" distinct values for S. This principle can be instantiated in various ways. The strictest proposal formulates l-diversity in terms of entropy. Because entropy is concave, entropy l-diversity requires that the full database have entropy at least log(l). D S denotes the (finite) domain of attribute S.</p><p>Entropy l-Diversity (Nominal S) <ref type="bibr" target="#b18">[18]</ref> A table T is entropy l-diverse with respect to quasi-identifier set Q 1 , ..., Q d and sensitive attribute S if, for every equivalence class E in T , P s∈D S -p(s|E)log p(s|E) ≥ log(l), where p(s|E) is the fraction of tuples in E with S = s.</p><p>For numeric sensitive attributes, diversity is more subtle. For example, if S = Salary, an equivalence class containing salaries {100K, 101K, 102K} is considered 3-diverse, but intuitively does not protect privacy as well as an equivalence class containing salaries {1K, 50K, 500K}. For this reason, we define a new diversity requirement that guarantees a certain level of dispersion within each equivalence class: </p><formula xml:id="formula_1">Squared-Error Diversity (Numeric S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification &amp; Regression</head><p>In classification/regression, attributes are typically characterized by at most one of the following types:</p><p>• Target attribute (C or R) The goal of classification is to build a model that accurately predicts the value of a nominal class label (C). Regression aims to predict a numeric attribute (R).</p><p>• Predictor attributes Some set of (discrete or continuous) predictor attributes (also commonly called features) are used to predict the target attribute.</p><p>When a target classification or regression model is considered in conjunction with anonymity, each attribute has two</p><formula xml:id="formula_2">¡ ¢ £ ¤ ¥ ¦ § ¨ © ¦ § ¢ ¥ § ¥ ¢ ¦ § ¦ ¦ § § ¥ © ¥ ¦ ¤ ¦ § ¥ © ¥</formula><p>Figure <ref type="figure">1</ref>: A possible value generalization hierarchy for the Nationality domain characterizations. In the remainder of this paper, we will assume that the set of predictor attributes is a quasi-identifier. Under this assumption, it is contradictory to categorize an attribute as both target and sensitive, and we disallow this categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recoding</head><p>Numerous recoding techniques have been proposed for sanitizing microdata to satisfy an anonymity constraint. In a relational database, each attribute X has a domain of values D X . A global recoding achieves anonymity by mapping the quasi-identifier domains to ranges or coarsened values.</p><p>Global recoding can be broken down into two sub-classes <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>. If the quasi-identifier consists of d attributes (Q1, ..., Q d ), a single-dimensional global recoding is defined by a set of functions φ 1 , ..., φ d such that each φ i : D Q i → D . An anonymous view V of T is obtained by applying each φ i to the value of Q i in each tuple of T .</p><p>On the other hand, a multidimensional global recoding is defined by a single function φ : DQ 1 ×...×DQ d → D , which is used to recode the domain of unique vectors associated with the quasi-identifier. In this case, V is obtained by applying φ to the vector of quasi-identifier values in each tuple of T .</p><p>For attributes with continuous or ordinal (ordered categorical) domains, it is convenient to think of each vector of quasi-identifier values q 1 , ..., q d as a point in a ddimensional space. A class of multidimensional recoding models partitions the domain space into non-overlapping ddimensional rectangular regions <ref type="bibr" target="#b17">[17]</ref>. Recoding function φ is defined by mapping each point to the region in which it is contained. Thus, each region corresponds to an equivalence class in anonymous view V . 2  When the domain of a quasi-identifier attribute is nominal, this partitioning may be further constrained by a userdefined value generalization hierarchy, or partial order, as described by Samarati and Sweeney <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. For example, Figure <ref type="figure">1</ref> shows a possible hierarchy for the Nationality domain; the domain values are found at the leaves. The notation F rench European indicates that F rench is descended from European in the hierarchy.</p><p>The hierarchy can be used in several ways to constrain the set of possible recodings <ref type="bibr" target="#b16">[16]</ref>. In this paper, within a particular d-dimensional region, we require that if φ maps a leaf value v to some ancestor a, then all leaves that are descended from a must also be mapped to a.</p><p>Every single-dimensional recoding can be equivalently expressed as a multidimensional recoding, but the reverse is frequently not true <ref type="bibr" target="#b17">[17]</ref>. Depending on the distribution of the data, this can affect data quality. For example, consider a dataset with exactly two predictors/quasi-identifiers (Age and Zip). Suppose the distribution of class labels (+, -) is as shown in Figure <ref type="figure" target="#fig_0">2</ref>, and that k = 3. In this case, there is 2 Hyper-rectangular regions are easily expressed in tabular form using range values (e.g., Age = <ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref><ref type="bibr">[27]</ref><ref type="bibr">[28]</ref><ref type="bibr">[29]</ref><ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref>). a k-anonymous multidimensional recoding that groups together only records with like labels, but this cannot be accomplished with single-dimensional recoding, which requires that the values of each attribute be recoded uniformly.</p><formula xml:id="formula_3">¡ ¢ £ ¤ ¥( a) Multidimensional ¡ ¢ £ ¤ ¥ (b) Single-Dimensional</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WORKLOAD-AWARE ANONYMIZATION</head><p>This section proposes several algorithms for creating a single snapshot of a given data set that respects a given anonymity constraint, but remains useful for executing a particular class of workloads. The target class of workloads is specified by the following parameters:</p><formula xml:id="formula_4">1. A set of predictor attributes (Q 1 , ..., Q d ) 2.</formula><p>Either a set of one or more nominal target class labels (C 1 , ..., C m ), or numeric target attributes (R 1 , ..., R m ) 3. Optionally, a set of selection predicates (P R1, ..., P Rn)</p><p>The anonymity constraint is k-anonymity, optionally extended by l-diversity or squared-error diversity. Also, we assume that the predictor attributes are a quasi-identifier.</p><p>In the simplest case, when the target workload consists of one classification or regression model, without selection predicates, the heuristics used by our algorithms implement entropy l-diversity and squared-error diversity in reverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single Target Classification Model</head><p>The Mondrian algorithm was recently proposed for kanonymization using multidimensional recoding <ref type="bibr" target="#b17">[17]</ref>. The algorithm is based on a greedy recursive partitioning of the (multidimensional) quasi-identifier domain space (see Figure <ref type="figure" target="#fig_1">3</ref>). In order to obtain approximately uniform partition occupancy, <ref type="bibr" target="#b17">[17]</ref> suggests recursively choosing the split attribute with the largest normalized range of values, and (for continuous or ordinal attributes) partitioning the data around the median value of the split attribute. This process is repeated until no allowable split remains, meaning that a particular region cannot be further divided without violating the anonymity constraint, or constraints imposed by value generalization hierarchies. We refer to this algorithm as Median Mondrian.</p><p>When the (set of) target mining model(s) is known, we can improve this heuristic. First consider a single target classification model, with predictor attributes Q1, ..., Q d (also the quasi-identifier) and class label C. In this case, we propose a heuristic partitioning scheme based on information gain, which is reminiscent of decision tree construction. Intuitively, the goal of this greedy criterion is to produce homogeneous partitions of class labels.</p><p>At each recursive step, we choose the split that minimizes the weighted entropy over the set of resulting partitions (without violating the anonymity constraint). P denotes the current (recursive) tuple set, and partitions P denotes the set of partitions resulting from the candidate split. p(c|P ) is the fraction of tuples in P with class label C = c. We refer to this algorithm as InfoGain Mondrian.</p><formula xml:id="formula_5">Entropy(P, C) = X partitions P |P | |P | X c∈D C -p(c|P )logp(c|P )<label>(1)</label></formula><p>InfoGain Mondrian handles continuous quasi-identifier values as they are typically handled by decision-trees, partitioning around the threshold value with smallest entropy (see <ref type="bibr" target="#b12">[12]</ref>). The data is first sorted with respect to the split attribute. Then the data is scanned, and each time there is a change in class label, this candidate threshold is checked with respect to anonymity and entropy. In the event that no candidate threshold satisfies the anonymity constraint, the median is also checked as a default.</p><p>InfoGain Mondrian scales to large data sets through a straightforward adaptation of an existing scalable decisiontree induction scheme, such as RainForest <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Single Target Regression Model</head><p>Similar greedy heuristics can be used when the target attribute is numeric. Specifically, we use the mean squared error (MSE) to measure the impurity of target attribute R within a candidate partition P . A heuristic inspired by the CART algorithm for regression trees <ref type="bibr">[7]</ref> recursively chooses the split that minimizes the weighted sum of MSEs over the set of resulting partitions. r(P ) denotes the mean value of R in P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M SE(P</head><formula xml:id="formula_6">) = 1 |P | X i∈P (r i -r(P )) 2 W eighted M SE = X P artitions P |P | |P | (M SE(P )) = 1 |P | X P artitions P X i∈P (r i -r(P )) 2</formula><p>Because |P | is constant for all candidate splits, the algorithm chooses the split that minimizes the following expression (without violating anonymity). We call this Least Squared Deviance (LSD) Mondrian. This algorithm handles continuous attributes through discretization.</p><formula xml:id="formula_7">Error 2 (P, R) = X P artitions P X i∈P (r i -r(P )) 2<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Target Models</head><p>In certain cases, we would like to allow the data recipient to build several models, to accurately predict the marginal distributions of several class labels (C 1 , ..., C m ) or regression attributes (R 1 , ..., R m ). InfoGain Mondrian and LSD Mondrian can be extended to handle multiple discrete and numeric target attributes, respectively.</p><p>For classification, there are two ways to make this extension. In the first approach, the data recipient would build a single model to predict the vector of class labels, C 1 , ..., C m , which has domain D C 1 × ... × D Cm . A greedy split criterion would minimize entropy with respect to this single variable.</p><p>However, in this simple approach, the size of the domain grows exponentially with the number of target attributes.  To avoid potential problems due to data sparsity, we instead simplify the problem by assuming independence among target attributes. This is a reasonable assumption because we are ultimately only concerned about the marginal distribution of each target attribute. Under the independence assumption, a greedy split criterion minimizes the sum of weighted entropies:</p><formula xml:id="formula_8">m X i=1 Entropy(P, C i )<label>(3)</label></formula><p>In regression (the squared error split criterion in particular), there is no analogous distinction between treating the set of target attributes as a single variable and assuming independence. For example, if we have two target attributes, R1 and R2, the joint error is the distance between an observed point (r 1 , r 2 ) and the centroid (r 1 (P ), r 2 (P )) in 2dimensional space. The squared joint error is just the sum of individual squared errors, (r 1 -r 1 (P )) 2 + (r 1 -r 2 (P )) 2 . For this reason, the greedy split criterion minimizes the sum of squared error:</p><formula xml:id="formula_9">m X i=1 Error 2 (P, R i ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Selection</head><p>Sometimes one or more of the tasks in the target workload will use only a subset of the released data, and it is important that this data can be selected precisely, despite recoding. For example, a researcher may want to build a model using only males over 40, but this is difficult if the ages of some men are recoded to the range [30 -50]. This problem was originally described in <ref type="bibr" target="#b17">[17]</ref>.</p><p>Consider a set of selection predicates (P R1, ..., P Rm) defined by boolean functions of the quasi-identifier attributes</p><formula xml:id="formula_10">(Q 1 , ..., Q d ). Conceptually, each P R i defines a query region R i in the domain space such that R i = {p ∈ D Q 1 ×...×D Q d : P Ri(p) = true}.</formula><p>For the purposes of this work, we only consider selections for which the query region can be expressed as a hyper-rectangle. (Some additional selections can be decomposed into two or more hyper-rectangles, and incorporated as separate queries.)</p><p>A multidimensional recoding function φ divides the domain space into non-overlapping regions P1, ..., Pn. Formally, the recoding region</p><formula xml:id="formula_11">P i = {p ∈ D Q 1 × ... × D Q d : φ(p) = p i },</formula><p>where p i is a particular generalization of the Figure <ref type="figure">4</ref>: Selection example quasi-identifier vector. When evaluating P Ri over the recoded view V , it may be that no subset of the recoding regions can be combined to produce query region R i . Instead, it is intuitive to return the tuples from V that are contained in any recoding region overlapping R i . More formally,</p><formula xml:id="formula_12">Overlap(R i ) = ∪{P i : P i ∩ R i = φ} P R i (V ) = {φ(p) : φ(p) ∈ V ∧ p ∈ Overlap(R i )}</formula><p>Notice that this will often produce a larger result set than evaluating P Ri over the original table T ; the imprecision is the difference in size between these two result sets.</p><formula xml:id="formula_13">imprecision(P R i , {P 1 , ..., Pn}) = |P R i (V )| -|P R i (T )|<label>(5)</label></formula><p>For example, Figure <ref type="figure">4</ref> shows a 2-dimensional domain space. The shaded area represents a query region, and the tuples of T are represented by points. The recoding regions are bounded by dotted lines and numbered. Recoding regions 2, 3, and 4 overlap the query region. If we evaluated this query using the original data, the result set would include 6 tuples. However, evaluating the query using the recoded data yields 10 tuples, an imprecision of 4.</p><p>Ideally, the goal of selection-oriented anonymization is to divide the domain space into a set of (anonymous) recoding regions that minimize imprecision for the set of target predicates. We incorporate this goal into the Mondrian algorithm through a new greedy splitting heuristic. Specifically, at each recursive step, when partitioning a recursive region P , we choose the split that minimizes the total imprecision for the set of resulting regions {P 1 , ..., P n }:</p><formula xml:id="formula_14">m X i=1 imprecision(P R i , {P 1 , ..., P n }) (6)</formula><p>The algorithm proceeds until there is no allowable split that reduces the imprecision of the current partition P , and continuous attributes are handled through discretization. We will call this algorithm Selection Mondrian.</p><p>In practice, we expect this technique to be used most often for simple selections, such as breaking down health data by state. After incorporating selections, we continue to anonymize each resulting partition independently, using the appropriate classification-or regression-oriented algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>Our experimental evaluation has several goals, the first of which is to provide some insight about quality evaluation methodology. We describe an experimental protocol for evaluating an anonymization algorithm with respect to a target data mining workload, and we compare the results to those obtained using some simpler quality measures. The second goal is to evaluate the algorithms described in Section 3. In particular, we assess the impact of incorporating a set of target classification or regression models into the anonymization, and multidimensional recoding. Also, we evaluate the effectiveness of our algorithms with respect to selections, projections, and multiple target models.</p><formula xml:id="formula_15">¢ £ ¤ ¡ ¢ ¥ ¤ ¦ ¢ £ ¤ ¦ ¢ ¥ ¤ § ¨ ©</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>Given a target classification or regression workload, the most direct way to evaluate the quality of an anonymization is by training each target model using the anonymized data, and evaluating the resulting models using predictive accuracy (classification), mean absolute error (regression), or similar measures. We will call this methodology model evaluation. All of our model evaluation experiments follow a common protocol:</p><p>1. The data is first divided into training and testing sets (or 10-fold cross-validation sets), Ttrain and Ttest.</p><p>2. The anonymization algorithm determines recoding function φ using only the training set T train . Anonymous view V train is obtained by applying φ to T train .</p><p>3. The same recoding function φ is then applied to the testing set (T test ), yielding V test .</p><p>4. The classification or regression model is trained using V train , and tested using V test .</p><p>This experimental design is different from the setup used by Fung et al. <ref type="bibr" target="#b13">[13]</ref> for an important reason. In <ref type="bibr" target="#b13">[13]</ref>, the combined training and testing sets were anonymized using a single-dimensional recoding algorithm based on information gain. Following this step, the data was separated into training and testing sets. In our opinion, this setup is inappropriate for evaluating the anonymization algorithm because incorporating the test set when choosing a recoding is tantamount to looking at the test set while doing feature selection. Instead, all of our experiments hold out the test set during both the anonymization and training phases.</p><p>We used k-anonymity as the anonymity constraint, and we used the implementations of the following learning algorithms provided by the Weka software package <ref type="bibr" target="#b25">[25]</ref>:</p><p>• Decision Tree (J48) Default settings were used.</p><p>• Naive Bayes Supervised discretization was used for continuous attributes; otherwise all default settings were used.</p><p>• Random Forests Each classifier was comprised of 40 random trees, and all other default settings were used.</p><p>• Support Vector Machine (SMO) Default settings were used, including a linear kernel function.</p><p>• Linear Regression Default settings were used.</p><p>• Regression Tree (M5) Default settings were used.  In addition to model evaluation, we also measured certain characteristics of the anonymized training data to see if there was any correlation between these simpler measures and the results of the model evaluation. Specifically, we measured the average equivalence class size, and for classification tasks, we measured the conditional entropy of the class label given the partitioning:</p><formula xml:id="formula_16">H(C|P ) = X partitions p p(p) X classes c -p(c|p) log p(c|p) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning from Regions</head><p>When single-dimensional recoding is used, standard learning algorithms can be applied directly to the resulting point data, notwithstanding the "coarseness" of some points <ref type="bibr" target="#b13">[13]</ref>. Although multidimensional recoding techniques are more flexible, using the resulting hyper-rectangular data to train standard data mining models poses an additional challenge.</p><p>To address this problem, we make a simple observation. Because we restrict the recoding regions to include only ddimensional hyper-rectangles, each region can be uniquely represented as a point in (2 * d)-dimensional space. For example, Figure <ref type="figure" target="#fig_2">5</ref> shows a 2-dimensional rectangle, and its unique representation as a 4-tuple. This assumes a total order on the values of each attribute, similar to the assumption made by support vector machines.</p><p>Following this observation, we adopt a simple pre-processing technique for learning from regions. Specifically, we extend the recoding function φ to map data points to d-dimensional  Our primary goal in developing this technique is to establish the utility of our anonymization algorithms. There are many possible approaches to the general problem of learning from regions. For example, Zhang and Honavar proposed an algorithm for learning decision trees from attribute values at various levels of a taxonomy tree <ref type="bibr" target="#b26">[26]</ref>. However, a full comparison is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Data</head><p>Our first set of experiments used synthetic data based on the classification generator introduced by Agrawal et al. <ref type="bibr" target="#b3">[3]</ref>. Predictor/quasi-identifier attributes were generated according to the distributions described in Figure <ref type="figure" target="#fig_3">6</ref>, and class labels were generated as a function of the predictor values. We present results for four representative label functions, chosen from the original ten (functions 2,4,6,7). To simplify the evaluation, we applied the labeling functions deterministically, without injecting noise.</p><p>Notice that the basic labeling functions in Figure <ref type="figure" target="#fig_3">6</ref> include a number of constants (e.g., 75K). In order to get a more robust understanding of the behavior of the various anonymization algorithms, for functions 2, 4, and 6, we instead generated many independent data sets, varying the function constants independently at random over the range of the attribute.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> notes, for each predictor/quasi-identifier attribute, whether it was treated as continuous or nominal (with an associated generalization hierarchy) during anonymization.</p><p>In addition to the synthetic data, we also used several real-world data sets. The first was derived from a sample of the 2003 Public Use Microdata, distributed by the United States Census American Community Survey<ref type="foot" target="#foot_1">3</ref> , with target attribute Salary. This data was used for both classification and regression, and contained 49,657 records. For classification, we replaced the numeric Salary with a Salary class (&lt; 30K or ≥ 30K); approximately 56% of the data records had Salary &lt; 30K. For classification, this is similar to the Adult database from the UCI Machine Learning Repository <ref type="bibr" target="#b6">[6]</ref>, which has been used in numerous k-anonymity evaluations. However, we chose to compile a new data set that can be used for both classification and regression.</p><formula xml:id="formula_17">¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¡ ¦ § ¨ © £ ¤ ! " # # $ % &amp; # ' ( ) 0 1 0 2 3 4 5 3 6 3 7 2 8 9 @ 3 0 2 A 9 2 B ) 0 3 2 C 5 D A E B 0 3 2 A 9 2 B ) 0 3 2 (a) J48 ¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¡ ¦ § ¨ © £ ¤ ! " # # $ % &amp; # ' ( ) 0 1 0 2 3 4 5 3 6 3 7 2 8 9 @ 3 0 2 A 9 2 B ) 0 3 2 C 5 D A E B 0 3 2 A 9 2 B ) 0 3 2 (b) Naive Bayes ¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¡ ¦ § ¨ © £ ¤ ! " # # $ % &amp; # ' ( ) 0 1 0 2 3 4 5 3 6 3 7 2 8 9 @ 3 0 2 A 9 2 B ) 0 3 2 C 5 D A E B 0 3 2 A 9 2 B ) 0 3 2 (c) Random Forests ¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¡ ¦ § ¨ © £ ¤ ! " # # $ % &amp; # ' ( ) 0 1 0 2 3 4 5 3 6 3 7 2 8 9 @ 3 0 2 A 9 2 B ) 0 3 2 C 5 D A E B 0 3 2 A 9 2 B ) 0 3 2 (d) SVM</formula><p>The second real data set is the smaller Contraceptives database from the UCI Repository, which contained 1,473 records after removing those with missing values. This data includes nine socio-economic indicators, which are used to predict the choice of contraceptive method (long-term, shortterm, or none) among sampled Indonesian women. Summaries of both real data sets are provided in Figure <ref type="figure" target="#fig_4">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Previous Algorithms</head><p>InfoGain Mondrian and LSD Mondrian combine multidimensional recoding with classification-and regression-oriented splitting heuristics. In this section, we evaluate the effects of these two components through a comparison with two previous anonymization algorithms. All of the experiments in this section consider a single target model, constructed over the entire anonymized training set.</p><p>Several previous algorithms have incorporated a single target classification model while choosing a single-dimensional recoding <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>. To gage the impact of multidimensional recoding, we compared InfoGain Mondrian and the greedy Top-Down Specialization (TDS) algorithm <ref type="bibr" target="#b13">[13]</ref>. Also, multidimensional recoding was used in Median Mondrian <ref type="bibr" target="#b17">[17]</ref>, without regard to workload. We compare this to InfoGain Mondrian and LSD Mondrian to gage the effects of incorporating a target model.</p><p>Using the synthetic data, Figure <ref type="figure" target="#fig_5">8</ref> compares the predictive accuracy of classifiers trained on data produced by the different anonymization algorithms. In these experiments, we generated 100 independent training and testing sets, each containing 1000 records, and we fixed k = 25. The results are averaged across these 100 trials. For comparison, we also include the accuracies of classifiers trained on the (not anonymized) original data.</p><p>InfoGain Mondrian consistently outperforms both TDS and Median Mondrian, a result that is overwhelmingly significant based on a series of paired t-tests. It is important to note that the pre-processing step used to convert regions to points (Section 4.2) is only used for the multidimensional recodings; the classification algorithms run unmodified on the single-dimensional recodings produced by TDS <ref type="bibr" target="#b13">[13]</ref>. Thus, should a better technique be developed for learning from regions, this would improve the results for InfoGain Mondrian, but it would not affect TDS. <ref type="foot" target="#foot_2">4</ref>We performed a similar set of experiments using the realworld data. Figures <ref type="figure" target="#fig_6">9(a,</ref><ref type="figure">b,</ref><ref type="figure">c</ref>) show results for the Census classification data, for increasing k. The graphs show test set accuracy (averaged across 10 folds) for three learning algorithms. The variance across the folds was quite low, and the differences between InfoGain Mondrian and TDS, and between InfoGain Mondrian and Median Mondrian, were highly significant based on paired t-tests.</p><p>It is important to point out that in certain cases, notably Random Forests, the learning algorithm overfits the model when trained using the original data. For example, the model for the original data in Figure <ref type="figure" target="#fig_6">9</ref>(c) gets 97% accuracy on the training set, but only 73% accuracy on the test set. When overfitting occurs, it is not surprising that the models trained on anonymized data obtain higher accuracy because anonymization acts as a form of feature selection/construction. Interestingly, we also tried applying a traditional form of feature selection (ranked feature selection based on information gain) to the original data, Next, Figures <ref type="figure" target="#fig_6">9(d,</ref><ref type="figure">e</ref>) show conditional entropy and average equivalence class size measurements, averaged across the ten anonymized training folds of the Census classification data. Average equivalence class size, which does not take into account any characteristics of the workload, is not a very good indicator of model accuracy. Conditional entropy, which incorporates the target class label, is a lot better; low conditional entropy generally indicates higher accuracy.</p><p>We performed the same set of experiments using the Contraceptives database, and observed similar behavior. Info-Gain Mondrian yielded higher accuracy than TDS or Median Mondrian. Results for J48 are shown in Figure <ref type="figure" target="#fig_6">9</ref>(f). The remaining results are omitted due to space constraints.</p><p>For regression, we found that LSD Mondrian generally led to better models than Median Mondrian. Figure <ref type="figure" target="#fig_6">9</ref>(i) shows the mean absolute test set error for the M5 regression tree, using the Census regression data. A similar relative comparison was observed for linear regression, but the overall error was higher because Salary is non-linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multiple Target Models</head><p>In Section 3.3 we described a simple adaptation to the basic InfoGain Mondrian algorithm that allowed us to incorporate more than one target attribute, expanding the set of models for which a particular anonymization is "optimized." To evaluate this technique, we performed a set of experiments using the synthetic classification data, increasing the number of class labels.</p><p>Figure <ref type="figure">10</ref> shows average test set accuracies for J48. We first generated 100 independent training and testing sets, containing 1000 records each. We used synthetic labeling functions 2-6,7, and 9 from the Agrawal generator <ref type="bibr" target="#b3">[3]</ref>, randomly varying the constants in functions 2-6 as described in Section 4.3.</p><p>Each column in the figure <ref type="figure">(models A-G</ref>) represents the average of 25 random permutations of the synthetic functions. The anonymizations (rows in the figure) are "optimized" for an increasing number of target models. (For example, the anonymization in the bottom row is optimized exclusively for model A.) There are two important things to note from the chart, and similar behavior was observed for the other classification algorithms.</p><p>• Looking at each model (column) individually, when the model is included in the anonymization (above the bold line), test set accuracy is higher than when the model is not included (below the line). • we increase the number of included models (moving upward above the line within each column), the test set accuracy tends to decrease. This is because the quality of the anonymization with respect to each individual model is "diluted" by incorporating additional models.</p><formula xml:id="formula_18">¡ ¢ £ ¤ ¥ ¦ § ¨ © § ¨ § ¨ © ¨ § ¨ © § ¨ § ¨ ¨ § § ¨ § ¨ § ¨ § ¨ § ¨ § § § ¨ § ¨ § ¨ § ¨ § ¨ § § © ¨ © § ¨ ¨ ¨ § ¨ § ¨ § ¨ § © § © § ¨ § ¨ § ¨ © § © § § © ¨ § ¨ © § § ¨ © § ¨ § § § § © § § © ¨ § § § § § § ! " ! " ! # ! " ! # ! $ ! " ! # ! $ ! % ! " ! # ! $ ! % ! &amp; ! " ! # ! $ ! % ! &amp; ! '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Selection</head><p>In Section 3.4, we discussed the importance of preserving selections, and described an algorithm for incorporating rectangular selection predicates into an anonymization. We conducted an experiment using the synthetic data (1,000 generated records), but treating synthetic Function 2 as a selection predicate. Figure <ref type="figure">11</ref> shows the imprecision of this selection when evaluated using the recoded data. The figure shows results for data recoded using three different anonymization algorithms. The first algorithm is Median Mondrian, with greedy recursive splits chosen from amongst all of the quasi-identifier attributes. It also shows a restricted variation of Median Mondrian, where splits are made with respect to only Age and Salary. Finally, it shows the results of Selection Mondrian, incorporating Function 2 as three separate rectangular query regions. It is intuitive that imprecision increases with k, and that imprecision is reduced by incorporating the selection into the anonymization.</p><p>Incorporating selections can also affect model quality. In the absence of selections, InfoGain and LSD Mondrian choose recursive splits using a greedy criterion driven by the target model(s). When selections are included, the resulting partitions may not be the same as those that would be chosen based on the target model(s). In the worst case, there may be a selection on an attribute that is uncorrelated with the target attribute.</p><p>To test this intuition, we performed an experiment using the Census classification data. To simulate the effect of selections that are uncorrelated with the target model, we first assigned each training tuple to one of n groups, chosen uniformly at random. (We assume |Data| n ≥ k.) This mimics the behavior of Selection Mondrian for a set of equality selections on a new attribute, Group number, which takes values 1, ..., n. We then anonymized each group independently, using either InfoGain Mondrian or Median Mondrian. Once recodings were determined for each training group, we randomly assigned each test tuple to one of the n groups, and recoded the tuple using the recoding function for that group. Finally, we trained a single classification model using the full recoded training set (union of all training groups), and tested using the full recoded test set. This process was repeated for each of ten folds.</p><p>The results of this experiment for J48 are shown in Figure <ref type="figure" target="#fig_6">9</ref>(g), for increasing n and k = 50. As expected, accuracy decreases slightly as the number of selections (n) increases. However, several selections can be incorporated without large negative effects. Similar results were observed for the other classification algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Projection</head><p>Sometimes not every model constructed by the data recipient will use the full set of predictor attributes; rather, they will use a projected attribute subset. We conducted an experiment to compare anonymization algorithms when only a subset of the released predictor attributes is actually used. First, we ranked the attributes using the original data and a greedy information gain criterion. Then we removed the attributes in order, from most to least predictive, and constructed classification models using the remaining attributes. We fixed k = 100.</p><p>As expected, test set accuracy decreases as the most predictive attributes are dropped. However, the rate of this decline varies depending on the anonymization algorithm used. Figure <ref type="figure" target="#fig_6">9</ref>(h) shows the observed accuracies for J48 using the Census database. Because of the single-dimensional recoding pattern, which is known to preserve fewer attributes over non-uniform quasi-identifier distributions <ref type="bibr" target="#b17">[17]</ref>, this rate of decay is the most precipitous for TDS.</p><p>The results were similar for the other classification algorithms and the Contraceptives data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>The most closely-related work includes several algorithms that have incorporated a single classification model (constructed over the full data set) while choosing a k-anonymous single-dimensional recoding. The proposed algorithms include top-down <ref type="bibr" target="#b13">[13]</ref> and bottom-up <ref type="bibr" target="#b24">[24]</ref> greedy heuristic searches, and genetic algorithms <ref type="bibr" target="#b15">[15]</ref>. Each of these papers used the target classification model to evaluate the recoding. Additionally, other recent work suggested using a workload of aggregate queries as a tool for evaluating the quality of anonymizations <ref type="bibr" target="#b17">[17]</ref>.</p><p>Numerous other k-anonymization algorithms have been proposed <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. However, much of the previous work has sought to optimize simple general-purpose measures of quality, such as the size of equivalence classes, or the total number of generalizations/suppressions.</p><p>Aside from k-anonymity, a variety of other methods have been proposed for protecting individual privacy while allowing certain data mining tasks. One widely-studied approach is based on the randomized response paradigm <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b21">21]</ref>. The main advantage of generalization is that the released data is "truthful," though at a coarsened level of granularity. This allows additional workloads to be carried out using the data, including selection. Generalization also has similar advantages as compared to data swapping <ref type="bibr" target="#b20">[20]</ref>.</p><p>Several cluster-based techniques have also been proposed that are similar in spirit to k-anonymity. The condensation approach first divides the data into "condensation groups" with required minimal occupancy, and then generates point data based on the aggregate statistical properties of each group <ref type="bibr" target="#b1">[1]</ref>. Microaggregation first clusters the data into (ideally homogeneous) groups of required minimal occupancy, and then publishes the centroid of each group <ref type="bibr" target="#b10">[10]</ref>. However, neither of these approaches requires that the resulting groups be hyper-rectangular, nor do they handle categorical attributes with hierarchical generalization constraints.</p><p>Finally, privacy-preserving histogram sanitization was proposed with the similar goal of guaranteeing that individuals blend into a crowd, based on some suitable distance measure <ref type="bibr" target="#b8">[8]</ref>. However, the probabilistic privacy definition does not capture situations where the identification of even a single individual would be considered a breach, and the proof of privacy is highly dependent on the original data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>k-Anonymity and l-diversity are widely-studied techniques for protecting individual privacy in microdata release. Subject to the anonymity requirement, the data should remain as useful as possible with respect to the workload for which it will ultimately be used.</p><p>This paper provided algorithms for incorporating a class of target workloads, consisting of classification or regression models, as well as selection predicates, when generating an anonymous data recoding. An extensive experimental study validated the effectiveness of these algorithms with respect to a variety of workloads. Additionally, our results show that simple quality measures are not always indicative of data quality with respect to a particular workload.</p><p>This work also brought to light several interesting opportunities for future work. As described in Section 4.4, anonymization sometimes behaves as a form of feature selection or construction. This has some interesting implications because multidimensional recoding naturally leads to a form of feature selection where different attributes are conditionally retained (at varying levels of granularity) for different data subsets. In the future, it will be valuable to characterize the situations under which this approach leads to better predictive accuracy than traditional feature selection.</p><p>Additionally, our selection-oriented anonymization algorithm (Section 3.4) currently only supports selections that can be expressed as rectangular regions. Although we expect simple queries to be the most common, we are working to extend this algorithm to a more expressive class of queries.</p><p>Finally, a full study of the learning from regions problem is the topic of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing multidimensional and singledimensional recoding in two dimensions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Basic Mondrian algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mapping a d-dimensional rectangular region to 2 * d attributes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Synthetic predictor/quasi-identifier attributes and class label functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Summary of real-world data sets regions, and in turn, to map these regions to their unique representations as points in (2 * d)-dimensional space.Our primary goal in developing this technique is to establish the utility of our anonymization algorithms. There are many possible approaches to the general problem of learning from regions. For example, Zhang and Honavar proposed an algorithm for learning decision trees from attribute values at various levels of a taxonomy tree<ref type="bibr" target="#b26">[26]</ref>. However, a full comparison is beyond the scope of this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average predictive accuracy for models trained using anonymized synthetic data (k=25)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Comparing anonymization techniques using real-world data and this did not improve the accuracy of random forests for any number of chosen attributes. We suspect that this discrepancy is due to the flexibility of the recoding techniques. Single-dimensional recoding (TDS) is more flexible than traditional feature selection because it can incorporate attributes at varying levels of granularity. Multidimensional recoding is more flexible still because it (conditionally) incorporates different attributes for different data subsets.Next, Figures9(d,e) show conditional entropy and average equivalence class size measurements, averaged across the ten anonymized training folds of the Census classification data. Average equivalence class size, which does not take into account any characteristics of the workload, is not a very good indicator of model accuracy. Conditional entropy, which incorporates the target class label, is a lot better; low conditional entropy generally indicates higher accuracy.We performed the same set of experiments using the Contraceptives database, and observed similar behavior. Info-Gain Mondrian yielded higher accuracy than TDS or Median Mondrian. Results for J48 are shown in Figure9(f). The remaining results are omitted due to space constraints.For regression, we found that LSD Mondrian generally led to better models than Median Mondrian. Figure9(i) shows the mean absolute test set error for the M5 regression tree, using the Census regression data. A similar relative comparison was observed for linear regression, but the overall error was higher because Salary is non-linear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Average test set accuracy for multiple incorporated target models (J48, k=25)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table T is squared- error diverse with respect to quasi-identifier set Q 1 , ..., Q d and sensitive attribute S if, for every equivalence class E in T , P i∈E (si -s(E)) 2 ≥ error, where s(E) is the mean value of S in E, and error is the diversity parameter.</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We assume that Alice only receives one version of any given data set and that she does not collude with others receiving data distributions from the same source.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www.census.gov/acs/www/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that by mapping to 2 * d dimensions, we effectively expand the hypothesis space considered by the linear SVM. Thus, it is not surprising that this improves accuracy for the non-linear class label functions (Figure8(d)).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Our thanks to Bee-Chung Chen, Hector Corrada Bravo, Ted Wild, and Jude Shavlik for insightful conversations, to Jesse Davis for comments on an earlier draft of this paper, and to Benjamin Fung for providing an implementation of the TDS algorithm.</p><p>This work was supported by an IBM Ph.D. fellowship and National Science Foundation Grant IIS-0524671.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A condensation approach to privacy-preserving data mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anonymizing tables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Database mining: A performance perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Privacy-preserving data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data privacy through optimal k-anonymization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freidman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth International Group</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the utility of privacy-preserving histograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prediction cubes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical data-oriented microaggregation for statistical disclosure control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mateo-Sanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Privacy preserving mining of association rules</title>
		<author>
			<persName><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the handling of continuous-valued attributes in decision tree generation</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="87" to="102" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Top-down specialization for information and privacy preservation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RainForest: A framework for fast decision tree construction of large datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transforming data to satisfy privacy constraints</title>
		<author>
			<persName><forename type="first">V</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incognito: Efficient full-domain k-anonymity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mondrian multidimensional k-anonymity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venkitasubramaniam</surname></persName>
		</author>
		<title level="m">l-Diversity: Privacy beyond k-anonymity. In ICDE</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the complexity of optimal k-anonymity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Practical data-swapping: The first steps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maintaining data privacy in association rule mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Haritsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Protecting respondents&apos; identities in microdata release</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samarati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Achieving k-anonymity privacy protection using generalization and suppression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l Journal on Uncertainty, Fuzziness, and Knowledge-based Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="571" to="588" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bottom-up generalization: A data mining solution to privacy protection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning decision tree classifiers from attribute value taxonomies and partially specified data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
