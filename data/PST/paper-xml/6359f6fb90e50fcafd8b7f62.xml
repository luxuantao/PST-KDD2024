<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING ON LARGE-SCALE TEXT-ATTRIBUTED GRAPHS VIA VARIATIONAL INFERENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-26">26 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Meng Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<addrLine>5 Sea AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>7 HEC Montr?al</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Canadian Institute for Advanced Research (CIFAR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mila</forename><surname>-Qu?bec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Institute</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING ON LARGE-SCALE TEXT-ATTRIBUTED GRAPHS VIA VARIATIONAL INFERENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-26">26 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.14709v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by large language models and training GNNs on big graphs. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows to separately train the two modules but at the same time allows the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are ubiquitous in the real world. In many graphs, nodes are often associated with text attributes, resulting in text-attributed graphs (TAGs) <ref type="bibr" target="#b27">(Yang et al., 2021)</ref>. For example, in social graphs, each user might have a text description; in paper citation graphs, each paper is associated with its textual content. Learning on TAG has become an important research topic in multiple areas including graph learning, information retrieval, and natural language processing.</p><p>In this paper, we focus on a fundamental problem, learning effective node representations, which could be used for a variety of applications such as node classification and link prediction. Intuitively, a TAG is rich in textual and structural information, both of which could be beneficial for learning good node representations. The textual information presents rich semantics to characterize the property of each node, and one could use a pre-trained language model (LM) (e.g., BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>) as a text encoder. Meanwhile, the structural information preserves the proximity between nodes, and connected nodes are more likely to have similar representations. Such structural relationships could be effectively modeled by a graph neural network (GNN) via the message-passing mechanism. In summary, LMs leverage the local textual information of individual nodes, while GNNs use the global structural relationship among nodes.</p><p>An ideal approach for learning effective node representations is therefore to combine both the textual information and graph structure. One straightforward solution is to cascade an LM-based text encoder and GNN-based message-passing module and train both modules together. Such an approach is effective when graphs are small. However, it becomes problematic once the graphs become very large. This is because the size of the computational graph is proportional to the size of the graph structure between nodes as well as the language model capacity, and the memory complexity is proportional to the size of the observed graph and the number of parameters in the LM. Therefore, on real-world TAGs where various nodes are densely connected, the memory cost of this method would become unaffordable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>To address such a problem, multiple solutions have been proposed. These methods reduce either the capacity of LMs or the size of graph structures for GNNs. More specifically, some studies choose to fix the parameters of LMs without fine-tuning them <ref type="bibr" target="#b14">(Liu et al., 2020)</ref>. Some other studies reduce graph structures via edge sampling, and perform message-passing only on the sampled edges <ref type="bibr" target="#b32">(Zhu et al., 2021;</ref><ref type="bibr">Li et al., 2021a;</ref><ref type="bibr" target="#b27">Yang et al., 2021)</ref>. Despite the improved scalability, reducing the LM capacity or graph size would sacrifice the model effectiveness, leading to degraded performance of learning effective node representation. Therefore, we are wondering whether there exists a scalable and effective approach to integrate big LMs and GNNs on large text-attributed graphs.</p><p>In this paper, we propose such an approach, named Graph and Language Learning by Expectation Maximization GLEM. In the GLEM, instead of simultaneously training both the LMs and GNNs, we leverage a variational EM framework <ref type="bibr" target="#b15">(Neal &amp; Hinton, 1998)</ref> to alternatively update the two modules. Take the node classification task as an example. The LM uses local textual information of each node to learn a good representation for label prediction, which thus models local marginal label distributions of nodes. By contrast, for a node, the GNN leverages the labels and textual encodings of surrounding nodes for label prediction, and it essentially defines a global conditional label distribution. The two components are optimized to maximize a variational lower bound of the log-likelihood function, which can be achieved by alternating between an E-step and an M-step, where at each step we fix one component to update the other one. This separate training framework significantly improves the efficiency of GLEM, allowing it to scale up to real-world TAGs. In each step, one component presents pseudolabels of nodes for the other component to mimic. By doing this, GLEM can effectively distill the local textual information and global structural information into both components, and thus GLEM enjoys better effectiveness in node classification. We conduct extensive experiments on 3 benchmark datasets to demonstrate the superior performance of GLEM. Notably, by leveraging the merits of both graph learning and language learning, GLEM-LM achieves on par or even better performance than existing GNN models, GLEM-GNN achieves new state-of-the-art results on ogbn-arxiv and ogbn-product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Text-attributed graph (TAG) <ref type="bibr" target="#b27">(Yang et al., 2021</ref>) is a widely existing data format, where each node is associated with a text feature. Representation learning on TAGs has been attracting growing attention in graph machine learning, and one of the most important problems is node classification.</p><p>The problem can be formalized as a text representation learning task, where the goal is to use the text feature of each node for learning. Early works resort to convolutional neural networks <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b18">Shen et al., 2014)</ref> or recurrent neural networks <ref type="bibr" target="#b21">Tai et al. (2015)</ref>. Recently, with the superior performance of transformers <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> and pre-trained language models <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b28">Yang et al., 2019)</ref>, LMs have become the go-to model for encoding contextual semantics in the sentences for text representation learning. At the same time, the graph learning area has been vastly developed by graph neural networks (GNNs) <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b24">Velickovic et al., 2018;</ref><ref type="bibr" target="#b26">Xu et al., 2019;</ref><ref type="bibr" target="#b30">Zhang &amp; Chen, 2018;</ref><ref type="bibr" target="#b22">Teru et al., 2020)</ref>. A GNN takes numerical features as input and learns node representation by transforming representations and aggregating them according to the graph structure. With the capability of considering both node attributes and graph structure, GNNs have shown great performances in various applications, including node classification and link prediction. Nevertheless, LMs and GNNs only use parts of observed information (i.e., textual or structural) for representation learning, and the results remain to be improved.</p><p>There are some recent efforts focusing on the combination of GNNs and LMs, which allows one to enjoy the merits of both models. One widely adopted way is to encode the texts of nodes with a fixed LM, and further treat the LM embeddings as features to train a GNN for message passing. Recently, a few methods propose to utilize domain-adaptive pretraining <ref type="bibr" target="#b4">(Gururangan et al., 2020)</ref> on TAGs and predict the graph structure using LMs <ref type="bibr" target="#b2">(Chien et al., 2022;</ref><ref type="bibr" target="#b29">Yasunaga et al., 2022)</ref> to provide better LM embeddings for GNN. Despite better results, these LM embeddings still remain unlearnable in the GNN training phase. Such separated training paradigm ensures the model scalability as the number of trainable parameters in GNNs is relatively small. However, its performance is hindered by task and topology-irrelevant semantic modeling process. To overcome these limitations, endeavors have been made <ref type="bibr" target="#b32">(Zhu et al., 2021;</ref><ref type="bibr">Li et al., 2021a;</ref><ref type="bibr" target="#b27">Yang et al., 2021;</ref><ref type="bibr" target="#b1">Bi et al., 2021;</ref><ref type="bibr" target="#b16">Pang et al., 2022)</ref> to co-train GNNs and LM under a joint learning framework. However, such co-training approaches suffer from severe scalability issues as all the neighbors need to be encoded by language models from scratch, incurring huge extra computation cost. In practise, these models restrict the messagepassing to very few (e.g. 3 <ref type="bibr">(Li et al., 2021a;</ref><ref type="bibr" target="#b32">Zhu et al., 2021)</ref>) sampled first-hop neighbors, resulting in severe information loss.</p><p>To summarize, existing methods of fusing LMs and GNNs suffer from either unsatisfactory results or poor scalability. In contrast to these methods, GLEM uses a pseudo-likelihood variational framework to integrate an LM and a GNN, which allows the two components to be trained separately, leading to good scalability. Also, GLEM encourages the collaboration of both components, so that it is able to use both the textual semantics and structural semantics for representation learning, and thus enjoys better effectiveness.</p><p>Lastly, our work is also related to <ref type="bibr">GMNN Qu et al. (2019)</ref>, which also uses a pseudo-likelihood variational framework for node representation learning. However, GMNN aims to combine two GNNs for general graphs and it does not consider modeling textual features. Different from GMNN, GLEM focuses on TAGs, which are more challenging to deal with. Also, GLEM fuses a GNN and an LM, which can better leverage both structural features and textual features in a TAG, and thus achieves SOTA results on a few benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>In this paper, we focus on learning representations for nodes in TAGs, where we take node classification as an example for illustration. Before diving into the details of our proposed GLEM, we start with presenting a few basic concepts, including the definition of TAGs and how LMs and GNNs can be used for node classification in TAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TEXT-ATTRIBUTED GRAPH</head><p>Formally, a TAG G S = (V, A, s V ) is composed of nodes V and their adjacency matrix A ? R |V |?|V | , where each node n ? V is associated with a sequential text feature s n . In this paper, we study the problem of node classification on TAGs. Given a few labeled nodes y L of L ? V , the goal is to predict the labels y U for the remaining unlabeled objects U = V \ L.</p><p>Intuitively, node labels can be predicted by using either the textual information or the structural information, and representative methods are language models (LMs) and graph neural networks (GNNs) respectively. Next, we introduce the high-level ideas of both methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LANGUAGE MODELS FOR NODE CLASSIFICATION</head><p>Language models aim to use the sentence s n of each node n for label prediction, resulting in a text classification task <ref type="bibr" target="#b19">(Socher et al., 2013;</ref><ref type="bibr" target="#b25">Williams et al., 2018)</ref>. The workflow of LMs can be characterized as below:</p><formula xml:id="formula_0">p ? (y n |s n ) = Cat(y n | softmax(MLP ? (h n ))); h n = SeqEnc ? (s n ),<label>(1)</label></formula><p>where SeqEnc ? is a text encoder such as a transformer-based model <ref type="bibr" target="#b23">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b28">Yang et al., 2019)</ref>, which projects the sentence s n into a vector representation h n . Afterward, the node label distribution y n can be simply predicted by applying an MLP ? with a softmax function to h n .</p><p>Leveraging deep architectures and pre-training on large-scale corpora, LMs achieve impressive results on many text classification tasks <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Liu et al., 2019)</ref>. Nevertheless, the memory cost is often high due to large model sizes. Also, for each node, LMs solely use its own sentence for classification, and the interactions of nodes are ignored, leading to suboptimal results especially on nodes with insufficient text features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GRAPH NEURAL NETWORKS FOR NODE CLASSIFICATION</head><p>Graph neural networks approach node classification by using the structural interactions between nodes. Specifically, GNNs leverage a message-passing mechanism, which can be described as:</p><formula xml:id="formula_1">p ? (y n |A) = Cat(y n | softmax(MLP ? (h (L) n )); h (l) n = ?(AGG(MSG(h (l-1) NB(n) ), A)), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ? is an activation function, MSG(?) and AGG(?) stand for the message and aggregation functions respectively, NB(n) denotes the neighbor nodes of n. Given the initial node representations, a GNN iteratively performs update by applying the message function and the aggregation function, so that the learned node representations can well capture the structural interactions between nodes.</p><p>Afterward, an MLP is applied to the final node representation for label prediction.</p><p>With the message-passing mechanism, GNNs are able to effectively leverage the structural information for node classification. Despite the good performance on many graphs, GNNs are not able to well utilize the textual information, and thus GNNs often suffer on nodes with few neighbors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM training GNN training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-Step</head><p>Figure <ref type="figure">1</ref>: The proposed GLEM framework trains GNN and LM separately in a variational EM framework: In E-step, an LM is trained towards predicting both the gold and GNN predicted pseudo-labels; In M-step, a GNN is trained by predicting LM-inferenced pseudo-labels using the embeddings and labels predicted by LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we introduce our proposed approach which combines GNN and LM for node representation learning in TAGs. Existing methods either suffer from scalability issues or have poor results in downstream applications such as node classification. Therefore, we are looking for an approach which enjoys both good scalability and capacity.</p><p>Towards this goal, we take node classification as an example and propose GLEM. GLEM leverages a variational EM framework, where the LM uses the text information of each sole node to predict its label, which essentially models the local marginal label distribution of each node; whereas the GNN leverages the text and label information of surrounding nodes for label prediction, which characterizes the global conditional label distribution. The two modules are optimized by alternating between an E-step and an M-step. In the E-step, we fix the LM, and the GNN is optimized by using the node representations learned by the LM as features and the node labels inferred by the LM as target. By doing this, the GNN can effectively capture the global correlations of nodes for precise label prediction. In the M-step, we fix the GNN and let the LM mimic the labels inferred by the GNN, allowing the global knowledge learned by the GNN to be distilled into the LM. With such a framework, the LM and GNN can be trained separately, leading to better scalability. Meanwhile, the LM and GNN are encouraged to benefit each other, without the sacrifice of model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THE PSEUDOLIKELIHOOD VARIATIONAL FRAMEWORK</head><p>Our approach is based on a pseudo-likelihood variational framework, which offers a principled and flexible formalization for model design. To be more specific, the framework tries to maximize the log-likelihood function of the observed node labels, i.e., p(y L |s V , A). Directly optimizing the function is often hard due to the unobserved node labels y U , and thus the framework instead optimizes the evidence lower bound as below:</p><formula xml:id="formula_3">log p(y L |s V , A) ? E q(y U |s U ) [log p(y L , y U |s V , A) -log q(y U |s U )],<label>(3)</label></formula><p>where q(y U |s U ) is a variational distribution and the above inequality holds for any q. The ELBO can be optimized by alternating between optimizing the distribution q (i.e., E-step) and the distribution p (i.e., M-step). In the E-step, we aim at updating q to minimize the KL divergence between q(y U |s U ) and p(y U |s V , A, y L ), so that the above lower bound can be tightened. In the M-step, we then update p towards maximizing the following pseudolikelihood <ref type="bibr" target="#b0">(Besag, 1975)</ref> function:</p><formula xml:id="formula_4">E q(y U |s U ) [log p(y L , y U |s V , A)] ? E q(y U |s U ) [ n?V log p(y n |s V , A, y V \n ].<label>(4)</label></formula><p>The pseudo-likelihood variational framework yields a formalization with two distributions to maximize data likelihood. The two distributions are trained via a separate E-step and M-step, and thus we no longer need the end-to-end training paradigm, leading to better scalability which naturally fits our scenario. Next, we introduce how we apply the framework to node classification in TAGs by instantiating the p and q distributions with GNNs and LMs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PARAMETERIZATION</head><p>The distribution q aims to use the text information s U to define node label distribution. In GLEM, we use a mean-field form, assuming the labels of different nodes are independent and the label of each node only depends on its own text information, yielding the following form of factorization:</p><formula xml:id="formula_5">q ? (y U |s U ) = n?U q ? (y n |s n ).<label>(5)</label></formula><p>As introduced in Section 3.2, each term q ? (y n |s n ) can be modeled by a transformer-based LM q ? parameterized by ?, which effectively models the fine-grained token interactions by the attention mechanism <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>.</p><p>On the other hand, the distribution p defines a conditional distribution p ? (y n |s V , A, y V \n ), aiming to leverage the node features s V , graph structure A, and other node labels y V \n to characterize the label distribution of each node n. Such a formalization can be naturally captured by a GNN through the message-passing mechanism. Thus, we parameterize p ? (y n |s V , A, y V \n ) as a GNN p ? parameterized by ? to effectively model the structural interactions between nodes. Note that the GNN p ? takes the node texts s V as input to output the node label distribution. However, the node texts are discrete variables, which cannot be directly used by the GNN. Thus, in practice we first encode the node texts with the LM q ? , and then use the obtained embeddings as a surrogate of node texts for the GNN p ? .</p><p>In the following sections, we further explain how we optimize the LM q ? and the GNN p ? to let them collaborate with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">E-STEP: LM OPTIMIZATION</head><p>In the E-step, we fix the GNN and aim to update the LM to maximize the evidence lower bound. By doing this, the global semantic correlations between different nodes can be distilled into the LM.</p><p>Formally, maximizing the evidence lower bound with respect to the LM is equivalent to minimizing the KL divergence between the posterior distribution and the variational distribution, i.e., KL(q ? (y U |s U )||p ? (y U |s V , A, y L )). However, directly optimizing the KL divergence is nontrivial, as the KL divergence relies on the entropy of q ? (y U |s U ), which is hard to deal with. To overcome the challenge, we follow the wake-sleep algorithm <ref type="bibr" target="#b7">(Hinton et al., 1995)</ref> to minimize the reverse KL divergence, yielding the following objective function to maximize with respect to the LM q ? :</p><formula xml:id="formula_6">-KL(p ? (y U |s V , A, y L ))||q ? (y U |s U ) = E p ? (y U |s V ,A,y L ) [log q ? (y U |s U )] + const = n?U E p ? (yn|s V ,A,y L ) [log q ? (y n |s n )] + const,<label>(6)</label></formula><p>which is more tractable as we no longer need to consider the entropy of q ? (y U |s U ). Now, the sole difficulty lies in computing the distribution p ? (y n |s V , A, y L ). Remember that in the original GNN which defines the distribution p ? (y n |s V , A, y V \n ), we aim to predict the label distribution of a node n based on the surrounding node labels y V \n . However, in the above distribution p ? (y n |s V , A, y L ), we only condition on the observed node labels y L , and the labels of other nodes are unspecified, so we cannot compute the distribution directly with the GNN. In order to solve the problem, we propose Preprint to annotate all the unlabeled nodes in the graph with the pseudo-labels predicted by the LM, so that we can approximate the distribution as follows:</p><formula xml:id="formula_7">p ? (y n |s V , A, y L ) ? p ? (y n |s V , A, y L , ?U\n ),<label>(7)</label></formula><p>where ?U\n = {? s } s?U \n with each ?s ? q ? (y s |s s ).</p><p>Besides, the labeled nodes can also be used for training the LM. Combining it with the above objective function, we obtain the final objective function for training the LM:</p><formula xml:id="formula_8">O(q) = ? n?U E p(yn|s V ,A,y L ,? U \n ) [log q(y n |s n )] + (1 -?) n?L log q(y n |s n ),<label>(8)</label></formula><p>where ? is a hyperparameter. Intuitively, the second term n?L log q(y n |s n ) is a supervised objective which uses the given labeled nodes for training. Meanwhile, the first term could be viewed as a knowledge distilling process which teaches the LM by forcing it to predict the label distribution based on neighborhood text-information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">M-STEP: GNN OPTIMIZATION</head><p>During the GNN phase, we aim at fixing the language model q ? and optimizing the graph neural network p ? to maximize the pseudo-likelihood as introduced in equation 4.</p><p>To be more specific, we use the language model to generate node representations h V for all nodes and feed them into the graph neural network as text features for message passing. Besides, note that equation 4 relies on the expectation with respect to q ? , which can be approximated by drawing a sample ?U from q ? (y U |s U ). In other words, we use the language model q ? to predict a pseudo-label ?n for each unlabeled node n ? U , and combine all the labels {? n } n?U into ?U . With both the node representations and pseudo-labels from the LM q ? , the pseudo-likelihood can be rewritten as follows:</p><formula xml:id="formula_9">O(?) = ? n?U log p ? (? n |s V , A, y L , ?U\n ) + (1 -?) n?L log p ? (y n |s V , A, y L\n , ?U ),<label>(9)</label></formula><p>where ? is a hyperparameter which is added to balance the weight of the two terms. Again, the first term can be viewed as a knowledge distillation process which injects the knowledge captured by the LM into the GNN via all the pseudo-labels. The second term is simply a supervised loss, where we use observed node labels for model training.</p><p>Finally, the workflow of the EM algorithm is summarized in Fig. <ref type="figure">1</ref>. The optimization process iteratively does the E-step and the M-step. In the E-step, the pseudo-labels predicted by the GNN together with the observed labels are fed into the LM for model training. In the M-step, the LM provides both text embeddings and pseudo-labels for the GNN, which are treated as input and target respectively for label prediction. Once trained, both the GNN and the LM can be used for node label prediction. In our experiments, we empirically compare the performance of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate the proposed GLEM framework, where two settings are considered. The first setting is transductive node classification, where given a few labeled nodes in a TAG, we aim to classify the rest of the nodes. Besides that, we also consider a structurefree inductive setting, and the goal is to transfer models trained on labeled nodes to unseen nodes, for which we only observe the text attributes without knowing their connected neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETUP</head><p>Datasets. Three TAG node classification benchmarks are used in our experiment, including ogbnarxiv, ogbn-products, and ogbn-papers100M <ref type="bibr" target="#b8">(Hu et al., 2020)</ref>. The statistics of these dataset are shown in Table <ref type="table" target="#tab_0">1</ref>. Following the OGB benchmarking protocol, the mean and standard deviation of validation and test accuracy over three runs are reported.</p><p>Compared Methods. We compare against LMs, GNNs, and methods combining both of worlds.</p><p>For language models, we apply DeBERTa <ref type="bibr" target="#b6">He et al. (2021)</ref> to our setting by fine-tuning it on labeled nodes, and we denote it as LM-Ft. For GNNs, a few well-known GNNs are selected, i.e., GCN <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2017)</ref> and GraphSAGE <ref type="bibr" target="#b5">(Hamilton et al., 2017)</ref>. Three top-ranked baselines on leaderboards are included, i.e., RevGAT <ref type="bibr">(Li et al., 2021b)</ref>, GAMLP <ref type="bibr" target="#b31">(Zhang et al., 2022)</ref>, SAGN <ref type="bibr" target="#b20">(Sun &amp; Wu, 2021)</ref>. For each GNN, we try different kinds of node features, including (1) the raw feature of OGB, denoted as X OGB ;</p><p>(2) the LM embedding inferenced by pre-trained LM, i.e. the DeBERTa-base checkpoint<ref type="foot" target="#foot_0">1</ref> , denoted as X PLM ;</p><p>(3) the GIANT <ref type="bibr" target="#b2">(Chien et al., 2022)</ref> feature, denoted as X GIANT .</p><p>Implementation Details. We adopt the DeBERTa <ref type="bibr" target="#b6">(He et al., 2021)</ref> as the LM model and fine-tune it for node classification on each dataset to provide initial checkpoints for LMs. During optimization, GLEM can start with either the E-step or the M-step, and the priority is decided according to the performance. For example, if GNN outperforms LM, we will start with the M-step for GNN training. To provide embedding and predictions for the first GNN M-step, we use a pre-trained DeBERTa and fine-tune it on node classification on each tasks. To provide embedding and predictions for the first-LM E-step, we use a pre-trained GNN predictions, e.g. the original GNN predictions, for the initial target labels. The best EM-iteration is chosen based on the validation accuracy of GLEM-GNN. For fair comparison against other feature learning methods such as GIANT, the hyper-parameters of GNNs are set to the best settings described in the paper or in the official repository, and other parameters are tuned by grid search. The code to reproduce our results is available at https://github.com/AndyJZhao/GLEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRANSDUCTIVE NODE CLASSIFICATION</head><p>Main Results. Next, we evaluate GLEM in the transductive setting. The results of three OGB datasets are presented in Table <ref type="table" target="#tab_1">2</ref>. For LMs, we see that fine-tuned LMs (LM-Ft) have competitive results, showing the importance of text attributes in a TAG. By further leveraging the structural information for message passing, our approach (GLEM-LM) achieves significant improvement over LMs, which demonstrates its advantage over LMs.</p><p>For GNN-based methods, we see that for each GNN model, using OGB and GIANT node embeddings as GNN inputs (X OGB and X GIANT ) yields strong results. However, these embeddings remain Preprint  unchanged during training. By dynamically updating the LM to generate more useful node embeddings for the GNN, the GNN of our approach (GLEM-GNN) significantly outperforms all the other methods with fixed node embeddings in most cases. Notably, GLEM-GNN achieves new state-of-the-art results on ogbn-arxiv and ogbn-products.</p><p>Scalability. One key challenge of fusing LMs and GNNs lies in the scalability. When using large LMs with numerous parameters, the combined method will suffer from severe scalability problems. GLEM eases the challenge through the EM-based optimization paradigm, allowing it to be adapted to large LMs. To verify this claim, we train GLEM with DeBERTa-large <ref type="bibr" target="#b6">(He et al., 2021)</ref> on ogbn-arxiv. The results are reported in Table <ref type="table" target="#tab_2">3</ref>. We observe that GLEM is able to generalize to DeBERTa-large with about 0.4B parameters, showing the appealing scalability. Besides, for every LM, applying GLEM yields consistent improvement, which proves the effectiveness of GLEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">STRUCTURE-FREE INDUCTIVE NODE CLASSIFICATION</head><p>Besides the transductive setting, inductive settings are also important, where we aim to train models on nodes of a training graph, and then generalize models to new nodes of test graphs. In many real cases, these new nodes are often low-degree nodes or even isolated nodes, meaning that we can hardly use the structural information for node classification. Therefore, we consider a challenging setting named structure-free inductive setting, where we assume for each test node, we only observe its text attributes without any connected neighbors. For this setting, we consider different methods for label prediction, including GNN models (GNN), neural networks without using structural information (MLP), and GLEM. The results are shown in Table <ref type="table" target="#tab_3">4</ref>.</p><p>We can see that, compared with that of MLPs and LMs, the structure-free inductive setting is a more challenging task especially for GNNs where a sheer performance drop is observed. Meanwhile, by effectively fusing with graph learning, GLEM-LM is able to consider local semantics as well as neighboring structural information, leading to more accurate structure-free inductive predictions. Besides, the generated embeddings are able to boost other models (e.g., see X GLEM -deep in the MLP and LM sections), enabling both MLP and GNNs with better structure-free inference ability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">COMPARISON OF DIFFERENT TRAINING PARADIGMS</head><p>As discussed before, besides directly fine-tuning LM (denoted as LM-Ft), a few training paradigms have been proposed for fusing GNNs and LMs. One paradigm is to use fixed/static LMs to generate node embeddings for GNN to do label prediction (denoted as Static). Besides that, another paradigm is to restrict message passing to a few sampled neighbors, so that the memory cost can be reduced (denoted as Joint). In this section, we compare our proposed paradigm GLEM) against the others.</p><p>For each paradigm, we choose two models trained with it. The results are presented in Table <ref type="table" target="#tab_4">5</ref>. we see that although static training has the best efficiency, its classification accuracy is not very high due to the restricted model capacity caused by the fixed LM. On the other hand, the joint training paradigm has the worst efficiency and effectiveness due to the reduced graph structure. Finally, our proposed paradigm achieves the optimal classification results, thanks to its ability to encourage the collaboration of LMs and GNNs. Meanwhile, our paradigm remains close to static training in terms of efficiency (time/epoch). To summarize, our proposed approach achieves much better results than other paradigms without the sacrifice of efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">CONVERGENCE ANALYSIS</head><p>In GLEM, we utilize the variational EM algorithm for optimization, which consists of an E-step training GLEM-LM and an M-step training GLEM-GNN in each iteration. Here, we analyze the convergence of GLEM by looking into the training curves of validation accuracy on ogbn-arxiv and OGB-Products. From the results in Fig. <ref type="figure" target="#fig_1">2</ref>. We can clearly see that with each E-step and M-step, both the performance of GLEM-GNN and the GLEM-LM consistently increase to a maximum point and converge in a few iterations. Notably, GLEM takes only one iteration to converge on ogbn-arxiv dataset, which is very efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper studies how to fuse LMs and GNNs together for node representation learning in TAGs.</p><p>We propose an approach GLEM based on a pseudo-likelihood variational framework. GLEM alternatively updates the LM and GNN via an E-step and an M-step, allowing for better scalability. In each step, both GNN and LM are mutually enhanced by learning from pseudo-labels predicted by the other module, fusing graph and language learning together. Extensive experiments on multiple datasets in two settings demonstrate the effectiveness of GLEM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>by LM Pseudo-label by GNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The convergence curves of GLEM on OGB datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the OGB datasets<ref type="bibr" target="#b8">(Hu et al., 2020)</ref>.</figDesc><table><row><cell></cell><cell>#Nodes</cell><cell cols="3">#Edges Avg. Node Degree Train / Val / Test (%)</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>13.7</cell><cell>54 / 18 / 28</cell></row><row><cell>ogbn-products</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>50.5</cell><cell>8 / 2 / 90</cell></row><row><cell cols="3">ogbn-papers100M 111,059,956 1,615,685,872</cell><cell>29.1</cell><cell>78 / 8 / 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy for the obgn-arxiv and ogbn-products datasets. (mean ? std%, the best results are bolded and the runner-ups are underlined). G ? denotes the improvements of GLEM-GNN over the same GNN trained on X OGB ; L ? denotes the improvements of GLEM-LM over LM-Ft. "+" denotes additional tricks are implemented in the original GNN models.</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell></cell><cell></cell><cell>GNN</cell><cell></cell><cell></cell><cell></cell><cell>LM</cell></row><row><cell></cell><cell></cell><cell>X OGB</cell><cell>X GIANT</cell><cell>X PLM</cell><cell>GLEM-GNN</cell><cell>G ?</cell><cell>LM-Ft</cell><cell>GLEM-LM</cell><cell>L?</cell></row><row><cell></cell><cell>GCN</cell><cell cols="4">val 73.00 ? 0.17 74.89 ? 0.17 47.56 ? 1.91 76.86 ? 0.19 test 71.74 ? 0.29 73.29 ? 0.10 48.19 ? 1.47 75.93 ? 0.19</cell><cell cols="3">3.86 75.27 ? 0.09 76.17 ? 0.47 0.90 4.19 74.13 ? 0.04 75.71 ? 0.24 1.58</cell></row><row><cell>arxiv</cell><cell>SAGE GAMLP</cell><cell cols="7">val 72.77 ? 0.16 75.95 ? 0.11 56.16 ? 0.46 76.45 ? 0.05 test 71.49 ? 0.27 74.35 ? 0.14 56.39 ? 0.82 75.50 ? 0.24 val 62.20 ? 0.11 75.01 ? 0.02 71.14 ? 0.19 76.95 ? 0.14 14.75 75.27 ? 0.09 75.64 ? 0.30 0.44 3.68 75.27 ? 0.09 75.32 ? 0.04 0.6 4.01 74.13 ? 0.04 74.53 ? 0.12 1.44 test 56.53 ? 0.02 73.35 ? 0.14 70.15 ? 0.22 75.62 ? 0.23 19.09 74.13 ? 0.04 74.48 ? 0.41 2.04</cell></row><row><cell></cell><cell>RevGAT</cell><cell cols="4">val 75.01 ? 0.10 77.01 ? 0.09 71.40 ? 0.23 77.49 ? 0.17 test 74.02 ? 0.18 75.90 ? 0.19 70.21 ? 0.30 76.97 ? 0.19</cell><cell cols="3">2.48 75.27 ? 0.09 75.75 ? 0.07 0.48 2.95 74.13 ? 0.04 75.45 ? 0.12 1.32</cell></row><row><cell></cell><cell>SAGE</cell><cell cols="4">val 91.99 ? 0.07 93.47 ? 0.14 86.74 ? 0.31 93.84 ? 0.12 test 79.21 ? 0.15 82.33 ? 0.37 71.09 ? 0.65 83.16 ? 0.19</cell><cell cols="3">1.85 91.82 ? 0.11 92.71 ? 0.15 0.71 3.95 79.63 ? 0.12 81.25 ? 0.15 1.61</cell></row><row><cell>products</cell><cell>GAMLP</cell><cell cols="4">val 93.12 ? 0.03 93.99 ? 0.04 91.65 ? 0.17 94.19 ? 0.01 test 83.54 ? 0.09 83.16 ? 0.07 80.49 ? 0.19 85.09 ? 0.21</cell><cell cols="3">1.07 91.82 ? 0.11 90.56 ? 0.04 -1.26 1.55 79.63 ? 0.12 82.23 ? 0.27 2.60</cell></row><row><cell></cell><cell>SAGN+</cell><cell cols="4">val 93.02 ? 0.04 93.64 ? 0.05 92.78 ? 0.04 94.00 ? 0.03 test 84.35 ? 0.09 86.67 ? 0.09 84.20 ? 0.39 87.36 ? 0.07</cell><cell cols="3">0.98 91.82 ? 0.11 92.01 ? 0.05 0.21 3.01 79.63 ? 0.12 84.83 ? 0.04 5.17</cell></row><row><cell>papers</cell><cell>GAMLP GAMLP+</cell><cell cols="4">val 71.17 ? 0.14 72.70 ? 0.07 69.78 ? 0.07 71.71 ? 0.09 test 67.71 ? 0.20 69.33 ? 0.06 65.94 ? 0.10 68.25 ? 0.14 val 71.59 ? 0.05 73.05 ? 0.04 69.87 ? 0.06 72.11 ? 0.06 test 68.25 ? 0.11 69.67 ? 0.05 66.36 ? 0.09 68.91 ? 0.12</cell><cell cols="3">0.54 68.05 ? 0.03 69.94 ? 0.16 1.89 0.54 63.52 ? 0.06 64.80 ? 0.06 1.78 0.45 68.05 ? 0.03 70.00 ? 0.08 1.95 0.41 63.52 ? 0.06 65.42 ? 0.04 1.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experiments with DeBERTa-large as LM backbone, RevGAT as GNN backbone.</figDesc><table><row><cell>Methods</cell><cell cols="3">Val. accuracy Test accuracy # Parameters</cell></row><row><cell>GNN-XOGB</cell><cell>75.01 ? 0.10</cell><cell>74.02 ? 0.18</cell><cell>2,098,256</cell></row><row><cell>GNN-XGIANT</cell><cell>77.01 ? 0.09</cell><cell>75.90 ? 0.19</cell><cell>1,304,912</cell></row><row><cell>GLEM-GNN-base</cell><cell>77.49 ? 0.17</cell><cell>76.97 ? 0.19</cell><cell>1,835,600</cell></row><row><cell>GLEM-GNN-large</cell><cell>77.92 ? 0.06</cell><cell>77.62 ? 0.16</cell><cell>2,228,816</cell></row><row><cell>LM-base-Ft</cell><cell>75.27 ? 0.09</cell><cell>74.13 ? 0.04</cell><cell>138,632,488</cell></row><row><cell>LM-large-Ft</cell><cell>75.08 ? 0.06</cell><cell>73.81 ? 0.08</cell><cell>405,204,008</cell></row><row><cell>GLEM-LM-base</cell><cell>75.75 ? 0.07</cell><cell>75.45 ? 0.12</cell><cell>138,632,488</cell></row><row><cell>GLEM-LM-large</cell><cell>77.16 ? 0.04</cell><cell>76.80 ? 0.05</cell><cell>405,204,008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Structure free inductive experiments on ogbn-arxiv and ogbn-products. The validation and test accuracies, denoted as w/ struct and wo struct, and their relative differences (the lower the better) are reported. Boldfaced numbers indicate the best performances in each group.</figDesc><table><row><cell>Type</cell><cell>Methods</cell><cell></cell><cell>Arxiv</cell><cell></cell><cell></cell><cell>Products</cell></row><row><cell></cell><cell></cell><cell>w/ struct</cell><cell>wo struct</cell><cell>diff</cell><cell>w/ struct</cell><cell>wo struct</cell><cell>diff</cell></row><row><cell></cell><cell>XOGB</cell><cell cols="2">57.65 ? 0.12 55.50 ? 0.23</cell><cell>-2.15</cell><cell cols="2">75.54 ? 0.14 61.06 ? 0.08 -14.48</cell></row><row><cell></cell><cell>XLM-Ft</cell><cell cols="2">74.56 ? 0.01 72.98 ? 0.06</cell><cell>-1.58</cell><cell cols="2">91.79 ? 0.01 79.93 ? 0.22 -11.86</cell></row><row><cell>MLP</cell><cell cols="3">XGLEM-light 75.20 ? 0.03 73.32 ? 0.31</cell><cell>-1.88</cell><cell cols="2">91.96 ? 0.01 79.38 ? 0.14 -12.58</cell></row><row><cell></cell><cell cols="3">XGLEM-deep 75.57 ? 0.03 73.90 ? 0.08</cell><cell>-1.67</cell><cell cols="2">91.85 ? 0.02 80.04 ? 0.15 -11.81</cell></row><row><cell></cell><cell>XOGB-light</cell><cell cols="5">70.73 ? 0.02 48.59 ? 0.19 -22.14 90.54 ? 0.04 51.23 ? 0.17 -39.31</cell></row><row><cell></cell><cell cols="3">XGLEM-light 76.73 ? 0.02 73.94 ? 0.03</cell><cell>-2.79</cell><cell cols="2">92.95 ? 0.03 78.75 ? 0.39 -14.20</cell></row><row><cell>GNN</cell><cell>XOGB-deep</cell><cell cols="5">72.67 ? 0.03 50.92 ? 0.19 -21.75 91.85 ? 0.11 32.71 ? 2.23 -59.14</cell></row><row><cell></cell><cell cols="3">XGLEM-deep 76.79 ? 0.06 74.29 ? 0.11</cell><cell>-2.50</cell><cell>93.22 ? 0.03</cell><cell>79.81 ? 0.01 -13.41</cell></row><row><cell></cell><cell>Fine-tune</cell><cell cols="2">75.27 ? 0.09 74.13 ? 0.04</cell><cell>-1.14</cell><cell cols="2">91.82 ? 0.11 79.63 ? 0.12 -12.19</cell></row><row><cell>LM</cell><cell cols="3">GLEM-light 75.49 ? 0.11 74.50 ? 0.16 GLEM-deep 75.59 ? 0.08 74.60 ? 0.05</cell><cell>-0.99 -0.99</cell><cell cols="2">91.90 ? 0.06 79.53 ? 0.13 -12.37 91.81 ? 0.04 79.69 ? 0.51 -12.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different training paradigms of fusing LM and GNNs. The max batch size (bsz.) and time/epoch are tested on a single 32GB GPU.</figDesc><table><row><cell></cell><cell></cell><cell>LM-Ft</cell><cell>Static</cell><cell>Joint</cell><cell></cell><cell cols="2">GLEM</cell></row><row><cell>Datasets</cell><cell>Metric</cell><cell cols="6">DeBERTa-base SAGE-X OGB joint-BERT-tiny GraphFormers GLEM-G-step GLEM-L-step</cell></row><row><cell></cell><cell>val. acc.</cell><cell>75.27 ? 0.09</cell><cell>72.77 ? 0.16</cell><cell>71.58 ? 0.18</cell><cell>73.33 ? 0.06</cell><cell>76.45 ? 0.05</cell><cell>75.32 ? 0.04</cell></row><row><cell></cell><cell>test acc.</cell><cell>74.13 ? 0.04</cell><cell>71.49 ? 0.27</cell><cell>70.87 ? 0.12</cell><cell>72.81 ? 0.20</cell><cell>75.50 ? 0.24</cell><cell>74.53 ? 0.12</cell></row><row><cell>Arxiv</cell><cell>parameters max bsz.</cell><cell>138,632,488 30</cell><cell>218,664 all nodes</cell><cell>110,694,592 200</cell><cell>110,694,592 180</cell><cell>545,320 all nodes</cell><cell>138,632,488 30</cell></row><row><cell></cell><cell>time/epoch</cell><cell>2760s</cell><cell>0.09s</cell><cell>1827s</cell><cell>4824s</cell><cell>0.13s</cell><cell>3801s</cell></row><row><cell></cell><cell>val. acc.</cell><cell>91.82 ? 0.11</cell><cell>91.99 ? 0.07</cell><cell>90.85 ? 0.12</cell><cell>91.77 ? 0.09</cell><cell>93.84 ? 0.12</cell><cell>92.71 ? 0.15</cell></row><row><cell></cell><cell>test acc..</cell><cell>79.63 ? 0.12</cell><cell>79.21 ? 0.15</cell><cell>73.13 ? 0.11</cell><cell>74.72 ? 0.16</cell><cell>83.16 ? 0.19</cell><cell>81.25 ? 0.15</cell></row><row><cell>Products</cell><cell>parameters max bsz.</cell><cell>138,637,871 30</cell><cell>206,895 all nodes</cell><cell>110,699,975 100</cell><cell>110,699,975 100</cell><cell>548,911 80000</cell><cell>138,637,871 30</cell></row><row><cell></cell><cell>time/epoch</cell><cell>5460s</cell><cell>8.1s</cell><cell>8456s</cell><cell>12574s</cell><cell>153s</cell><cell>7740s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/microsoft/DeBERTa-base</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical analysis of non-lattice data</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="195" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging bidding graphs for advertiser-aware relevance modeling in sponsored search</title>
		<author>
			<persName><forename type="first">Shuxian</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxuan</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2215" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adsgnn: Behavior-graph augmented relevance modeling in sponsored search</title>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chirag</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Torsten</forename><surname>Suel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained fact verification with kernel graph attention network</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving relevance modeling via heterogeneous behavior graph learning in bing ads</title>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3713" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GMNN: graph markov neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with self-labelenhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Komal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><forename type="middle">G</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graphformers: Gnn-nested transformers for representation learning on textual graph</title>
		<author>
			<persName><forename type="first">Junhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Textgnn: Improving text encoder via graph neural network in sponsored search</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pelger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
