<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting</title>
				<funder>
					<orgName type="full">Infocomm Media Development Authority</orgName>
				</funder>
				<funder>
					<orgName type="full">Trust Tech Funding Initiative and A*STAR SERC Central Research Fund (UIBR)</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation, Singapore and Infocomm Media Development Authority</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-04">4 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
							<email>yufei.wang@students.mq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
							<email>axsun@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
							<email>li_bing@cfar.a-star.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kwok-Yan</forename><surname>Lam</surname></persName>
							<email>kwokyan.lam@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-04">4 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.01709v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the finetuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) have both complicated graph structures and rich textual information over the facts. Despite being large, many facts are still missing. Knowledge Graph Completion (KGC) is a fundamental task to infer the missing facts from the existing KG information.</p><p>Graph-based KGC models <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b41">Yang et al., 2015;</ref><ref type="bibr" target="#b7">Dettmers et al., 2018)</ref> represent entities and relations using trainable embeddings. These models are trained to keep the connections between entities and relations over structural paths, and tail entities are inferred via various transitional relations. Despite being effective in modelling KG structural information, these methods are unable to incorporate linguistic context. Recently, pretrained language models (PLMs) are applied to fill up this gap <ref type="bibr" target="#b42">(Yao et al., 2019;</ref><ref type="bibr">Wang et al., 2021a;</ref><ref type="bibr" target="#b38">Xie et al., 2022)</ref>. The proposed solutions often directly fine-tune the PLMs to choose the correct entities either relying on pure textual context or using structural add-ons as a complementary <ref type="bibr">(Wang et al., 2021a)</ref>. However, PLMs are normally equipped with large-scale parameters and linguistic inherence obtained from their pre-training stage. As a result, these PLM-based models remain overwhelmingly focusing on the textual information in KGs and tend to overlook the graph structure. For example, given an incompleted fact (Mona Lisa, painted by, ?), the PLM-based models may con-fuse between Leonardo DiCaprio and Leonardo da Vinci simply because they are textually similar. Thus, in this paper, we focus on the research question: Can we effectively fuse the KG structural information into the PLM-based KGC models?</p><p>To this end, we propose a novel CSProm-KG model (Conditional Soft Prompts for KGC) which is a structure-aware frozen PLMs that could effectively complete the KGC task. The core of CSProm-KG is Conditional Soft Prompt that is an structure-aware version of Soft Prompt <ref type="bibr" target="#b18">(Li and Liang, 2021;</ref><ref type="bibr" target="#b16">Lester et al., 2021)</ref>. Previously, Soft Prompt is a sequence of unconditional trainable vectors that are prepended to the inputs of frozen PLMs. Such design could effectively avoid the issue of over-fitting towards textual information caused by fine-tuning and allow the frozen PLMs to learn the downstream tasks <ref type="bibr" target="#b35">(Wang et al., 2022)</ref>. However, such naive Soft Prompts cannot represent any structural information in KG. To remedy this, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (c), we propose the prompt vectors conditioned on the KG entities and relations embeddings. Specifically, we use the entity and relation embeddings to generate Conditional Soft Prompts which are then fed into the frozen PLMs to fuse the textual and structural knowledge together. The fused Conditional Soft Prompts are used as inputs to the graph-based KGC model that produces the final entity ranking results. We further propose Local Adversarial Regularization to improve CSProm-KG to distinguish textually similar entities in KG.</p><p>We evaluate CSProm-KG on various KGC tasks and conduct experiments on WN18RR, FB15K-237 and Wikidata5M for Static KGC (SKGC), and on ICEWS14 and ICEWS05-15 for Temporal KGC (TKGC). CSProm-KG outperforms a number of competitive baseline models, including both graphbased and PLM-based models. We conduct ablation studies to show the strength of prompt-based methods against the fine-tuning counterparts and the effectiveness of each proposed components. We also demonstrate the flexibility of CSProm-KG with different graph-based models, and the training and inference efficiency of CSProm-KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph-based methods Graph-based methods represent each entity and relation with a continuous vector by learning the KG spatial structures. They use these embeddings to calculate the dis-tance between the entities and KG query to determine the correct entities. The training objective is to assign higher scores to true facts than invalid ones. In static KGC (SKGC) task, there are two types of methods: 1) Translational distance methods measure the plausibility of a fact as the distance between the two entities, <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b19">Lin et al., 2015;</ref><ref type="bibr" target="#b36">Wang et al., 2014)</ref>; 2) Semantic matching methods calculate the latent semantics of entities and relations <ref type="bibr" target="#b25">(Nickel et al., 2011;</ref><ref type="bibr" target="#b41">Yang et al., 2015;</ref><ref type="bibr" target="#b7">Dettmers et al., 2018)</ref>. In temporal KGC (TKGC) task, the systems are usually based on SKGC methods, with additional module to handle KG factual tuples timestamps <ref type="bibr" target="#b6">(Dasgupta et al., 2018;</ref><ref type="bibr" target="#b10">Goel et al., 2020;</ref><ref type="bibr" target="#b12">Han et al., 2021)</ref>.</p><p>PLM-based methods PLM-based methods represent entities and relations using their corresponding text. These methods introduce PLM to encode the text and use the PLM output to evaluate the plausibility of the given fact. On SKGC, <ref type="bibr" target="#b42">Yao et al. (2019)</ref> encode the combined texts of a fact, then a binary classifier is employed to determine the plausibility. To reduce the inference cost in <ref type="bibr" target="#b42">Yao et al. (2019)</ref>, <ref type="bibr">Wang et al. (2021a)</ref> exploit Siamese network to encode (h, r) and t separately. Unlike previous encode-only model, <ref type="bibr" target="#b38">Xie et al. (2022)</ref>; <ref type="bibr" target="#b26">Saxena et al. (2022)</ref> explore the Seq2Seq PLM models to directly generate target entity text on KGC task. <ref type="bibr" target="#b3">Brown et al. (2020)</ref> first find the usefulness of prompts, which are manually designed textual templates, in the GPT3 model. <ref type="bibr" target="#b32">Wallace et al. (2019)</ref>; <ref type="bibr" target="#b27">Shin et al. (2020)</ref> extend this paradigm and propose hard prompt methods to automatically search for optimal task-specific templates. However, the selection of discrete prompts involves human efforts and difficult to be optimized together with the downstream tasks in an end-toend manner. <ref type="bibr" target="#b18">(Li and Liang, 2021;</ref><ref type="bibr" target="#b16">Lester et al., 2021)</ref> relax the constraint of the discrete template with trainable continuous vectors (soft prompt) in the frozen PLM. As shown in <ref type="bibr" target="#b18">Li and Liang (2021)</ref>; <ref type="bibr" target="#b16">Lester et al. (2021)</ref>; <ref type="bibr" target="#b20">Liu et al. (2021)</ref>, frozen PLM with Soft Prompt could achieve comparative performance on various NLP tasks, despite having much less parameters than fully trainable PLM models. To the best of our knowledge, we are the first to apply Soft Prompt to PLM-based KGC model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first formulate Knowledge Graph Completion in Sec. 3.1. We then introduce CSProm-KG in Sec. 3.2 to Sec. 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Graph Completion</head><p>Knowledge graph (KG) is a directed graph with a collection of fact tuples. Let T = {V, R, L, M } be a KG instance, where V , R, L and M denote the entity, relation, edge (fact) and meta information set respectively. Each edge e ? L is (h, r, t, m) ? V ? R ? V ? M which connects head entity h and target entity t with relation type r, and is associated with meta information m. In Static KGs (SKG), no meta information is involved (i.e. M = ?). In Temporal KGs (TKG), each fact has a corresponding timestamp and M includes all fact timestamps. Knowledge Graph Completion (KGC) is to predict the target entity for KG queries (h, r, ?, m). The queries (?, r, t, m) are converted into (t, r -1 , ?, m), where r -1 is the inverse of r.</p><p>In this paper, CSProm-KG learns a score function f (h, r, t, m) : V ? R ? V ? M ? V that assigns a higher score for valid facts than the invalid ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CSProm-KG Overview</head><p>Motivated by the observation that Soft Prompts in a frozen PLM is effective in solving the over-fitting issue <ref type="bibr" target="#b35">(Wang et al., 2022)</ref>, we apply Soft Prompts in CSProm-KG to avoid the KGC models overly focusing on the textual information. Although several research initiatives have explored the utilization of both structural and textual information for NLP tasks <ref type="bibr" target="#b17">(Li et al., 2022;</ref><ref type="bibr" target="#b20">Xiao et al., 2021)</ref>, none of them is capable of solving the over-fitting issue over textual information in the context of KGC.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the architecture of CSProm-KG which includes three important components: a fully trainable Graph-based KGC model G, a frozen Pretrained language model (PLM) P , and a trainable Conditional Soft Prompt S. Firstly, the embeddings in G, which are explicitly trained to predict entities using structural knowledge, are used to generate the parameters of S. In this way, S is equipped with KG structural knowledge. We then feed the generated S, as well as the corresponding text of entities and relations, into P . Finally, the PLM outputs of S are extracted as the final inputs to G which produces final results for the KGC tasks. This allows the structural knowledge from G and the textual knowledge from P to be equally fused via S. To further improve the robustness of CSProm-KG, we propose Local Adversarial Regularization, which selects textually similar entities for training to be detailed shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-based KGC Model G</head><p>In CSProm-KG, the graph-based KGC models G represents KG entities and relations as continuous embeddings. Given a KG query (h, r, ?, m), we represent h and r as embeddings E e and E r ? R d where d is the embedding size. E e and E r are used at both inputs and outputs. At inputs, we use these embeddings to generate Conditional Soft Prompt which further interacts with the textual inputs of the frozen PLM P . At outputs, we use these embeddings to calculate f (h, r, t, m) which produces the entity ranking for KG queries. For example, when using ConvE as G, the corresponding f (h, r, t, m) is the dot-product between the representation of (h, r) and the tail entity embeddings. Note that, CSProm-KG is flexible enough to work well with any existing graph-based KGC models. We will show this flexibility in Sec. 4.4.</p><p>3.4 Pre-trained Language Model P Let's assume that the pre-trained language model P has l transformer layers with hidden size H. To represent a KG query (h, r, ?, m), we jointly represent h, r and m by extracting and concatenating their corresponding raw tokens, including their names and their corresponding descriptions if available.</p><p>We connect the texts of h and r with a special token [SEP], and feed the joint text into the frozen PLM P . For TKGC tasks, we simply add the event timestamp after the joint text of h and r. We show the effectiveness of this design choice in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conditional Soft Prompt S</head><p>Soft Prompt prepends a sequence of trainable embeddings at the inputs to a frozen Pre-trained Language model. <ref type="bibr" target="#b18">Li and Liang (2021)</ref> propose Layerwise Soft Prompt which inserts relatively short prompt sequences (e.g., 5 -10 vectors) at each layer and allows frequent interaction with the entities' and relations' textual information in PLMs.</p><p>Inspired by this, we propose a novel Conditional Soft Prompt which has k trainable vectors on each layer. Specifically, the i th input for the j th layer h j i ? R H is defined as:</p><formula xml:id="formula_0">h j i = ? ? ? s j i i ? k w i (i &gt; k) ? (j = 0) Trans(h j-1 : ) i</formula><p>Otherwise (1) where Trans(?) is the forward function of Transformer layer in P , w i is the fixed input word embedding vector and s j i is the i th prompt vector at j th layer. The Trans(?) works on the entire sequence (prompt + text). Conditional Soft Prompt is designed to connect with embeddings in G, we use the embeddings of entities and relations E e and E r to generate Conditional Soft Prompt S. Formally,</p><formula xml:id="formula_1">S = [F (E e ); F (E r )]</formula><p>(2)</p><formula xml:id="formula_2">F (x) = W out ? (ReLU(W in ? x))<label>(3)</label></formula><p>where W in ? R d h ?d and W out ? R (l * H * k/2)?d h are trainable weight matrices and d h is the middle hidden size for the mapping layers. We then re-organize F (E e ) and F (E r ) into a sequence of input embeddings and evenly distribute them into each PLM layer. In this process, the input tokens for P and Conditional Soft Prompt S are fully interacted with each other, allowing the structural knowledge in G (linearly mapped to S) and textual knowledge in P to be fully fused together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Local Adversarial Regularization</head><p>As PLMs are frozen, the model may lose part of flexibility in distinguishing textually similar entities via tuning of the Transformer layers. To enhance CSProm-KG's ability to distinguish textually similar entities, inspired by <ref type="bibr" target="#b11">(Goodfellow et al., 2015)</ref> </p><formula xml:id="formula_3">L l (h, r, t, m) = max(f (h, r, t, m) - 1 n n i=0 f (h, r, t ? i , m) + ?, 0)<label>(4)</label></formula><p>where t ? i is an sampled LAR entity of t, ? is the margin hyperparameter, n is the number of sampled LAR entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Training and Inference</head><p>For training, we leverage the standard cross entropy loss with label smoothing and LAR:</p><formula xml:id="formula_4">L c (h,r, t, m) = -(1 -?) ? log p(t|h, r, m) - ? |V | t ? ?V /t ? log p(t ? |h, r, m) (5) L = (h,r,t,m)?T L c (h, r, t, m) + ? ? L l (h, r, t, m)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR FB15K-237 Wikidata5M</head><p>MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10</p><p>Graph-Based Methods</p><p>TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> .243</p><p>. <ref type="bibr">043 .441 .532 .279 .198 .376 .441 .253 .170 .311</ref> .392 DistMult <ref type="bibr" target="#b41">(Yang et al., 2015)</ref> . <ref type="bibr">444 .412 .470 .504 .281 .199 .301 .446 .253 .209 .278</ref> .334 ComplEx <ref type="bibr" target="#b30">(Trouillon et al., 2016)</ref> . where p(t|h, r, m) m) , ? is the label smoothing value and ? is the LAR term weight. For inference, CSProm-KG first computes the representations for KG query (h, r, ?, m), then uses the entity embeddings in G to compute the entity ranking. While other PLM-Based KGC models such as StAR <ref type="bibr">(Wang et al., 2021a)</ref> requires |V | PLM forward pass computation for entity embeddings. Thus, CSProm-KG is more computationally efficient than these baselines (See Sec. 4.3).</p><formula xml:id="formula_5">= exp f (h,r,t,m) t ? ?V exp f (h,r,t ? ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first compare CSProm-KG with other competitive baselines in the SKGC and TKGC benchmarks in Sec. 4.1. We then conduct ablation studies to verify the effectiveness of our propose components in CSProm-KG in Sec. 4.2. We further show the efficiency and flexibility of CSProm-KG in Sec. 4.3 and 4.4, respectively.</p><p>Dataset WN18RR <ref type="bibr" target="#b7">(Dettmers et al., 2018)</ref> and FB15K-237 <ref type="bibr" target="#b29">(Toutanova and Chen, 2015)</ref> are the most popular SKGC benchmarks where all inverse relations are removed to avoid data leakage. Wiki-data5M <ref type="bibr">(Wang et al., 2021b</ref>) is a recently proposed large-scale SKGC benchmark. For TKGC, we use ICEWS14 <ref type="bibr" target="#b9">(Garc?a-Dur?n et al., 2018)</ref> and <ref type="bibr">ICEWS05-15 (Garc?a-Dur?n et al., 2018)</ref> which include political facts from the Integrated Crisis Early Warning System <ref type="bibr" target="#b2">(Boschee et al., 2015)</ref>. More dataset details are shown in Table <ref type="table" target="#tab_13">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>All the experiments are conducted on a single GPU (RTX A6000). We tune the learning rate ? ? {1e-3, 5e-4, 1e-4}, batch size B ? {128, 256, 384, 450}, prompt length P l ? {2, 5, 10} and LAR term weight ? ? {0.0, 0.1, 0.2}. While ? &gt; 0, we employ 8 LAR samples for each training instance and gradually increase the LAR term weight from 0 to ? using a step size of ? step = 1e-5. CSProm-KG uses the BERT-Large <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and ConvE <ref type="bibr" target="#b7">(Dettmers et al., 2018)</ref> model. We set the label smoothing to 0.1 and optimize CSProm-KG with AdamW <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2019)</ref>. We choose the checkpoints based on the validation mean reciprocal rank (MRR). We follow the filtered setting in <ref type="bibr" target="#b1">Bordes et al. (2013)</ref> to evaluate our model. Detailed model hyperparameters for each dataset are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main result</head><p>Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_2">2</ref> present the main SKGC and TKGC results, respectively, which demonstrate statistical significance (t-student test, p &lt; 0.05).</p><p>Results on SKGC As for the popular mediumsized KGC benchmarks, CSProm-KG achieves state-of-the-art or competitive performance compared with PLM-based KGC models. In particular, on FB15K-237, CSProm-KG consistently outperforms all PLM-based KGC models and achieves 6.5% (from 0.336 to 0.358) relative MRR im-ICEWS14 ICEWS05-15 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 Graph-Based Methods TTransE <ref type="bibr" target="#b15">(Leblay and Chekol, 2018)</ref> . Results of TKGC Table <ref type="table" target="#tab_2">2</ref> reports the experiment results on the ICEWS14 and ICEWS05-15 benchmarks. On ICEWS14, CSProm-KG substantially outperforms existing TKGC methods (e.g., at least 0.03 MRR higher than previous works). On ICEWS05-15, CSProm-KG is 0.028 and 0.045 higher than the best TKGC methods in terms of MRR and H@1, though being slightly worse on H@10 than Tero and ATiSE. On both benchmarks, CSProm-KG sets new state-of-the-art performance. Note that the TKGC baseline models are often specifically designed and optimized for the TKGC task, while the only modification to CSProm-KG is to add timestamp into its input. This further shows that our proposed CSProm-KG method is a generally strong solution for various of KGC tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>We conduct ablation study to show the effectiveness of our proposed components on WN18RR. Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure" target="#fig_3">5</ref> summarize the ablation study results. KG Query Structure As we discussed in Sec. 3, for each KG Query (h, r, ?, m), we jointly concatenate their textual information and feed them into the frozen PLM (as shown in Figure <ref type="figure">3</ref>). To demonstrate the effectiveness of this design choice, we replace it with a Separated Strategy that is similar to the Siamese network used in <ref type="bibr">Wang et al. (2021a)</ref>. That is, as shown in Figure <ref type="figure">4</ref>, we separately encode the textual information of h and r using PLMs. Table <ref type="table" target="#tab_3">3</ref> Line 2 shows the performance of this Sep- Role of Graph-based KGC Models Table <ref type="table" target="#tab_3">3</ref> Line 3 shows the performance of CSProm-KG without any graph-based KGC models. For this ablation, we directly use the outputs of PLM to predict the target entity. We observe that removing this graph-based KGC model leads to a performance drop (i.e., by 0.030 MRR and 0.033 H@10). This shows that even after the complex interaction in the PLMs, an appropriate graph-based KGC model could still provide additional useful structural knowledge. This experiment verifies the necessity of combining PLM-based and graph-based KGC models together.</p><p>Soft Prompt Design <ref type="bibr" target="#b16">Lester et al. (2021)</ref> recently propose another Soft Prompt variant which puts longer trainable vectors at the bottom input layer. We refer it as non-layer-wise Soft Prompt. Table <ref type="table" target="#tab_3">3</ref> Line 4 shows the performance using this variant on WN18RR. CSProm-KG with layer-wise soft prompt model outperforms the non-layer-wise counterpart by a large margin (i.e., 0.053 MRR and 0.066 H@10), which suggests that the layerwised Soft Prompt is more effective on KGC tasks. This could be explained by the fact that, to maintain similar trainable parameters, non-layer-wised Soft Prompt requires much longer prompt vector sequences at the input, while self-attention modules are often ineffective when handling long sequences <ref type="bibr" target="#b43">(Zaheer et al., 2020)</ref>.</p><p>Local Adversarial Regularization Table <ref type="table" target="#tab_3">3</ref> Lines 5 to 8 show the ablation for adversarial regularization. Line 5 shows CSProm-KG without LAR falls behind the full CSProm-KG model by 0.041 MRR, indicating the important of LAR. From Lines 6, 7, 8, we investigate the importance of LAR entity source. We observe that CSProm-KG with LAR entities that share common keywords (in name or description) outperforms the one with random LAR entities, indicating the importance of selecting appropriate adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLM Training Strategy</head><p>We empirically verify the effect of freezing PLM in CSProm-KG. Table <ref type="table" target="#tab_3">3</ref> Lines 9 -12 show the performance of CSProm-KG with different level of parameter frozen. In general, the more trainable parameters in CSProm-KG, the poorer CSProm-KG performs. CSProm-KG w/ fully fine-tuned drops significantly, by 0.138 MRR (Line 12). We further show the changes of performance as we increase the number of trainable parameters of the PLMs in Figure <ref type="figure" target="#fig_3">5</ref>. We freeze the PLM parameters starting from bottom layers (orange) and starting from top layers (blue).</p><p>Both experiments suggest that the performance of CSProm-KG remains nearly unchanged until the freezing operations are applied to the last few layers. As most of the layers frozen, the performance of CSProm-KG grows dramatically. Interestingly, we find freezing parameters from bottom layers performs slightly better than from top layers. This could be because lower layers in BERT could capture low-level semantics (e.g., phrase features) and this information is more beneficial to the KGC task. In summary, the frozen PLM prevents CSProm-KG from over-fitting the KG textual information, and therefore allows CSProm-KG to achieve substantial improvements in KGC tasks.   Furthermore, we conduct an investigation involving the utilization of a fully fine-tuned BERT to represent the input head entity and relation, without using prompt learning or a graph-based models. However, we find instability during the training process of this model, and consequently, the resulting model achieve very low performance compared to the results reported above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Efficiency</head><p>Table <ref type="table" target="#tab_7">5</ref> shows the model efficiency for CSProm-KG and other PLM-based KGC methods on a single RTXA6000 GPU. CSProm-KG requires much less training and evaluation time. Compared with KG-BERT <ref type="bibr" target="#b42">(Yao et al., 2019)</ref> and StAR <ref type="bibr">(Wang et al., 2021a)</ref>  KG-BERT and StAR require the PLM outputs to represent all KG entities, which introduces significant computational cost. In contrast, CSProm-KG only applies BERT to represent the input queries and directly uses entity embedding matrix to compute entity ranking. We also compare CSProm-KG with GenKGC <ref type="bibr" target="#b38">(Xie et al., 2022)</ref> and KG-S2S <ref type="bibr" target="#b4">(Chen et al., 2022)</ref>, recently proposed PLMbased Sequence-to-Sequence KGC models. They directly generate the correct entity names and does not require to use the outputs of PLMs to represent large-scale KG entities. However, it has to maintain a huge search space for the entity names during inference and becomes much slower than CSProm-KG (e.g., 0.2m vs. 104m and 115m). In summary, CSProm-KG maintains higher-level efficiency (as well as performance) compared to other PLM-based KGC methods with similar model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Flexibility to Graph-based KGC models</head><p>As we discussed in Sec. 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>In this section, we showcase how Conditional Soft Prompt could prevent CSProm-KG from overfitting to textual information.  <ref type="table" target="#tab_3">3</ref>). In the first case, CSProm-KG produces two different occupations that are relevant to the whaler in the KG Query, whilst CSProm-KG w/o Conditional Soft Prompt ranks two sea animal names as the outputs. This could be caused by the surface keywords seaman and ship in the KG Query. In the second case, the expected entity should be an award for the band Queen. CSProm-KG successful pick up the correct answer from many award entities using the existing KG structures, while CSProm-KG w/o Conditional Soft Prompt confuses in those candidates which are textually similar and unable to rank the groundtruth entity into top-2. In summary, CSProm-KG maintains a balance between textual and structural knowledge, while CSProm-KG w/o Conditional Soft Prompt often focuses too much on the textual information in the KG Query.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose CSProm-KG, a PLMbased KGC model that effectively fuses the KG structural knowledge and avoids over-fitting towards textual information. The key innovation of CSProm-KG is the Conditional Soft Prompt that connects between a graph-based KGC models and a frozen PLM avoiding the textual over-fitting issue. We conduct experiments on five popular KGC benchmarks in SKGC and TKGC settings and the results show that CSProm-KG outperforms several strong graph-based and PLM-based KGC models. We also show the efficiency and flexibility of CSProm-KG. For future work, we plan to adapt our method to other relevant knowledge-intensive downstream tasks, such as fact checking and openended question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>CSProm-KG successfully integrates both graphbased and textual representations in the KGC task, achieving substantial performance and efficiency improvement. However, similar to other PLMbased methods, this comes at the cost of increased computational resources (v.s. graph-based KGC models). In addition, we find that CSProm-KG may occasionally collapse on small KGC benchmarks (e.g. WN18RR) under specific random seeds. This is probably due to the nature of Soft Prompts, which involve much smaller number of trainable parameters, compared to fine-tuned models. However, we never see similar phenomena when training CSProm-KG in the large KGC benchmarks (e.g., Wikidata5M). We plan to solve these issues for CSProm-KG as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset</head><p>We use SKGC datasets released from <ref type="bibr" target="#b42">(Yao et al., 2019)</ref> and TKGC datasets from <ref type="bibr" target="#b9">(Garc?a-Dur?n et al., 2018)</ref>. We follow the original split in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>Hyperparameters are selected with grid search on the validation set. The optimal hyperparameters are presented in Table <ref type="table" target="#tab_14">9</ref> Dataset ? B P l ? WN18RR 5e-4 128 10 0.1 FB15K-237 5e-4 128 10 0.1 Wikidata5M 1e-4 450 5 0.0 ICEWS14 5e-4 384 5 0.1 ICEWS05-15 5e-4 384 5 0.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Methods</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Given head entity h and relation r, KGC is to find out the true tail entity t. Graph-based KGC models represent h and r as embeddings (rectangular boxes) to learn the KG structure information (Figure.a). PLMbased KGC models only feed the textual knowledge (triangle boxes) of h and r into the Pre-trained Language Model (PLM) to predict the missing entity (Figure.b). CSProm-KG fuses both types of information via the Soft Prompt and uses a graph-based KGC model to make the final prediction (Figure.c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of CSProm-KG for the KG query (Steve Jobs, Place of Birth, ?, 1955-02-24). CSProm-KG uses the embeddings of entities and relations (randomly initialized before training) to generate Conditional Soft Prompt. In the frozen PLMs, Conditional Soft Prompt fully interacts with the textual information of the KG queries. The outputs are fed into graph-based KGC model to make the final prediction. To improve CSProm-KG's ability in distinguishing textually similar entities, we further add LAR examples that are similar to the tail entities during training. CSProm-KG effectively learns both structural and textual knowledge in KG.</figDesc><graphic url="image-4.png" coords="3,166.04,91.96,253.84,95.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Joint Strategy used in CSProm-KG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The effect of parameter frozen on WN18RR. Orange and Blue lines indicate the performance when freezing parameters from bottom and top layers in PLM. The X-axis shows the number of frozen layers and the Y-axis shows the corresponding performance MRR.</figDesc><graphic url="image-34.png" coords="7,306.14,592.08,218.27,109.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we introduce an Adversarial Regularization term. Different from conventional adversarial regularization which generates virtual examples that do not exist, our adversarial examples are picked from the local entity set V that are of concrete mean-</figDesc><table /><note><p><p><p>ings. Specifically, given a KG query (h, r, ?, m) and ground-truth entity t, CSProm-KG treats entities that are textually similar to t as adversarial examples. We refer these samples as Local Adversarial Regularization (LAR) entities. To allow efficient training, we define LAR samples as the ones sharing the common tokens in entity names and descriptions with t, enabling us to pre-compute these LAR samples before training. This is different from previous works</p><ref type="bibr" target="#b24">(Miyato et al., 2017;</ref><ref type="bibr" target="#b23">Madry et al., 2018;</ref><ref type="bibr" target="#b11">Goodfellow et al., 2015)</ref> </p>that generate virtual adversarial examples using training perturbation with large computational costs. Specifically, the LAR training objective is:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of different baseline methods on the SKGC datasets. WN18RR and FB15K-237 results are taken fromWang et al. (2021a). Wikidata5M results are taken from<ref type="bibr" target="#b26">Saxena et al. (2022)</ref>. The best PLM-based method results are in bold and the second best results are underlined.</figDesc><table><row><cell></cell><cell>449</cell><cell cols="2">.409 .469</cell><cell>.530</cell><cell>.278</cell><cell cols="2">.194 .297</cell><cell>.450</cell><cell>.308</cell><cell>.255</cell><cell>-</cell><cell>.398</cell></row><row><cell>ConvE (Dettmers et al., 2018)</cell><cell>.456</cell><cell cols="2">.419 .470</cell><cell>.531</cell><cell>.312</cell><cell cols="2">.225 .341</cell><cell>.497</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE (Sun et al., 2019)</cell><cell>.476</cell><cell cols="2">.428 .492</cell><cell>.571</cell><cell>.338</cell><cell cols="2">.241 .375</cell><cell>.533</cell><cell>.290</cell><cell cols="2">.234 .322</cell><cell>.390</cell></row><row><cell cols="2">CompGCN (Vashishth et al., 2020) .479</cell><cell cols="2">.443 .494</cell><cell>.546</cell><cell>.355</cell><cell cols="2">.264 .390</cell><cell>.535</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PLM-Based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KG-BERT (Yao et al., 2019)</cell><cell>.216</cell><cell cols="2">.041 .302</cell><cell>.524</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.420</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MTL-KGC (Kim et al., 2020)</cell><cell>.331</cell><cell cols="2">.203 .383</cell><cell>.597</cell><cell>.267</cell><cell cols="2">.172 .298</cell><cell>.458</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>StAR (Wang et al., 2021a)</cell><cell>.401</cell><cell cols="2">.243 .491</cell><cell>.709</cell><cell>.296</cell><cell cols="2">.205 .322</cell><cell>.482</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MLMLM (Clou?tre et al., 2021)</cell><cell>.502</cell><cell cols="2">.439 .542</cell><cell>.611</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.223</cell><cell cols="2">.201 .232</cell><cell>.264</cell></row><row><cell>KEPLER (Wang et al., 2021b)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.210</cell><cell cols="2">.173 .224</cell><cell>.277</cell></row><row><cell>GenKGC (Xie et al., 2022)</cell><cell>-</cell><cell cols="2">.287 .403</cell><cell>.535</cell><cell>-</cell><cell cols="2">.192 .355</cell><cell>.439</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KGT5 (Saxena et al., 2022)</cell><cell>.508</cell><cell>.487</cell><cell>-</cell><cell>.544</cell><cell>.276</cell><cell>.210</cell><cell>-</cell><cell>.414</cell><cell>.300</cell><cell cols="2">.267 .318</cell><cell>.365</cell></row><row><cell>KG-S2S (Chen et al., 2022)</cell><cell>.574</cell><cell cols="2">.531 .595</cell><cell>.661</cell><cell>.336</cell><cell cols="2">.257 .373</cell><cell>.498</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSProm-KG</cell><cell>.575</cell><cell cols="2">.522 .596</cell><cell>.678</cell><cell>.358</cell><cell cols="2">.269 .393</cell><cell>.538</cell><cell>.380</cell><cell cols="2">.343 .399</cell><cell>.446</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of different baseline methods on the TKGC datasets. The results of baseline are obtained from original papers.</figDesc><table><row><cell></cell><cell>255</cell><cell>.074</cell><cell>-</cell><cell>.601</cell><cell>.271</cell><cell>.084</cell><cell>-</cell><cell>.616</cell></row><row><cell>HyTE (Dasgupta et al., 2018)</cell><cell>.297</cell><cell>.108</cell><cell>.416</cell><cell>.655</cell><cell>.316</cell><cell>.116</cell><cell>.445</cell><cell>.681</cell></row><row><cell>ATiSE (Xu et al., 2019)</cell><cell>.550</cell><cell>.436</cell><cell>.629</cell><cell>.750</cell><cell>.519</cell><cell>.378</cell><cell>.606</cell><cell>.794</cell></row><row><cell>DE-SimplE (Goel et al., 2020)</cell><cell>.526</cell><cell>.418</cell><cell>.592</cell><cell>.725</cell><cell>.513</cell><cell>.392</cell><cell>.578</cell><cell>.748</cell></row><row><cell>Tero (Xu et al., 2020)</cell><cell>.562</cell><cell>.468</cell><cell>.621</cell><cell>.732</cell><cell>.586</cell><cell>.469</cell><cell>.668</cell><cell>.795</cell></row><row><cell>TComplEx (Lacroix et al., 2020)</cell><cell>.560</cell><cell>.470</cell><cell>.610</cell><cell>.730</cell><cell>.580</cell><cell>.490</cell><cell>.640</cell><cell>.760</cell></row><row><cell>TNTComplEx (Lacroix et al., 2020)</cell><cell>.560</cell><cell>.460</cell><cell>.610</cell><cell>.740</cell><cell>.600</cell><cell>.500</cell><cell>.650</cell><cell>.780</cell></row><row><cell>T+TransE (Han et al., 2021)</cell><cell>.553</cell><cell>.437</cell><cell>.627</cell><cell>.765</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>T+SimplE (Han et al., 2021)</cell><cell>.539</cell><cell>.439</cell><cell>.594</cell><cell>.730</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PLM-Based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KG-S2S (Chen et al., 2022)</cell><cell>.595</cell><cell>.516</cell><cell>.642</cell><cell>.737</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSProm-KG</cell><cell>.628</cell><cell>.548</cell><cell>.677</cell><cell>.773</cell><cell>.628</cell><cell>.545</cell><cell>.678</cell><cell>.783</cell></row><row><cell></cell><cell></cell><cell>Noted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">that the improvement on FB15K-237 is barely com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">parable to that on WN18RR, and this discrepancy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">can be explained by the existence of Cartesian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Product Relations (CPRs) in FB15K-237, which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">are noisy and semantically meaningless relations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(Chen et al., 2022; Lv et al., 2022; Akrami et al.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2020). On the Wikidata5M benchmark, CSProm-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">KG significantly outperforms previous methods,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">showing the advantages of CSProm-KG on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">large-scale KGs. These results verify that with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">frozen PLM and accordingly much less trainable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">parameters, CSProm-KG can achieve remarkable</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">performance on various KGs with different scales.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>provement. These PLM-based baselines are all fully fine-tuned, indicating the importance of using parameter-effective prompts in the KGC task. Compared with graph-based methods, CSProm-KG outperforms baseline methods by a large margin on WN18RR (i.e. 0.575 v.s. 0.479 on MRR) and on FB15K-237 (i.e. 0.358 v.s. 0.355 on MRR).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study regarding important components in CSProm-KG on the benchmark of WN18RR.</figDesc><table><row><cell cols="2">No. Model</cell><cell cols="3">MRR H@1 H@10</cell></row><row><cell>1</cell><cell>CSProm-KG</cell><cell>.575</cell><cell>.522</cell><cell>.678</cell></row><row><cell>2</cell><cell>CSProm-KG w/ Separated Strategy</cell><cell>.520</cell><cell>.470</cell><cell>.622</cell></row><row><cell>3</cell><cell>CSProm-KG w/o Graph KGC model</cell><cell>.545</cell><cell>.495</cell><cell>.645</cell></row><row><cell>4</cell><cell>CSProm-KG w/ non-LW Soft Prompt</cell><cell>.522</cell><cell>.473</cell><cell>.612</cell></row><row><cell>5</cell><cell>CSProm-KG w/o LAR</cell><cell>.534</cell><cell>.489</cell><cell>.624</cell></row><row><cell>6</cell><cell>CSProm-KG w/ LAR from Name</cell><cell>.557</cell><cell>.513</cell><cell>.643</cell></row><row><cell>7</cell><cell>CSProm-KG w/ LAR from Description</cell><cell>.551</cell><cell>.501</cell><cell>.647</cell></row><row><cell>8</cell><cell>CSProm-KG w/ Random LAR</cell><cell>.545</cell><cell>.500</cell><cell>.630</cell></row><row><cell>9</cell><cell>CSProm-KG w/ the last layer tunable</cell><cell>.537</cell><cell>.494</cell><cell>.621</cell></row><row><cell cols="3">10 CSProm-KG w/ the last 4 layers tunable .437</cell><cell>.410</cell><cell>.488</cell></row><row><cell cols="3">11 CSProm-KG w/ the last 6 layers tunable .441</cell><cell>.415</cell><cell>.493</cell></row><row><cell cols="2">12 CSProm-KG w/ fully finetune</cell><cell>.436</cell><cell>.409</cell><cell>.484</cell></row><row><cell cols="2">13 Ensemble model</cell><cell>.481</cell><cell>.549</cell><cell>.630</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Prompt length study of CSProm-KG on WN18RR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, CSProm-KG is 10x faster in training and 100x faster in evaluation. This is because both</figDesc><table><row><cell>Method</cell><cell>PLM</cell><cell cols="3">#Total #Trainable T/Ep</cell><cell>Inf</cell></row><row><cell>KG-BERT</cell><cell cols="2">RoBERTa base 125M RoBERTa large 355M</cell><cell cols="3">125M 355M 142m 2928m 79m 954m</cell></row><row><cell>StAR</cell><cell cols="2">RoBERTa base 125M RoBERTa large 355M</cell><cell cols="2">125M 355M 103m 42m</cell><cell>27m 34m</cell></row><row><cell>GenKGC</cell><cell>BART base BART large</cell><cell>140M 400M</cell><cell>140M 400M</cell><cell>5m 11m</cell><cell>88m 104m</cell></row><row><cell>KG-S2S</cell><cell>T5 base T5 large</cell><cell>222M 737M</cell><cell>222M 737M</cell><cell>10m 27m</cell><cell>81m 115m</cell></row><row><cell>CSProm-KG</cell><cell>BERT base BERT large</cell><cell>126M 363M</cell><cell>17M 28M</cell><cell>4m 12m</cell><cell>0.1m 0.2m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of model efficiency for CSProm-KG and other PLM-based methods on WN18RR with FP32 precision. #Total and #Trainable denotes the total and trainable parameters, respectively. T/Ep and Inf denotes the training time per epoch and inference time.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>KG .499 ?.256 .462 ?.419 .515 ?.074 .569 ?.037 KG .543 ?.099 .494 ?.082 .562 ?.092 .639 ?.135 KG .575 ?.119 .522 ?.103 .596 ?.126 .678 ?.147 WN18RR results of CSProm-KG with different graph-based methods.</figDesc><table><row><cell>3, CSProm-KG is able</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table 7 lists the top two entities ranked by CSProm-KG and CSProm-KG w/o Conditional Soft Prompt (i.e., CSProm-KG w/ FT in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>: Grammy Award for Best Pop Performance by Group with Vocal [...] A2: MTV Video Music Award for Best Visual Effects [the following is ...] CSProm-KG w/o Conditional Soft Prompt: A1: Grammy Award for Best Music Film [the grammy award for best ...] A2: Razzie Award for Worst Original Song [the razzie award for worst...]</figDesc><table><row><cell>KG Query:</cell></row><row><cell>whaler [a seaman who works on a ship that hunts whales] | hypernym</cell></row><row><cell>CSProm-KG:</cell></row><row><cell>A1  *  : tar [a man who serves as a sailor]</cell></row><row><cell>A2: crewman [a member of a flight crew]</cell></row><row><cell>CSProm-KG w/o Conditional Soft Prompt:</cell></row><row><cell>A1: pelagic bird [bird of the open seas]</cell></row><row><cell>A2: mackerel [any of various fishes of the family scombridae]</cell></row><row><cell>KG Query:</cell></row><row><cell>Queen [queen are a british rock band formed in london in 1970 ...] | award</cell></row><row><cell>CSProm-KG:</cell></row><row><cell>A1</cell></row></table><note><p>*  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Case study of CSProm-KG. Texts in brackets are entity descriptions. * denotes ground-truth entity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table 8 shows the statistics of the datasets. All of these datasets are open-source English-written sources without any offensive content. They are introduced only for research use.</figDesc><table><row><cell>Dataset</cell><cell>|E|</cell><cell>|R|</cell><cell>|Train|</cell><cell>|Valid|</cell><cell>|Test|</cell></row><row><cell>SKGC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell>FB15K-237</cell><cell>14,541</cell><cell>237</cell><cell>272,115</cell><cell cols="2">17,535 20,466</cell></row><row><cell>Wikidata5M</cell><cell cols="4">4,594,485 822 20,614,279 5,163</cell><cell>5,133</cell></row><row><cell>TKGC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICEWS14</cell><cell>6,869</cell><cell>230</cell><cell>72,826</cell><cell>8,941</cell><cell>8,963</cell></row><row><cell>ICEWS05-15</cell><cell>68,544</cell><cell>358</cell><cell>189,635</cell><cell>1,004</cell><cell>2,158</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Statistics of the Datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Optimal hyperparameters.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank the anonymous reviewers for their insightful suggestions to improve this paper. This research / project is supported by the <rs type="funder">National Research Foundation, Singapore and Infocomm Media Development Authority</rs> under its <rs type="funder">Trust Tech Funding Initiative and A*STAR SERC Central Research Fund (UIBR)</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of <rs type="institution">National Research Foundation, Singapore</rs> and <rs type="funder">Infocomm Media Development Authority</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realistic reevaluation of knowledge graph completion methods: An experimental study</title>
		<author>
			<persName><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Samiul Saeef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3380599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference</title>
		<meeting>the 2020 International Conference on Management of Data, SIGMOD Conference 2020, online conference<address><addrLine>Portland, OR, USA]</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-14">2020. June 14-19, 2020</date>
			<biblScope unit="page" from="1995" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Lautenschlager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Shellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Starz</surname></persName>
		</author>
		<author>
			<persName><surname>Ward</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/28075</idno>
		<title level="m">ICEWS Coded Event Data</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Harvard Dataverse</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge is flat: A seq2seq generative framework for various knowledge graph completion</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Yan</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022</title>
		<meeting>the 29th International Conference on Computational Linguistics, COLING 2022<address><addrLine>Gyeongju</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-10-12">2022. October 12-17, 2022</date>
			<biblScope unit="page" from="4005" to="4017" />
		</imprint>
	</monogr>
	<note>Republic of Korea. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MLMLM: link prediction with mean likelihood masked language model</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Clou?tre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Trempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.378</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="page" from="4321" to="4331" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyte: Hyperplane-based temporally aware knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Swayambhu</forename><forename type="middle">Nath</forename><surname>Shib Sankar Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1225</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="2001" to="2011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastijan</forename><surname>Dumancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="4816" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diachronic embedding for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3988" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Time-dependent entity embedding is not all you need: A re-evaluation of temporal knowledge graph completion models under a unified framework</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.639</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="8104" to="8118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning for knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.153</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
		<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08">2020. December 8-13, 2020</date>
			<biblScope unit="page" from="1737" to="1743" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensor decompositions for temporal knowledge base completion</title>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deriving validity time in knowledge graph</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melisachew</forename><forename type="middle">Wudage</forename><surname>Chekol</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3191639</idno>
	</analytic>
	<monogr>
		<title level="m">Companion of the The Web Conference 2018 on The Web Conference 2018</title>
		<meeting><address><addrLine>Lyon , France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23">2018. April 23-27, 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1771" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sk2: Integrating implicit sentiment knowledge and explicit syntax knowledge for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengwei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511808.3557452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;22</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1114" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25">2015. January 25-30, 2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/2110.07602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do pretrained models benefit knowledge graph completion? a reliable evaluation and a reasonable approach</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3570" to="3581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML 2011</title>
		<meeting>the 28th International Conference on Machine Learning, ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011-06-28">2011. June 28 -July 2, 2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence knowledge graph completion and question answering</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.10321</idno>
		<idno>CoRR, abs/2203.10321</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
		<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing NLP</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1221</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">2021a. Structure-augmented text representation learning for efficient knowledge graph completion. In WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450043</idno>
		<ptr target="ACM/IW3C2" />
		<imprint>
			<date type="published" when="2021">April 19-23, 2021</date>
			<biblScope unit="page" from="1737" to="1748" />
			<pubPlace>Slovenia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">KEPLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00360</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PromDA: Prompt-based data augmentation for lowresource NLU tasks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.292</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4242" to="4255" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Qu?bec City, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014-07-27">2014. July 27 -31, 2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BERT4GCN: Using BERT intermediate layers to augment GCN for aspect-based sentiment classification</title>
		<author>
			<persName><forename type="first">Zeguan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congjian</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9193" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">From discrimination to generation: Knowledge graph completion with generative transformer</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2202.02113</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Temporal knowledge graph embedding model based on additive time series decomposition</title>
		<author>
			<persName><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Nayyeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fouad</forename><surname>Alkhoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed Shariat</forename><surname>Yazdi</surname></persName>
		</author>
		<idno>CoRR, abs/1911.07893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tero: A time-aware knowledge graph embedding via temporal rotation</title>
		<author>
			<persName><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Nayyeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fouad</forename><surname>Alkhoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed Shariat</forename><surname>Yazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
		<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1583" to="1593" />
		</imprint>
	</monogr>
	<note>December 8-13. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno>CoRR, abs/1909.03193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">We also compare CSProm-KG against several competitive PLM-based methods</title>
		<author>
			<persName><forename type="first">Csprom-Kg ;</forename><surname>Bordes</surname></persName>
			<affiliation>
				<orgName type="collaboration">) and CompGCN</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">For TKGC, we compare CSProm-KG with graph-based TKGC baselines, including: TTransE (Leblay and Chekol</title>
		<title level="s">variety of stateof-the-art baseline methods on SKGC and TKGC tasks. For SKGC, we include popular graph-based methods, i.e. TransE</title>
		<editor>
			<persName><forename type="first">(</forename><surname>Star</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>GenKGC</addrLine></address></meeting>
		<imprint>
			<publisher>DE-SimplE</publisher>
			<date type="published" when="2013">2013. 2015. 2016. 2019. 2020. 2019. 2020. 2022. 2022. 2018. 2019. 2020. 2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>TNTComplEx. T+TransE (Han et al., 2021), T+SimplE (Han et al., 2021). PLM-based baselines for TKGC includes KG-S2S (Chen et al., 2022</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
