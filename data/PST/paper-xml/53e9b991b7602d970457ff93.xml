<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrated Management of Application Performance, Power and Cooling in Data Centers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gmach</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Hyser</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhikui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cullen</forename><surname>Bash</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Hoover</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharad</forename><surname>Singhal</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hewlett-Packard Laboratories</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrated Management of Application Performance, Power and Cooling in Data Centers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20E3ABFB28037DD59BEF02BA81C00DC1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data centers contain IT, power and cooling infrastructures, each of which is typically managed independently. In this paper, we propose a holistic approach that couples the management of IT, power and cooling infrastructures to improve the efficiency of data center operations. Our approach considers application performance management, dynamic workload migration/consolidation, and power and cooling control to "right-provision" computing, power and cooling resources for a given workload. We have implemented a prototype of this for virtualized environments and conducted experiments in a production data center. Our experimental results demonstrate that the integrated solution is practical and can reduce energy consumption of servers by 35% and cooling by 15%, without degrading application performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Data centers are proliferating worldwide as a result of the demand for IT services. We have previously reported a breakdown of the costs involved in building and operating data centers <ref type="bibr" target="#b0">[1]</ref>. In this work, we found that the power required to run the IT equipment and cooling equipment in a data center is a significant portion of the variable costs of operation. As such, the reduction of power consumption is of critical concern to data center owners and operators.</p><p>Traditionally, the management of data centers has been divided into separate practices. IT professionals manage the IT gear, and other mechanical trades manage the data center facilities, i. e., the power, cooling and building infrastructures. Ideally, the cooling and power resources would be allocated dynamically according to the needs of the IT equipment, but the information needed to do this is usually unavailable. As a result, the typical practice in facilities management is to over-provision power and cooling to ensure that all IT equipment is adequately powered and cooled in all circumstances.</p><p>The discrete management of those "silos" results in inefficiencies, and the cost of these inefficiencies will increase in the future. By 2012, the EPA <ref type="bibr" target="#b1">[2]</ref> predicts energy consumption in data centers will double from 2007 levels. Energy prices are also increasing, and the rate of increase is expected to rise due to regulatory and social concerns over green house gas emissions from energy production.</p><p>Significant cost savings are possible if these distinct silos of IT, power and cooling are managed in concert. But energy efficiency is for naught if the data center cannot deliver its IT services according to predefined SLA or QoS goals. Failures to meet SLAs can result in substantial business revenue loss. Amazon found that every additional 100 ms of latency cost them 1% loss in sales, and Google observed that an extra 0.5 s in search page generation time reduced traffic by 20% <ref type="bibr" target="#b2">[3]</ref>. It is important to allocate IT resources dynamically to meet SLAs to respond to changing workloads.</p><p>From a cooling perspective, a data center is a dynamic thermal environment (Figure <ref type="figure" target="#fig_0">1</ref>). Typically, there are significant nonuniformities due to variations in airflow, physical placement of IT equipment and the placement of workload within the IT equipment <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. As a result, some locations in a data center (e. g., Rack 34 in Figure <ref type="figure" target="#fig_0">1</ref>) are much more efficiently cooled than others (e. g., <ref type="bibr">Rack 10)</ref>. A significant opportunity for energy savings lies in the integrated control and placement of IT workloads within the physical facility according to cooling efficiency. Placing or moving workloads into proper locations can make the cooling infrastructure operate more efficiently and hence can result in substantial reduction in cooling power.</p><p>Recent research has focused on improving both SLA management and data center energy efficiency, mostly with respect to resource allocation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, capacity planning and workload management <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, power <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and thermal management <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, most published research focuses on managing performance, IT resources, workload, power and cooling in isolated ways. Approaches that are not holistic, particularly those that do not consider the interactions between IT resources and cooling facilities, may indeed meet SLAs, but will not realize the energy savings of a cooling efficiency-aware approach.</p><p>In this paper, we propose an integrated solution that couples the management of IT resources and cooling infrastructure to improve the overall efficiency of data center operations. Our solution considers interactions between dynamic resource allocation, workload placement and cooling control. By properly provisioning computing, power and cooling resources, our solution can meet application-level performance goals while dramatically reducing the energy consumption of both the IT and cooling systems.</p><p>We have implemented a prototype of this integrated data center management solution and validated our solution through experiments in a production data center. Experimental results show that our solution is able to maintain performance goals while reducing energy consumption by more than 35% for IT infrastructure and 15% for cooling infrastructure. Fig. <ref type="figure">2</ref>: Architecture of our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OUR APPROACH A. Background</head><p>A data center is a complex system consisting of building, power and cooling infrastructure, and IT equipment and applications.</p><p>We use the term IT to refer to IT equipment and applications and services running in a data center. In this paper, we consider an environment where multiple applications are hosted within a common pool of virtualized servers. Applications consist of one or more interacting components, each running within its own virtual machine (VM). The resources of a physical server (node) is shared by the hosted VMs, including CPU capacity, disk access and network I/O bandwidth. Resources are allocated at run time through a per-node VM monitor or hypervisor (e. g., Xen CPU scheduler <ref type="bibr">[13]</ref>). Each application is assumed to have an application-level performance target or resource-level utilization threshold. The goal of SLA management is to meet the target through dynamic resource allocation while minimizing the over-provisioning of resources.</p><p>The term facilities refers to all components (e. g., power or cooling equipment) that are not directly involved in providing IT services. One function of data center facilities is to remove heat generated by the IT equipment. Computer Room Air Conditioners (CRACs) perform this action. The speed of the blowers within each CRAC can be varied as can the set point for the supply air temperature (SAT). The goal of cooling management is to reduce energy consumption required for cooling the data center while maintaining its own QoS guarantees, typically stated as server or blade inlet temperature set points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of Our Approach</head><p>The architecture shown in Figure <ref type="figure">2</ref> consists of a set of controllers, sensors and actuators on both IT and cooling equipment. IT management includes application, node and pod controllers. Application controllers and node controllers continually adjust resource allocations among individual workloads on a shared server in response to dynamically changing workloads. A pod controller dynamically migrates workloads as necessary to avoid nodes where aggregate resource demand exceeds available capacity, while simultaneously consolidating workloads to fewer nodes and opportunistically powering down unused nodes to save power and cooling costs. The set of powered nodes are chosen based on their cooling efficiencies, i. e., how efficiently they can be cooled.</p><p>Cooling management includes our zonal cooling controller, Dynamic Smart Cooling (DSC) <ref type="bibr" target="#b11">[12]</ref>. DSC adjusts the blower speed and the SAT set points of the CRACs based on data from temperature sensors that measure the inlet temperature of IT equipment. This allows DSC to supply the requisite cooling to the IT equipment without over-cooling the room.</p><p>IT and cooling management are integrated through Daffy. Daffy uses a data model to link information in the thermal model such as sensor readings with information in the IT model such as node location. It further automatically collects IT and facility data (e. g., LWPI, which is defined in Section II-D1) and delivers it to controllers at run time. The pod controller uses LWPI values in its workload placement decisions to account for differences in thermal efficiency within the data center. Daffy also updates DSC with the server state information (e. g., server power status), which is received from the pod controllers. DSC then adjusts the CRACs accordingly to react to the changes in IT workload demand.</p><p>All control-including dynamic resource allocation, workload migration and dynamic cooling-occurs automatically according to real-time measurements of performance, resource consumption, and thermal conditions within the data center and is governed by a set of policy settings. We discuss each of the management modules and integrations in detail next. C. Integrated IT Management 1) Application Controller: Each application controller controls a single application composed of one or more application components, with each component hosted inside a VM. The policy associated with an application controller includes the response time target, control interval, utilization thresholds, etc. The application controller periodically generates appropriate utilization targets for the corresponding VMs to ensure that the application's performance goals are met. For that, it estimates utilization targets based on a performance model. We next outline the performance model used to derive utilization targets from application response time targets for multi-tier applications in virtualized environments.</p><p>In this work, we focus on CPU resources, since CPU-intensive business logic processing in multi-tier applications is often the bottleneck. In the following discussion, we assume that CPU is the single bottleneck resource; that CPU is the only resource to be dynamically allocated among VMs; and that non-CPU resources are adequately provisioned and the effect of contention for these resources on response time (i. e., queuing delay) is negligible.</p><p>Assuming that a Poisson process is a good approximation of request arrivals, we model each CPU as an M/G/1/PS queue. In accordance with queuing theory, the CPU resident time of all requests served in a tier is represented by u/(1 -u). The total CPU resident time across all tiers, then, can be described as in Equation <ref type="formula" target="#formula_0">1</ref>where M is the number of tiers, u m is the CPU utilization of the VM at tier m and λ is the request rate.</p><formula xml:id="formula_0">r cpu = 1 λ M m=1 u m 1 -u m<label>(1)</label></formula><p>Multi-tier applications typically support a number of transaction types (e. g., login, browse, check-out). Let α n represent service time of a transaction type n on all non-CPU resources on the execution path of that transaction type. Then the mean resident time on non-CPU resources can be approximated by the weighted sum of each transaction type's service time as in Equation <ref type="formula" target="#formula_1">2</ref>where N is the number of transaction types and λ n is the request rate of transaction type n. The aggregate request rate λ equals to N n=1 λ n .</p><formula xml:id="formula_1">r non cpu = 1 λ N n=1 α n λ n<label>(2)</label></formula><p>α n can be estimated through linear regression using Equation 2 over multiple measurement intervals as introduced in <ref type="bibr" target="#b12">[14]</ref>. Combining Equations 1 and 2, the mean response time can be represented as follows.</p><formula xml:id="formula_2">r = 1 λ ( M m=1 u m 1 -u m + N n=1 α n λ n )<label>(3)</label></formula><p>Transforming Equation <ref type="formula" target="#formula_2">3</ref>, the utilization targets u ref m (m = 1, . . . , M) for each tier can be derived for a given response time target r ref . We assume that the utilization targets of all the tiers are the same. Note that this does not imply resource allocations are the same across tiers, because the resource requests of different tiers can be significantly different. This allows Equation 3 to be solved directly to obtain the utilization target for each tier as follows:</p><formula xml:id="formula_3">u ref m = r ref λ - N n=1 α n λ n M + r ref λ - N n=1 α n λ n (4)</formula><p>Once an application controller determines the individual utilization targets for each of the component VMs, it sends the targets to the node controllers managing the corresponding VMs. The node controllers then allocate resources accordingly.</p><p>2) Node Controller: Each node is associated with a node controller. Its role is to maintain the utilization targets for all hosted VMs by dynamically adjusting their resource allocation. We define the utilization of a VM as the ratio between its resource consumption and its resource allocation. For example, if the measured average CPU consumption of a VM is 60 CPU shares in a control interval, and the specified utilization target is 75%, then the utilization controller will drive the CPU allocation for the VM towards 80 CPU shares in subsequent control intervals. A node controller consists of a set of utilization controllers (one for each individual VM) and one arbiter. The utilization controller collects the average resource consumption of each VM from sensors and determines the required resource allocation to the VM such that a specified utilization target can be achieved. For a given consumption, a change in allocation leads to the change in utilization. Using utilization instead of allocation targets decouples an application controller from the underlying physical resource allocation details.</p><p>All utilization controllers feed the desired resource allocations (referred to as requests) to the arbiter controller, which determines the set of actual resource allocations (referred to as allocations). If the sum of all requests is less than the node's capacity, then all the requests are granted. Additionally, excess capacity is distributed among the VMs in proportion to their individual requests. However, if the sum of all requests exceeds the node's capacity, the arbiter performs service-level differentiation based on workload priorities as defined in perservice policies set by data center operators. In our current implementation, a workload with a higher priority level always has its request satisfied before a workload with lower priority.</p><p>3) Pod Controller: Data centers are rarely a homogeneous set of servers that are completely fungible. Networking limitations may require that networks be divided into sub-networks. Performance limits may dictate that storage systems be partitioned between groups of servers. We call any homogeneous collection of servers a pod and dedicate a controller to it. Up to several hundreds of nodes and a few thousands of VMs may comprise a pod. A data center may have multiple pods, and hence multiple pod controllers.</p><p>A pod controller dynamically arranges VM workloads within its pod based on resource requests (e. g., CPU, LAN, SAN and memory needs) of the node controllers associated with servers in the pod. Changes are made via VM live migration. Candidate arrangements are generated and evaluated by a genetic algorithm. The objective function used is designed to consolidate the workload onto the fewest number of nodes possible that can be most efficiently cooled and that have the best average utilization. Penalties are assessed for arrangements that include overloaded nodes, that require a large number of migrations to achieve, that use nodes to be avoided as per policy, and that use nodes that cannot be efficiently cooled. The terms of the objective function are weighted to achieve a balance between potentially performance-disrupting migrations, consolidation and relocation objectives, and the movement of workload to more efficiently cooled areas of the data center. Per-VM migration penalties can be added to minimize movement of performance sensitive VMs. The pod controller evaluates and possibly makes changes to arrangements every 60 s in the experiments described herein.</p><p>The pod controller also opportunistically shuts down servers that are not used or less efficiently cooled to save power. Node power control and the integration of cooling management is discussed in section II-D3. D. Integrated Data Center Management 1) Cooling Management: A traditional CRAC unit in a data center is controlled via the return air temperature. IT equipment inlet temperatures are not measured directly and cannot be assured. Cooling resources, therefore, are typically overprovisioned to insure that inlet temperature requirements are met. Dynamic Smart Cooling (DSC), by contrast, utilizes data from either a distributed temperature sensor network placed near the inlets of racks that house IT equipment or from IT equipment directly. The data is used to determine the thermal zones of each CRAC and then to provide proper cooling to each zone <ref type="bibr" target="#b11">[12]</ref>. In this manner, cooling resources can be correctly provisioned in real-time according to IT equipment needs. This has the benefit of reducing energy consumption, improving effective cooling capacity and improving IT equipment reliability.</p><p>2) Local Workload Placement Index: To determine areas of efficient cooling, we use the Local Workload Placement Index (LWPI), first introduced in Cool Job Allocation (CJA), with a modified formulation. LWPI is a measure of how efficiently a location in the data center can be cooled <ref type="bibr" target="#b13">[15]</ref>. LWPI is the sum of three components, has units of temperature, and is described in Equation <ref type="formula">5</ref>.</p><p>LWPI node = (Thermal Management Margin) node + (AC Margin) node -(Hot Air Recirculation) node (5) The first term in Equation 5 is the Thermal Management Margin at a compute node. This is the difference between a node's inlet air temperature and its specified set point, which is usually given by the manufacturer as a maximum recommended temperature, less some margin.</p><p>The second term is Air Conditioning (AC) Margin. AC Margin is the sum over all CRACs of the difference between the CRACs current supply air temperature and their minimum possible set point, weighted by the degree to which the particular CRAC can actually supply cold air to the node and influence its inlet air temperature <ref type="bibr" target="#b11">[12]</ref>.</p><p>The final component is Hot Air Recirculation which quantifies the amount of hot air recirculation at the node and is the difference between the node's inlet air temperature and the temperature of the air supplied through vent tiles in close proximity to the node.</p><p>Higher LWPI values indicate higher thermal efficiency. LWPI is only well-defined at thermal sensor locations near the inlets of compute nodes. Temperature measurements can be obtained through an external sensor network or from inlet air temperature sensors incorporated into recent server hardware. LWPI is computed directly for nodes with sensors. Linear interpolation is used for other nodes. The pod controller uses LWPI to choose the best location to place the different workloads.</p><p>3) Integration of IT and Cooling Management: The integration of IT and cooling management is through Daffy, which uses a hybrid physical-logical data model that relates facility domain objects to IT domain objects. From the facility domain, Daffy includes the location, name, make-and-model of CRACs, PDUs and racks, as well as facilities-level sensing capabilities such as DSC thermal sensors and CRAC operating points. Most of these objects are represented as volumes in three-dimensional geometry. From the IT domain, Daffy includes the location, name, make-and-model, and ownership (among other attributes) of IT equipment in the data center.</p><p>This data model allows locating IT equipment relative to the facilities infrastructure, in particular to cooling resources. For example, to return the LWPI value for a given IT node, it is necessary to locate the node in the data center and to obtain temperature measurements. The latter requires knowledge of the IT device model (to determine if there is an inlet air temperature sensor) and the relative location of sensors in the sensor network (if there is not).</p><p>Daffy automatically collects and delivers thermal metrics such as LWPI values to the pod controller, which uses the LWPI values in its workload placement decisions. As workload is consolidated, the pod controller selects idle nodes to power off. Given the movement of workload to more efficiently cooled areas, the least efficiently cooled nodes as determined by LWPI are powered down first. As workload increases, the LWPI values associated with powered down nodes are used to determine the order in which nodes are powered up.</p><p>Once the pod controller consolidates workloads and turns off idle servers, the information is sent to Daffy and Daffy interacts with DSC to adjust cooling resources according to predefined policies. For example, if servers in a rack are shutdown, the inlet air temperature of the rack can be raised. Daffy locates the corresponding temperature sensors associated with the servers and changes their settings in DSC. In response to this change, DSC reduces the corresponding CRAC outputs to save energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Communication between Controllers</head><p>We implemented a prototype of our solution running in a Xen 3.2 environment. We have a per node sensor that provides resource consumption and a per application sensor that collects response times. We assume a fixed set of applications. Future work will support dynamic service joining/leaving. Messages between controllers are exchanged based on the asynchronous message queue protocol (AMQP), which allows highly efficient communication between loosely connected components. For example, application controllers don't necessarily know on which nodes VMs are hosted and thus which node controllers they need to talk to. The number of messages an application controller sends is linear with the number of application components. Each node controller sends and receives messages in proportion to the number of VMs running on the node. The number of messages the pod controller receives is linear with the total number of VMs in the pod. Typically, a pod comprises up to a few hundred of nodes and a few thousands of VMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion and Clarification</head><p>While the pod controller does consider memory, LAN and SAN in its decisions, the performance model is limited to a single bottleneck resource. Including non-CPU resources is under investigation. The use of a simpler performance model is motivated by its robustness and practicality of acquiring model information from production applications. Prior work shows that this model works well. Measured response time prediction errors were less than 15% for experiments in <ref type="bibr" target="#b12">[14]</ref> and <ref type="bibr" target="#b14">[16]</ref>. We are working to enhance our model based on the advanced model proposed in <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. Further, performance model based predictive control can be integrated with feedback control to provide a more robust solution for performance management. Prior work has shown that the integration can provide both stability and short reaction time and the solution is robust w.r.t. bias or errors in the model prediction <ref type="bibr" target="#b16">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION</head><p>We designed and conducted a set of experiments to evaluate the application performance, resources required, and power used by both computing and cooling equipment. In this section, we compare our solution with non-integrated approaches and demonstrate that the integrated approach significantly reduces resource and power consumption without degrading application performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Testbed</head><p>Our experiments were conducted in a data center located at HP in Palo Alto, California. Figure <ref type="figure" target="#fig_0">1</ref> shows the layout of this data center. The IT equipment includes servers, networking switches and storage equipment and is organized as ten rows of racks in a standard "hot-aisle-cold-aisle" configuration. Six CRAC units cool the room. Five sensors on each rack measure the inlet air temperatures of the contained equipment in real time. DSC <ref type="bibr" target="#b11">[12]</ref> controls the CRACs based on these measurements. The servers used in these experiments are located in top half of Rack 10 and the bottom half of Rack 34. We have determined empirically that Rack 10 is cooled mainly by CRAC 3 and CRAC 4 and that Rack 34 is primarily cooled by CRAC 1. We observed that the middle of Rack 10 had lower LWPI values relative to the rest of the rack. This is a result of a network switch in the middle of the rack that exhausts hot air in proximity to the inlets of some servers. Though undesirable, this is a common occurrence in data centers, as it facilitates cable routing. We observed that the bottom of Rack 34 had fairly high LWPI values in general. This is due to ease of cooling locations that are in close proximity to floor vent tiles.</p><p>Our experiment utilized 20 nodes to host 35 VMs. Among these 20 nodes, there were nine nodes with two Intel Pentium D 3. 2 GHz CPUs and 11 nodes with two Intel Xeon 3. 6 GHz CPUs. All nodes ran SLES 10 SP2 Linux with a Xen 3.2 kernel. Inside the management domain (Domain-0) of each physical node, a node agent exposed an interface for monitoring the node and its VMs and for allocating resources to the VMs. The Xen Credit Scheduler was used and made adjustment of runtime CPU shares for VMs. The node controllers ran inside the management domains, while the application and pod controllers, Daffy and DSC ran on nodes other than these twenty.</p><p>We define CPU capacity, requests and allocations in units of CPU shares. A CPU share denotes one percentage of utilization of a processor with a clock rate of 1 GHz. A scale factor adjusts for the capacity between nodes with different processor speeds or architectures. In our testbed, the 3.6 GHz CPUs were assigned 360 shares and the 3.2 GHz CPUs were assigned 320 shares. We note that such factors are only approximate; the calculation of more precise scale factors is beyond the scope of this paper.</p><p>There were two types of applications in our testbed: 3-tier applications and computational workloads. The 3-tier application was a modified version of RUBiS <ref type="bibr" target="#b17">[19]</ref>. RUBiS is an on-line auction benchmark comprised of a front-end Apache Web server, a JBoss application server, and a back-end MySQL database server. Each application component ran inside one VM. To emulate a real application with highly dynamic workload, we emulated the demands of a globally-distributed business-critical HP application called VDR <ref type="bibr" target="#b12">[14]</ref> by replacing VDR transactions in the original traces with RUBiS transactions of the same rank and replaying the trace. The SLOs of RUBiS were defined as average response time thresholds. The computational application was a single program that consumes specified CPU resources. We used a CPU load trace that varied from 20 to 150 CPU shares to simulate changes of resource demands in real applications. The SLOs of computational applications were specified as CPU utilization targets.</p><p>Response times for each individual request of RUBiS were logged by the workload generator. Power consumption of a node was estimated as a function of utilization by using linear interpolation between the measured apparent power consumed by the node at idle and its fully utilized state <ref type="bibr" target="#b18">[20]</ref>. We recorded directly the reported blower power for the six CRAC units in our data center. We are using blower power as a proxy for cooling power, due to the difficulty of isolating the data center from other uses of our chiller plant. We also recorded the SAT for each CRAC; this will be used later to approximate the balance of the cooling power used by the chiller plant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application Level Performance Management</head><p>To determine if the application controller and node controller could manage the performance of the RUBiS application under time-varying workloads, we ran experiments driven by the VDR traces. We set the response time targets for the applications to 40 ms. Every 30 s, the application controller computed required CPU utilization targets based on recent workload measurements for the three virtual machines hosting the application tiers. Every 10 s, the node controllers adjusted the CPU shares allocated to virtual machines.</p><p>Figure <ref type="figure" target="#fig_1">3a</ref> shows the response times of the multi-tier application averaged every one minute for a 10 hour experiment. As shown in the figure, most of the time, our controllers maintained the response time below its target. Throughout the experiment period, the average response time was 34 ms and 86% of the requests had response time below the target. The dynamic CPU shares allocated to each VM are shown in Figure <ref type="figure" target="#fig_1">3b</ref>. Note that the web tier was always lightly loaded and its CPU share was set to the minimum of 45 CPU shares. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Integrated Data Center Management</head><p>In this section, we compare the following four approaches: A: Fixed Approach. CPU shares are dynamically adjusted to meet a specified, constant utilization targets and the workload placement is static. This approach is widely used in production systems such as HP's gWLM <ref type="bibr" target="#b19">[21]</ref> and IBM's Enterprise Workload Manager <ref type="bibr" target="#b20">[22]</ref>. B: Integrated Performance Management. Application controllers and node controllers dynamically allocate CPU resource to meet performance targets, but workload placement is static. C: Integrated IT Management. Similar to approach (B) but with the integration of dynamic workload placement. A pod controller migrates and consolidates workloads when needed. DSC controls cooling resources independently. D: Integrated Data Center Management. Full integration of performance management (application and node controller), dynamic workload placement (pod controller) and cooling management (DSC) as presented in Section II. For each approach, we conducted 10 hour experiments with 20 nodes and 35 VMs-6 for two RUBiS applications and 29 for computational workloads-as described in Section III-A. For each run, we initially placed the workloads onto nodes according to their peak demands. We used a response time target of 40 ms for both RUBiS applications except for approach (A) and a utilization target of 80% for the computational workloads. For the fixed approach (A), operators of systems typically do not know how to set the utilization target from an application level performance target, and often use a default settings provided by a vendor, for instance, 75% in the HP gWLM. In our experiments, we used a more conservative utilization target of 50% for the RUBiS applications. Results show individual 10-hour runs consisting of over 1 million requests. Other runs with identical settings showed little variance because test workloads were driven by traces. We plan to validate our controllers on production systems where statistical measures may be more meaningful.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the aggregate demand in CPU shares of all workloads over time for all experiments. The demand starts out high, drops after 30 minutes, increases around the fourth hour, and drops again afterward. This is typical of day-time demand for business applications that show peaks in the morning and at lunch time. In the following sections, we compare the required IT resources, application performance, server power and cooling power consumption of the four approaches.</p><p>1) Computing Resource and Power Consumption: We first compare the computing resources and power consumption from the approaches with static workload placement, i. e., approach (A) and (B), with results from the approaches with dynamic workload placement, i. e., approaches (C) and (D). In (C) and (D), the pod controller is configured to dynamically consolidate workloads and opportunistically turn on or off nodes. As shown in Figure <ref type="figure">5</ref>, the integrated approaches lead to significant savings of both computing resource and power consumption.</p><p>Figure <ref type="figure">5a</ref> shows the number of nodes that were active during the experiments. The numbers were constant in (A) and (B) as workloads were placed onto all nodes at the beginning and all nodes were used. For (C) and (D), the pod controller consolidated workloads and turned off unused nodes when load dropped. Hence, fewer nodes were used compared with (A) and (B). We note that (C) and (D) slightly differed from each other, which resulted from different workload placement decisions made by the pod controller. Figure <ref type="figure">5b</ref> shows the average server utilization, defined as the ratio of total allocation divided by total capacity of active nodes. The server utilization of (B) was slightly higher than (A) because the application controller in (B) computed lower VM utilization targets than the default 50% used in (A) for the RUBiS applications. However, consolidation in (C) (D) achieved a much higher server utilization. Keeping the utilization high and turning off unused nodes helped to save power, as shown in Figure <ref type="figure">5c</ref>. Compared with (A) and (B), the integrated approaches (C) and (D) helped to reduce the total node power consumption by 35% over the 10 hour experiment.</p><p>2) Application Performance: Figure <ref type="figure">6</ref> shows the Cumulative Distribution Functions (CDFs) for the response times of individual requests from RUBiS applications for the four approaches. The worst performance was achieved by the fixed approach (A) because the 50% utilization targets are too high. The approach (B) which uses the application and node controller with a static workload placement had the best performance. Compared with (A), approach (B) was able to derive the appropriate utilization targets from the performance targets through the performance model. Furthermore, the initial placement of the workloads was based on their peak demand, thus most requests from the node controllers could be satisfied. The RUBiS performance from the approaches (C) and (D) were comparable to that of (B), but with much less computing resources and power consumption. The performance fell slightly because there was less excess capacity available on the nodes due to workload consolidation, and performance suffered for short intervals on overloaded nodes until the pod controller was able to re-balance the workloads. Similar results (not shown) were obtained for the computation workloads. For these workloads, the average server utilizations of (C) and (D) were 81.6% and 80.4% respectively compared   with 80% in (A) and (B). In summary, the integrated approaches (C) and (D) managed to significantly reduce required computing resources and server power consumption without significantly compromising the application performance.</p><p>3) Cooling Resource Consumption: Figure <ref type="figure" target="#fig_7">7</ref> shows results of cooling management achieved by integrated IT and cooling management in approach (D), compared to that in (C) where IT management and cooling control operated independently from each other. For the approach (C), the pod controllers dynamically consolidated workloads and shutdown nodes without considering the cooling efficiency of nodes and without direct communication with Daffy. In our fully integrated approach (D), the pod controller monitored LWPI and always placed workloads onto nodes with the highest LWPI values. Moreover, the pod controller informed Daffy which nodes were running and which nodes were off. Daffy then directed DSC to adjusting cooling resources accordingly.</p><p>Figure <ref type="figure" target="#fig_7">7a</ref> compares the mean LWPI value of all active nodes used by the integrated management (D) with the average LWPI value of all nodes available in the experiment. As shown in the Figure, the integrated approach (D) always placed and consolidated workloads onto nodes with better cooling efficiency, i. e., higher LWPI values. Recall that workloads were initially placed on nodes in both Rack 10 and 34. Due to the switch  The total power consumed by the blowers for the six CRAC units in the data center is shown in Figure <ref type="figure" target="#fig_7">7b</ref>. Without the integration of IT and cooling, the blower power from (C) was constant at about 24.7 KW over the experiment period. Compared with (C), our fully integrated solution (D) managed to reduce total blower power consumption on average by 15% over the 10 hour experiment. The saving was achieved through the interaction of IT and cooling management. In (D), when workload demand dropped at time 01:00 and 06:00, the pod controller consolidated workloads onto nodes in Rack 34 (with higher LWPI values) and shutdown nodes in Rack 10 (with lower LWPI values) and notified Daffy of these changes. Daffy then directed DSC to adjust cooling accordingly.</p><p>As shown in the figure, the integrated solution (D) reduced the average blower power consumption up to 8 KW, more than 30% compared with that with (C) when workloads were consolidated (e.g., time 1:00-3:00 and 6:00-10:00) without regard to cooling considerations. We note that though we only managed a small part of the overall data center in this experiment, the savings through the integration of IT and cooling management were significant, because blower power of a CRAC is a cubic function of blower speed, and a small number of hot spots in the data center can dominate the cooling necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary</head><p>Table <ref type="table" target="#tab_1">I</ref> summarizes the performance and power results of the four approaches over the 10 hour experiments. The column "App. Perf." shows the percentage of requests that were below the response time target for RUBiS. "Server power" means the mean total power consumption of all the 20 nodes in the testbed and the column "Blower power" shows the total power consumption of the six CRAC units in the data center.</p><p>The results from the fixed approach (A), which employs fixed utilization targets for the workloads, are taken as the baseline. Note that this approach is widely used in today's production systems. The integrated performance management approach (B) deployed application controllers to dynamically adjust utilization targets of the RUBiS application such that response time targets can be met. This improved the performance of RUBiS applications and 79% of requests had response times below the target, 18% more than in (A). The integration of the pod controller helped to significantly reduce computing resources and node power usage with some performance loss as discussed in section III-C2. The average power consumption of the IT infrastructure from (C) and (D) was reduced by 35% compared to (A) and (B) and the average saving per server was 70 W. We anticipate substantial savings when our approach is extended to more servers in the data center.</p><p>Although in our experiments we managed a small fraction of the data center, the integrated management approach (D) reduced the CRACs blower power consumption by 15% over the 10 hour experiment. However, blower power represents only a fraction of the total power consumption from the cooling infrastructure. The majority of the power consumption takes the form of thermodynamic work performed by the central chiller plant that provides chilled water to the CRACs. As discussed earlier, this is difficult to isolate and measure directly. From earlier experience <ref type="bibr" target="#b21">[23]</ref>, we used the CRAC SATs to estimate the thermodynamic work and determine the total power (blower power and chiller power). The savings were found to be 38 KW over the 10 hour experiment. This represents a 16% savings in total cooling power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATED WORK</head><p>Traditional IT management solutions often over-provision IT resources, sometimes by large amounts, in order to satisfy application performance requirement such as end-to-end response time thresholds. Virtualization techniques provide mechanisms for workload management that can provide resources to applications on their demand. Workload may be consolidated based on prediction of their demand in the future <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. When application performance such as response time has to be guaranteed, performance models <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> can be applied to predict the resource requirement to meet the performance target. Recently control theory has been used for design of controllers that can respond to the time-varying work through dynamic resource allocation <ref type="bibr" target="#b22">[24]</ref>. However, none of the previous work has integrated dynamic resource allocation and virtual machine migration for application performance management; nor does it consider the impact of workload consolidation on the overall energy efficiency of data centers.</p><p>The increasing cost of energy has given rise to a number of studies on power issues <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. A few have investigated minimizing power consumption while simultaneously meeting performance constraints <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[25]</ref>, but these efforts do not in general deal with the substantial power required by the cooling infrastructure, despite cooling being a major consumer of power in data centers <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. VMware <ref type="bibr" target="#b24">[26]</ref> has Distributed Power Management that consolidates workloads through live virtual migration and suspends idle machines. However, no consideration is made of the impact of migrations-dynamic workload placement-on application performance and the underlying cooling infrastructure.</p><p>Research has also begun in recent years on active control strategies for thermal management within the data center <ref type="bibr" target="#b11">[12]</ref>. A few researchers have studied the management of temperature and cooling costs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, since these solutions lack integration with IT management, they only solve part of the problem and cannot take full advantage of energy savings. The lack of IT information prevents thermal management solutions from making better or smarter decisions, e. g., reducing the output of appropriate CRAC units in response to reduced IT workloads within a cooling zone.</p><p>In <ref type="bibr" target="#b16">[18]</ref>, we integrated feedback control and performance model-based prediction for application performance management but only through dynamic resource allocation. An automated IT resource and workload management system for data centers was proposed in <ref type="bibr" target="#b25">[27]</ref>, but it did not take into account either application performance or cooling management. The genetic algorithm of the pod controller is similar to the one in <ref type="bibr" target="#b26">[28]</ref> but additionally considers thermal management. In <ref type="bibr" target="#b9">[10]</ref>, we studied the unified workload and power management for server enclosures through dynamic voltage scaling, workload consolidation, and active fan control. End-to-end application performance was not considered. In <ref type="bibr" target="#b3">[4]</ref>, a cooling-aware workload placement and migration solution has been developed and evaluated in a data center environment, but it did not attempt to enforce SLA compliance.</p><p>The existence of these individual controllers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b16">[18]</ref> does not imply a working integrated system. Further, prior integrations have been partial and ad-hoc. None have considered application performance and energy efficiency together. The proposed solution provides full integration of performance, workload migration, power and cooling management in a systematic and automatic way. The novelty lies in linking IT and facility controllers to act in a coordinated manner. The proposed hybrid data model and Daffy address this.</p><p>V. CONCLUSIONS AND FUTURE WORK Data centers are a mix of IT, power and cooling infrastructure. Data center management requires management of IT resource allocation, workload, and power and cooling management. When these management functions are performed independently, particularly without regard to the interaction of IT resource and cooling elements, it is impossible to meet SLAs and achieve near optimal energy savings. This paper presents an integrated solution that tightly couples IT management and cooling infrastructure management to improve overall efficiency of data center operation.</p><p>Future work will consider the integration of finer power control, such as dynamic CPU speed scaling. We are also planning to look at other IT resources (e. g., memory, networking and storage) in addition to CPU. Other interesting research topics include the incorporation of additional SLO metrics like availability and reliability to our model, and the management of workload across data centers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Cooling efficiency overview of a data center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Application performance management. C. Integrated Data Center Management</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Total workload demand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 09:00 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 09:00 10:00 Total node power in KW Time A B C D (c) Total power consumption of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Integrated workload management.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 09:00 1001:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 09:00 10:00 Total blower power in KW Time C D (b) Data center blower power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Integrated IT and cooling management. that exhausts hot air into the cold aisle of Rack 10, the nodes there are harder to cool and thus have lower LWPI values. The disparity between LWPI values for nodes in Rack 10 and 34 drove consolidation towards nodes in Rack 34. As shown in the figure, at time 01:00 and time 06:00, the mean LWPI value across active nodes increased as the pod controller moved all VMs from Rack 10 to Rack 34 and powered off Rack 10 nodes that had lower LWPI values.The total power consumed by the blowers for the six CRAC units in the data center is shown in Figure7b. Without the integration of IT and cooling, the blower power from (C) was constant at about 24.7 KW over the experiment period. Compared with (C), our fully integrated solution (D) managed to reduce total blower power consumption on average by 15% over the 10 hour experiment. The saving was achieved through the interaction of IT and cooling management. In (D), when workload demand dropped at time 01:00 and 06:00, the pod controller consolidated workloads onto nodes in Rack 34 (with higher LWPI values) and shutdown nodes in Rack 10 (with lower LWPI values) and notified Daffy of these changes. Daffy then directed DSC to adjust cooling accordingly.As shown in the figure, the integrated solution (D) reduced the average blower power consumption up to 8 KW, more than 30% compared with that with (C) when workloads were consolidated (e.g., time 1:00-3:00 and 6:00-10:00) without regard to cooling considerations. We note that though we only managed a small part of the overall data center in this experiment, the savings through the integration of IT and cooling management were significant, because blower power of a CRAC is a cubic function of blower speed, and a small number of hot spots in the data center can dominate the cooling necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Summary of the results</figDesc><table><row><cell>Approach</cell><cell>App.</cell><cell>Server</cell><cell>Blower</cell></row><row><cell></cell><cell>Perf.</cell><cell>power</cell><cell>power</cell></row><row><cell>A: Fixed approach</cell><cell>61%</cell><cell cols="2">3.81 KW 24.7 KW</cell></row><row><cell>B: Integrated Performance mgmt.</cell><cell>79%</cell><cell cols="2">3.87 KW 24.7 KW</cell></row><row><cell>C: Integrated IT mgmt.</cell><cell cols="3">70.1% 2.61 KW 24.7 KW</cell></row><row><cell>D: Integrated data center mgmt.</cell><cell cols="3">70.1% 2.47 KW 20.9 KW</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-4244-5367-2/10/$26.00 c 2010 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2010" xml:id="foot_1"><p>IEEE/IFIP Network Operations and Management Symposium -NOMS 2010</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2010" xml:id="foot_2"><p>IEEE/IFIP Network Operations and Management Symposium -NOMS 2010</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Energy flow in the information technology stack: Coefficient of performance of the ensemble and its impact on the total cost of ownership</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beitelmal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP Labs, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Report to congress on server and data center energy efficiency, public law</title>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Environmental Protection Agency, ENERGY STAR Program</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Make data useful</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<ptr target="http://glinden.blogspot.com/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thermal policies and active workload migration within data centers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASME InterPACK Conf</title>
		<meeting>of ASME InterPACK Conf<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thermal considerations in cooling large-scale high compute density data centers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beitelmal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th Intersociety Conf. on Thermal and Thermomechanical Phenomena (ITHERM)</title>
		<meeting>of the 8th Intersociety Conf. on Thermal and Thermomechanical Phenomena (ITHERM)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-adaptive capacity management for multi-tier virtualized environments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th IFIP/IEEE Symposium on Integrated Management (IM)</title>
		<meeting>of the 10th IFIP/IEEE Symposium on Integrated Management (IM)</meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application performance management in virtualized server environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Khana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kochut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/IFIP Network Operations &amp; Management Symposium (NOMS)</title>
		<meeting>of the IEEE/IFIP Network Operations &amp; Management Symposium (NOMS)</meeting>
		<imprint>
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capacity management and demand prediction for next generation data centers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Web Services (ICWS)</title>
		<meeting>of the IEEE Int. Conf. on Web Services (ICWS)<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Workload analysis and demand prediction of enterprise data center applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Symposium on Workload Characterization (IISWC)</title>
		<meeting>of the IEEE Int. Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified thermal and power management in server enclosures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ASME InterPACK</title>
		<meeting>of the ASME InterPACK<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">2009. Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble-level power management for dense blade servers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>of the 33rd annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic thermal management of aircooled data centers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intersociety Conf. on Thermal and Thermomechanical Phenomena in Electronic Systems (ITHERM)</title>
		<meeting>of the Intersociety Conf. on Thermal and Thermomechanical Phenomena in Electronic Systems (ITHERM)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting nonstationarity for performance prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data center workload placement for energy efficiency</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASME InterPACK Conf</title>
		<meeting>of ASME InterPACK Conf<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A systematic and practical approach to generating policies from service level objectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Milojicic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IFIP/IEEE Int. Symp. on Integrated Network Management (IM)</title>
		<meeting>of the IFIP/IEEE Int. Symp. on Integrated Network Management (IM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analytical model for multi-tier internet services and its applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pacificiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spreitzery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tantawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMETRICS</title>
		<meeting>of ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Appraise: Application-level performance management in virtualized server environments</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Networking and Service Management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rubis rice university bidding system</title>
		<ptr target="http://www.cs.rice.edu/CS/Systems/DynaServer/rubis" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No &quot;power&quot; struggles: Coordinated multi-level power management for the data center</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>of the 13th Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">HP Global Workload Manager (gWLM)</title>
		<ptr target="http://mslweb.rsn.hp.com/gwlm/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">IBM Enterprise Workload Manager</title>
		<ptr target="http://www-03.ibm.com/servers/eserver/zseries/zos/ewlm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing centralized chiller system performance for dynamic smart cooling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beitelmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP Labs, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automated control of multiple virtualized resources</title>
		<author>
			<persName><forename type="first">P</forename><surname>Padala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the EuroSys</title>
		<meeting>of the EuroSys<address><addrLine>Nuremberg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03">2009. Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coordinating multiple autonomic managers to achieve specified power-performance tradeoffs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lefurgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th IEEE Int. Conf. on Autonomic Computing (ICAC)</title>
		<meeting>of the 4th IEEE Int. Conf. on Autonomic Computing (ICAC)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Esx</forename><surname>Vmware</surname></persName>
		</author>
		<author>
			<persName><surname>Server</surname></persName>
		</author>
		<ptr target="http://vmware.com/products/vi/esx/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">1000 islands: Integrated capacity and workload management for the next generation data center</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th IEEE Int. Conf. on Autonomic Computing (ICAC&apos;08)</title>
		<meeting>of the 5th IEEE Int. Conf. on Autonomic Computing (ICAC&apos;08)<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Integrated Approach to Resource Pool Management: Policies, Efficiency and Quality Metrics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Belrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 38th IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN)</title>
		<meeting>of the 38th IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN)<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making scheduling cool: Temperature-aware workload placement in data centers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Annual Technical Conference</title>
		<meeting>of USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
