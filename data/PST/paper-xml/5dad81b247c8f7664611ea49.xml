<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image denoising using deep CNN with batch renormalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-24">August 24, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
							<email>chunweitian@163.com</email>
							<affiliation key="aff3">
								<orgName type="department">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shenzhen Medical Biometrics Perception and Analysis Engineering Laboratory</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<email>yongxu@ymail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shenzhen Medical Biometrics Perception and Analysis Engineering Laboratory</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin, Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neural</forename><surname>Networks</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Shenzhen Medical Biometrics Perception and Analysis Engineering Laboratory</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen, Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin, Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image denoising using deep CNN with batch renormalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-24">August 24, 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">5E0C34D52435FB97F02AFDAFE45124A3</idno>
					<idno type="DOI">10.1016/j.neunet.2019.08.022</idno>
					<note type="submission">Received date : 30 September 2018 Revised date : 12 August 2019 Accepted date : 19 August 2019 Preprint submitted to Neural Networks</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image denoising</term>
					<term>CNN</term>
					<term>Residual learning</term>
					<term>Batch renormalization</term>
					<term>Dilated convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image denoising aims to recover a clean image from a noisy image, which is a classical-inverse problem in computer vision. Since image-denoising techniques can recover original images well, and restore the details, they are widely applied in many fields, such as remote-sensing image <ref type="bibr" target="#b17">[15]</ref> and medical image <ref type="bibr" target="#b39">[37]</ref>. For a noisy image y, the problem of image denoising can be represented by y = x + υ, where x is the original image (also referred to as the clean image) and υ represents additive Gaussian noise (AWGN) with standard deviation σ. In the view of Bayesian rule, imageprior-based methods are good choices for image denoising. For example, block-matching and three-dimensional (3D) filtering (BM3D) exploited collaborative alteration to enhance the sparsity for image denoising <ref type="bibr" target="#b13">[11]</ref>. The simultaneous use of sparse representation based on dictionary learning and non-local means based on self-similarities can remove the noise from noisy images <ref type="bibr" target="#b44">[42]</ref>. Non-locally centralized sparse representation (NCSR) centralized the sparse coding to suppress the sparse-coding noise <ref type="bibr" target="#b16">[14]</ref>. Weighted nuclear norm minimization (WNNM) <ref type="bibr" target="#b21">[19]</ref>, markov random field (MRF) <ref type="bibr" target="#b45">[43,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b4">2]</ref>, gradient methods <ref type="bibr" target="#b5">[3,</ref><ref type="bibr" target="#b71">69]</ref> and total-variation (TV) methods <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b7">5]</ref> are also very popular image-denoising methods.</p><p>Although the above methods have shown excellent performance in image denoising, most of these methods are faced with two major challenges <ref type="bibr" target="#b63">[61]</ref>: (1) manual tuning of the penalty parameters, and (2) complex optimized algorithms. Owing to the adaptive strong learning ability, deeplearning techniques, especially convolutional neural networks (CNNs) <ref type="bibr" target="#b40">[38]</ref>, have become the most favored methods of addressing these issues <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b30">28]</ref>. Specifically, the recently proposed deep CNNs have been widely applied in image restoration <ref type="bibr" target="#b35">[33]</ref>. For example, low-resolution images were directly mapped into high-resolution images by a CNN <ref type="bibr" target="#b14">[12]</ref>. Residual and iterative ideas were embedded into the CNN to improve the performance for image super-resolution <ref type="bibr" target="#b55">[53]</ref>. The combination of an optimization method and a CNN was a good tool for image super-resolution <ref type="bibr" target="#b57">[55,</ref><ref type="bibr" target="#b49">47]</ref>. Increasing the diversity of the network was also very effective for image restoration <ref type="bibr" target="#b69">[67]</ref>. CNNs with prior knowledge can better deal with real-noisy images <ref type="bibr" target="#b64">[62]</ref>. Optimizing the network architecture was popular for image restoration <ref type="bibr" target="#b28">[26]</ref>. Although the above deep-network methods can improve the denoising performance, most of these methods suffer from the problems of performance saturation (i.e., vanishing or exploding gradients), <ref type="bibr" target="#b25">[23]</ref> and the difficulty of training deep networks <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b47">45]</ref>. Additionally, their partial networks with batch normalization (BN) <ref type="bibr" target="#b27">[25]</ref> can generate errors in small mini-batches. Some of the above methods have a very high computational cost for image restoration.</p><p>In this paper, we propose a novel network for image denoising named a batch-renormalization denoising network (BRDNet). First, BRDNet combines two networks to increase the width of BRDNet and obtain more features for image denoising. Next, BRDNet uses batch renormalization (BRN) <ref type="bibr" target="#b26">[24]</ref> to address the small mini-batch problem, and applies residual learning (RL) with skip connection <ref type="bibr" target="#b25">[23]</ref> to obtain clean images. Finally, to reduce the computational cost, dilation convolutions (Dilated Conv) <ref type="bibr" target="#b62">[60]</ref> are used to capture more features. Also, extensive experiments demonstrate that the proposed BRDNet outperforms state-of-the-art methods, e.g., a denoising convolutional neural network (DnCNN) <ref type="bibr" target="#b63">[61]</ref>, a fast and flexible denoising network (FFDNet) <ref type="bibr" target="#b65">[63]</ref>, and an image-restoration CNN (IRCNN) <ref type="bibr" target="#b64">[62]</ref>. Additionally, the extension of BRDNet also has good effects on both synthetic and real noisy images.</p><p>The proposed method comprising BRDNet has the following contributions:</p><p>(1) A novel deep CNN is proposed for image denoising in this paper, which can directly obtain a clean image from a noisy image. Unlike the existing CNN denoising methods, the proposed network increases the width rather than depth to enhance the learning ability of the denoising networks.</p><p>(2)Batch renormalization is used for image denoising, which can address small mini-batch problems. Moreover, BRN can also accelerate the convergence of training the network, and does not have any requirement for a specific hardware platform. As a consequence, the combination of BRN and CNN for image denoising is a good choice for low-configuration hardware devices, such as GTX 960 and GTX 970.</p><p>(3) BRDNet uses dilated convolutions to enlarge the receptive field, which enables the network to extract more context information and reduce the computational cost. Moreover, it can also prevent vanishing or exploding gradients. In addition, residual learning can further promote the image-denoising performance.</p><p>(4) Experimental results prove that BRDNet is robust to both synthetic and real noisy images. The rest of this paper is organized as follows. In Section 2, we provide related techniques of the proposed method. In Section 3, we offer the proposed method. In Section 4, we present experimental results of the proposed method. In Section 5, we present conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep CNNs for Image Denoising</head><p>Deep CNNs are very popular for image denoising. For example, a 17-layer DnCNN <ref type="bibr" target="#b63">[61]</ref> has been proposed as a CNN-based method of predicting noise. This baseline improves the denoising performance by stacking multiple convolutional layers, a skip connection, and batch normalization <ref type="bibr" target="#b27">[25]</ref>. The combination of a CNN and a prior was effective for blind denoising <ref type="bibr" target="#b65">[63]</ref>. A discriminative learning method with an optimization method was used to deal with real noisy images <ref type="bibr" target="#b64">[62]</ref>. Improving a spatial activation function was exploited to reduce the computational cost in image denoising <ref type="bibr" target="#b32">[30]</ref>. The recently proposed generative adversarial network (GAN) is very popular for image denoising <ref type="bibr" target="#b8">[6]</ref>. A CNN with a prior was also very effective for noise of certain type <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b15">13]</ref>. The CNN is very robust for other applications, such as medical images <ref type="bibr" target="#b36">[34]</ref>. The aforementioned deep-network-based-CNN methods <ref type="bibr" target="#b53">[51]</ref> have achieved better performance than BM3D. Motivated by that, we use a deep CNN for image denoisining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Batch Renormalization</head><p>To the best of our knowledge, the manner of end-to-end connection is an important factor for the success of a CNN. A CNN generally consists of an activation function <ref type="bibr" target="#b34">[32]</ref>, pooling operation <ref type="bibr" target="#b23">[21]</ref>, initial parameter setting <ref type="bibr" target="#b24">[22]</ref>, and gradient-based optimization methods <ref type="bibr" target="#b31">[29]</ref> and convolutional kernels. Although these plug-in components can improve the performance in image applications compared with traditional methods, such as multi-layer perception (MLP), the distribution of training data is clearly changed by a convolutional operation. The amount of training data is larger, the predicted results will be more inaccurate. To solve this problem, BN was developed <ref type="bibr" target="#b27">[25]</ref>. The BN method used a normalization operation, and scale and shift operations to resolve the internal covariate shift problem. The proposed BN method can not only prevent exploding or vanishing gradients, and accelerate the convergence of training the network, but can also improve performance. However, BN was invalid for small mini-batches, which seriously limited the applications of BN, such as for image detection and video tracking <ref type="bibr" target="#b26">[24]</ref>. Inspired by the fact that BRN can effectively address the dilemma of BN, because BRN used individual samples instead of the entire mini-batch to approximate the distribution of training data. BRN can effectively solve the small mini-batch size and non-independent identically distributed mini-batch (non-i.i.d. mini-batch) problems. The non-i.i.d. mini-batch problem is that samples in same mini batch are non-independent identically and distributed, which can make machine learning or deep learning methods have poor performance. For more information about BRN, please refer to <ref type="bibr" target="#b26">[24]</ref>. As a consequence, a CNN with BRN is a good choice for image denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Residual Learning and Dilated Convolution</head><p>Residual learning was proposed by He et al. <ref type="bibr" target="#b25">[23]</ref> to achieve the tradeoff between increasing depth and addressing network performance degradation. It fused extracted features and the input of several stack layers as the input of the current layer, which can solve the vanishing or exploding gradients problem. To this end, many variants of residual neural network (ResNet) were proposed for low-level-vision tasks <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b33">31]</ref>. For example, very deep super-resolution (VDSR) <ref type="bibr" target="#b29">[27]</ref> exploited global residual learning (GRL) for image restoration. GRL and gradient clipping operations can make VDSR accelerate the convergence. A deeply recursive convolutional network (DRCN) <ref type="bibr" target="#b30">[28]</ref> combined a recursive mechanism and GRL to address the overfitting problem for image restoration. A deep recursive residual network (DRRN) integrated GRL, local RL, and recursive learning to improve image-restoration performance <ref type="bibr" target="#b55">[53]</ref>.</p><p>Extracting more suitable features is important for image recognition <ref type="bibr" target="#b59">[57,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b58">56,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b42">40]</ref>, image segmentation <ref type="bibr" target="#b41">[39]</ref>, object detection <ref type="bibr" target="#b11">[9]</ref>, image denoising <ref type="bibr" target="#b18">[16]</ref>, and image super-resolution <ref type="bibr" target="#b63">[61,</ref><ref type="bibr" target="#b65">63]</ref>. Traditional CNNs used pooling operations to reduce the dimensionality from original images. However, the results suffered from information loss. Enlarging the receptive field was one of the effective approaches used to address this issue. Increasing depth and filter size are very popular ways to enlarge the receptive field. However, increasing depth may lead to network performance degradation. Enlarging filter size will increase the number of parameters and computational cost. The recently proposed dilated convolution used 3 × 3 filters and large depth to address the above receptive field problems <ref type="bibr" target="#b62">[60]</ref>. The receptive field size of a dilated convolution can use the dilation factor f and number of dilated convolution layers, n, as (4n + 1)×(4n + 1). For example, the designed network has five dilated convolution layers with 3 × 3 filters when f = 2 and n = 10, and the receptive field size of the network is 41 × 41, which is equivalent to a common CNN with 20 layers. Thus, the dilated convolution technique has great prospects. Existing studies show that dilated convolutions also have good effects on image denoising <ref type="bibr" target="#b56">[54]</ref>. However, some scholars ignore the combination of BRN, RL, and dilated convolutions in low-level-vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed CNN-based Denoising Method</head><p>In this section, we propose BRDNet based on a CNN for image denoising. Generally, training of a deep CNN model includes two stages for specific task: network design and model learning on training samples. For the design of network architecture, we concatenated two CNNs to design a novel network. For the model learning, we integrated BRN, RL, and dilated convolutions into the designed network for training a denoising model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>In general, different network architectures can extract different features <ref type="bibr" target="#b69">[67]</ref>. These extracted features are complementary in image denoising. Increasing the width of the network to enhance performance is a good choice for image denoising. Thus, we propose a novel network based on two networks as shown in Figure . 1. This network is called BRDNet and consists of two different networks: an upper and lower networks. The upper network only consists of RL and BRN. The lower network includes BRN, RL, and dilated convolutions. It is known that the receptive field is larger, the designed network will have higher computational cost. Therefore, we choose one network (the lower network) to use dilated convolutions. Take into account the balance of performance and efficiency, the 2-8 and 10-15 layers of the lower network use dilated convolutions to capture more context information. The first and sixteenth layers use BRN to normalize data, which makes the outputs of the two sub-networks keep the same distribution. Also, BRN is very useful for small-batch-size tasks, which is very beneficial for low-configuration hardware platforms, such as GTX960 and GTX970. Next, an RL technique is fused into two-channel networks to improve performance. The effectiveness of each technique was tested (see Section 4. The receptive field can capture more information from the context with the help of the dilation factor. For example, the receptive filed size is (2l +1)×(2l +1) when the dilation factor in the first network is 1, where l is the number of layers. In addition, ⊕ denotes the implementation of the RL idea in this paper, that is, the subtraction operation in BRDNet in practice. The 'Concat' operation is used to concatenate two sub-networks in BRDNet by their channels. For example, if the output sizes of the above two sub-networks are 64 × 3 × 3 × c, the output size of their combinations by concatenation operation is 64 × 3 × 3 × 2c.  We refer to the lower network with a depth of 17 as the second network. The first, ninth, and sixteenth layers in the second network are Conv+BRN+ReLU. Layers 2-8 and 10-15 are dilated convolutions. The final layer is Conv. The filter size of each layer is the same as that for the first network. However, layers 2-8 and 10-15 receive more information from a broader field, because the dilation factor is 2. Specifically, for layers 2-8 and 10-15, their receptive field size can be computed by (4n + 1))(4n + 1), respectively. As a result, the receptive fields of all 17 layers are 3, <ref type="bibr" target="#b9">(7,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b33">31)</ref>, 33, <ref type="bibr" target="#b37">(35,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b61">59)</ref>, and 61, which achieves a performance comparable to that with 30 layers under the same filter-size settings. In other words, dilated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof convolutions can reduce computational cost for image denoising. Also, the dilated convolutions and two sub-networks can reduce depth. That is, BRDNet has only 18 layers, which is relatively shallow, and does not result in vanishing or exploding gradients.</p><p>The merits of BRDNet as described above are four-fold: (1) it increases width by using two sub-networks rather than depth to improve the denoising performance, (2) it uses BRN to address small-batch and internal covariate-shift problems, (3) it applies RL to prevent vanishing or exploding gradients, and (4) it utilizes dilated convolutions to save computational cost. Further, experimental results show that BRDNet is more effective than other state-of-the-art denoising methods, such as DNCNN, FFDNet, and IRCNN, which proves the effectiveness of the proposed network. In Section 4.3, we present results of testing the contributions of RL, BRN, and dilated convolutions for BRDNet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Function</head><p>In our work, motivated by GooLeNet and DnCNN, we choose the mean-square error (MSE) to obtain the optimal parameters of the network. Let x be a clean image and y be a noisy image. When training dataset {x j , y j } N j=1 is given, BRDNet uses RL to obtain a model and predict a residual image f (y), where f (y) denotes the noise mapping. Then, we transform a noisy image into a clean image via x = yf (y). In other words, we have f (y) = yx, and the training samples approximately obtain this equality. Specifically, the optimal parameters can be obtained by minimizing the following loss function with Adam <ref type="bibr" target="#b31">[29]</ref>:</p><formula xml:id="formula_0">l(θ) = 1 2N N ∑ j=1 ∥f (y i , θ) -(y i -x i )∥ 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where N is the number of noisy-image patches and θ denotes the parameters of the proposed model. In general, different regions of an image have different structural information. To this end, the noisy-image patches are easier to use to learn position-specific features than the entire noisy image <ref type="bibr" target="#b70">[68]</ref>. Moreover, using noisy-image patches can significantly save memory and reduce computational cost compared with the entire noisy image <ref type="bibr" target="#b70">[68]</ref>. As seen in Eq.</p><p>(1), it can be known that the objective function is relevant to the noisy image y , clean image x, and residual image f (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Integration of BRN, Dilated Convolution, and RL</head><p>One advantage of BRDNet is the combination of two different and complementary networks for image denoising. As shown in Figure . 1, the first network mainly includes BRN and residual learning. The second network integrates BRN, dilated convolution, and RL. From Figure . 1, we can see that BRDNet can obtain a latent clean image by predicting additive white Gaussian noise with standard deviation σ (σ = 75). That is, at first, BRDNet can be used to predict the noise υ. Then, it uses the obtained noise υ to produce clean image x. The designed network adheres to the following rules.</p><p>First, it is known that deeper networks can result in vanishing or exploding gradients. Thus, we design a novel denoising network called BRDNet, that uses two different sub-networks to reduce the depth of the network and obtain more features. The depth is reduced and it does not generate vanishing or exploding gradients.</p><p>Second, the distribution of training data is changed through a convolutional kernel. BN is considered to be a good choice for addressing the problem. However, it is not very effective when the batch size is small, which limits its applications. In real applications, many hardware devices limited memory, and can run programs with high computational complex. Thus, we use BRN instead of BN to normalize the data, and improve the convergence speed of the denoising network. The principle of BRN is as follows <ref type="bibr" target="#b26">[24]</ref>. Algorithm 1 The implementations of Batch Renormalization Input: Values of x over a training mini-batch B = {x 1...m }; parametersγ, β; current moving mean µ and standard deviation σ; moving average update rate α; maximum allowed corrections τ max , d max . Output:</p><formula xml:id="formula_2">y i = BatchRenorm(x i );updated µ ,σ. µ B ← 1 m m ∑ i=1 x i . σ B ← √ ε + 1 m m ∑ i=1 (x i -µ B ) 2 . r ← stop grdient(clip [ 1 / r max , r max ] ( σ B σ ) ). d ← stop grdient(clip [-dmax,dmax] ( µ B -µ σ ) ). xi ← x i -µ B σ B × γ + d. y i ← γ xi + β. µ := µ + α(µ B -µ) //Update moving averages. σ := σ + α(σ B -σ). Inference: y ← γ × x-µ σ + β.</formula><p>Third, it is known that the deep network can extract more accurate features. However, a deep network will lose some context information. As a consequence, we use dilated convolutions in BRDNet to enlarge the receptive field and capture more context information. Specifically, dilated convolutions can use fewer layers to play the same role as more layers. It is known from previous research that increasing the width can extract more features <ref type="bibr" target="#b54">[52]</ref>, and BRDNet has the network architecture, that increases the width of the network rather than its depth to extract more robust features (referred to as a two-channel network). Therefore, the combination of two-channel networks and dilated convolution is very effective in improving image-denoising performance. Moreover, reduction of the network depth can also prevent vanishing or exploding gradients. In this way, BRDNet can reduce the computational cost. However, the lower network has only dilated convolutions, which can make the two sub-networks generate complementary features and improve the network's generalization ability. It seems that, although the two sub-networks do not have deep depths, integrating them can perform very well in comparison with very deep single networks, e.g., FFDNet and IRCNN. In our opinion, dilated convolutions have similar functions to deep networks in increasing the size of the receptive field. Finally, we use a RL method in BRDNet to improve performance again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we mainly introduce the experimental results from the following aspects: datasets,experimental setting, component analysis, BRDNet for gray synthetic noisy images, and for color synthetic noisy images, real-noisy-image denoising, and running time. First, we present the parameters of BRDNet. Next, we prove the effectiveness of main techniques detailed in this paper. Then, the performance of BRDNet on the BSD68 and Set12 <ref type="bibr" target="#b50">[48]</ref> public datasets for gray synthetic noisy image denoising is reported. Several state-of-the-art methods for gray synthetic noisy image denoising, such as BM3D <ref type="bibr" target="#b13">[11]</ref>, WNNM <ref type="bibr" target="#b21">[19]</ref>, MLP <ref type="bibr" target="#b6">[4]</ref>, trainable nonlinear reaction diffusion (TNRD) <ref type="bibr" target="#b9">[7]</ref>, expected patch log likelihood (EPLL) <ref type="bibr" target="#b70">[68]</ref>, cascade of shrinkage fields (CS-F) <ref type="bibr" target="#b52">[50]</ref>, DnCNN <ref type="bibr" target="#b63">[61]</ref>, IRCNN <ref type="bibr" target="#b64">[62]</ref>, and FFDNet <ref type="bibr" target="#b65">[63]</ref> are chosen for comparison with the proposed method.</p><p>To test the performance of the proposed method, we use the recognized peak signal-to-noise ration (PSNR) <ref type="bibr" target="#b66">[64]</ref> and visual effect to verify the denoising effect. If the PSNR value of the denoising method on the test dataset is larger, the denoising method has exhibited better performance. Furthermore, to clarify the visual effect on the denoised images, we zoom in on one area from one obtained potential clean image to demonstrate it. If the magnified area is clearer, we deem that the tested method is more effective. In particular, P SN R = 10 * log 10 (M AX) 2 /M SE, where M AX represents the maximum pixel value of each image. It is noted that M AX is 1.0 in Figures. <ref type="figure" target="#fig_8">3 to 8</ref>. The M SE is the error between a real clean image and a predicted clean image, and computed by</p><formula xml:id="formula_3">M SE = 1 n n ∑ j=1 n ∑ i=1 (x i j -y i j ) 2</formula><p>, where x i j and y i j denote the pixels of point (i, j) from a given clean and recovered clean images, respectively. To give an example, we assume that a given real clean image from cc dataset is xx in Section 4.5, the corresponding recovered clean image is yy from denoising model of BRDNet. MAX is the maximum pixel between xx and yy. Thus, their PSNR values can be computed by P SN R = 10 * log 10 (M AX) 2 /M SE. Next, we show the performance of BRDNet on color synthetic noisy images denoising. We also illustrate the results of our method and several popular methods on real noisy images. Specifically, we refer to Refs. <ref type="bibr" target="#b63">[61,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b61">59]</ref>, and obtain convincing denoising results of other comparative methods for synthetic and real noisy images. Finally, we measure the computational cost, the results of which show that our method is also very competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets 4.1.1. Training Datasets</head><p>For Gaussian image denoising, we used 3,859 images from the Waterloo Exploration Database to train the model. The training images were cropped into 20 × 58, 319 patches in *.bmp format. The patch size was set as 50 × 50. The patch size was set as 50 × 50 for the following reasons: our designed network consists of two sub-networks, the depth of each being 17, the depth of BRDNet is 18, and the receptive field of one network is 2 × 17 + 1 = 35 and that of the other is 61. If patch size is obviously greater than receptive field size, the designed network will consume more computational cost. Therefore, we calculated the average value of the receptive field size of the two sub-networks as the receptive field of BRDNet, and its size is [(35+61)+2]/2 = 49. Here the patch size is set as 50 × 50 (50&gt;49) in this work. It is noted that patch size of 50 is less receptive field size of the second network, which can not fully map the second sub-network. However, the patch size is greater receptive field size of the first network, which can provide complementary information for the second network. Thus, to make a tradeoff between efficiency and performance, patch size of 50 is proper.</p><p>For real noisy image denoising, we used 100 images from Ref. <ref type="bibr" target="#b60">[58]</ref> to train the model. These real noisy images were captured by different cameras, i.e., Canon 5D Mark II, Canon 80D, Canon 600D, Nikon D800, and Sony A7 II, the sensor sizes of the cameras and scenes are all different. The training images were cropped into 423,200 patches in *.bmp format. Each patch size is the same as the gray and color image patch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Test Datasets</head><p>For gray-noisy image denoising, we use the Gaussian noise to train the denoising model. According to the DnCNN and FFDNet methods, we choose Berkeley segmentation dataset 68 (BS-D68) and Set12 <ref type="bibr" target="#b50">[48]</ref> as test datasets. The BSD68 dataset includes 68 natural images with a size of 481 × 321 or 321 × 481. The Set12 is composed of 12 gray images.</p><p>For color-noisy image denoising, we use CBSD68, Kodak24 <ref type="bibr" target="#b20">[18]</ref>, and McMaster to test the BRDNet for image denoising. The CBSD68 includes 68 color images and is the same background as BSD68. The Kodak24 is composed of 24 natural images and their sizes are 500 × 500. The McMaster consists of 18 color images and their sizes are 500 × 500.</p><p>It is known that real noisy images are usually captured by cameras of different types with different ISO values <ref type="bibr" target="#b48">[46]</ref>. Motivated by the fact, we choose cc <ref type="bibr" target="#b46">[44]</ref> as the test dataset for real noisy image, as shown in Figure . 2. The cc dataset has 15 noisy images, that are captured by three different cameras, i.e., Nikon D800, Nikon D600, and Canon 5D Mark III with different ISO values (i.e., 1,600, 3,200, and 6,400).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>We set the BRDNet depth as 18 for gray synthetic, color synthetic and real noisy images denoising. The objective function is used to predict the residual image as shown in Eq. (1). We utilize learning rate of 1×10 -3 , beta 1 of 0.9, beta 2 of 0.999, epsilon of 1×10 -8 , and the method of Ref. <ref type="bibr" target="#b24">[22]</ref> to initialize the weights. The mini-batch size is 20. The number of epochs is 50 for training the BRDNet models. The learning rates of the 50 epochs vary from 1×10 -3 to 1×10 -4 .</p><p>We apply the Keras packet <ref type="bibr" target="#b10">[8]</ref> to train the proposed BRDNet denoising model. All experiments are implemented in the Ubuntu 14.04 and Python 2.7 environments, and run on a PC with an Intel Core i7-6700 CPU, 16GB RAM, and a Nvidia GeForce GTX 1080Ti graphical processing unit (GPU). The Nvidia CUDA of 8.0 and cuDNN of 7.5 are chosen to accelerate the computational ability of the GPU. It takes approximately 144h to train the proposed model for color synthetic noisy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Component Analysis</head><p>To test the effectiveness of each technique for BRDNet on image denoising, we design six visual images. The effectiveness of the RL for image denoising is shown in Figures. <ref type="figure">3</ref> and<ref type="figure">4</ref>. It is obvious from the figures that, the average PSNR of BRDNet without a RL operation is lower than that of BRDNet with a RL operation. The RL techniques in two sub-networks are used to mapping. Finally, the RL in the end of BRDNet is used to obtain the final clean image via noisy image and predicted noise mapping. The performance of using three RLs and only one RL in BRDNet for image denoising can be shown via 'Two sub-networks without RL' and 'BRDNet' in Table <ref type="table" target="#tab_0">1</ref>. In addition, some other plug-in units, such as BRN and dilated convolutions, are also very beneficial to improve the denoising performance. samples instead of the entire mini-batch to approximate the distribution of training data. This approach is not only effective in tackling small-batch-size and non-i.i.d. mini-batch problems, but also inherits the aforementioned merits of BN. BRN was described in detail in Section 2.3.</p><p>To verify that BRN is more effective in image denoising than BN with a small batch, we design several experiments with σ = 15. Batch sizes are set to 20, 32, and 64, respectively. The training dataset is the Waterloo Exploration Database <ref type="bibr" target="#b43">[41]</ref>. The test set is McMaster <ref type="bibr" target="#b67">[65]</ref>. From Table <ref type="table" target="#tab_2">2</ref>, when the batch size is smaller, BRN's performance is better. For example, BRDNet exhibits excellent performance for Batchsize = 20. Thus, BRN is more suitable for image denoising under small mini-batch conditions. Small mini-batches are very suitable to low-configuration hardware, thus, BRDNet with BRN is very appropriate for real applications. The integration of RL and BRN for image denoising is also interesting. Accordingly, we perform comparative experiments, the results of which are in Figures. <ref type="figure" target="#fig_5">5</ref> and<ref type="figure" target="#fig_6">6</ref>, where the average PSNR of BRN with RL is higher than that of single RL in image denoising .</p><p>Dilated convolution allows the network to capture more information from the context by enlarging the receptive field. Thus, dilated convolutions are used in BRDNet for image denoising. The effectiveness of dilated convolutions is shown in Figures. <ref type="figure" target="#fig_7">7</ref> and<ref type="figure" target="#fig_8">8</ref>. It can be seen from the figures that the performance of the combination of RL, BRN, and dilated convolution is better than that of the combination of RL and BRN for image denoising. Moreover, the PSNR of the lower network is higher than that of the lower network without dilated convolutions from     <ref type="bibr" target="#b69">[67]</ref> that are complementary in image denoising. Thus, we design two different network architectures for BR-Net (i.e., upper and lower networks) to improve denoising performance. The fusion of two networks is more useful than a single network in image denoising. For example, BRDNet has a better PSNR than these of the upper and lower networks when σ = 50, as shown Table <ref type="table" target="#tab_0">1</ref>. In addition, it is noteworthy that we use only dilated convolutions in the lower network for image denoising, which can not only produce a greater difference between two networks, but also have a higher efficiency than those of two networks with dilated convolutions, proved in Table <ref type="table" target="#tab_3">3</ref>. When two sub-networks (i.e., two lower networks) utilize dilated convolutions for 2-8 and 10-15 layers, their running time is higher than that of BRNet. However, their performance is the same as shown in Table <ref type="table" target="#tab_0">1</ref>. Thus, network architecture of BRDNet is proper. Further, BRDNet's performance is illustrated in Sections 4.4-4.6.   <ref type="table" target="#tab_4">4</ref>, the proposed BRDNet can obtain the highest PSNR, which is better than those of the benchmarks, BM3D and DnCNN, for gray-image denoising. The best and second-best PSNR results for different σ values are highlighted in red and blue, respectively, in Table <ref type="table" target="#tab_4">4</ref>. The average PSNR of BRDNet is 0.72 dB higher than that of BM3D for σ = 25, which indicates that BRDNet has better performance.</p><p>To easily observe the performance of BRDNet and other methods, we zoom in on one area from one potential clean image obtained using different methods, as illustrated in Figure. 9. To observe the performance of a single class image, we use Set12 to conduct the experiments. Table <ref type="table">5</ref> shows the PSNR values of a single image from Set12 obtained using the proposed method and the above-    For color-noisy image denoising, we exploit six noise levels (σ = 15, 25, 35, 50, 60, 75) to train the models. We compar BRDNet with the state-of-the-art methods on the CBSD68, Kodak24 <ref type="bibr" target="#b20">[18]</ref>, and McMaster datasets for color-image denoising. The best and second-best PSNR results for different σ values are highlighted in red and blue, respectively, in Table <ref type="table">6</ref>. It can be seen from the table that the proposed BRDNet is more effective than other methods in color-image denoising, which indicates that the proposed method is more robust to low-and high-level noises. For example, for Kodak24 and σ = 75, the average PSNR of BRDNet is 0.24 dB higher than that of FFDNet. For McMaster and σ = 15, the average PSNR of BRDNet is 0.50 dB higher than that obtained using the proposed method are clearer than those obtained using other methods, that is, the proposed method is more suitable for color-image denoising. From the experimental results for the gray and color images, it can be seen that the proposed method is more robust and effective than other state-of-the-art methods in image denoising. From these results, it is known that the proposed BRDNet is superior to both traditional denoising methods, such as BM3D and state-of- of 256 × 256, 512 × 512, and 1024 × 1024 with σ = 25. Also, we find that BRDNet is also very competitive with some methods on a GPU, such as DnCNN, as shown in Table <ref type="table" target="#tab_6">8</ref>.</p><p>A good denoiser should realize a tradeoff between PSNR and running time <ref type="bibr" target="#b64">[62]</ref>. The effectiveness of the proposed BRDNet with BRN has been proved via previously described experiments. For example, BRDNet with BRN has superiority to low-configuration hardware, such as GTX960 and GTX970. Further, due to the complementarity of two sub-networks, BRDNet is more robust than state-of-the-art denoising methods, such as DnCNN in color synthetic and real noisy images. In addition, shallow architecture of BRDNet with dilated convolutions is also very competitive with two DnCNNs in performance and complexity for image denoising as shown in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_7">9</ref>, respectively. That is, the proposed BRDNet has small computational cost, which is very suitable to smart phone and camera. In summary, those experiments all prove that the proposed BRDNet is a stronger denoiser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l r e p r o o f</head><p>Journal Pre-proof </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel model-based-CNN denoiser named BRDNet, that combines two different networks to enhance image-denoising performance. Moreover, BRDNet uses BRN, RL, and dilated convolutions to improve the denoising performance, and make the model more easily trained. BRN is used not only to accelerate the convergence of BRDNet, but also to address the small-batch problem. The RL is applied to separate the noise from noisy images, and to obtain latent clean images in BRDNet. Dilated convolutions can enlarge the receptive field to obtain more context information. Experimental results show that BRDNet is very competitive with other stateof-the-art methods for image denoising. In the future, we plan to use CNN with prior knowledge to deal with more complex real noisy image denoising, such as low-light and blurred images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3), and the performance of this network is shown in Section 4. The detailed information of BRDNet is introduced in the latter section. The depth of the upper network (also called the first network) is 17. It consists of two different types of layers: Conv+BRN+ReLU and Conv. Conv, BRN, and ReLU are referred to as convolution, batch renormalization, and rectified linear units, respectively. Conv+BRN+ReLU means that convolution, batch renormalization, and rectified linear units are implemented in sequence. Layers 1-16 are Conv+BRN+ReLU and the 17th layer is Conv. Except for the first and last layers, the size of each layer is 64×3 × 3 × 64. The sizes of the first and last layers are c × 3 × 3 × 64 and 64 × 3 × 3 × c, respectively, where c denotes the number of channels. Here c = 1 and c = 3 represent the numbers of the pixel channels of gray and color images, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of proposed BRDNet network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Gaussian denoising results for two specific models, one consisting of RL, and the other of only convolutional layers. Both were trained with σ = 15. Results for 68 images from the CBSD68 dataset were evaluated.</figDesc><graphic coords="13,116.09,378.64,359.49,269.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>J o u r</head><label></label><figDesc>n a P r e -p r o o f Journal Pre-proof</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Gaussian denoising results for two specific models, one consisting of BRN and RL and the other of only RL. Both were trained with σ = 15. Results for 68 images from the CBSD68 dataset were are evaluated.</figDesc><graphic coords="15,116.09,111.55,346.15,259.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Gaussian denoising results for two specific models, one consisting of BRN and RL and the other of RL only. Both were trained with σ = 50. Results for 68 images from the CBSD68 dataset were evaluated.</figDesc><graphic coords="16,116.09,111.55,346.15,259.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Gaussian denoising results of two specific models, one consisting of BRN and RL and the other of RL, BRN, and Dilated Conv. Both were trained with σ = 15. Results for 68 images from the CBSD68 dataset were are evaluated.</figDesc><graphic coords="17,116.09,111.55,346.15,259.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Gaussian denoising results of two specific models, one consisting of BRN and RL and the other consists of RL, BRN and Dilated Conv. Both were are trained with σ = 50. Results for 68 images from the CBSD68 were evaluated.</figDesc><graphic coords="18,116.09,111.55,346.15,259.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Twelve widely-used test images in experiments.</figDesc><graphic coords="18,237.62,516.10,59.27,61.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>J o u rFigure 9 :Figure 11 :</head><label>911</label><figDesc>Figure 9: Denoising results of one image from the BSD68 dataset with noise level 25 using for different methods: (a) original image, (b) noisy image /20.30 dB, (c) WNNM/29.75 dB, (d) E-PLL/29.59 dB, (e) TNRD/29.76 dB, (f) DnCNN/30.16 dB, (g) BM3D/29.53 dB, (h) IRCNN/30.07 dB, and(i) BRDNet/30.27 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Gaussian denoising results of eight specific models for color synthetic noisy images. All four models were trained with σ = 50 and evaluated on the CBSD68 dataset.</figDesc><table><row><cell>Methods</cell><cell>Epochs=50</cell></row><row><cell>Upper network</cell><cell>27.33</cell></row><row><cell>Lower network</cell><cell>28.06</cell></row><row><cell>Lower network without dilated convolutions</cell><cell>27.74</cell></row><row><cell>Two sub-networks with dilated convolutions</cell><cell>28.16</cell></row><row><cell>Two sub-networks without RL</cell><cell>28.11</cell></row><row><cell>DnCNN</cell><cell>28.01</cell></row><row><cell>Concatenation of two DnCNNs</cell><cell>28.01</cell></row><row><cell>BRDNet</cell><cell>28.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 , which</head><label>1which</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of image-denoising results for BRN and BN in proposed network.</figDesc><table><row><cell cols="5">Datasets Methods Batchsize=20 Batchsize=32 Batchsize=64</cell></row><row><cell>McMaster</cell><cell>BRN BN</cell><cell>35.08 34.94</cell><cell>35.09 35.07</cell><cell>35.08 35.07</cell></row><row><cell cols="5">also proves the effectiveness of dilated convolutions in BRDNet for image denoising.</cell></row><row><cell cols="5">It is known that different network architectures can generate different features</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Running time of two different methods for denoising images of sizes 256×256, 512×512 and 1024 × 1024. BRDNet for Gray-and Color-Image denoising For gray-noisy image denoising, BRDNet and several state-of-the-art methods (i.e., BM3D, WNNM, EPLL, MLP, CFS, TNRD, DnCNN, IRCNN and FFDNet) are used to conduct experiments on the BSD68. As shown in Table</figDesc><table><row><cell cols="5">Methods Two sub-networks with dilated convolutions GPU Device 256 × 256 512 × 512 1024 × 1024 0.081 0.238 0.935</cell></row><row><cell>BRDNet</cell><cell>GPU</cell><cell>0.062</cell><cell>0.207</cell><cell>0.788</cell></row><row><cell>4.4.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average PSNR (dB) results of different methods on BSD68 dataset with noise levels of 15, 25 and 50. The best and second-best PSNR results for each method are highlighted in red and blue, respectively, in Table5. Figure.11 illustrates the visual results obtained from the aforementioned methods. It can be seen from the figure that the proposed method can recover and obtain clearer images compared with the other methods.</figDesc><table><row><cell cols="11">Methods BM3D WNNM EPLL MLP CSF TNRD DnCNN IRCNN FFDNet BRDNet</cell></row><row><cell>σ = 15</cell><cell>31.07</cell><cell>31.37</cell><cell>31.21</cell><cell>-</cell><cell cols="2">31.24 31.42</cell><cell>31.72</cell><cell>31.63</cell><cell>31.62</cell><cell>31.79</cell></row><row><cell>σ = 25</cell><cell>28.57</cell><cell>28.83</cell><cell cols="4">28.68 28.96 28.74 28.92</cell><cell>29.23</cell><cell>29.15</cell><cell>29.19</cell><cell>29.29</cell></row><row><cell>σ = 50</cell><cell>25.62</cell><cell>25.87</cell><cell cols="2">25.67 26.03</cell><cell>-</cell><cell>25.97</cell><cell>26.23</cell><cell>26.19</cell><cell>26.30</cell><cell>26.36</cell></row><row><cell cols="2">mentioned methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>PSNR (dB) results for different methods on real noisy images.</figDesc><table><row><cell>Camera settings</cell><cell cols="8">CBM3D [11] MLP [4] TNRD [7] DnCNN [61] CSF [50] NC [36] WNNM [19] BRDNet</cell></row><row><cell></cell><cell>39.76</cell><cell>39.00</cell><cell>39.51</cell><cell>37.26</cell><cell>35.68</cell><cell>38.76</cell><cell>37.51</cell><cell>37.63</cell></row><row><cell>Canon 5D ISO=3200</cell><cell>36.40</cell><cell>36.34</cell><cell>36.47</cell><cell>34.13</cell><cell>34.03</cell><cell>35.69</cell><cell>33.86</cell><cell>37.28</cell></row><row><cell></cell><cell>36.37</cell><cell>36.33</cell><cell>36.45</cell><cell>34.09</cell><cell>32.63</cell><cell>35.54</cell><cell>31.43</cell><cell>37.75</cell></row><row><cell></cell><cell>34.18</cell><cell>34.70</cell><cell>34.79</cell><cell>33.62</cell><cell>31.78</cell><cell>35.57</cell><cell>33.46</cell><cell>34.55</cell></row><row><cell>Nikon D600 ISO=3200</cell><cell>35.07</cell><cell>36.20</cell><cell>36.37</cell><cell>34.48</cell><cell>35.16</cell><cell>36.70</cell><cell>36.09</cell><cell>35.99</cell></row><row><cell></cell><cell>37.13</cell><cell>39.33</cell><cell>39.49</cell><cell>35.41</cell><cell>39.98</cell><cell>39.28</cell><cell>39.86</cell><cell>38.62</cell></row><row><cell></cell><cell>36.81</cell><cell>37.95</cell><cell>38.11</cell><cell>35.79</cell><cell>34.84</cell><cell>38.01</cell><cell>36.35</cell><cell>39.22</cell></row><row><cell>Nikon D800 ISO=1600</cell><cell>37.76</cell><cell>40.23</cell><cell>40.52</cell><cell>36.08</cell><cell>38.42</cell><cell>39.05</cell><cell>39.99</cell><cell>39.67</cell></row><row><cell></cell><cell>37.51</cell><cell>37.94</cell><cell>38.17</cell><cell>35.48</cell><cell>35.79</cell><cell>38.20</cell><cell>37.15</cell><cell>39.04</cell></row><row><cell></cell><cell>35.05</cell><cell>37.55</cell><cell>37.69</cell><cell>34.08</cell><cell>38.36</cell><cell>38.07</cell><cell>38.60</cell><cell>38.28</cell></row><row><cell>Nikon D800 ISO=3200</cell><cell>34.07</cell><cell>35.91</cell><cell>35.90</cell><cell>33.70</cell><cell>35.53</cell><cell>35.72</cell><cell>36.04</cell><cell>37.18</cell></row><row><cell></cell><cell>34.42</cell><cell>38.15</cell><cell>38.21</cell><cell>33.31</cell><cell>40.05</cell><cell>36.76</cell><cell>39.73</cell><cell>38.85</cell></row><row><cell></cell><cell>31.13</cell><cell>32.69</cell><cell>32.81</cell><cell>29.83</cell><cell>34.08</cell><cell>33.49</cell><cell>33.29</cell><cell>32.75</cell></row><row><cell>Nikon D800 ISO=6400</cell><cell>31.22</cell><cell>32.33</cell><cell>32.33</cell><cell>30.55</cell><cell>32.13</cell><cell>32.79</cell><cell>31.16</cell><cell>33.24</cell></row><row><cell></cell><cell>30.97</cell><cell>32.29</cell><cell>32.29</cell><cell>30.09</cell><cell>31.52</cell><cell>32.86</cell><cell>31.98</cell><cell>32.89</cell></row><row><cell>Average</cell><cell>35.19</cell><cell>36.46</cell><cell>36.61</cell><cell>33.86</cell><cell>35.33</cell><cell>36.43</cell><cell>35.77</cell><cell>36.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Run time for different methods in denoising images of sizes 256 × 256, 512 × 512, and 1024 × 1024.</figDesc><table><row><cell>Methods BM3D[11]</cell><cell cols="4">Device 256 × 256 512 × 512 1024 × 1024 CPU 0.59 2.52 10.77</cell></row><row><cell cols="2">WNNM[19] CPU</cell><cell>203.1</cell><cell>773.2</cell><cell>2536.4</cell></row><row><cell>EPLL[68]</cell><cell>CPU</cell><cell>25.4</cell><cell>45.5</cell><cell>422.1</cell></row><row><cell>MLP[4]</cell><cell>CPU</cell><cell>1.42</cell><cell>5.51</cell><cell>19.4</cell></row><row><cell>TNRD[7]</cell><cell>CPU</cell><cell>0.45</cell><cell>1.33</cell><cell>4.61</cell></row><row><cell>CSF[50]</cell><cell>GPU</cell><cell>-</cell><cell>0.92</cell><cell>1.72</cell></row><row><cell cols="2">DnCNN[61] GPU</cell><cell>0.036</cell><cell>0.111</cell><cell>0.410</cell></row><row><cell>BRDNet</cell><cell>GPU</cell><cell>0.062</cell><cell>0.207</cell><cell>0.788</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Complexity analysis of BRDNet, DnCNN and two DnCNNs.</figDesc><table><row><cell>Methods</cell><cell cols="2">Parameters GFlops</cell></row><row><cell>DnCNN[61]</cell><cell>0.56M</cell><cell>1.40</cell></row><row><cell>Concatenation of two DnCNNs</cell><cell>1.11M</cell><cell>2.78</cell></row><row><cell>BRDNet</cell><cell>1.11M</cell><cell>2.78</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J o u r n a l P r e -p r o o fJournal Pre-proof</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J o u r n a l P r e -p r o o f</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>J o u r n a l P r e -p r o o f Journal Pre-proof Acknowledgments This paper is supported in part by the National Nature Science Foundation of China (Grant no. 61876051), in part by the Shenzhen Municipal Science and Technology Innovation Council (Gant no. JCYJ20170811155725434).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">BRDNet for Real Noisy Image Denoising</head><p>To test the performance of the proposed method for real noisy images, we choose the popular methods, e.g., color block-matching and 3-D filtering(CBM3D), MLP , TNRD, DnCNN, CSF <ref type="bibr" target="#b52">[50]</ref>, noise clinic (NC) <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b37">35]</ref> and WNNM <ref type="bibr" target="#b21">[19]</ref> to design contrast experiments. It can be seen from Table <ref type="table">7</ref> that using the proposed method results in 0.12 dB, 0.96 dB and 2.87 dB improvements over TNRD, WNNM, and DnCNN, respectively. In the table, red and blue entries represent the best and second-best results, respectively, for condition in Table <ref type="table">7</ref>. Thus, the proposed method is more suitable to deal with more complex noisy images, such as real noisy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Running Time</head><p>Testing speed is a more important index than training speed in evaluating performance for lowlevel vision <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b52">50]</ref>. We therefore compare BM3D, WNNM, EPLL, MLP, TNRD, CSF and DnCNN with the proposed method in running time experiments, using gray noisy images of sizes We would like to submit the manuscript entitled "Image Denoising Using Deep CNN with Batch Renormalization", which we wish to be considered for publication in "Neural Networks". No conflict of interest exits in the submission of this manuscript, and manuscript is approved by all authors for publication. We would like to declare on behalf of my co-authors that the work described was original research that has not been published previously, and not under consideration for publication elsewhere, in whole or in part. All the authors listed have approved the manuscript that is enclosed.</p><p>In this paper, a batch-renormalization denoising network (BRDNet) is proposed for image denoising. Specifically, we combine two networks to increase the width of the network, and thus obtain more features. Because batch renormalization is fused into BRDNet, we can address the internal covariate shift and small mini-batch problems. Residual learning is also adopted in a holistic way to facilitate network training. Dilated convolutions are exploited to extract more information for denoising tasks. Extensive experimental results show that BRDNet outperforms state-of-the-art image-denoising methods. The code of BRDNet is accessible at http://www.yongxu.org/lunwen.html.</p><p>We deeply appreciate your consideration of my manuscript, and look forward to receiving comments from the reviewers. If you have any queries, please don't hesitate to contact us at the address below. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">PSNR (dB) results for different methods on 12 widely used images with noise levels of 15</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">man House Peppers Starfish Monarch Airplane Parrot Lena Barbara Boat Man Couple Average Noise level σ = 15</title>
		<author>
			<persName><forename type="first">C</forename><surname>Images</surname></persName>
		</author>
		<idno>BM3D[11] 31.91 34.93 32.69 31.14 31.85 31.07 31.37 34.26 33.10 32.13 31.92 32.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">the-art denoising methods, such as DnCNN in gray and color noisy images. Figure 12: Denoising results for one color image from the McMaster dataset with noise level 35: (a) original image/ σ = 35, (b) noisy image/18</title>
		<idno>CBM3D/31.04 dB</idno>
		<imprint>
			<biblScope unit="volume">62</biblScope>
		</imprint>
	</monogr>
	<note>d) FFDNet/31.94 dB, and</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning real-time mrf inference for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training an active random field for real-time image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2451" to="2462" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2419" to="2434" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An algorithm for total variation minimization and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical imaging and vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Keras: Deep learning library for theano and tensorflow</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/k7" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich features and precise localization with region proposal network for object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Nonlocality-reinforced convolutional neural networks for image denoising</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02112</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Denoising prior driven deep neural network for image restoration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06756</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An improved quantum-behaved particle swarm optimization for endmember extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked convolutional denoising auto-encoders for feature representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature extraction methods for palmprint recognition: A survey and evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kodak lossless true color image suite</title>
		<author>
			<persName><forename type="first">R</forename><surname>Franzen</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak4" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition using both visible light image and near-infrared image and a deep network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1945" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Formresnet: formatted residual learning for image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1034" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Kligvasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06445</idno>
		<title level="m">Learning a spatial activation function for efficient image restoration</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Iterative residual network for deep joint image demosaicking and denoising</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06403</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep cnn based mr image denoising for tumor segmentation using watershed transform</title>
		<author>
			<persName><forename type="first">G</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Iskandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alghazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2.3</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiscale image blind denoising</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3149" to="3161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The noise clinic: a blind image denoising algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Pro-J o u r n a l P r e -p r o o f Journal Pre-proof cessing On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Group-sparse representation with dictionary learning for medical image denoising and fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3450" to="3459" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cvpr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structurally incoherent low-rank nonnegative matrix factorization for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5248" to="5260" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Waterloo exploration database: New challenges for image quality assessment models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wavelet-based image denoising using a markov random field a priori model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malfait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="565" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A holistic approach to cross-channel image noise modeling and its application to image denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joo Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Nonlocal similarity modeling and deep cnn gradient prior for super resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="916" to="920" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Si-Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06042</idno>
		<title level="m">A concatenated residual network for image deblurring</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dilated residual network for image denoising</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<idno>arX- iv:1708.05473</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Low-rank representation with adaptive graph regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Inter-class sparsity based discriminative least square regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="36" to="47" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02603</idno>
		<title level="m">Real-world noisy image denoising: A new benchmark</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">External prior guided internal prior learning for real-world noisy image denoising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2996" to="3010" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>arX- iv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn based image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Color demosaicking by local directional interpolation and nonlocal adaptive thresholding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23016</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Image restoration: From sparse and low-rank priors to deep priors [lecture notes]</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="172" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gradient histogram estimation and preservation for texture enhanced image denoising</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2459" to="2472" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
