<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pseudo-Gray Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kenneth</forename><surname>Zeger</surname></persName>
						</author>
						<author>
							<persName><roleName>FELLOW, IEEE</roleName><forename type="first">Allen</forename><surname>Gersho</surname></persName>
						</author>
						<title level="a" type="main">Pseudo-Gray Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31242B33652FBEB4EC8D7E35BA3E297A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A pseudo-gray code is an assignment of n-bit binary indexes to 2" points in a Euclidean space so that the Hamming distance between two points corresponds closely to the Euclidean distance.</p><p>Pseudo-Gray coding provides a redundancy-free error protection scheme for vector quantization (VQ) of analog signals when the binary indexes are used as channel symbols on a discrete memoryless channel and the points are signal codevectors. Binary indexes are assigned to codevectors in a way that reduces the average quantization distortion introduced in the reproduced source vectors when a transmitted index is corrupted by channel noise. A globally optimal solution to this problem is generally intractable due to an inherently large computational complexity. A locally optimal solution, the Binary Switching Algorithm, is introduced, based upon the objective of minimizing a useful upper bound on the average system distortion. The algorithm yields a signillcant, duction in average distortion, and converges in reasonable running times. The use of pseudo-Gray coding is motivated by the increasing need for low bit-rate VQ-based encoding systems that operate on noisy channels, such as in mobile radio speceh communications.</p><p>' """1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>long-standing problem in quantization theory is how to effec-A tively combat the performance degradation caused by noisy channels. One approach is to use redundancy bits for error control coding. A fairly straightforward argument shows, however, that superior performance can always be achieved by instead using these extra bits to design a higher resolution quantizer with no explicit error control. In effect, the error control coding is built into the higher resolution quantizer.</p><p>On the other hand, for a quantizer with a fixed codebook and no error correction, performance gain can be achieved by judiciously assigning channel words (i.e., binary indexes) to codevectors. Intuitively, vectors that are "close" to each other should be assigned indexes which differ in as few bit positions as possible. In this way, channel errors cause an index to be decoded as a vector which approximates the codevector that was supposed to be correctly decoded (i.e., without channel noise). The problem of how to make these assignments in an effective manner has essentially been an unanswered question until recently. This paper proposes a solution to this question based on a certain upper bound approximation that turns out to be exact in the case of the mean-square distortion criterion. An index assignment algorithm is presented that proves very effective in controlling performance degradation caused by channel noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>Vector Quantization (VQ) has been established as an important and successful source coding technique in many digital communica- tion applications (see, for example, 111). Real-time implementations of VQ coding systems are beginning to emerge and are likely to play an increasing role in voice, image, and video communications systems in the future. Often, the binary indexes produced by a VQ coder account for most or all of the information transmitted to a receiver across a noisy channel. This commonly occurs in mobile radio and satellite communications, where low bit-rate digital speech applications can suffer severe quality degradation from the effects of channel noise on transmitted codevector indexes.</p><p>Several studies of classical Gray coding have been conducted for scalar quantization of sources. The numerical and perceptual effects of channel errors on PCM signals have been investigated in <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr">Rydbeck and Sundberg [4]</ref> demonstrated the importance of proper index assignment in scalar quantizer design for noisy channels. The design of optimal codes for discrete-alphabet uniformly distributed sources using a mean-square fidelity criterion was considered by Wolf and Redinbo <ref type="bibr" target="#b6">[7]</ref> and Redinbo <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">[9]</ref>. Many papers have studied combined source and channel coding, but generally ignore the channel codeword assignment question (see <ref type="bibr">[lo]-[16]</ref>). The problem of finding an optimal index mapping function is, however, considered by <ref type="bibr">Farvardin and Vaishampayan [17]</ref> in the context of combined source-channel coding of scalar PCM signals. Cox et al.</p><p>1181 have designed an ad hoc channel error protection scheme for subband coding of speech signals, by identifying important channel bits and using rate compatible punctured convolutional coding. Specific techniques for reducing the effects of channel errors without adding redundancy bits have been given for certain image coding applications using Walsh Transforms <ref type="bibr">[19]</ref> and DFCM <ref type="bibr">[20]</ref>.</p><p>Few research efforts in the past have focused on the index assignment problem for vector quantizers. The growing popularity of vector quantization as a realizable source coding technique has drawn attention to the advantages of codevector index assigning schemes, and several limited heuristic algorithms have recently been described [21]- <ref type="bibr">[23]</ref>. In <ref type="bibr">[24]</ref>, a technique is described that recursively constructs good index assignments by examining the related minimum weight hypercube problem. By using a well-known solution to the weighted matching problem in graph theory, their algorithm is both efficient and yields good results for certain quantized images corrupted by channel noise. Finally, simulated annealing techniques have been utilized for improving the index assignment functions of vector quantizers in <ref type="bibr">[25]-[27]</ref>.</p><p>This paper offers a systematic examination of the index assignment problem in vector quantization and presents an algorithm that effectively reduces the average distortion of a VQ system by rearranging the positions of codevectors in a given codebook. The algorithm is guaranteed to converge to a locally optimal state after a finite number of iterations, and operates over a wide class of common distortion functions. Pseudo-Gray coding can provide additional protection from channel errors in applications where the primary channel coding is achieved as part of the digital modulation scheme, as in trellis-coded modulation. This work was motivated by such a situation where the application was speech coding for mobile satellite communications <ref type="bibr">[21]</ref>.</p><p>In Section 11, a formal model of vector quantization on a noisy channel is given, and the pseudo-Gray coding problem is introduced. In Section III, equations describing an upper bound on the expected overall system distortion due to quantization and channel errors are given. The class of distortion measures considered includes metrics and positive powers of norms. For each distortion measure, it is shown that one can minimize a quantity (common to each distortion function) independent of the source statistics, with a 0090-6778/90/1100-2147$01.00 O 1990 IEEE certain upper bound assumption. The general case of multiple bit errors occurring in a single transmitted binary index is initially examined. The results are then specialized to the case of at most a single bit error per transmitted index. These results lead to the goal of seeking a rearrangement of a given codebook that minimizes an expected distortion term. In the single bit error case, it is noted that this minimization is independent of the value of the channel's crossover probability. Section IV describes the Binary Switching Algorithm in detail, and discusses the computational complexity requirements of the algorithm. Numerical results of experiments using the Binary Switching Algorithm are given in Section V, which exhibit the achievable gain in SNR using pseudo-Gray coding. It is shown that on binary symmetric channels, over a wide range of bit error probabilities, one can obtain SNR gains in the range of 1-5 dB with vector quantized waveforms. Such an improvement is often quite significant, and particularly so considering that an increase in the bit rate is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VEC~OR QUANTIZATION MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . VQ on a Noiseless Channel</head><p>Vector Quantization encodes each vector from a sequence of source vectors with a channel symbol-a binary word chosen from a finite set. A typical VQ system contains a finite predetermined collection of codevectors (a codebook), and a vector distortion measure which, when given two vectors, yields a distance (or distortion) between them. A sequence of input vectors (e.g., a block of sequential samples of a waveform) is coded by the VQ system by associating with each input vector the binary word (or index) of a codevector whose distance from the input vector is minimized. This index is subsequently transmitted to a receiver which decodes the codevector associated with the index (by a table lookup operation) and uses the codevector as an approximation to the original input vector to the system.</p><p>The design of codebooks for a given digital encoding system is performed before the actual use of the VQ coder. Only VQ systems that use static codebooks, which retain the same codevectors throughout their use, will be considered. At the time of VQ operation, the entire codebook is completely determined and available to both a transmitter (encoder) and a receiver (decoder). In systems with dynamically changing codebooks, the algorithms described in this paper can be generalized by reexecuting the procedures whenever a codebook undergoes change, although this may prove very costly in terms of computational complexity.</p><p>A vector quantizer Q is a mapping of p-dimensional Euclidean space R P into a finite subset Y of R P given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q : R p -+ Y (1)</head><p>where Y = ( y , , , y , ; -~, y , -, } a n d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>y , e R P f o r O ~i ~N -</head><p>1.</p><p>The ordered set Y is known as a codebook and the N elements of Y are called codevectors. For convenience, it is assumed that the size of the codebook Y is N = 2', where b is a positive integer.</p><p>The subscripts of the codevectors are the codevector indexes, each index representing a b-bit binary channel word, written as decimal integers for notational brevity. The set of N vector indexes can be written as (0, A vector quantizer Q can be decomposed into two separate mappings: a coder and a decoder. From this definition, it follows that</p><formula xml:id="formula_0">N-U 1 R i = R P and R i f l R j = I#J i = O</formula><p>whenever i f j . ( <ref type="formula">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>)</head><p>A vector quantizer is completely specified by its partition { R i } and output set Y. Fig. <ref type="figure">1</ref> is a block diagram of a vector quantizer on a noiseless channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VQ on a Noisy Channel</head><p>The effects of channel errors on indexes can result in significant distortion in decoded vectors. The magnitude of this degradation is measured in terms of the distortion function defined for a VQ system. The indexing of vectors in a codebook influences the average distortion caused by channel errors. By arranging the codevectors such that index errors cause incorrectly received vectors to be close on average to the original vectors, the expected distortion due to channel noise can be reduced. The problem of finding the best codebook rearrangement involves searching every possible index assignment for the one that yields the best possible performance. This task requires enormous computational complexity due to the combinatoric nature of the problem, since there are N! assignments of indexes to N codevectors. A suboptimal solution to this problem is thus sought.</p><p>Define the bitwise addition operation ai :{O, l}, x (0, l}, 4 (0, 1 I b as foUows. If i = i,i,i b E (0, I } ~ and j = j, j 2 . . . j , E (0, l},, where i,, !, E (0,l) for 1 s k s b, then i e~ j is the ( i e (0, 11,)</p><p>where 9 is a random variable taking on values from the set (0, l},.</p><p>7 is a function of the random variable 9 that describes the effect of channel errors upon a transmitted binary index.</p><p>Let S , denote the set of all one-to-one functions *:(O, l } b -P (0, l}*. Each of the N ! bijections (permutations) ' K E S, is called an index assignment function for the quantizer Q. For each index i e (0, l},, ?r uniquely maps i to another index of (0, namely, r(i). A permutation can be thought of as a rearrangement of the order in which vectors appear in a VQ codebook.</p><p>Definition: For any vector quantizer Q = D O C and for any permutation r of (0, l } b , a noisy channel vector quantizer, Q , , is a mapping from R P to the set of codevectors Y given by</p><formula xml:id="formula_1">Q, = D o ~-' o ~~u o C (<label>6</label></formula><formula xml:id="formula_2">)</formula><p>where 7 is a noisy channel mapping, and U-' is the inverse function of the permutation U.</p><p>For any input vector, the output quantized vector produced by Q, is a function of the random variable 9 associated with the noisy channel and the input random variable from RP. For a given channel, Q, can be represented by the triple ( C , D , r ) . A block diagram depicting a noisy channel vector quantizer is shown in Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>In the case of a noiseless channel, the channel noise random variable q is identically zero, 7 is the identity mapping on (0, </p><p>Y j -Let i = C ( X ) be the unique integer in (0, l } b such that XcRi and assume that the source vectors are independent of the channel noise q . Let p denote the probability mass function of the codevectors induced by the source statistics, given by j ~ 7c-1 D *</p><p>where Pr denotes the probability of a random event. The number p ( y , ) gives the probability that a particular codevector is selected by the encoder to represent an analog input vector. This probability distribution is determined by the input signal vector statistics. The vector probabilities p ( yk) can be computed a priori by tabulating statistics on the relative frequency with which each codevector is chosen as the best match.</p><p>The binary index transmitted across the channel to represent X is the received index is ~( a ( i ) ) , and the index of the codevector used to approximate X in the receiver is a -' ( ~( ~( i ) ) ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If we define</head><formula xml:id="formula_4">j A a -' ( . ( a ( i ) ) ) = a -' ( a ( i ) e q ) ,</formula><p>(</p><p>then codevector yi is chosen by the encoder as an approximation of X, and y j is the decoded codevector selected as an approximation of X. Index i is a random variable depending on the source statistics of X, and index j is a random variable depending on both X and on the channel noise statistics determined by the discrete random variable q , as well as on the deterministic mapping a. The total distortion due to the combined effect of the quantization and channel index errors is d( X, y j ) . In order to optimize the performance of the coding system for a given codebook, the expected value of this distortion must be minimized over all possible permutations in S,. Thus, we take the expectation in <ref type="bibr" target="#b6">(7)</ref> by averaging over both the distribution of the input vector X and the distribution of channel bit errors. The quantity to be minimized can be written as neighbor and centroid conditions. In the presence of channel noise, generalized versions of these conditions are satisfied [ 151. <ref type="bibr">Totty and</ref> Clark <ref type="bibr" target="#b2">[3]</ref> showed (later generalized by Messerschmitt [28]) that when channel noise is added to an optimal quantizer, the mean-square error can be separated into the sum of the mean-square error for the quantizer without channel noise and a term due only to channel noise, as given by the following Lemma.</p><p>Lemma 1: The average distortion of a noisy channel quantizer with the mean-square distortion function that satisfies the centroid condition can be written as E I( X -y j I( = E 11 X -yi 11 + E )I yi -Yj1I2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MINIMUM DISTORTION INDEX ASSIGNMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Optimal Pseudo-Gray Coding</head><p>To minimize (12) requires determining which permutation K minimizes the quantity e,. In general, this may be quite a difficult task since both the source and channel statistics must be taken into account. There is, however, an asymptotic procedure for determining the best permutation a , the permutation that achieves emin. A training set T is a finite collection of vectors,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T = { W ~. W ~, . . . , W M } (13)</head><p>where wk E R P for 1 I k I M . In practice, a training set is much larger than a codebook, i.e., I TI % I Y I. The expected system distortion e , for a given permutation a can be approximated by</p><formula xml:id="formula_6">i M</formula><p>where the index j is determined for each wk by a channel simulation and the given mapping a. For sufficiently large numbers of training vectors, M , the summation in (14) will approach the actual value of e,. Hence, by approximating the source statistic by those of a large training set of vectors, and by simulating the channel characteristics, there is an effective procedure for determining the value of e , with any desired accuracy for any given permutation a.</p><p>By iterating this procedure for each ?res,, one can determine which permutation a is optimal (yields the minimal expected distortion e,). If d is the mean-square distortion function and the codevectors satisfy the centroid condition, e , can be computed without a channel simulation, by using Lemma 1 .</p><p>To determine the optimal codebook permutation in this way, however, is a problem of enormous complexity. Very large training sets must be used to accurately simulate the effects of channel errors, and every permutation in S, must be checked to determine which is optimal. Since S , has N! permutations, even a relatively small codebook size (e.g., N = 16) presents a formidable challenge. For this reason, a suboptimal solution to the pseudo-Gray coding problem is sought. We concentrate on finding a permutation a which reduces the average distortion e , by way of a locally optimal algorithm. Attention is restricted to certain classes of commonly used distortion functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Optimum norm distortion functions, the latter being of the form</head><p>We consider metric distortion functions and rth-power of a where 11 . 11 is any norm on R P and r 2 1 . When r = 2, we get the important squared-error distortion function, using the usual Euclidean norm. It is shown that the average distortion with each distance measure can be upper bounded by quantities that are amenable to minimization via a common descent algorithm.</p><p>Case Id is a Metric: By rewriting d ( X , y j ) and taking expectations, e , can be decomposed into the sum of an average distortion of a vector quantizer on a noiseless channel and a term due to channel errors, With the mean-square distortion function, an optimal vector quantizer on a noiseless channel satisfies the well-known nearest-Note that if the encoder C is a nearest-neighbor encoder, then</p><formula xml:id="formula_7">E [ d ( X , ~j ) -d ( X , ~i ) ] 2 0 e , = E [ d( X , y j ) ] 5 U, U, = E [ d ( X , ~i ) ] + E [ d ( y j , &amp; ) I . (<label>17</label></formula><formula xml:id="formula_8">)</formula><p>which implies that channel errors on transmitted indexes always increase the average system distortion, as would be expected. The triangle inequality implies where (18) (19) The quantity U, provides an upper bound for the minimum total expected distortion of the VQ system. This motivates us to seek to minimize this upper bound as a method of reducing the expected codebook distortion. This approach does not necessarily guarantee minimization of the total distortion, but is a heuristic solution that can contribute to its reduction. Our experimental results help to justify its use.</p><p>The quantity E [ d ( X , vi)], the expected distortion due to the approximation of an input vector by a codevector, is independent of the assignment of indexes to codevectors, and depends only upon the original design of the codebook. It is thus a constant with respect to the minimization problem (over all permutations). To minimize the upper bound U,, it suffices to find Case 2 -d is an rth Power of Norm: Where r = 1 , d is a metric so that Case 1 applies as well. The Minkowski inequality for expectations can be written as [29] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!Ir i</head><p>From (23) it can be seen that e , is upper bounded by a quantity containing two components. The first is independent of the permutation x , and the second component, containing d ( y i , y . ) , varies depending on the choice of x. To minimize the upper Lounds in both Case 1 and Case 2 (for a fixed collection of codevectors), it suffices to minimize the quantity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0, E [ d( y i , y j ) ]</head><p>(24) over all permutations in S , . The subscript x indicates the fact that the value of the expectation depends on x.</p><p>The inequality e , 5 U: is valid over a broad class of quantizers, namely, any quantizer with a distortion measure that is an rth power of a norm and which transmits symbols across a discrete memoryless channel. In the special case when the distortion function is the mean-square criterion and the quantizer is optimal for a noiseless channel, Lemma 1 shows that e , = U,, so minimizing D, over all x actually minimizes e , as well (instead of an upper bound on e,).</p><p>We examine the expected value D, in terms of the probability functions of X and q under the assumption of a memoryless binary symmetric channel. The expectation in D, can be reduced to a very simple formula involving a summation of "costs" of codevectors, where each cost is a readily computable function. This provides the basis for an efficient algorithm that performs the pseudo-Gray minimization in a locally optimal manner. Since the channel noise is assumed independent of the source and {R,} partitions the space RP, we have</p><formula xml:id="formula_9">N -1 . Pr [ q = t] Pr [ X E R , ] . (25)</formula><p>Using the fact that i = k whenever XER,,</p><formula xml:id="formula_10">N -1 N -1 D, = P ( y k )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d ( y k , Y , -l ( , ( k ) e t ) )</head><p>Pr [ q = t l ' (26)</p><formula xml:id="formula_11">k = O t = o</formula><p>Note that by minimizing the upper bound in (19) and (23) rather than the exact average distortion, we have eliminated any need to consider the explicit distribution of the input vector X in order to find an optimal permutation. It is sufficient to know the codevector probabilities p ( y k ) and the channel statistics.</p><p>For each binary index q E (0, l}b and each integer m with 0 5 m 5 b , define the mth-neighbor set of q as N m ( q</p><formula xml:id="formula_12">) = { r E { O , l } b : H ( q , r ) = m } (27 )</formula><p>where H ( e , * ) is the Hamming distance function. N m ( q ) is the set of all integers whose binary representations differ from that of q in exactly m positions (have Hamming distance equal to m). Equivalently, N m ( q ) is the set of indexes that index q can be transformed into as the result of exactly m bit errors. If q is a b-bit binary word, then counting the number of different ways that q can be changed in exactly m bit positions gives</p><p>IN"(q)I = (; ). In a binary symmetric channel (BSC) with error transition probability e,</p><formula xml:id="formula_13">Pr [ q = t ] = EH(',') ( 1 -4b-H(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>f20). (29)</head><p>If a channel is binary symmetric, then for any integers s, t E {O, l}b,</p><formula xml:id="formula_14">P r [ q = s ] = P r [ q = t ] i f f H ( s , O ) = H ( t , O ) . (<label>30</label></formula><formula xml:id="formula_15">)</formula><p>The probability that a transmitted binary word from (0, l}b is received as a particular binary word depends only on their Hamming distance apart from each other and for a fixed m , Pr [ q = t] is a constant over all t €Nm(C(X)). For each m , with 0 5 m 5 b and H( t , 0) = m , define</p><formula xml:id="formula_16">qm = Pr [ q = t ] = em(l -c ) b -m . (<label>31</label></formula><formula xml:id="formula_17">)</formula><p>The second summation in (26) is taken over the set of all 2' codevectors in Y. This set of vectors can be partitioned into b neighbor sets, such that each set in the partition of Y consists of all vectors whose indexes are a fixed Hamming distance from the index i. Using this observation, the summation over k can be written as two summations: which measures the total contribution to bit error distortion due to possible channel errors when a particular codevector, y, is selected by the encoder. In terms of the cost functions, the problem of minimizing D, reduces to finding a permutation a that minimizes the equivalent quantity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2151</head><p>N -1</p><p>where the minimization ranges over all possible permutation functions in S,.</p><p>We wish to minimize D, in the hope of reducing the expected distortion introduced to the VQ system from the combined effect of coding error and channel noise. In general, as can be seen in the above equation, this minimization involves the knowledge of the channel's error transition probability E as well as the distance function d and the codevector probabilities p ( y , ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Asymptotic Properties</head><p>When the channel error probability E is sufficiently small, the probability of multiple bit errors in an index is very small relative to the probability of zero or one bit error. Often it is adequate to consider only the effects of single bit errors on channel words. If one assumes only single bit errors, then to minimize D,, it suffices to minimize a simplified version of (36) that does not depend on the value of E, and is less computationally demanding.</p><p>Neglecting the effect of multiple bit errors assumes that qm = 0 for m 2 2 or, equivalently for every codevector y E Y,</p><formula xml:id="formula_18">Substituting for q, , N-I .. . D, = ~( 1 - C : ) ( y k ) k = O (<label>37</label></formula><formula xml:id="formula_19">)</formula><p>To minimize 0, in this case, it suffices to find the permutation function a which minimizes the quantity N-1 .. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d , = q y y , ) . k = O (39)</head><p>Since minimizing d , does not depend upon the value of the parameter E, the solution applies to any memoryless binary symmetric channel. This result leads to a tractable algorithm for solving the pseudo-Gray problem in a locally optimal manner. All that need be known a priori about the communication channel in this situation is that it is memoryless and binary symmetric in nature. The exact frequency in which errors occur is not significant in determining which codebook permutation function minimizes the derived upper bound.</p><p>It is important to examine the accuracy of the distortion upper bounds U, and U: given in (19) and ( <ref type="formula">23</ref>). In actual VQ systems, one often encounters channels with relatively low noise levels or vector quantizers with very high resolution. We consider the deviation of U, and U. from e , in these situations in limiting cases. As the channel becomes less noisy or as the resolution of the quantizers increases, the upper bounds approach equality. For a given source, and either a fixed channel or a fixed noiseless quantizer, there always exist noisy channel vector quantizers that make the upper bounds U, and U. arbitrarily accurate. In this uniform sense, the bounds are as tight as possible. The following lemma summarizes this property. Lemma 2: In the limit of high resolution or low channel noise, each of the bounds U, and U. tends to e , , independent of the permutation a.</p><p>The results of the previous sections can be generalized and applied to arbitrary error correcting codes. In general, if more errors occur than a code is capable of handling, invalid data are received (assuming retransmissions are unallowable). If, however, a given channel code is used to code vector quantization indexes, additional performance can be achieved. Beyond a code's error correcting capability, incorrect codevector indexes are received and decoded as erroneous codevectors. The expectation of the resulting vector distortion can be reduced by a priori carefully assigning channel words to codevector indexes. Pseudo-Gray coding can "extend" the error correcting capability of a given channel code in the sense that, in addition to correcting all errors less than a certain amount, it can reduce the average error that a VQ systems experiences when more errors occur than the channel code can handle. Applications of pseudo-Gray coding to error correcting codes will be presented in a future publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BINARY SWCHING ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Algorithm Description</head><p>In this section, a Binary Switching Algorithm is presented that performs pseudo-Gray coding on a given VQ codebook. A description of a channel (such as the error probability E on a BSC) and a predesigned VQ codebook are given as input. The algorithm eventually halts and gives as output an assignment of binary indexes (i.e., a mapping a) to the vectors of the codebook. The new index assignment provides an improvement in the average distortion due to channel errors, over an arbitrary index assignment, as confirmed by experimental results (Section V).</p><p>At this point, a simple fact is worth noting. For any index mapping a, the selection of codevector yi as an approximation to some input vector X causes the transmission of index a(i). This is conceptually equivalent to first shuffling the vectors in a codebook, so that for each index i , vector yi moves to location a(i), and then using the identity mapping for a.</p><p>An interesting question naturally arises with an algorithm that produces as its output a permutation a. What data representation should be used to specify a? The easiest way to specify ?r apparently is to produce the original codebook, but with the codevector locations shuffled in the described manner. This has the added advantage that no memory is needed to specify a other than the storage required for the codebook itself. The algorithm thus receives a codebook as input, and outputs the same codebook but with its vectors in different locations.</p><p>The Binary Switching Algorithm rearranges a codebook such that the summation in (36) is locally a minimum. The main idea involves iteratively switching the positions of two codevectors to reduce the term D, after every switch. A monotonic decrease in D, results as the algorithm progresses. Each such switch constitutes a change in the permutation a. The choice of which pair of vectors to switch in the codebook at each iteration is determined by a heuristic ordering process, which works well in practice. Each codevector y is assigned a cost, C,( y ) , as given in (35). The codevectors are sorted in decreasing order of their cost values. The vector with the largest cost, say, y o , is selected as a candidate to be switched first. A trial is conducted, where yo is temporarily switched with each of the other codevectors to determine the potential decrease in D, following each switch. The codevector which yields the greatest decrease in D, when switched with yo is then switched with y o . If no such vector exists (an unsuccessful switch attempt), then the second highest cost vector is used to check for the most cost-efficient switch, and so on. If every codevector is such that when switched with every other codevector, no decrease in D, results, then the algorithm halts in a locally optimal state. By ordering the codevectors based upon a measure of the quality of their codebook positions (costs), the algorithm is given a sense of direction in terms of which vectors are important to switch first. The vectors with the highest costs contribute largely to the expected VQ distortion, which is equal to the sum of these costs. If the total number of program iterations is limited because of running time constraints, then this directedness becomes increasingly significant.</p><p>The completion of each switch or unsuccessful switch attempt constitutes the end of an iteration. Following such a switch, the process repeats; the vectors are resorted based on cost, and new switches are attempted.</p><p>It is important to point out that from any initial codebook index assignment, any other index assignment can be "reached" by performing vector switches entirely of the type described above. This follows trivially from the well-known result in group theory that any element of the symmetric group can be decomposed into a product of 2-cycles. This fact guarantees that the restriction to vector switches does not exclude the globally optimal index assignment as a potential final permutation of the BSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Initializing a Codebook Permutation</head><p>The input to the Binary Switching Algorithm is a codebook with a particular initial index assignment. The initial index assignment (i.e., initial vector locations) can affect the performance of the pseudo-Gray algorithm, since only a local optimum is determined. There are many different ways to specify an initial index assignment.</p><p>The simplest initial assignment is an arbitrary one. That is, take the codebook and input it directly to the pseudo-Gray algorithm following codebook design, with the vectors in whatever order they appear. In this case, the entire process of minimizing D, consists only of the vector switching operations described.</p><p>With the use of preprocessing, an initial index assignment can be constructed in an attempt to reduce the quantity D, when the algorithm reaches a locally optimal state. Such techniques are generally heuristic in nature. One such preprocessing technique involves examining the codevectors that have the highest probabilities of being used [i.e., the largest p ( y k ) ] . Since these vectors are used more frequently than others in the codebook, they can initially be assigned to have Hamming neighbors that are very close in terms of the distortion function d . This can be achieved by distributing several of the highest probability codevectors throughout the codebook and placing close vectors as their nearest neighbors. In this way, the process of minimizing D, is given a "head start,'' based on an intuitive notion of what an optimal codebook permutation should look like. Another option for codebook initialization is one that makes use of random codebook permutations. Initially, a predetermined number of random index assignments is performed. The assignment with the lowest distortion, D,, is then chosen as the starting codebook for the vector switching algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Halting Conditions</head><p>There are several options available to determine when the pseudo-Gray algorithm will halt. As noted previously, the default is for the algorithm to keep running until a local optimum is reached. In this situation, if any two codevectors are switched in position, then no decrease in D, can result. This stopping condition can yield extremely long running times for large codebooks. Of course, since running the algorithm on a codebook is a one-time operation, this may be tolerable.</p><p>There is no known reasonable bound on the number of iterations required before the algorithm halts. A weak upper bound on running time can be obtained by considering the maximum number of vector switches possible. Each vector switch yields a strictly positive decrease in Dr, and hence a codebook with an index assignment mapping A that is different from any codebook permutation that the algorithm had previously generated (otherwise, two different distortions would result from the same permutation A ) . Since there are at most N! distinct codebook permutations, there can be at most N! vector switches before the algorithm halts. In practice, this upper bound is normally not closely approached, as can be seen in the tabulation of execution times (Table <ref type="table">11</ref>) for various trials of the algorithm.</p><p>An alternative to stopping when no more vector switches are possible is to perform some random perturbations from the locally optimal state, attempting to find a decrease in D,. If such a new codebook permutation is found, then the entire algorithm can be used again to locally optimize this new codebook. With this method, one continually reaches locally optimal states followed by randomly jumping into new states with less distortion.</p><p>If computation time is critical, an upper bound can be set on the number of iterations to perform in the program. A program variable Z,, is provided to specify the maximum number of iterations allowed before the program should halt. In this situation, the program keeps switching codevectors until either a local optimum is reached or the designated number of iterations is performed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Switching Algorithm</head><formula xml:id="formula_20">O( N log N) O( N log N) O( N log N) O ( N ) O(l0g N ) O( N log N) O ( N 2 ) O( N ) O ( N 2 ) ow3) O( N log N) O ( N 2 log N)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Complexity</head><p>Since the optimization problem at hand involves summations that range over the set of all possible codebook permutations, it is important to consider the computational complexity of such a task. An optimal solution can be found by way of exhaustively searching the N! possible configurations of the codebook, which for even small size codebooks can be prohibitive. A codebook of size 32 has approximately 10" different configurations, making an exhaustive search essentially impossible.</p><p>The Binary Switching Algorithm provides a reduced complexity method for obtaining good codebook permutations. The computationally demanding portions of the algorithms are the execution of the procedure SORT-INDEXES and the calculation of the variable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.</head><p>After the first iteration, many of the computational requirements can be reduced by updating various quantities, rather than recomputing them in their entirety. The algorithm's complexity requirements can be divided between those of the first iteration (which can be interpreted as a startup overhead cost) and those of subsequent iterations. By making the single bit error assumption, the algorithm's complexity can be reduced. The complexities of the more efficient single bit error case and the general case are analyzed in this section and are given as functions of N, the codebook size.</p><p>The procedure SORT-INDEXES sorts the N indexes in (0, l } b such that the corresponding codevectors in Y are in decreasing order of costs, C,. Assuming the costs are precomputed, the sort can be done in average time O( N log N) using a standard sorting algorithm [30]. Following the initial sort, a more efficient sort update can be implemented when only single bit errors are assumed. The complexity of initially computing various quantities needed by the algorithm turns out to be more demanding than the complexity of subsequent iterations. By examining the number of terms in the summation in (33), it can be seen that the complexity of computing the term C!m)(yk) for a fixed integer m and a fixed codevector y, is Using ( 3 3 , the complexity of computing C,(y,) for a fixed codevector y, is thus and the complexity in the single bit error case is For any iteration other than the initial one, let T ' be the permutation that results when two vectors are switched in location from the previous permutation T . The value of D,. (respectively, d , , ) can be obtained by modifying the value of D, <ref type="bibr">(respectively, d,)</ref> according to the changes induced by a single vector switch. The relationship between T and T ' is that there exist 2 indexes r, Omitting the single bit error assumption, each vector switch has an effect on the cost of every codevector. In general, for two codevectors switched, y , and y,, the terms C,(y,) and C,(y,) must be entirely recalculated in time O( N), while for each remaining codevector y , the quantity C,( y ) can be computed by updating at most two of its mth costs, CLm)(y). Furthermore, for each of these two mth costs, at most two components of the sum in definition of C!mm,( y ) need to be adjusted, so recomputing C:m)( y ) can be done in constant time. Computing 0,. -D, can therefore be done in time</p><formula xml:id="formula_21">O ( 2 N + ( N -2) * 2) = O ( N ) .</formula><p>During each iteration of the algorithm, 6 must be computed for each trial pair of vectors switched, before determining which vector pair provides the most gain. The number of such computations of 6 is bounded by the total number of possible pairs of codevectors, which is O ( N 2 ) . The total computational complexity of an iteration is then of order N 2 times the complexity of computing 6. These are summarized in Table <ref type="table" target="#tab_3">I</ref>.</p><p>Although the order of magnitude of run times for the BSA can be estimated by complexity analysis, often the actual run times must be known. The BSA typically required about 25 h of CPU time on a 2 MIPS machine (SUN-3/180) for a codebook of size 256, and about 13 h for s u e 128, each with the single bit error assumption. There may be interest in using the BSA for even larger codebooks and omitting the single bit error assumption, in which case greater run times can be expected, although more efficient programming and faster computers may ease the burden somewhat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>The Binary Switching Algorithm was implemented in software and tested on various inputs. It was allowed to run until the distortion reached a local optimum. A Euclidean mean-square distortion measure was used as the fidelity criterion (avoiding a channel simulation), and codebooks were designed using a standard Generalized Lloyd Algorithm [31] with training ratios of at least 50. The resulting codebooks with arbitrary initial index assignments were used as input to the BSA, which yielded as output the same codebook but in a different ordering.</p><p>The input signals tested include speech waveforms, 1st-order Gauss-Markov processes, and Gaussian i.i.d. signals. The speech waveforms were sampled at 8 kHz, digitized, and partitioned into input vectors of dimension 4 with codebooks of size 64, 128, or 256. The Gauss-Markov processes were of the form x, = a x , -, + w, (45) where w, is a zero-mean, unit variance, Gaussian white noise process, with a = 0.9, a = 0.5, or a = 0 (Gaussian i.i.d.). Each of these was tested with quantizers having the following codebook size and dimension values (N, p): <ref type="bibr">(16,4), (64,6), (256,4)</ref>.</p><p>In order to reduce the running time of the algorithm, the single bit error assumption was used. Designing the codebook permutations for single bit errors proved satisfactory even when multiple bit For each codebook tested, a computation was made at various channel noise levels of the system's signal-to-noise ratio (SNR), defined as IOlog,, (u,'/u:), where U, ' and u,' are the signal and noise variances, respectively. The SNR's were computed for the codebooks both before and after applying the BSA for BSC bit error probabilities ranging from to lo-'. In addition, by making a minor alteration to the BSA, it is possible to produce rearmaged codebooks with locally maximum average distortion 0,. The previous experiments were repeated for these codebooks with locally "bad" index assignments. The impor-tance of bad codebook permutations is that they provide upper bounds on the expected distortion that would result from using random codebook permutations in a noisy channel VQ system. The signal-to-noise ratios of the "worst case" and "best case" codebooks produced by pseudo-Gray coding provide a range of possible performancces for a typically chosen codebook that has not been pseudo-Gray coded. Of course, neither are truly "worst" nor "best" cases because the algorithm is not guaranteed to achieve a global optimization. The performance of the BSA can be measured either by the dB gain in SNR of a best case codebook over the initial codebook or over a worst case codebook. The various SNR's are plotted in Figs. <ref type="figure">3</ref><ref type="figure" target="#fig_10">4</ref><ref type="figure">5</ref>.</p><p>The increase in SNR from the initial codebook to a "best case'. codebook was observed to generally be less than 2.5 dB over the range of channel error probability E between 0 and 0.1, while the increase in SNR from the "worst case" codebook to a "best case" codebook reached as high as 6 dB over this range. The increase in SNR achievable using pseudo-Gray coding is more prominent for higher channel error probabilities.</p><p>The SNR's of the codebooks processed by the BSA are often substantial increases over both the starting codebook SNR's and those in the worst cases. The improvements tended to rise with increasing codebook size. This may be due in part to the fact that in larger size codebooks, there is a greater "flexibility" to rearrange vectors, since there are many more locations available, and since the total number of possible vector switches at any one time for a codebook of size N grows as O ( N z ) .</p><p>An interesting fact that should be noted is that at relatively high channel noise levels, the performances of vector quantizers tested with the "best" index assignments can decrease as the vector dimension increases. For example, at a fixed rate of 1 b/source sample, the SNR of a vector quantizer for a Gaussian i.i.   <ref type="bibr">3(a)</ref> and <ref type="bibr">4(a)</ref>]. This performance is consistent with that reported in [16], and is due to the fact that the quantizers are not "channel optimized," since their encoders and decoders were designed under noiseless channel assumptions. In contrast, truly optimal quantizers on noisy channels would be expected to increase in performance as the vector dimension increases. Some important measurable parameters can yield insight into the success of the algorithm. The number of iterations executed by the algorithm before halting and the number of vector switches carried out during the program's operation were recorded for various codebook sizes and are shown in Table <ref type="table" target="#tab_4">II</ref>. The number of iterations is uirmuy prupuruuniu (igriurirrg uie riiimniiu iruiiilluauun uverhead) to the actual running time of the program, and generally increased with codebook size. It was observed while running the BSA that vector switches occurred much more frequently at the beginning of the program than toward the end (when approaching a final permutation). This artifact was likely a result of the ordering imposed on the codevectors before allowing vector switches. Similarly, the number of iterations executed remained constant in order of magnitude for codebooks of the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUDING REMARKS</head><p>Using the BSA for VQ index assignment can lead to increased performance in the presence of channel noise. Although optimal index assignment is a computationally demanding feat, the BSA provides efficient means of obtaining locally optimal solutions. We note that pseudo-Gray coding can be extended to systems with error correction coding-a topic of a future publication.</p><p>Several questions remain concerning VQ design for noisy channels. An interesting one concerns the performance of high-resolution quantizers when channel noise is added. More precisely, suppose a sequence of quantizers Q, is given where the nth quantizer has n output points and such that lim,,,E[d(X, Q,(X))] = 0. If a fixed level of channel noise (e.g., BSC) is added, must it still be the case that this limit equals zero? Another problem of great interest is to provide tight mathematical bounds on the potential performance gain that pseudo-Gray coding can provide for a given source and quantizer. Analytic estimates on the running time of the BSA would also be of value.</p><p>Studies are in progress to utilize pseudo-Gray coding in more complex coding structures, such as in vector-excited and adaptivepredictive coding schemes and with error correction coding. Integrating pseudo-Gray coding into systems specifically designed for noisy channels should become an important task of the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A coder C: R P -+ (0, l } b maps the vectors of R P to b-bit indexes, and a decoder D:(O, l } b + Y maps indexes to vectors in RP. Thus, the set of all binary words of length b bits. Block diagram of VQ on noiseless channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>element of (0, l } b with binary representation clcz cb, where ck = i, + j k (mod2) for 1 5 k I b. That is, each binary digit ck is the logical exclusive-OR of i, and j , . If the indexes in the set (0, l } b are transmitted across a noisy channel, their values in general will be received as different indexes in the same set (0, l } b . Hence, a memoryless noisy channel can be represented by a mapping ~( 0 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>l X + Y l l y 5 ( E l l X l l y + (EllYllr)T (21) e , = Ell X -Y j I I r = Ell(X -Y i ) + (Yi -Yj)Ilr 5 U; (22) (EIIX-yiIIr)r + ( E I I ~~-~~I I ~) ~ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, if b = 4, then the neighbor sets of the index "2" given in decimal notation are N'(2) = {2}, "(2) = {3,0,6, lo}, N2(2) = { 1 , 4 , 8 , 7 , 11, 14}, N3(2) = (12, 15,9,5}, and N4(2) = (13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>y k ) measures the relative contribution to the overall expected bit error distortion of the codebook, when exactly m bit errors occur, and y , is the codevector selected by the encoder. The above formulation yields a means of minimizing the average distortion upper bounds, U, and U:. Note m = 0 implies N o ( r ( k ) ) = {x(k)}, so that Finally, define the total cost, C,( y ) , of a given codevector y , with respect to the permutation a as b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>{ y ; : 0 s i 5 Nl } Vector probabilities (0 s i _C N -1). Distortion function. Channel error probability (for a BSC). Number of iterations to perform before halting. Default is 00 (runs until a local optimum is reached). Parameters: Pseudo-Gray coded codebook (permuted version of Y). Initial value of distortion in Y. ri is the initial permutation. Final value of distortion in Y. rf is the final permutation Procedures Used in the Algorithm: INITIALIZE-CODEBOOK( ): Preprocess codebook Y before running the vector switching algorithm on it. SORT-INDEXES(): Construct an array A ( n ) of the indexes of codevectors sorted in order of decreasing Cost. SWITCH(I, j ) : Switch the index assignments of the codevectors yi and yj in Y. UPDATE-COST( j ) : Calculate and return the quantity C,( y ; ) using the vector probabilities p ( y). DISTORTION(): Calculate and return the quantity D, for codebook Y. A is an array of indexes IF j = A( i) THEN NEXT j D, := DISTORTION() SWITCH ( A ( i ) , j) 0,. := DISTORTION ( ) 6 := 0,. -D, ;;;Current permutation is K ;;;Create a new permutation K '</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Switching two vectors affects the values of C,(y,) for at most 2 b [i.e., O(1og N ) ] codevectors, since the number of vectors with Hamming distance one from either of the two switched vectors is at most 2 b . Sorting the indexes in this case reduces to the problem of resorting O ( N ) indexes, given that @log N) of them have been shuffled. To do this, one can first resort the O(1og N) shuffled indexes in average time O(log Nlog log N), and then merge them with the remaining indexes in time O( N ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>From ( 36 )</head><label>36</label><figDesc>, the complexity of initially computing D, is O ( N 2 ) since C,( y,) must be computed N times. With single bit errors, it suffices to compute d, instead of 0,. The complexity of initially (0, l } b such that ~( r ) = ~' ( s ) , r(s) = ~' ( r ) , and a(1) = r'(/) for every 1 s (0, l } b -{ r , s}. To calculate the value d r , , the computing d , is O ( N * p P ( T ( k ) ) I ) = O ( N . (;)) = O ( N l 0 g N ) . (43)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>s E relation d,. = (d,, -d,) + d , and the fact that d , is known give N -I 6 = d,. -d , = [cy( y,) -c:y y , ) ] . (44) k=O The only terms in the above summation that can be nonzero are those for which r ( k ) is not equal to ~( r ) , r ( s ) , or any element in the neighbor sets N ' ( r ( r ) ) and N'(?r(s)). Hence, if C:')(y,) # C;')(y,), then ~( k ) E A , where A = { T ( r ) , ~( s ) } U N ' ( r ( r ) ) U N ' ( T ( s ) ) . To compute 6, it suffices to sum over all k in the set T -' ( A ) , giving Each of the first two terms above can be computed in time O(1og N). Each difference of costs in the summation of the third term above can be computed in time constant in N, since only one term in (35) (taking m = 1) needs to be updated for each term. Thus, since 1 A I s 1 + 1 + b + b = O(1og N), the quantity d,. -d r can be computed in time O(1og N + log N + I A I) = O(1og N).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of vector quantizer with index permutation on a Binary Symmetric Channel as a function of bit error probability. (a) Codebook size = 64, vector dimension = 6, source = Gaussian iid. (b) Codebook size = 64, vector dimension = 6, source = 1st-order Gauss-Markov (a = 0.5). (c) Codebook size = 64, vector dimension = 6, source = 1st-order Gauss-Markov (a = 0.9). (d) Codebook size = 64, vector dimension = 6, source = speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>-</head><label></label><figDesc>d. source decreases from 1.4 dB to 1.1 dB for a bit error probability of 0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Paper approved by the Editor for Quantization, Speech/Image Coding of the IEEE Communications Society. Manuscript received July 13, 1988; revised October 24, 1989. This work was supported in part by the Jet Propulsion Laboratory, California Institute of Technology, sponsored by NASA, and by the General Electric Company, Bell Communications Research, Inc., and the State of California MICRO program. This paper was presented in part at the International Conference on Communications, Philadelphia, PA, June 1988. K. Zeger is with the Department of Electrical Engineering, University of Hawaii, Holmes Hall 483, Honolulu, HI %822. A. Gersho is with the Department of Electrical and Computer Engineering, Communications Research Laboratory, University of California, Santa Barbara, CA 93106.</figDesc><table /><note><p>IEEE Log Number 9039096.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>l}b, X - 2 149 -7 t - i c -</head><label></label><figDesc></figDesc><table><row><cell>Channel</cell></row><row><cell>output Vector</cell></row><row><cell>For a given vector quantizer Q , a noisy channel vector quantizer is</cell></row><row><cell>completely specified by choosing a permutation function a. Hence,</cell></row><row><cell>for a given Q, one seeks to minimize the average distortion e over</cell></row><row><cell>all permutation functions ?r</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>E S,, Noise + 9 emin = min{e,: r e s N } .</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>COMPUTATIONAL TIME COMPLEXITIES PER ITERATION. N = CODEBOOK SIZE</figDesc><table><row><cell>General Case</cell><cell></cell><cell>Single Bit Errors</cell><cell></cell></row><row><cell>Initialization</cell><cell>Updates</cell><cell>Initialization</cell><cell>Updates</cell></row><row><cell>SORT-INDEXES</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D,, 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II SIMULATION</head><label>II</label><figDesc>RESULTS OF BINARY SWITCHING ALGORITHM (dim = 4)</figDesc><table><row><cell></cell><cell cols="2">Best Codebook</cell><cell cols="2">Worst Codebook</cell></row><row><cell></cell><cell>Number</cell><cell>Number</cell><cell>Number</cell><cell>Number</cell></row><row><cell></cell><cell>of</cell><cell>of</cell><cell>of</cell><cell>of</cell></row><row><cell>CB Size</cell><cell>Iterations</cell><cell>Switches</cell><cell>Iterations</cell><cell>Switches</cell></row><row><cell>16</cell><cell>37</cell><cell>9</cell><cell>59</cell><cell>15</cell></row><row><cell>32</cell><cell>91</cell><cell>24</cell><cell>277</cell><cell>66</cell></row><row><cell>64</cell><cell>1571</cell><cell>85</cell><cell>6342</cell><cell>610</cell></row><row><cell>128</cell><cell>8945</cell><cell>273</cell><cell>46 01 1</cell><cell>1982</cell></row><row><cell>256</cell><cell>58312</cell><cell>987</cell><cell>297 410</cell><cell></cell></row></table><note><p><p>6002</p>[see Figs.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I '</head><p>' ' " " r I ' ' " " " I ' ' " " " 1 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vector quantization: A pattern matching technique for speech coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The effect of digital errors on PCM transmission of compandored speech</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dostis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2227" to="2243" />
			<date type="published" when="1965-12">Dec. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reconstruction error in waveform transmission</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Totty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Znform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="336" to="338" />
			<date type="published" when="1967-04">Apr. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of digital errors in nonlinear PCM</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rydbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Sundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="1976-01">Jan. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effects of channel errors on the signal-to-noise performance of speech encoding systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effects of transmission errors on the mean-squared error performance of transform coding systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zelinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1979-10">Oct. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The optimum mean-square estimate for decoding binary block codes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Redinbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Znform. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="344" to="351" />
			<date type="published" when="1974-05">May 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimum symbol-by-symbol mean-square error channel coding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Redinbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Znform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="387" to="392" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the design of mean-square error channel coding systems using cyclic codes</title>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="406" to="413" />
			<date type="published" when="1982-05">May 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combined source-channel coding of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Modestino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1644" to="1659" />
			<date type="published" when="1979-11">Nov. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combined source-channel coding of images using the block cosine transform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Modestino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daut</surname></persName>
		</author>
		<author>
			<persName><surname>Vickers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1261" to="1274" />
			<date type="published" when="1981-09">Sept. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint source and channel trellis encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Znform. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="516" to="519" />
			<date type="published" when="1981-07">July 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantization for noisy channels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurtenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. COmmun</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="291" to="302" />
			<date type="published" when="1969-04">Apr. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combined source and channel coding for variable-bit-rate speech transmission</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="2017" to="2036" />
			<date type="published" when="1983-09">Sept. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vector quantization design for memoryless noisy channels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ZEEE Znt. Conf. Commun</title>
		<meeting>ZEEE Znt. Conf. Commun<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A study of vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Inform. Theory</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal quantizer design for noisy channels: An approach to combined source-channel coding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaishampayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZEEE Trans. Znform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="827" to="838" />
			<date type="published" when="1987-11">Nov. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Partial correction of transmission errors in Walsh transform image without reference to error correction coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hagenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-E</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Znt. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>Znt. Conf. Acoust., Speech, Signal essing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1615">May 1978. Dec. 1983. 1615-1636, NOV. 1975. 1988</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="235" to="238" />
		</imprint>
	</monogr>
	<note>A sub-band coder designed for combined source and channel coding</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partial correction of transmission errors in DPCM without recourse to error correction coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Inr</title>
		<meeting>IEEE Inr<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977-06">June 1977. June 1987</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="351" to="353" />
		</imprint>
	</monogr>
	<note>Commun.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for assigning binary indices to the codevectors of a multi-dimensional quantizer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho ; De Marca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Commun</title>
		<meeting>IEEE Int. Conf. Commun<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-06">June 1987. June 1987</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="654" to="656" />
		</imprint>
	</monogr>
	<note>Zero redundancy channel coding in vector quantisation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust zero-redundancy vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">N.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Commun</title>
		<meeting>IEEE Int. Conf. Commun<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R B</forename><surname>De Marca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Mobile Satellite Conf</title>
		<meeting>Mobile Satellite Conf</meeting>
		<imprint>
			<publisher>JPL Publ</publisher>
			<date type="published" when="1988-05">May 1988</date>
			<biblScope unit="page" from="88" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using simulated annealing to design transmission codes for analogue sources</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Moulsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="617" to="618" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal binary code word assignment for vector quantization over a noisy channel -An application of simulated annealing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. Inform. Theory</title>
		<meeting><address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">In July 1990 he joined the Electrical Engineering Faculty at the University of Hawaii at Manoa as an Assistant Professor. His present research interests include combined source/channel coding, speech and image compression, and computational complexity theory. Dr. Zeger was awarded a four-year Faculty Development Graduate Fellowship by the American Electronics Association in 1985, and was the recipient of a University of California Regents Fellowship in 1989. He is a member of the Communication Theory Technical Committee of the</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Messerschmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="692" to="698" />
			<date type="published" when="1979-11">Nov. 1979. June 1990</date>
			<publisher>IEEE Communications Society</publisher>
			<pubPlace>Ojai, CA</pubPlace>
		</imprint>
	</monogr>
	<note>during the years 1982-1985. In 1984 he served as a consultant to Automatic Data Processing Co. on digital network design</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">78-F&apos;82) is Pro-Eastern Limited, 1981. fessor of Electrical and Computer Engineering at D. E. Knuth, The Art of Computer Programming Vol 3. Reading, the University of California</title>
		<author>
			<persName><forename type="first">B R</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(s'ss-M'@-Sm ;</forename><surname>Informa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the B.S. degree from M.I.T. in 1960 and the</title>
		<meeting><address><addrLine>New Delhi, India; Santa Barbara MA Addison-Wesley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973-01">1973. Jan 1980</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
	<note>Modern Probability Theory. Ph.D. degree from Cornell University in 1963. He was at Bell Laboratories from 1963 to 1980. His current research activities are in compression of speech, audio, images, and video signals. He</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
