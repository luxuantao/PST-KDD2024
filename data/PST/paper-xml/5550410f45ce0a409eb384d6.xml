<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
							<email>jdonahue@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Lisa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<email>rohrbach@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@cs.uml.edu</email>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†UT</orgName>
								<address>
									<settlement>Austin Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>‡UMass Lowell Lowell</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>ICSI Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72F7BC32274C69F8ABA2884D2A075ADC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep" in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognition and description of images and videos is a fundamental challenge of computer vision. Dramatic  progress has been achieved by supervised convolutional models on image recognition tasks, and a number of extensions to process video have been recently proposed. Ideally, a video model should allow processing of variable length input sequences, and also provide for variable length outputs, including generation of full-length sentence descriptions that go beyond conventional one-versus-all prediction tasks. In this paper we propose long-term recurrent convolutional networks (LRCNs), a novel architecture for visual recognition and description which combines convolutional layers and long-range temporal recursion and is end-to-end trainable (see Figure <ref type="figure" target="#fig_1">1</ref>). We instantiate our architecture for specific video activity recognition, image caption genera-tion, and video description tasks as described below.</p><p>To date, CNN models for video processing have successfully considered learning of 3-D spatio-temporal filters over raw sequence data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref>, and learning of frame-to-frame representations which incorporate instantaneous optic flow or trajectory-based models aggregated over fixed windows or video shot segments <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>. Such models explore two extrema of perceptual time-series representation learning: either learn a fully general time-varying weighting, or apply simple temporal pooling. Following the same inspiration that motivates current deep convolutional models, we advocate for video recognition and description models which are also deep over temporal dimensions; i.e., have temporal recurrence of latent variables. RNN models are well known to be "deep in time"; e.g., explicitly so when unrolled, and form implicit compositional representations in the time domain. Such "deep" models predated deep spatial convolution models in the literature <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Recurrent Neural Networks have long been explored in perceptual applications for many decades, with varying results. A significant limitation of simple RNN models which strictly integrate state information over time is known as the "vanishing gradient" effect: the ability to backpropogate an error signal through a long-range temporal interval becomes increasingly impossible in practice. A class of models which enable long-range learning was first proposed in <ref type="bibr" target="#b11">[12]</ref>, and augments hidden state with nonlinear mechanisms to cause state to propagate without modification, be updated, or be reset, using simple memory-cell like neural gates. While this model proved useful for several tasks, its utility became apparent in recent results reporting large-scale learning of speech recognition <ref type="bibr" target="#b9">[10]</ref> and language translation models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>We show here that long-term recurrent convolutional models are generally applicable to visual time-series modeling; we argue that in visual tasks where static or flat temporal models have previously been employed, long-term RNNs can provide significant improvement when ample training data are available to learn or refine the representation. Specifically, we show LSTM-type models provide for improved recognition on conventional video activity challenges and enable a novel end-to-end optimizable mapping from image pixels to sentence-level natural language descriptions. We also show that these models improve generation of descriptions from intermediate visual representations derived from conventional visual models.</p><p>We instantiate our proposed architecture in three experimental settings (see Figure <ref type="figure" target="#fig_4">3</ref>). First, we show that directly connecting a visual convolutional model to deep LSTM networks, we are able to train video recognition models that capture complex temporal state dependencies (Figure <ref type="figure" target="#fig_4">3</ref> left; Section 4). While existing labeled video activity datasets may not have actions or activities with extremely complex time dynamics, we nonetheless see improvements on the order of 4% on conventional benchmarks.</p><p>Second, we explore direct end-to-end trainable image to sentence mappings. Strong results for machine translation tasks have recently been reported <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>; such models are encoder/decoder pairs based on LSTM networks. We propose a multimodal analog of this model, and describe an architecture which uses a visual convnet to encode a deep state vector, and an LSTM to decode the vector into an natural language string (Figure <ref type="figure" target="#fig_4">3</ref> middle; Section 5). The resulting model can be trained end-to-end on large-scale image and text datasets, and even with modest training provides competitive generation results compared to existing methods.</p><p>Finally, we show that LSTM decoders can be driven directly from conventional computer vision methods which predict higher-level discriminative labels, such as the semantic video role tuple predictors in <ref type="bibr" target="#b29">[30]</ref> (Figure <ref type="figure" target="#fig_4">3</ref> right; Section 6). While not end-to-end trainable, such models offer architectural and performance advantages over previous statistical machine translation-based approaches, as reported below.</p><p>We have realized a generalized "LSTM"-style RNN model in the widely-adopted open source deep learning framework Caffe <ref type="bibr" target="#b13">[14]</ref>, incorporating the specific LSTM units of <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: Recurrent Neural Networks (RNNs)</head><p>Traditional RNNs (Figure <ref type="figure" target="#fig_2">2</ref>, left) can learn complex temporal dynamics by mapping input sequences to a sequence of hidden states, and hidden states to outputs via the following recurrence equations (Figure <ref type="figure" target="#fig_2">2</ref>, left):</p><formula xml:id="formula_0">h t = g(W xh x t + W hh h t 1 + b h ) z t = g(W hz h t + b z )</formula><p>where g is an element-wise non-linearity, such as a sigmoid or hyperbolic tangent, x t is the input, h t 2 R N is the hidden state with N hidden units, and y t is the output at time t. For a length T input sequence hx 1 , x 2 , ..., x T i, the updates above are computed sequentially as h 1 (letting h 0 = 0), y 1 , h 2 , y 2 , ..., h T , y T . Though RNNs have proven successful on tasks such as speech recognition <ref type="bibr" target="#b42">[43]</ref> and text generation <ref type="bibr" target="#b37">[38]</ref>, it can be difficult to train them to learn long-term dynamics, likely due in part to the vanishing and exploding gradients problem <ref type="bibr" target="#b11">[12]</ref> that can result from propagating the gradients down through the many layers of the recurrent network, each corresponding to a particular timestep. LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information.</p><p>As research on LSTMs has progressed, hidden units with varying connections within the memory unit have been proposed. We use the LSTM unit as described in <ref type="bibr" target="#b45">[46]</ref> (Figure <ref type="figure" target="#fig_2">2</ref>, right), which is a slight simplification of the one described in <ref type="bibr" target="#b9">[10]</ref>. Letting (x) = (1 + e x ) 1 be the sigmoid nonlinearity which squashes real-valued inputs to a [0, 1] range, and letting (x) = e x e x e x +e x = 2 (2x) 1 be the hyperbolic tangent nonlinearity, similarly squashing its inputs to a <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref> range, the LSTM updates for timestep t given inputs x t , h t 1 , and c t 1 are:   <ref type="bibr" target="#b45">[46]</ref>, a slight simplification of the architecture described in <ref type="bibr" target="#b8">[9]</ref>, which was derived from the LSTM initially proposed in <ref type="bibr" target="#b11">[12]</ref>).</p><formula xml:id="formula_1">i t = (W xi x t + W hi h t 1 + b i ) f t = (W xf x t + W hf h t 1 + b f ) o t = (W xo x t + W ho h t 1 + b o ) g t = (W xc x t + W hc h t 1 + b c ) c t = f t c t 1 + i t g t h t = o t (c t ) x t h t</formula><p>In addition to a hidden unit h  <ref type="figure"></ref>and<ref type="figure">i</ref> t and f t can be thought of as knobs that the LSTM learns to selectively forget its previous memory or consider its current input. Likewise, the output gate o t learns how much of the memory cell to transfer to the hidden state. These additional cells enable the LSTM to learn extremely complex and long-term temporal dynamics the RNN is not capable of learning. Additional depth can be added to LSTMs by stacking them on top of each other, using the hidden state of the LSTM in layer l 1 as the input to the LSTM in layer l.</p><formula xml:id="formula_2">t 2 R N ,</formula><p>Recently, LSTMs have achieved impressive results on language tasks such as speech recognition <ref type="bibr" target="#b9">[10]</ref> and machine translation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>. Analogous to CNNs, LSTMs are attractive because they allow end-to-end fine-tuning. For example, <ref type="bibr" target="#b9">[10]</ref> eliminates the need for complex multi-step pipelines in speech recognition by training a deep bidirectional LSTM which maps spectrogram inputs to text. Even with no language model or pronunciation dictionary, the model produces convincing text translations. <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b4">[5]</ref> translate sentences from English to French with a multilayer LSTM encoder and decoder. Sentences in the source language are mapped to a hidden state using an encoding LSTM, and then a decoding LSTM maps the hidden state to a sequence in the target language. Such an encoder decoder scheme allows sequences of different lengths to be mapped to each other. Like <ref type="bibr" target="#b9">[10]</ref> the sequence-to-sequence architecture for machine translation circumvents the need for language models.</p><p>The advantages of LSTMs for modeling sequential data in vision problems are twofold. First, when integrated with current vision systems, LSTM models are straightforward to fine-tune end-to-end. Second, LSTMs are not confined to fixed length inputs or outputs allowing simple modeling for sequential data of varying lengths, such as text or video. We next describe a unified framework to combine LSTMs with deep convolutional networks to create a model which is both spatially and temporally deep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Long-term Recurrent Convolutional Network (LRCN) model</head><p>This work proposes a Long-term Recurrent Convolutional Network (LRCN) model combinining a deep hierarchical visual feature extractor (such as a CNN) with a model that can learn to recognize and synthesize temporal dynamics for tasks involving sequential data (inputs or outputs), visual, linsguistical or otherwise. Figure <ref type="figure" target="#fig_1">1</ref> depicts the core of our approach. Our LRCN model works by passing each visual input v t (an image in isolation, or a frame from a video) through a feature transformation V (v t ) parametrized by V to produce a fixed-length vector representation t 2 R d . Having computed the feature-space representation of the visual input sequence h 1 , 2 , ..., T i, the sequence model then takes over.</p><p>In its most general form, a sequence model parametrized by W maps an input x t and a previous timestep hidden state h t 1 to an output z t and updated hidden state h t . Therefore, inference must be run sequentially (i.e., from top to bottom, in the Sequence Learning box of Figure <ref type="figure" target="#fig_1">1</ref>), by computing in order:</p><formula xml:id="formula_3">h 1 = f W (x 1 , h 0 ) = f W (x 1 , 0), then h 2 = f W (x 2 , h 1 )</formula><p>, etc., up to h T . Some of our models stack multiple LSTMs atop one another as described in Section 2.</p><p>The final step in predicting a distribution P (y t ) at timestep t is to take a softmax over the outputs z t of the sequential model, producing a distribution over the (in our case, finite and discrete) space C of possible per-timestep</p><formula xml:id="formula_4">outputs: P (y t = c) = exp(Wzczt,c+bc) P c 0 2C</formula><p>exp(Wzcz t,c 0 +bc) . The success of recent very deep models for object recognition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> suggests that strategically composing many "layers" of non-linear functions can result in very powerful models for perceptual problems. For large T , the above recurrence indicates that the last few predictions from a recurrent network with T timesteps are computed by a very "deep" (T -layered) non-linear function, suggesting that the resulting recurrent model may have similar representational power to a T -layer neural network. Critically, however, the sequential model's weights W are reused at every timestep, forcing the model to learn generic timestepto-timestep dynamics (as opposed to dynamics directly conditioned on t, the sequence index) and preventing the parameter size from growing in proportion to the maximum number of timesteps.</p><p>In most of our experiments, the visual feature transformation corresponds to the activations in some layer of a deep CNN. Using a visual transformation V (.) which is time-invariant and independent at each timestep has the important advantage of making the expensive convolutional inference and training parallelizable over all timesteps of the input, facilitating the use of fast contemporary CNN implementations whose efficiency relies on independent batch processing, and end-to-end optimization of the visual and sequential model parameters V and W .</p><p>We consider three vision problems (activity recognition, image description and video description) which instantiate one of the following broad classes of sequential learning tasks:</p><p>1. Sequential inputs, fixed outputs (Figure <ref type="figure" target="#fig_4">3</ref>, left): hx 1 , x 2 , ..., x T i 7 ! y. The visual activity recognition problem can fall under this umbrella, with videos of arbitrary length T as input, but with the goal of predicting a single label like running or jumping drawn from a fixed vocabulary.</p><p>2. Fixed inputs, sequential outputs (Figure <ref type="figure" target="#fig_4">3</ref>, middle):</p><formula xml:id="formula_5">x 7 ! hy 1 , y 2 , ..., y T i.</formula><p>The image description problem fits in this category, with a non-time-varying image as input, but a much larger and richer label space consisting of sentences of any length.</p><p>3. Sequential inputs and outputs (Figure <ref type="figure" target="#fig_4">3</ref>, right): hx 1 , x 2 , ..., x T i 7 ! hy 1 , y 2 , ..., y T 0 i. Finally, it's easy to imagine tasks for which both the visual input and output are time-varying, and in general the number of input and output timesteps may differ (i.e., we may have T 6 = T 0 ). In the video description task, for example, the input and output are both sequential, and the number of frames in the video should not constrain the length of (number of words in) the natural-language description.</p><p>In the previously described formulation, each instance has T inputs hx 1 , x 2 , ..., x T i and T outputs hy 1 , y 2 , ..., y T i. We describe how we adapt this formulation in our hybrid model to tackle each of the above three problem settings. With sequential inputs and scalar outputs, we take a late fusion approach to merging the per-timestep predictions hy 1 , y 2 , ..., y T i into a single prediction y for the full sequence. With fixed-size inputs and sequential outputs, we simply duplicate the input x at all T timesteps x t := x (noting this can be done cheaply due to the time-invariant visual feature extractor). Finally, for a sequence-to-sequence problem with (in general) different input and output lengths, we take an "encoder-decoder" approach inspired by <ref type="bibr" target="#b46">[47]</ref>. In this approach, one sequence model, the encoder, is used to map the input sequence to a fixed-length vector, then another sequence model, the decoder, is used to unroll this vector to sequential outputs of arbitrary length. Under this model, the system as a whole may be thought of as having T + T 0 timesteps of input and output, wherein the input is processed and the decoder outputs are ignored for the first T timesteps, and the predictions are made and "dummy" inputs are ignored for the latter T 0 timesteps. Under the proposed system, the weights (V, W ) of the model's visual and sequential components can be learned jointly by maximizing the likelihood of the ground truth outputs y t conditioned on the input data and labels up to that point (x 1:t , y 1:t 1 ) In particular, for a particular training sequence (x t , y t ) T t=1 , we minimize the negative log likelihood L(V, W ) = P T t=1 log P V,W (y t |x 1:t , y 1:t 1 ). One of the most appealing aspects of the described system is the ability to learn the parameters "end-to-end," such that the parameters V of the visual feature extractor learn to pick out the aspects of the visual input that are relevant to the sequential classification problem. We train our LRCN models using stochastic gradient descent with momentum, with backpropagation used to compute the gradient rL(V, W ) of the objective L with respect to all parameters (V, W ).</p><p>We next demonstrate the power of models which are both deep in space and deep in time by exploring three applications: activity recognition, image description, and video description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Activity recognition</head><p>Activity recognition is an example of the first sequential learning task described above; T individual frames are inputs into T convolutional networks which are then connected to a single-layer LSTM with 256 hidden units. A large body of recent work has proposed deep architectures for activity recognition ( <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>). <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16]</ref> both propose convolutional networks which learn filters based on a stack of N input frames. Though we analyze clips of 16 frames in this work, we note that the LRCN system is more  flexible than <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16]</ref> since it is not constrained to analyzing fixed length inputs and could potentially learn to recognize complex video sequences (e.g., cooking sequences as presented in 6). <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>use recurrent neural networks to learn temporal dynamics of either traditional vision features ([1]) or deep features ([2]</head><p>), but do not train their models end-to-end and do not pre-train on larger object recognition databases for important performance gains. We explore two variants of the LRCN architecture: one in which the LSTM is placed after the first fully connected layer of the CNN (LRCN-fc 6 ) and another in which the LSTM is placed after the second fully connected layer of the CNN (LRCN-fc 7 ). We train the LRCN networks with video clips of 16 frames. The LRCN predicts the video class at each time step and we average these predictions for final classification. At test time, we extract 16 frame clips with a stride of 8 frames from each video and average across clips.</p><p>We also consider both RGB and flow inputs. Flow is computed with <ref type="bibr" target="#b3">[4]</ref> and transformed into a "flow image" by centering x and y flow values around 128 and multiplying by a scalar such that flow values fall between 0 and 255. A third channel for the flow image is created by calculating the flow magnitude. The CNN base of the LRCN is a hybrid of the Caffe <ref type="bibr" target="#b13">[14]</ref> reference model, a minor variant of AlexNet <ref type="bibr" target="#b21">[22]</ref>, and the network used by Zeiler &amp; Fergus <ref type="bibr" target="#b47">[48]</ref>. The net is pre-trained on the 1.2M image ILSVRC-2012 <ref type="bibr" target="#b31">[32]</ref> classification training subset of the Im-ageNet <ref type="bibr" target="#b6">[7]</ref> dataset, giving the network a strong initialization to facilitate faster training and prevent over-fitting to the relatively small video datasets. When classifying center crops, the top-1 classification accuracy is 60.2% and 57.4% for the hybrid and Caffe reference models, respectively. In our baseline model, T video frames are individually classified by a CNN. As in the LSTM model, whole video classifica-tion is done by averaging scores across all video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation</head><p>We evaluate our architecture on the UCF-101 dataset <ref type="bibr" target="#b36">[37]</ref> which consists of over 12,000 videos categorized into 101 human action classes. The dataset is split into three splits, with a little under 8,000 videos in the training set for each split.</p><p>Table <ref type="table" target="#tab_2">1</ref>, columns 2-3, compare video classification of our proposed models (LRCN-fc 6 , LRCN-fc 7 ) against the baseline architecture for both RGB and flow inputs. Each LRCN network is trained end-to-end. The LRCN-fc 6 network yields the best results for both RGB and flow and improves upon the baseline network by 0.49 % and 5.27% respectively.</p><p>RGB and flow networks can be combined by computing a weighted average of network scores as proposed in <ref type="bibr" target="#b32">[33]</ref>. Like <ref type="bibr" target="#b32">[33]</ref>, we report two weighted averages of the predictions from the RGB and flow networks in Table <ref type="table" target="#tab_2">1</ref> (right). Since the flow network outperforms the RGB network, weighting the flow network higher unsurprisingly leads to better accuracy. In this case, LRCN outperforms the baseline single-frame model by 3.82%.</p><p>The LRCN shows clear improvement over the baseline single-frame system and approaches the accuracy achieved by other deep models. <ref type="bibr" target="#b32">[33]</ref> report the results on UCF-101 by computing a weighted average between flow and RGB networks (86.4% for split 1 and 87.6% averaging over all splits). <ref type="bibr" target="#b15">[16]</ref> reports 65.4% accuracy on UCF-101, which is substantially lower than our LRCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Image description</head><p>In contrast to activity recognition, the static image description task only requires a single convolutional network dataset, with both RGB and flow inputs. Values for split-1 as well as the average across all three splits are shown. Our LRCN model consistently and strongly outperforms a model based on predictions from the underlying convolutional network architecture alone. On split-1, we show that placing the LSTM on fc6 performs better than fc7.</p><p>since the input consists of a single image. A variety of deep and multi-modal models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed for image description; in particular, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> combine deep temporal models with convolutional representations. <ref type="bibr" target="#b19">[20]</ref>, utilizes a "vanilla" RNN as described in Section 2, potentially making learning long-term temporal dependencies difficult. Contemporaneous with and most similar to our work is <ref type="bibr" target="#b17">[18]</ref>, which proposes a different architecture that uses the hidden state of an LSTM encoder at time T as the encoded representation of the length T input sequence. It then maps this sequence representation, combined with the visual representation from a convnet, into a joint space from which a separate decoder predicts words. This is distinct from our arguably simpler architecture, which takes as per-timestep input a copy of the static input image, along with the previous word. We present empirical results showing that our integrated LRCN architecture outperforms these prior approaches, none of which comprise an end-to-end optimizable system over a hierarchy of visual and temporal parameters. We now describe our instantiation of the LRCN architecture for the image description task. At each timestep, both the image features and the previous word are provided as inputs to the sequential model, in this case a stack of LSTMs (each with 1000 hidden units), which is used to learn the dynamics of the time-varying output sequence, natural language. At timestep t, the input to the bottom-most LSTM is the embedded ground truth word from the previous timestep w t 1 . For sentence generation, the input becomes a sample ŵt 1 from the model's predicted distribution at the previous timestep. The second LSTM in the stack fuses the outputs of the bottom-most LSTM with the image representation V (x) to produce a joint representation of the visual and language inputs up to time t. (The visual model V (x) used in this experiment is the base Caffe <ref type="bibr" target="#b13">[14]</ref> reference model, very similar to the well-known AlexNet <ref type="bibr" target="#b21">[22]</ref>, pre-trained on ILSVRC-2012 <ref type="bibr" target="#b31">[32]</ref> as in Section 4.) Any further LSTMs in the stack transform the outputs of the LSTM below, and the fourth LSTM's outputs are inputs to the softmax which produces a distribution over words p(w t |w 1:t 1 , V (x)). Following <ref type="bibr" target="#b18">[19]</ref>, we refer to the use of the bottom-most LSTM to exclusively process the language input (with no visual input) as the factored version of the model. We study the importance of this in Appendix ?? by comparing it to an unfactored variant. See Figure <ref type="figure">6</ref> for details on the variants we study.</p><p>Without any explicit language modeling or defined syntax structure, the described LRCN system learns mappings from pixel intensity values to natural language descriptions that are often semantically descriptive and grammatically correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation</head><p>We evaluate our image description model for retrieval and generation tasks. We first show the effectiveness of our model by quantitatively evaluating it on the image and caption retrieval tasks proposed by <ref type="bibr" target="#b25">[26]</ref> and seen in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. We report results on Flickr30k <ref type="bibr" target="#b27">[28]</ref>, and the newly released COCO2014 <ref type="bibr" target="#b23">[24]</ref> datasets, both with five sentence annotations per image.</p><p>Retrieval results are recorded in Table <ref type="table">2</ref>. We report median rank, Medr, of the first retrieved ground truth image or caption and Recall@K, the number of images or captions for which a correct caption or image is retrieved within the top K results. Our model consistently outperforms the strong baselines from recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref> as can be seen in Table <ref type="table">2</ref>. Here, we note that the new Oxford-Net model in <ref type="bibr" target="#b17">[18]</ref> outperforms our model on the retrieval task. However, OxfordNet <ref type="bibr" target="#b17">[18]</ref> utilizes a better-performing convolutional network <ref type="bibr" target="#b34">[35]</ref> to get the additional edge over the base ConvNet <ref type="bibr" target="#b17">[18]</ref> result. The strength of our temporal model (and integration of the temporal and visual models) can be more directly measured against the ConvNet <ref type="bibr" target="#b17">[18]</ref> result, which uses the same base CNN architecture <ref type="bibr" target="#b21">[22]</ref> pretrained on the same data.</p><p>To evaluate sentence generation, we primarily use the BLEU <ref type="bibr" target="#b26">[27]</ref> metric which was designed for automated evaluation of statistical machine translation. BLEU is a modified form of precision that compares N-gram fragments of the hypothesis translation with multiple reference translations. We use BLEU as a measure of similarity of the descriptions. The unigram scores (B-1) account for the adequacy of (or the information retained) by the translation, while longer Ngram scores (B-2, B-3) account for the fluency. We compare our results with <ref type="bibr" target="#b24">[25]</ref> (on Flickr30k), and two strong nearest neighbor baselines computed using AlexNet fc 7 and fc 8 layer outputs. We used 1-nearest neighbor to retrieve the most similar image in the training database and average the BLEU score over the captions. The results on Flickr30k are reported in Table <ref type="table">3</ref>. Additionally, we report results on the new COCO2014 <ref type="bibr" target="#b23">[24]</ref> dataset which has Table 2: Image description: retrieval results for the Flickr30k <ref type="bibr" target="#b27">[28]</ref> and COCO2014 <ref type="bibr" target="#b23">[24]</ref> datasets. R@K is the average recall at rank K (high is good). Medr is the median rank (low is good). Note that <ref type="bibr" target="#b17">[18]</ref> achieves better retrieval performance using a stronger CNN architecture see text. <ref type="bibr" target="#b24">[25]</ref> 54 Table <ref type="table">3</ref>: Image description: Sentence generation results (BLEU scores (%)ours are adjusted with the brevity penalty) for the Flickr30k <ref type="bibr" target="#b27">[28]</ref> and COCO 2014 <ref type="bibr" target="#b23">[24]</ref> test sets.</p><formula xml:id="formula_6">Flickr30k [28] B-1 B-2 B-3 B-4 m-RNN</formula><p>each image is annotated with 5 or more image annotations. We isolate 5,000 images from the validation set for testing purposes and the results are reported in Table <ref type="table">3</ref>.</p><p>Based on the B-1 scores in Table <ref type="table">3</ref>, generation using LRCN performs comparably with m-RNN <ref type="bibr" target="#b24">[25]</ref> in terms of the information conveyed in the description. Furthermore, LRCN significantly outperforms the baselines and the m-RNN with regard to the fluency (B-2, B-3) of the generation, indicating the LRCN retains more of the bigrams and trigrams from the human-annotated descriptions.</p><p>In addition to standard quantitative evaluations, we also employ Amazon Mechnical Turkers (AMT) to evaluate the generated sentences. Given an image and a set of descriptions from different models, we ask Turkers to rank the sentences based on correctness, grammar and relevance. We compared sentences from our model to the ones made publicly available by <ref type="bibr" target="#b17">[18]</ref>. As seen in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Video description</head><p>In video description we want to generate a variable length stream of words, similar to Section 5. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> propose methods for generating sentence descriptions for video, but to our knowledge we present the first application of deep models to the video description task.</p><p>The LSTM framework allows us to model the video as a variable length input stream as discussed in Section 3. However, due to limitations of available video description datasets we take a different path. We rely on more "traditional" activity and video recognition processing for the input and use LSTMs for generating a sentence.</p><p>We first distinguish the following architectures for video description (see Figure <ref type="figure" target="#fig_1">10</ref>). For each architecture, we assume we have predictions of objects, subjects, and verbs present in the video from a CRF based on the full video input. In this way, we observe the video as whole at each time step, not incrementally frame by frame.</p><p>(a) LSTM encoder &amp; decoder with CRF max. (Figure <ref type="figure" target="#fig_1">10(a</ref>)) The first architecture is motivated by the video description approach presented in <ref type="bibr" target="#b29">[30]</ref>. They first recognize a semantic representation of the video using the maximum a posterior estimate (MAP) of a CRF taking in video features as unaries. This representation, e.g. hperson,cut,cutting boardi, is then concatenated to a input sentence (person cut cutting board) which is translated to a natural sentence (a person cuts on the board) using phrase-based statistical machine translation (SMT) <ref type="bibr" target="#b20">[21]</ref>. We replace the SMT with an LSTM, which has shown state-of-the-art performance for machine translation between languages <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>. The architecture (shown in Figure <ref type="figure" target="#fig_1">10(a)</ref>) has an encoder LSTM (orange) which encodes the one-hot vector (binary index vector in a vocabulary) of the input sentence as done in <ref type="bibr" target="#b38">[39]</ref>. This allows for variable-length inputs. (Note that the input sentence might have a different number of words than elements of the semantic representation.) At the end of the   encoder stage, the final hidden unit must remember all necessary information before being input into the decoder stage (pink) in which the hidden representation is decoded into a sentence, one word at each time step. We use the same twolayer LSTM for encoding and decoding.</p><p>(b) LSTM decoder with CRF max. (Figure <ref type="figure" target="#fig_1">10</ref>(b)) In this variant we exploit that the semantic representation can be encoded as a single fixed length vector. We provide the entire visual input representation at each time step to the LSTM, analogous to how an entire image is provided as an input to the LSTM in image description.</p><p>(c) LSTM decoder with CRF prob. (Figure <ref type="figure" target="#fig_1">10</ref>(c)) A benefit of using LSTMs for machine translation compared to phrase-based SMT <ref type="bibr" target="#b20">[21]</ref> is that it can naturally incorporate probability vectors during training and test time which allows the LSTM to learn uncertainties in visual generation rather than relying on MAP estimates. The architecture is the the same as in (b), but we replace max predictions with probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation</head><p>We evaluate our approach on the TACoS multilevel <ref type="bibr" target="#b28">[29]</ref> dataset, which has 44,762 video/sentence pairs (about 40,000 for training/validation). We compare to <ref type="bibr" target="#b29">[30]</ref> who use max prediction as well as a variant presented in <ref type="bibr" target="#b28">[29]</ref> which takes CRF probabilities at test time and uses a word lattice to find an optimal sentence prediction. Since we use the max prediction as well as the probability scores provided by <ref type="bibr" target="#b28">[29]</ref>, we have an identical visual representation. <ref type="bibr" target="#b28">[29]</ref> uses dense trajectories <ref type="bibr" target="#b43">[44]</ref> and SIFT features as well as temporal context reasoning modeled in a CRF.</p><p>Table <ref type="table" target="#tab_7">5</ref> shows the BLEU-4 score. The results show that (1) the LSTM outperforms an SMT-based approach to video description; (2) the simpler decoder architecture (b) and (c) achieve better performance than (a), likely because the input does not need to be memorized; and (3) our approach achieves 28.8%, clearly outperforming the best reported number of 26.9% on TACoS multilevel by <ref type="bibr" target="#b28">[29]</ref>. More broadly, these results show that our architecture is not restricted to deep neural networks inputs but can be cleanly integrated with other fixed or variable length inputs from other vision systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We've presented LRCN, a class of models that is both spatially and temporally deep, and has the flexibility to be applied to a variety of vision tasks involving sequential inputs and outputs. Our results consistently demonstrate that by learning sequential dynamics with a deep sequence model, we can improve on previous methods which learn a deep hierarchy of parameters only in the visual domain, and on methods which take a fixed visual representation of the input and only learn the dynamics of the output sequence.</p><p>As the field of computer vision matures beyond tasks with static input and predictions, we envision that "doubly deep" sequence modeling tools like LRCN will soon become central pieces of most vision systems, as convolutional architectures recently have. The ease with which these tools can be incorporated into existing visual recognition pipelines makes them a natural choice for perceptual problems with time-varying visual input or sequential outputs, which these methods are able to produce with little input preprocessing and no hand-designed features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose Long-term Recurrent Convolutional Networks (LRCNs), a class of architectures leveraging the strengths of rapid progress in CNNs for visual recognition problem, and the growing desire to apply such models to time-varying inputs and outputs. LRCN processes the (possibly) variable-length visual input (left) with a CNN (middle-left), whose outputs are fed into a stack of recurrent sequence models (LSTMs, middle-right), which finally produce a variable-length prediction (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A diagram of a basic RNN cell (left) and an LSTM memory cell (right) used in this paper (from<ref type="bibr" target="#b45">[46]</ref>, a slight simplification of the architecture described in<ref type="bibr" target="#b8">[9]</ref>, which was derived from the LSTM initially proposed in<ref type="bibr" target="#b11">[12]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task-specific instantiations of our LRCN model for activity recognition, image description, and video description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our approaches to video description. (a) LSTM encoder &amp; decoder with CRF max (b) LSTM decoder with CRF max (c) LSTM decoder with CRF probabilities. (For larger figure zoom or see supplemental).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Activity recognition: Comparing single frame models to LRCN networks for activity recognition in the UCF-101<ref type="bibr" target="#b36">[37]</ref> </figDesc><table><row><cell>Model</cell><cell cols="2">Single Input Type RGB Flow</cell><cell cols="2">Weighted Average 1 /2, 1 /2 1 /3, 2 /3</cell></row><row><cell>Single frame (split-1)</cell><cell>69.00</cell><cell>72.20</cell><cell>75.71</cell><cell>79.04</cell></row><row><cell>LRCN-fc6 (split-1)</cell><cell>71.12</cell><cell>76.95</cell><cell>81.97</cell><cell>82.92</cell></row><row><cell>LRCN-fc7 (split-1)</cell><cell>70.68</cell><cell>69.36</cell><cell>79.01</cell><cell>80.51</cell></row><row><cell>Single frame (all splits) LRCN-fc6 (all splits)</cell><cell>67.70 68.19</cell><cell>72.19 77.46</cell><cell>75.87 80.62</cell><cell>78.84 82.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>80,000 training images, and 40,000 validation images. Similar to Flickr30k,</figDesc><table><row><cell></cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell><cell>Medr</cell></row><row><cell cols="4">Caption to Image (Flickr30k)</cell><cell></cell></row><row><cell>DeViSE [8]</cell><cell>6.7</cell><cell>21.9</cell><cell>32.7</cell><cell>25</cell></row><row><cell>SDT-RNN [36]</cell><cell>8.9</cell><cell>29.8</cell><cell>41.1</cell><cell>16</cell></row><row><cell>DeFrag [15]</cell><cell>10.3</cell><cell>31.4</cell><cell>44.5</cell><cell>13</cell></row><row><cell>m-RNN [25]</cell><cell>12.6</cell><cell>31.2</cell><cell>41.5</cell><cell>16</cell></row><row><cell>ConvNet [18] LRCN 2f (ours)</cell><cell>11.8 17.5</cell><cell>34.0 40.3</cell><cell>46.3 50.8</cell><cell>13 9</cell></row><row><cell cols="4">Image to Caption (Flickr30k)</cell><cell></cell></row><row><cell>DeViSE [8]</cell><cell>4.5</cell><cell>18.1</cell><cell>29.2</cell><cell>26</cell></row><row><cell>SDT-RNN [36]</cell><cell>9.6</cell><cell>29.8</cell><cell>41.1</cell><cell>16</cell></row><row><cell>DeFrag [15]</cell><cell>16.4</cell><cell>40.2</cell><cell>54.7</cell><cell>8</cell></row><row><cell>m-RNN [25]</cell><cell>18.4</cell><cell>40.2</cell><cell>50.9</cell><cell>10</cell></row><row><cell>ConvNet [18] LRCN 2f (ours)</cell><cell>14.8 23.6</cell><cell>39.2 46.6</cell><cell>50.9 58.3</cell><cell>10 7</cell></row><row><cell cols="4">Caption to Image (COCO)</cell><cell></cell></row><row><cell>LRCN 2f (ours)</cell><cell>29.0</cell><cell>61.6</cell><cell>74.8</cell><cell>3</cell></row><row><cell cols="4">Image to Caption (COCO)</cell><cell></cell></row><row><cell>LRCN 2f (ours)</cell><cell>39.1</cell><cell>69.0</cell><cell>80.9</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 ,</head><label>4</label><figDesc>our finetuned (FT) LRCN model performs on par with the Nearest Neighbour (NN) on correctness and relevance, and better on</figDesc><table><row><cell></cell><cell>Correctness</cell><cell>Grammar</cell><cell>Relevance</cell></row><row><cell>TreeTalk [23]</cell><cell>4.08</cell><cell>4.35</cell><cell>3.98</cell></row><row><cell>OxfordNet [18] NN [18]</cell><cell>3.71 3.44</cell><cell>3.46 3.20</cell><cell>3.70 3.49</cell></row><row><cell>LRCN fc8 (ours) LRCN FT (ours)</cell><cell>3.74 3.47</cell><cell>3.19 3.01</cell><cell>3.72 3.50</cell></row><row><cell>Captions</cell><cell>2.55</cell><cell>3.72</cell><cell>2.59</cell></row><row><cell cols="4">Table 4: Image description: Human evaluator rankings from 1-6</cell></row><row><cell cols="4">(low is good) averaged for each method and criterion. We eval-</cell></row><row><cell cols="4">uated on 785 Flickr images selected by the authors of [18] for</cell></row><row><cell cols="4">the purposes of comparison against this similar contemporary ap-</cell></row><row><cell>proach.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">grammar. We show example sentence generations in Fig-</cell></row><row><cell>ure 7.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Video description: Results on detailed description of TACoS multilevel<ref type="bibr" target="#b28">[29]</ref>, in %, see Section C.3 for details.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Oriol Vinyals for valuable advice and helpful discussion throughout this work. This work was supported in part by DARPA's MSEE and SMISC programs, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Vision and Learning Center. The GPUs used for this research were donated by the NVIDIA Corporation. Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action classification in soccer videos with long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICANN</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Behavior Understanding</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video in sentences out</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YouTube2Text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014. 2, 4, 5, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human focused video description</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhuditnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Deep Learning Workshop</title>
		<meeting>NIPS Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2012. 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Treetalk: Composition and compression of trees for image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Micah Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Peter Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<imprint>
			<date type="published" when="2014">2014. 2, 4, 5, 12</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
	</analytic>
	<monogr>
		<title level="m">A dataset of 101 human actions classes from videos in the wild</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards textually describing complex video contents with audio-visual concept classifiers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting recurrent neural networks for robust ASR</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
