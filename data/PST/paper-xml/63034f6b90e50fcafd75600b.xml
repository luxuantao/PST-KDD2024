<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical Counterfactual Policy Learning for Top-? Recommendations</title>
				<funder ref="#_dVHvQfv">
					<orgName type="full">MOST of Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaxu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jui-Nan</forename><surname>Yen</surname></persName>
							<email>juinanyen@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Yuan</surname></persName>
							<email>bwyuan@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Rundong</forename><surname>Shi</surname></persName>
							<email>shirundong@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Yan</surname></persName>
							<email>yanpeng04@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Jen</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Practical Counterfactual Policy Learning for Top-? Recommendations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539295</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Policy learning</term>
					<term>Selection bias</term>
					<term>Counterfactual learning</term>
					<term>Recommender systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For building recommender systems, a critical task is to learn a policy with collected feedback (e.g., ratings, clicks) to decide which items to be recommended to users. However, it has been shown that the selection bias in the collected feedback leads to biased learning and thus a sub-optimal policy. To deal with this issue, counterfactual learning has received much attention, where existing approaches can be categorized as either value learning or policy learning approaches. This work studies policy learning approaches for top-? recommendations with a large item space and points out several difficulties related to importance weight explosion, observation insufficiency, and training efficiency. A practical framework for policy learning is then proposed to overcome these difficulties. Our experiments confirm the effectiveness and efficiency of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many applications, users are overwhelmed by a large number of items. To satisfy their needs more efficiently, recommender systems have been constructed to pre-select items according to their preference and some contextual information. Learning a policy to decide which items to be recommended is the core problem of building a recommender system. A widely used approach is to consider rewards (like ratings, clicks, and dwell time) for historical recommendations as labels and solve classification/regression problems to learn a model (e.g., matrix factorization <ref type="bibr" target="#b9">[10]</ref>, factorization machines <ref type="bibr" target="#b16">[17]</ref>) as an estimator of rewards. Then the system decides future recommendations according to the estimated rewards from the learned model.</p><p>However, recent works argue that such a learning scheme may lead to biased recommendations. The reason is that the historical recommendations are decided by a previously deployed policy, which is generally referred to as the behavior policy. The mismatch between data distributions of the behavior policy and the new policy introduces the selection bias. To remove the bias, the key idea is to consider rewards for both recommended and non-recommended items. However, the realization of this idea is non-trivial. Unlike rewards for recommended items, rewards for non-recommended items, often known as counterfactual data, are generally not available. Learning an unbiased policy without counterfactual data, which is also referred to as counterfactual learning, is an emerging topic in recommender systems.</p><p>Generally speaking, existing approaches of counterfactual learning fall into two groups, value learning approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> and policy learning approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. A review of them will be given in Section 2.2. Between the two approaches, besides few works (e.g., <ref type="bibr" target="#b7">[8]</ref>) that propose combining these approaches for recommender systems, a systematic comparison between these two paradigms is lacking. Our original purpose is to fill this gap. However, when we attempt to implement policy learning approaches for a large-scale top-? recommender system, where ? items are recommended for each context, we meet the following challenges.</p><p>? The propensity of a top-? recommendation can be very small, resulting in the explosion of importance weights and poor performances of the learned policy. ? For recommender systems with a large item space, it is almost impossible to observe every item for every context. Thus, for some rarer contexts, the empirical reward of each item can be far away from the underlying true reward distribution. In particular, the empirical reward can be wrongly concentrated on one single random variable for the feature vector of a context and a reward vector of ? items ? feature matrix of ? items A, A ? random variable for a permutation of ? items and the item placed at position ? C random variable for a reward vector of items in A item if only a few items are observed and one of them is largely amplified by its low propensity weight. This can make the learned policy to be very sharp, a situation which has been pointed out to be harmful to robustness and generalization.</p><p>? Training efficiency is a serious issue as optimization problems of policy learning require computations over the entire item space, the size of which can reach over millions in a real-world system. This work aims to overcome these difficulties with the following contributions.</p><p>? We propose a regularized per-item estimator to quell the first issue mentioned above. ? We propose a formulation to handle the probability mass and thus avoid the sharpness of the learned policy. ? We introduce a novel and efficient training method for policy learning. It can be effectively deployed for large-scale top-? recommender systems. The paper is organized as follows. The preliminary is in Section 2, which includes the problem setting and a review of existing approaches of counterfactual learning. We position the above-mentioned difficulties and propose the corresponding solutions in Section 3. Related works are given in Section 4. In Section 5, a series of experiments confirm our contributions. Finally, Section 6 concludes this work. A list of notations used in this work is in Table <ref type="table" target="#tab_0">1</ref>. Supplementary materials and programs/data used for our experiments are available at https://www.csie.ntu.edu.tw/~cjlin/papers/counterfactual_ topk/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>In this section, after describing the problem settings, we briefly review existing approaches for counterfactual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setting</head><p>In this work, we focus on top-? recommender systems with ? items (e.g., movies, products). Let A = {1, . . . , ?} and ? ? R ??? ? be the associated side-feature matrix. The interactions between users and a top-? recommender system can be described as the following procedures.</p><p>? When a user visits a webpage with ? recommendation positions, a feature vector u ? R ? ? including the contextual information (e.g., information of a user or a webpage) is sent to the system, where u follows an underlying distribution Pr(u). ? Rewards (e.g., clicks, views) c for all items follow another underlying distribution Pr(c | u; ? ), where for each item ?, the corresponding c ? depends only on its feature vector ? ? in the matrix ? and the feature vector u above. Note that ? ? is the ?th row of ? . ? Given the received u, a recommendation policy ? deployed in the system recommends ? distinct items according to the probability ? (A | u; ? ). These items form a ?-element permutation of A as following,</p><formula xml:id="formula_0">A ? ? (A, ?) ? {(A 1 , . . . , A ? ) | A 1 , . . . , A ? ? A; A ? ? A ? , ?? ? ? }.</formula><p>(1) ? Once items in A are recommended to users, their corresponding rewards C = (c A 1 , . . . , c A ? ) are revealed to the system, while rewards of other non-recommended items are still unknown. With the repetition of the above procedures, the system can collect a set S, which includes vast logged events (u, A, C). One of the most crucial tasks of building a recommender system is to learn a new policy ? so that the probability distribution ? (A | u; ?, ? ) parameterized by ? maximizes the expected cumulated reward</p><formula xml:id="formula_1">? ? = E ? ? ? (c, A) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">E ? ? [?] = E Pr(u) E ? (A|u;? ,? ) E Pr(c|u;? ) [?] (3) and ? (c, A) = ?? ? ?=1 c A ?<label>(4)</label></formula><p>is the cumulated reward from A. To distinguish ? from the previous policy ?, the latter is also called the behavior policy. A natural setting to obtain a useful ? is by using the historical events in S. However, the challenge comes from the fact that directly maximizing the expected reward in (2) requires rewards c for all items.</p><p>Unfortunately, among all ?! (?-? )! permutations as the candidate of A, we only observe the one recommended by ? for each context u in S. Opposed to the logged events in S, these events of non-recommended permutations are called counterfactual data. Conducting policy learning with partial rewards in logged events is also referred to as counterfactual learning or batch learning from logged bandit feedback <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Review on Existing Approaches of Counterfactual learning</head><p>Existing approaches of counterfactual learning can be roughly categorized into two groups, the value learning approach and the policy learning approach. They differ on explicitly or implicitly learning the new policy ?. Subsequently, we give a brief review of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Value Learning Approaches.</head><p>The main idea of value learning methods is to learn a reward estimation model ? (?) parametrized by ? to model the reward probability Pr(c | u; ? ). To have an unbiased estimate, it has been pointed out <ref type="bibr" target="#b24">[25]</ref> that we should minimize the following full-labeled risk</p><formula xml:id="formula_3">?(? ) = E Pr(u) E Pr(c|u;? ) [ ?? ? ?=1 ? (c ? , ? (? ; u, ? ? ))],<label>(5)</label></formula><p>where ? ? is the feature vector of the ?th item extracted from row ? of ? , and ? (?, ?) is a loss function (e.g., logistic loss or squared loss). However, for each event (u, A, C), we only have rewards contained in C, which correspond to those selected items included in A. It has been shown <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> that because the behavior policy ? (A | u; ? ) non-uniformly selects items with the highest reward, considering a modified form of (5) with only items included in A results in a heavily biased ? (?).</p><p>For addressing the bias, a commonly used way is to apply an inverse-propensity-score (IPS) method <ref type="bibr" target="#b4">[5]</ref>. The main idea is to re-weigh the loss term of each recommended item by its inverse propensity score such that the following unbiased risk is minimized</p><formula xml:id="formula_4">? IPS (? ) = E Pr(u) E ? (A|u;? ) E Pr(c|u;? ) [ ?? ? ?=1 ? (c A ? , ? (? ; u, ? A ? )) ? (A ? | u; ? ) ],<label>(6)</label></formula><p>where ? (A ? | u; ? ) is the propensity score, which is a value proportional to the probability of the item being recommended at the position. For ? (A ? | u; ? ), we can infer it with the behavior policy ? as follows</p><formula xml:id="formula_5">? (A ? | u; ? ) = ?? ??? (A,? ) ? ( ? | u; ? )?[A ? = ?? ],<label>(7)</label></formula><p>where ?[?] is the indicator function. In some case, when the behavior policy ? is not reserved, we need to estimate ? (A ? | u; ? ) from S.</p><p>Unfortunately, it is known that accurately estimating ? (A ? | u; ? ) is not an easy task. Therefore, some recent works (e.g., <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>) extend <ref type="bibr" target="#b5">(6)</ref> to minimize the following doubly robust risk <ref type="bibr" target="#b3">[4]</ref> for mitigating the issue caused by poor estimates of propensity scores.</p><formula xml:id="formula_6">? DR (? ) = E Pr(u) E ? (A|u;? ) E Pr(c|u;? ) [? ?? ? ?=1 l (? ? , ? (? ; u, ? ? )) + ?? ? ?=1 ? (c A ? , ? (? ; u, ? A ? )) -?? (? A ? , ? (? ; u, ? A ? )) ? (A ? | u; ? ) ],<label>(8)</label></formula><p>where ?? is an imputed reward of item ? and l (?) is the loss function for the imputation part. Because the imputed rewards of those nonrecommended items are less reliable than recommended items, a small user-specified hyper-parameter ? is applied to balance the two parts.</p><p>The reward estimation model ? (?) is learned from the following minimization problem min ? ? IPS (? )(or ? DR (? )) + ??(? ),</p><p>where ?(? ) is a regularizer to avoid overfitting (e.g., l2 regularizer), and ? is a pre-specified regularization coefficient. Then, the optimal ? (A | u; ?, ? ) can be obtained by selecting the top-? items with the highest reward estimates from ? (?). However, it has been pointed out <ref type="bibr" target="#b24">[25]</ref> that such a deterministic policy causes difficulties for optimizing future policies. Thus the ?-greedy method is always applied to impose stochasticity. That is, we greedily select the item with the highest reward estimates with probability 1 -?, and uniformly draw a random item with probability ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Policy Learning</head><p>Approaches. Instead of estimating rewards, policy learning approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> directly find an optimal ? (A | u; ?, ? ) to maximize ? ? in (2). Similar to value learning approaches discussed in the previous section, since S is generally collected by a non-uniform ?, we should apply the inverse propensity weighting method to correct the distribution mismatching between ? and ?. This leads to the following estimate of ? ? ,</p><formula xml:id="formula_8">? ? IPS = E ? ? w A ? (c, A) ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_9">E ? ? [?] = E Pr(u) E ? (A|u;? ) E Pr(c|u;? ) [?]<label>(11) and w</label></formula><formula xml:id="formula_10">A = ? (A | u; ?, ? ) ? (A | u; ? ) . (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>It is known that ? ? IPS is an unbiased estimate of ? ? under the following assumption. The proof can be found in our supplementary materials.</p><p>Assumption 1. For any A and u, ?</p><formula xml:id="formula_12">(A | u; ?, ? ) ? 0 only if ? (A | u; ? ) ? 0.</formula><p>It indicates that our new policy ? only considers items with nonzero probability in our behavior policy ?.</p><p>Thus we can solve the following problem to learn ?,</p><formula xml:id="formula_13">max ? ?(? ) = ? ? IPS (? ) -??(? ),<label>(13)</label></formula><p>where similar to (5), a regularizer ?(? ) is considered to avoid overfitting. To solve ( <ref type="formula" target="#formula_13">13</ref>), the following policy gradient method is applied,</p><formula xml:id="formula_14">? ? ? + ???(? ), (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>where ? is a specified learning rate. In <ref type="bibr" target="#b13">(14)</ref>, ??(? ) is computed as follows,</p><formula xml:id="formula_16">??(? ) =E ? ? ? (c, A) ?? (A | u; ?, ? ) ? (A | u; ? ) -???(? ) =E ? ? ? (c, A) ? (A | u; ?, ? ) ? (A | u; ? ) ? log ? (A | u; ?, ? ) -???(? ),<label>(15)</label></formula><p>where the second equality is from the following chain rule,</p><formula xml:id="formula_17">? log ? = 1 ? ?? . (<label>16</label></formula><formula xml:id="formula_18">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A PRACTICAL FRAMEWORK FOR POLICY LEARNING</head><p>Except a recent work <ref type="bibr" target="#b7">[8]</ref> that proposes a new model that combines approaches of both value learning and policy learning for recommender systems, few studied these two paradigms together in this field. Thus the original purpose of this work is to detailedly compare them for recommendation problems. However, to implement them for a large-scale top-? recommender system, where both the number of logged events and the number of items are tremendous, we find some extra difficulties in policy learning approaches. In this section, after positioning a difficulty in each of the following subsections, we propose the corresponding solution. Finally, integrating these solutions leads to a practical framework of policy learning for top-? recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regularized Per-item IPS Estimator</head><p>3.1.1 Weight Explosion in IPS Estimators. In practice, given finite historical events logged by ?, we generally derive an unbiased estimate of ? ? IPS via Monte Carlo approximation. Expressly, assuming that the collected set includes ? logged events, we maximize the following estimate of ? ? IPS .</p><formula xml:id="formula_19">V ? IPS (? ) = 1 ? ?? ? ?=1 w ? A ? (c ? , A ? ).<label>(17)</label></formula><p>The quality of V ? IPS estimating ? ? deeply relies on the range of w A . To explain this, we make the following assumptions. Assumption 2. For any A and u, the reward c A for every</p><formula xml:id="formula_20">A ? A is bounded in [0, 1]. Assumption 3. For any A and u, if ? (A | u; ? ) ? 0, then w A = ? (A | u; ?, ? ) ? (A | u; ? ) ? ? max .</formula><p>Then, we give the following theorem. 1   Theorem 1. Given a finite policy class H with |H | = ? , we assume that Assumptions 1-3 are all satisfied. Then, with probability at least 1 -?, we have</p><formula xml:id="formula_21">sup ? ? H | V ? IPS -? ? | ? ?? max ?? 2 ? log 2? ? .</formula><p>From Theorem 1, we conclude that the quality of V ? IPS can suffer when the range of w A is large. Unfortunately, this situation commonly occurs for large-scale top-? recommender systems. The leading cause of this issue is that the number of possible item permutations A is an extremely large number ?!/(? -?)!. As a consequence, for most item permutations, both ? (A | u; ?, ? ) and ? (A | u; ? ) can be very small. During the training procedure, any slight disagreement between ? (A | u; ?, ? ) and ? (A | u; ? ) may cause an explosion of w A . This conjecture has been confirmed in a real-world top-? advertising system <ref type="bibr" target="#b10">[11]</ref>. When ? grows, by increasing the disagreement between ? (A | u; ?, ? ) and ? (A | u; ? ), they demonstrate a vast gap between V ? IPS and ? ? . 3.1.2 Weight Factorization and Pruning. The explosion of w A is caused by the massive number of possible permutations of ? items. Thus, for avoiding this issue, an intuitive idea is to assume that ? (A | u; ?, ? ) and ? (A | u; ? ) can be respectively factorized as</p><formula xml:id="formula_22">? (A | u; ?, ? ) = ? ?=1 ? (A ? | u, A 1:? -1 ; ?, ? ),<label>(18)</label></formula><p>and</p><formula xml:id="formula_23">? (A | u; ? ) = ? ?=1 ? (A ? | u, A 1:? -1 ; ? ).<label>(19)</label></formula><p>This decomposes the decision of A into sequential decisions of A ? in ? sub-contexts. Now each sub-context consists of not only u but also items A 1:? -1 that have been recommended beforehand. From (4), we can rewrite <ref type="bibr" target="#b9">(10)</ref> as</p><formula xml:id="formula_24">? ? IPS = E ? ? w A ( ?? ? ?=1 c A ? ) = E ? ? ?? ? ?=1 w A c A ? .<label>(20)</label></formula><p>From ( <ref type="formula" target="#formula_22">18</ref>) and ( <ref type="formula" target="#formula_23">19</ref>), we can rewrite <ref type="bibr" target="#b11">(12)</ref> as</p><formula xml:id="formula_25">w A = ? ?=1 w ? ,<label>(21)</label></formula><p>where</p><formula xml:id="formula_26">w ? = ? (A ? | ?? ; ?, ? ) ? (A ? | ?? ; ? ) and ?? = [ u A 1:? -1 ]<label>(22)</label></formula><p>are respectively the importance weight and the feature vector of the ?th sub-context. However, an explosive w A may still occur as it involves the product of ? unbounded w ? . To address the issue, our idea is to prune w ? included in w A . The following theorem shows that under certain 1 See the proof in our supplementary materials. conditions, pruning ? ?=?+1 w ? will not change the unbiasedness of the estimate. <ref type="foot" target="#foot_0">2</ref>Theorem 2. Assume that</p><formula xml:id="formula_27">? ? (c A ? |u, A 1:? , A ?+1:? ) = ? ? (c A ? |u, A 1:? ).</formula><p>The resulting estimate</p><formula xml:id="formula_28">E ? ? ?? ? ?=1 ( ? ?=1 w ? )c A ? after pruning ? ?=?+1 w ? is still unbiased to ? ? .</formula><p>According to the problem setting described in Section 2.1, we assume ? A ? is only dependent on u and ? A ? , so the conditions required in Theorem 2 can hold for our considered scenarios. Thus we can prune ? ?=?+1 w ? in <ref type="bibr" target="#b19">(20)</ref> without introducing any bias. Next we discuss how to prune ? -1 ?=1 w A ? . Let</p><formula xml:id="formula_29">? ? IIPS = E ? ? [ ?? ? ?=1 w ? c A ? ]<label>(23)</label></formula><p>be the resulting estimator after pruning ? -1 ?=1 w ? . From ( <ref type="formula" target="#formula_22">18</ref>), <ref type="bibr" target="#b18">(19)</ref>, and ( <ref type="formula" target="#formula_25">21</ref>),</p><formula xml:id="formula_30">|? ? IPS -? ? IIPS | = E ? ? [ ?? ? ?=1 ? -1 ?=1 w ? -1 w ? c A ? ] =E ? ? [ ?? ? ?=1 ? (A 1:? -1 | u; ?, ? ) -? (A 1:? -1 | u; ? ) ? (A 1:? -1 | u; ? ) w ? c A ? ].<label>(24)</label></formula><p>This then implies that the loss of pruning ? -1 ?=1 w ? can be bounded by the difference between ? and ?. Intuitively, as long as ? imitates ?, the product can be pruned without any loss. However, any improvement of ? over ? can not be achieved, which is contradictory to the purpose of policy optimization. Instead, we alleviate the loss caused by pruning ? -1 ?=1 w ? through constraining the difference between ? and ?. Consider Pinsker's inequality defined as follows</p><formula xml:id="formula_31">sup |? -? | ? ?? 1 2 ? KL (? || ?),<label>(25)</label></formula><p>where ? KL (? || ?) is the Kullback-Leibler divergence between ? and ?.</p><formula xml:id="formula_32">? KL (? || ?) = -E Pr(u) ?? A ? (A | u; ? ) log( ? (A | u; ?, ? ) ? (A | u; ? ) ) = -E Pr(u) E ? (A|u;? ) log( ? (A | u; ?, ? ) ? (A | u; ? ) ) = -E Pr(u) E ? (A|u;? ) ?? ? ?=1 log( ? (A ? | ?? ; ?, ? ) ? (A ? | ?? ; ? )</formula><p>).</p><p>(26) The last equality in <ref type="bibr" target="#b25">(26)</ref> follows from <ref type="bibr" target="#b17">(18)</ref> and <ref type="bibr" target="#b18">(19)</ref>. Then, the task of alleviating the loss caused by pruning ? -1 ?=1 w ? can be accomplished by bounding ? KL (? || ?). Therefore, we consider to bound ? KL (? || ?) as an additional constraint by the method of adding a penalty function and propose the following regularized per-item IPS (RIIPS) estimator.</p><formula xml:id="formula_33">? ? RIIPS = ? ? IIPS -?? KL (? || ?), (<label>27</label></formula><formula xml:id="formula_34">)</formula><p>where ? is the penalty hyper-parameter decided by a validation procedure. Finally, for policy learning, by equipping the regularization term, we achieve the following problem.</p><p>max</p><formula xml:id="formula_35">? E ? ? ?? ? ?=1 ? (A ? | ?? ; ?, ? ) ? (A ? | ?? ; ? ) c A ? + ? log(? (A ? | ?? ; ?, ? )) -??(? ),<label>(28)</label></formula><p>where ? in ? KL (? || ?) has been omitted due to its independence to ? . It is straightforward to solve the problem by the policy gradient method in <ref type="bibr" target="#b13">(14)</ref>. Specifically, with the trick defined in <ref type="bibr" target="#b15">(16)</ref>, the gradient of the objective function in ( <ref type="formula" target="#formula_35">28</ref>) can be computed as</p><formula xml:id="formula_36">E ? ? ?? ? ?=1 ( ? (A ? | ?? ; ?, ? ) ? (A ? | ?? ; ? ) c A ? + ?)? log(? (A ? | ?? ; ?, ? ))</formula><p>+ ???(? ).</p><p>(29) Now, it comes to how to model ?. Let ? (?) be a score function parametrized by ? . To guarantee that the A resulting from ? is a ?element permutation of A, we apply the Plackett-Luce (PL) model <ref type="bibr" target="#b14">[15]</ref>, which requires to exclude items recommended beforehand for each sub-context. Recalling <ref type="bibr" target="#b21">(22)</ref>, we can divide each sub-context ?? into two parts: u for the inference of ? (?) and A 1:? -1 for item exclusion. Then, with the softmax function, ? is defined as,</p><formula xml:id="formula_37">? (A ? | ?? ; ?, ? ) = ? ? (? ;u,? A ? ) ? ?=1,??A 1:? -1 ? ? (? ;u,? ? ) . (<label>30</label></formula><formula xml:id="formula_38">)</formula><p>According to A 1:? -1 , items recommended previously are explicitly excluded from the item space A. For practical convenience, instead of (30), during training we can use the following formulation,</p><formula xml:id="formula_39">(A ? | ?? ; ?, ? ) = ? ? (? ; ?? ,? A ? ) ? ?=1 ? ? (? ; ?? ,? ? ) ,<label>(31)</label></formula><p>which does not explicitly exclude A 1:? -1 but instead includes A 1:? -1 as additional contextual features for ? (?). The PL model is then only used for generating a ?-element permutation of A during the online deployment. Empirically, the models trained with (30) and (31) have similar performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Policy Learning</head><p>The main idea of IPS and its derivatives is to divide the observed reward c A ? by the propensity ?, so that in expectation this adjusted empirical reward will match the underlying reward distribution. As described in <ref type="bibr" target="#b17">[18]</ref>, IPS generally works when the behavior policy is stochastic and a sufficient amount of events are observed. However, for recommender systems, the context feature u is often very sparse and high-dimensional. This makes some contexts very rare while the number of items is usually very large. Consequently, it is almost impossible to go through every item for every context. In fact, it is more likely that we only observed a small fraction of items for each context.</p><p>To see why this can be bad for IPS, we give a simple example in Figure <ref type="figure" target="#fig_0">1</ref>, which only involves two items and two independent contexts. Specifically, we assume the first context appears more frequently than the second one. Under these two contexts, the empirical rewards of the two items are observed. We show the normalized empirical reward in Figure <ref type="figure" target="#fig_0">1(a)</ref>. From Figure <ref type="figure" target="#fig_0">1</ref>(b), we can see that for both contexts, the learned policy from IPS almost puts the probability mass entirely on the item with the highest empirical reward, and the other item is simply ignored. This is known as the winner-takes-all effect, and it has been pointed out <ref type="bibr" target="#b7">[8]</ref> that the effect can be harmful to the robustness and generalization of the learned policy from IPS. The issue is particularly acute when we do not have enough observations, and as in this case, the item with the highest empirical reward might not actually be the best one.</p><p>To handle the winner-takes-all effect, <ref type="bibr" target="#b7">[8]</ref> proposes the so-called log-IPS as below. By applying the log transformation to our RIIPS and switching to a minimization setting, <ref type="bibr" target="#b27">(28)</ref> becomes</p><formula xml:id="formula_40">min ? E ? ? [ ?? ? ?=1 -Z A ? log(? (A ? | ?? ; ?, ? )) ] + ??(? ),<label>(32)</label></formula><p>where</p><formula xml:id="formula_41">Z A ? = c A ? ? (A ? | ?? ; ? ) + ? .<label>(33)</label></formula><p>According to <ref type="bibr" target="#b7">[8]</ref>, the log transformation in (32) moves the global optimum and thus makes the learned policy from log-IPS allocate probability mass proportional to the normalized empirical reward in Figure <ref type="figure" target="#fig_0">1(a)</ref>. This results in what is shown in Figure <ref type="figure" target="#fig_0">1(c)</ref>. Thus, if the empirical reward spreads across multiple items with enough observations, then log-IPS can alleviate the winner-takes-all effect successfully. For example, as the first context occurs frequently, both items have been observed for several times and the ratio of their empirical reward becomes even and stable. A comparison between Figure <ref type="figure" target="#fig_0">1</ref>(c) and Figure <ref type="figure" target="#fig_0">1</ref>(b) shows that log-IPS makes good use of this even ratio while IPS does not.</p><p>However, we argue that for contexts with few occurrences, it is likely that one item will wrongly occupy a large portion of the empirical reward. The reason behind this is that when one of the many lower propensity items is selected, its empirical reward will be largely amplified by the inverse propensity scaling. If we do not have enough observations, this will result in a high concentration of empirical rewards as the second context in Figure <ref type="figure" target="#fig_0">1</ref>(a). In this case, log-IPS does not help much, as the probability mass of the learned policy is still highly concentrated on one single item.</p><p>To address this problem, our idea is to introduce a hyper-parameter which can further smoothen the probability mass of the learned policy. However, we find it difficult to introduce such a hyperparameter in (32), so an alternative formulation is needed. To this end, we connect (32) to the multi-class classification. A closer look at (32) shows that if the softmax function is used for ? as in (31), then log ? in (32) is the well-known cross-entropy loss function commonly applied for multi-class classification problems. Specifically, (31) is the probability for an instance to be in class A ? and (32) can be considered as to minimize a weighted negative log-likelihood.</p><p>A common alternative used for multi-class classification problems is the one-versus-all scheme. In this setting, a binary probability model is considered for each class. That is, the recommended item A ? is seen as positive and the non-recommended items are seen as negative. Then, for class A ? , the following probability model is considered</p><formula xml:id="formula_42">Pr(positive | ? (? ; ?? , ? A ? )) = ? ( ? (? ; ?? , ? A ? )),<label>(34)</label></formula><p>where ? (?) is the sigmoid function. In the one-versus-all setting, each binary problem corresponds to its own parameter ? A ? , so an independent optimization problem to minimize the negative log-likelihood of (34) is solved. Keeping this process in mind, here we still consider one ? all binary models and propose the following formulation,</p><formula xml:id="formula_43">min ? E ? ? ? ?? ?=1 Z A ? ? + log ( ? (? ; ?? , ? A ? )) + ? ? ?? ?=1,??A ? ? - log ( ? (? ; ?? , ? ? )) + ??(? ),<label>(35)</label></formula><p>where</p><formula xml:id="formula_44">? + log (?) = log ? (?) and ? - log (?) = log(1 -? (?)), (<label>36</label></formula><formula xml:id="formula_45">)</formula><p>and ? is a hyper-parameter explained below. This formulation sums over all the binary classification problems in the one-versus-all scheme. To construct ?, we must combine all ? probability models in (34) into one. This issue has been well studied in the literature of the one-versus-all multi-class classification; see, for example, Section 4.1 in <ref type="bibr" target="#b6">[7]</ref>. Here, instead of using some sophisticated settings, we use the following heuristic to construct ?.</p><formula xml:id="formula_46">? (A ? | ?; ?, ? ) = ? ( ? (? ; u, ? A ? )) ? ?=1,??A 1:? -1 ? ( ? (? ; u, ? ? )) , (<label>37</label></formula><formula xml:id="formula_47">)</formula><p>which also explicitly excludes A 1:? -1 as in (30). One major benefit of (35) is that the loss function is now decoupled for the positive and negative labels, and the hyper-parameter ? can be naturally added in front of ? - log (?) to control the smoothness of the final probability distribution. As we can see in Figure <ref type="figure" target="#fig_0">1(d)</ref>, if we set ? = 0.1, this new setting can alleviate the winner-takes-all effect for both the first and the second contexts. In practice, we can use a grid search to find a suitable value for ?, and the probability mass of the learned policy will then adapt accordingly. We thus name our approach Adaptive-RIIPS, as it can adapt the smoothness of the learned policy to address the winner-takes-all effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Training</head><p>Another critical challenge of policy learning approaches for recommender systems is the training efficiency, which is rarely discussed in past works. As we mentioned in Section 2.2.2, the policy gradient method is applied to solve a policy learning problem. For each context, this involves the calculation of all possible decisions of ?. For ( <ref type="formula" target="#formula_13">13</ref>), the number of possibilities is equal to the number of all ?-element permutations of A, which has the order of O ( ?! (?-? )! ). Clearly, for any top-? recommender system with a tremendous number of contexts and items, solving <ref type="bibr" target="#b12">(13)</ref> through the policy gradient method is infeasible.</p><p>As for the RIIPS estimator proposed in Section 3.1, we decompose the ?-element permutation decision for each context into ? sequential decisions for ? sub-contexts. Thus ? is only responsible for deciding a single item A ? from ? items for each sub-context, and the decision space of ? is remarkably reduced to ?. Consequently, given a training set including ? logged events, the cost of computing (29) by going through all events becomes O(??), where ? = ?? is the number of sub-contexts after decomposition.</p><p>However, since both ? and ? can reach over millions in recommender systems, the training time of ( <ref type="formula">29</ref>) is still prohibitive with the O(??) cost. This difficulty has been reported and addressed in recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> of policy learning with a large action (item) space. Their main idea is to reduce the cost by subsampling items from A. But just as reported in these works, this kind of subsampling mechanism often degenerates the performance. To have a better trade-off between the training efficiency and the performance, we propose a novel solution in this work based on (35), which considers a non-subsampled training setting. As shown in <ref type="bibr" target="#b22">[23]</ref>, such a non-subsampled setting empirically leads to less performance loss than the subsampled one. Define ?? = [</p><formula xml:id="formula_48">? ? A 1:? -1 ],</formula><p>where ? = (? -1) ? ? + ?, to be the ?th sub-context of the ?th context realization ? ? . Thus, for ? = ?? sub-contexts after the decomposition from ? contexts, we derive the following empirical formulation of (35).</p><p>min</p><formula xml:id="formula_49">? 1 ? ?? ? ?=1 ? ? ? + log ( ??? ? ) + ? ?? ? ?=1,??? ? ? - log ( ?? ? ) + ??(? ),<label>(38)</label></formula><p>where ?? ? = ? (? ; ?? , ? ? ), ? ? A ? and ? ? R ? . Here ? and ? contain realizations of A ? and Z A ? for ? sub-contexts, respectively. With this formulation, we transform the policy learning problem to a binary classification problem of ?? training instances. Similar to the situation of the RIIPS estimator, solving (38) by conventional optimization methods is not easy. For example, the stochastic gradient (SG) method may face difficulties in the process of sampling ?? samples <ref type="bibr" target="#b22">[23]</ref>.</p><p>To overcoming this difficulty, our idea is to connect (38) with the extreme similarity learning problem, which learns the relation between a huge number of (context, item) pairs. Efficient training has been well studied for such problems, like <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. This connection relies on the following facts.</p><p>? In our experiments, we consider a two-tower structure for the model ? (?). <ref type="foot" target="#foot_1">3</ref> This setting is very prevalent in modern recommender systems. ? By applying the second-order Taylor expansion, the second term in (38) can be approximated with a weighted squared loss as ? sq ( ?? ? , ?? ? ) = ? ? ? ( ?? ? -?? ? ) 2 , where ? ? ? is a cost weight associated with the loss and ?? ? is an imputed value. <ref type="foot" target="#foot_2">4</ref>As these two facts satisfy the requirements in works of extreme similarity learning, we can apply efficient optimization methods for this problem. In our implementation, we consider the Gauss-Newton method proposed in <ref type="bibr" target="#b25">[26]</ref>, which can factorize all operations involving the O(??) cost into a series of operations with a much smaller O(?) + O(?) cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>In previous sections, we discuss the counterfactual learning in a storyline of recommender systems, where actions decided by a policy are specified as recommended items. For general scenarios, counterfactual learning has been immensely discussed in recent studies. The mainline of these studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> focus on stabilizing the learning process and consider learning a policy that decides only one single action. To the best of our knowledge, <ref type="bibr" target="#b1">[2]</ref> is the first work extending policy learning to top-? recommender systems. Similar to our discussion in Section 3.1, they meet the difficulty caused by the vast number of item permutations. To make the problem tractable, they impose the following two constraints.</p><p>? For each context, at most one item in A has a non-zero reward, which is denoted by A + . ? For each A, the ? recommended items are sampled with replacement during training but without replacement during deployment.</p><p>Under the above constraints, they propose the following top-? REINFORCE estimator,</p><formula xml:id="formula_50">V ? top-K (? ) = E ? ? [ 1 -(1 -? (A + | u; ? ; ? )) ? ? (A + | u; ? ) c A + ],<label>(39)</label></formula><p>where 1 -(1 -? (A + | u; ? ; ? )) ? is the probability of A + being included in ? items sampled by ? with replacement, and ? (A + | u; ? ) is the propensity score the same as that in <ref type="bibr" target="#b5">(6)</ref>. For the gradient of (39), there will be an additional multiplier derived from , which allocates some probability mass of the learned policy to other items of interest and thus helps to alleviate the winner-takes-all effect.</p><p>Our idea of pruning importance weights in the RIIPS estimator is inspired by reinforcement learning <ref type="bibr" target="#b15">[16]</ref>, where the pruning method is used for decreasing weights from long horizons. A similar idea appeared in the policy evaluation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> for top-? recommendations, where both ? and ? are pre-known and fixed. For policy learning, we are not the only work to apply this idea. A parallel work <ref type="bibr" target="#b12">[13]</ref> also considers pruning the weight around the current action. However, different from their empirical study, we offer a theoretical analysis on the loss of the pruning in Section 3.1. This Figure <ref type="figure">2</ref>: An illustration of the supervised-to-bandit conversion, where ? is the behavior policy, and ? is the policy to be learnt and evaluated.</p><p>analysis leads to the addition of a regularization term to alleviate the loss, whose effectiveness is confirmed in our empirical experiments.</p><p>On the other hand, while most works of policy learning only consider a relatively small action space (e.g., less than hundreds), <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref> are the pioneers to extend policy learning for tremendous actions. To tackle the training efficiency issue described in Section 3.3, which is resulted from going over all the possible actions, they pre-select actions from A to reduce the cost. Specifically, in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, they perform sampled softmax <ref type="bibr" target="#b0">[1]</ref> for training ?. In <ref type="bibr" target="#b12">[13]</ref>, they consider a heuristic ? (?) to pre-select actions, which results in their proposed POXM estimator as follows.</p><formula xml:id="formula_51">V ? POXM (? ) = E ? ? [ ?? ? ?=1 ? (A ? | ?? , ? (u); ?, ? ) ? (A ? | ?? ; ? ) (c A ? -?)],<label>(40)</label></formula><p>where they only consider actions in the pre-selected set ? (u) of each context such that</p><formula xml:id="formula_52">? (A ? | ?? , ? (u); ?, ? ) = ? ? ? ? ? ? ? ? ? (? ;u,? A ? )</formula><p>? ?? (u) \A 1:? -1 ? ? (? ;u,? ? )</p><formula xml:id="formula_53">A ? ? ? (u), 0 A ? ? ? (u).</formula><p>Besides, they follow <ref type="bibr" target="#b8">[9]</ref> to adjust ? in (40). In <ref type="bibr" target="#b12">[13]</ref>, they empirically construct ? (u) by referring to the behavior policy ? such that ? (u) ={A ? | A ? has the largest ? (? ? | ?? ; ? ),</p><formula xml:id="formula_54">and 1 ? ? ? ?},<label>(41)</label></formula><p>where ? is a specified number of pre-selected actions. The performance of this kind of methods highly relies on the quality of the pre-selection procedure. For example, in <ref type="bibr" target="#b2">[3]</ref>, they find the number of sampled actions significantly impacts the performance of the learned policy. For POXM, if ? can successfully cover actions with the highest rewards with only a small action space, POXM should perform better than approaches learned from the whole action space. However, the task to construct such an effective ? is just as challenging as learning ?. That is to say, if for all contexts, we are already able to obtain the wanted ?, which includes the actions with the highest rewards, then there may be no need for us to learn a new ?. In contrast, our proposed training method in Section 3.3 does not require any pre-selection on items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, after presenting our experimental setup for online simulation, we conduct a series of experiments to compare our proposed framework in Section 3 with other existing state-of-theart approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup for Online Simulation</head><p>To simulate the problem setting described in Section 2.1, we following the supervised-to-bandit conversion in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> to generate our data from MovieLens 1M (ml1m) and 10M (ml10m) data sets. <ref type="foot" target="#foot_3">5</ref>We first randomly split a data set into four independent subsets: D 1 , D 2 , D va and D te . Then, as shown in Figure <ref type="figure">2</ref>, we learn a behavior policy ? with D 1 and deploy it on D 2 by logging a (u, A, C) event for each context to form a set S 2 . Next we conduct different approaches to learn a new policy ?, where D Va is used as the validation set for tuning hyper-parameters. Finally, we evaluate ? on the test set D Te and compare the resulting performance. The whole procedure is illustrated in Figure <ref type="figure">2</ref>, while the statistics of the generated data sets are in Table <ref type="table" target="#tab_2">2</ref>. More details of data preprocessing and constructing ? are in Appendices A.2 and A.3.</p><p>For learning ?, we compare seven approaches, which can be grouped into four categories: ? Value-IPS, Value-DR: They are two value learning approaches solving ( <ref type="formula" target="#formula_7">9</ref>) respectively with ( <ref type="formula" target="#formula_4">6</ref>) and ( <ref type="formula" target="#formula_6">8</ref>). ? BanditNet: This approach considers the estimate in <ref type="bibr" target="#b19">(20)</ref> without pruning w A . To alleviate the possible issue caused by any extremely large w A , it additionally applies the technique proposed in <ref type="bibr" target="#b8">[9]</ref>. ? Top-? REINFORCE <ref type="bibr" target="#b1">[2]</ref> and POXM <ref type="bibr" target="#b12">[13]</ref>: They are two state-ofthe-art competitors in the policy learning for top-? decision making; see the discussion in Section 4. For POXM, we follow <ref type="bibr" target="#b12">[13]</ref> to construct ? (?) by ( <ref type="formula" target="#formula_54">41</ref>) and conduct grid searches on ?. ? RIIPS, Log-RIIPS and Adaptive-RIIPS: Three approaches derived from our proposed RIIPS estimator, which respectively solve problems in ( <ref type="formula" target="#formula_35">28</ref>), (32) and the approximation of (38). For Adaptive-RIIPS, we empirically set all ?? ? = -1 and consider ? ? ? = ? as a hyper-parameter to tune.</p><p>More details of the hyper-parameter setting and selection are in Appendix A.4.</p><p>To evaluate ?, we simulate the online environment. Specifically, we compute the following average cumulated reward (ACR) on a data set with fully observed rewards for all items.</p><formula xml:id="formula_55">ACR = 1 m ?? m ?=1 ? (? ? , A ? ),<label>(42)</label></formula><p>where m is the number of contexts included in this set, ? ? is the fullyobserved reward vector of context ?, and A ? is the set of ? items recommended according to ? (A ? | ? ? ; ? ). Here we focus on the exploitation ability of each learned policy ?, so the decision of A ? is made by selecting top ? items with the highest scores predicted by ? (?). For consistency, we use the same ? for constructing S 2 and evaluating models on D Va and D Te . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison on Various Approaches</head><p>By comparing results in Table <ref type="table" target="#tab_3">3</ref>, we have the following observations.</p><p>? Firstly, Value-DR is much better than Value-IPS. This result confirms what we described in Section 3.2. That is, the IPS approach may not work properly when the amount of observed events is deficient. Compared to Value-IPS, Value-DR imputes rewards for unobserved items and reweighs the loss term for these items, which can be seen as a way to tune the reward distribution adaptively. This leads to a great improvement on the performance. ? RIIPS makes a significant improvement over BanditNet when ? = 10. The reason is that RIIPS prunes w A . As reported in <ref type="bibr" target="#b10">[11]</ref>, the value of w A gets exploded very easily when ? &gt; 1, which leads to a huge divergence between V ? IPS and ? ? . BanditNet's performance suffers without pruning w A .</p><p>? In Table <ref type="table" target="#tab_3">3</ref>, RIIPS is better than RIIPS (? = 0) for ? = 10, which confirms the rationality of adding the regularization term ? KL (? || ?) with pruning ? -1 ?=1 w ? . ? Compared to other approaches, top-? REINFORCE is almost the second optimal approach in policy learning when ? = 10. This verifies the applicability of (39) proposed in <ref type="bibr" target="#b1">[2]</ref> and confirms the necessity for alleviating the winner-takes-all effect. ? Adaptive-RIIPS proposed in this work performs the best. As stated in Section 3.2, by tuning ?, we can adapt the probability mass of the learned policy. This effectively addresses the winnertakes-all effect and overcomes the drawback of Log-RAIPS. ? In our data sets, POXM performs the worst because as described in Section 4, it is tough to get an effective ? (u). Even though our behavior policy ? is trained on D 1 with fully-revealed ?, the pre-selected ? still can not cover enough items with the highest rewards. Increasing the size of ? may help, but it also lowers the training efficiency. This issue might limit the usability of POXM. To further confirm the efficiency and effectiveness of the framework proposed in this work, we compare the performance and training time of Adaptive-RIIPS trained by two different optimizers: Adagrad and Gauss-Newton. In Table <ref type="table" target="#tab_4">4</ref>, we can observe that Adaptive-RIIPS with Gauss-Newton consistently converges faster and better than the one with Adagrad. Take ml1m with ? = 1 for example. Gauss-Newton not only speeds up more than 10 times for the model training but also improves the model performance by about 6% respectively when compared to Adagrad. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we categorize the conventional approaches of counterfactual learning for recommender systems into two classes: value learning and policy learning. To do a comparison between these two kinds of approaches, we point out that some existing difficulties in the policy learning approaches for large top-? recommender systems must be addressed first.</p><p>? It is likely that the extremely small propensity of a top-? recommendation would lead to a poor estimation of V ? IPS and thus a sub-optimal policy.</p><p>? The robustness and generalization of policy learning approaches suffer from the limited observations of each item for each context. ? The issue of training efficiency in recommender systems with a large item space limits the usability of policy learning approaches. To address the above difficulties, we derive a novel policy learning framework. We introduce a regularized per-item approach to balance between the variance and the bias of policy learning. Then, through decoupling the objective function and introducing an extra hyper-parameter for tuning the smoothness, we manage to improve the robustness of policy learning. For efficient training, we integrate our proposed approach with the two-tower structure and the algorithm developed in <ref type="bibr" target="#b25">[26]</ref> to be a framework able to train large top-? recommender systems. With experiments conducted on real-world data sets, we confirm the effectiveness and efficiency of our proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example with two contexts and two items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Main notation.</figDesc><table><row><cell cols="2">?, ?, ? numbers of contexts, items, and positions</cell></row><row><cell>?, ?</cell><cell>behavior (deployed) policy and new policy</cell></row><row><cell>A</cell><cell>set of ? items</cell></row><row><cell>u, c</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Data statistics, where ? is the number of items. | ? | indicates the number of elements in a set.</figDesc><table><row><cell>Data Set</cell><cell>?</cell><cell>|D 1 |</cell><cell>|D 2 |</cell><cell>|D Va |</cell><cell>|D Te |</cell></row><row><cell>ml1m</cell><cell>3,513</cell><cell>12,066</cell><cell>27,153</cell><cell>9,051</cell><cell>12,077</cell></row><row><cell cols="6">ml10m 10,210 139,022 312,836 104,279 139,081</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>For two data sets, we report the ACR of all approaches on D Te . All scores below are scaled up by 100.</figDesc><table><row><cell></cell><cell cols="2">ml1m</cell><cell cols="2">ml10m</cell></row><row><cell>Approach</cell><cell cols="4">? = 1 ? = 10 ? = 1 ? = 10</cell></row><row><cell>Value-IPS</cell><cell>24.04</cell><cell>25.91</cell><cell>25.23</cell><cell>17.07</cell></row><row><cell>Value-DR</cell><cell>38.80</cell><cell>28.75</cell><cell>27.46</cell><cell>26.25</cell></row><row><cell>POXM</cell><cell>28.66</cell><cell>16.48</cell><cell>22.74</cell><cell>14.54</cell></row><row><cell cols="2">top-? REINFORCE 34.47</cell><cell>29.13</cell><cell>30.41</cell><cell>25.39</cell></row><row><cell>BanditNet</cell><cell>34.51</cell><cell>18.21</cell><cell>30.38</cell><cell>19.14</cell></row><row><cell>RIIPS</cell><cell>35.99</cell><cell>28.44</cell><cell>30.41</cell><cell>24.19</cell></row><row><cell>Log-RIIPS</cell><cell>35.11</cell><cell>26.40</cell><cell>28.73</cell><cell>20.57</cell></row><row><cell>Adaptive-RIIPS</cell><cell cols="4">39.02 32.27 34.66 28.27</cell></row><row><cell>RIIPS (? = 0)</cell><cell>34.47</cell><cell>23.01</cell><cell>30.41</cell><cell>23.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison on Adaptive-RIIPS with two different optimizers: Adagrad and Gauss-Newton. Training time and performance under ? = 1 and 10 for two datasets are shown.</figDesc><table><row><cell cols="2">Dataset Optimizers</cell><cell>? = 1</cell><cell>? = 10</cell></row><row><cell></cell><cell></cell><cell cols="2">Time(s) ACR Time(s) ACR</cell></row><row><cell>ml1m</cell><cell>Adagrad Gauss-Newton</cell><cell>739.04 58.12</cell><cell>36.74 6028.79 30.70 39.02 1422.50 32.70</cell></row><row><cell>ml10m</cell><cell>Adagrad Gauss-Newton</cell><cell cols="2">13826.98 34.24 60850.22 27.07 673.52 34.66 14500.11 28.27</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>See the proof in our supplementary materials.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>See the details in Appendix A.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>See the detailed derivations in our supplementary materials.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://grouplens.org/datasets/movielens/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by <rs type="funder">MOST of Taiwan</rs> grant <rs type="grantNumber">110-2221-E-002-115-MY3</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dVHvQfv">
					<idno type="grant-number">110-2221-E-002-115-MY3</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Details of Model Setting and its Connection with Extreme Similarity Learning Problems</head><p>The connection between our proposed Adaptive-RIIPS and extreme similarity learning problems relies on the fact that the twotower structure models are prevalent in modern recommender systems. Let ? = [ ? ? ? ? ]. The two-tower structure transforms ? and ? into two ?-dimensional embeddings respectively by learning two embedding functions ? (? ? ; ?) and ?(? ? ; ?), and then ? (? ; ?, ?) = ? (? ? ; ?) ? ?(? ? ; ?) is the score function of this structure. Many commonly-used recommendation models fall into the two-tower structure. For example, when ? (?) and ?(?) are linear embedding functions, it has been pointed out <ref type="bibr" target="#b27">[28]</ref> that ? (?) is a variant of the factorization machine <ref type="bibr" target="#b16">[17]</ref>. Besides, non-linear embedding functions (e.g., neural networks) are also widely applied in two-tower recommendation models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Through applying the two-tower structure models for ? (? ; ?, ?), the problem (38) can be considered as an extreme similarity learning problem. That is, we learn the similarity between a sub-context and an item from extremely large ?? (sub-context, item) pairs. Here the similarity presents the tendency of an item being recommended by ? under a given sub-context. To avoid any O(??) cost in solving an extreme similarity learning problem, existing works in this field <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> impose the following squared loss on a certain part of the loss function. In our case, the second term in (38), which deals with the similarity between sub-contexts and non-recommended items, is replaced with ? sq ( ?? ? , ?? ? ) = ? ? ? ( ?? ? -?? ? ) 2 , where ? ? ? is a cost associated with the loss and ? is an imputed value. For each ? ? ? and ?? ? , it is required that they can be decomposed to multiple parts solely related to ? or ?, respectively.</p><p>Then, the remaining issue is how to choose ? ? ? and impute ?? ? . We apply the second-order Taylor expansion of ? - log ( ?? ? ) at our chosen point ?? for each item to get the following alternative loss function for non-recommended items. 6   1 2</p><p>By applying (43) to (38), we attain the following problem to solve.</p><p>where for all ?, ? ? ? = ?? 2 ? - log ( ?? ), and ?? ? =</p><p>are pre-defined constants solely dependent to ?, and ?? ? comes from the two-tower structure. As deciding ?? is like to select a hyperparameter, we instead treat ? ? ? and ?? ? as two hyper-parameters and tune them by grid search. In our experiments, this simple transformation is found to be effective enough empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of Data Preprocessing</head><p>For MovieLens 1M (ml1m) and 10M (ml10m) data sets, we follow <ref type="bibr" target="#b22">[23]</ref> to binarize ratings included in original sets. We consider pairs 6 See the detailed derivations in our supplementary materials.</p><p>with rating ? 4 as positive while the rest including unrated items as negative. As original sets do not include contextual information, we need to generate the contextual features ? and context-aware rewards ? respectively being realizations of u and c. Specifically, for each user, we group his/her positive items into a set and then randomly divide it into two subsets equally. Positive items in the first subset, like movies the user has watched, are used as the contextual information to construct ? ? {0, 1} ? where</p><p>1 item ? is included in the first subset, 0 otherwise.</p><p>Besides, positive items in the second subset, like movies the user is going to watch, are used for constructing ? where</p><p>item ? is included in the second subset, 10 -3 otherwise.</p><p>We repeat the above procedure to generate ten (?, ?) pairs for each user. For ? , we generate a feature vector of each item by one-hot encoding with its identity number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details of Behavior Policy ?</head><p>For learning ?, similar to past works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, we learn ? (?) by a value learning approach with the fully-revealed ? included in D 1 . We apply the approach in <ref type="bibr" target="#b23">[24]</ref> for this task. With the learned ? (?), we transform it into a policy ? defined in <ref type="bibr" target="#b18">(19)</ref> by the PL ranking model defined in (30) such that for each sub-context, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Details of Hyper-Parameter Selection</head><p>For the two-tower structure we use in all approaches, the output size of the embeddings is set to be 128. Adaptive-RIIPS uses the Gauss-Newton method proposed in <ref type="bibr" target="#b25">[26]</ref>, while all the other approaches are trained by Adagrad. For Adagrad, the learning rate is initialized to 0.05 and the batch size is set to 10% of the training data. As an exception, for ml10m with ? = 10, the batch size of Adagrad is set to 1% of the training data due to the size of our RAM. For Value-DR with (8), we set ?? = log( CTR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-CTR</head><p>), ??, where CTR is the average click-through-rate from an unbiased set collected by a uniform behavior policy. For convenience, we directly compute CTR from D 2 , which is unbiased due to fully-observed reward vectors. We train each approach for T epochs, retain the model at the epoch with the best validation performance, and report the test performance by applying the model to predict the test set. For Adagrad, T = 500 and for Gauss-Newton, T = 30.</p><p>The following hyper-parameters are selected by a grid search. Candidates of each hyper-parameter are also listed below.</p><p>? ? ? {4 -1 , 4 0 , ..., 4 4 }: the ?2 regularization coefficient.</p><p>? ? ? {0, 0.01, 0.1, 1}: the coefficient of ? KL (? || ?).</p><p>? ?, ? ? {16 -5 , 16 -4 , 16 -3 , 16 -2 }: the coefficients of the square loss term in ( <ref type="formula">8</ref>) and (44). ? ? ? {0.6, 0.8, 1.0, 1.2}: the Lagrange multiplier in <ref type="bibr" target="#b8">[9]</ref>.</p><p>? ? ? {0.1, 0.3, 0.5}: the tolerance in the relative stopping condition for Gauss-Newton. ? ? ? {10, 20, 50}: the size of ? for POXM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quick Training of Probabilistic Neural Nets by Importance Sampling</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-S?bastien</forename><surname>Sen?cal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Ninth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top-k off-policy correction for a REINFORCE recommender system</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="456" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">User Response Models to Improve a REINFORCE Recommender System</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>WSDM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Doubly Robust Policy Evaluation and Learning</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dud?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Generalization of Sampling Without Replacement From a Finite Universe</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donovan</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952. 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information and Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized Bradley-Terry Models and Multi-class Probability Estimates</title>
		<author>
			<persName><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruby</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/generalBT.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="85" to="115" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint Policy-Value Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Jeunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Bompaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning with logged bandit feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale validation of counterfactual learning methods: A test-bed</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Lefortier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Inference and Learning of Hypothetical and Counterfactual Interventions in Complex Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Offline evaluation of ranking policies with click models</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwa</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning from eXtreme Bandit Feedback</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Off-policy Learning in Two-stage Recommender Systems</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Analyzing and Modeling Rank Data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">I</forename><surname>Marden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eligibility Traces for Off-Policy Policy Evaluation</title>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning (ICML)</title>
		<meeting>the Seventeenth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Off-policy bandits with deficient support</title>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch learning from logged bandit feedback through counterfactual risk minimization</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1731" to="1755" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Self-normalized Estimator for Counterfactual Learning</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3231" to="3239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Off-Policy Evaluation for Slate Recommendation</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dud?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3635" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selection of Negative Samples for One-class Matrix Factorization</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/one-class-mf/biased-mf-sdm-with-supp.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIAM International Conference on Data Mining (SDM)</title>
		<meeting>SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Unified Algorithm for One-class Structured Matrix Factorization with Side Information</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Yuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dihillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/ocmf-side/biased-leml-aaai-with-supp.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving Ad Click Prediction by Considering Non-displayed Events</title>
		<author>
			<persName><forename type="first">Jui-Yang</forename><surname>Bowen Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng-Yuan</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chihyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/occtr/ctr_oc.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 28th ACM International Conference on Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient optimization methods for extreme similarity learning with nonlinear embeddings</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengrui</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/similarity_learning/pq.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased Ad click prediction for position-aware advertising systems</title>
		<author>
			<persName><forename type="first">Yaxu</forename><surname>Bowen Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jui-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/debiases/debiases.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Recommender Systems</title>
		<meeting>the 14th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">One-class Field-aware Factorization Machines for Recommender Systems with Implicit Feedbacks</title>
		<author>
			<persName><forename type="first">Meng-Yuan</forename><surname>Bowen Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jui-Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/papers/ocffm/imp_ffm.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
