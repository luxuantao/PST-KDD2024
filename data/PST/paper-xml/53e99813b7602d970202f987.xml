<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mathematics of Operations Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-09-22">22 September 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Garud</forename><forename type="middle">N</forename><surname>Iyengar</surname></persName>
							<email>garud@ieor.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">IEOR Department</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mathematics of Operations Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-09-22">22 September 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">01414254694B8B4AC896E99CD9DB051D</idno>
					<idno type="DOI">10.1287/moor.1040.0129</idno>
					<note type="submission">Received December 17, 2002; revised October 10, 2003, and June 11, 2004.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dynamic programming</term>
					<term>robust optimization</term>
					<term>Markov decision processes</term>
					<term>ambiguity MSC2000 subject classification: Primary: 90C39, 90C47</term>
					<term>secondary: 90C40, 90C25 OR/MS subject classification: Primary: Dynamic programming/optimal control, decision analysis-risk</term>
					<term>secondary: probability-Markov processes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a robust formulation for discrete time dynamic programming (DP). The objective of the robust formulation is to systematically mitigate the sensitivity of the DP optimal policy to ambiguity in the underlying transition probabilities. The ambiguity is modeled by associating a set of conditional measures with each state-action pair. Consequently, in the robust formulation each policy has a set of measures associated with it. We prove that when this set of measures has a certain "rectangularity" property, all of the main results for finite and infinite horizon DP extend to natural robust counterparts. We discuss techniques from Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> for constructing suitable sets of conditional measures that allow one to efficiently solve for the optimal robust policy. We also show that robust DP is equivalent to stochastic zero-sum games with perfect information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. This paper is concerned with sequential decision making in uncertain environments. Decisions are made in stages and each decision, in addition to providing an immediate reward, changes the context of future decisions; thereby affecting the future rewards. Due to the uncertain nature of the environment, there is limited information about both the immediate reward from each decision and the resulting future state. In order to achieve a good performance over all the stages, the decision maker has to trade-off the immediate payoff with future payoffs. Dynamic programming (DP) is the mathematical framework that allows the decision maker to efficiently compute a good overall strategy by succinctly encoding the evolving information state. In the DP formalism the uncertainty in the environment is modeled by a Markov process whose transition probability depends both on the information state and the action taken by the decision maker. It is assumed that the transition probability corresponding to each state-action pair is known to the decision maker, and the goal is to choose a policy, i.e., a rule that maps states to actions, that maximizes some performance measure. Puterman <ref type="bibr" target="#b20">[20]</ref> provides a excellent introduction to the DP formalism and its various applications. In this paper, we assume that the reader has some prior knowledge of DP.</p><p>The DP formalism encodes information in the form of a "reward-to-go" function (see Puterman <ref type="bibr" target="#b20">[20]</ref> for details) and chooses an action that maximizes the sum of the immediate reward and the expected "reward-to-go." Thus, to compute the optimal action in any given state the "reward-to-go" function for all the future states must be known. In many applications of DP, the number of states and actions available in each state are large; consequently, the computational effort required to compute the optimal policy for a DP can be overwhelming-Bellman's "curse of dimensionality." For this reason, considerable recent research effort has focused on developing algorithms that compute an approximately optimal policy efficiently (Bertsekas and Tsitsiklis <ref type="bibr" target="#b4">[5]</ref>, de Farias and Van Roy <ref type="bibr" target="#b7">[8]</ref>).</p><p>Fortunately, for many applications the DP optimal policy can be computed with a modest computational effort. In this paper we restrict attention to this class of DPs. Typically, the transition probability of the underlying Markov process is estimated from historical data and is, therefore, subject to statistical errors. In current practice, these errors are ignored and the optimal policy is computed assuming that the estimate is, indeed, the true transition probability. The DP optimal policy is quite sensitive to perturbations in the transition probability and ignoring the estimation errors can lead to serious degradation in performance (Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>, Tsitskilis et al. <ref type="bibr" target="#b24">[24]</ref>). Degradation in performance due to estimation errors in parameters has also been observed in other contexts (Ben-Tal and Nemirovski <ref type="bibr" target="#b2">[3]</ref>, Goldfarb and Iyengar <ref type="bibr" target="#b13">[14]</ref>). Therefore, there is a need to develop DP models that explicitly account for the effect of errors.</p><p>In order to mitigate the effect of estimation errors we assume that the transition probability corresponding to a state-action pair is not exactly known. The ambiguity in the transition probability is modeled by associating a set s a of conditional measures with each stateaction pair s a . We adopt the convention of the decision analysis literature wherein uncertainty refers to random quantities with known probability measures and ambiguity refers to unknown probability measures (see, e.g., Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>). Consequently, in our formulation each policy has a set of measures associated with it. The value of a policy is the minimum expected reward over the set of associated measures, and the goal of the decision maker is to choose a policy with maximum value; i.e., we adopt a maximin approach. We will refer to this formulation as robust DP. We prove that, when the set of measures associated with a policy satisfy a certain "rectangularity" property (Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>), the following results extend to natural robust counterparts: the Bellman recursion, the optimality of deterministic policies, the contraction property of the value iteration operator, and the policy iteration algorithm. "Rectangularity" is a sort of independence assumption and is a minimal requirement for these results to hold. However, this assumption is not always appropriate, and is particularly troublesome in the infinite horizon setting (see Appendix A for details). We show that if the decision maker is restricted to stationary policies the effects of the "rectangularity" assumption are not serious.</p><p>There is some previous work on modeling ambiguity in the transition probability and mitigating its effect on the optimal policy. Satia and Lave <ref type="bibr" target="#b22">[22]</ref>, White and Eldieb <ref type="bibr" target="#b25">[25]</ref> and Bagnell et al. <ref type="bibr" target="#b1">[2]</ref> investigate ambiguity in the context of infinite horizon DP with finite state and action spaces. They model ambiguity by constraining the transition probability matrix to lie in a prespecified polytope. They do not discuss how one constructs this polytope. Moreover, the complexity of the resulting robust DP is at least an order of magnitude higher than DP. Shapiro and Kleywegt <ref type="bibr" target="#b23">[23]</ref> investigate ambiguity in the context of stochastic programming and propose a sampling-based method for solving the maximin problem. However, they do not discuss how to choose and calibrate the set of ambiguous priors. None of this work discusses the dynamic structure of the ambiguity; in particular, there is no discussion of the central role of "rectangularity." Our theoretical contributions are based on recent work on uncertain priors in the economics literature (Gilboa and Schmeidler <ref type="bibr" target="#b11">[12]</ref>, Epstein and Schneider <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, Hansen and Sargent <ref type="bibr" target="#b14">[15]</ref>). The focus of this body of work is on the axiomatic justification for uncertain priors in the context of multi-period utility maximization. It does not provide any means of selecting the set of uncertain priors nor does it focus on efficiently solving the resulting robust DP.</p><p>While this paper was being prepared for submission, we became aware of a technical report by Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> where they independently formulate robust DP and develop a robust Bellman recursion. Although not explicitly stated, the rectangularity assumption is implicit in their construction. They identify sets of conditional measures based on likelihood measures that have the following desirable properties: The sets provide a means for setting any desired level of confidence in the robust optimal policy; for a given confidence level, the corresponding set from each family is easily parameterizable from data; and the complexity of solving the robust DP corresponding to these families of sets is only modestly larger than the nonrobust counterpart.</p><p>This paper is organized as follows. In §2 we formulate finite horizon robust DP and the "rectangularity" property that leads to the robust counterpart of the Bellman recursion. In §3 we formulate the robust extension of discounted infinite horizon DP. In §4 we describe three families of sets of conditional measures that are all based on the relative entropy or the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>259</head><p>Kullback-Leibler distance. The results in this section, although independently obtained, are not new and were first obtained by Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> (see <ref type="bibr">Nilim and El Ghaoui [18]</ref>). In §5 we show that the robust DP is equivalent to stochastic two-player zero-sum games with perfect information. Section 6 concludes the paper with some remarks.</p><p>2. Finite horizon robust dynamic programming. Decisions are made at discrete points in time t ∈ T = 0 1 referred to as decision epochs. In this section we assume that T finite, i.e., T = 0 N -1 for some N ≥ 1. At each epoch t ∈ T the system occupies a state s ∈ t , where t is assumed to be discrete (finite or countably infinite). In a state s ∈ t the decision maker is allowed to choose an action a ∈ t s , where t s is assumed to be discrete. Although many results in this paper extend to nondiscrete state and action sets, we avoid this generality because the associated measurability issues would detract from the ideas that we want to present in this work.</p><p>For any discrete set , we will denote the set of probability measures on by . Decision makers can choose actions either randomly or deterministically. A random action is a state s ∈ t corresponds to an element q s ∈ s with the interpretation that an action a ∈ s is selected with probability q s a . Degenerate probability measures that assign all the probability mass to a single action correspond to deterministic actions.</p><p>Associated with each epoch t ∈ T and state-action pair s a , a ∈ s , s ∈ t , is a set of conditional measures t s a ⊆ t+1 with the interpretation that if at epoch t, action a is chosen in state s, the state s t+1 at the next epoch t + 1 is determined by some conditional measure p sa ∈ t s a . Thus, the state transition is ambiguous. We adopt the convention of the decision analysis literature wherein uncertainty refers to random quantities with known probability measures and ambiguity refers to unknown probability measures (see, e.g., Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>).</p><p>The decision maker receives a reward r t s t a t s t+1 when the action a t ∈ s t is chosen in state s t ∈ at the decision epoch t, and the state at the next epoch is s t+1 ∈ . Since s t+1 is ambiguous, we allow the reward at time t to depend on s t+1 as well. Note that one can assume, without loss of generality, that the reward r t • • • is certain. The reward r N s at the epoch N is only a function of the state s ∈ N .</p><p>We will refer to the collection of objects T t t t r t • • • t ∈ T as a finite horizon ambiguous Markov decision process (AMDP). The notation above is a modification of that in Puterman <ref type="bibr" target="#b20">[20]</ref> and the structure of ambiguity is motivated by Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>.</p><p>A decision rule d t is a procedure for selecting actions in each state at a specified decision epoch t ∈ T . We will call a decision rule history dependent if it depends on the entire past history of the system as represented by the sequence of past states and actions; i.e., d t is a function of the history h t = s 0 a 0 s t-1 a t-1 s t A policy prescribes the decision rule to be used at all decision epochs. Thus, a policy is a sequence of decision rules; i.e., = d t t ∈ T . Given the ambiguity in the conditional measures, a policy induces a collection of measure on the history space N . We assume that the set of measures consistent with a policy has the following structure.</p><p>Assumption 2.1 (Rectangularity). The set of measures consistent with a policy is given by</p><formula xml:id="formula_0">= P ∀ h N ∈ N P h N = t∈T p h t a t s t+1 p h t ∈ d t t ∈ T = d 0 × d 1 × • • • × d N -1 (3)</formula><p>where the notation in <ref type="bibr" target="#b2">(3)</ref> simply denotes that each p ∈ is a product of p t ∈ d t , and vice versa.</p><p>The rectangularity assumption is motivated by the structure of the recursive multiple priors in Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>. We will defer discussing the implications of this assumption until after we define the objective of the decision maker.</p><p>The reward V 0 s generated by a policy starting from the initial state s 0 = s is defined as follows.</p><p>V 0 s = inf</p><formula xml:id="formula_1">P∈ E P t∈T r t s t d t h t s t+1 + r N s N (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>where E P denotes the expectation with respect to the fixed measure P ∈ . Equation (4) defines the reward of a policy to be the minimum expected reward over all measures consistent with the policy . Thus, we take a worst-case approach in defining the reward. In the optimization literature this approach is known as the robust approach (Ben-Tal and Nemirovski <ref type="bibr" target="#b3">[4]</ref>). Let denote the set of all history dependent policies. Then the goal of robust DP is to characterize the robust value function</p><formula xml:id="formula_3">V * 0 s = sup ∈ V 0 s = sup ∈ inf P∈ E P t∈T r t s t d t h t s t+1 + r N s N (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>and an optimal policy * if the supremum is achieved.</p><p>In order to appreciate the implications of the rectangularity assumption the objective (5) has to be interpreted in an adversarial setting: The decision maker chooses ; an adversary observes , and chooses a measure P ∈ that minimizes the reward. In this context, rectangularity is a form of an independence assumption: The choice of particular distribution p ∈ s t a t in a state-action pair s t a t at time t does not limit the choices of the adversary in the future. This, in turn, leads to a separability that is crucial for establishing the robust counterpart of the Bellman recursion (see Theorem 2.1). Similar separability assumptions also appear in the robust control literature in the context of obtaining lower bounds for system performance (see, e.g., Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>). The rectangularity assumption is not always appropriate (see Appendix A for an example of such a situation). This assumption can be somewhat justified in the context of finite-horizon problems by invoking time-inhomogeneity. However, such an explanation does not extend to infinite horizon models. We will discuss the implications of rectangularity in infinite horizon models in §3.</p><p>The optimistic value V 0 s 0 of a policy starting from the initial state s 0 = s is defined as</p><formula xml:id="formula_5">V 0 s = sup P∈ E P t∈T r t s t d t h t s t+1 + r N s N (6)</formula><p>Let V 0 s 0 P denote the nonrobust value of a policy corresponding to a particular choice P ∈ . Then V 0 s 0 ≥ V 0 s 0 P ≥ V 0 s 0 . Analogous to the robust value function V * 0 s , the optimistic value function V * 0 s is defined as Remark 2.1. Since our interest is in computing the robust optimal policy * , we will restrict attention to the robust value function V * 0 . However, all the results in this paper imply a corresponding result for the optimistic value function V * 0 with the inf P∈ • replaced by sup P∈ • .</p><formula xml:id="formula_6">V * 0 s = sup ∈ V 0 s =</formula><p>Let V n h n denote the reward obtained by using policy over epochs n n+1 N -1, starting from the history h n ; i.e.,</p><formula xml:id="formula_7">V n h n = inf P∈ n E P N -1 t=n r t s t d t h t s t+1 + r N s N (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where rectangularity implies that the set of conditional measures n consistent with the policy and the history h n is given by</p><formula xml:id="formula_9">n = P n n → N -1 t=n t × t+1 ∀ h n ∈ n P h n a n s n+1 a N -1 s N = N -1 t=n p h t a t s t+1 p h t ∈ d t t = n N -1 = d n × d n+1 × • • • × d N -1 = d n × n+1<label>(9)</label></formula><p>Let V * n h n denote the optimal reward starting from the history h n at the epoch n; i.e.,</p><formula xml:id="formula_10">V * n h n = sup ∈ n V n h n = sup ∈ n inf P∈ n E P N -1 t=n r t s t d t h t s t+1 + r N s N (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where n is the set of all history dependent randomized policies for epochs t ≥ n.</p><p>Theorem 2.1 (Bellman Equation). The set of functions V * n n = 0 1 N satisfies the following robust Bellman equation:</p><formula xml:id="formula_12">V * N h N = r N s N V * n h n = sup a∈ s n inf p∈ s n a E p r n s n a s +V * n+1 h n a s n= 0 N -1<label>(11)</label></formula><p>Proof. From (9) it follows that</p><formula xml:id="formula_13">V * n h n = sup ∈ inf P= p P ∈ d n × n+1 E P N -1 t=n r t s t d t h t s t+1 + r N s N</formula><p>Since the conditional measures P do not affect the first term r n s n d n h n s n+1 , we have:</p><formula xml:id="formula_14">V * n h n = sup ∈ n inf p P ∈ d n × n+1 E p r n s n d n h n s n+1 +E P N -1 t=n+1 r t s t d t h t s t+1 +r N s N = sup ∈ n inf p∈ d n E p r n s n d n h n s n+1 + inf P∈ n+1 E P N -1 t=n+1 r t s t d t h t s t+1 +r N s N = sup ∈ n inf p∈ d n E p r n s n d n h n s n+1 +V n+1 h n d n h n s n+1 (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where the last equality follows from the definition of V n+1 h n+1 in (8). . Therefore, <ref type="bibr" target="#b11">(12)</ref> implies that</p><formula xml:id="formula_16">V * n h n ≤ sup ∈ n inf p∈ d n E p r n s n d n h n s n+1 + V * n+1 h n d n h n s n+1 = sup d n ∈ n inf p∈ d n E p r n s n d n h n s n+1 + V * n+1 h n d n h n s n+1 (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where n is the set of all history-dependent decision rules at time n, and ( <ref type="formula" target="#formula_16">13</ref>) follows from the fact that the term within the expectation only depends on</p><formula xml:id="formula_18">d n ∈ n . Since V * n+1 h n+1 = sup ∈ n+1 V n+1 h n+1 , it follows that for all &gt; 0 there exists a policy n+1 ∈ n+1 such that V n+1 n+1 h n+1 ≥ V * n+1 h n+1 -, for all h n+1 ∈ n+1 . For all d n ∈ n , d n n+1 ∈ n . Therefore, V * n h n = sup ∈ n inf p∈ d n E p r n s n d n h n s n+1 + V n+1 h n d n h n s n+1 ≥ sup d n ∈ n inf p∈ d n E p r n s n d n h n s n+1 + V n+1 n+1 h n d n h n s n+1 ≥ sup d n ∈D n inf p∈ d n E p r n s n d n h n s n+1 + V * n+1 h n d n h n s n+1 -<label>(14)</label></formula><p>Since &gt; 0 is arbitrary, ( <ref type="formula" target="#formula_16">13</ref>) and ( <ref type="formula" target="#formula_18">14</ref>) imply that</p><formula xml:id="formula_19">V * n h n = sup d n ∈ n inf p∈ d n E p r n s n d n h n s n+1 + V * n+1 h n d n h n s n+1</formula><p>The definition of d n in (2) implies that V * n h n can be rewritten as follows.</p><formula xml:id="formula_20">V * n h n = sup q∈ s n inf p s n a ∈ n s n a a∈ s n q a s∈ p s n a s r n s n a s + V * n+1 h n a s = sup q∈ s n a∈ s n q a inf p s n a ∈ n s n a s∈ p s n a s r n s n a s + V * n+1 h n a s = sup a∈ s n inf p∈ n s n a s∈ p s r n s n a s + V * n+1 h n a s (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>where <ref type="bibr" target="#b14">(15)</ref> follows from the fact that sup u∈W w u ≥ u∈W q u w u for all discrete sets W , functions w W → R, and probability measures q on W . As mentioned before, while this paper was being prepared for publication we became aware of a technical report by Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> where they independently formulate robust DP and present a robust Bellman recursion.</p><p>The following corollary establishes that one can restrict the decision maker to deterministic policies without affecting the achievable robust reward. Proof. This result follows from <ref type="bibr" target="#b10">(11)</ref>. The details are left to the reader. Next, we show that it suffices to restrict oneself to deterministic Markov policies, i.e., policies where the deterministic decision rule d t at any epoch t is a function of only the current state s t .</p><p>Theorem 2.2 (Markov Optimality). For all n = 0 N , the robust value function V * n h n is a function of the current state s n alone, and V * n s n = sup ∈ MD V n s n , n ∈ T , where MD is the set of all deterministic Markov policies. Therefore, the robust Bellman equation <ref type="bibr" target="#b10">(11)</ref> reduces to</p><formula xml:id="formula_22">V * n s n = sup a∈ s n inf p∈ n s n a E p r n s n a s + V * n+1 s n∈ T (16)</formula><p>Proof. The result is established by induction on the epoch t. For t = N , the value function V * N h N = r N s N and is, therefore, a function of only the current state. Next, suppose the result holds for all t &gt; n. From the Bellman equation ( <ref type="formula" target="#formula_12">11</ref>) we have</p><formula xml:id="formula_23">V * n h n = sup a∈ s n inf p∈ n s n a E p r n s n a s + V * n+1 h n a s = sup a∈ s n inf p∈ n s n a E p r n s n a s + V * n+1 s<label>(17)</label></formula><p>where <ref type="bibr" target="#b16">(17)</ref> follows from the induction hypothesis. Since the right-hand side of ( <ref type="formula" target="#formula_23">17</ref>) depends on h n only via s n , the result follows.</p><p>The recursion relation ( <ref type="formula">16</ref>) forms the basis for robust DP. This relation establishes that, provided V * n+1 s is known for all s ∈ , computing V * n s reduces to a collection of optimization problems. Suppose the action set s is finite. Then the optimal decision rule d * n at epoch n is given by</p><formula xml:id="formula_24">d * n s = arg max a∈ s inf p∈ n s a E p r n s a s + V n+1 s</formula><p>Hence, in order to compute the value function V * n efficiently one must be able to efficiently solve the optimization problem inf p∈ s a E p v for a specified s ∈ , a ∈ s and v ∈ R . In §4 we describe three families of sets s a of conditional measures for which inf p∈ s a E p v can be solved efficiently.</p><p>As noted in Remark 1, Theorem 2.2 implies the following result for the optimistic value function V * n . Theorem 2.3. For n = 0 N , the optimistic value function V * n h n is a function of the current state s n alone, and</p><formula xml:id="formula_25">V * n s n = sup ∈ MD V n s n n ∈ T</formula><p>where MD is the set of all deterministic Markov policies. Therefore,</p><formula xml:id="formula_26">V * n s n = sup a∈ s n sup p∈ n s n a E p r n s n a s + V * n+1 s n∈ T (18)</formula><p>3. Infinite horizon robust dynamic programming. In this section we formulate infinite horizon robust DP with a discounted reward criterion and describe methods for solving this problem. Robust infinite horizon DP with finite state and action spaces was addressed in Satia <ref type="bibr" target="#b21">[21]</ref> and Satia and Lave <ref type="bibr" target="#b22">[22]</ref>. A special case of the robust DP where the decision Downloaded from informs.org by [129.93. <ref type="bibr" target="#b15">16</ref> maker is restricted to stationary policies appears in Bagnell et al. <ref type="bibr" target="#b1">[2]</ref>. We will contrast our contributions with the previous work as we establish the main results of this section.</p><p>The setup is similar to the one introduced in §2. As before, we assume that the decisions epochs are discrete; however, now the set T = 0 1 2 = Z + . The system state s ∈ , where is assumed to be discrete, and in state s ∈ the decision maker is allowed to take a randomized action chosen from a discrete set s . As the notation suggests, in this section we assume that the state space is not a function of the decision epoch t ∈ T .</p><p>Unlike in the finite horizon setting, we assume that the set of conditional measures s a ⊆ is not a function of the decision epoch t ∈ T ; i.e., time is homogeneous. Thus, one is forced to carefully delineate the dynamic structure of uncertainty. We consider two distinct models for the uncertainty, or equivalently, the adversary.</p><p>(i) Static model: The adversary is restricted to choose the same, but unknown, p sa ∈ s a every time the state-action pair s a is encountered.</p><p>(ii) Dynamic model: The adversary is allowed to choose a possibly different conditional measure p ∈ s a every time the state-action pair s a is encountered. Thus, in this model, the set of measures consistent with a policy satisfies rectangularity; i.e., = t∈T d t . As mentioned in the introduction, the goal of the robust formulation is to systematically mitigate the effect of errors associated with estimating the state transitions; i.e., the state transition is, in fact, fixed but the decision maker is only able to estimate it to within a set. Thus, the static model is appropriate for this scenario. However, computing the optimal policy for the static model is NP-hard; therefore, we will restrict attention to the dynamic model. Clearly the value function in the dynamic model is a lower bound for the value function in the static model. We contrast the implications of the two models in Lemma 3.3. Bagnell et al. <ref type="bibr" target="#b1">[2]</ref> also has some discussion on this issue.</p><p>As before, the reward r s t a t s t+1 is a function of the current state s t , the action a t ∈ s t , and the future state s t+1 ; however, it is not a function of the decision epoch t. We will also assume that the reward is bounded; i.e., sup s s ∈ a∈ s r s a s = R &lt; . The reward V s received by employing a policy when the initial state s 0 = s is given by</p><formula xml:id="formula_27">V s = inf P∈ E P t=0 t r s t d t h t s t+1 (<label>19</label></formula><formula xml:id="formula_28">)</formula><p>where ∈ 0 1 is the discount factor. It is clear that for all policies , sup s∈ V s ≤ R/ 1 -. The optimal reward in state s is given by</p><formula xml:id="formula_29">V * s = sup ∈ V s = sup ∈ inf P∈ E P t=0 t r s t d t h t s t+1 (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>where is the set of all history-dependent randomized policies. The optimistic value function V * can be defined as follows.  <ref type="bibr" target="#b20">[20]</ref>.</p><formula xml:id="formula_31">V * s = sup ∈ sup P∈ E P t=0 t r</formula><p>Let V denote the set of all bounded real-valued functions on the discrete set . Let V denote the L norm on V; i.e.,</p><formula xml:id="formula_32">V = max s∈ V s</formula><p>Then V • is a Banach space. Let be any subset of all deterministic Markov decision rules. Define the robust Bellman operator on V as follows: For all V ∈ V,</p><formula xml:id="formula_33">V s = sup d∈ inf p∈ s d s E p r s d s s + V s s ∈ (<label>22</label></formula><formula xml:id="formula_34">)</formula><p>Theorem 3.2 (Bellman Equation). The operator satisfies the following properties:</p><p>(a) The operator is a contraction mapping on V; in particular, for all U V ∈ V,</p><formula xml:id="formula_35">U -V ≤ U -V (23) (b) The operator equation V = V has a unique solution. Moreover, V s = sup d t ∈ inf P∈ E P t=0 t r s t d t h t s t+1</formula><p>where is defined in <ref type="bibr" target="#b2">(3)</ref>.</p><p>Proof. Let U V ∈ V. Fix s ∈ , and assume that U s ≥ V s . Fix &gt; 0 and choose d ∈ such that for all s ∈ , inf p∈ s d s</p><formula xml:id="formula_36">E p r s d s s + U s ≥ U s - Choose a conditional probability measure p s ∈ s d s , s ∈ , such that E p s r s d s s + V s ≤ inf p∈ s d s E p r s d s s + V s + Then 0 ≤ U s -V s ≤ inf p∈ s d s E p r s d s s + U s + - inf p∈ s d s E p r s d s s + V s ≤ E p s r s d s s + U s + -E p s r s d s s + V s - = E p s U -V + 2 ≤ E p s U -V + 2 ≤ U -V + 2</formula><p>Repeating the argument for the case U s ≤ V s implies that</p><formula xml:id="formula_37">U s -V s ≤ U -V + 2 ∀ s ∈ i.e., U -V ≤ U -V + 2 .</formula><p>Since was arbitrary, this establishes part (a) of the theorem.</p><p>Since is a contraction operator on a Banach space, the Banach fixed point theorem implies that the operator equation V = V has a unique solution V ∈ V. Fix such that d t ∈ , for all t ≥ 0. Then</p><formula xml:id="formula_38">V s = V s ≥ inf p 0 ∈ s d 0 s E p 0 r s d 0 s s 1 + V s 1<label>(24)</label></formula><p>≥ inf</p><formula xml:id="formula_39">p 0 ∈ s d 0 s E p 0 r s d 0 s s 1 + inf p 1 ∈ s 1 d 1 s 1 E p 1 r s 1 d 1 s 1 s 2 + V s 2 (25) = inf P∈ E P 1 t=0 r s t d t s t s t+1 + 2 V s t+1 (<label>26</label></formula><formula xml:id="formula_40">)</formula><p>where <ref type="bibr" target="#b24">(24)</ref> follows from the fact that choosing a particular action d 0 s can only lower the value of the right-hand side; <ref type="bibr" target="#b25">(25)</ref> follows by iterating the same argument once more; and (26) follows from the rectangularity assumption. Thus, for all n ≥ 0,</p><formula xml:id="formula_41">V s ≥ inf P∈ E P n t=0 r s t d t s t s t+1 + n+1 V s t+1 = inf P∈ E P t=0 r s t d t s t s t+1 + n+1 V s t+1 - t=n+1 t r s t d t s t s t+1 ≥ V s -n+1 V - n+1 R 1 -</formula><p>where R = sup s s ∈ a∈ s r s a s &lt; . Since n is arbitrary, it follows that</p><formula xml:id="formula_42">V s ≥ sup d t ∈ ∀ t V s<label>(27)</label></formula><p>Fix &gt; 0 and choose a deterministic decision rule d ∈ such that for all s ∈ ,</p><formula xml:id="formula_43">V s = V s ≤ inf p∈ s d s E p r s d s s + V s +</formula><p>Consider the policy = d d . An argument similar to the one above establishes that for all n ≥ 0,</p><formula xml:id="formula_44">V s ≤ V s + n V + 1 -<label>(28)</label></formula><p>Since and n are arbitrary, it follows from ( <ref type="formula" target="#formula_42">27</ref>) and (28) that V s = sup d t ∈ ∀ t V s . Moreover, for all &gt; 0, there exists an -optimal stationary policy; i.e., there exists</p><formula xml:id="formula_45">= d d such that V ≥ V * -.</formula><p>Proof. The results follow by setting = d and = s∈ s respectively. Theorem 3.1 and part (b) of Corollary 3.1 for the special case of finite state and action spaces appears in Satia <ref type="bibr" target="#b21">[21]</ref> and Satia and Lave <ref type="bibr" target="#b22">[22]</ref> with an additional assumption that the set of conditional measures The Robust Value Iteration Algorithm:</p><formula xml:id="formula_46">Input: V ∈ V, &gt; 0 Output: V such that V -V * ≤ e/2</formula><p>For each s ∈ , set V s = sup a∈ s inf p∈ s a E p r r a s + V s .</p><formula xml:id="formula_47">while V -V ≥ 1 -/ 4 • do V = V ∀ s ∈ , set V s = sup a∈ s inf p∈ s a E p r r a s + V s . end while return V Figure 1. Robust value iteration algorithm.</formula><p>s a .) Also, they do not explicitly prove that the solution of (30) is indeed the robust value function. Theorem 3.2 for general , and in particular for = d , is new. The special case = d is crucial for establishing the policy improvement algorithm.</p><p>From Theorem 3.2, Corollary 3.1, and convergence results for contraction operators on Banach spaces, it follows that the robust value iteration algorithm displayed in Figure <ref type="figure">1</ref> computes an -optimal policy. This algorithm is the robust analog of the value iteration algorithm for nonrobust DPs (see §6.3.2 in Puterman <ref type="bibr" target="#b20">[20]</ref> for details). The following lemma establishes this approximation result for the robust value iteration algorithm. Lemma 3.1. Let V be the output of the robust value iteration algorithm shown in Figure <ref type="figure">1</ref>. Then</p><formula xml:id="formula_48">V -V * ≤ 4</formula><p>where V * is the optimal value defined in <ref type="bibr" target="#b20">(20)</ref>. Let d be the decision rule</p><formula xml:id="formula_49">inf p∈ s d s E p r s d s s + V s ≥ sup a∈ s inf p∈ s a E p r s a s + V s -<label>2</label></formula><p>Then, the policy = d d is -optimal. Moreover, the overall complexity of computing the policy is C S log R/ / log 1/ , where C is cost of computing sup a∈ s inf p∈ s a E p r r a s + V s for a fixed s ∈ .</p><p>Proof. Since Lemma 3.2 establishes that D is a contraction operator, this result is a simple extension of Theorem 6.3.1 in Puterman <ref type="bibr" target="#b20">[20]</ref>. The details are left to the reader.</p><p>Suppose the action set s is finite. Then robust value iteration reduces to</p><formula xml:id="formula_50">V s = max a∈ s inf p∈ n s a E p r s a s + V n+1 s</formula><p>For this iteration to be efficient, one must be able to efficiently solve the optimization problem inf p∈ s a E p v for a specified s ∈ , a ∈ s , and v ∈ R . These optimization problems are identical to those solved in finite state problems. In §4 we show that for suitable choices for the set s a of conditional measures the complexity of solving such problems is only modestly larger than evaluating E p v for a fixed p.</p><p>We next present a policy iteration approach for computing V * . As a first step, Lemma 3.2 below establishes that policy evaluation is a robust optimization problem. Proof. The constraint in (31) can be restated as V ≤ d V , where d = with = d . Corollary 3.1 implies that V = d V ; i.e., V is feasible for (31). Therefore, the optimal value of (31) is at least s∈ s V s . For every s ∈ , choose p s ∈ s d s such that</p><formula xml:id="formula_51">V s = d V s ≥ E p s r s d s s + V s - Then for any V feasible for (31), V s -V s ≤ E p s r s d s s + V s -E p s r s d s s + V s - = E p s V s -V s +</formula><p>Iterating this argument for n time steps, we get the bound</p><formula xml:id="formula_52">V s -V s ≤ n V -V + 1 -</formula><p>Since n and are arbitrary, all V feasible for (31) satisfy V ≤ V . Since s &gt; 0, s ∈ , it follows that the value of (31) is at most s∈ s V s . This establishes the result. Since E p r s + V is a linear function of p, (31) is a convex optimization problem. Typically, (31) can be solved efficiently only if is finite and the robust constraint can be reformulated as a small collection of deterministic constraints. In §4 we discuss some natural candidates for the set s a of conditional measures. Dualizing the constraints in (31) leads to a compact representation for some of these sets. However, for most practical applications, the policy evaluation step is computationally expensive and is usually replaced by an m-step look-ahead value iteration (Puterman <ref type="bibr" target="#b20">[20]</ref>).</p><p>Lemma 3.1 leads to the robust policy iteration algorithm displayed in Figure <ref type="figure" target="#fig_5">2</ref>. Suppose (31) is efficiently solvable; then finite convergence of this algorithm for the special case of finite state and action spaces follows from Theorem 6.4.2 in Puterman <ref type="bibr" target="#b20">[20]</ref>. A rudimentary version of robust policy iteration algorithm for this special case appears in Satia and Lave <ref type="bibr" target="#b22">[22]</ref> (see also Satia <ref type="bibr" target="#b21">[21]</ref>). They compute the value of a policy = d d , i.e., solve the robust optimization problem (31), via the following iterative procedure:</p><p>(a) For every s ∈ , fix p s ∈ s d s . Solve the set of equations</p><formula xml:id="formula_53">V s = E p s r s d s s + V s s ∈</formula><p>Since &lt; 1, this set of equations has a unique solution. See Theorem 6.1.1 in Puterman <ref type="bibr" target="#b20">[20]</ref>.</p><p>The Robust Policy Iteration Algorithm:  If V s = E ps r s d s s + V s , for all s ∈ , stop; otherwise, p s ← p s , s ∈ , return to (a). However, it is not clear, and Satia and Lave <ref type="bibr" target="#b22">[22]</ref> do not show, that this iterative procedure converges.</p><formula xml:id="formula_54">Input: decision rule d 0 , &gt; 0 Output: -optimal decision rule d * Set n = 0 and n = d n d n . Solve (31) to compute V n . Set V ← V n , = s∈ s For each s ∈ , choose d n+1 s ∈ a ∈ s inf p∈ s a E p r s a s + V s ≥ V s - setting d n+1 s = d n s if possible. while d n+1 = d n do n = n + 1; Solve (31) to compute V n . Set V ← V n , = s∈ s For each s ∈ , choose</formula><p>Given the relative ease with which value iteration and policy iteration translate to the robust setting, one might attempt to solve the robust DP by the following natural analog of the linear programming method for DP (Puterman <ref type="bibr" target="#b20">[20]</ref>):</p><formula xml:id="formula_55">maximize s∈ s V s subject to V s ≥ inf p∈ s a E p r s a s + V s a ∈ s s ∈<label>(32)</label></formula><p>Unfortunately, (32) is not a convex optimization problem. Hence, the LP method does not appear to have a tractable analog in the robust setting.</p><p>Recall that in the beginning of this section we had proposed two models for the adversary. The first was a dynamic model where the measures consistent with a policy satisfies rectangularity. So far we have assumed that this model prevails. In the second, static model, the adversary was restricted to employing a fixed p sa ∈ s a whenever the state-action pair s a is encountered. The last result in this section establishes that if the decision maker is restricted to stationary policies the implications of the static and dynamic models are, in fact, identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.3 (Dynamic vs. Static Adversary). Let d be any decision rule and let = d d</head><p>be the corresponding stationary policy. Let V and V be the value of the in the dynamic and static model respectively. Then V = V .</p><p>Proof. We prove the result for deterministic decision rules. The same technique extends to randomized policies but the notation becomes complicated.</p><p>Clearly V ≥ V . Thus, we only need to establish that V ≤ V . Fix &gt; 0 and choose p → such that ps ∈ s d s for all s ∈ , and V s ≥ E ps r s d s s + V s -Let V p denote the nonrobust value of the policy corresponding to the fixed conditional measure p. Clearly V p ≥ V . Thus, the result will follow if we show that V p ≤ V .</p><p>From results for nonrobust DP we have that V p = E ps r s d s s + V p s . Therefore,</p><formula xml:id="formula_56">V p -V s ≤ E p s r s a s + V p s -E p s r s d s s + V s - = E p s V s -V s +</formula><p>Iterating this bound for n time steps, we get</p><formula xml:id="formula_57">V p s -V s ≤ n V p -V + 1 -</formula><p>Since n and are arbitrary, it follows that V p ≤ V .</p><p>In the proof of the result we have implicitly established that the "best-response" of dynamic adversary when the decision maker employs a stationary policy is, in fact, static; i.e., the adversary chooses the same p sa ∈ s a every time the pair s a is encountered. Consequently, the optimal stationary policy in a static model can be computed by solving (30). Bagnell et al. <ref type="bibr" target="#b1">[2]</ref> establish that when the set s a of conditional measures is convex and the decision maker is restricted to stationary policies, the optimal policies for Downloaded from informs.org by [129.93. <ref type="bibr" target="#b15">16</ref> the decision maker and the adversary are the same in both the static and dynamic models. We extend this result to nonconvex sets. In addition we show that the value of any stationary policy, optimal or otherwise, is the same in both models. While solving (30) is, in general, NP-complete (Littman <ref type="bibr" target="#b15">[16]</ref>), the problem is tractable provided the sets s a are "nice" convex sets. In particular, the problem is tractable for the families of sets discussed in §4.</p><p>Lemma 3.3 highlights an interesting asymmetry between the decision maker and the adversary that is a consequence of the fact that the adversary plays second. While it is optimal for a dynamic adversary to play static (stationary) policies when the decision maker is restricted to stationary policies, it is not optimal for the decision maker to play stationary policies against a static adversary. The optimal policy for the decision maker in the static model are the so-called universal policy <ref type="bibr">(Cover [6]</ref>).</p><p>4. Computational complexity. Sections 2 and 3 were devoted to extending results from nonrobust DP theory. In this section we focus on computational issues. Since computations are only possible when state and action spaces are finite (or are suitably truncated versions of infinite sets), we restrict ourselves to this special case. Although independently obtained, most of the results in this section are not new; they first appeared in Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> (see also <ref type="bibr">Nilim and El Ghaoui [18]</ref>). We include them here for completeness.</p><p>In the absence of any ambiguity, the value of an action a ∈ s in state s ∈ is given by E p v = p T v, where p is the conditional measure and v is a random variable that takes value v s = r s a s + V s in state s ∈ . Thus, the complexity of evaluating the value of a state-action pair is . When the conditional measure is ambiguous, the value of the state-action pair s a is given by inf p∈ s a E p v . In this section, we discuss three families of sets of conditional measures s a which only result in a modest increase in complexity, typically logarithmic in . These families of sets are constructed from approximations of the confidence regions associated with density estimation. Note that since sup p∈ s a E p v = -inf p∈ s a E p -v , it follows that the recursion <ref type="bibr" target="#b18">(18)</ref> for the optimistic value function can also be computed efficiently for these families of sets. Finally, we discuss the issue of error propagation in the Bellman equation when the inner optimization problem is solved approximately.</p><p>As mentioned in the introduction, the motivation for the robust methodology was to systematically correct for the statistical errors associated with estimating the transition probabilities using historical data. Thus, a natural choice for the sets s a of conditional measures are the confidence regions associated with density estimation. In this section, we show how to construct such sets for any desired confidence level ∈ 0 1 . We refer the reader to Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> for the proof of the assertion that inf p∈ s a E p v can be efficiently solved for this class of sets.</p><p>First consider the case where the underlying Markov chain is stationary. Suppose we have historical data consisting of triples = s j a j s j j ≥ 1 , with the interpretation that state s j was observed in period t + 1 when the action a j was employed in state s j in period t. Then the maximum likelihood estimate psa of the conditional measure corresponding to the state-action pair s a is given by psa = arg max where D p 1 p 2 is the Kullback-Leibler or the relative entropy distance (see Chapter 2 in Cover and Thomas <ref type="bibr" target="#b6">[7]</ref>) between two measures p 1 p 2 ∈ and is defined as follows:</p><formula xml:id="formula_58">D p 1 p 2 = s∈ p 1 s log p 1 s p 2 s (<label>35</label></formula><formula xml:id="formula_59">)</formula><p>The function D p 1 p 2 ≥ 0 with equality if and only if p 1 = p 2 (however, D p 1 p 2 = D p 2 p 1 ). Thus, we have that the maximum likelihood estimate of the conditional measure is given by</p><formula xml:id="formula_60">psa s = q s = n s s a u∈ n u s a s s ∈ a ∈ s (36)</formula><p>More generally, let g j → R, j = 1 k be k functions defined on the state space (typically, g j s = s j , i.e., jth moment) and let</p><formula xml:id="formula_61">ḡj sa =</formula><p>1 n sa s∈ n s s a g j s j = 1 k be the sample averages of the moments corresponding to the state-action pair s a . Let p 0 sa ∈ be the prior distribution on conditioned on the state-action pair s a . Then the maximum likelihood solution psa is given by</p><formula xml:id="formula_62">psa = arg min p∈ E p g j = ḡj sa j=1 k D p p 0<label>(37)</label></formula><p>provided the set p ∈ E p g j = ḡj sa j = 1 k = . Let p sa , a ∈ s s ∈ denote the unknown true state transition of the stationary Markov chain. Then a standard result in statistical information theory (see Cover and Thomas <ref type="bibr" target="#b6">[7]</ref> for details) implies the following convergence in probability:</p><formula xml:id="formula_63">n sa D p sa psa =⇒ 1 2 2 S -1<label>(38)</label></formula><p>where n sa = s ∈ n s s a is the number of samples of the state-action pair s a and 2 S -1 denotes a 2 random variable with S -1 degrees of freedom (note that the maximum likelihood estimate psa is, itself, a function of the sample size n sa ). Therefore,</p><formula xml:id="formula_64">P p D p psa ≤ t ≈ P 2 S -1 ≤ 2n sa t = -1 2n sa t Let ∈ 0 1 and t = -1 -1 / 2n sa . Then = p ∈ D p psa ≤ t (39)</formula><p>is the -confidence set for the true transition probability p sa . Since D p q is a convex function of the pair p q (Cover and Thomas <ref type="bibr" target="#b6">[7]</ref>), is convex for all t ≥ 0. Next, we handle the case of time-varying uncertainty sets t s a introduced in §2. Since the uncertainty in this case is more refined, the data required to estimate the uncertainty structure needs to be more refined. We need data of the form = t j s j a j s j j ≥ 1 with the interpretation that state s j was observed in period t j + 1 when the action a j was employed in state s j in period t j . The first step is to "slice" the data according the period t to obtain t = t j s j a j s j t j = t , t ∈ T . Next, the data t is used to estimate t s a using the technique detailed above. Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</p><p>Relative entropy-based uncertainty sets were first introduced in Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>. We provide an information-theoretic rationale for employing these sets. The following result establishes that an -approximate solution for the robust problem corresponding to the set in (39) can be computed efficiently. </p><formula xml:id="formula_65">minimize E p v subject to p ∈ = p ∈ D p q ≤ t q ∈ (<label>40</label></formula><formula xml:id="formula_66">)</formula><p>where t &gt; 0, is equal to</p><formula xml:id="formula_67">-min ≥0 t + log E q exp - v (<label>41</label></formula><formula xml:id="formula_68">)</formula><p>The complexity of computing an -optimal solution for ( <ref type="formula" target="#formula_67">41</ref>) is S log 2 v max t t + log q min /2 t , where v = max s∈ v s -min s∈ v s and q min = P v s = min v .</p><p>We refer the reader to Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> for a proof of this lemma. Since log 1 + x ≤ x for all x ∈ R, it follows that</p><formula xml:id="formula_69">D p q = s∈ p s log p s q s ≤ s∈cS p s • p s -q s q s = s∈ p s -q s 2 q s</formula><p>Thus, a conservative approximation for the uncertainty set defined in (39) is given by</p><formula xml:id="formula_70">= p ∈ s∈ p s -q s 2 q s ≤ t<label>(42)</label></formula><p>Sets of the form (42) were also introduced in Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>. Our new contribution is to show that this family of sets is a conservative approximation for the relative entropy-based sets. Next, we show that this approximation allows us to compute the exact value of the inner optimization problem.</p><p>Lemma 4.2. The value of the optimization problem:</p><formula xml:id="formula_71">minimize E p v subject to p ∈ = p ∈ s∈ p s -q s 2 q s ≤ t (43) is equal to max ≥0 E q v --tVar q v -<label>(44)</label></formula><p>and the complexity of ( <ref type="formula" target="#formula_71">44</ref>) is log .</p><p>Proof. Let y = p -q. Then p ∈ if and only if s y 2 s /q s ≤ t, s y s = 0 and y ≥ -q. Thus, the value of (43) is equal to Lagrangian duality implies that the value of (45) is equal to</p><formula xml:id="formula_72">E q v + min</formula><formula xml:id="formula_73">E q v + max ≥0 ≥0 min y s y 2 s /q s ≤t - s∈ s q s + s∈ y s v s --s = max ≥0 ∈R E q v --t s∈ q s v s -s -2 (46) = max ≥0 E q v --tVar q v -<label>(47)</label></formula><p>This establishes that the value of ( <ref type="formula">43</ref>) is equal to that of (44).</p><p>The optimum value of the inner minimization in ( <ref type="formula">46</ref>) is attained at</p><formula xml:id="formula_74">y * s = - tq s z s z s ∈ where z s = q s v s -s -E q v - , s ∈ . Let = s ∈ s &gt; 0 .</formula><p>Then complementary slackness conditions imply that y * s = -q s , for all s ∈ , or equivalently,</p><formula xml:id="formula_75">v s -s = z √ t + E q v -= ∀ s ∈<label>(48)</label></formula><p>i.e., v s -s is a constant for all s ∈ . Since the optimal value of ( <ref type="formula">43</ref>) is at least v min = min s∈ v s , it follows that ≥ v min . Suppose is known. Then the optimal * is given by</p><formula xml:id="formula_76">* s = v s - v s ≥ 0 otherwise<label>(49)</label></formula><p>Thus, dual optimization problem (44) reduces to solving for the optimal . To this end, let v k 1 ≤ k ≤ denote the values v s s ∈ arranged in increasing order-an log operation. Let q denote the sorted values of the measure q. Suppose ∈ vn vn+1 . Then</p><formula xml:id="formula_77">E q v -= a n + b n Var q v -= c n + b n 2 + a n + b n 2</formula><p>where a n = k≤n q k v k , b n = k&gt;n q k , and c n = k≤n q k v2 k . The dual objective f as a function of is</p><formula xml:id="formula_78">f = E q v --tVar q v - = a n + b n -t c n + b n 2 -a n + b n 2</formula><p>If is optimal, it must be that f = 0; i.e., is root of the quadratic equation</p><formula xml:id="formula_79">b 2 n c n + b n 2 -a n + b n 2 = t b n 1 -b n -a n 2<label>(50)</label></formula><p>Thus, the optimal can be computed by sequentially checking whether a root of (50) lies in vn vn+1 , n = 1 . Let a 0 = c 0 = 0 and b 0 = 1. Then</p><formula xml:id="formula_80">a n = a n-1 + q n v n b n = b n-1 -qn-1 c n = c n-1 + q n v2 n n = 1</formula><p>and computing the roots of (50) for a particular n is 1 operation. Hence, computing the optimal is operation, and the overall complexity of computing a solution of ( <ref type="formula" target="#formula_71">44</ref> Note that we compute the exact value (modulo finite precision errors) of the optimization problem (43). Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> show that the complexity of computing an -optimal solution for (43) is 1 5 log v max / . From Lemma 12.6.1 in Cover and Thomas <ref type="bibr" target="#b6">[7]</ref>, we have that</p><formula xml:id="formula_81">D p q ≥ 1 2 ln 2 p -q 2 1</formula><p>where p -q 1 is the 1 -distance between the measures p q ∈ . Thus, the set</p><formula xml:id="formula_82">= p p -q 1 ≤ 2 ln 2 t (51)</formula><p>is an outer approximation, i.e., relaxation, of the relative entropy uncertainty set (39). This approximation also allows us to compute the exact value of the inner optimization problem.</p><p>Lemma 4.3. The value of the optimization problem:</p><formula xml:id="formula_83">minimize E p v subject to p ∈ = p p -q 1 ≤ 2 ln 2 t (52)</formula><p>is equal to</p><formula xml:id="formula_84">E q v - 1 2 2 ln 2 t min ≥0 max s v s -s -min s v s -s (53)</formula><p>and the complexity of ( <ref type="formula">53</ref>) is log .</p><p>Proof. Let y s = p s -q s , s ∈ . Then p ∈ if and only if y 1 ≤ 2 ln 2 t, s∈ y s = 0, and y ≥ -q. Therefore, the value of (52) is equal to From Lagrangian duality we have that the value of this optimization problem is equal to </p><formula xml:id="formula_85">E q v + minimize</formula><formula xml:id="formula_86">E q v + max ≥0 ∈R min y y 1 ≤ √ 2 ln 2 t - s∈ s q s + s∈ y s v s -s - = E q v + max ≥0 ∈R - s∈ s q s -2 ln 2 t v --1 = max ≥0 E q v --<label>1</label></formula><formula xml:id="formula_87">f = E q v -- 1 2 2 ln 2 t max s v s -s -min s v s -s = k≤n q k v k + 1 2 2t ln 2 v1 + b n -2 ln 2 t</formula><p>where b n = k&gt;n qk . Since f is linear, the optimal is always obtained at the end points. Thus, the optimal value of is given by = min v n b n &lt; 2 ln 2 t</p><p>Since b 1 = 1 -q1 and b n = b n-1 -qn-1 , n ≥ 2, it follows that can be computed in time.</p><p>In §4 we have, so far, been concerned with solving or suitably approximating the value of the inner optimization problem. The decision maker is, on the other hand, interested in approximating the value function V s . In the finite horizon case, the inner optimization problem is solved for each n ∈ T and each state-action pair s t a t ∈ t × t . Hence, an approximation error in any stage n propagates all the way back to stage 0. In the infinite horizon setting, the approximation error propagates via the value iteration step. Consequently, in order to compute an -optimal solution for the value function, one has to carefully set the level of approximation for the inner optimization problem. We refer readers interested in this problem to Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref> where the authors provide a careful analysis of the error propagation.</p><p>5. Games with perfect information. In this section we show that robust DP with dynamic uncertainty is equivalent to zero-sum games with perfect information (Gillette <ref type="bibr" target="#b12">[13]</ref>, Altman et al. <ref type="bibr" target="#b0">[1]</ref>). We will restrict attention to finite horizon robust DP. We assume that there are N -1 decision epochs with the reward at epoch N given by the function r N . For simplicity of exposition, we assume that the state space and the action space are not functions of time. The extension to discounted infinite horizon games is straightforward.</p><p>Consider a two-person zero-sum stochastic game with a countable state space . Let ˜ and denote the metric spaces of actions for players 1 and 2 respectively. Let p • s a b ∈ denote the transition probability when in state s ∈ player 1 chooses action a ∈ ˜ s and player 2 chooses action b ∈ s . Let r t × ˜ × denote the reward function at time t. A stochastic game is said to have perfect information if the state space can be partitioned as = 1 ∪ 2 , 1 ∩ 2 = , such that the action set for player 1, ˜ s , is a singleton for all s ∈ 2 and the action set of player 2, s , is a singleton for all s ∈ 1 . A special case of games with perfect information are games where player 2 chooses the action b ∈ s after observing the action a ∈ ˜ s of player 1. Robust dynamic programming with dynamic model for uncertainty is a game of this form: Player 1 is the decision maker who chooses an action a ∈ s and player 2 is the adversary who chooses the state transition p sa ∈ s a after observing a ∈ s . Robust dynamic programming can be reduced to a game with perfect information as follows: where denotes an arbitrary singleton. (v) Define the state transition p t s s ã b as follows.</p><formula xml:id="formula_88">p t s s ã b =                1 s = s ∈ ã ∈ s b = s = s ã ∈ × t = 2n 0 ≤ n ≤ N -1 p sa s = s a ∈ × ã = b = p sa ∈ s a t = 2n + 1 0 ≤ n ≤ N -1 0 otherwise<label>(56)</label></formula><p>(vi) The reward function r t s ã b is defined as follows.</p><formula xml:id="formula_89">r t s ã b =        r t s ã s = s ∈ ã ∈ s b = t = 2n 0 ≤ n ≤ N -1 r N s t = 2N 0 otherwise<label>(57)</label></formula><p>We assume here that the reward function of the robust dynamic program only depends on the current state-action pair. The extension to reward functions of the form r s a s is straightforward. The game constructed by (i)-(vi) above proceeds as follows. In state s ∈ , the decision maker chooses an action a ∈ s and receives a reward r s a . The new state of the system is now s a ∈ × . In this state, the adversary chooses p sa ∈ s a that determines the new state s ∈ . This state transition does not yield any reward to the decision maker. Thus, the game constructed above is a sequential game and each decision-making stage in the robust DP corresponds to two sequential stages in the stochastic game. We will denote the stages corresponding to the nth robust DP stage by the pair 2n 2n + 1 . In order to define the value of this stochastic two-person game we need to make the space of actions convex by allowing randomized actions. In state s ∈ ⊂ a randomized action for Player 1 is given, as before, by a probability measure q ∈ s . In state s a ∈ × ⊂ a randomized action for Player 2, i.e., the adversary, is given by a measure p ∈ s a = conv s a , the convex hull of the set s a . Note that we have implicitly made the set of transition measures s a convex. Standard results in the theory of zero-sum games (see, e.g., Nowak <ref type="bibr" target="#b19">[19]</ref>) imply that the value functions V k k = 0 2N of the finite horizon zero-sum stochastic game constructed above is characterized by the recursion</p><formula xml:id="formula_90">V k s = max q∈ Ã s min p∈ s E qp r s a b + V k+1 s 0 ≤ k ≤ 2N -1 s ∈ V 2N s = r N s s ∈ ⊂<label>(58)</label></formula><p>where</p><formula xml:id="formula_91">E qp r s a b + V k+1 s = s s r s a b + V k+1 s p d s s a b q da p db</formula><p>From the dynamics described in (54)-(57) and the fact that the game is sequential, it follows that recursion (58) can be simplified as follows: From the construction of the stochastic game, we have that the value function V n s of the robust DP can be identified with V 2n s , i.e., V n s = V 2n s . Thus, we have that</p><formula xml:id="formula_92">V 2n s = max q∈ Ã s E q r s a b + min p∈conv s a E p V 2 n+1 n= 1 N -1 s ∈ V 2N s = r N s s ∈<label>(</label></formula><formula xml:id="formula_93">V n s = max q∈ Ã s E q r s a b + min p∈conv s a E p V n+1</formula><p>Since E p V n+1 is a linear function of the measure p, it follows that min p∈conv s a</p><formula xml:id="formula_94">E p V n+1 = min p∈ s a E p V n+1</formula><p>An argument similar to the one used to establish (15) in Theorem 2.1 establishes that the decision maker can be restricted to deterministic actions. Thus, we recover the robust Bellman recursion</p><formula xml:id="formula_95">V n s = max a∈ Ã s min p∈ s a E p r s a b + V n+1 6. Conclusion.</formula><p>In this paper we propose a robust formulation for the discrete time DP. This formulation attempts to mitigate the impact of errors in estimating the transition probabilities by choosing a maximin optimal policy, where the minimization is over a set of transition probabilities. This set summarizes the limited knowledge that the decision maker has around the transition probabilities of the underlying Markov chain. A natural family of sets describing the knowledge of the decision maker are the confidence regions about the maximum likelihood estimates of the transition probability. Since these confidence regions are described in terms of the relative entropy or the Kullback-Liebler distance, we are naturally led to the uncertainty sets discussed in §4. This family of sets was first introduced in Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>.</p><p>Since the transition probabilities are ambiguous, every policy now has a set of measures associated with it. We prove that when this set of measures satisfies a certain rectangularity property most of the important results in DP theory, such as the Bellman recursion, the optimality of deterministic Markov policies, the contraction property of the value iteration operator, etc., extend to natural robust counterparts. On the computational front, we show that the computational effort required to solve the robust DP corresponding to sets of conditional measures based on confidence regions is only modestly higher than that required to solve the nonrobust DP. Although independently obtained, these results were first obtained in Nilim and El Ghaoui <ref type="bibr" target="#b16">[17]</ref>. While parts of the theory presented in this paper have been addressed by other authors, we provide a unifying framework for the theory of robust DP.</p><p>The robust value function V * provides a lower bound on the achievable performance; one can also define an optimistic value function V * that provides an upper bound on the achievable performance. All the results in this paper imply corresponding results for the optimistic value function; i.e., in particular there is value iteration and a policy iteration algorithm that efficiently characterizes the optimistic value function.</p><p>As indicated in the introduction, we restricted our attention to problems where the nonrobust DP is tractable. In most of the interesting applications of DP, this is not the case and one has to resort to approximate DP. One would, therefore, be interested in developing the robust counterpart of approximate DP. Such an approach might be able to prevent instabilities observed in approximate DP (Bertsekas and Tsitsiklis <ref type="bibr" target="#b4">[5]</ref>).</p><p>Appendix A. Consequences of rectangularity. We will begin with an example that illustrates the inappropriateness of rectangularity in a finite horizon setting. This example is a dynamic version of the Ellsberg Urn problem (Ellsberg <ref type="bibr" target="#b8">[9]</ref>) discussed in Epstein and Schneider <ref type="bibr" target="#b9">[10]</ref>.</p><p>Suppose an urn contains 30 red balls and 60 balls that are either blue or green. At time 0 a ball is drawn from the urn and the color of the ball is revealed at time t = intermediate time t = 1 the decision maker is told whether the drawn ball is green. Thus, the state transition structure is as shown in Figure <ref type="figure" target="#fig_14">3</ref> where p b = P ball is blue . Suppose p b ∈ p p ⊆ 0 2/3 is ambiguous. Consider the robust optimal stopping problem where the state transition is given by Figure <ref type="figure" target="#fig_14">3</ref>. In each state s ∈ t at time t = 0 1, there are two actions s c available, where c denotes continue and s denotes stop. Let = d0 d1 denote the policy that chooses the deterministic action c in every state s ∈ t , t = 0 1. Then the state-transition structure in Figure <ref type="figure" target="#fig_14">3</ref> implies that the conditional measures consistent with the decision rules di , i = 0 1, are given by The problem arises because the information structure in Figure <ref type="figure" target="#fig_14">3</ref> assumes that there is a single urn that decides the conditional measures at both epochs t = 0 1; whereas, rectangularity demands that the conditional measures at epochs t = 0 1, be independent; i.e., in this case, they should be determined by an independent copy of the urn used at t = 0. Assuming that rectangularity holds in this setting is equivalent to assuming that apriori distribution on the composition of the urn is given by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>279</head><p>A very counterintuitive prior indeed! This example clearly shows that rectangularity may not always be an appropriate property to impose on an AMDP. In spite of the counterexample above, rectangularity is often appropriate for finite horizon AMDPs because the sources of the ambiguity in different periods are typically independent of each other.</p><p>Rectangularity implies that the adversary is able to choose a different conditional measure every time a state-action pair s a is encountered. This adversary model should not raise an alarm in a finite horizon setting where a state-action pair is never revisited. However, the situation is very different in an infinite horizon setting where a state-action can be revisited. In this setting rectangularity may not be appropriate when there is ambiguity but the transition probabilities are not dynamically changing. Deciding whether rectangularity is appropriate can often be a function of the time scale of events. Suppose one is interested in a robust analysis of network routing algorithms where the action in each node is the choice of the outgoing edge and the ambiguity is with respect to the delay on the network edges. For a traffic network the rectangularity assumption might be appropriate because the time elapsed in returning to a node is sufficiently long so that the parameters could have shifted. On the other hand, for data networks that operate at much higher speeds the ambiguity might be evolve on a slower time scale, and therefore, rectangularity might not be appropriate. On a positive note, Lemma 3.3 shows that the problems with rectangularity disappear if one restricts the decision maker to stationary policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 2 . 1 .</head><label>21</label><figDesc>Let D be the set of all history-dependent deterministic policies. Then D is adequate for characterizing the value function V n in the sense that for all n = 0 N -1, V * n h n = sup ∈ D V n h n Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Corollary 3 . 1 .</head><label>31</label><figDesc>The properties of the operator imply the following: (a) Let d be any deterministic decision rule. Then the value V of the stationary policy = d d is the unique solution of the operator equation V s = inf p∈ s d s E p r s d s s + V s s ∈ S (29) (b) The value function V * is the unique solution of the operator equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s a is convex. (Their proof, in fact, extends to nonconvex Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS 267</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 3 . 2 (</head><label>32</label><figDesc>Policy Evaluation). Let d be a deterministic decision rule and = d d be the corresponding stationary policy. Then V is the optimal solution of the robust optimization problem maximize s∈ s V s subject to V s ≤ E p r s + V ∀ p ∈ s d s s ∈ (31) where s &gt; 0, s ∈ , and r s ∈ with r s s = r s d s s , s ∈ . Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming 268 Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>d n+1 s ∈ a ∈ s inf p∈ s a E p r s a s + V s ≥ V ssetting d n+1 s = d n s if possible. end while return d n+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Robust policy iteration algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>s = s j a j s j is the number of samples of the triple s a s . Let q ∈ be defined asq s = n ss a u∈ n u s a s ∈ Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS 271 Then, (33) is equivalent to psa = arg min p∈ D q p (34)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 4 . 1 .</head><label>41</label><figDesc>The value of the optimization problem:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS 273</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>) is log . Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>s∈ y s v s subject to y 1 ≤ 2</head><label>12</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Let * be the optimal dual solution and let = max s∈ v s - * s . It is easy to see that * s = v s -v s &gt; 0 otherwise Thus, dual optimization problem (44) reduces to solving for the optimal . To this end, let v k 1 ≤ k ≤ denote the values v s s ∈ arranged in increasing order-an log operation. Let q denote the sorted values of the measure q. Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS 275 Suppose ∈ vn vn+1 . Then, the dual function f is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(i) Define = ∪ × . (ii) At the decision epoch t = 0, the state s ∈ ⊂ . (iii) For all s ∈ ⊂ , define ˜ s = s s = (54) where denotes an arbitrary singleton. Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS (iv) For all s a ∈ × ⊂ , define ˜ s a = for some</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>59) Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS 277</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Dynamic Ellsberg experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>3 1 3 -</head><label>33</label><figDesc>p r p b p g ∈ = 1 Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc><table><row><cell>Iyengar: Robust Dynamic Programming</cell></row><row><cell>Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Let t denote the set of all histories h t . Then a randomized decision rule d t is a map d t t → s t . A decision rule d t is called deterministic if it puts all the probability mass on a single action a ∈ s t , and Markovian if it is a function of the current state s t alone. Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc><table><row><cell></cell><cell cols="3">Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</cell></row><row><cell cols="4">The set of all conditional measures consistent with a deterministic Markov decision</cell></row><row><cell>rule d t is given by</cell><cell></cell><cell></cell><cell></cell></row><row><cell>d t = p t →</cell><cell>t+1</cell><cell>∀ s ∈ t p s ∈ t s d t s</cell><cell>(1)</cell></row></table><note><p><p><p><p>i.e., for every state s ∈ , the next state can be determined by any p ∈ t s d t s . The set of all conditional measures consistent with a history dependent decision rule d t is given by</p>d t = p t → s t × t+1 ∀ h ∈ t p h a s = q d t</p>h a p s t a s p s t a ∈ s t a a ∈ s t s ∈ t+1</p><ref type="bibr" target="#b1">(2)</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>sup Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc><table><row><cell>∈</cell><cell>sup P∈</cell><cell>E P</cell><cell>t∈T</cell><cell>r t s t d t h t s t+1 + r N s N</cell><cell>(7)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. of Operations Research 30(2), pp. 257-280, © 2005 INFORMS Let d n h n s n+1 denote any realization of the random action-state pair corresponding to the (randomized) decision rule d n . Then V n+1 h n d n h n</figDesc><table><row><cell>V  *  n+1 h n d n h n</cell><cell>s n+1</cell><cell>Mathematics s n+1</cell><cell>≤</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc><table><row><cell>Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Since s a only depends on the current state-action pair, this result follows from robust extensions of Theorem 5.5.1, Theorem 5.5.3, and Proposition 6.2.1 in Puterman</figDesc><table><row><cell>Iyengar: Robust Dynamic Programming Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</cell><cell>265</cell></row><row><cell>Proof.</cell><cell></cell></row><row><cell>s t d t h t s t+1</cell><cell>(21)</cell></row></table><note><p><p><p>As noted in Remark 2.1, all the results in this section imply a corresponding result for the optimistic value function V * with the inf P∈ • replaced by sup P∈ • .</p>The following result is the infinite horizon counterpart of Theorem 2.2.</p>Theorem 3.1 (Markov Optimality). The decision maker can be restricted to deterministic Markov policies without any loss in performance; i.e., V * s = sup ∈ MD V s , where MD is the set of all deterministic Markov policies.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</figDesc><table><row><cell>Mathematics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved.</figDesc><table><row><cell>Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="257" xml:id="foot_0"><p>Downloaded from informs.org by [129.93.16.3] on 22 September 2015, at 12:11 . For personal use only, all rights reserved. Iyengar: Robust Dynamic Programming</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Mathematics of Operations Research 30(2), pp. 257-280, © 2005 INFORMS</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The author thanks the anonymous referees for insightful comments that have substantially improved the presentation and the conceptual content of the paper. The author is grateful to Prof. Eugene A. Feinberg for bringing the connection between robust DP and stochastic games with perfect information to his attention. The author's research was partially supported by NSF Grants CCR-00-09972 and DMS-01-04282, as well as by ONR Grant N000140310514.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, At: 12:11 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted discounted games with perfect information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Dynamic Games and Applications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Filar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Gaitsgory</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mizukami</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhauser</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="303" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Solving uncertain Markov decision problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<idno>TR-01-25</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, CMU, CMU-RI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust truss topology design via semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="991" to="1016" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robust convex optimization. Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="769" to="805" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neuro-Dynamic Programming</title>
		<meeting><address><addrLine>Belmont, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal portfolios</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Finance</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The linear programming approach to approximate dynamic programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>De Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="850" to="865" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Risk, ambiguity and the Savage axioms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ellsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quart. J. Econom</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="669" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive multiple priors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://rcer.econ.rochester.edu" />
	</analytic>
	<monogr>
		<title level="j">J. Econom. Theory</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning under ambiguity</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<idno>497</idno>
		<ptr target="http://rcer.econ.rochester.edu" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maxmin expected utility with non-unique priors</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmeidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Econom</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="141" to="153" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic games with zero stop probabilities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gillette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dresher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</editor>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1957">1957</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust portfolio selection problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sargent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robust control and model uncertainty</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Memoryless policies: Theoretical limitations and practical results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Animals to Animats: SAB &apos;94</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Cliff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Husbands</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust solutions to Markov decision problems with uncertain transition matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nilim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Forthcoming</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<idno>129.93.16.3</idno>
	</analytic>
	<monogr>
		<title level="m">12:11 . For personal use only, all rights reserved</title>
		<imprint>
			<date type="published" when="2005">22 September 2015. 2005</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="257" to="280" />
		</imprint>
	</monogr>
	<note>Downloaded from informs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robustness in Markov decision problems with uncertain transition matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nilim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On zero-sum stochastic games with general state space</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. Probab. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="13" to="32" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Markovian decision process with uncertain transition matrices or/and probabilistic observation of states</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Satia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Markov decision processes with uncertain transition probabilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Satia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="728" to="740" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kleywegt</surname></persName>
		</author>
		<title level="m">Minimax analysis of stochastic problems. Optim. Methods and Software Forthcoming</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic catalog mailing policies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<idno>FRPS05-202</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Durham, NC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Fuqua School of Business, Duke University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Markov decision processes with imprecise transition probabilities</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Eldieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="739" to="749" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m">12:11 . For personal use only, all rights reserved</title>
		<imprint/>
	</monogr>
	<note>Downloaded from informs.org by [129.93.16.3] on 22 September 2015</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
