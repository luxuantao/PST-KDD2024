<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAPID: Rating Pictorial Aesthetics using Deep Learning *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<email>xinlu@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hailin</forename><surname>Jin</surname></persName>
							<email>hljin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
							<email>jiayang@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
							<email>jwang@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Information Sciences and Technology and Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RAPID: Rating Pictorial Aesthetics using Deep Learning *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7771417CC4129E9B23D6A90000434825</idno>
					<idno type="DOI">10.1145/2647868.2654927</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4.7 [Image Processing and Computer Vision]: Feature measurement</term>
					<term>I.4.10 [Image Processing and Computer Vision]: Image Representation</term>
					<term>I.5 [Pattern Recognition]: Classifier design and evaluation Algorithms, Experimentation Deep Learning</term>
					<term>Image Aesthetics</term>
					<term>Multi-Column Deep Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective visual features are essential for computational aesthetic quality rating systems. Existing methods used machine learning and statistical modeling techniques on handcrafted features or generic image descriptors. A recentlypublished large-scale dataset, the AVA dataset, has further empowered machine learning based approaches. We present the RAPID (RAting PIctorial aesthetics using Deep learning) system, which adopts a novel deep neural network approach to enable automatic feature learning. The central idea is to incorporate heterogeneous inputs generated from the image, which include a global view and a local view, and to unify the feature learning and classifier training using a double-column deep convolutional neural network. In addition, we utilize the style attributes of images to help improve the aesthetic quality categorization accuracy. Experimental results show that our approach significantly outperforms the state of the art on the AVA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automated assessment or rating of pictorial aesthetics has many applications. In an image retrieval system, the ranking algorithm can incorporate aesthetic quality as one of the factors. In picture editing software, aesthetics can be used in producing appealing polished photographs. Datta et al. <ref type="bibr" target="#b6">[6]</ref> and Ke et al. <ref type="bibr" target="#b13">[13]</ref> formulated the problem as a classification or regression problem where a given image is mapped to an aesthetic rating, which is normally quantized with discrete values. Under this framework, the effectiveness of the image representation, or the extracted features, can often be the accuracy bottleneck. Various handcrafted aestheticsrelevant features have been proposed <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>, including low-level image statistics such as distributions of edges and color histograms, and high-level photographic rules such as the rule of thirds.</p><p>While these handcrafted aesthetics features are often inspired from the photography or psychology literature, they share some known limitations. First, the aesthetics-sensitive attributes are manually designed, hence have limited scope. It is possible that some effective attributes have not yet been discovered through this process. Second, because of the vagueness of certain photographic or psychologic rules and the difficulty in implementing them computationally, these handcrafted features are often merely approximations of such rules. There is often a lack of principled approach to improve the effectiveness of such features.</p><p>Generic image features <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b22">22]</ref> are proposed to address the limitations of the handcrafted aesthetics features. They used well-designed common image features such as SIFT and Fisher Vector <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, which have been successfully used for object classification tasks. The generic image features have been shown to outperform the handcrafted aesthetics features <ref type="bibr" target="#b23">[23]</ref>. However, because these features are meant to be generic, they may be unable to attain the upper performance limits in aesthetics-related problems.</p><p>In this work, we intend to explore beyond generic image features by learning effective aesthetics features from images directly. We are motivated by the recent work in large scale image classification using deep convolutional neural networks <ref type="bibr" target="#b15">[15]</ref> where the features are automatically learned from RGB images. The deep convolutional neural network takes pixels as inputs and learns a suitable representation through multiple convolutional and fully connected layers. However, the originally proposed architecture cannot be directly applied to our task. Image aesthetics relies on a combination of local and global visual cues. For example, the rule of thirds is a global image cue while sharpness and noise  levels are local visual characteristics. Given an image, we generate two heterogeneous inputs to represent its global cues and local cues respectively. Figure <ref type="figure" target="#fig_1">1</ref> illustrates global vs. local views. To support network training on heterogeneous inputs, we extend the method in <ref type="bibr" target="#b15">[15]</ref> by developing a double-column neural network structure which takes parallel inputs from the two columns. One column takes a global view of the image and the other column takes a local view of the image. We integrate the two columns after some layers of transformations to form the final classifier. We further improve the aesthetic quality categorization by exploring style attributes associated with images. We named our system RAPID, which stands for RAting PIctorial aesthetics using Deep learning. We used a recently-released large dataset to show the advantages of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Earlier visual aesthetics assessment research focused on examining handcrafted visual features based on common cues such as color <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>, texture <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b13">13]</ref>, composition <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b7">7]</ref>, and content <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b7">7]</ref>, as well as generic image descriptors <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b24">24]</ref>. Commonly investigated color features include lightness, colorfulness, color harmony, and color distribution <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>. Texture descriptors vary from waveletbased texture features <ref type="bibr" target="#b6">[6]</ref>, distribution of edges, to blur descriptors and shallow depth-of-field descriptors <ref type="bibr" target="#b13">[13]</ref>. Composition features typically include the rule of thirds, size and aspect ratio <ref type="bibr" target="#b20">[20]</ref>, and foreground and background composition <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b7">7]</ref>. There have been attempts to represent the content of images using people and portrait descriptors <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b7">7]</ref>, scene descriptors <ref type="bibr" target="#b7">[7]</ref>, and generic image features such as SIFT <ref type="bibr" target="#b18">[18]</ref>, GIST <ref type="bibr" target="#b28">[28]</ref>, and Fisher Vector <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Despite the success of handcrafted and generic visual features, the usefulness of automatically learned features have been demonstrated in many vision applications <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b30">30]</ref>. Recently, trained deep neural networks are used to build and associate mid-level features with class labels. Convolutional neural network (CNN) <ref type="bibr" target="#b16">[16]</ref> is one of the most powerful learning architectures among the various types of neural networks (e.g., Deep Belief Net <ref type="bibr" target="#b10">[10]</ref> and Restricted Boltzmann Machine <ref type="bibr" target="#b9">[9]</ref>). Krizhevsky et al. <ref type="bibr" target="#b15">[15]</ref> significantly advanced the 1000-class classification task in ImageNet challenge with a deep architecture of CNN in conjunction with dropout and normalization techniques, Sermanet et al. <ref type="bibr" target="#b30">[30]</ref> achieved the-state-of-the-art performance on all major pedestrian detection datasets, and Ciresan et al. <ref type="bibr" target="#b4">[4]</ref> reached a near-human performance on the MNIST<ref type="foot" target="#foot_0">1</ref> dataset.</p><p>The effectiveness of CNN features has also been demonstrated in image style classification <ref type="bibr" target="#b12">[12]</ref>. Without training deep neural network, Karayev et al. extracted existing Decaf features <ref type="bibr" target="#b8">[8]</ref> and used those features as input for style classification. There are key differences between that work <ref type="bibr" target="#b12">[12]</ref> and ours. First, they mainly targeted style classification whereas we focus on aesthetic categorization, which is a different problem. Second, they used existing features as input to classification and did not train specific neural networks for style or aesthetics categorization. In contrast, we train deep neural networks directly from RGB inputs, which are optimized for the given task. Third, they relied on features from global views, while we leverage heterogeneous input sources, i.e., global and local views, and propose doublecolumn neural networks to learn features jointly from both sources. Finally, we propose a regularized neural network based on related attributes to further boost aesthetics categorization.</p><p>As designing handcrafted features has been widely considered an appropriate approach in assessing image aesthetics, insufficient effort has been devoted to automatic feature learning on a large collection of labeled ground-truth data. The recently-developed AVA dataset <ref type="bibr" target="#b24">[24]</ref> contains 250, 000 images with aesthetic ratings and a 14, 000 subset with style labels (e.g., rule of thirds, motion blur, and complementary colors), making automatic feature learning using deep learning approaches possible.  <ref type="bibr" target="#b15">[15]</ref>.</p><p>In this work, we train deep neural networks on the AVA dataset to categorize image aesthetic quality. Specifically, we propose a double-column CNN architecture to automatically discover effective features that capture image aesthetics from two heterogeneous input sources. The proposed architecture is different from the recent work in multi-column neural networks <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b1">1]</ref>. Agostinelli et al. <ref type="bibr" target="#b1">[1]</ref> extended stacked sparse autoencoder to a multi-column version by computing the optimal column weights and applied the model to image denoising. Ciresan et al. <ref type="bibr" target="#b4">[4]</ref> averaged the output of several columns trained on inputs with different standard preprocessing methods. Our architecture is different from that work because the two columns in our architecture are jointly trained using two different inputs: The first column of the network takes global image representation as the input, while the second column takes local image representations as the input. This allows us to leverage both compositional and local visual information.</p><p>The problem of assessing image aesthetics is also relevant to recent work of image popularity estimation <ref type="bibr" target="#b14">[14]</ref>. Aesthetic value is connected with the notion of popularity, while there is a fundamental difference between the two concepts. Aesthetics concerns primarily with the nature and appreciation of beauty, while in the measurement of popularity both aesthetics and how interesting the visual stimulus is to the viewer population are important. For instance, a photograph of some thought-provoking subject may not be considered of high aesthetic value, but can be appreciated by many people based on the subject alone. On the other hand, a beautiful picture of flowers may not be able to reach the state of popularity if the viewers don't consider the subject of sufficient interestingness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>Our main contributions are as follows.</p><p>• We conducted systematic evaluation of the single-column deep convolutional neural network approach with different types of input modalities for aesthetic quality categorization;</p><p>• We developed a double-column deep convolutional neural network architecture to jointly learn features from heterogeneous inputs;</p><p>• We developed a regularized double-column deep convolutional neural network to further improve aesthetic categorization using style attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE ALGORITHM</head><p>Patterns in aesthetically-pleasing photographs often indicate photographers' visual preferences. Among those patterns, composition <ref type="bibr" target="#b17">[17]</ref> and visual balance <ref type="bibr" target="#b25">[25]</ref> are important factors <ref type="bibr" target="#b2">[2]</ref>. They are reflected in the global view (e.g., top row in Figure <ref type="figure" target="#fig_1">1</ref>) and the local view (e.g., bottom row in the Figure ). Popular composition principles include the rule of thirds, diagonal lines, and golden ratio <ref type="bibr" target="#b11">[11]</ref>, while visual balance is affected by position, form, size, tone, color, brightness, contrast, and proximity to the fulcrum <ref type="bibr" target="#b25">[25]</ref>. Some of these patterns are not well-defined or even abstract, making it difficult to calculate those features for assessing image aesthetic quality. Motivated by this, we aim to leverage the power of CNN to automatically identify useful patterns and employ learned visual features to rate or to categorize the aesthetic quality of images.</p><p>However, applying CNN to the aesthetic quality categorization task is not straightforward. The different aspect ratios and resolutions in photographs and the importance of image details in aesthetics make it difficult to directly train CNN where inputs are typically normalized to the same size and aspect ratio. A challenging question, therefore, is to perform automatic feature learning with regard to both the global and the local views of the input images. To address this challenge, we take several different representations of an image, i.e., the global and the local views of the image, which can be encoded by jointly considering those heterogeneous representations. We first use each of the representations to train a single-column CNN (SCNN) to assess image aesthetics. We further developed a double-column CNN (DCNN) to allow our model to use the heterogeneous inputs from one image, aiming at identifying visual features in terms of both global and local views. Finally, we investigate how the style of images can be leveraged to boost aesthetic classification accuracy <ref type="bibr" target="#b29">[29]</ref>. We present an aesthetic quality categorization approach with style attributes by learning a regularized double-column network (RDCNN), a three-column network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-column Convolutional Neural Network</head><p>Deep convolutional neural network <ref type="bibr" target="#b15">[15]</ref> takes inputs of fixed aspect ratio and size. However, an input image can be of arbitrary size and aspect ratio. To normalize image sizes, we propose three different transformations: center-crop (gc), warp (gw), and padding (gp), which reflect the global view (Ig) of an image I. gc isotropically resizes original images by normalizing their shorter sides to a fixed length s. Centercrop normalizes the input to generate a s × s × 3 input. gc was adopted in a recent image classification work <ref type="bibr" target="#b15">[15]</ref>. gw anisotropically resizes (or warps) the original image into a normalized input with a fixed size s × s × 3. gp resizes the original image by normalizing the longer side of the image to a fixed length s and padding border pixels with zeros to generate a normalized input of a fixed size s × s × 3. For each image I and each type of transformation, we generate an s × s × 3 input I j g with the transformation gj, where j ∈ {c, w, p}. As resizing inputs can cause harmful information loss (i.e., the high-resolution local views) for aesthetic assessment, we also use randomly sampled fixed size Global"View" Fine:grained"View" </p><formula xml:id="formula_0">l(W) = N i=1 c∈C I(yi = c) log p(yi = c | xi, wc) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where N is the number of images, W = {wc}c∈C is the set of model parameters, and I(x) = 1 iff x is true and vice versa.</p><p>The probability p(yi = c | xi, wc) is expressed as</p><formula xml:id="formula_2">p(yi = c | xi, wc) = exp (w T c xi) c ∈C exp (w T c xi) .<label>(2)</label></formula><p>The aesthetic quality categorization task can be defined as a binary classification problem where each input patch is associated with an aesthetic label c ∈ C = {0, 1}. In Section 2.3, we explain a SCNN for image style categorization, which can be considered a multi-class classification task.</p><p>As indicated by the previous study <ref type="bibr" target="#b15">[15]</ref>, the architecture of the deep neural network may critically affect the performance. Our experiments suggest that the general guideline for training a good-performing network is to first allow sufficient learning power of the network by using sufficient number of neurons. Meanwhile, we adjust the number of convolutional layers and the fully-connected layers to support the feature learning and classifier training. In particular, we extensively evaluate the network trained with different numbers of convolutional layers and fully-connected layers, and with or without normalization layers. Candidate architectures are shown in Table <ref type="table" target="#tab_2">1</ref>. To determine the optimal architecture for our task, we conduct experiments on candidate architectures and pick the one with the highest performance, as shown in Figure <ref type="figure">2</ref>.</p><p>With the selected architecture, we train SCNN with four different types of inputs (I c g , I w g , I p g , I r l ) using the AVA dataset <ref type="bibr" target="#b24">[24]</ref>. During training, we handle the overfitting problem by adopting dropout and shuffling the training data in each epoch. Specifically, we found that lr serves as an effective data augmentation approach which alleviates overfitting. Because I r l is generated by random cropping, an image contributes to the network training with different inputs when a different patch is used.</p><p>We experimentally evaluate the performance of these inputs with SCNN. Results will be presented in Section 3. I w g performs the best among the three global input variations (I c g , I w g , I p g ). I r l yields an even better results compared with I w g . Hence, we use I r l and I w g as the two inputs to train the proposed double-column network. In our experiments, we fix the dropout rate as 0.5 and initiate the learning rate with 0.001. Given a test image, we compute its normalized input and followed by generating the input patch, with which we calculate the probability of the input patch being assigned to each aesthetic category. We repeat this process for 50 times, average those results, and pick the class with the highest probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Double-column Convolutional Neural Network</head><p>For each image, its global or local information may be lost when transformed to a normalized input using gc, gw, gp, or lr. Representing an image through multiple inputs can somewhat alleviate the problem. As a first attempt, we generate one input to depict the global view of an image and another to represent its local view.</p><p>We propose a novel double-column convolutional neural network (DCNN) to support automatic feature learning with heterogeneous inputs, i.e., a global-view input and a localview input. We present the architecture of the DCNN in Figure <ref type="figure" target="#fig_2">3</ref>. As shown in the figure, networks in different columns are independent in convolutional layers and the first two fully-connected layers. The inputs of the two columns are I w g and I r l . We take the two 256 × 1 vectors from each of the fc256 layer and jointly train the weights of the final fully-connected layer. We avoid the interaction between two columns in convolutional layers because they are in different spatial scales. During training, the error is back propagated to the networks in each column respectively with stochastic gradient descent. With the proposed architecture, we can also automatically discover both the global and the local features of an image from the fc1000 layers and fc256 layers.</p><p>The proposed network architecture could easily be expanded to multi-column convolutional networks by incorporating more types of normalized inputs. DCNN allows different architectures in individual networks, which may facilitate the parameter learning for networks in different columns.</p><p>In our work, network architectures are the same for both columns. Given a test image, we perform a similar procedure as we do with SCNN to evaluate the aesthetic quality of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning and Categorization with Style Attributes</head><p>The discrete aesthetic labels, i.e., high quality and low quality, provided weak supervision to make the network converge properly due to the large intra-class variation. This motivates us to exploit extra labels from the training images to help identify their aesthetic characteristics. We propose to leverage style attributes, such as complementary colors, macro, motion blur, rule of thirds, shallow depth-of-field (DOF), to help determine the aesthetic quality of images because they are regarded as highly relevant attributes <ref type="bibr" target="#b24">[24]</ref>.</p><p>There are two natural ways to formulate the problem. The first is to leverage the idea of multi-task learning <ref type="bibr">[5]</ref>, which jointly construct feature representation and minimize the classification error for both labels. Assuming we have aesthetic quality labels {yai} and style labels {ysi} for all training images, the problem becomes an optimization prblem: where X is the features of all training images, CA is the label set for aesthetic quality, CS is the label set for style, and Wa = {wac}c∈C A and Ws = {wsc}c∈C S are the model parameters. It is more difficult to obtain images with style attributes. In the AVA benchmark, among 230, 000 image with aesthetic labels only 14, 000 of them have style labels. As a result, we cannot jointly perform aesthetics categorization and style classification with a single neural network due to many missing labels.</p><p>Alternatively, we can use ideas from inductive transfer learning <ref type="bibr" target="#b29">[29]</ref>, where we target minimizing the classification error with one label, whereas we construct feature representations with both labels. As we only have a subset of images with style labels, we first train a style classifier with them. We then extract style attributes for all training images, and applied those attributes to regularize the feature learning and classifier training for aesthetic quality categorization.</p><p>To learn style attributes for 230, 000 training images, we first train a style classifier by performing the training procedure discussed in Section 2.1 on 11, 000 labeled training images (Style-SCNN). We adopted the same architecture as shown in Figure <ref type="figure">2</ref>. The only difference is that we reduced the number of filters in the the first and fourth convolutional layers to a half due to the reduced number of training images. With Style-SCNN, we are maximizing the log likelihood function in Equation 1 where C is the set of style labels in the AVA dataset. We experimentally select the best architectures (to be shown in Table <ref type="table" target="#tab_5">4</ref>) and inputs (I c g , I w g , I p g , I r l ). The details are described in Section 3. Given an image, we apply the learned weights and extract the features from the fc256 layer as its style attribute.</p><p>To facilitate the network training with style attributes of images, we propose a regularized double-column convolutional neural network (RDCNN) with the architecture shown in    tion does not involve style attributes xs. In each learning iteration, we only fine-tuned the parameters in the aesthetic column and the learning process is supervised by the aesthetic label. The parameters of the style column are fixed and the style attributes x is essentially serve as a regularizer for training the aesthetic column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head><p>We evaluated the proposed method for aesthetics quality categorization on the AVA dataset <ref type="bibr" target="#b24">[24]</ref>. We first introduce the dataset. Then we report the performance of SCNN with different network architectures and normalized inputs. Next, we present aesthetic quality categorization results with DCNN and qualitatively analyze the benefits of the double-column architecture over a single-column one. We also demonstrate the performance of RDCNN with the accuracy of trained style classifier and aesthetic categorization results with style attributes incorporated. Finally, we summarize the computational efficiency of SCNN, DCNN, and RDCNN in training and testing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Dataset</head><p>The AVA dataset contains a total of 250, 000 images, each of which has about 200 aesthetic ratings ranging from one to ten. We followed the experimental settings in <ref type="bibr" target="#b24">[24]</ref>, and used the same collection of training data and testing data: 230, 000 images for training and 20, 000 images for testing. Training images are divided into two categories, i.e., lowquality images and high-quality images, based on the same criteria as <ref type="bibr" target="#b24">[24]</ref>. Images with mean ratings smaller than 5 -δ are referred to as low-quality images, those with mean ratings larger than or equal to 5+δ are high-quality images. We set δ to 0 and 1 respectively to generate the binary ground truth labels for the training images. Images with ratings between 5 -δ and 5 + δ are discarded. With δ = 0, there are 68, 000 low-quality images and 167, 000 high-quality images. With δ = 1, there are 7, 500 low-quality images and 45, 000 high-quality images. For the testing images, we fix δ to 0, regardless what δ is used for training. This results in 5, 700 low-quality images and 14, 000 high-quality images for testing.</p><p>To learn style attributes, we use the subset of images with style labels from the AVA dataset as the training set. The 14 style classes include complementary colors, duotones, HDR, image grain, light on white, long exposure, macro, motion blur, negative images, rule of thirds, shallow DOF, silhouettes, soft focus, and vanishing point. The subset contains 11, 000 images for training and 2, 500 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCNN Results</head><p>We compare the performance of SCNN with different layer combinations and normalized inputs on aesthetic quality categorization task. Table <ref type="table" target="#tab_2">1</ref> presents seven different architectures and their overall accuracy. As shown, the selected layer for each architecture is labeled with a check mark. In all seven architectures, we use I r l as the input with δ = 0. The results show that the architecture Arch 1 performs the best, which partially indicates the importance of choosing a proper number of convolutional layers and fully connected layers, and having normalization layers.  Table <ref type="table" target="#tab_4">3</ref>, the performance of this setting is better than the state of the art on the AVA dataset for both δ = 0 and δ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DCNN Results</head><p>We adopt the SCNN architecture Arch 1 for both columns in DCNN. Figure <ref type="figure" target="#fig_5">5</ref> illustrates the filters of the first convolutional layer for trained DCNN. The first 64 are from the local column (with the input I r l ), while the last 64 are from the global column (with the input I w g ). Compared with the filters trained in the object recognition task on CIFAR dataset 2 (shown in Figure <ref type="figure" target="#fig_6">6</ref>), the filters learned with image aesthetic labels are smoother and cleaner without radical intensity changes. This indicates that differences between low-aesthetic and high-aesthetic image cues mainly lie in the amount of texture and the complexity of the whole image. The difference can be observed from typical test images presented in Figure <ref type="figure">7</ref>. The images ranked the highest in aesthetics are generally smoother than those ranked the lowest. This finding substantiates the importance of simplicity and complexity features recently proposed for analyzing visual emotions <ref type="bibr" target="#b19">[19]</ref>.</p><p>To quantitatively demonstrate the effectiveness of trained DCNN, we compare its performance with that of the SCNN as well as <ref type="bibr" target="#b24">[24]</ref>. As shown in Table <ref type="table" target="#tab_4">3</ref>, DCNN outperforms SCNN for both δ = 0 and δ = 1, and significantly outperforms the earlier work. To further demonstrate the effectiveness of joint training of DCNN, we compare DCNN with AVG SCNN, which averaged the two SCNN results with I w g and I r l as inputs. As shown in Table <ref type="table" target="#tab_4">3</ref>, DCNN outperforms the AVG SCNN for both δ values.  To qualitatively analyze the benefits of the double-column architecture, we visualize ten test images correctly classified by DCNN but incorrectly by SCNN. We present the examples in Figure <ref type="figure" target="#fig_7">8</ref>. Images in the first row are misclassified by SCNN with the input I r l . Images in the second row are misclassified with the input I w g . The label on each image indicates the ground-truth aesthetic quality. As shown, images misclassified by SCNN with the input I r l usually contain a dominant object, which is because I r l does not consider the global information in an image. Images misclassified by SCNN with the input I w g often have detailed information in their local views that can improve the classifier if can be properly leveraged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Categorization with Style Attributes</head><p>To demonstrate the effectiveness of the style attributes for aesthetic quality categorization, we first evaluate the style classification accuracy with SCNN. We then compare the performance of RDCNN with DCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Style Classification</head><p>We train the style classifier with SCNN, and visualize the filters learned by the first convolutional layer of SCNN in Figure <ref type="figure" target="#fig_9">9</ref>. We test the trained model on 2, 573 images.  For each image, we randomly sample 50 patches of the size 224 × 224 × 3, and average the prediction results. To compare our results with the results reported in <ref type="bibr" target="#b24">[24]</ref>, we use the same experimental setting. We perform similar experiments as discussed in Section 3.2 by comparing different architectures and normalized inputs. The comparison results for different architectures are shown in Table <ref type="table" target="#tab_5">4</ref>. The selected layer for each architecture is labeled with a check mark. We achieve the best accuracy for style classification with Arch 1 and I r l as input (Table <ref type="table" target="#tab_6">5</ref>). It indicates the importance of local view in determining the style of an image. It shows the effectiveness of lr as a data augmentation strategy in case of limited training data. We did not compare our style classification results with Karayev et al. <ref type="bibr" target="#b12">[12]</ref> as their evaluations were done on a randomly selected subset of test images.</p><note type="other">High% High% High% High% High% High% High% High% High% High% High% Low% Low% Low% Low% Low%</note><p>The Average Precision(AP) and Mean Average Precision (MAP) are also calculated. The best MAP we achieved is 56.81% which outperforms the accuracy of 53.85% reported in <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Aesthetic Quality Categorization with Style Attributes</head><p>We demonstrate the effectiveness of style attributes by comparing the best aesthetic quality categorization accuracy we have achieved with and without style attributes. As shown in Table <ref type="table" target="#tab_4">3</ref>, RDCNN outperforms DCNN for both δ values.</p><p>To qualitatively analyze the benefits brought with the regularized double-column architecture, we show typical test images that have been correctly classified by RDCNN but misclassified by DCNN Figure <ref type="figure" target="#fig_10">10</ref>. Those examples correctly classified by RDCNN are with the following styles: rule-of-thirds, HDR, black and white, long exposure, complementary colors, vanishing point, and soft focus. This indicates that styles attributes help aesthetic quality categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computational Efficiency</head><p>Training SCNN for a specific input type takes about two days. Training DCNN takes about three days. For RDCNN, style attribute training takes roughly a day, and RDCNN training three to four days. Classifying 2,000 images (each with 50 views) takes about 50 minutes, 80 minutes, and 100 minutes for SCNN, DCNN, and RDCNN, respectively, with Nvidia Tesla M2070/M2090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We present a double-column deep convolutional neural network approach for aesthetic quality rating and categorization. Rather than designing handcrafted features or adopting generic image descriptors, aesthetic-related features are learned automatically. Feature learning and classifier training are unified with the proposed deep neural network approach. The double-column architecture takes into account both the global view and local view of an image for judging its aesthetic quality. Besides, image style attributes are leveraged to improve the accuracy. Evaluating with the AVA dataset, which is the largest benchmark with rich aesthetic ratings, our approach shows significant better results than earlier-reported results on the same dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Global views and local views of an image. Global views are represented by normalized inputs: center-crop, warp, and padding (shown in the top row). Local views are represented by randomly-cropped inputs from the original high-resolution image (examples shown).</figDesc><graphic coords="2,272.87,160.09,88.55,88.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Double-column convolutional neural network. Each training image is represented by its global and local views, and is associated with its aesthetic quality label: 0 refers to a low quality image and 1 refers to a high quality image. Networks in different columns are independent in convolutional layers and the first two fully-connected layers. The final fully-connected layer are jointly trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Regularized double-column convolutional neural network (RDCNN). The style attributes xs are generated through pre-trained Style-SCNN and we leveraged the style attributes to regularize the training process of RDCNN. The dashed line indicates that the parameters of the style column is fixed during RDCNN training. While training the RDCNN, we only fine-tuned the parameters in the aesthetic column and the learning process is supervised by the aesthetic label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Two normalized inputs of the aesthetic column are I w g and I r l , same as in DCNN (Section 2.2). The input of the style column is I r l . The training of RDCNN is done by solving the following optimization problem: max Xa,Wa N i=1 c=1∈Ca I(yai = c) log p(yai | xai, xsi, wac) , (4) where xsi are the style attributes of the i-th training image, xai are the features to be learned. Note that the maximiza-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: 128 convolutional kernels of the size 11 × 11 × 3 learned by the first convolutional layer of DCNN for aesthetic quality categorization. The first 64 are from the local view column (with the input I r l ) and the last 64 are from the global view column (with the input I w g ).</figDesc><graphic coords="6,54.40,235.82,239.83,61.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 64 convolutional kernels of the size 5 × 5 × 3 learned by the first convolutional layer of CNN for object classification on the CIFAR dataset.</figDesc><graphic coords="6,54.34,380.79,240.03,62.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Test images correctly classified by DCNN but misclassified by SCNN. The first row shows the images that are misclassified by SCNN with the input I r l . The second row shows the images that are misclassified by SCNN with the input I w g . The label on each image indicates the ground-truth aesthetic quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2</head><label></label><figDesc>http://www.cs.toronto.edu/~kriz/cifar.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: 32 convolutional kernels of the size 11 × 11×3 learned by the first convolutional layer of Style-SCNN for style classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Test images correctly classified by RDCNN and misclassified by DCNN. The label on each image indicates the ground truth aesthetic quality of images.</figDesc><graphic coords="9,150.57,204.16,98.80,79.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy for Different SCNN Architectures</figDesc><table><row><cell>conv1</cell><cell>pool1</cell><cell>rnorm1</cell><cell>conv2</cell><cell>pool2</cell><cell>rnorm2</cell><cell>conv3</cell><cell>conv4</cell><cell>conv5</cell><cell>conv6</cell><cell>fc1K</cell><cell>fc256</cell><cell>fc2</cell><cell>Accuracy</cell></row><row><cell>(64)</cell><cell></cell><cell></cell><cell>(64)</cell><cell></cell><cell></cell><cell>(64)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of Aesthetic Quality Categorization with Different Inputs</figDesc><table><row><cell>δ</cell><cell>I r l</cell><cell>I w g</cell><cell>I c g</cell><cell>I p g</cell></row><row><cell cols="5">0 71.20% 67.79% 65.48% 60.43%</cell></row><row><cell cols="5">1 68.63% 68.11% 69.67% 70.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of Aesthetic Quality Categoriza-</figDesc><table><row><cell cols="4">tion for Different Methods</cell><cell></cell><cell></cell></row><row><cell>δ</cell><cell>[24]</cell><cell cols="4">SCNN AVG SCNN DCNN RDCNN</cell></row><row><cell cols="3">0 66.7% 71.20%</cell><cell>69.91%</cell><cell>73.25%</cell><cell>74.46%</cell></row><row><cell>1</cell><cell>67%</cell><cell>68.63%</cell><cell>71.26%</cell><cell>73.05%</cell><cell>73.70%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Accuracy for Different Network Architectures for Style Classification</figDesc><table><row><cell>conv1</cell><cell>pool1</cell><cell>rnorm1</cell><cell>conv2</cell><cell>pool2</cell><cell>rnorm2</cell><cell>conv3</cell><cell>conv4</cell><cell>conv5</cell><cell>conv6</cell><cell>fc1K</cell><cell>fc256</cell><cell>fc14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of Style Classification with Different Inputs AP 56.93% 44.52% 45.74% 41.78% MAP 56.81% 47.01% 48.14% 44.07% Accuracy 59.89% 48.08% 48.85% 46.79%</figDesc><table><row><cell>I r l</cell><cell>I w g</cell><cell>I c g</cell><cell>I p g</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://yann.lecun.com/exdb/mnist/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>√ √ √ √ 70.93%</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>√ √ √ √ 47.44% 52.16%</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research has been primarily supported by Penn State's</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive multi-column deep neural networks with application to robust image denoising</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">In Art and visual Perception: A psychology of the creative eye</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arnheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>University of California Press</publisher>
			<pubPlace>Los Angeles. CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A framework for photo-quality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using computational approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531v1</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing image style</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Winnermoller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What makes an image popular</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Litzel</surname></persName>
		</author>
		<title level="m">On Photographic Composition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Amphoto Books</publisher>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning beautiful (and ugly) attributes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An exploratory investigation into factors affecting visual balance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Niekamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Communication and Technology: A Journal of Theory, Research, and Development</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color compatibility from large datasets</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage features learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aesthetics-preserving features</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
