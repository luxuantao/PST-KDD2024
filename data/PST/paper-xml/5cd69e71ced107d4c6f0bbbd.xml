<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Analysis of Mobile Physiological, Environmental and Location Sensor Data for Emotion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Eiman</forename><surname>Kanjo</surname></persName>
							<email>eiman.kanjo@ntu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computing and technology Department</orgName>
								<orgName type="institution">Nottingham Trent University</orgName>
								<address>
									<settlement>Nottingham</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eman</forename><forename type="middle">M G</forename><surname>Younis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Minia University</orgName>
								<address>
									<region>Minia</region>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siang</forename><surname>Chee</surname></persName>
						</author>
						<author>
							<persName><surname>Ang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering and Digital Arts</orgName>
								<orgName type="institution">University of Kent</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning Analysis of Mobile Physiological, Environmental and Location Sensor Data for Emotion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D0520FF29C1F200B90BFF9D7B11214F1</idno>
					<idno type="DOI">10.1016/j.inffus.2018.09.001</idno>
					<note type="submission">Received date: 15 January 2018 Revised date: 14 August 2018 Accepted date: 1 September 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Fusion deep learning</term>
					<term>emotion recognition</term>
					<term>convoltutional neural network</term>
					<term>long short-term memory mobile sensing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection and monitoring of emotions are important in various applications, e.g. to enable naturalistic and personalised human-robot interaction.</p><p>Emotion detection often require modelling of various data inputs from multiple modalities, including physiological signals (e.g.EEG and GSR), environmental data (e.g. audio and weather), videos (e.g. for capturing facial expressions and gestures) and more recently motion and location data. Many traditional machine learning algorithms have been utilised to capture the diversity of multimodal data at the sensors and features levels for human emotion classification.</p><p>While the feature engineering processes often embedded in these algorithms are beneficial for emotion modelling, they inherit some critical limitations which may hinder the development of reliable and accurate models. In this work, we adopt a deep learning approach for emotion classification through an iterative process by adding and removing large number of sensor signals from different modalities. Our dataset was collected in a real-world study from smart-phones and wearable devices. It merges local interaction of three sensor modalities: on-body, environmental and location into global model that represents signal dynamics along with the temporal relationships of each modality. Our approach employs a series of learning algorithms including a hybrid approach</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• First hybrid Deep Learning approach applied to environmental and onbody sensor data.</p><p>• Deep-learning is effective in human emotion classification.</p><p>• Multi-model data fusion approach performs better than single modality classification.</p><p>• Hybrid approach with combined CNN and LSTM exceed the performance of CNN alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint submitted to Journal of Information Fusion</head><p>September 4, 2018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The growing popularity of sensors and low power integrated circuits, together with the increasing use of wireless networks have led to the development of affordable, robust and efficient wearable devices which can capture and transmit data in real time for a long period of time. These data sources provide a unique opportunity for innovative ways in recognising human activities through human physiological sensing while also taking into account other natural environmental factors, such as weather, noise levels, etc. This could potentially contribute to better management of chronic diseases such as diabetes, asthma and cardiovascular diseases <ref type="bibr" target="#b0">[1]</ref>. For example, extensive research has focused on automatic detection of physical exercises which are linked to a range of health related issues <ref type="bibr" target="#b1">[2]</ref>. Due to these potential impacts, research work is on the rise with many algorithms being developed for a range of application areas in healthcare (e.g. symptoms monitoring, home-based rehabilitation) and beyond (e.g. security, logistics supports) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Some of these machine learning algorithms include multivariate regression, K-nearest Neighbour (KNN) classification combined with Dynamic Time Warping (DTW), etc. In addition,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>given the importance of mental health and its increasing impact on societies, researchers are now finding ways to accurately detect human emotion with the hope to develop intervention strategies for mental health and to provides rich contextual information which can be used to better understand mental health issues <ref type="bibr" target="#b3">[4]</ref>. Furthermore, there have also been significant interests in emotion detection in human-computer interactions <ref type="bibr" target="#b4">[5]</ref> due to its potential use, allowing us to design intelligent computer systems which are adaptable according to users emotional states, ensuring convergence and optimisation of human-computer interaction. Therefore, there have been numerous attempts to exploit machine learning techniques utilising sensor datasets for automatic emotion detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">9]</ref>. To date, a significant amount of research in automatic emotion detection has been carried out primarily using visual, audio and movement data (e.g. facial expression, body postures, speeches) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>. With the increased availability of low-cost wearable sensors (e.g. Fitbit, Microsoft writs bands), there is an emergence in research interest in using human physiological data (e.g. galvanic skin response (GSR), heart rate (HR), electroencephalography( EEG), etc.) for emotion detection. Apart from these, given the intimate links between emotion and environmental factors <ref type="bibr" target="#b5">[6]</ref>, studies are starting to look into using environmental sensors data and location patterns to infer human emotion <ref type="bibr" target="#b5">[6]</ref>. Despite the possibility of sensing a wide range of information (from human physiology to environment), automatic human emotion classification remains very challenging due to the idiosyncrasy and variability of human emotional expressions <ref type="bibr" target="#b11">[11]</ref>. The range of modalities of emotion expression could be very broad, with many of these modalities still being inaccessible to current sensor technology (e.g. blood chemistry). Many accessible physiological signals may be non-differentiable in emotion detection <ref type="bibr" target="#b11">[11]</ref>. Furthermore, studies in automatic emotion detection rely on controlled samples in lab settings, where specific emotions are artificially triggered using audio-visual stimuli (e.g. presenting photos or videos to participants) or by asking participants to carry out carefully designed tasks to induce emotional states <ref type="bibr" target="#b12">[12]</ref>. Although this type of controlled studies is valuable for certain applications (e.g. clinical diagnosis in</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T healthcare)</formula><p>, its use is rather limited to strictly controlled environments. For emotion detection technology to be useful in the everyday management of mental health and mobile human-computer interaction in the wild, we are interested in techniques which allow us to detect emotion on-the-go and in real-life settings. In this paper, we explore a deep learning approach for multivariate time series classification, combining environmental, physiological and location sensor data using smart phones and wristbands. Inspired by the deep feature learning in images and speech recognition <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15]</ref>, we explore a deep learning framework for multivariate time series classification for emotion recognition in the wild, where users are walking in a urban area. Deep learning relieves the burden of manually extracting hand-crafted features for machine learning models. Instead, it can learn a hierarchical feature representation from raw data automatically. We leverage this characteristic by building models using a range of deep learning methods to train raw sensor data. This eliminates the need for data pre-processing and feature space construction, and simplifies the overall machine learning process <ref type="bibr" target="#b16">[16]</ref>. Due to its success in image and speech classi- 4. The analysis and fusion of human physiological, environmental and location features individually and combined to explore its significance in emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In recent years, smart phones and many wearable devices such as smart watches and wristbands are equipped with a range of sensors which can continuously monitor human physiological signals (e.g. heart rate, motions/movements, location data) and in some cases the ambient environment data (e.g. noise, brightness, etc.). This led to the emergence of large datasets in a wide variety of research areas such as healthcare and smart city. This burst of on-Body and environmental data presents an unprecedented opportunity for healthcare research, but it requires the development of new tools and approaches to deal with large multidimensional datasets. In the past decades, researchers from var-</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>ious fields, particularly in ubiquitous and mobile computing have been exploring</p><p>the possibilities harnessing these data to infer or predict human behaviour, with varying levels of success <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b22">25]</ref>. Given the relative ease of collecting time series sensor datasets, researchers have investigated the relationship between these sensor data and human emotion. The majority employ traditional statistical analysis methods and machine learning techniques. Often, a number of hand-crafted features that summarise the raw sensor data are extracted from the less structured data. These features are then filtered empirically or using structured algorithms through a feature selection process <ref type="bibr" target="#b16">[16]</ref>.</p><p>Features with low level of correlation with its corresponding label are excluded (through dimensionality reduction). Moreover, features are often removed to avoid collinearity, when excessive correlation among explanatory variables (features) exist in the dataset. Given the list of selected features, computational models are built which help classify or predict human activity/emotion using machine learning models such as logistic regression based models <ref type="bibr" target="#b5">[6]</ref>, support vector machines (SVM), decision trees, artificial neural networks (ANN), etc. <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b24">27]</ref>. Although hand-crafted features have yielded promising results, they are domain-specific, and often poorly generalise to other similar problem domains.</p><p>Handcrafted-based approaches involve laborious human intervention for selecting the most discriminating features and decision thresholds from sensor data.</p><p>Handcrafted features have a decisive impact on models <ref type="bibr" target="#b16">[16]</ref> and often utilise statistical variables, e.g., mean, variance, kurtosis and entropy, as distinctive representation features. Moreover, traditional machine learning and feature engineering algorithms may not be efficient enough to extract the complex and non-linear patterns generally observed in multimodal time series datasets. In addition, traditional feature engineering could also result in a large output features set <ref type="bibr" target="#b25">[28]</ref>. This is problematic because it is difficult to know without training which features are relevant to a given task, and which are noise. As a result, the ability to select features from a huge feature set is critical and will require additional dimensional reduction techniques to process these features. Furthermore, feature extraction and feature selection are computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The computational cost of feature selection may increase combinatorially as the number of features increases <ref type="bibr" target="#b25">[28]</ref>. In general, search algorithms may not be able to converge to optimal feature sets for a given model <ref type="bibr" target="#b16">[16]</ref>. Given the complexity of human emotion detection, it is important to have abstract representations of the sensor data which are invariant to local changes in the data.</p><p>Learning such invariant features is a major challenge in pattern recognition (for example learning features which are invariant to the time of data collection).</p><p>Traditional shallow methods, which contain only a small number of non-linear operations, do not have the capacity to accurately model such variation of time computer vision <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b27">30]</ref>, speech recognition <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b29">32]</ref> and natural language processing [20, <ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b31">34]</ref> where it performs better than standard machine learn-</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>ing methods and the performance is comparable to human level. While some attempts at detecting human activity and emotion have been made using deep learning <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr">20]</ref>, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">9]</ref>, it is still a new and growing area of research which requires further work. In recent years, deep learning has been increasingly used in the field of human activity recognition <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">21]</ref>. While progress has been made, human activity recognition remains a challenging task. This is partly due to the broad range of human activities as well as the rich variation in how a given activity can be performed. Since deep learning is capable of high-level abstraction of data, it can be used to develop self-configurable frameworks for human activity as well as emotion recognition. For instance, in an attempt to improve performance accuracy of activity recognition using mobile phone triaxial accelerometer data, <ref type="bibr" target="#b17">[17]</ref> utilised a hybrid approach of deep learning and hidden Markov models(HMM). This approach allows to model deep hierarchical representations of spatial data with restricted Boltzmann machines (RBM)</p><p>and stochastic modelling of temporal sequences in the HMM models. The proposed approach was reported to have performed better than traditional methods of using shallow networks with handcrafted features. Other deep learning architectures, including Convolutional Neural Network (CNN) and Recurrent</p><p>Neural Network (RNN) have been increasingly applied in activity recognition problems. The performance of CNN for some activity recognition tasks was explored by <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b34">37]</ref>. Building on CNNs successes in image recognition, <ref type="bibr" target="#b13">[13]</ref> developed a method based on CNN and applied it in activity recognition prob- </p><formula xml:id="formula_3">lems</formula><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>in this dataset. <ref type="bibr" target="#b34">[37]</ref> showed that CNN also outperformed other conventional machine learning methods (e.g. KNN and SVM) in two other activity recognition datasets: breakfast activity and gesture recognition. A CNN is used in <ref type="bibr" target="#b35">[38]</ref> to extract features for gait pattern recognition so that labour intensive handcrafted feature extraction process is avoided. Furthermore, CNNs have been applied for detection of stereotypical movements in Autism <ref type="bibr" target="#b36">[39]</ref>, where they significantly improved upon the state-of-the-art. Recurrent neural network (RNN)</p><p>relying on Long Short-Term Memory (LSTM) cells have gained popularity due to its ability to exploit the temporal dependencies in time series data. LSTM have recently achieved impressive performance in various time-dependent applications, such as machine translations, automatic video subtitling, and others <ref type="bibr" target="#b37">[40]</ref>. A biometrics application of LSTM has been explored by <ref type="bibr" target="#b38">[41]</ref>   • The convolutional layer is the main building block of a CNN which determines the output of connected inputs in within local subregions. This is done via a set of learnable filters (kernels) which are convolved across the the width and height of the input data, calculating the scalar product between the values of the filter and the input, hence producing a two dimensional activation map of that filter. Through this, CNNs are able to learn filters which activate when specific type of features at some spatial position of the input are detected.</p><p>• The pooling layer will perform downsampling along the spatial dimensionality of the given input, further reducing the number of weights within that activation.</p><p>• The fully-connected layers are standard deep neural networks and attempt to produce predictions from the activation, to be used for classification or regression.</p><p>Convolution is the key operation in CNN. By convolution of the input signal with a linear filter (or kernel), adding a bias term and then applying a non-linear function, a 2D matrix named feature map is obtained, representing local correlation across the input signal. Specifically, for a certain convolutional layer,</p><formula xml:id="formula_5">A C C E P T E D M A N U S C R I P T</formula><p>the units in it are connected to a local subregion of units in the (l-1)th layer.</p><p>Note that all the units in one feature map share the same weight vector (for kernel) and bias, hence, the total number of parameters is much less than traditional multilayer fully connected neural networks with the same number of hidden layers. This indicates that CNN has a sparse network connectivity <ref type="bibr" target="#b14">[14]</ref>,</p><p>which results in considerably reduced computational complexity compared with the fully connected neural network. For a richer representation of the input, each convolutional layer can produce multiple feature maps. Though units in adjacent convolutional layers are locally connected, various salient patterns of the input signals at different levels can be obtained by stacking several convolutional layers to form a hierarchy of progressively more abstract features. For the jth feature map in the lth convolutional layer Cl,j, the unit at the mth row and the nth column is denoted as vm,nl,j and the value of vm,nl,j is defined by:</p><formula xml:id="formula_6">vm, nl, j = σ(bl, j + k pa = 0P l, a1 pb = 0P l,<label>(1)</label></formula><p>b -1wpa, pbl, j, kvm + pa, n + pbl1, k)</p><formula xml:id="formula_7">∀n = 1, 2, , N l, m = 1, 2, , M l</formula><p>where Ml and Nl are height and width of feature map Cl,j. bl,j is the bias of this feature map, k indexes over the set of feature map in the (l-1)th layer, wpa,pbl,j,k is the value of convolutional kernel at position (pa,pb), Pl,a and Pl,b are the size of the convolutional kernel, and σ() is the Rectified Linear Units (ReLU) nonlinear function. ReLU is defined by:</p><formula xml:id="formula_8">σ(x) = max(0, x)<label>(2)</label></formula><p>The proposed convolution operation is performed without zero padding (unlike the conventional approaches of image processing). This means each dimension of feature map will be reduced after a convolution operation. Thus:</p><formula xml:id="formula_9">M l = M l -1 -P l, a + 1N l = N l -1 -P l, b + 1 (<label>3</label></formula><formula xml:id="formula_10">)</formula><p>where l is the index of the layer that performs convolutional operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T   a number between 0 and 1 for each number in the cell state C t-1 . 1 represents completely keep this while 0 represents completely remove this. The output ft of the gate is formalised as:</p><formula xml:id="formula_11">f t = σ(W f • [h t-1 , x t ] + bf )<label>(4)</label></formula><p>Then the cell decides which new information will be stored in the cell state. This has two parts. First, a sigmoid layer known as the input gate layer decides which values will be updated. Then, a tanh layer creates a vector of new candidate values, Ĉt , which could be added to the state. These two will be combined to create an update to the state, as follow:</p><formula xml:id="formula_12">it = σ(W i • [h t-1 , x t ] + b i ) (5) Ĉt = tanh(W C • [h t-1 , x t ] + b C )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Then, we update the old cell state, C t-1 , into the new cell state C t as follow:</p><formula xml:id="formula_13">C t = C t-1 * f t + i t Ĉt<label>(7)</label></formula><p>To produce the output, a Sigmoid layer is first run, which decides which parts of the cell state will be output. Then, the cell state is fed through tanh (to push the values to be between -1 and 1) and multiplied by the output of the Sigmoid gate, as follow:</p><formula xml:id="formula_14">ot = σ(W o • [h t-1 , x t ] + b o )<label>(8)</label></formula><formula xml:id="formula_15">h t = tanh(C t ) * o t<label>(9)</label></formula><p>As we used Softmax as our last activation, our loss function is cross entropy loss:</p><formula xml:id="formula_16">Loss = - i log exp(W x i ) j exp(W x j )<label>(10)</label></formula><p>Finally, Adam Optimizer can be used to have a better navigation through the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we explain in details the dataset used for emotion in the wild classification and the architectures of deep learning models used for experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The EnvBodySens DataSet</head><p>We use the EnvBodySens dataset <ref type="bibr" target="#b5">[6]</ref> to evaluate the models, which con- The statistical data analysis of the dataset is reported in <ref type="bibr" target="#b5">[6]</ref>. Participants were asked to periodically report how they feel based on predefined emotion scale as they walked around the city centre. We adopted the 5-step SAM Scale for Valence taken from <ref type="bibr" target="#b42">[46]</ref> to simplify the continuous labelling process. On average, 134 self-reports were entered per participants. We disabled the screen auto sleep mode on our mobile devices, so the screen was kept on during the data collection process. <ref type="bibr" target="#b5">[6]</ref>. Data from six users were excluded due to logging problem. For example, one user was unable to collect data due to battery problem with the mobile phone, another user switched the application off accidentally. The correlation matrix in <ref type="bibr" target="#b5">[6]</ref> shows a low level of correlation between the independent variables, suggesting that our model will not be affected by the multi-collinearity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model implementation</head><p>We use TensorFlow <ref type="bibr" target="#b43">[47]</ref> to implement our models and Tensorboard for visualisation on Xeon E5-2640 v4 Processor (25Mb Cache, 2.4GHz, 8 core). In this paper, we first train a Multi-layer Perceptron (MLP) for emotion classification based on twenty raw sensor input from three modalities: i) on-body (i.e. physiological and motion/movement data), ii) environmental, iii) and location data.</p><p>Initially, we train each modality individually and then we combine all sensor input modalities in a separate training process, see Figure <ref type="figure" target="#fig_6">2</ref> for the four different learning architectures. Then we evaluate the performance of each modality against the combined model. This is then followed by training deep learning models in order to test the efficacy of the deep learning approach for accurately classifying multimodal time series data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data pre-processing</head><p>After the data collection the signals were pre-processed and cleaned. The first and the last 30 s were removed from the start of the data collection process for each user data, the reason for this step is that users needed a few seconds to fully engage in the movement and also few seconds to terminate the data collection process. A non-overlapping sliding window strategy has been adopted to segment the time series signal.12 shows the difference between the two segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MLP Models</head><p>Our implementation of "Multi-Layer Perceptron" (MLP) network consists of two hidden layers. The first layer has 64 neuron, whereas the second hidden layer has 32 neurons. The input layer is 20*40 dimensions per iteration. The output layer has 5 neurons, each corresponds to the 5 emotional classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CNN Models</head><p>We start with the notations used in the CNN. A sliding window strategy is adopted to segment the time series signal into a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CNN-LSTM Models</head><p>CNN-LSTM has a similar structure as CNN, with an added LSTM layer (see Figure3). In particular, the temporal dimension of the data is preserved during the convolution operation, and the resulting fully connected layer is fed into LSTM cell( see Figure <ref type="figure" target="#fig_9">4</ref>). Each LSTM cell keeps track of an internal state that represents its memory. Over time the cells learn to output, overwrite, or reset their internal memory based on their current input and the history of past internal states. The MaxPool kernel is 2x2, stride is 1x2, i.e. strides= [1, 1, 2, 1],</p><p>padding is 1. So that the temporal dimension is preserved and we only shrink output of these filters are also shown in Figure <ref type="figure" target="#fig_9">4</ref>. We train all models mentioned above on each subject dataset using fused data from all sensors modalities. We also train the models on three subsets of the data based on three modalities: on-Body, Env and Location. In total, we train 12 models on each user dataset (3x3 models on subsets and 3 models on fused data). Here, n=40 raw samples and c=20 the number of the attributes from sensor input. Similarly, c=2 for the location models, c=3 is for Env models (Noise, Air-Pressure and UV) and c =15 for the On-body models (the rest of the attributes).</p><formula xml:id="formula_17">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T   and neutral "3". In addition, we have cropped the first few minutes of the data recording when users are stationary and using default rating (label) at 3. We believe our dataset is reasonably balanced with small variation from one user to another. Modern deep learning techniques allow us to train a network in batches by interleaving multiple sequences together. Among others, batching allows to further exploit the power of matrix multiplication on the GPU and to avoid loading all data into memory at once. The batch size has implications for the robustness of the error that is propagated in the learning phase <ref type="bibr" target="#b26">[29]</ref>. Figure <ref type="figure" target="#fig_8">3</ref> shows an example of 3 batches that encode 3 sequences of 5 samples each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T      We have attempted to combine all participants data into one single dataset for emotion detection, however we found a high across-subject variation in the dataset which led to low model accuracy of less than 50%. This observation is in agreement with previous studies <ref type="bibr" target="#b45">[49]</ref> which verify that emotion recognition is subject dependent which makes it difficult to obtain a generalised model across  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 . 2 . 3 .</head><label>123</label><figDesc>fication, deep learning has been increasingly used for non-image/speech data, including human activity recognition using time series data such as in the case of smart phone accelerometer data [17, 18, 1, 19]. There have also been recent attempts using deep learning for emotion detection, although most studies have only looked at lab based emotion data [20, 9]. Specifically, we propose a Multi-Channels Deep Convolutional Neural Network (MC-DCNN) model. We follow a hybrid approach based on Convolutional Neural Network and Long Short-term Memory Recurrent Neural Network (CNN-LSTM) inspired by previous state of the art [19, 21] which have been applied to human activity using accelerometer data. The majority of the studies employing deep learning on activity recognition are restricted to a handful of data channels as opposed to this study where we utilise 20 sensor channels from three different modalities to classify emotion against self-reported emotion labels. The main contribution of our work lies in: The use of multimodal sensor feeds (physiological, environmental and lo-A C C E P T E D M A N U S C R I P T cation data) for emotion detection using features automatically extracted with deep learning approach. Although deep learning has been used in human activity/emotion detection, few studies looked into multimodal datasets. Specifically, to the best of our knowledge, no other work has applied deep learning on the combination of physiological, environmental and location data for emotion recognition. The collection of real-world data from participants walking in a transited city location wearing a wristband and smart phone, while reporting their emotion periodically using a smart phone. The data therefore better reflect the complexity of real life environments. Most previous studies in automatic emotion detection are carried out in controlled lab settings as opposed to "in the wild" (i.e. in participants' natural environments), therefore the results are restricted to narrow application domains. Various experiments carried out to compare different architectures of deep neural networks, including hybrid models using hybrid multi-channel sensor data (beyond human activity recognition).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>series data. Therefore, to overcome the difficulties in obtaining effective and robust features from time-series data, many researchers have turned their attention to deep learning approaches. One interesting property of deep learning techniques is that they can work on raw data and automate the feature extraction and selection. Noisy time series samples are fed into the network as input data, and during each transformation, a hidden representation of inputs from the prior layer is generated to form a higher hierarchical architecture of data representation (i.e. features). One can train the network by adjusting the mapping parameters, in order to obtain finer abstraction levels. Specifically, each layer in a deep learning model combines outputs from the previous layer and transforms them via a non-linear function to form a new feature set. This gives a deep learning model the ability to automatically learn features directly from the underlying sensor data, forming a hierarchy where basic features are detected in the first layers, and in the deeper layers the abstract features from previous layers are combined to form complex feature maps. Empirical studies showed that data representations obtained from stacking up non-linear feature extracting layers as in deep learning often yield better results, e.g., improved classification model accuracy [18], better generative models (to produce better quality samples) [18], and the invariant characteristics of data representations [18]. Deep learning techniques have already made significant impacts in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>C C E P T E D MA N U S C R I P T data and financial data<ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b18">21]</ref>. The inputs in a convolutional layer connect to the subregions of the layers instead of being fully-connected as in traditional neural networks models. These group of inputs in subregions share the same weights, therefore the inputs of a CNN produce spatially-correlated outputs, whereas in traditional neural networks (NN), each input has individual weight and hence produce independent outputs. In a neural network with only fullyconnected layers, the number of weights can increase quickly as the dimension of the input increases. CNNs reduce the number of weights compared with NN with the reduced number of connections through weights sharing and downsampling. CNNs typically consist of of three types of layers: convolutional layers, pooling/downsampling layers and fully-connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This allows a RNN to continue to learn over many time steps, thereby opening a channel to link causes and effects remotely. 1The structure of a LSTM cell is illustrated in Figure and the mechanism of the gates is described as follows: The first step in a LSTM cell is to decide what information we will forget from the cell state. This decision is made by a Sigmoid layer called the forget gate layer. It looks at h t-1 and x t , and outputs ACCEPTED MANUSCRIPT A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Long short term memory (LSTM) block [44].</figDesc><graphic coords="15,151.45,151.23,267.48,272.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>sists of 40</head><label>40</label><figDesc>data files collected from 40 female participants (average age of 28) walking around the city centre in Nottingham, UK on specific routes. The dataset is composed of on-body data such as heart rate (HR), galvanic skin response (SGR), body temperature, motion data (accelerometer and gyro), environmental data such as noise levels, UV, air pressure and location data, GPS locations associated with time stamp and self report emotion levels (5-step Self-Assessment-Manikin (SAM) Scale for valence) logged by the EnvBodySens mobile application on Android phones (Nexus), connected wirelessly to Microsoft A C C E P T E D M A N U S C R I P T wrist Band 2 [45]. The participants were asked to spend no more than 45 minutes walking in the city center. Data was collected in similar weather conditions (average 20 • degrees), at around 11am. During the data collection process, 550,432 sensor data frames were collected as well as 5,345 self-report responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning Architectures, four models are trained, a) for On-Body, b) for Env , c) for Location separately d) and then fused using all the data input and feed it into the Deep Layers.</figDesc><graphic coords="18,118.02,151.23,334.32,89.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(n, c, t) tensor, where n = number of instances, c = sensor channels, t = time steps. After preliminary experiments with various deep learning topologies using multiple modalities combinations, we choose the CNN architecture as follows: Input of n batch x 20 channels x t window size, 2 convolutional layers (Conv1, Conv2), 2 maxpooling A C C E P T E D M A N U S C R I P T layers (Pooling1, Pooling2) and fully-connected layer as shown in 3. The first layer Conv1 has 32 filters (feature maps) while the second one Conv2 has 64 filters. This procedure may hinder partially the generality of the created models, as the average cross-validation accuracy is used to guide the feature selection search. However, the comparison between single, multiple modalities, and across fusion approaches is fair because all experiments follow the same procedure. The window size, r=40 (i.e. the height of sliding window) is chosen experimentally, by trying different sample rates from 10 to 100 as shown later in table 2. The convolution kernel is 2x2, stride is 1x1, i.e. strides= [1,1,1,1], Padding=1 (which does not shrink the matrix).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CNN architecture</figDesc><graphic coords="19,118.02,331.96,334.33,138.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CNN-LSTM Model Architecture, we train 4 models separately, these are a) On-Body, b) Env , c) for Location d) and then we fused model using all the data input and feed it into the Deep Layers</figDesc><graphic coords="20,118.02,151.23,334.33,265.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of average accuracy levels of all models</figDesc><graphic coords="21,118.02,484.38,334.33,149.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 . 5 ,</head><label>45</label><figDesc>illustrate the confusion matrices yielded by the three models based from one user data. There is a slight confusion between state 0 and 1 (negative emotions), which is improved when LSTM is added to the architecture. During the user study, we have made a great effort to ask users the meaning of each class and the difference between the very negative label "0"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Confusion matrices of three models for one user data (fused data).</figDesc><graphic coords="23,330.33,451.64,86.53,84.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Radar charts showing the accuracy levels of three models(a) MLP, (b) CNN , and CNN=LSTM, based on ten users data in ad-hoc and fused modes.</figDesc><graphic coords="24,221.36,267.84,127.65,105.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The accuracy levels of 10 users across all the models in ad-hoc and fused modes.</figDesc><graphic coords="24,151.45,430.94,267.46,222.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Accuracy and F-Measure levels of the base learners and the Stacking learner[6].</figDesc><graphic coords="27,118.02,151.23,334.34,138.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>individuals.Others have successfully created a universal deep learning model for gesture data as gestures performed by different individuals are typically quite similar.For emotion however , there is higher levels of variations between individuals. Our results, confirm this, and verify that the emotion recognition is subject dependent as the accuracy varies from subject to subject and exhibits high variance of accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Illustration of sliding window steps and overlapping</figDesc><graphic coords="28,184.88,524.80,200.61,107.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="25,118.02,154.95,334.34,207.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="25,118.02,402.67,334.35,252.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>In a traditional neural network (with only fully connected layers) we assume that all inputs are independent of each other. In CNN, we have seen that inputs can be grouped into subregions where features are spatially dependent on each other and share the same weights. For some classification/learning tasks, the inputs are temporally dependent. For instance, if we want to predict the next word in a sentence, it is important to know which words came before it. RNNs can perform the classification task for every element of a time sequence, with the output being depended on the previous computations. Another way to think</figDesc><table><row><cell>2.2. Recurrent Neural Networks (RNN)</cell></row></table><note><p>about RNNs is that they have a memory which captures information about what has been calculated so far. In other words, RNNs take as their input not just the current input data they see, but also what they perceived one step back in time. The decision a RNN reached at time step t -1 affects the decision it will reach at time step t. Hence, RNNs have two sources of input, the present and the recent past. Here is what a typical RNN looks like: In theory RNNs can make use of information in arbitrarily long sequences, but in practice they have difficulties learning long-range dependencies due to the vanishing gradient problem [43]. The vanishing gradient problem is the result of RNN seeking to establish connections between the final output and inputs from many time steps before as a RNN passes through many stages of multiplication. To address this, we adopt Long Short-term Memory (LSTM) as the RNN memory unit. LSTMs help preserve the error that can be backpropagated through time and layers by using a gated cell which determines what information from the prior step should be forgotten and what information in current time step should be remembered into the next state, via gates that open and close (activate and deactivate).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Average Performance metrics for all the models</figDesc><table><row><cell></cell><cell>Average</cell><cell cols="5">Precision Recall F-measure Accuracy RMSE</cell></row><row><cell>MLP</cell><cell>All</cell><cell>0.734</cell><cell>0.728</cell><cell>0.729</cell><cell>72.9</cell><cell>0.95975</cell></row><row><cell></cell><cell>Body</cell><cell>0.654</cell><cell>0.621</cell><cell>0.63</cell><cell>62.2</cell><cell>1.264</cell></row><row><cell></cell><cell cols="2">Environment 0.424</cell><cell>0.428</cell><cell>0.424</cell><cell>42.6</cell><cell>1.54</cell></row><row><cell></cell><cell>Location</cell><cell>0.59</cell><cell>0.605</cell><cell>0.58</cell><cell>60.2</cell><cell>1.22</cell></row><row><cell>CNN</cell><cell>All</cell><cell>0.818</cell><cell>0.79</cell><cell>0.787</cell><cell>78.6</cell><cell>0.788</cell></row><row><cell></cell><cell>Body</cell><cell>0.734</cell><cell>0.712</cell><cell>0.709</cell><cell>70.8</cell><cell>1.01</cell></row><row><cell></cell><cell cols="2">Environment 0.529</cell><cell>0.47</cell><cell>0.468</cell><cell>46.5</cell><cell>1.41</cell></row><row><cell></cell><cell>Location</cell><cell>0.79</cell><cell>0.761</cell><cell>0.769</cell><cell>78.7</cell><cell>0.99</cell></row><row><cell cols="2">CNN-LSTM All</cell><cell>0.927</cell><cell>0.95</cell><cell>0.949</cell><cell>94.7</cell><cell>0.291</cell></row><row><cell></cell><cell>Body</cell><cell>0.881</cell><cell>0.878</cell><cell>0.874</cell><cell>87.3</cell><cell>0.6</cell></row><row><cell></cell><cell cols="2">Environment 0.607</cell><cell>0.593</cell><cell>0.574</cell><cell>59.7</cell><cell>1.18</cell></row><row><cell></cell><cell>Location</cell><cell>0.655</cell><cell>5.586</cell><cell>0.621</cell><cell>64</cell><cell>1.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Average accuracy of CNN+LSTM models using different sliding window sizes. Bold numbers represent the best performing window size</figDesc><table><row><cell cols="4">Window Size F-measure Accuracy RMSE</cell></row><row><cell>20</cell><cell>0.942</cell><cell>94</cell><cell>0.5</cell></row><row><cell>40</cell><cell>0.949</cell><cell>94.7</cell><cell>0.291</cell></row><row><cell>60</cell><cell>0.946</cell><cell>94.7</cell><cell>0.313</cell></row><row><cell>80</cell><cell>0.911</cell><cell>92.7</cell><cell>0.8</cell></row><row><cell>100</cell><cell>0.922</cell><cell></cell><cell>1.1</cell></row><row><cell>120</cell><cell>0.912</cell><cell>92.5</cell><cell>1.3</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Physical activity assessment with accelerometers: an evaluation against doubly labeled water</title>
		<author>
			<persName><forename type="first">G</forename><surname>Plasqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Westerterp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obesity</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2371" to="2379" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on human activity recognition using wearable sensors</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Labrador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1192" to="1209" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial expression recognition in adolescents with mood and anxiety disorders</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leibenluft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychiatry</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1172" to="1174" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepcare: A deep dynamic memory model for predictive medicine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition in human-computer interaction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Votsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fellenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="80" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards unravelling the relationship between on-body, environmental and emotion data using sensor information fusion approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Younis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sherkat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Notimind: Utilizing responses to smart phone notifications as affective sensors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2017.2755661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Physiological signals based human emotion recognition: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jerritta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing and its Applications (CSPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="410" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international conference on Multimodal interfaces</title>
		<meeting>the 6th international conference on Multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Emotions in context: examining pervasive affective sensing systems, applications, and analyses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chamberlain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Personal and Ubiquitous Computing</publisher>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Affective computing: challenges</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ecg pattern analysis for emotion detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agrafioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="115" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Survey on feature extraction and applications of biosignals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="161" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep activity recognition models with triaxial accelerometers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop: Artificial Intelligence Applied to Assistive Technologies and Smart Environments</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordóñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Alajmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">El</forename><surname>Mawass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chamberlain</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACII.2013.138</idno>
		<title level="m">Shopmobia: An emotion-based shop rating system, in: Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neuroplace: Categorizing urban places according to mental states</title>
		<author>
			<persName><forename type="first">L</forename><surname>Al-Barrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M G</forename><surname>Younis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Things of the internet (toi): Physicalization of notification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp &apos;18 Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eiman</surname></persName>
		</author>
		<title level="m">UbiComp &apos;18 Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Emoecho: A tangible interface to convey and communicate emotions</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotional expression recognition using support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimodal Interfaces</title>
		<meeting>International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emotion recognition using a hierarchical binary decision tree approach</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1162" to="1171" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature selection from huge feature sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phone recognition with the mean-covariance restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="127" to="135" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for human activity recognition using mobile sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Mengshoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Computing, Applications and Services (MobiCASE), 2014 6th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="197" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for human activity recognition with smartphone sensors</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ronao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks on multichannel time series for human activity recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3995" to="4001" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gadaleta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idnet</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03238</idno>
		<title level="m">Smartphone-based gait recognition with convolutional neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bizzego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Furlanello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01865</idno>
		<title level="m">Convolutional neural network for stereotypical motor movement detection in autism</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning human identity from motion patterns</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barbello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1810" to="1820" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for time-series modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Längkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A conceptual framework for integrated analysis of environmental quality and quality of life</title>
		<author>
			<persName><forename type="first">E</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Barrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reyes-Paecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schlink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kabisch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ecolind.2014.06.002</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1470160X14002532" />
	</analytic>
	<monogr>
		<title level="j">Ecological Indicators</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="664" to="668" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Others</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<ptr target="http://tensorflow.org/" />
		<title level="m">{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="https://software.intel.com/en-us/iot/hardware/edison" />
		<title level="m">Intel Edison kernel description</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learninig of eeg signals for emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mehmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m">IEEE International Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
