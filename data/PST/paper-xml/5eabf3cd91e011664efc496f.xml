<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows</title>
				<funder ref="#_SCHbTz8">
					<orgName type="full">Samsung Research Funding Center of Samsung Electronics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ipoom</forename><surname>Jeong</surname></persName>
							<email>ipoom.jeong@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seihoon</forename><surname>Park</surname></persName>
							<email>seihoon.park@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Electronics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><surname>Woo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA47549.2020.00039</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic Scheduling</term>
					<term>Register Renaming</term>
					<term>Memory Disambiguation</term>
					<term>Energy Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance gap between in-order (InO) and out-of-order (OoO) cores comes from the ability to dynamically create highly optimized instruction issue schedules. In this work, we observe that a significant amount of performance benefit of OoO scheduling can also be attained by supplementing a traditional InO core with a small and speculative instruction scheduling window, namely SpecInO. SpecInO monitors a small set of instructions ahead of a conventional InO scheduling window, aiming at issuing ready instructions behind longlatency stalls. Simulation results show that SpecInO captures and issues 62% of dynamic instructions out of program order.</p><p>To this end, we propose a CASINO core microarchitecture that dynamically and speculatively generates OoO schedules with near-InO complexity, using CAScaded IN-Order scheduling windows. A Speculative IQ (S-IQ) issues an instruction if it is ready, or otherwise passes it to the next IQ. At the last IQ, instructions are scheduled in program order along serial dependence chains. The net effect is OoO scheduling via collaboration between cascaded InO IQs. To support speculative execution with minimal cost overhead, we propose a novel register renaming technique that allocates free physical registers only to instructions issued from the S-IQ. The proposed core performs dynamic memory disambiguation via an on-commit valuecheck by extending the store buffer already existing in an InO core. We further optimize energy efficiency by filtering out redundant associative searches performed by speculated loads. In our analysis, CASINO core improves performance by 51% over an InO core (within 10 percentage points of an OoO core), which results in 25% and 42% improvements in energy efficiency over InO and OoO cores, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In-order (InO) cores have been widely used due to their low complexity and high energy efficiency, but have provided limited performance. This is because the nature of InO scheduling leads to missed opportunities for exploiting instruction-level parallelism (ILP) and memory-level parallelism (MLP). Out-of-order (OoO) cores have overcome these limitations by dynamically constructing data dependence graphs for all in-flight instructions and reordering them according to the availability of source operands and execution resources. To support OoO scheduling, today's processors are equipped with the wakeup/select and memory disambiguation schemes. It is well known that these schemes account for a large portion of OoO core's power dissipation, because the issue queue (IQ) and load/store unit (LSU) are comprised of heavily multi-ported content addressable memories (CAMs) and random access memories (RAMs), and accessed multiple times by each instruction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>For decades, researchers have tried to make an OoO core more energy efficient by addressing the complexity of the scheduling logic <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> or reducing the accesses to powerhungry structures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Recently, an alternative approach has been introduced to provide high performance while keeping energy consumption low: Enabling an InO core pipeline to perform OoO execution. This approach can be broadly categorized into two groups based on how they generate OoO schedules. The first group leverages the observation that an OoO core spends most of the time to produce the same schedules repeatedly <ref type="bibr" target="#b8">[9]</ref>. Therefore, an InO pipeline could achieve near-OoO performance by memoizing issue schedules created by an OoO execution engine, and replaying the stored schedules on an InO engine for future iterations <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These schemes achieve significant performance improvements, but cannot be implemented without the OoO engine. Furthermore, it is not trivial to correctly replay the memoized schedules on the InO engine because such schedules are generated assuming register renaming, branch prediction, and memory disambiguation on the OoO engine.</p><p>The second group relies on slices <ref type="bibr" target="#b13">[14]</ref>, each of which is extracted along a serial dependence chain. Instructions in a performance-critical slice are scheduled in order but independently from the rest of the application using multiple parallel IQs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. This is effectively a limited form of OoO scheduling. However, the slice-based approaches could experience a severe slowdown because the various shapes and sizes of dependence chains may impede the exploitation of ILP and MLP in such parallel InO scheduling windows <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>To sum up, discovered architectures so far suffer from either high complexity or limited performance, since an InO pipeline alone cannot generate dynamic issue schedules optimized for the underlying microarchitecture. Furthermore, generated issue schedules are incapable of reacting to unexpected dynamic events (e.g., cache misses and branch mispredictions) on an InO execution engine, which accounts for non-negligible performance benefit of OoO execution <ref type="bibr" target="#b8">[9]</ref>.</p><p>To address these issues, we explore a different approach that a scheduling window, called SpecInO, speculatively issues ready-to-execute instructions by examining dynamic instructions in program order. In each cycle, SpecInO examines a few instructions younger than those stalling the issue at the head of the conventional InO IQ. If some ready-to-execute instructions are detected, they are issued immediately. Otherwise, SpecInO moves towards younger instructions. This key feature prevents SpecInO from stalling on pending long-latency operations, thereby allowing more instructions to be scheduled out of program order. The net result is that SpecInO effectively exposes both ILP and MLP, while appropriately reacting to the dynamic events, e.g., cache misses. Simulation results show that SpecInO improves the performance of an InO core by 49%.</p><p>To this end, we propose a CASINO core microarchitecture that dynamically generates OoO issue schedules optimized for the underlying InO pipeline by realizing the concept of SpecInO in a cost-and complexity-effective manner. Starting from a 2-wide stall-on-use InO core, we first split the IQ into two parts: a Speculative IQ (S-IQ) and a normal IQ. The S-IQ is a first-in-first-out (FIFO) queue that schedules readyto-execute instructions at its head. However, it does not stall by instructions belonging to a dependence chain involving long-latency operations; instead, it merely forwards such instructions to the IQ, where they are scheduled in program order. Therefore, instructions can be captured and issued out of order, as a result of the collaboration between two InO scheduling windows. To support speculative execution with minimal overheads, free physical registers are allocated only to instructions issued from the S-IQ. Also, memory operations are executed speculatively, via an energy-efficient memory disambiguation technique. The proposed method is built upon an on-commit value-check <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which eliminates the need for associative searches on the load queue (LQ) by delaying the validation of load speculation from the issue of older stores to the commit of the speculated load itself. This technique is further optimized in our design by reducing the associative searches on the store queue (SQ), at the issue and commit of loads, by tracking the outstanding store addresses using a simple counter array.</p><p>The contributions of this paper are as follows:</p><p>? We discover that most of the benefits of OoO scheduling can be achieved by examining a small set of instructions in order but not waiting for all prior data dependences to be resolved. Based on this observation, we design an energy-efficient core microarchitecture that dynamically and speculatively generates OoO schedules by using cascaded InO IQs. ? We propose cost-effective and energy-efficient register renaming and memory disambiguation schemes that are well-suited for the key design philosophy of the proposed instruction scheduling mechanism. ? We evaluate the proposed core microarchitecture via detailed simulations and prove that it can be a good alternative to a high-performance OoO processor (IPC within 10 percentage points with 42% higher energy efficiency). We also demonstrate that CASINO core can easily be scaled to wider issue designs, achieving near-OoO performance while maintaining complexity and energy efficiency close to InO designs.  In this section, we investigate InO and OoO scheduling schemes and the impact of ILP and MLP on performance. The core and memory subsystem configurations are described in Section V. To this end, we present the key intuition driving this work: The benefit of aggressive OoO scheduling can be achieved by supplementing speculative InO scheduling capability to an InO pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. In-Order Scheduling</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates how each scheduling scheme wakes up and issues instructions. We assume that i1, i5, i7, and i9 are ready to be issued and the corresponding execution resources (e.g., the functional units (FUs) or LSU) are available. Under InO scheduling (Figure <ref type="figure" target="#fig_0">1a</ref>), N oldest instructions are examined, where N refers to the issue width (set to two in this example). Our baseline, a stall-on-use InO core, stalls when an instruction with unavailable source operands reaches the head of the IQ. Therefore, only i1 can be issued; i2 and the following instructions should wait until i2's data dependences are resolved. The worst case is that i2 reads the value produced by a long-latency instruction, such as a cache-missing load. In this case, i5 cannot be issued until the memory request is serviced and all preceding instructions are issued. This restriction leads to missed opportunities to execute independent (but not contiguous) instructions in parallel, resulting in a considerably low performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Out-of-Order Scheduling</head><p>In each cycle, a fully OoO scheduling checks dependences between all in-flight instructions and issues readyto-execute instructions out of program order (i1 and i5 in Figure <ref type="figure" target="#fig_0">1b</ref>). By removing the constraints imposed by the sequential issue, an OoO core efficiently exploits ILP and MLP and achieves 68% higher performance than an InO core (Figure <ref type="figure" target="#fig_1">2</ref>). To support OoO execution as well as to preserve sequential program semantics, state-of-the-art OoO cores are equipped with the following architectural features which are typically implemented with complex and powerhungry structures <ref type="bibr" target="#b19">[20]</ref>. 1) Dynamic Scheduling: The primary goal of dynamic scheduling is to issue ready-to-execute instructions as soon as possible. After rename, instructions are dispatched to the instruction window, where they are executed out of order and committed in original program order. Such inflight instructions are scheduled by the IQ which consists of the wakeup logic, select logic, and payload RAM <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. The wakeup and select logic is responsible for examining the readiness of the source operands of inflight instructions and choosing candidates for issue. The payload RAM holds the information on instructions that are waiting to be scheduled. Throughout this paper, we assume the OoO IQ that consists of the CAM-type wakeup logic and prefix-sum circuit select logic with age matrix (oldest-first selection), without compaction circuit (random queue) <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>.</p><p>2) Register Renaming: To eliminate false dependences (i.e., Write-after-Write (WAW) and Write-after-Read (WAR)) while maintaining the precise state under dynamic scheduling, OoO cores have adopted the register renaming. The rename logic allocates a free physical register to every instruction having a destination operand. Source operands are renamed by reading the architectural-to-physical mappings in the register alias table (RAT). The register renaming is quite simple and effective, but requires a large number of physical registers (in general, equal to the sum of the number of architectural registers and in-flight instructions), to avoid dispatch stalls due to a lack of free physical registers.</p><p>3) Memory Disambiguation: Modern OoO cores reorder memory operations by speculatively issuing loads ahead of the older stores with unresolved addresses <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[27]</ref>. Memory disambiguation is supported by the LSU which consists of the LQ, SQ, and store buffer (SB). The LQ and SQ keep track of in-flight loads and stores, respectively, until they are ready to be committed. A committed store is located in SB until it is retired from the pipeline by updating the L1 data cache.</p><p>The LQ, SQ, and SB are implemented as FIFO CAM structures. Each LQ entry holds the target address of the corresponding load. Each SQ/SB entry holds the target address and data that will be written to the memory. When a load is issued, it accesses the L1 and at the same time searches SQ and SB using its target address to get the value from the most recent (in program order) store to the same address. If there is a match, the data in the SQ or SB is directly forwarded to the load, which eliminates the need to wait for all older stores to resolve target addresses and update memory. Otherwise, data from the L1 is used. When the address of a store is calculated, the LQ is searched by the store for matching younger and speculatively executed loads. If there is a match (i.e., memory order violation), such a prematurely-serviced load and all subsequent instructions are flushed from the pipeline and fetched again.</p><p>To summarize, the current state of the art, as well as a brute-force approach, is that the scheduling logic monitors dependences between all in-flight instructions and issues ready instructions at each cycle. This requires a large number of associative searches on the IQ and LSU, thereby accounting for the majority of OoO core's energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. InO + Speculative InO Scheduling</head><p>To bridge the performance and energy efficiency gaps between InO and OoO cores, we explore alternative scheduler designs by supplementing a conventional InO core with a small and speculative scheduling window, namely SpecInO. Rather than examining all in-flight instructions in a large scheduling window, SpecInO examines only a few instructions with the following key features: 1) If it finds ready-to-execute instructions, such instructions are issued immediately; 2) Otherwise, it moves forward to younger instructions. The net effect is that ready-to-execute instructions are issued as soon as possible, while the others are scheduled along the serial dependence chains that must be honored. As an illustrative example, in Figure <ref type="figure" target="#fig_0">1c</ref>, SpecInO examines [i4 -i5] and issues i5, not waiting for older non-ready instructions to be issued. Then, SpecInO moves beyond i5 to examine [i6 -i7]. Older non-ready instructions ([i2 -i4]) are scheduled in program order at the IQ head.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> demonstrates the performance potential of Spe-cInO, assuming that instructions are renamed properly and the architectural state is updated correctly. Each model is denoted as SpecInO[WS, SO], where Window Size (WS) is the number of instructions examined in a cycle, and Sliding Offset (SO) is the number of IQ entries SpecInO moves if it does not detect any instructions to issue. To tease apart the performance contributions from ILP and MLP, we allow SpecInO to issue either only non-memory instructions (Non-mem) or both memory and non-memory instructions (All Types). In these simulations, we make two major observations. First, SpecInO[2, 2] models show some performance improvements, but they are less than those of SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> models. This is because SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> has more opportunities for speculative issue. SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b1">2]</ref> slides too fast when it cannot find ready-to-execute instructions, even if the younger one becomes ready in the next cycle, which can be caught by SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Instructions exiting SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b1">2]</ref> should wait until they reach the IQ head, which significantly reduces the overall issue rate. Hereafter, we only consider SpecInO[2, 1] models.</p><p>The second observation is that exploiting MLP is essential to achieve near-OoO performance. Figure <ref type="figure" target="#fig_1">2</ref> shows that Spe-cInO (Non-mem) achieves 33% performance improvements by exploiting ILP beyond long-latency operations, but it still shows much less performance than OoO scheduling. This is because SpecInO (Non-mem) does not allow loads and stores to be issued speculatively, and thus instructions that directly/indirectly depend on them cannot be issued until their producers are issued at the IQ head and complete execution. In other words, disabling the speculative issue of memory operations impedes the exploitation of ILP. Removing this constraint provides an additional performance gain of 16 percentage points, which is within 11 percentage points of OoO scheduling.</p><p>The reasons for significant performance gains of SpecInO can be summarized as follows: 1) In general, the distance (i.e., the number of IQ entries) between an instruction leaving SpecInO without being issued and its producer in the IQ is short (0.8 entries on average). Therefore, such instructions have a high likelihood of being issued at the head of the IQ immediately after their producers complete execution; 2) SpecInO examines and issues instructions in order (i.e., from older to younger instructions), which implicitly performs the oldest-first scheduling scheme that usually shows better performance than others <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CASINO CORE MICROARCHITECTURE</head><p>This section presents a CASINO core microarchitecture with detailed architectural support for realizing SpecInO scheduling. We first briefly describe our baseline stall-on-use InO core. Then, we present low-cost, complexity-effective mechanisms enabling OoO scheduling while retaining precise exceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Stall-on-Use In-Order Core</head><p>After being fetched and decoded, instructions are dispatched to the IQ where they wait until all prior dependences are resolved. With a stall-on-use policy, the pipeline does not get stall by a long-latency instruction (e.g., cache-missing load); following instructions are continuously issued until the consumer of the value produced by such a long-latency instruction reaches the IQ head. Therefore, the stall-onuse policy effectively hides the long-latency operations by allowing the issue of independent instructions and OoO completion. The correct execution is maintained by the scoreboard (SCB) <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. The role of the SCB is twofold: 1) It retains precise exceptions by enforcing InO write back of instructions; 2) It keeps track of the state of each destination register and examines data dependences to allow independent instructions to be issued. A store committed from the SCB remains in the SB until it reaches the SB head. Then, the store is retired and its data is written to the L1 when the corresponding cache line has permission to be written. Hereafter, the term baseline refers to the baseline stall-on-use InO core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Realizing Speculative InO Scheduling</head><p>1) Speculative Issue: One complexity-effective way to realize SpecInO is to split the conventional IQ into two parts: a Speculative IQ (S-IQ) and a normal IQ (Figure <ref type="figure" target="#fig_0">1d</ref>). Instructions at the head of each queue are examined in order. However, the S-IQ does not wait for an instruction at its head to become ready. Instead, it merely passes such a non-ready instruction to the IQ <ref type="foot" target="#foot_0">1</ref> . In other words, the S-IQ acts as a filter for IQ; if an instruction is ready within the scheduling window of the S-IQ (i5 in Figure <ref type="figure" target="#fig_0">1d</ref>), it is issued immediately and not scheduled again in the IQ. If it is not ready (i4 in Figure <ref type="figure" target="#fig_0">1d</ref>), the instruction is passed to the IQ and scheduled in program order. In this way, instructions behind long-latency stalls can be issued from the S-IQ without waiting for all prior dependences to be resolved.</p><p>2) Register Renaming: The source and destination operands of dynamically reordered instructions need to be renamed to eliminate false dependences. A straightforward solution is allocating a new physical register to every destination operand, which guarantees correct data communication between instructions through the physical register file (PRF) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>. However, applying a conventional register renaming scheme requires a large number of physical registers, thereby incurring additional area and energy overheads. In this work, we take a different approach by leveraging the fact that instructions passed to the IQ are scheduled in program order: skipping the allocation of free physical registers to the instructions that are not speculatively issued. Such instructions are issued and executed one-by-one from the IQ, and therefore, there is no need to allocate new physical registers even though some of them update the same architectural register. WAW hazards caused by OoO completion can be removed by using the SCB in the baseline (Section III-C3). WAR dependences between instructions issued from the IQ and S-IQ are removed by allocating free physical registers to the instructions issued from the S-IQ.</p><p>3) Memory Disambiguation: To achieve near-OoO performance, it is essential to execute memory operations out of order (Section II-C). However, adopting a conventional hardware memory disambiguation technique is too costly and may increase power dissipation over a desired power budget. A simple way to address this issue is to wait for all previous stores to resolve the target addresses and then issue the following loads, thereby eliminating the need for associative LQ searches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, this approach could significantly degrade performance if an address-generating instruction (AGI) belongs to a dependence chain involving one or more long-latency instructions. In this work, we employ the on-commit value-check <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> that performs memory disambiguation without the LQ. The key insight in this approach is that the validation of load speculation does not need to be achieved through associative LQ searches by the stores being issued. Instead, the responsibility of validation is shifted to the speculated load itself; the load replays on commit by searching the SB and re-checks the loaded value. Note that, our baseline already has the SB  C. Details of CASINO Core Microarchitecture 1) Overview of CASINO Core Pipeline: Figure <ref type="figure" target="#fig_2">3</ref> represents the pipeline of CASINO core that is built upon a 2wide stall-on-use InO core. To support SpecInO scheduling, the IQ is divided into two separate pieces: an S-IQ and a normal IQ. The rename logic (including the RAT, free list, and recovery log) is newly added (black boxes and arrows). Some structures (e.g., the PRF, reoder buffer (ROB), PRF scoreboard, and SQ/SB) are extended from the baseline (gray boxes) to support the OoO issue and completion of instructions. Such structures are allocated when an instruction leaves the S-IQ. The operation of the front-end pipeline is the same as that of the baseline.</p><p>2) Register Renaming: Figure <ref type="figure">4</ref> illustrates how instructions are renamed at the S-IQ head, assuming SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. To provide a better understanding, we mark newly added features in our renaming scheme as black boxes and arrows. As discussed in Section III-B, CASINO does not assign a new physical register if an instruction is not eligible for speculative issue (i.e., not ready). Therefore, the proposed scheme should perform the scoreboard reads after the RAT reads, which may increase the critical path of register renaming. To address this issue, we assume that the register renaming proceeds in two stages, where stage 1 is composed of two sub-stages <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. The first half of stage 1 reads the architectural-to-physical mappings of source operands in the RAT (upper-left). The scoreboard reads are continued at the second half of stage 1 (upper-right). The new mappings of destination operands are written to the RAT at stage 2.</p><p>Destination operands are renamed based on the availability of source physical registers. For example, if I1 is not ready, a physical register currently mapped to its destination operand is reassigned as a destination tag (a black MUX). If I2 is not ready, its renaming is disabled because the sliding offset is set to 1; regardless of whether I1 is speculatively issued or not, I2 cannot exit the S-IQ if it is not ready. Then, I2 is renamed in the next cycle. If both I1 and I2 are ready, two free physical registers are assigned. To support back-to-back issue of dependent instructions, the source tags of current instructions are compared (in parallel to the scoreboard reads) with the destination tags of instructions in former group (not shown in the figure for brevity). If  <ref type="bibr">Figure 4</ref>: Register renaming in CASINO core matched and the former instruction(s) is a single cycle operation with available source operands, current instruction identifies its source operand(s) as ready.</p><p>3) Issue, Execute, and Write Back: In a cycle, up to two instructions are issued from the S-IQ and/or IQ. If more than two instructions are ready for issue, target instructions are selected by simple arbitration that gives higher priority to instructions from the IQ. This is because the oldestfirst scheduling scheme achieves better performance than other schemes due to the criticality of instructions <ref type="bibr" target="#b24">[24]</ref>, and instructions from the IQ are always older than those from the S-IQ. The selected instructions are issued to the FUs or LSU according to their types.</p><p>At the end of the execution, instructions write the results to different locations, according to their issue types. If an instruction was issued from the S-IQ, a result value is written to its destination physical register. On the other hand, an instruction issued from the IQ must not update the destination register since such a register might be shared by multiple instructions from the IQ. In this case, a result value is temporarily written to a small data buffer, which plays a similar role as the SCB in the baseline (Section III-A). A data buffer entry is allocated when an instruction is issued from the IQ and released when a corresponding instruction is committed from the ROB. A value kept in the buffer is directly forwarded to the PRF read stage when its consumer is issued. In this manner, WAW dependences are removed while not allocating free physical registers to the instructions passed to the IQ.</p><p>The availability of each physical register is maintained in the PRF scoreboard, which is extended from the baseline to hold a larger number of physical registers. Each PRF scoreboard entry holds the state of the corresponding physical register (a Ready bit, Issue bit, and Delay field) <ref type="bibr" target="#b32">[32]</ref>. When an instruction is issued, it resets the Ready bit, sets the Issue bit, and updates the Delay field with its execution latency. At every cycle, the Delay fields with the Issue bit set are decremented by one. When the Delay field becomes zero, the Ready bit is set. To cope with register sharing between instructions in the IQ, we add a ProducerCount field to each PRF scoreboard entry, indicating the number of instructions that have been mapped but not yet issued. Since instructions in the IQ are issued in program order, the last instance of a physical register is ready only when all instructions mapped to it complete execution. The ProducerCount is incremented each time the physical register is reassigned to an instruction steered to the IQ, and decremented at the issue of one of associated instructions. The Issue bit is set if the ProducerCount becomes zero. We employ a 4-entry data buffer and 2-bit ProducerCount, allowing up to four issued (but not committed) instructions and three instructions (in the IQ) per physical register, respectively. 4) Memory Disambiguation: CASINO has the SQ and SB implemented as a single CAM structure, where the SQ part and SB part are divided logically using three pointers: SQ tail, boundary between SQ head and SB tail, and SB head <ref type="bibr" target="#b18">[19]</ref>. This is a simple extension of the SB which already exists in the baseline. A store is dispatched to the SQ tail when it leaves the S-IQ. After being committed at the SQ head, it is located in the SB tail. Finally, it is retired to update the L1 at the SB head.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> describes how a load is issued and replayed in our design. When a load is issued, the SQ and/or SB are searched for an older store matching the load address. If there is a match, the load checks whether there exist unresolved stores younger than the matched store. If such unresolved stores exist, the load sets the sentinel (i.e., its location in the ROB <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[26]</ref>) on the oldest one among them. If there are no matched stores, the oldest unresolved store is set with the sentinel. In both cases, the location of the target store in the SQ is written to the load's ROB entry. The sentinel on a store can be replaced if the store is in the SQ and a new load trying to set the sentinel is younger than the previous one. At commit, the load searches the SB (from the SB tail, i.e., the nearest store, to the store that it sets the sentinel) to validate the speculation. If there is a match, memory order violation is detected, and such a prematurely-serviced load and all subsequent instructions are flushed from the pipeline and re-executed. Otherwise, the sentinel on the store is removed if the current load is the latest one setting the sentinel. A store at the SB head is allowed to be retired if there is no sentinel on it.</p><p>Load-load ordering is enforced by delaying the retirement of a remote store (conflicting with a load that is issued earlier than older non-performed loads) until the speculative load is committed <ref type="bibr" target="#b18">[19]</ref>. This eliminates the need for LQ searches while supporting total store ordering (TSO). To do this, a speculatively issued load sets sentinel to the corresponding cache line, and the cache line withholds acknowledgement for invalidation from a remote store until the sentinel is removed by the load at commit.</p><p>The on-commit value-check effectively removes the associative searches on the LQ. However, the number of searches on the SQ/SB increases because each load performs one additional search at commit to validate its speculation. Therefore, the SQ/SB becomes a large, associative, centralized structure, even though it consumes less energy than the conventional LSU. To address this issue, we leverage the observation in <ref type="bibr" target="#b33">[33]</ref> that (i) not only are memory order violations rare, (ii) but also many stores and loads do not even access the same address. Our key insight is that avoiding the order violation with an unresolved store requires a prediction, but with an already resolved store does not. Based on (i), we ignore the potential dependences between unresolved stores and a currently issued load (performance penalty will be negligible) and focus only on the apparent dependences between already resolved stores and a currently issued load. In other words, a load skips an SQ/SB search when it is assured that a store with a resolved matching address does not reside in the SQ/SB.</p><p>To do this, all the addresses of issued but not yet retired (i.e., outstanding) stores need to be maintained. However, buffering all these addresses is inefficient and requires CAM searches as well. Here we exploit (ii) to build a powerefficient storage that holds the information on outstanding stores, namely an Outstanding Store Counter Array (OSCA) (Fig. <ref type="figure" target="#fig_3">5</ref>). The OSCA is a hash table indexed by the lower bits of the memory addresses. Each OSCA entry is a saturating counter that holds the number of outstanding stores. An OSCA entry is incremented by the issued stores and decremented by the retired stores. When a load is issued, the corresponding counter is accessed, and a load skips an SQ/SB search if the counter value is zero. Even though the current counter value may not reflect older but not issued stores, memory order violations do not increase. This is because the role of the OSCA is to remove a load's SQ/SB search when it is assured that there are no outstanding stores with the same target address <ref type="bibr" target="#b34">[34]</ref>.</p><p>Another issue is the aliasing problem where operations with different target addresses access the same OSCA entry. Address aliasing can cause false positives that cause redundant SQ/SB searches. As pointed out in (ii), however, the matching addresses rarely appear, and thus the aliases between different stores also rarely occur. To mitigate the aliasing between different target addresses, we use a heuristic to find the optimal design point and configure the OSCA to hold 64 counters. Other implementation issues are discussed in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Commit of Instructions:</head><p>CASINO preserves the program semantics using the ROB that keeps track of the order of in-flight instructions, where each ROB entry ID is encoded by Buyuktosunoglu's method <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b18">[19]</ref>. ROB entries are allocated in program order when instructions leave the S-IQ, and updated out of order when they complete execution. At the ROB head, an instruction is committed by releasing the physical register previously allocated to its destination operand. Note that, in our renaming scheme, free physical registers are allocated only to the speculatively issued instructions. Therefore, an instruction scheduled from the IQ must not release the previously mapped (currently it mapped) physical register to retain the precise state. Even though the ROB is multi-ported memories, its complexity and power consumption are relatively low in modern designs, because the separate IQ and LSU eliminate the need for associative searches on the ROB.</p><p>If any exceptions or mis-speculations are detected, the recovery log is used to recover the RAT and PRF scoreboard and release the speculatively allocated physical registers. CASINO allocates free physical registers conditionally, and thus the recovery log needs to hold only a small set of register mappings. Therefore, the recovery process is completed in a few cycles, which is much faster than using mappings in the ROB. ProducerCount is recovered by sequentially dequeuing instructions in the IQ (younger than offending one). Based on our simulation, they are usually similar or less than mis-speculated instructions responsible for the RAT recovery (i.e., that are issued from the S-IQ), and thus they have negligible impact on the critical path of recovery. Meanwhile, instructions older than the offending instruction are committed one-by-one and speculated loads validate their speculations and reset the corresponding sentinels. If an offending instruction is a mis-speculated load, this step is not required because it is the oldest in-flight instruction. Finally, all the sentinels in the SB are cleared. Until the offending instruction reaches the ROB head, additional sentinels are not allowed to be set <ref type="bibr" target="#b18">[19]</ref>. The recovery of the OSCA is handled differently because it holds both committed and uncommitted stores. When a recovery starts, stores younger than the offending instruction are dequeued one-by-one from the SQ tail, and corresponding counters are decremented. Since CASINO uses a moderate size SQ/SB (8 entries), allowing two stores (per cycle) to access the OSCA is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSIONS ON CASINO CORE</head><p>In this section, we address some issues that arise when implementing CASINO core microarchitecture.</p><p>1) Register Renaming: Our proposed renaming scheme uses less physical registers than a conventional scheme. Thus, it is implemented with the smaller-sized RAT, PRF, and PRF scoreboard. For the PRF scoreboard, the number of ports and overall accesses do not increase because the availability of source operands needs to be examined in each cycle, even though a conventional scheme is employed.</p><p>CASINO core uses the data buffer to temporarily store the results of instructions issued from the IQ, i.e., a data buffer entry is maintained from issue to commit of such an instruction. This is much shorter than the lifetime of a physical register in a conventional scheme, where it must be maintained from rename to redefinition (commit of younger instruction updating the same destination operand). Since instructions passed to the IQ generally belong to long dependence chains, the interval from rename to issue would be long. Shorter lifetime indicates larger effective size, resulting in a more efficient use of storage space.</p><p>2) Store Queue/Buffer: In our design, a load encounters one of the following situations: 1) Being issued from the S-IQ, a load first looks up the OSCA. If the corresponding counter value is not zero, it searches SQ/SB and sets the sentinel, if necessary; 2) Being issued from the IQ, it looks up the OSCA and searches the SB part according to the counter value. In this case, the load does not need to set the sentinel because all the prior stores have already been issued; 3) Reaching the ROB head, it searches the SB and removes the sentinel which it has set. If there is a match in the target address before the sentinel, the pipeline is flushed.</p><p>We need a modification in the SQ/SB structure to perform searches only on one part, either an SQ part or an SB part. This can be implemented by selecting the target part before actually performing the comparisons <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>. Also, we need oldest-first and youngest-first select logic to find the youngest store with a matched address and the oldest store with unresolved address, respectively <ref type="bibr" target="#b26">[26]</ref>. Finally, the results from the select logic are compared to decide whether to set the sentinel or not. For more details, see <ref type="bibr" target="#b18">[19]</ref>. The proposed filtering mechanism allows some loads to skip searches on the SQ/SB. In this case, only unresolved stores are searched to set the sentinel, through a 1-bit Resolved flag per SQ entry that indicates whether the address of the corresponding store is resolved or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Outstanding Store Counter Array:</head><p>To keep track of outstanding stores correctly, the issue of a store must stall when the corresponding OSCA entry is saturated. This stall may cause substantial performance degradation as well as a deadlock situation; a store at the IQ head cannot be issued even after commit, if the corresponding counter is saturated by the succeeding stores issued from the S-IQ. As this store prevents the subsequent stores from being retired, the corresponding counter cannot be decremented forever. To address this issue, we configure the size of each counter to log 2 (SQ+SB entries) bits, which allows a counter to hold all outstanding stores.</p><p>Unaligned memory accesses are handled by indexing the OSCA using a range where a target address resides. With 4-byte range, for example, memory accesses with lower bits [0,1,2,3] access the same OSCA entry (shift-right two). Therefore, unaligned accesses within 4-byte range can be caught. If a store data is larger than 4 bytes or unaligned, consecutive OSCA entries are updated. Similarly, a load checks consecutive entries for larger or unaligned data. In practice, unaligned memory accesses rarely occur, and thus the effectiveness of the OSCA would not be undermined. Since the OSCA is a small, direct-mapped, tagless SRAM, it can be accessed within a cycle. Therefore, we believe that the two sequential accesses to the OSCA and SQ/SB can be conducted within the data cache access latency (4 or more cycles in modern processors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL METHODOLOGY</head><p>In the evaluation, we use an execution-driven, cycle-level x86 processor simulator, Multi2Sim <ref type="bibr" target="#b38">[38]</ref>, which is heavily modified to implement and evaluate the proposed design. The memory system is modeled by integrating a DDR4 DRAM simulator, Ramulator <ref type="bibr" target="#b39">[39]</ref>, with the processor simulator. The architectural parameters of the evaluated models are listed in Table <ref type="table" target="#tab_3">I</ref>. For a fair comparison, we equally set the width of the pipeline and the size of the scheduling window of all evaluated models. Cache and memory systems are also configured to be the same since we focus primarily on revealing the implications of various scheduling techniques and the potential of the proposed design while minimizing interference from other architectural features. Other parameters are set considering trade-offs between performance and area/energy overheads. Further architectural exploration for wider issue machines is presented in Section VI-F.</p><p>We evaluate our design by running 12 SPECint and 13 SPECfp applications from the SPEC CPU2006 benchmark suite <ref type="bibr" target="#b40">[40]</ref>. For each application, we pick up the most representative region of 300 million instructions using the Sim-Point methodology <ref type="bibr" target="#b41">[41]</ref>. A simulation is performed using the reference input set after a warm-up phase of 300 million instructions. To estimate energy consumption and chip area overhead, we use modified versions of McPAT <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref> and CACTI 6.5 <ref type="bibr" target="#b44">[44]</ref>, considering only core components excluding L2 cache, main memory, and interconnection networks. We model the data paths and structures at the 22nm technology node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND ANALYSIS</head><p>A. Performance 1) Comparison to InO and OoO: Figure <ref type="figure" target="#fig_4">6</ref> shows the performance of Load Slice Core (LSC) <ref type="bibr" target="#b14">[15]</ref>, Freeway core <ref type="bibr" target="#b15">[16]</ref>, CASINO core, and OoO core, normalized to that of an InO core. CASINO achieves significant performance gains across all applications over InO (51% on average and a maximum of 89% in cactusADM). This is due to the ability of CASINO to exploit both ILP and MLP by speculatively issuing any type of instruction. The performance of CASINO is within 10 percentage points of OoO. This performance disparity comes from OoO's ability to issue instructions at any location in the IQ, while CASINO examines only those within scheduling windows at the heads of the S-IQ and IQ. Note that, CASINO performs instruction scheduling without power-consuming wakeup and select operations as well as associative searches on the LQ. Moreover, free physical registers are allocated only to the speculatively issued instructions, and thus the required number of physical registers is reduced to a large extent (36% fewer physical registers are used in this experiment). In h264ref, CASINO ConV <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b13">14]</ref> ConD <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b13">14]</ref> ConV <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b24">24]</ref> Counts  shows slightly better performance than OoO. This is because h264ref consists of a large number of loads and stores which intricately depend on each other. Therefore, memory order violations are detected frequently in OoO even though it uses a memory dependence predictor <ref type="bibr" target="#b45">[45]</ref>. On the other hand, memory operations in CASINO are examined sequentially at each head of the S-IQ and/or IQ. A memory order violation occurs only when a load is issued from the S-IQ while an older store with (actually) the same target address resides in the IQ, which is relatively rare in our design.</p><p>2) Comparison to LSC and Freeway: Figure <ref type="figure" target="#fig_4">6</ref> also presents the performance of slice-out-of-order (sOoO) cores, the most relevant prior works that aimed at energy-efficient MLP exploitation by extending an InO core. In these experiments, we use 32-entry IQs with unlimited other resources to examine the potential of sOoO cores. LSC <ref type="bibr" target="#b14">[15]</ref> extracts backward instruction slices that end with loads or stores by using an iterative backward dependence analysis. Instructions identified as belonging to slices are dispatched to a bypass queue (B-IQ) and issued in program order, but independently from instructions at the main queue (A-IQ). By issuing potentially cache-missing loads early, multiple memory accesses can be overlapped, and therefore their latencies are hidden behind the stall of the first miss. All AGIs are issued strictly in order from the B-IQ, and thus the memory order violations never occur. On average, LSC achieves 28% higher performance than InO by scheduling instructions in a memory slice earlier than others.</p><p>In some cases, however, LSC may lose opportunities to extract MLP when consecutive slices have a dependence relationship, and a dependent slice blocks the issue of younger, independent slices. To address this limitation, Freeway <ref type="bibr" target="#b15">[16]</ref> introduces a dependence-aware slice scheduling policy. In Freeway, dependent slices, which have at least one instruction that depends on a load of an older slice, are dispatched to a yielding queue (Y-IQ). Therefore, slices in the B-IQ can be issued without stalls caused by inter-slice dependences. CASINO shows 17% better performance than Freeway that achieves 34% performance gain over InO. Two key features contribute to the relative benefit of CASINO. First, CASINO speculatively issues instructions regardless of their types, i.e., AGIs or not. Therefore, CASINO exploits both ILP and MLP by utilizing the cascaded IQs. Note that, even though sOoO cores primarily focus on the exploitation of MLP, the early issue of AGIs benefits from some amount  of ILP as a positive side-effect. The second feature is that the memory disambiguation technique in CASINO allows ready-to-execute loads to be issued beyond older unresolved loads and stores, which contributes to the additional performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Register Renaming</head><p>The S-IQ finds a ready instruction behind stalls by passing prior non-ready instructions to the IQ. With conventional renaming scheme, instructions cannot be passed if there are no free physical registers. Therefore, a lack of free physical registers may undermine the advantage of the speculative issue capability of CASINO. Increasing the size of PRF is not an attractive way considering area and energy overheads. We address this problem by skipping the allocation of a physical register to instructions passed to the IQ. Figure <ref type="figure" target="#fig_7">7a</ref> shows performance and physical register allocation counts per cycle when we apply the conventional (ConV) and conditional (ConD) renaming schemes to CASINO using [N integer, M floating-point] physical registers. Being equipped with 32 integer and 14 floating-point registers, ConD allocates 27% fewer registers than ConV in each cycle, which contributes to the increase of both speculative (Sp) and normal issue rates as depicted in Figure <ref type="figure" target="#fig_7">7b</ref>. The reason is that, at the S-IQ head, instructions with unavailable source(s) can be passed to the IQ without consuming registers and more instructions with available sources can be issued speculatively using the registers saved by the passed instructions. The net result is a 10% increase in overall issue rate and an 6% improvement in performance by applying a conditional renaming scheme. ConV <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b24">24]</ref> shows similar performance gain with ConD <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b13">14]</ref>, which indicates that the proposed renaming scheme increases the effective size of the PRF by 57%. On average, 65% of dynamic instructions are issued from the S-IQ with the proposed renaming scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Memory Disambiguation</head><p>Figure <ref type="figure" target="#fig_9">8a</ref> and Figure <ref type="figure" target="#fig_9">8b</ref> represent the LSQ-related activity counts, performance, and energy efficiency of CASINO with various memory disambiguation schemes. Presented values are normalized to that of Fully OoO with 16-entry LQ that has been adopted in conventional OoO cores. When we force AGIs to be issued in program order at the S-IQ head (AGI Ordering), LQ reads (R), writes (W), and associative searches (S) are eliminated. However, performance  decreases by 11% compared to Fully OoO, because some AGIs belonging to dependence chains with long-latency operations cause the S-IQ stalls. By applying on-commit value-check (NoLQ) <ref type="bibr" target="#b18">[19]</ref>, performance improves slightly over Fully OoO, due to the elimination of LQ-causing stalls while scheduling memory operations out of order. However, as pointed out in Section III-C4, the SQ becomes more power-hungry structure due to increasing associative searches by the speculated loads (31%), resulting in only 6% improved energy efficiency. Finally, we supplement NoLQ with the OSCA and allow a load to bypass the search on the SQ when the corresponding counter value is zero. This scheme reduces the SQ searches of NoLQ by 70%, resulting in an additional energy efficiency gain of 5 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Area Overhead and Energy Consumption</head><p>Figure <ref type="figure" target="#fig_11">9a</ref> shows the core area of InO, CASINO, and OoO, where the height of each stacked bar represents the total core area relative to the baseline. We consider only core components, excluding uncore components such as the L2 and main memory. Our estimation using CACTI <ref type="bibr" target="#b44">[44]</ref> indicates that the total area overhead of CASINO over InO is 5%, which is very conservative considering significant performance improvements. The area-normalized performance (i.e., performance/area) of CASINO is 43% and 16% better than InO and OoO, respectively.</p><p>Figure <ref type="figure" target="#fig_11">9b</ref> shows energy consumption, where each stacked bar indicates the total energy consumption of a target model (sum of static and dynamic energy). Even for some extended or added structures, significant performance improvements and low complexity overhead of CASINO lead to substantially lower energy consumption. CASINO consumes 22% more energy than InO, but 37% less than OoO. Applying on-commit value-check to OoO (OoO+NoLQ) reduces its energy consumption by 8%. Nevertheless, CASINO still consumes 31% less energy. As depicted in Figure <ref type="figure" target="#fig_11">9a</ref> and Figure <ref type="figure" target="#fig_11">9b</ref>, the structures for OoO execution in CASINO (i.e., the S-IQ and LSU) take much less area and energy than those in OoO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Design Space Exploration</head><p>In this section, we explore the design space of a CASINO core by assessing the performance benefits with various IQ sizes and the speculative issue policies.  The size of the IQ determines how far ahead SpecInO can examine and issue instructions from a longlatency instruction. Therefore, determining the optimal IQ size is a crucial design choice. On the other hand, the S-IQ only buffers dispatched instructions waiting to be scheduled by SpecInO, and thus its size has a negligible impact on performance. Figure <ref type="figure" target="#fig_13">10a</ref> shows the breakdown of committed instructions, issued from either the S-IQ (S-Issue) or the IQ (Issue), and performance with various IQ sizes assuming SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> with unlimited other resources. As the IQ size increases, more non-ready instructions can be passed to the IQ, and thus the fraction of Issue increases. Accordingly, from 4 to 12 entries, performance improves as SpecInO finds ready instructions more aggressively. However, performance degrades with the IQ lager than 12 entries because larger IQ does not always guarantee higher scheduling performance. Once an instruction enters the IQ, it cannot be scheduled until all prior instructions are issued, which otherwise would be issued from the S-IQ. Therefore, some instructions prematurely lose the opportunities to be issued from the S-IQ.</p><p>2) SpecInO Configuration: With larger WS (window size) and SO (sliding offset), ready instructions can be detected earlier in the S-IQ, but it does not always guarantee high performance while increasing implementation cost (e.g., ports of the RAT and PRF scoreboard) and power dissipation. As WS increases, the S-IQ may discover ready instructions early if they are within SpecInO window. In such cases, however, older non-ready instructions will be passed to the IQ where they wait to reach the IQ head, even though some of them become ready a few cycles later. In a similar vein, large SO sometimes limits the opportunities to find ready instructions since instructions leave the S-IQ prematurely, which otherwise would be scheduled by reading currently executed results. Figure <ref type="figure" target="#fig_13">10b</ref> illustrates the impact of WS and SO on performance. Note that, WS should be the same or larger than SO. Otherwise, some instructions might be passed to the IQ while not being examined by SpecInO. As WS and SO increase, performance reaches its pick around SpecInO[2, 1] and then decreases. Based on these results, we conclude that the optimal configuration of the S-IQ is SpecInO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Toward Wider Superscalar</head><p>Modern InO cores are usually implemented with 2-way issue width <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, and OoO cores are with 4 or more ways <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, based on their capability to exploit parallelism. In this section, we conduct experiments to further explore the CASINO's ability to exploit ILP and MLP.</p><p>Figure <ref type="figure" target="#fig_14">11</ref> shows the performance and energy efficiency (i.e., PER, performance/energy) of evaluated machines with the pipeline widths of Configured with a 2-way issue width, CASINO shows 25% and 42% higher energy efficiency than InO and OoO, respectively. This is due to the significant performance improvements with low complexity overhead, even though some structures are added or extended from InO. Also, the structures for OoO scheduling in CASINO (i.e., the S-IQ and LSU) consume much less energy than those in OoO. As the issue width increases, the performance of CASINO and OoO improves significantly, mainly due to their ability of OoO scheduling from a large pool of in-flight instructions. Nevertheless, the energy efficiency of CASINO and OoO decreases because the complexity of the rename logic and LSU grows rapidly. With a 4-way issue configuration, CASINO achieves 2.0? energy efficiency over OoO (18% less than InO), while providing performance within 13 percentage points of that of OoO (68% higher than InO). Considering a limited performance of InO and low energy efficiency of OoO, CASINO can be an attractive alternative to achieve both high performance and high energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>1) MLP Extraction: Modern high-performance superscalar processors use dynamic scheduling that generates OoO issue schedules between independent instructions, based on the availability of their source operands. In most cases, the unavailable source operands are directly or indirectly dependent on the cache-missing loads, and thus exploiting MLP plays a crucial role in achieving high performance. A large body of prior work has focused on extracting MLP on an OoO core, such as prefetching, runahead execution <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>, and speculative pre-execution <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b55">[55]</ref>. These techniques succeed in hiding long memory latencies, but are not suitable for an energy-constrained environment (e.g., mobile), due to the complexity of underlying OoO pipeline and additional software and/or hardware support. To address this issue, another work has proposed slice-based MLP exploitation techniques built upon an energy-efficient stall-on-use InO core <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>2) Energy-Efficient Dynamic Scheduling: To reduce the power consumption of dynamic scheduling, researchers have proposed to bring the concept of hybrid scheduling into the OoO pipeline. The key insight behind these works is that instructions belong to a single dependence chain (or a slice <ref type="bibr" target="#b13">[14]</ref>) cannot be executed in parallel, and thus, such instructions do not need to be scheduled by the complex wakeup and select logic. In <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b56">[56]</ref>, instructions dependent on long-latency operations are temporarily kept in a small buffer until their dependences are resolved. Later, such instructions are pushed back to the IQ and scheduled with minimum wakeup and select operations. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> steer instructions that would not benefit from OoO scheduling to the InO IQ(s), based on the availability and dependences of the source operands. Separate InO IQs in <ref type="bibr" target="#b6">[7]</ref> reduce stalls caused by the intermingling of dependence chains. However, their performance is still highly dependent on the shapes of dependence chains <ref type="bibr" target="#b16">[17]</ref>.</p><p>Another approach is filtering ready-to-execute instructions using an InO execution engine. "Flea-flicker" two pass pipelining <ref type="bibr" target="#b57">[57]</ref> employs two sequential InO back-ends to hide cache miss stalls. Ready-at-dispatch instructions are executed in the advance pipeline, but the others are deferred to the backup pipeline. FXA <ref type="bibr" target="#b5">[6]</ref> employs an InO execution unit (IXU) that consists of the FUs and a bypass network, in front of an OoO back-end to execute ready-at-dispatch instructions in an energy-efficient manner. CASINO takes a finer-grained approach using a single InO execution engine, and achieves the same goal while the other shared resources are slightly modified. Therefore, the duplication of execution resources and/or the OoO scheduling logic is not required.</p><p>Inspired by the repetitive behaviors of instruction traces <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref> and their issue schedules <ref type="bibr" target="#b8">[9]</ref>, replaybased instruction scheduling techniques have been proposed to achieve near-OoO performance with high energy efficiency <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The key idea is to store an instruction scheduling order generated by an OoO execution engine. In future iterations, an energy-efficient InO pipeline executes the instruction stream in the memoized order. There are also some opportunities for such instructions to bypass several pipeline stages (e.g., the fetch and decode stages) <ref type="bibr" target="#b12">[13]</ref>. In our proposed design, however, instruction scheduling is determined solely by the InO pipeline without the help of an OoO pipeline. Thus, our design significantly reduces the implementation cost and hardware complexity. Also, since prior work schedules dynamic instructions at a coarser granularity (as the unit of multiple basic blocks), their recovery penalties from unexpected events (e.g., cache misses, mis-speculations, and exceptions) are more significant than those of CASINO. These recovery penalties are non-negligible from the performance perspective <ref type="bibr" target="#b8">[9]</ref>.</p><p>3) Memory Disambiguation: Several memory disambiguation techniques have been proposed either to improve the accuracy of load speculation <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b65">[65]</ref> or to reduce the number of power-consuming associative searches to the LSQ <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b66">[66]</ref>, <ref type="bibr" target="#b67">[67]</ref>. Another approach is to eliminate the LQ or SQ, as well as the associative searches on these structures <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b69">[69]</ref>. We adopt the latter approach because it can easily be implemented by extending the existing SB in the baseline InO core. Furthermore, we relieve the pressure on the centralized SQ by using a small counter array, based on the observation that memory order violations rarely occur <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b66">[66]</ref>.</p><p>4) PRF Management: To relieve the pressure on the PRF, researchers have proposed various register renaming schemes which can be broadly categorized into two groups: 1) delaying the allocation of physical registers from rename to write back <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b71">[71]</ref>, and 2) releasing short-lived registers as soon as their values are fetched to all consumers <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b75">[75]</ref>, <ref type="bibr" target="#b76">[76]</ref>. Recently, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b78">[78]</ref> go one step further by leveraging physical register sharing for a register consumed only once. For its consumer, such a register is reassigned as a destination register. CASINO performs a different type of register renaming for the same purpose; skipping register allocation for instructions scheduled in program order, and thus multiple instructions in the IQ naturally share a single physical register without performance degradation. In addition, the proposed renaming scheme is an attractive approach for an energy-constrained environment, because it requires neither a prediction mechanism nor checkpointed PRF <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b75">[75]</ref>, <ref type="bibr" target="#b76">[76]</ref>, <ref type="bibr" target="#b77">[77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>By leveraging the key insight that the performance benefit of OoO scheduling can be attained by supplementing a conventional InO design with the capability of speculative issue, this paper proposes a CASINO core microarchitecture. A CASINO core dynamically and speculatively creates OoO schedules by using the cascaded InO IQs, thereby achieving OoO scheduling with near-InO complexity. The proposed renaming scheme effectively eliminates false dependences using only a few physical registers. We also present a memory disambiguation technique that is implemented by extending the SB in the baseline. Our evaluation shows that more than half of dynamic instructions are issued from the S-IQ, resulting in substantial improvement in performance over the baseline InO core, which is comparable to the aggressive OoO design in some cases. A comprehensive design space exploration leads us to conclude that a CASINO core is a good candidate for a high-performance OoO core.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of instruction scheduling schemes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance potential of SpecInO scheduling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CASINO core microarchitecture overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Load/Store unit in CASINO core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: IPC comparison between LSC [15], Freeway [16], CASINO, and OoO normalized to the baseline InO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Sp-N-mem Mem N-mem (b) Rate of instruction issue</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effectiveness of conditional register renaming</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effectiveness of memory disambiguation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Area and energy consumption</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Configura?on of S-IQ [WS, SO](b) WS and SO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Impact of speculative issue policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Evaluation on wider-issue designs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I :</head><label>I</label><figDesc>Core and Memory Subsystem Configurations</figDesc><table><row><cell>Parameters</cell><cell>InO</cell><cell>CASINO</cell><cell>OoO</cell></row><row><cell>Core</cell><cell cols="3">2-wide superscalar @ 2GHz</cell></row><row><cell>Pipeline depth</cell><cell>7 stages</cell><cell>9 stages</cell><cell>9 stages</cell></row><row><cell>Issue queue</cell><cell cols="2">16 entries 4(S-IQ)/12(IQ)</cell><cell>16 entries</cell></row><row><cell>Load queue</cell><cell>-</cell><cell>-</cell><cell>16 entries</cell></row><row><cell>Store queue/buffer</cell><cell>4 entries</cell><cell>8 entries</cell><cell>8 entries</cell></row><row><cell>Physical register file</cell><cell>-</cell><cell cols="2">32 INT, 14 FP 48 INT, 24 FP</cell></row><row><cell>Instruction window</cell><cell>4-entry SCB</cell><cell>32-entry ROB</cell><cell>32-entry ROB</cell></row><row><cell>Functional units</cell><cell cols="3">2 integer ALUs, 2 FP units, 2 AGUs</cell></row><row><cell></cell><cell></cell><cell cols="2">TAGE branch predictor:</cell></row><row><cell>Branch predictor</cell><cell cols="3">17-bit GHR with one bimodal and</cell></row><row><cell></cell><cell cols="3">four tagged predictors, overall 32 KiB.</cell></row><row><cell>BTB</cell><cell cols="3">512 sets, 4-way set associative</cell></row><row><cell>L1 Inst. cache</cell><cell cols="3">32 KiB, 8-way, 4-cycle latency</cell></row><row><cell>L1 Data cache</cell><cell cols="3">32 KiB, 8-way, 4-cycle latency</cell></row><row><cell>Unified L2 cache</cell><cell cols="3">1 MiB, 16-way, 11-cycle latency with stride-based prefetcher</cell></row><row><cell>Main memory</cell><cell cols="3">4 GiB, DDR4 DRAM, 2400 MT/s, 1 channel, 1 rank</cell></row><row><cell cols="3">sufficient to hide this recovery latency.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>3 and 4.  As the pipeline width increases, the sizes of the ROB, IQ, LSQ, and PRF are doubled (i.e., quadrupled at 4-way issue designs). In CASINO, one (3way) or two (4-way) 8-entry S-IQs are inserted between the existing S-IQ and IQ. Ready instructions can be issued at the head of any IQ. Each of the S-IQ and IQ are configured with SpecInO<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Conditional register renaming is disabled in these experiments because instructions can be issued from one of the intermediate S-IQs, after being renamed at the head of the first S-IQ.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Based on our simulation, if the source operands are ready, waiting for unavailable resources (e.g., physical registers, FUs, and cache lines) at the S-IQ head achieves better performance than simply passing such instructions to the IQ.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work was supported by <rs type="funder">Samsung Research Funding Center of Samsung Electronics</rs> under Project Number <rs type="grantNumber">SRFC-IT1801-04</rs>. <rs type="person">W. W. Ro</rs> is the corresponding author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SCHbTz8">
					<idno type="grant-number">SRFC-IT1801-04</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Runtime power monitoring in high-end processors: Methodology and empirical data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 36th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy-effective issue logic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Folegnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
		<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="230" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CMOS VLSI design: A circuits and systems perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Weste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing power requirements of instruction scheduling through dynamic allocation of multiple datapath resources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kucuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 34th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="90" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long term parking (LTP): Criticality-aware resource allocation in OOO processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 48th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="334" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A front-end execution architecture for high energy efficiency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shioya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="419" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FIFOrder MicroArchitecture: Ready-aware instruction scheduling for OoO processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="716" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficiently scaling out-oforder cores for simultaneous multithreading</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Sleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual International Symposium on Computer Architecture</title>
		<meeting>the 43rd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="431" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discerning the dominant out-of-order performance advantage: Is it speculation or dynamism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcfarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DynaMOS: Dynamic schedule migration for heterogeneous cores</title>
		<author>
			<persName><forename type="first">S</forename><surname>Padmanabha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 48th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="322" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The heterogeneous block architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fallin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 32nd International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="386" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting instruction level parallelism in processors by caching scheduled groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Execution cache-based microarchitecture for power-efficient superscalar processors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Talpes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Program slicing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Software Engineering</title>
		<meeting>the 5th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="439" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The load slice core microarchitecture</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Allam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Freeway: Maximizing MLP for slice-out-of-order execution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dependence-based scheduling revisited: A tale of two baselines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Salverda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Annual Workshop on Duplicating, Deconstructing, and Debunking</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two techniques to enhance the performance of memory consistency models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 International Conference on Parallel Processing</title>
		<meeting>the 1991 International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The superfluous load queue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 51st Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Quantifying the complexity of superscalar processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wisconsin-Madison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences Department</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On pipelining dynamic instruction scheduling logic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 33rd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A high-speed dynamic instruction scheduling scheme for superscalar processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 34th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of issue queue delay: Banking tag RAM and identifying correct critical path</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 29th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="313" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance improvement by prioritizing the issue of the instructions in unconfident branch slices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 51st Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">40-entry unified outof-order scheduler and integer execution unit for the AMD Bulldozer x86-64 core</title>
		<author>
			<persName><forename type="first">M</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arekapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Solid-State Circuits Conference Digest of Technical Papers (ISSCC)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="80" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An oldest-first selection logic implementation for noncompacting issue queues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Moursy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International ASIC/SOC Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MLP-aware dynamic instruction window resizing for adaptively exploiting both ILP and MLP</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 46th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting ILP designs for throughput-oriented GPGPU architecture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Computer architecture: A quantitative approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RENO: A rename-based instruction optimizer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 32nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stage, pipelined register renaming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Safi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veneris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Very Large Scale Integration Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1926" to="1931" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Morphcore: An energy-efficient microarchitecture for high performance ILP and high throughput TLP</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 45th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing design complexity of the load/store queue</title>
		<author>
			<persName><forename type="first">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 36th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Checkpoint processing and recovery: Towards scalable large instruction window processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 36th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="423" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Use of selective precharge for low-power content-addressable memories</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1788" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Energy-efficient instruction dispatch buffer design for superscalar processors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kucuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kogge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 international symposium on Low power electronics and design</title>
		<meeting>the 2001 international symposium on Low power electronics and design</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="237" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Content-addressable memory (CAM) circuits and architectures: A tutorial and survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pagiamtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikholeslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of solid-state circuits</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi2Sim: A Simulation Framework for CPU-GPU Computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ubal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>of the 21st International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ramulator: A fast and extensible dram simulator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer architecture letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SPEC CPU2006 Benchmark Tools</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Spradling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 42nd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quantifying sources of error in mcpat and potential impacts on architectural studies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CACTI-P: Architecture-level modeling for srambased structures with advanced leakage reduction techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
		<meeting>the International Conference on Computer-Aided Design</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="http://www.arm.com/products/processors/cortex-a/cortex" />
		<title level="m">Cortex A9 processor</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cortex-A53 is ARM&apos;s next little thing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="12" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cortex-A57 extends ARM&apos;s reach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bolaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inside 6th-generation Intel core: New microarchitecture codenamed Skylake</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>-Y. Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mandelblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahatekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Big.LITTLE processing with ARM Cortex-A15 &amp; Cortex-A7</title>
		<author>
			<persName><forename type="first">P</forename><surname>Greenhalgh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note>ARM White paper</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving data cache performance by pre-executing instructions under a cache miss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on Supercomputing</title>
		<meeting>the 11th international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
		<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SPEAR: A hybrid model for speculative pre-execution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gaudiot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Parallel and Distributed Processing Symposium</title>
		<meeting>the 18th International Parallel and Distributed Processing Symposium</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Design and evaluation of a hierarchical decoupled architecture</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Despain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gaudiot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="259" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A large, fast instruction window for tolerating cache misses</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Beating in-order stalls with &quot;flea-flicker&quot; two-pass pipelining</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 36th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">387</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Trace cache: A low latency approach to high bandwidth instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 29th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Energy and performance improvements in microprocessor design using a loop cache</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1999 IEEE International Conference on Computer Design: VLSI in Computers and Processors</title>
		<meeting>1999 IEEE International Conference on Computer Design: VLSI in Computers and Processors</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="378" to="383" />
		</imprint>
	</monogr>
	<note>Cat. No. 99CB37040</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynamic speculation and synchronization of data dependences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Streamlining inter-operation memory communication via data dependence prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 30th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Memory dependence prediction using store sets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Symposium on Computer Architecture</title>
		<meeting>the 25th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Speculation techniques for improving load related instruction scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Symposium on Computer Architecture</title>
		<meeting>the 26th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scalable store-load forwarding via store queue index prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 38th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Software-hardware cooperative memory disambiguation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scalable hardware memory disambiguation for high ILP processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 36th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">399</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Store Vulnerability Window (SVW): Re-execution filtering for enhanced load optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 32nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="458" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Memory ordering: A valuebased approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International Symposium on Computer Architecture</title>
		<meeting>the 31st Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">NoSQ: Store-load communication without a store queue</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 39th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Virtual-physical registers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1998 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Delaying physical register allocation through virtual-physical registers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Monreal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vi?als</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="186" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cherry: Checkpointed early resource recycling in out-oforder microprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 35th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A group-commit mechanism for ROB-based processors implementing the X86 ISA</title>
		<author>
			<persName><forename type="first">F</forename><surname>Afram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Increasing processor performance through early register release</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ergin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Design: VLSI in Computers and Processors</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
	<note>ICCD 2004. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Address-value decoupling for early register deallocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sharkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 International Conference on Parallel Processing (ICPP&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Compiler directed early register release</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ergin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A novel register renaming technique for out-of-order processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Arnau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tubella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">OverCome: Coarsegrained instruction commit with handover register renaming</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1802" to="1816" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
