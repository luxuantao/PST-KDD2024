<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence Level Contrastive Learning for Text Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-12">12 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shusheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
							<email>xizhang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
								<address>
									<country>Shanghai China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence Level Contrastive Learning for Text Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-12">12 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.03481v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document summarization is the task of rewriting a long document into a shorter form while still preserving its important content, which requires the model to understand the entire document. Many approaches for summarization has been explored in the literature and the most popular ones are extractive summarization and abstractive summarization <ref type="bibr" target="#b16">(Nenkova and McKeown 2011)</ref>. Summaries in their nature are abstractive. The summaries generated by extractive summarization methods are usually long and redundant, which bring bad reading experience. Therefore, we focus on abstractive summarization in this paper. Abstractive summarization is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem <ref type="bibr" target="#b21">(Sutskever, Vinyals, and Le 2014)</ref>, where a document is viewed as a sequence of words and its summary another sequence of words <ref type="bibr">(Nallapati et al. 2016)</ref>.</p><p>Although abstractive models have been more and more powerful due to recent introduction of large pre-trained Transformers <ref type="bibr" target="#b14">(Liu and Lapata 2019;</ref><ref type="bibr" target="#b18">Raffel et al. 2020;</ref><ref type="bibr" target="#b5">Dong et al. 2019</ref>; <ref type="bibr" target="#b13">Lewis et al. 2020)</ref>, the training paradigm for abstractive models is still not changed, which is to minimize the negative log-likelihood (NLL) between the model predicted word distributions and the gold summary. One great property of the summarization task is that a document and its summary should convey the same meaning, which is not modeled explicitly by the NLL loss.</p><p>In computer vision, contrastive learning methods for unsupervised image representation learning advanced the stateof-the-art in object detection and image segmentation <ref type="bibr" target="#b9">(He et al. 2020b)</ref>. The key idea is to minimize distances (or maximize similarities) between feature representations of different views of the same image (positive examples), while to maximize the distances between feature representations of views of different images (negative examples) <ref type="bibr" target="#b9">(He et al. 2020b;</ref><ref type="bibr">Chen et al. 2020)</ref>. As mentioned earlier, in summarization a document and its summary should convey the same meaning. Therefore, we view a document, its gold summary and its model generated summaries as different views of the same meaning representation and during training, we maximize the similarities between them. To achieve that, we propose SeqCo (as shorthand for Sequence Level Contrastive Learning), which is based on contrastive learning. In addition to the gold summaries, we also use the dynamically generated summaries from our model during training to increase the diversity of inputs to SeqCo. In text summarization, an abstractive summarization model needs to first encode the document and then generate the summary. The contrastive objective in SeqCo tries to map representations of a document and its summary (or generated summary) to the same vector space, which intuitively helps the generation of summaries. Specifically, a document may contain distinct (or unnecessary) information from its summary. During training time, the contrastive objective between the document and summary actually encourages the model to encode important (and necessary) information from the document, otherwise the distance between the representations of document and summary will be large (the objective updates model parameters to make it small). Intuitively, the capability of encoding important information from documents would help to generate better summaries.</p><p>In experiments, we find our proposed contrastive learning based model SeqCo consistently improves upon a strong ab-stractive summarization model based on BART <ref type="bibr" target="#b13">(Lewis et al. 2020</ref>) across three different summarization datasets (i.e., CN-N/DailyMail <ref type="bibr" target="#b10">(Hermann et al. 2015)</ref>, New York Times <ref type="bibr" target="#b19">(Sandhaus 2008)</ref> and XSum <ref type="bibr" target="#b16">(Narayan, Cohen, and Lapata 2018)</ref>). Human evaluation also shows that our model SeqCo achieves better faithfulness ratings compared to its counterpart without contrastive objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most popular paradigms for summarization are extractive and abstractive based approaches. We focus on abstractive summarization. Abstractive summarization may add new words or phrases when generating summaries, which is usually viewed as a sequence to sequence learning problem <ref type="bibr">(Nallapati et al. 2016;</ref><ref type="bibr" target="#b20">See, Liu, and Manning 2017;</ref><ref type="bibr" target="#b16">Paulus, Xiong, and Socher 2018;</ref><ref type="bibr" target="#b8">Gehrmann, Deng, and Rush 2018)</ref>. Probably because small and shallow LSTM (Hochreiter and Schmidhuber 1997) based attentive seq2seq models <ref type="bibr" target="#b21">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b1">Bahdanau, Cho, and Bengio 2015)</ref> without pre-training are not powerful enough to model documents. Quality of summaries produced by these mdoels are not satisfactory <ref type="bibr" target="#b14">(Liu and Lapata 2019)</ref>. As the recent introduction of large pre-trained transformer models <ref type="bibr" target="#b14">(Liu and Lapata 2019;</ref><ref type="bibr" target="#b5">Dong et al. 2019;</ref><ref type="bibr" target="#b28">Zou et al. 2020;</ref><ref type="bibr" target="#b13">Lewis et al. 2020;</ref><ref type="bibr" target="#b26">Zhang et al. 2020;</ref><ref type="bibr" target="#b18">Raffel et al. 2020)</ref>, abstractive models are greatly improved. Best results for summarization are achieved by finetuning large models pre-trained with generation (or summarization) tailored objectives on huge amount of unlabeled text (≥160G). <ref type="bibr" target="#b5">Dong et al. (2019)</ref> pre-train jointly designed Transformer encoder and decoder with language model and masked language model objectives. <ref type="bibr" target="#b26">Zhang et al. (2020)</ref> predict gapped sentences from a document removing these sentences and <ref type="bibr" target="#b13">Lewis et al. (2020)</ref> propose sentence permutation and text infilling tasks to pre-train seq2seq transformers. There is also some work on combining extractive and abstractive summarization models <ref type="bibr" target="#b9">(He et al. 2020a;</ref><ref type="bibr" target="#b14">Dou et al. 2021)</ref> or multiple summarization models <ref type="bibr" target="#b14">(Liu, Dou, and Liu 2021)</ref>. Unfortunately, pre-training transformers from scratch or combining multiple summarization systems are expensive, while our model can be applied to the light-weighted finetuning stage.</p><p>Convoluational neural networks pre-trained with contrastive learning methods advance the state-of-the-art in object detection and image segmentation in computer vision <ref type="bibr" target="#b9">(He et al. 2020b</ref>). The idea is to minimize the distances be- These works above suggest that using a large number of negative examples is crucial to obtain good performance, which also increases the complexity for implementation. There is also an interesting line of work without using negative examples. <ref type="bibr" target="#b1">Caron et al. (2020)</ref> employ online clustering to assign codes for two views of the same image and then use representation of one view to predict the cluster codes of the other. During the training of BYOL <ref type="bibr" target="#b9">(Grill et al. 2020)</ref>, they only minimized the distance between representations of two views of the same image and they use a momentum encoder for the target view to stabilize the training. <ref type="bibr" target="#b3">Chen and He (2020)</ref> find that even the momentum encoder can be removed, although there might be a small drop in performance. The contrastive learning method used in our model is most related to BYOL <ref type="bibr" target="#b9">(Grill et al. 2020)</ref> in the sense that we do not use negative examples either and we also employ a momentum encoder. In the models above, contrastive learning is applied in the unsupervised pre-training stage, which create different views of the same image by using effective data argumentation methods. In this paper, we take advantage of the nature of the summarization task and use the document, gold summary, and generated summary as different views of the same meaning representation (note that a summary is a shorter form of the original document). To fit sequence-tosequence learning models for text generation, we handles two sequence of embeddings of discrete words, while the vision models handle two single embeddings of fixed dimensions. In addition, the generated summary are created dynamically during training with a model, which are more diverse than using non-model-based approaches in vision tasks.</p><p>In NLP, previously contrastive learning methods are mostly used in pre-training or natural language understanding tasks. For example, word2vec <ref type="bibr" target="#b15">(Mikolov et al. 2013</ref>) learns the word embeddings by distinguishing words in a windows (positive examples) w.r.t. the current word and words randomly sampled (negative examples) using negative sampling. <ref type="bibr" target="#b12">(Iter et al. 2020</ref>) propose a contrastive learning based method for language model pre-training, which predicts the relative distance between sentences using randomly sampled sentences as negative examples. More recently, MatchSum <ref type="bibr" target="#b27">(Zhong et al. 2020)</ref> formulates extractive summarization as a semantic text matching problem using contrastive learning. <ref type="bibr" target="#b24">Wu et al. (2020)</ref> measures the summary qualities without reference summaries by contrasting the document with the summaries using a ranking model. GSum <ref type="bibr" target="#b14">(Dou et al. 2021)</ref> takes different kinds of external guidance as additional input to the document and advances summarization performance significantly. SimCLS <ref type="bibr" target="#b14">(Liu and Liu 2021)</ref> proposes a contrastive based framework for abstractive summarization, which trains a model to rerank the candidate summaries of an abstractive model. We add constrastive learning to the training of an abstractive model by enforcing similarities between document, summary and generated summary, which does not need negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we describe our contrastive learning model SeqCo (as shorthand for Sequence Level Contrastive Learning) for abstractive text summarization. We first introduce abstractive text summarization models (i.e., Seq2Seq model), on which our model is based. Then we present SeqCo, which adapts contrastive learning to the sequence-to-sequence learning setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Abstractive Text Summarization</head><p>For text summarization, we can view the document as a long sequence of tokens<ref type="foot" target="#foot_0">1</ref> and the summary as a short sequence of tokens. Let X = (x 0 = &lt;s&gt;, x 1 , x 2 , . . . , x |X| = &lt;/s&gt;) denote a document (i.e., the long sequence of tokens) and Y = (y 0 = &lt;s&gt;, y 1 , y 2 , . . . , y |Y | = &lt;/s&gt;) its summary (i.e., the short sequence of tokens), where &lt;s&gt; and &lt;/s&gt; are begin and end of sequence tokens. We predict Y one token at a time given X. We adopt the Transformer model <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>, which is composed of an encoder Transformer and a decoder Transformer. Specifically, the encoder Transformer maps X into a sequence of hidden states E = (e 0 , e 1 , . . . , e |X| ).</p><formula xml:id="formula_0">E = Trans E (X)<label>(1)</label></formula><p>Supposing that the first t − 1 tokens y 1:t−1 have been generated and we are generating y t . The decoder Transformer computes the current hidden state o t by self attending to the encoder hidden states E and proceeding tokens y 0:t−1 .</p><formula xml:id="formula_1">o t = Trans D (y 0:t−1 , E)<label>(2)</label></formula><p>Note that during training, we can obtain</p><formula xml:id="formula_2">O = (o 1 , ..., o |Y |) in parallel. O = Trans D (Y, E)<label>(3)</label></formula><p>The probability of y t can be estimated using a linear projection and a softmax function</p><formula xml:id="formula_3">p(y t |y 0:t−1 , X) = softmax(W o o t )<label>(4)</label></formula><formula xml:id="formula_4">L NLL = − 1 |Y | |Y | t=1 log p(y t |y 0:t−1 , X)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SeqCo: Sequence Level Contrastive Learning for Text Summarization</head><p>In text summarization, the summary Y is a shorter form of the input document X and they should convey the same meaning.</p><p>Therefore, X and Y should be close in the semantic space at least after certain types of transformations. However, a</p><p>Seq2Seq model is trained using the negative log-likelihood loss (see Equation ( <ref type="formula" target="#formula_4">5</ref>)) and there is no explicit modeling for the similarity between X and Y . Further, during the training phase, given X as input, the model can also generate output sequences from its distribution by either beam search or sampling. Let Ŷ denote one sample the model generated from X. Intuitively, Ŷ should also be similar to both X and Y . As shown in figure <ref type="figure" target="#fig_1">1</ref>, we enforce the similarities between X, Y and Ŷ during model training. To do this, we propose SeqCo, which is a contrastive learning based model for text summarization.</p><p>Contrastive learning methods are proposed in the context of self-supervised learning for image representations <ref type="bibr" target="#b25">(Wu et al. 2018;</ref><ref type="bibr" target="#b9">He et al. 2020b;</ref><ref type="bibr" target="#b1">Caron et al. 2020;</ref><ref type="bibr" target="#b9">Grill et al. 2020;</ref><ref type="bibr" target="#b3">Chen and He 2020)</ref> Inspired by <ref type="bibr" target="#b9">Grill et al. (2020)</ref> and <ref type="bibr" target="#b3">Chen and He (2020)</ref>, we propose a model that does not need negative examples. In the following, we first define similarity measures between sequences and then we present how to equip the similarity measures into our training objective.</p><p>Sequence Representation Suppose that we have two sequences S i = (w i 0 , w i 1 , w i 2 , ..., w i |Si| ) and S j = (w j 0 , w j 1 , w j 2 , ..., w j |Sj | ). S i and S j are two sequences, which we will maximize their similarity in Eq. 15. For example, S i and S j can be a document X and its gold summary Y, or document and generated summary, or gold summary and generated summary, just like Fig. <ref type="figure" target="#fig_3">2</ref>. Before going to the similarity computation, we first convert them into sequences of hidden representations. We designed two mapping functions here. The first one (f E θ ) is unconditional, which reuses the encoder of our Seq2Seq model (see Section 3.1):</p><formula xml:id="formula_5">f E θ (S i ) = g(Trans E (S i ))<label>(6)</label></formula><p>where Trans E (•) is the Transformer encoder described in Equation (1) and g(•) is a feed-forward network that is used to give more freedom for encoding S i . Here we use θ to denote the parameters in f E θ (•). The second mapping function (f D θ ) is conditional, which takes of the input sequence into account.<ref type="foot" target="#foot_1">2</ref> Let X denote the input sequence and S i is its gold output sequence or a sequence generated by the Seq2Seq model. In this mapping function, we employ both the encoder and the decoder of the Seq2Seq model (see Section 3.1 for details):</p><formula xml:id="formula_6">f D θ (S i ) = g(Trans D (S i , Trans E (X)))<label>(7)</label></formula><p>where Trans E (•) and Trans D (•) are the Transformer encoder and decoder described in Equation ( <ref type="formula" target="#formula_0">1</ref>) and (3). As mentioned earlier, g(•) is a feed-forward network to give more freedom for encoding S i . In f D θ (•), we intend to use X as additional input to encode S i more accurately in vector space. During Sequence Similarity After defining the mapping functions, we are ready to compute sequence similarities. Without losing generality, let f θ denote the mapping function, where θ is the parameter of the function. Note that f θ can be either f E θ or f D θ (see Eq. ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_6">7</ref>) for details). We additionally employ another mapping function f ξ , which has the same architecture as f θ , but with parameter ξ. We obtain the representations of S i and S j by applying f θ and f ξ to them:</p><formula xml:id="formula_7">H i = (h i 0 , h i 1 , . . . , h i |Si| ) = f θ (S i ) H j = (h j 0 , h j 1 , . . . , h j |Sj | ) = f ξ (S j )<label>(8)</label></formula><p>To fully utilize the word-to-word interactions between the two sequences S i and S j , we apply a cross attention between H i and H j :</p><formula xml:id="formula_8">H i = MultiHeadAttn(H j , H i , H i ) (9)</formula><p>where MultiHeadAttn(•, •, •) is the multi-head attention module <ref type="bibr" target="#b23">(Vaswani et al. 2017</ref>) and H j , H i and H i are the query, key and value matrices, respectively. Note that the resulting H i and H j have the same size. The similarity between S i and S j is the averaged cosine similarities of all vectors with the same index:</p><formula xml:id="formula_9">sim(S i , S j ) = 1 |S j | + 1 |Sj | k=0 cos( h i k , h j k )<label>(10)</label></formula><p>We adopt multi-head attention (MHA) for similarity computation for two reasons. 1) The sequences (esp. documents) are long and MHA takes all pairs of tokens across two sequences into account, which is intuitively more powerful than [CLS] pooling based methods (will introduce below).</p><p>2) The two sequences we compare may have different lengths (e.g., a document v.s. a summary). MHA can convert the hidden states of one sequence to the same length as the hidden states of another sequence (see Equation <ref type="formula">9</ref>), which are easier to use for the similarity computation. Note that we can also define a simpler similarity function using the [CLS] pooling as in BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>:</p><formula xml:id="formula_10">sim(S i , S j ) = cos(q(h i 0 ), h j 0 ) (11)</formula><p>where q is a feed-forword network to project h i 0 following <ref type="bibr" target="#b9">Grill et al. (2020)</ref>. We obtained worse results using the similarity measure above (see Section 4.4 for details) and the measure also sometimes leads to numerical errors during training.</p><p>Training To make S i and S j closer, we can minimize the following loss:</p><formula xml:id="formula_11">L θ,ξ (S i , S j ) = 1 − sim(S i , S j ) (12)</formula><p>As mentioned earlier, f θ (the encoding function for S i ) and f ξ (the encoding function for S j ) use different set of parameters (i.e., θ and ξ). If we update the parameters in both f θ and f ξ simultaneously, the optimization maybe too easy, which may lead to collapsed solutions <ref type="bibr" target="#b9">(Grill et al. 2020</ref>). So we use f ξ to produce regression targets for f θ . Specifically, we do not update the parameters in f ξ during the optimization of the loss above and ξ is a moving average of θ:</p><formula xml:id="formula_12">ξ = τ ξ + (1 − τ )θ<label>(13)</label></formula><p>where τ ∈ [0, 1] is a hyper-parameter to control the extend of retaining ξ. This contrastive objective is demonstrated in figure 2. Note that L θ,ξ (S i , S j ) is not symmetric and we make the loss symmetric as follows:</p><formula xml:id="formula_13">L sim (S i , S j ) = L θ,ξ (S i , S j ) + L θ,ξ (S j , S i )<label>(14)</label></formula><p>Hence, θ in f θ will have more chances to be updated. As mentioned earlier, the encoding function f θ can be either f E θ or f D θ . We use L E sim to denote the loss function using f E θ and L D sim to denote the loss function using f D θ . To enforce the similarities between the document X, its gold summary Y and one of the model generated summary Ŷ , we employ the following loss function as our final training loss<ref type="foot" target="#foot_2">3</ref> :</p><formula xml:id="formula_14">L =L NLL + λ x−y L E sim (X, Y ) + λ x−ŷ L E sim (X, Ŷ ) + λ y−ŷ L E sim (Y, Ŷ ) + λ D y−ŷ L D sim (Y, Ŷ )<label>(15)</label></formula><p>This objective contains five terms. L NLL is the negative loglikelihood; L D sim is the similarity loss w.r.t. (Y, Ŷ ) with f D θ ; L E sim terms are the similarity losses with f E θ w.r.t. (X, Y ), (X, Ŷ ) and (Y, Ŷ ). λ x−y , λ x−ŷ , λ y−ŷ and λ D y−ŷ are weight hyper-parameters for the last four terms. We completely train the model end-to-end following this loss function and empirically find that using a single similarity loss works better than using multiple ones (see Section 4.4), which is also more efficient for training. For example, we can set λ x−ŷ = 1.0 and</p><formula xml:id="formula_15">λ x−y = λ y−ŷ = λ D y−ŷ = 0.</formula><p>When Ŷ is adopted, The model iteratively generates Ŷ by using the loss to update parameters and generating new Ŷ . Since Ŷ can not be perfect, iteratively generating Ŷ makes it change toward ground-truth summary and make the positive examples for contrastive learning more accurate and diverse. Since SeqCo is designed for the finetuning stage, and the model SeqCo based on (i.e., BART) is pre-trained with a denoising auto-encoding objective, it can naturally generate the sequence with the same meaning as the input even before fine-tuning in a specific dataset. In addition, enforcing the similarity of y and ŷ does not equals optimizing NLL, since the similarity loss is on sequence level while the NLL loss is on token level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we assess the preformance of our contrastive learning model on the task of text summarization. We will first introduce the datasets we used. Then we present our implementation details. Finally, we compare our model with multiple previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>CNNDM We conduct our experiments on three summarization datasets. The CNN/DailyMail dataset (CNNDM; Hermann et al. 2015) contains news articles and their associated highlights (i.e., reference summaries) from the CNN and Daily Mail websites. We follow the standard pre-processing steps in <ref type="bibr" target="#b20">(See, Liu, and Manning 2017)</ref>  XSum The articles in the XSum dataset <ref type="bibr" target="#b16">(Narayan, Cohen, and Lapata 2018)</ref> are from the BBC website with accompanying single sentence summaries, which are professionally written. We use the official splits of (Narayan, Cohen, and Lapata 2018) (i.e., 204,045 articles for training, 11,332 articles for validation and 11,334 articles for test).</p><p>All datasets are tokenized with the byte-pair encoding of GPT2 <ref type="bibr" target="#b17">(Radford et al. 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our model is initialized from BART Large <ref type="bibr" target="#b13">(Lewis et al. 2020)</ref>. Therefore, the size is identical with BART Large <ref type="bibr" target="#b13">(Lewis et al. 2020)</ref>. Specifically, the encoder and decoder are all 12-layer transformers with 16 attention heads, hidden size 1,024 and feed-forward filter size 4,096, which amounts to 406M trainable parameters. We also have additional component for contrastive learning. The feedforward network g (see Equation ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_6">7</ref>)) for projecting sequence features contains one hidden layer of 4,096 neurons with ReLU activation function. The multi-head attention module (see Equation ( <ref type="formula">9</ref>)) used to compute cross attention between sequences also has 16 heads. These two components above contribute to an extra 13M trainable parameters.</p><p>We optimize the model using Adam with β 1 = 0.9, β 2 = 0.999. Following <ref type="bibr" target="#b13">(Lewis et al. 2020)</ref>, we employ a linear schedule for the learning rate. We firstly warmup the model by increasing the learning rate linearly to a peak learning rate and then decrease the learning rate linearly to zero. The peak learning rate, warmup steps, total number of updates and batch size are tuned on validation sets and are different across datasets, which are 1000, 20000, 4e − 5, 128 on CNNDM, 500, 5000, 2e − 5, 64 on NYT, 500, and 15000, 6e − 5, 64 on XSum. In all datasets, the number of training epochs are between 5 to 10. During the optimization, parameters ξ in the online encoding function f ξ (see Equation ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_6">7</ref>)) are not updated. Parameters ξ inf ξ are updated following Equation ( <ref type="formula" target="#formula_12">13</ref>) with τ = 0.99. We employ label smoothing of 0.1 <ref type="bibr" target="#b22">(Szegedy et al. 2016;</ref><ref type="bibr" target="#b23">Vaswani et al. 2017)</ref>. The models for CNNDM are trained on 8 Tesla V100 GPUs, and the models for the other datasets are trained on 4 Tesla V100 GPUs. During decoding, we select minimum generated length and length penalty according to ROUGE scores on the validation set. Following (Paulus, Xiong, and Socher 2018), we also blocked repeated trigrams during beam search. Following <ref type="bibr" target="#b13">(Lewis et al. 2020)</ref>, the articles are truncated to 1024 tokens in both training and decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluations</head><p>We use ROUGE <ref type="bibr" target="#b14">(Lin 2004)</ref> to measure the quality of generated summaries. We reported full-length F1 based ROUGE-1, ROUGE-2 and ROUGE-L scores on CNNDM and XSum datasets. Following <ref type="bibr" target="#b7">(Durrett, Berg-Kirkpatrick, and Klein 2016)</ref>, we use the limited-length recall based ROUGE-1, ROUGE-2 and ROUGE-L on NYT, where generated summaries are truncated to the length of gold summaries. ROUGE scores are computed with the ROUGE-1.5.5.pl script<ref type="foot" target="#foot_4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We present our main results on the CNNDM dataset in Table 1. We compare our model against both extractive and abstractive systems. The first block summarizes the results for extractive systems. Lead3 is a baseline which simply takes the leading three sentences in a document as its summary. BERTEXT <ref type="bibr" target="#b14">(Liu and Lapata 2019)</ref> employs BERT as encoder and predicts whether a sentence is a summary. MatchSum <ref type="bibr" target="#b27">(Zhong et al. 2020)</ref> is the best performing extractive models, which formulates summarization as a semantic text matching problem using contrastive learning. The abstractive models are in the second block. PTGen (See, Liu, and Manning 2017) is a LSTM-based Seq2Seq model augmented with copy and coverage models. Large pre-trained language models mostly dominate summarization. BERTSUMEXTABS <ref type="bibr" target="#b14">(Liu and Lapata 2019</ref>) is an abstractive model with encoder initialized with BERT and decoder randomly initialized. UniLM <ref type="bibr" target="#b5">(Dong et al. 2019</ref>) is trained using language modeling and masked language modeling objectives. T5 <ref type="bibr" target="#b18">(Raffel et al. 2020)</ref>, PE-GASUS <ref type="bibr" target="#b26">(Zhang et al. 2020)</ref>, BART <ref type="bibr" target="#b13">(Lewis et al. 2020</ref>) and STEP <ref type="bibr" target="#b28">(Zou et al. 2020</ref>) pre-train Seq2Seq transformers using different unsupervised text-to-text tasks. PEGASUS <ref type="bibr" target="#b26">(Zhang et al. 2020</ref>) is trained by predicting gapped sentences (selected by some heuristics) in a document given the document with these sentences masked. Similar to BERTSUMEXTABS, the encoder of STEP is initialized from RoBERTa <ref type="bibr" target="#b15">(Liu et al. 2019)</ref>. BART + R3F <ref type="bibr" target="#b0">(Aghajanyan et al. 2021</ref>) applies a trust region theory based fine-tuning method to BART. Our model is based on BART and therefore we also re-implement BART (BART ). These models above are single models. We also present the results of recent combination models in the third block. CTRLsum (He et al. 2020a) and GSum <ref type="bibr" target="#b14">(Dou et al. 2021)</ref> combine a keywords extraction model (or an extractive model) with an abstractive model by taking the resulting keywords (or sentences) as additional input. SimCLS <ref type="bibr">(Chen et al. 2020)</ref> and Refsum <ref type="bibr" target="#b14">(Liu, Dou, and Liu 2021)</ref> train re-ranking models to rank multiple candidate summaries.</p><p>The fourth block includes results of our model SeqCo. As mentioned in Section 3.2, we can do contrastive learning between document and gold summary (i.e., SeqCo (λ x−y )), document and generated summary (i.e., SeqCo (λ x−ŷ )) as well as gold summary and generated summary (i.e., SeqCo (λ y−ŷ )). Note SeqCo (λ * − * ) means that λ * − * &gt; 0 and all the other λs equal to zero in Equation ( <ref type="formula" target="#formula_14">15</ref>) <ref type="foot" target="#foot_5">6</ref> . We can see that SeqCo (λ x−y ), SeqCo (λ x−ŷ ) and SeqCo (λ y−ŷ ) all outperform BART significantly (p &lt; 0.05) measured by the ROUGE script, which demonstrates the effectiveness of our proposed contrastive methods. SeqCo (λ y−ŷ ) outperforms all single models in comparison (first two blocks) and differences between them are significant w.r.t. the ROUGE script. We also observe that using generated summaries in contrastive learning leads to better performance (i.e., results of SeqCo (λ x−ŷ ) and SeqCo (λ y−ŷ ) are better), which is not surprising. Generated summaries are created dynamically during training and they might be more diverse than gold summaries.</p><p>It is also possible to employ multiple pairs of text for contrastive learning. Results on validation set with different combinations of text pairs are shown in Table <ref type="table">2</ref>. We obtain worse results with more than one pair of text in contrastive learning. Perhaps because the information learned using different pair of text is a bit redundant. We compared the results on the validation and test sets of the other two datasets and observed similar trends. <ref type="foot" target="#foot_6">7</ref> We find best results are achieved by using a single similarity loss on all datasets except for the validation set of XSum, where SeqCo (λ x−y + λ y−ŷ ) and SeqCo (λ x−y + λ x−ŷ + λ y−ŷ ) outperform SeqCo(x-y) slightly. Given the fact that adding one more similarity loss increases around 30% training time and the observations above, we recommend using a single similarity loss. We probably need to encourage the "disagreement" between them (we leave this for future work). As mentioned in Section 3.2, we can also use decoder based encoding function f D θ (see the SeqCo (λ D y−ŷ ) and SeqCo (λ y−ŷ ) rows in Table <ref type="table">2</ref>) and we obtain worse results. It may because influencing the decoding during contrastive training is too aggressive. Therefore, we only report results of contrastive models on single pair of text (i.e., SeqCo (λ x−y ), SeqCo (λ x−ŷ ) and SeqCo (λ y−ŷ )) on NYT and XSum. Again in Section 3.2, we propose to employ multi-head attention based similarity modeling (see Equation ( <ref type="formula">9</ref>) and ( <ref type="formula" target="#formula_9">10</ref>)) rather than [CLS] based method (see Equation ( <ref type="formula">11</ref>)). It also shows attention based similarity, which takes associations across two sequences into account, is better (see SeqCo (λ y−ŷ ) and SeqCo (λ y−ŷ ) w/ [CLS] rows in Table <ref type="table">2</ref>).</p><p>Results on NYT are shown in Table <ref type="table" target="#tab_2">3</ref> and the trend is similar. ROBERTA-S2S is a transformer based Seq2Seq model with encoder initialized from RoBERTa <ref type="bibr" target="#b15">(Liu et al. 2019</ref>) and its results are reported in <ref type="bibr" target="#b28">(Zou et al. 2020)</ref>. SeqCo (λ x−ŷ ) outperforms BART by +1.0 ROUGE-1, +0.8 ROUGE-2 and +1.0 ROUGE-L and the differences between them are significant measured by the ROUGE script. SeqCo (λ x−ŷ ) obtains better results than all models in comparison. We again ob-  serve that using generated summaries in SeqCo are better than using gold summaries only. Table <ref type="table" target="#tab_3">4</ref> summarizes our results on the XSum dataset. BART (our reimplementation) are better at ROUGE-1, but worse at ROUGE-2 and ROUGE-L compared to BART. Se-qCo (λ x−y ) outperforms BART significantly measured with the ROUGE script. Results of SeqCo (λ x−y ) are better than all previously published models except for PEGASUS (Huge-News) and Refsum. It is not entirely surprising, because PEGASUS (HugeNews) is trained on 3,800 GB news data (the same genre as the XSum dataset), while PEGASUS(C4) is pre-trained on the C4 dataset consist of text from 350M Web pages (750GB) and performs worse than PEGASUS (HugeNews). Refsum reranks outputs of PEGASUS (Huge-News). Note that the pre-trained transformer (i.e., BART) in SeqCo is trained on only 160 GB data, which also contains data in other domains rather than news data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We do human evaluations on CN-NDM, NYT and XSum with 100 documents each. We asked the participants to rank the outputs of different systems according to their faithfulness and the mean rank scores (lower is better) are shown in table 5. We employed (self-reported) native speakers to annotate our output summaries on Amazon Mechanical Turk. To further guarantee the annotation quality, we filter out the annotated assignments which were done less than two minutes (average time spent per assignment is 6 minutes). After the filtering process, we guarantee each document is annotated by three annotators. In CNNDM and NYT datasets, Seqco outperforms BART significantly. In XSum dataset, there are no significant differences among these systems. It may be because generated summaries in XSum are shorter, which are difficult for annotators to tell the differences. We calculate the ratios of agreement between annotators (i.e., ratio of all three annotators' agreement and ratios of at least two annotators' agreement) to measure the agreement for human evaluation. As shown in table <ref type="table" target="#tab_5">6</ref>, there are around 30% of summaries that all of 3 participants give the same annotations, and more than 90% of summaries obtained the same annotations by at least 2 annotators. In addition, the Fleiss' Kappa scores are 0.329 on CNNDM, 0.313 on NYT and 0.364 on XSum, which demonstrate a fair degree of agreement. We believe the agreement between annotators is reasonable. Analysis Different from CNNDM and NYT, why does using generated summaries in contrastive learning perform worse on XSum? As shown in Table <ref type="table" target="#tab_6">7</ref>, it may because XSum is more abstractive (see the novel ngram statistics of Gold on the three datasets) and more difficult. As a result, the generated summaries are easier to have different meanings from their documents and gold summaries (at least in the early stage of training). Maybe that is the reason why the x − ŷ and y − ŷ objective is worse than the x − y objective. CNNDM and NYT are less abstractive and the generated summaries could retain the main meanings more easily and are also more diverse (compared to gold summaries), which leads to the x − ŷ and y − ŷ objectives work better.</p><formula xml:id="formula_16">systems BART x − y x − ŷ y − ŷ CNNDM 2.</formula><p>We can also see from Table <ref type="table" target="#tab_6">7</ref> that SeqCo can either be more abstractive than BART or almost as abstractive as BART. To choose the contrastive objective, our suggestion is 1) for the datasets whose summaries are highly abstractive, choose the x − y pair as the contrastive objective; 2) for less abstractive datasets (the case for most datasets), choose either x − ŷ or y − ŷ as the contrastive objective. As far as we observed, the performance of x − ŷ and y − ŷ are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We list the ablation results on three datasets in the appendix A. We compared single similarity loss v.s. multiple similarity losses on the validation and test sets and observed the similar trends. We find best results are achieved by using a single similarity loss on all datasets except for the validation set of XSum, where SeqCo (λ x−y + λ y−ŷ ) and SeqCo (λ x−y + λ x−ŷ + λ y−ŷ ) outperform SeqCo(x-y) slightly. Given the fact that adding one more similarity loss increases around 30% training time and the observations above, we recommend using a single similarity loss.</p><p>Example Outputs Some example outputs of SeqCo and BART are also listed in appendix B. In conclusion, BART sometimes miss some important points, while SeqCo can do better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In text summarization, a document, its gold summary and model generated summaries can be viewed as different views of the same meaning representation. We propose SeqCo, a sequence level contrastive learning model for text summarization, which intends to minimize distances between the document, its summary and its generated summaries during training. Experiments on three summarization datasets (CNNDM, NYT and XSum) show that SeqCo consistantly improves a strong Seq2Seq text generation model. In the future, we plan to extend SeqCo in the multi-lingual or crosslingual text generation tasks. We observed in experiments that using multiple contrastive objectives did not improve the results. We are interested in developing methods for regularizing different contrastive objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article</head><p>Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she had been imprisoned was liberated -timing that showed how close the Jewish diarist had been to surviving the Holocaust. But new research released by the Anne Frank House shows that Anne and her older sister, Margot Frank, died at least a month earlier than previously thought. Researchers re-examined archives of the Red Cross, the International Training Service and the Bergen-Belsen Memorial, along with testimonies of survivors. They concluded that Anne and Margot probably did not survive to March 1945 -contradicting the date of death which had previously been determined by Dutch authorities. In 1944, Anne and seven others hiding in the Amsterdam secret annex were arrested and sent to the Auschwitz-Birkenau concentration camp. Anne Frank's final entry . That same year, Anne and Margot were separated from their mother and sent away to work as slave labor at the Bergen-Belsen camp in Germany. Days at the camp were filled with terror and dread, witnesses said. The sisters stayed in a section of the overcrowded camp with no lighting, little water and no latrine. They slept on lice-ridden straw and violent storms shredded the tents, according to the researchers. Like the other prisoners, the sisters endured long hours at roll call. Her classmate, Nannette Blitz, recalled seeing Anne there in December 1944: "She was no more than a skeleton by then. She was wrapped in a blanket; she couldn't bear to wear her clothes anymore because they were crawling with lice." Listen to Anne Frank's friends describe her concentration camp experience . As the Russians advanced further, the Bergen-Belsen concentration camp became even more crowded, bringing more disease. A deadly typhus outbreak caused thousands to die each day. Typhus is an infectious disease caused by lice that breaks out in places with poor hygiene. The disease causes high fever, chills and skin eruptions. "Because of the lice infesting the bedstraw and her clothes, Anne was exposed to the main carrier of epidemic typhus for an extended period," museum researchers wrote. They concluded that it's unlikely the sisters survived until March, because witnesses at the camp said the sisters both had symptoms before February 7. "Most deaths caused by typhus occur around twelve days after the first symptoms appear," wrote authors Erika Prins and Gertjan Broek. The exact dates of death for Anne and Margot remain unclear. Margot died before Anne. "Anne never gave up hope," said Blitz, her friend. "She was absolutely convinced she would survive." Her diary endures as one of the world's most popular books. Read more about Anne Frank's cousin, a keeper of her legacy . Table <ref type="table" target="#tab_0">11</ref>: An example document and its gold summary sampled from the test splitting of CNNDM along with the outputs of BART and SeqCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article</head><p>The Rev. Robert H. Schuller, California televangelist and founder of the television ministry "Hour of Power," died Thursday, according to his family. He was 88 years old. Schuller, also the founder of Crystal Cathedral megachurch, had been diagnosed with esophageal cancer in August 2013, a release from "Hour of Power" said. "My father-in-law passed away peacefully early this morning. He was a great Dad and a great man of God," said Schuller's daughter-in-law, Donna Schuller, in a Twitter message. Schuller's life followed an almost Shakespearean arc. He was born in a Iowa farmhouse without running water and longed to preach from his earliest days. In his autobiography, "Prayer: My Soul's Adventure with God," he described standing alone by a river and picturing himself delivering sermons to a rapt congregation. After attending a Hope College and Western Theological Seminary in Michigan, he met his wife of more than 60 years, Arvella, while preaching at her church (she was the organist). With their young family in tow, the Schullers caravanned west to California, where he rented a drive-in theater and preached from the roof of the snack bar. It was beneath the dignity of Christian ministry, some local pastors huffed. The "passion pits" where teenagers necked was no place for the gospel. Schuller was undeterred, and he quickly outgrew the drive-in. He called the explosive growth of his tiny congregation a "miracle," though his many mainstream critics had other names for it. His confident, breezy version of Christianity -too breezy, by some estimations -drew hordes of seekers and lapsed Christians who were put off by the hellfire fulminations of many post-War American preachers. Schuller sold a softer, gentler message, which borrowed heavily, he acknowledged, from the father of the feel-good gospel, Norman Vincent Peale. He preached not to convert or condemn people, but to encourage them, a sentiment he called "possibility thinking." People loved it. "Evangelicalism at its best wants to be innovative and reach people," said Timothy Larsen, a professor of Christian thought at Wheaton College in Illinois. "And Schuller was a master at that." "What he got right is that the gospel is good news," Larsen continued. "And he preached an uplifting message about personal transformation and uplift and hope." Some of Schuller's favored phrases, though, struck others as cornpone Christianity. "Turn your hurt into a halo?" said Randall Balmer, a professor of American religious history at Dartmouth College, citing one such phrase. "That's pretty weak tea." Still, Balmer gives Schuller some credit. "It may be bad theology, but it's brilliant marketing." In 1970, Schuller began broadcasting "Hour of Power," believed to be one of the first, if not the very first, Sunday service to be shown regularly on television. With his genial smile, priestly robes and gray hair, he looked and talked like a guy who wanted nothing more than to see his flock succeed. The show, which ran for decades, reached millions, making Schuller a televangelist before the term became tarnished by the sins of his many successors. Schuller's crowning achievement, at least architecturally, still stands in Orange County, California, though it is now owned by the Roman Catholic Church. The Crystal Cathedral, a great gleaming edifice with 10,000 glass panels, gave worshipers a look at the clouds that house the heavens, while Schuller preached in the pulpit below. The message was clear to many: The road to the former ran through the latter. During the 1980s and 1990s, Schuller's star continued to rise, with presidents stopping by the Crystal Cathedral -often during campaigns, it should be said -and future megachurch pastors like Rick Warren and Bill Hybels seeking his advice. As Schuller aged, though, his family was beset by a succession scandal straight from the pages of "King Lear." He tried to install his only son, Bobby Jr., as pastor of Crystal Cathedral. But the preaching styles of father and son were too different for the congregation -measured at times at 10,000 strong -to countenance. Bobby Schuller Jr. left "Hour of Power" and the pulpit at Crystal Cathedral after a short time. As the family searched for a new successor and tussled over finances, viewers and donations to the church and its television show dropped precipitously. Crystal Cathedral Ministries filed for bankruptcy in 2010, citing debts of more than $43 million, according to The Associated Press. Schuller's empire, which once soared as high as his glassy cathedral, had fallen to dust. Eventually, Schuller's grandson, also named Bobby, took over "Hour of Power," though at a different church. In a statement on Thursday, the younger Schuller recalled standing atop Crystal Cathedral's 12-story Tower of Hope with his grandfather as they surveyed the surrounding landscape. "You could see the whole world from there," he said. People we've lost in 2015 . CNN's Stella Chan reported from Los Angeles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Summary</head><p>The Rev. Robert Schuller , 88 , had been diagnosed with esophageal cancer in 2013 . His TV show , " Hour of Power , " was enormously popular in the 1970s and 1980s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BART</head><p>The Rev. Robert H. Schuller had been diagnosed with esophageal cancer in August 2013 . He was the founder of the television ministry "Hour of Power" and the Crystal Cathedral megachurch . He sold a softer, gentler message, which borrowed heavily from the father of the feel-good gospel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SeqCo</head><p>The Rev. Robert H. Schuller died Thursday at 88, his family says . He was the founder of the television ministry "Hour of Powe" and the Crystal Cathedral megachurch . He had been diagnosed with esophageal cancer in August 2013, the ministry says . Table <ref type="table" target="#tab_0">12</ref>: An example document and its gold summary sampled from the test splitting of CNNDM along with the outputs of BART and SeqCo.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>tween feature representations of different views of the same image (positive examples), while to maximize the distances between feature representations of views of different images (negative examples). To discriminate positive examples from negative examples, He et al. (2020b) maintain a queue of negative sample representations and utilize momentum updates for encoder of the queue to stabilize these representations. Chen et al. (2020) use other examples from the same batch as negative examples and as a result, they need a large batch size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We enforce the similarities between the document, gold summary and model generated summary.</figDesc><graphic url="image-1.png" coords="3,95.74,54.00,155.02,123.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. The training objective tries to make representations of different views of the same image closer (positive examples) while representations of views of different images apart from each other (negative examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The contrastive objective. S i and S j are two sequences to contrast, f θ and f ξ have the same architecture, θ in f θ is updated by gradient decent while ξ in f ξ is the moving average of θ.</figDesc><graphic url="image-2.png" coords="4,77.85,54.00,190.80,143.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>4 and the resulting dataset contains 287,226 articles for training, 13,368 for validation and 11,490 for test.NYT The New York Times dataset (NYT; Sandhaus 2008) is composed of articles published by the New York Times with summaries written by library scientists. Following the pre-processing procedures in<ref type="bibr" target="#b7">(Durrett, Berg-Kirkpatrick, and Klein 2016;</ref><ref type="bibr" target="#b14">Liu and Lapata 2019)</ref>, we first obtain 110,540 articles with abstractive summaries. The test set is constructed from the 9,706 articles published after January 1, 2007. After removing articles whose summaries are shorter than 50 words, the final test set contains 3,452 articles. The remaining 100,834 articles are filtered and splitted into 38,264 articles for training and 4,000 articles for validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Summary Museum : Anne Frank died earlier than previously believed . Researchers re-examined archives and testimonies of survivors . Anne and older sister Margot Frank are believed to have died in February 1945 . BART Anne Frank died of typhus in a Nazi concentration camp at the age of 15 in 1945 . The date of death had previously been determined by Dutch authorities . Researchers re-examined archives of the Red Cross, the International Training Service and the Bergen-Belsen Memorial . They concluded that Anne and Margot probably did not survive to March 1945 . SeqCo New research shows Anne Frank and her older sister, Margot Frank, died at least a month earlier than previously thought . Researchers re-examined archives of the Red Cross, the International Training Service and the Bergen-Belsen Memorial . They concluded that Anne and Margot probably did not survive to March 1945 -contradicting the date of death which had been determined .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the test split of CNNDM using full length F1 based ROUGE-1/2/L. means our own re-implementation.</figDesc><table><row><cell>Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell cols="2">Extractive</cell><cell></cell><cell></cell></row><row><cell>Lead3</cell><cell>40.34</cell><cell>17.70</cell><cell>36.57</cell></row><row><cell>BERTEXT (Liu and Lapata 2019)</cell><cell>43.85</cell><cell>20.34</cell><cell>39.90</cell></row><row><cell>MATCHSUM (Zhong et al. 2020)</cell><cell>44.41</cell><cell>20.86</cell><cell>40.55</cell></row><row><cell cols="2">Abstractive</cell><cell></cell><cell></cell></row><row><cell>PTGen (2017)</cell><cell>39.53</cell><cell>17.28</cell><cell>36.38</cell></row><row><cell>BERTSUMEXTABS (2019)</cell><cell>42.13</cell><cell>19.60</cell><cell>39.18</cell></row><row><cell>UniLM (Dong et al. 2019)</cell><cell>43.47</cell><cell>20.30</cell><cell>40.63</cell></row><row><cell>T5 (Raffel et al. 2020)</cell><cell>43.52</cell><cell>21.55</cell><cell>40.69</cell></row><row><cell>PEGASUS (C4)</cell><cell>43.90</cell><cell>21.20</cell><cell>40.76</cell></row><row><cell>PEGASUS (HugeNews)</cell><cell>44.17</cell><cell>21.47</cell><cell>41.11</cell></row><row><cell>STEP (Zou et al. 2020)</cell><cell>44.03</cell><cell>21.13</cell><cell>41.20</cell></row><row><cell>BART (Lewis et al. 2020)</cell><cell>44.16</cell><cell>21.28</cell><cell>40.90</cell></row><row><cell>BART (Lewis et al. 2020)</cell><cell>44.10</cell><cell>21.31</cell><cell>40.91</cell></row><row><cell>BART + R3F (2021)</cell><cell>44.38</cell><cell>21.53</cell><cell>41.17</cell></row><row><cell cols="2">Combination Methods</cell><cell></cell><cell></cell></row><row><cell>CTRLsum (He et al. 2020a)</cell><cell>45.65</cell><cell>22.35</cell><cell>42.50</cell></row><row><cell>GSum (Dou et al. 2021)</cell><cell>45.94</cell><cell>22.32</cell><cell>42.48</cell></row><row><cell>simCLS (Liu and Liu 2021)</cell><cell>46.67</cell><cell>22.15</cell><cell>43.54</cell></row><row><cell>Refsum (Liu, Dou, and Liu 2021)</cell><cell>46.12</cell><cell>22.46</cell><cell>42.92</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqCo (λx−y)</cell><cell cols="3">44.66 † 21.57* 41.38*</cell></row><row><cell>SeqCo (λ x−ŷ )</cell><cell cols="3">44.94 † 21.82 † 41.68 †</cell></row><row><cell>SeqCo (λ y−ŷ )</cell><cell cols="3">45.02 † 21.80 † 41.75 †</cell></row></table><note>SeqCo (λ x−y ), SeqCo (λ x−ŷ ) and SeqCo (λ y−ŷ ) stand for contrastive learning between document and gold summary, document and generated summary as well as gold and generated summary, respectively. * means outperforms BART significantly, † means outperforms best performing single model "BART+R3F" significantly (p &lt; 0.05). Models in "Combination Methods" employ multiple summarization models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the test split of NYT using limited-length recall based ROUGE. means our own re-implementation.</figDesc><table /><note>* means outperforms BART significantly (p &lt; 0.05).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on the test split of XSum using full length F1 based ROUGE. means our own re-implementation. * means outperforms BART significantly (p &lt; 0.05).</figDesc><table><row><cell>Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell cols="2">Extractive</cell><cell></cell><cell></cell></row><row><cell>Lead3</cell><cell>16.30</cell><cell>1.60</cell><cell>11.95</cell></row><row><cell>MATCHSUM</cell><cell>24.86</cell><cell>4.66</cell><cell>18.41</cell></row><row><cell cols="2">Abstractive</cell><cell></cell><cell></cell></row><row><cell>PTGen</cell><cell>28.10</cell><cell>8.02</cell><cell>21.72</cell></row><row><cell>BERTSUMEXTABS</cell><cell>38.81</cell><cell>16.50</cell><cell>31.27</cell></row><row><cell>ROBERTA-S2S</cell><cell>43.54</cell><cell>20.49</cell><cell>35.75</cell></row><row><cell>STEP (Zou et al. 2020)</cell><cell>43.02</cell><cell>20.11</cell><cell>35.34</cell></row><row><cell>PEGASUS (C4)</cell><cell>45.20</cell><cell>22.06</cell><cell>36.99</cell></row><row><cell>PEGASUS (HugeNews)</cell><cell>47.21</cell><cell>24.56</cell><cell>39.25</cell></row><row><cell>BART (Lewis et al. 2020)</cell><cell>45.14</cell><cell>22.27</cell><cell>37.25</cell></row><row><cell>BART (Lewis et al. 2020)</cell><cell>45.35</cell><cell>22.01</cell><cell>36.76</cell></row><row><cell cols="2">Combination Methods</cell><cell></cell><cell></cell></row><row><cell>GSum (Dou et al. 2021)</cell><cell>45.40</cell><cell>21.89</cell><cell>36.67</cell></row><row><cell>simCLS (Liu and Liu 2021)</cell><cell>47.61</cell><cell>24.57</cell><cell>39.44</cell></row><row><cell>Refsum (Liu, Dou, and Liu 2021)</cell><cell>47.45</cell><cell>24.55</cell><cell>39.41</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqCo (λx−y)</cell><cell cols="3">45.65* 22.41* 37.04*</cell></row><row><cell>SeqCo (λ x−ŷ )</cell><cell>45.6</cell><cell>22.36</cell><cell>36.94</cell></row><row><cell>SeqCo (λ y−ŷ )</cell><cell>45.52</cell><cell>22.24</cell><cell>36.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation on faithfulness with mean rank (lower is better). We randomly sample 100 documents for each dataset and asked the participants to rank the outputs of different systems according to their faithfulness. * means this result is significantly different (p &lt; 0.05) from BART.</figDesc><table><row><cell></cell><cell>62</cell><cell>2.51</cell><cell cols="2">2.45* 2.42*</cell></row><row><cell>NYT</cell><cell>2.68</cell><cell cols="3">2.46* 2.39* 2.46*</cell></row><row><cell>XSum</cell><cell>2.47</cell><cell>2.44</cell><cell>2.58</cell><cell>2.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The ratios of agreement between annotators.</figDesc><table><row><cell cols="2">Model 1-gram 2-gram 3-gram</cell></row><row><cell></cell><cell>CNNDM</cell></row><row><cell>Gold</cell><cell>0.1360 0.4871 0.6908</cell></row><row><cell cols="2">BART 0.0157 0.1140 0.2161</cell></row><row><cell cols="2">SeqCo 0.0228 0.1524 0.2769</cell></row><row><cell></cell><cell>NYT</cell></row><row><cell>Gold</cell><cell>0.1064 0.4260 0.6189</cell></row><row><cell cols="2">BART 0.0350 0.2231 0.3896</cell></row><row><cell cols="2">SeqCo 0.0368 0.2284 0.3961</cell></row><row><cell></cell><cell>XSum</cell></row><row><cell>Gold</cell><cell>0.3752 0.8328 0.9551</cell></row><row><cell cols="2">BART 0.2821 0.7341 0.8924</cell></row><row><cell cols="2">SeqCo 0.2929 0.7465 0.9015</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Proportions of novel n-grams w.r.t. original documents in gold and model generated summaries on the validation sets of CNNDM, NYT and XSum.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use tokens instead of words, because the sequence might be a sequence of sub-words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Note that in f D θ we only consider that Si and Si as the gold summary and the generated summary</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We can also use multiple generated summaries in training, we refrained to do so for efficiency reasons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Available at https://github.com/abisee/cnn-dailymail</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">with -c 95 -r 1000 -n 2 -a -m arguments</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">We tune λx−y, λ x−ŷ , λ y−ŷ ∈ {0.5, 1.0} on the validation set when &gt; 0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Detailed numbers are shown in Appendix.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablation Results</head><p>We list the ablation results on the validation and test set for three datasets in table 8, 9 and 10. We compared single similarity loss v.s. multiple similarity losses on the validation and test sets of the other two datasets and observed similar trends with CNNDM. We find best results are achieved by using a single similarity loss on all datasets except for the validation set of XSum, where SeqCo (λ x−y + λ y−ŷ ) and Se-qCo (λ x−y + λ x−ŷ + λ y−ŷ ) outperform SeqCo(x-y) slightly. Given the fact that adding one more similarity loss increases around 30% training time and the observations above, we recommend using a single similarity loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Examples</head><p>We list some examples of generated summaries and gold summaries in table 11 and 12 on the test set of CNNDM, where we can compare the outputs of BART and SeqCo. In conclusion, BART sometimes miss some important points, while SeqCo can do better.</p><p>In the document of Table <ref type="table">11</ref>, an important point is that Anne Frank and her older sister died earlier than previously believed. The output of BART doesn't mention this directly but describes two dates of their death, which is lack of the main idea and confusing. SeqCo points out this emphasis in the first sentence and then further explains, which is quite consistent with the meaning expressed by the gold summary.</p><p>For the document in Table <ref type="table">12</ref>, the most important thing is that Schuller died. BART focuses on what did do and doesn't mention the death, while the gold summary and SeqCo both describe his death in the first sentence, and then list some famous deeds in his lifetime. Death is more important than the deeds in this document.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Better Fine-Tuning by Reducing Representational Collapse</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno>CoRR, abs/1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2020</date>
		</imprint>
	</monogr>
	<note>Neural Machine Translation by Jointly Learning to Align and Translate</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1998" to="2008" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2021 Conference of the North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bottom-Up Abstractive Summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<idno>arXiv:2012.04281</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020a. 2020b</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ctrlsum: Towards generic controllable text summarization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4859" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Barcelona, Spain; Hong Kong, China; Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004. 2021. 2019. 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
	<note>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Dos Santos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gu Ì ‡lc ¸ehre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</editor>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2019. 2013. 2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Now Publishers Inc</publisher>
			<date type="published" when="2011">2018. 2011. 2018</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">e26752</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manyumwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online: Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3612" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extractive Summarization as Text Matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pre-training for Abstractive Document Summarization by Reinstating Source Text</title>
		<idno>45.24 22.10 42.01 44.10 21.31 40.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<biblScope unit="page" from="3646" to="3660" />
		</imprint>
	</monogr>
	<note>Validation set Test set R-1 R-2 R-L R-1 R-2 R-L BART</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.60 22.30 42.36 44.66 21.57 41.38</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.80 22.39 42.57 44.94 21.82 41.68</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.88 22.46 42.66 45.02 21.80 41.75</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.72 22.42 42.48 44.81 21.70 41.56</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">λ x−y + λ x−ŷ + λ y−ŷ</title>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.72 22.38 42.46 44.73 21.67 41.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.74 22.39 41.55 44.86 21.66 41.55</idno>
		<title level="m">D y−ŷ )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m">Results on the test split of CNNDM using full length F1 based ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L)</title>
				<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SeqCo (λ x−ŷ ) and SeqCo (λ y−ŷ ) stand for contrastive learning between document and gold summary, document and generated summary as well as gold and generated summary. means our own re-implementation. Validation set Test set R-1 R-2 R-L R-1 R-2 R-L BART</title>
		<author>
			<persName><surname>Seqco (λ X−y )</surname></persName>
		</author>
		<idno>50.75 31.71 46.32 53.20 35.04 49.23</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>50.85 31.63 46.38 53.79 35.43 49.84</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>51.27 31.99 46.74 54.25 35.82 50.24</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>50.79 31.61 46.33 53.70 35.33 49.78</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>51.38 32.01 46.87 54.14 35.69 50.11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">λ x−y + λ x−ŷ + λ y+ŷ</title>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>51.21 31.91 46.65 53.95 35.54 49.97</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Results on the validation and test split of NYT using limited-length recall based ROUGE. means our own reimplementation. Validation set Test set R-1 R-2 R-L R-1 R-2 R-L BART</title>
		<idno>45.38 22.13 36.80 45.35 22.01 36.76</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>46.66 22.42 37.14 45.65 22.41 37.04</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.59 22.39 37.05 45.60 22.36 36.94</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.41 22.25 36.97 45.30 22.15 36.78</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.59 22.39 37.08 45.52 22.24 36.90</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">λ x−y + λ x−ŷ + λ y+ŷ</title>
		<author>
			<persName><surname>Seqco</surname></persName>
		</author>
		<idno>45.77 22.52 37.16 45.51 22.22 36.87</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Results on the validation and test split of XSum using full length F1 based ROUGE. means our own re-implementation</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
