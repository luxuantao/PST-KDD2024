<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from concept drifting data streams with unlabeled data $</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-03-03">3 March 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<postCode>50405</postCode>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peipei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuegang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from concept drifting data streams with unlabeled data $</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-03-03">3 March 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">AA9641B8EE32158177FC150A6653D4ED</idno>
					<idno type="DOI">10.1016/j.neucom.2011.08.041</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data stream Semi-supervised classification Concept drift Unlabeled data</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing work on classification of data streams assumes that all streaming data are labeled and the class labels are immediately available. However, in real-world applications, such as credit fraud and intrusion detection, this assumption is not always valid. Thus, it is a challenge to learn from concept drifting data streams with unlabeled data. With this motivation, we propose a Semi-supervised classification algorithm for data streams with concept drifts and UNlabeled data (SUN) in this paper. In SUN, a clustering algorithm is developed from k-Modes and implemented to produce concept clusters at leaves in an incremental decision tree. In terms of deviations between history concept clusters and new ones, potential concept drifts are distinguished from noise. Extensive studies on both synthetic and real-world data demonstrate that SUN performs well compared to several state-of-theart online supervised and semi-supervised algorithms, even when there are more than 90% unlabeled data. A conclusion is hence drawn that SUN provides a promising framework for tackling concept drifting data streams with unlabeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of web service technology, streaming data from the Internet span across a wide range of domains, including online-shopping transactions, Internet search requests, etc. In contrast to the traditional data sources, data streams present new characteristics as being continuous, high-volume, open-ended and concept drifting. To handle these data streams, many mining approaches have been proposed, including streaming data classification algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, streaming data clustering algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>, sequential pattern mining algorithms from data streams <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref> and query algorithms for data streams <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b13">14]</ref>, etc.</p><p>Classification on data streams is one of the most important branches of streaming data mining, but it is still a challenge to perform classification in real-world data streams with new incremental classification models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>. This is because most existing work relevant to classification on data streams always assumes that all arrived streaming data are completely labeled and these labels can be utilized at hand. Unfortunately, this assumption is violated in many practical applications, especially in the fields of credit fraud, intrusion detection and web user profiling. In such cases, if we only wait for the future labels passively, it is likely that much potentially useful information is lost. Thus, it is significant and necessary to learn actively and immediately.</p><p>Motivated by this, a Semi-supervised algorithm of SUN for concept drifting data streams with UNlabeled data is proposed in this paper. SUN provides several contributions. (i) Unlike existing semi-supervised algorithms for data streams <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b21">23]</ref> with a clustering method, we develop a clustering algorithm based on k-modes to produce concept clusters at leaves in a decision tree built incrementally. This is conducive to improve the accuracy of labeling unlabeled data. (ii) We utilize the deviations between history concepts and new ones in tandem of the bottom-up search to detect potential concept drifts from noise. To the best of our knowledge, this is a new method to detect concept drifts from noisy data streams. (iii) We systematically study the performance of SUN in different ratios of unlabeled data with concept drifts. (iv) Experiments conducted on both synthetic and real-world data show that SUN could track concept drifts well in data streams with unlabeled data. In comparison with the stateof-the-art concept drifting algorithms of CVFDT <ref type="bibr" target="#b12">[13]</ref>, CDRDT <ref type="bibr" target="#b19">[21]</ref> and variants of Bagging <ref type="bibr" target="#b1">[2]</ref>, SUN outperforms them on the prediction accuracy, the runtime overhead and the resilience to noise even on unlabeled data streams. Meanwhile, SUN performs much better than semi-supervised algorithms on the prediction accuracy mentioned in <ref type="bibr" target="#b21">[23]</ref>.</p><p>The rest of this paper is organized as follows. We start with an overview of related work in Section 2 before we present our Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/neucom semi-supervised classification algorithm of SUN in Section 3. Section 4 provides the experimental studies and Section 5 summarizes our results and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section we analyze related work in two dimensions. One is related to streaming data classification algorithms based on single models and ensemble models, and the other refers to classification algorithms for unlabeled data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Streaming data classification algorithms</head><p>For classification of concept drifting data streams, there are two main approaches, single model-based classification algorithms and ensemble classification algorithms. In this subsection, we review representative classification algorithms based on these approaches as follows.</p><p>Single model-based streaming data classification algorithms: The CVFDT (Concept-adapting Very Fast Decision Tree learner) algorithm <ref type="bibr" target="#b12">[13]</ref> was proposed by <ref type="bibr" target="#b12">Hulten et al. in 2001.</ref> It is one of the best-known algorithms based on a single decision tree model that can efficiently classify continuously changing data streams. In CVFDT, a decision tree is learned by recursively replacing leaves with decision nodes. Each leaf stores sufficient statistics about attribute values. If there is enough statistical support in favor of one test, the split-test is installed in the evaluation function of Information Gain (denoted as GðÁÞÞ at this node and the relevant information of this node is descended into its child nodes. Meanwhile, the inequality of Hoeffding bounds <ref type="bibr" target="#b27">[29]</ref> is introduced to solve the problem how many examples are necessary to observe before installing a split-test. The definition is as follows. Consider a real-valued random variable r whose range is R. Suppose that we have made n independent observations of this variable, and computed their mean r, which shows that, with probability 1Àd, the true mean of the variable is at least rÀe:</p><formula xml:id="formula_0">Pðr Z rÀeÞ ¼ 1Àd,e ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi R 2 lnð1=dÞ=2n q<label>ð1Þ</label></formula><p>where R is set to log 2 9class9 (9class9 is the count of class labels), and the value of n refers to the minimum count of instances required in a split-test, denoted as n min . Suppose the difference between two candidate cut-points of attribute a j with the highest gain is DG ¼ Gða x j ÞÀGða y j Þ. For a given t, if DG4e or DGreot (t is a threshold for avoiding the case of ties), the x th cut point with the highest gain will be selected as the final split-point. In addition, CVFDT periodically scans the whole tree to search for internal nodes whose sufficient statistics indicate that concept drifts occur. An alternative subtree is started at every such node, and the old subtrees are replaced if alternative ones become more accurate on new data. More recently, many extended CVFDT algorithms have been proposed. For example, a Naı ¨ve-Bayes-classifier-based CVFDT algorithm (CVFDT NBC ) <ref type="bibr" target="#b22">[24]</ref> was proposed by <ref type="bibr" target="#b22">Nishimura et al. in 2008</ref>. CVFDT NBC applies Naı ¨ve Bayes classifiers in the leaf nodes of a decision tree induced by CVFDT. In 2009, an ambiguous CVFDT (aCVFDT) method <ref type="bibr" target="#b17">[18]</ref> was proposed by Liu et al. aCVFDT integrates ambiguities into CVFDT by exploring multiple options at each node whenever a node is to be split. When old concepts recur, it can immediately relearn them by using the corresponding options recorded at each node. Meanwhile, it uses an automatic concept drift detecting mechanism instead of the periodical scanning in CVFDT. These extended algorithms can induce higher accuracy classifiers than CVFDT does.</p><p>Ensemble model-based streaming data classification algorithms: On the other hand, representative ensembling algorithms for concept drifting data streams are as follows: (i) A SEA algorithm built from ensembling decision trees was first introduced for concept drifting data streams <ref type="bibr" target="#b28">[30]</ref> in 2001. It splits the data into batches, fits one decision tree per batch and discards old models in a heuristic approach. However, due to the lack of confidence weights and the limit of exchanging one model in each iteration, the recovery time from concept drifts seems unnecessarily long. (ii) A general framework of mining concept-drifting data streams using weighted ensemble classifiers <ref type="bibr" target="#b14">[15]</ref> was proposed by Wang et. al in 2003. Instead of continuously revising a single model, this algorithm trains an ensemble of classifiers from sequential data chunks in the stream and combines multiple classifiers weighted by their expected prediction accuracy on the test data. It could improve the classification accuracy compared to algorithms based single models. However, it is not suitable for various concept drifts. (iii) The CDRDT algorithm (a streaming data algorithm for Concept Drifts in Random Decision Tree ensembling) <ref type="bibr" target="#b19">[21]</ref> was presented by <ref type="bibr" target="#b5">Li et al. in 2009</ref> for tracking concept drifts from noisy data streams. It generates N-classifiers of random decision trees with various sequential chunks of data streams, and utilizes a double-threshold-based mechanism to discern concept drifts with noise. CDRDT is not only capable of adapting to the diverse types of concept drifts in data streams timely and effectively, but is also robust to noise. (iv) Meanwhile, two new variants of Bagging, Adaptive-Size Hoeffding Tree Bagging (called BagASHT) and adaptive window Bagging (called BagADWIN)were proposed by Albert et al. <ref type="bibr" target="#b1">[2]</ref>. Contrary to the CDRDT algorithm based on random models, BagASHT adopts weighted classifiers and replaces oversized Hoeffding trees with new ones, and BagADWIN uses an adaptive windowing mechanism as a change detector and as an estimator for the weights of the boosting method. Both variants of bagging could produce excellent accuracy compared with other ensembling methods.</p><p>In a word, CDRDT and variants of bagging based on different ensemble models are efficient algorithms for concept drifting data streams. Meanwhile, the aforementioned algorithms are only suitable for classification on completely labeled data streams. In Section 4, we select three representative classification algorithms based on single models and ensemble models, including CVFDT, CDRDT and variants of bagging in our comparative study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification algorithms for unlabeled data streams</head><p>There have been several significant efforts as follows on dealing with unlabeled data streams. (i) In 2004, a new framework of stream data mining based on decision trees <ref type="bibr" target="#b25">[27]</ref> was introduced by Fan et al. It builds an evolved decision tree using 10-30% of true class labels, which shows more accuracy than the original one on concept-drifting data streams. However, this model cannot cope with the concept drift caused by sudden shift of user interests. (ii) In 2005, a new computational framework for extending an incompletely labeled data stream was proposed by Widyantoro et al. <ref type="bibr" target="#b7">[8]</ref>. It could significantly improve performances of algorithms when learning from a sparsely labeled data stream with concept drifts. However, a potential weakness lies in the need to reproduce a new stream every time and the learner must relearn the new stream from scratch. (iii) In 2006, a semi-supervised learning algorithm of clustering-training was proposed by Wu et al. <ref type="bibr" target="#b21">[23]</ref>. It adopts k-Prototype clustering to select confidently unlabeled samples from data streams and uses them to re-train the classifier incrementally. However, this algorithm does not address the issue of concept drifts. (iv) In 2007, a concept tracker algorithm was proposed by Widyantoro for concept drifting data streams with unlabeled data <ref type="bibr" target="#b6">[7]</ref>. It first incrementally constructs a concept hierarchy from a data stream (mostly unlabeled data) in an unsupervised mode and then uses it to identify the instance classes. This algorithm is effective in the information-filtering domain. (v) In 2008, an ensemble classification model <ref type="bibr" target="#b24">[26]</ref> learned from a training set with both unlabeled data and a limited amount of labeled data was proposed by Masud et al. This model is built as micro-clusters (based on k-Means) using semi-supervised clustering and classification is performed with the k-nearest neighbor algorithm. (vi) In 2009, an OcVFDT (One-class Very Fast Decision Tree) algorithm for the problem of one-class classification of data streams <ref type="bibr" target="#b5">[6]</ref> was proposed by Li et al. OcVFDT could tackle the issue that only positive samples and unlabeled samples are observed from a training data stream. However, this algorithm also ignores the issue of concept drifts in data streams.</p><p>Contrary to the aforementioned algorithms, we propose a new semi-supervised classification algorithm of SUN for data streams with concept drifts and unlabeled data. SUN provides the following three characteristics. First, clustering is built on an incremental decision tree instead of building an ensemble of classification models in <ref type="bibr" target="#b24">[26]</ref>. Meanwhile, instead of k-Means (k-Median) and k-Prototype used in <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b21">23]</ref>, respectively, a clustering algorithm based on k-Modes is introduced and developed. It is triggered to create concept clusters at leaves and unlabeled data are labeled in the method of majority-class using these concept clusters. Second, according to the deviations between the history concept clusters and the new ones, potential concept drifts are detected. This is different from the drifting detection approaches involved in <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>. Meanwhile, the estimation metrics of concept drifting tracking in SUN are given in detail while the aforementioned approaches do not address these values. Third, an approach of bottom-up search is utilized to trace all drifting leaves. Meanwhile, several strategies, such as updating concept clusters and pruning sub-trees, are adopted for adaption to the new incoming concept drifts. In conclusion, SUN achieves better performances compared to several known online classification algorithms, utilizing only a fraction of the unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The SUN algorithm</head><p>In concept drift rate learning <ref type="bibr" target="#b8">[9]</ref>, the drift rate (e.g., D) specifies the probability that two successive target concepts disagree on a randomly drawn example, i.e., Probðf ðx n Þ a f ðx n þ 1 ÞÞ. Hence, the larger the drift rate, the more frequent the change of target concepts. A theoretical bound is provided in <ref type="bibr" target="#b8">[9]</ref> on the allowable drift rate, which guarantees tractability with an error of at most e, namely Drce 2 =d Á lnð1=eÞ ð 2Þ where c 40, d is the Vapnik-Chervonenkis dimension of a concept, and both values are constants. This bound indicates that it is more difficult to track concept drift in tandem of learning with fewer labeled instances per target concept (i.e., a higher drift rate). In other words, higher drift rates imply higher error rates classified in the learning. Thus, according to Eq. ( <ref type="formula">2</ref>), to reduce the rate of concept drift, one possible strategy as considered in this paper is to fill the gap in labeled data with relevant unlabeled data for improving the performance of the current concept drifting learner. This is also a conclusion from <ref type="bibr" target="#b6">[7]</ref>. The following describes an approach that (i) uses the information of labeled data to supervise the labeling of relevant unlabeled data based on an incremental decision tree; and (ii) utilizes the generated clusters to track potential concept drifts by the deviation measure between the history concept clusters and the new ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm description</head><p>Our SUN algorithm to be presented in this section aims to handle concept drifting data streams with unlabeled data. Algorithm 1 shows the processing flow of SUN, including two components of training and testing. First, we generate an incremental decision tree using the incoming streaming data. Meanwhile, we develop the k-Modes clustering algorithm to label unlabeled data and install concept drifting detection using clustering concepts in the growing of the tree (Steps 1-16). Second, if the streaming data reach the end, we evaluate the test data set using the current decision tree (Steps 17-19). The technical details of training and testing are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Building an incremental decision tree</head><p>In our SUN algorithm, a decision tree is built incrementally. More specifically, a training instance, x i , first traverses the decision tree from the root to an available leaf l (Steps 1-3). According to whether x i is labeled, the information is correspondingly loaded into the array of pLabelArr (related to labeled data) or pUnLabeledArr (related to unlabeled data). Meanwhile, if x i is a labeled instance, the statistics are updated, including arrays of pClassDistr and pAttrDistr relevant to the distribution of class labels and attribute values of all features respectively (Steps 5-7). It should be mentioned that attribute values of instances will be inserted into the array pAttrDistr by ascending order, because it is conducive to perform discretization on numerical attributes. In terms of the statistical information, if the count of instances arrived at the current tree-n t satisfies the threshold of detection period-DP, we will install clustering to label unlabeled data in tandem of relevant labeled data in each cluster and reuse the information of those unlabeled data at the current node for future split-test, i.e., updating the statistics of pClassDistr and pAttrDistr. Meanwhile, we will maintain the information of clusters for future concept drifting detection as shown in Steps 8-13 (more details to be given in Sections 3.3 and 3.4). Furthermore, if the statistical count at this node is up to the value of n min , the merit of split-test is evaluated in a heuristic method based on Information Gain and Hoeffding bounds inequality. This is similar to the method in CVFDT. Correspondingly, the current leaf is replaced with the decision node and child nodes are created (Step 15). To provide better services in the concept drifting detection, two necessary data structures are created in the growing of the current decision tree (Step 16), including (i) each new generated node maintains a pointer link to the parent node (denoted as mpf), and (ii) an array list is created to store all nodes corresponding to their depths in the current tree, called levelList. Label unlabeled data with labeled data at leaf-l in majority-class; 12:</p><formula xml:id="formula_1">Algorithm 1. SUN Input: Training set: TR ¼ fx 1 , . . . ,x i , . . .g; Test set: TE ¼ fx 0 1 , . . . ,</formula><p>Load the labeling information into pAttrArr and pClassArr for split-test; 13:</p><p>Detect concept drifts using concept clusters; 14: if the count of arrived instances at lÀn l meets n min 15:</p><p>Install the split-test for an optimal attribute with the highest information gain and grow child leaves; 16:</p><p>Record the information of new leaves by a list-levelList and generate the double link to parent nodes and child nodes; 17: for each testing instance x 0 j from TE 18: Classify with the current tree in majority-class; 19: return the error rate of classification;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Labeling unlabeled data</head><p>To fill the gap in labeled data with relevant unlabeled data, we utilize a clustering algorithm evolved from k-Modes <ref type="bibr" target="#b23">[25]</ref> to create concept clusters and implement labeling. With a traditional approach, converting categorical data into numerical values does not necessarily produce meaningful results in the case where categorical domains are not ordered. Thus, it is difficult to extend the clustering algorithm of k-Means or k-Median for handling mixed attributes. Also, the clustering algorithm of k-Prototypes is useful in dealing with mixed attributes but its performance depends on the influence of weight (i.e., r l ), and the choice of r l is dependent on distributions of numerical attributes. Considering the change of data distribution in data streams, it is hard to determine a general and optimal value. Therefore, we select an efficient clustering algorithm of k-Modes to label unlabeled data, because it is convenient to be extended for handling mixed attributes in tandem of a discretization method for numerical attributes. That is, in terms of the statistics in pAttrDistr, the sequential attribute-values are divided into min(10, the number of discrete attribute-values of a j -lenðpAttrDistr½a j ÞÞ intervals for each attribute. As shown in Steps 8-10 of SUN, if the count of training data arrived meets the detection period-DP, the k-Modes algorithm is installed at each available leaf, that is, the objective of clustering a set of n categorical objects into k clusters is to find W and M that minimize</p><formula xml:id="formula_2">FðW,MÞ ¼ X k l ¼ 1 X n i ¼ 1 w li Dis A ðm l ,e i Þ ð<label>3Þ</label></formula><p>subject to:</p><formula xml:id="formula_3">w li A f0; 1g, P k l ¼ 1 w li ¼ 1 and 0o P n i ¼ 1 w li o n, 1r l r k, 1 ri r n, where kð o nÞ is a known number of clusters, W ¼ ½w li is a k Â n{0, 1} matrix, M ¼ ½m 1 ,m 2 , . . . ,m k is a set of clusters</formula><p>and m l is the l th cluster center with the categorical attributes a 1 ,a 2 , . . . ,a 9A9 ð A AÞ, and each attribute a j describes a domain of values, denoted by DOMa j , associated with a defined semantic type and a data type. The minimization of F in Eq. (3) with the above constraints forms a class of constrained nonlinear optimization problems whose solutions are unknown. The usual method towards optimization of F is to use a partial optimization for M and W as in the following developed algorithm of k-Modes:</p><p>1. Select k distinct objects to generate an initial mode set M t ¼ ½m 1 ,m 2 , . . . ,m k corresponding to the distribution of class labels. Determine W t such that FðW t ,M t Þ is minimized (t¼1). 2. If t r maxIter (the maximum number of iterations), determine</p><formula xml:id="formula_4">M t þ 1 such that FðW t ,M t þ 1 Þ is minimized. If abs½FðW t ,M t þ 1 ÞÀ FðW t ,M t Þ oy (e.g., 10 À 5 ), then stop. 3. Determine W t þ 1 such that FðW t þ 1 ,M t þ 1 Þ is minimized. If abs½FðW t þ 1 ,M t þ 1 ÞÀFðW t ,M t þ 1 Þ o y, then stop; Otherwise set t ¼t þ 1 and go to Step 2.</formula><p>The matrices W and M are calculated according to the following two theorems provided in <ref type="bibr" target="#b23">[25]</ref>.</p><p>Theorem 1. When M is fixed, with the problem of minF(W, M), the minimizer W is given by</p><formula xml:id="formula_5">w li ¼ 1 when Dis A ðm l ,e i Þ o Dis A ðm h ,e i Þ,1 r hr k 0 otherwise</formula><p>Theorem 2. Let E be a set of categorical objects described by categorical attributes a 1 ,a 2 , . . . ,a 9A9 and DOMa j ¼ fv 1 a j ,v 2 a j , . . . ,v n j a j g, i.e., E ¼ fe 1 ,e 2 , . . . ,e n g and e q ¼ fe q1 ,e q2 , . . . ,e q9A9 g ð1r q rnÞ, where n j is the number of categories of attribute-a j for 1 r j r9A9. Let the cluster center m l be represented by fm l1 ,m l2 , . . . ,m l9A9 gð1 rl rkÞ. Then the quantity</p><formula xml:id="formula_6">P k l ¼ 1 P n i ¼ 1 w li Dis A ðm l ,e i Þ is minimized iff m lj ¼ v r a j A DOMa j , where 9fw li 9e ij ¼ v r a j ,w li ¼ 1g9 4 9fw li 9e ij ¼ v p a j ,w li ¼ 1g 9, 1r p rn j , for 1 rj r 9A9.</formula><p>Furthermore, to evaluate the value of F(W, M) in Eq. ( <ref type="formula" target="#formula_2">3</ref>), we give a new definition on the dissimilarity measure Dis A ðm l ,e i Þ between an object-e i and a cluster center-m l with respect to A below</p><formula xml:id="formula_7">Dis A ðm l ,e i Þ ¼ X a j A A Dis a j ðm l ,e i Þ ð<label>4Þ</label></formula><p>subject to</p><formula xml:id="formula_8">Dis a j ðm l ,e i Þ ¼ 1 when f ðm l ,a j Þ af ðe i ,a j Þ 1Àsim a j ðm l ,e i Þ otherwise<label>(</label></formula><p>where the similarity between the object-e i and the cluster centerm l with respect to a j , sim a j ðm l ,e i Þ, meets the following constraints: (a) for a categorical attribute:</p><formula xml:id="formula_9">sim aj ðm l ,e i Þ ¼ 0 when f ðm l ,a j Þ af ðe i ,a j Þ 9fe i 9f ðe i ,a j Þ ¼ f ðm l ,a j</formula><p>Þ,e i Am l g9=9m l 9 otherwise ( (b) for a numerical attribute:</p><formula xml:id="formula_10">sim a j ðm l ,e i Þ ¼ 1 when f l ðm l ,a j Þ rf ðe i ,a j Þ o f h ðm l ,a j Þ 0 otherwise</formula><p>where function-f indicates values of m l and e i mapped on attribute-a j , f l ðm l ,a j Þ specifies the lowest bound of the discretized interval that cluster m l projects on attribute-a j , while f h ðm l ,a j Þ refers to the highest bound, and 9m l 9 is the number of objects in this cluster. We should mention that the heterogeneous distance function-HVDM (Heterogeneous Value Difference Metric) <ref type="bibr" target="#b9">[10]</ref> is suitable for the distance estimation on mixed attributes. However, the statistical values in this function (e.g., the standard deviation of the numerical values of attributes, etc.) would be obtained after seeing all training data. Apparently, it is difficult to obtain the similarity estimation between instances from different streaming data chunks. Thus, a new definition on the dissimilarity measure is given below. Let us consider the following example to show the clustering process. In Table <ref type="table" target="#tab_2">1</ref>, five categorical objects e 1 , . . . ,e 5 are randomly selected from the STAGGER database <ref type="bibr" target="#b15">[16]</ref>. There are three dimensions of attributes and two class labels. According to the processing flow of clustering mentioned above, in the first step, let e 3 and e 5 be the initial cluster centers, i.e., k¼2. Correspondingly, a set of two cluster centers is generated, namely,</p><formula xml:id="formula_11">M 1 ¼ m 1 m 2 " # ¼ red triangle large red triangle small " #</formula><p>In terms of Theorem 1, the matrix-W is determined as follows: </p><formula xml:id="formula_12">W 1 ¼</formula><formula xml:id="formula_13">" # ¼ 0 1 1 1 0 1 0 0 0 1</formula><p>In this case, the value of FðW 1 ,M 1 Þ is equal to 4 by Eq. ( <ref type="formula" target="#formula_2">3</ref>).</p><p>In the second step, we first assign all objects to the corresponding clusters by the minimum dissimilarity. For instance, as Dis A ðm 1 ,e 1 Þ (whose value is 1 by Eq. ( <ref type="formula" target="#formula_7">4</ref>)) is greater than Dis A ðm 2 ,e 1 Þ (whose value is equal to 0), e 1 is assigned to cluster m 2 . However, in case Dis A ðm 1 ,e j Þ ¼ Dis A ðm 2 ,e j Þ is met, we cannot determine which one from these candidate clusters should be assigned. To handle this issue, the following mechanisms are utilized. (i) If the class labels of clusters are known, e j will be assigned to the cluster who has the same label with e j . (ii) Otherwise, a cluster from all candidate ones is randomly selected. In a similar way, the objects of e 2 ,e 3 , and e 4 are finally assigned to cluster m 1 , while the other ones are assigned to cluster m 2 . Secondly, by virtue of Theorem 2, the cluster centers are updated. In other words, the attribute value with the highest frequency in each cluster is selected to form a new cluster center. Thus, we can obtain a new cluster set-M 2 as follows:</p><formula xml:id="formula_14">M 2 ¼ m 1 m 2 " # ¼ red circle large red triangle small " #</formula><p>Based on the matrices of W 1 and M 2 , the value of FðW 1 ,M 2 Þ is calculated and it is equal to 3. In this case, the third step is installed.</p><p>In the third step, the matrix-W 1 is firstly updated to W 2 by Theorem 1, i.e. In this case, we will utilize the majorityclass of labeled data in the current cluster to label unlabeled data. An extreme case is addressed here. That is, if the labeled data are very sparse (or there are no labeled data at the clusters), we provide the following strategies to handle this case. (i) Select the nearest one of history concept clusters to label unlabeled data in the current cluster. (ii) Otherwise, if there are no history concept clusters, collect concept clusters with class labels from other leaves and select the nearest cluster by the similarity measure to label those unlabeled data. (iii) Select a class label randomly from all known class labels as the class label of the current cluster, and re-adjust the label after seeing future labeled data as the streaming data arrive. Moreover, the convergence of the k-Modes algorithm, based on the dissimilarity calculation measure for categorical attributes only, has been proved in <ref type="bibr" target="#b23">[25]</ref>, and actually it is also applicable to our SUN algorithm based on the new definition in Eq. ( <ref type="formula" target="#formula_7">4</ref>). Meanwhile, to avoid converging very slowly, we introduce a threshold-maxIter to limit the iteration number (it is set to 50 here). In this algorithm, with respect to the parameter-k, we initialize it with the value of 9class9. This is because the number of class labels indicates how many concept clusters are in the clustering. In addition, we should note that the clustering is installed at every interval of a certain number of instances (i.e., DP). Apparently, all arrived training instances in this period consist of a streaming data chunk. Thus, new concept clusters generated over the current data chunk (i.e., the current detection period) consist of a new set of concept clusters (denoted as M new ), while history concept clusters generated over the last data chunk consist of a set of history concept clusters (denoted as M hist ).</p><p>In terms of the aforementioned semi-supervised mechanism (i.e., clustering during the growing of a decision tree and labeling unlabeled data using the information of labeled data), it is beneficial to improve the accuracy of labeling unlabeled data. An analysis is given in detail as follows. Suppose the decision tree generated in SUN is a complete binary-tree. The height of this tree is denoted as l. Apparently, the total number of leaves is up to 2 lÀ1 . Meanwhile, suppose that there are m-instances arrived at the current tree and the probability that each training instance reaches a leaf is equal. Thus, the mean number of instances at a leaf amounts to m=2 lÀ1 ð Z 1Þ. Therefore, the least probability that all unlabeled data are labeled correctly at a leaf (denoted as pc) could be expressed as follows:</p><formula xml:id="formula_15">pc ¼ ð1=9class9Þ m=2 lÀ1<label>ð5Þ</label></formula><p>However, in the case without a decision tree, the value of pc amounts to 1=9class9 m . Apparently, the accuracy of labeling unlabeled data is improved largely for the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Detecting concept drifts</head><p>Based on the concept clusters of M new and M hist , a concept drifting detection is triggered at each leaf as shown in Step 13. In other words, we utilize the deviation between the set of history concept clusters and the set of new ones to evaluate concept drifts from noise. This is based on the statistics theory as follows. For a stationary distribution of the instances, the online error of Naı ¨ve Bayes will decrease; while the distribution function of the instances changes, the online error of Naı ¨ve Bayes will increase <ref type="bibr" target="#b20">[22]</ref>. According to this theory, in our algorithm, the change of the data distribution (and the change of online error of Naı ¨ve Bayes) implies the change of attribute dimensions, i.e., the deviation of concept clusters (and the data distribution of class labels) at the current clusters. Thus, in this paper we first compare the deviation between history concept clusters and new ones, and then consider the data distributions of class labels if necessary to track concept drifts. More precisely, to measure the deviation between concept clusters, we define three variables, namely (i) r new and (ii) r hist to specify the radius of M new and M hist , respectively, and (iii) dist to refer to the average distance between these two sets (see Section 3.5 for more details on these variables).</p><p>According to the relations among the aforementioned variables, we could obtain three cases of concept drifts as illustrated in Fig. <ref type="figure" target="#fig_2">1</ref> (which only shows the case of r new 4 r hist , but actually the other two cases are similar). More specifically, (a) if the value of dist is less than the minimum value between r new and r hist , a potential concept drift is considered. In this case, new concept clusters will be incorporated into the set-M hist , because we believe that there is no enough deviation to support the occurrence of concept drifts in the new concept clusters. (b) If the value of dist is limited to the bound of the minimum radius and the maximum one, it is considered as a noise impact. Thus, the set-M hist does not change while the set-M new is discarded. (c) Otherwise, it indicates that an abrupt drift occurs. In this case, the history concept  In addition, to locate all drifting leaves, a bottom-up search is adopted. Fig. <ref type="figure" target="#fig_3">2</ref> provides an illustration of search at bottom leaves. As shown in this figure, we traverse all leaves one by one corresponding to the links to the bottom level in the list of levelList <ref type="bibr" target="#b2">[3]</ref>. The traversal paths are marked by sequential numbers. Suppose concept drifts are detected at the d-node and the f-node, but no concept drift is detected at the e-node. In this case, it is considered that there is no concept drift in the sub-tree of b-node. Hence, we further traverse the remaining g-node and h-node on this level. Because concept drifts occur at these two nodes, it indicates that the sub-tree of c-node is outdated. We search the parent node-c corresponding to the parent pointer-m pf and install pruning. That is, child nodes of g and h are pruned, and the clustering information at these nodes is passed to their parent node. Meanwhile, the parent node is marked as a new leaf. Similarly, the whole traversal processing will be repeated corresponding to the levels of this tree from bottom to top until the root is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Prediction on the test data</head><p>In the last two steps of Algorithm 1, the prediction performance of the current model learned from the training data set with labeled data and unlabeled data is validated on the given test set. Namely, each testing instance first traverses from the root to an available leaf, and then the majority class label at the current leaf is selected as the class label of this instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Definitions and theorems</head><p>Regarding variables involved in Section 3.4, relevant definitions and theorems are given as follows.</p><p>Definition 1. The similarity between a cluster center-m l and an object-e i Þ: simðm l ,e i Þ ¼ P 9A9 j ¼ 1 ½sim a j ðm l ,e i Þ=9A9, where sim a j ðm l ,e i Þ is given in Eq. ( <ref type="formula" target="#formula_7">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. The similarity between a cluster center-m p and another cluster center-m</head><formula xml:id="formula_16">k : simðm p ,m k Þ ¼ P 9A9 j ¼ 1 ½sim a j ðm p ,m k Þ=9A9, subject to:</formula><p>(a) for a categorical attribute:</p><formula xml:id="formula_17">sim a j ðm p ,m k Þ ¼ 0 when f ðm p ,a j Þ af ðm k ,a j Þ 1 otherwise</formula><p>(b) for a numerical attribute:</p><formula xml:id="formula_18">sima j ðmp,m k Þ ¼ 0 when ½f l ðmp,a j Þ,f h ðmp,a j Þ \ ½f l ðm k ,a j Þ,f h ðm k ,a j Þ ¼ | 29½f l ðmp,a j Þ,f h ðmp,a j Þ \ ½f l ðm k ,a j Þ,f h ðm k ,a j Þ9 ½f h ðmp,a j ÞÀf l ðmp,a j Þ þ ½f h ðm k ,a j ÞÀf l ðm k ,a j Þ otherwise : 8 &lt; :</formula><p>and f l ðm l ,a j Þ,f h ðm l ,a j Þ,f l ðm p ,a j Þ and f h ðm p ,a j Þ are given in Eq. ( <ref type="formula" target="#formula_7">4</ref>).</p><p>Definition 3. The similarity between the set of new concept clusters-M new and the set of history ones-M hist :</p><formula xml:id="formula_19">simðM new ,M hist Þ ¼ ð P nnew p ¼ 1 max½simðm p , m k Þ, m k A M hist ,1 rk r n hist Þ= n new ,m p A M new</formula><p>, where n new ðn hist Þ specifies the total number of clusters in the corresponding set. Definition 4. The radius of the set of new concept clusters:</p><formula xml:id="formula_20">r new ¼ ½ P nnew p ¼ 1 ð P 9mp9 i ¼ 1 simðm p ,e i ÞÞ=9m p 9=n new</formula><p>, where e i A m p , m p A M new and 9m p 9 means the number of instances in this cluster. Similarly, we could obtain the radius of the set of history concept clusters:</p><formula xml:id="formula_21">r hist ¼ ½ P n hist k ¼ 1 ð P 9m k 9 i ¼ 1 simðm k ,e i ÞÞ=9m k 9=n hist .</formula><p>Definition 5. The distance between the set of new concept clusters and the set of history concept clusters:</p><formula xml:id="formula_22">dist ¼ 1Àsim ðM new ,M hist Þ.</formula><p>Theorem 3. 0 r simðm l ,e i Þ r1.</p><p>Proof. In Eq. ( <ref type="formula" target="#formula_7">4</ref>), 0 rsim a j ðm l ,e i Þ r 1 apparently holds in two cases. Thus, according to Definition 1, the inequality of 0 r P 9A9 j ¼ 1 ½sim a j ðm l ,e i Þ=9A9 r 9A9=9A9 r1 could be obtained. Therefore, 0 rsimðm l ,e i Þ r1 is proved. &amp;   Proof. According to Definition 2, if the inequality of 0 r simðm p ,m k Þ r1 holds, it infers that the inequality of 0 r P 9A9 j ¼ 1 ½sim a j ðm p ,m k Þ r 9A9 is satisfied. In other words, it indicates that the inequality of 0 rsim a j ðm p ,m k Þ r 1 holds. Now, we consider two cases to prove it respectively. &amp; Case 1. For a categorical attribute, the inequality of 0 r sim a j ðm p ,m k Þ r 1 is apparently met.</p><p>Case 2. For a numerical attribute, assuming</p><formula xml:id="formula_23">V ¼ 9½f l ðm p ,a j Þ, f h ðm p ,a j Þ \ ½f l ðm k ,a j Þ,f h ðm k ,a j Þ9,len p ¼ 9f h ðm p ,a j ÞÀ f l ðm p ,a j Þ9,len k ¼ 9f h ðm k ,a j ÞÀ f l ðm k ,</formula><p>a j Þ9 and minV ¼ minðlen p ,len k Þ, we could obtain the inequality of 0 rV rminV. Obviously, the following inequality of 0 r2 V r 2 minV r len p þlen k holds, iff len p ¼ len k ) 2V ¼ len p þ len k . Namely, the inequality of 0 r 2V=ðlen p þlen k Þ rðlen p þlen k Þ=ðlen p þlen k Þ ¼ 1 is also satisfied. Thus, the inequality of 0 r sim a j ðm p ,e i Þ r 1 is obtained. Therefore, the inequality of 0 r P 9A9 j ¼ 1 ½sim a j ðm p ,m k Þ r 9A9 holds in both cases, and 0 r sim ðm p ,m k Þ r 1 is proved.</p><formula xml:id="formula_24">Theorem 5. 0 r simðM new ,M hist Þ r1.</formula><p>Proof. Based on Theorem 4 and Definition 3, let maxF¼max½sim ðm p ,m k Þ,m k A M hist ,1 r k rn hist , the inequality of 0 rmaxF r 1 is satisfied so that 0 r P nnew p ¼ 1 maxF r n new is also met. Correspondingly, the inequality of 0 r</p><formula xml:id="formula_25">P nnew p ¼ 1 maxF=n new r n new =n new ¼ 1 holds. Thus, 0 rsimðM new ,M hist Þ r 1 is proved. &amp; Theorem 6. 0 r r new r 1ð0 rr hist r 1Þ.</formula><p>Proof. In Theorem 3, assuming sim mp ¼ P 9mp9 i ¼ 1 simðm p ,e i Þ, we could obtain 0 r sim mp r 9m p 9 and 0 rsim mp =9m p 9 r1. Correspondingly, the equality of r new ¼ ð P nnew p ¼ 1 sim mp =9m p 9Þ=n new rn new =n new ¼ 1 would be obtained by Definition 4. Thus, 0 r r new r1 is proved. Similarly, the inequality of 0 rr hist r 1 could be proved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the efficiency and effectiveness of our SUN algorithm, we have performed extensive experiments on benchmark concept drifting databases, real-word databases as well as noisy databases. Section 4.1 discusses the characteristics of databases used in the experiments. Section 4.2 compares our experimental results against concept drifting data stream algorithms of CVFDT <ref type="bibr" target="#b12">[13]</ref>, CDRDT <ref type="bibr" target="#b19">[21]</ref>, variants of Bagging <ref type="bibr" target="#b1">[2]</ref> <ref type="foot" target="#foot_1">1</ref> and several semisupervised algorithms referred in <ref type="bibr" target="#b21">[23]</ref>. All experiments were conducted on a P4 3.00 GHz PC with 2G main memory, running Windows XP Professional, and all algorithms were implemented in Cþþ except BagASHT and BagADWIN which were coded in Java from the open source of MOA (Massive Online Analysis: a software environment for implementing algorithms and running experiments for online learning from data streams) <ref type="bibr" target="#b11">[12]</ref>. SEA: SEA is a well-known data set of concept shifts <ref type="bibr" target="#b28">[30]</ref>. It consists of a three-dimensional feature space with two classes and four concepts. We use the SEA generator from MOA to generate a training set with 100k-sized instances (1k ¼1000) (each concept shift appears in every 25k-sized instances) and a test set with 50k-sized instances with four concepts. Both sets contain 10% class noise.</p><p>STAGGER: STAGGER is another standard database of conceptshifting data streams <ref type="bibr" target="#b15">[16]</ref>. It consists of three attributes, two classes and three different concepts. The training set used in the experiments contains 0.1k alternative concepts (each concept contains 1k random instances), while the test set contains 500ksized instances with the same instance interval of concepts. Both data sets contain 10% class noise and five dimensions of irrelevant attributes, and they are generated in the STAGGER generator from MOA.</p><p>KDDCup99. KDDCup99 is a database for network intrusion detection <ref type="bibr">[19]</ref>, which is simulated as streaming data with sampling changes <ref type="bibr" target="#b19">[21]</ref>. This data set contains 41 dimensions of attributes with 34 numerical attributes and 24 classes. To avoid the skewed distribution of classes, the data with minority classes (e.g., the number of instances for one class is lower than 1k) are filtered out. Hence, the remaining training set contains 490ksized instances and the test set contains 310k-sized instances. Both data sets contain eight distinct classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Real-world database</head><p>Shopping data: Shopping data are obtained via Yahoo web service interface <ref type="bibr" target="#b29">[31]</ref> as real streaming data. They are sampled from databases of product offers and merchants, including 84ksized training instances and 28k-sized test instances with 5 classes and 23 dimensions of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Noisy database LED:</head><p>The LED database contains 24-attribute dimensions with 17 irrelevant attributes. In our experiments, we use the data generator of LED in MOA to generate a training data set with 300k instances and a test data set with 250k instances. Noise rates vary from 5% to 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental evaluations</head><p>The parameter settings in CVFDT and CDRDT follow default values in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">21]</ref>, respectively, and BagASHT/BagADWIN follows the optimal settings in <ref type="bibr" target="#b1">[2]</ref>. For SUN, settings are as follows: n min ¼ 0:2k, t ¼ 0:05, d ¼ 10 À7 and DP¼0.2k. The values of t, d and n min are empirical values obtained from <ref type="bibr" target="#b12">[13]</ref>. The lower the value of DP, the more the false alarms; while the higher the value of DP, the more the missing concepts. To achieve a trade-off between false alarms and missing concepts in SUN, we empirically selected an optimal value DP¼ 200. All experimental results are averaged over 30 runs. Several symbols involved in this section are listed in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Labeling and classification</head><p>In this subsection, a set of experiments are conducted with unlabeled ratios. Our experiments show the relation between the labeling accuracy and the classification ability of the current model using the labeling strategy mentioned in Section 3.3. As shown in Fig. <ref type="figure" target="#fig_7">3</ref>, the error rate of classification on the test set does not increase explicitly until the unlabeled ratio amounts to a particular point. More precisely, for SEA, the fluctuation of error rates is limited to 1% when ulr r 60%, and then the classification accuracy is reduced by a large margin (ranging from 7.6% to 19.9%). For Shopping-data, the prediction accuracy sharply drops down to 83.0% at ulr ¼90% from 98.0% at ulr ¼ 80%. However, for STAGGER and KDDCup99, the point is ulr r98%. Meanwhile, as unlabeled ratios increase, the labeling accuracy decreases very slowly until another value of ulr. Before reaching this point, the labeling accuracy is always high. For example, the labeling accuracy on SEA is less than 95% only when ulr Z 50%, while for STAGGER, KDDCup99 and Shopping-data, the point is ulr ¼ 98%, ulr Z 70% and ulr Z80%, respectively. In general, the higher the labeling accuracy, the lower the classification error. However, as the value of ulr increases, the reduction of classification error by a large margin happens later than that of labeling accuracy. This infers that the classification accuracy deteriorates gradually by the prediction ability of concept clusters. In the following subsections, we only give the experimental results of SUN evaluated at ulr ¼ 50%, but actually all experimental conclusions are similar when ulr ¼10-99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Concept drifting detection</head><p>In this subsection, our objective is to evaluate whether SUN could handle scenarios where there is a concept drift. Corresponding drifting tracking curves on benchmark databases are depicted in Figs. <ref type="figure" target="#fig_8">4</ref> and<ref type="figure" target="#fig_9">5</ref>, where the occurrence positions of concept drifts are marked on dotted lines. In Fig. <ref type="figure" target="#fig_9">5</ref>, dotted lines reflect the distribution of class labels marked in the right y-axis. These figures only present tracking results over the sequential data chunks in case of ulr¼50%. Actually, the shapes of tracking curves in other cases of ulr varying from 0% to 90% are consistent with this case. As the tracking curves on STAGGER approximately coincide with the x-axis, the description of tracking curves is omitted. From the above observation, we can find that (i) if meeting a concept drift, tracking curves jump to peaks temporarily and then converge quickly, such as the cases in Fig. <ref type="figure" target="#fig_4">4</ref>. (ii) During the same concept, classification errors are decreasing. This is obviously shown in Fig. <ref type="figure" target="#fig_9">5</ref> that the instance interval spans over the range of [141k, 334k]. These cases infer that the current model could adapt to concept drifts. Furthermore, Table <ref type="table" target="#tab_4">3</ref> reports the detection results relevant to the estimation metrics <ref type="bibr" target="#b16">[17]</ref>-Falarms, Missing and Delay in the tracking. Because CVFDT and BagBest have no function to report results of these estimation metrics, there are hence only comparisons between SUN and CDRDT    in this table. We can see that on STAGGER, both algorithms miss all concept drifts. This is because prediction accuracies outputted incrementally are almost up to 100%. There is not enough variance to show the concept drifts. However, considering SEA and KDDCup99, a significant Delay is needed in SUN especially on SEA because instances are required in the clustering, but there are few variances between SUN and CDRDT regarding the values of Falarms and Missing. For instance, the value of Falarms in SUN is increased by around 2% and the value of Missing is increased by 2. In sum, SUN reacts to concept drifts slower than CDRDT, but it could perform as well on the metrics of Falarms and Missing. This conclusion is drawn from the detection results with ulr¼50%. More empirical results reveal that the lower the value of ulr, the better the performance in SUN on these three estimation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Prediction accuracy and runtime</head><p>In this subsection, we compare SUN with CVFDT, CDRDT and BagBest on the prediction accuracy and the consumption of runtime. First of all, a summary of experimental results on the prediction accuracy in Table <ref type="table" target="#tab_5">4</ref> presents that SUN wins three times, while CVFDT wins twice. More precisely, on SEA, the prediction accuracy in SUN is lower than CVFDT by 1.68% on average. On STAGGER, it performs as well as CDRDT. While on KDDCup99 and Shopping-data, the prediction accuracy in SUN is improved by the range of 3.68-21.27% as compared with CVFDT and BagBest. In addition, in comparison with a similar semisupervised algorithm <ref type="bibr" target="#b21">[23]</ref> based on clustering for data streams (called the clustering-training algorithm), experimental evaluations are concluded below. (i) On the simulation of streaming data involved in this algorithm, the prediction accuracy classified on 5k-sized test data in SUN amounts to 100% if 101k-sized training data contain less than 98% unlabeled instances. However, in the case of ulr ¼99%, it is still over 98%. (ii) On KDDCup99, even in the case of ulr ¼99%, the accuracy of classification is still around 94%. SUN performs as well as the algorithm of clustering-training, and both of them outperform all semi-supervised algorithms (i.e., selftraining, co-training and tri-training) mentioned in <ref type="bibr" target="#b21">[23]</ref> by a large margin (up to 30%).</p><p>Furthermore, regarding the overheads of runtime in Table <ref type="table" target="#tab_6">5</ref>, the total time consumption in SUN is only 1=721=2 of those in CDRDT and CVFDT. SUN performs as efficiently as BagBest (no more than 10 s) on databases of STAGGER, Shopping-data and SEA, because these three databases are small sized (less than 100k). Actually, additional experiments show that the advantage of runtime in SUN is more obvious as the sizes of data sets increase. However, on the KDDCup99 database, SUN well outperforms BagBest. The runtime overhead in SUN is less than 1 min while it is more than 1 hour in BagBest. The values of runtime in BagBest listed here are only 1/18 of the real runtime consumptions, and this is because the mean efficiency of the algorithm coded in Cþþ is up to 18 times faster as compared with that coded in Java <ref type="bibr" target="#b18">[20]</ref>. Therefore, we conclude that SUN is the most efficient algorithm of the four in the overall consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Robustness</head><p>In this last subsection, we evaluate the ability of resilience to noise in SUN. In one dimension, Fig. <ref type="figure" target="#fig_10">6</ref> shows the experimental results of SUN evaluated on various values of ulr. We can observe that the more the noise contained in the current training data, the worse the performance on the prediction accuracy. With a fixed noise rate, the prediction accuracy is reduced very slowly till reaching a marginal value of ulr. For instance, when learning on LED with 30%-noise, the maximum fluctuation of error rate is limited to 4.0% in the case of ulr r 70% while it extends to 10% when ulr 4 70%. This is the same as the previous conclusion in Section 4.3. In the other dimension, the performance on the robustness to noise in SUN is compared against BagBest, CDRDT and CVFDT. Experimental results shown in Fig. <ref type="figure" target="#fig_5">7</ref> reveal that SUN is the best algorithm even in the case of ulr¼50% when learning on the databases with less than 10%-noise. In fact, the error rate in SUN is higher than that of BagBest by around 2.0% varying from the case with 15%-noise to the case with 30%-noise, and it is higher than that of CDRDT by a small margin (1.81-5.02%) until the database contains more than 25%-noise. However, it is always lower than that of CVFDT by a large margin (10.17% at least). These results confirm that SUN presents robust resilience to a certain amount of noise when there is a large rate of unlabeled data (e.g., ulr¼50%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presented a Semi-supervised classification algorithm for data streams with concept drifts and UNlabeled data  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>W 2 ¼ w 11 w 12 w 13 w 14 w 15 w 21 w 22 w 23 w 24 w 25</head><label>21525</label><figDesc>In this case, the value of FðW 2 ,M 2 Þ is equal to 3. Apparently, the constraint condition is satisfied, i.e., FðW 2 ,M 2 ÞÀFðW 1 ,M 2 Þ ¼ 0 o y, the clustering iteration terminates. After clustering, training instances in the current streaming data chunk are divided into corresponding clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>clusters will be replaced by the new ones, because new concept clusters reflect better the data distribution of class labels in the recent data chunk. It is necessary to note that in the first case, to avoid missing the detection on concept shifts occurring in the data distributions of class labels, we will further investigate the change of class labels in clusters, if class labels in the new concept clusters are completely different from those in the history concept clusters. It is also considered as an abrupt concept drift. In a word, our drifting detection strategy relies on the fact that concept drifts are caused by either the distribution change of attribute values or the distribution change of class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Cases of concept drifts.</figDesc><graphic coords="6,113.41,58.64,358.92,151.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of bottom-up search.</figDesc><graphic coords="6,34.62,246.93,247.32,92.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 4 .</head><label>4</label><figDesc>0 r simðm p ,m k Þ r1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 7 .</head><label>7</label><figDesc>0 r dist r1 Proof. In accordance with Theorem 5 and Definition 5, it is obvious to obtain the inequality of 0 r dist r 1. &amp;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 1 .</head><label>1</label><figDesc>Streaming data 4.1.1. Benchmark databases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Classification and clustering results.</figDesc><graphic coords="8,133.72,515.35,318.24,214.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Drift tracking on SEA.</figDesc><graphic coords="8,303.61,274.89,247.32,89.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Drift tracking on KDDCup99.</figDesc><graphic coords="8,303.61,394.72,247.32,84.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Robustness performance varying with ulr values.</figDesc><graphic coords="9,313.43,288.77,247.32,121.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Initial height of a tree: h 0 ; Count of minimum split-examples: n min ; Detection period: DP; Flag of checking before splitting: bCheck;</figDesc><table><row><cell>Output: Classification error 1: Generate a single leaf for the current decision tree-T, i.e, the root; 2: for each traininginstance-x 9: for each leaf from bottom to top 10: Create concept clusters in k-Modes; 11:</cell></row></table><note><p><p><p>x 0 j , . . .g; Attribute set: A; i A TR 3: Sort x i into a leaf-l; 4: Increase the total number of instances arrived at tree TÀn t ; 5: Load x i into the array of pLabeledArr or pUnLabeledArr; 6: if x i is a labeled instance 7:</p>Update the statistics relevant to arrays of pAttrDistr and pClassDistr;</p>8: if the number of arrived instances in TÀn t meets DP</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>w 11 w 12 w 13 w 14 w 15 w 21 w 22 w 23 w 24 w 25</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Five objects from STAGGER.</figDesc><table><row><cell>Objects</cell><cell>attr 0</cell><cell>attr 1</cell><cell>attr 2</cell><cell>Class</cell></row><row><cell>e 1</cell><cell>Red</cell><cell>Triangle</cell><cell>Small</cell><cell>1</cell></row><row><cell>e 2</cell><cell>Green</cell><cell>Circle</cell><cell>Large</cell><cell>0</cell></row><row><cell>e 3</cell><cell>Red</cell><cell>Triangle</cell><cell>Large</cell><cell>0</cell></row><row><cell>e 4</cell><cell>Red</cell><cell>Circle</cell><cell>Medium</cell><cell>0</cell></row><row><cell>e 5</cell><cell>Red</cell><cell>Triangle</cell><cell>Small</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Variable declaration.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Statistical results of drifting detection.</figDesc><table><row><cell>Database</cell><cell>Sea</cell><cell></cell><cell>STAGGER</cell><cell></cell><cell cols="2">KDDCup99</cell></row><row><cell>Concepts</cell><cell>4</cell><cell></cell><cell>100</cell><cell></cell><cell>23</cell><cell></cell></row><row><cell>Algorithm</cell><cell>SUN</cell><cell>CDRDT</cell><cell>SUN</cell><cell>CDRDT</cell><cell>SUN</cell><cell>CDRDT</cell></row><row><cell>Falarms (%)</cell><cell>0.98</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2.06</cell><cell>0</cell></row><row><cell>Missing</cell><cell>1</cell><cell>0</cell><cell>100</cell><cell>100</cell><cell>7</cell><cell>5</cell></row><row><cell>Delay (k)</cell><cell>4.34</cell><cell>0.60</cell><cell>/</cell><cell>/</cell><cell>7.90</cell><cell>5.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Error rates of classification.</figDesc><table><row><cell>Database</cell><cell cols="2">Error-rate (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SUN</cell><cell>CDRDT</cell><cell>CVFDT</cell><cell>BagBest</cell></row><row><cell>KDDCup99-490k-310k-CD</cell><cell>5.32</cell><cell>9.17</cell><cell>23.48</cell><cell>9.18</cell></row><row><cell>STAGGER-100k-50k-D</cell><cell>0</cell><cell>0</cell><cell>10.89</cell><cell>10.89</cell></row><row><cell>Shopping-84k-28k-CD</cell><cell>2.08</cell><cell>14.63</cell><cell>23.35</cell><cell>5.86</cell></row><row><cell>SEA-100k-50k-C</cell><cell>18.27</cell><cell>18.28</cell><cell>16.59</cell><cell>17.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Overheads of runtime.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X.Wu  et al. / Neurocomputing 92 (2012) 145-155</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The variant of bagging with the best performance in the experiments is selected as the baseline algorithm, i.e., BagAHST or BagADWIN, collectively called BagBest. X. Wu et al. / Neurocomputing 92 (2012) 145-155</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research has been supported by the 973 Program of China under award 2009CB326203, the National Natural Science Foundation of China (NSFC) under Grants 60828005 and 60975034, the Natural Science Foundation of Anhui Province under Grant 090412044, and Special Funds of Thousand Talents Program under Grant 2010HGXJ0715.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FlockStream: a bio-inspired algorithm for clustering evolving data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Forestiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ITCAI&apos;09</title>
		<meeting>the ITCAI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavald</surname></persName>
		</author>
		<title level="m">New ensemble methods for evolving data streams</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
	<note>Proceedings of the KDD&apos;09</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TOPSIL-Miner: an efficient algorithm for mining top-K significant itemsets over data streams</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="242" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On classification and segmentation of massive audio data streams</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="156" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for clustering evolving data streams</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th VLDB Conference</title>
		<meeting>the 29th VLDB Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ocvfdt: one-class very fast decision tree for one-class classification of data streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD&apos;09</title>
		<meeting>the KDD&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data in concept drift learning</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Widyantoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="54" to="62" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relevant data expansion for learning concept drift from sparsely labeled data</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Widyantoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking drifting concepts by minimizing disagreement</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="27" to="45" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved heterogeneous distance functions</title>
		<author>
			<persName><forename type="first">D.-R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential pattern mining in multiple streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICDM&apos;05</title>
		<meeting>the ICDM&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="585" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Moa: massive online analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/moa-datastreamS" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining time-changing data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD&apos;01</title>
		<meeting>the KDD&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Consistent collective evaluation of multiple continuous queries for filtering heterogeneous data streams</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="210" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining concept-drifting data streams using ensemble classifiers</title>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD&apos;03</title>
		<meeting>the KDD&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="226" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental learning from noisy data</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Schlimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-H</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="354" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Issues in evaluation of stream learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sebastião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD&apos;09</title>
		<meeting>the KDD&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ambiguous decision trees for mining concept-drifting data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1347" to="1355" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Technical opinion: comparing Java vs. C/Cþþ efficiency differences to interpersonal differences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="109" to="112" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A random decision tree ensemble for mining concept drifts from noisy data streams</title>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="680" to="710" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clustering-training for data stream mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICDMW&apos;06</title>
		<meeting>the ICDMW&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning higher accuracy decision trees from concept drifting data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Industrial, Engineering &amp; Other Applications of Applied Intelligent Systems</title>
		<meeting>the 21st International Conference on Industrial, Engineering &amp; Other Applications of Applied Intelligent Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the impact of dissimilarity measure in k-modes clustering algorithm</title>
		<author>
			<persName><forename type="first">M.-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="503" to="507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A practical approach to classify evolving data streams: Training with limited amount of labeled data</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Masud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thuraisingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICDM&apos;08</title>
		<meeting>the ICDM&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decision tree evolution using limited number of labeled data items from drifting data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICDM&apos;04</title>
		<meeting>the ICDM&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="379" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nile: a query processing engine for data streams</title>
		<author>
			<persName><forename type="first">W.-G</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICDE&apos;04</title>
		<meeting>the ICDE&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="851" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A streaming ensemble algorithm (sea) for large-scale classification</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD&apos;01</title>
		<meeting>the KDD&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<ptr target="http://developer.yahoo.com/everythingS" />
	</analytic>
	<monogr>
		<title level="j">Yahoo Shopping Web Services</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient mining of skyline objects in subspaces over data streams</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="183" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
