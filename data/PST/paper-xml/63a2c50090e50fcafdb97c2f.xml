<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching</title>
				<funder>
					<orgName type="full">Guoqiang Institute, Tsinghua University</orgName>
				</funder>
				<funder ref="#_KKX3ygn">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">National Social Science Fund of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2,3,4,5</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">International Innovation Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
							<email>lipeng@air.tsinghua.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Institute for AI Industry Research</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>2,3,4,5</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">International Innovation Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Quan Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently there has been an emerging interest in unsupervised vision-and-language pre-training (VLP) that learns multimodal representations without parallel image-caption data. These pioneering works significantly reduce the cost of VLP on data collection and achieve promising results compared to supervised VLP. However, existing unsupervised VLP methods take as input pre-extracted region-based visual features from external object detectors, which both limits flexibility and reduces computational efficiency. In this paper, we explore end-to-end unsupervised VLP with a vision encoder to directly encode images. The vision encoder is pre-trained on image-only data and jointly optimized during multimodal pre-training. To further enhance the learned cross-modal features, we propose a novel pre-training task that predicts which patches contain an object referred to in natural language from the encoded visual features. Extensive experiments on four visionand-language tasks show that our approach outperforms previous unsupervised VLP methods and obtains new state-of-the-art results 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision-and-language pre-training (VLP) <ref type="bibr" target="#b24">(Lu et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr" target="#b11">Kim et al., 2021;</ref><ref type="bibr">Li et al., 2021c;</ref><ref type="bibr" target="#b37">Zhang et al., 2021;</ref><ref type="bibr" target="#b29">Radford et al., 2021;</ref><ref type="bibr" target="#b29">Ramesh et al., 2021)</ref> has achieved great success on a wide range of vision-and-language tasks, e.g., visual question answering <ref type="bibr" target="#b37">(Zhang et al., 2021)</ref>, image-text retrieval <ref type="bibr" target="#b29">(Radford et al., 2021)</ref> and text-to-image generation <ref type="bibr" target="#b29">(Ramesh et al., 2021)</ref>. The major challenge for VLP is how to bridge the gap between the representations of vision and language modalities, which is typically ad-dressed by training on large-scale parallel imagetext datasets <ref type="bibr" target="#b18">(Lin et al., 2014;</ref><ref type="bibr" target="#b12">Krishna et al., 2017;</ref><ref type="bibr" target="#b30">Sharma et al., 2018;</ref><ref type="bibr" target="#b25">Ordonez et al., 2011)</ref> with specially designed pre-training tasks. However, these datasets require either extensive human annotations or massive data cleaning efforts, making them difficult to collect, especially when compared to the large amount of unimodal data.</p><p>To alleviate this problem, there has recently emerged some works exploring unsupervised vision-and-language pre-training (UVLP), where only non-parallel image and text data is leveraged <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b38">Zhou et al., 2022)</ref>. Specifically, <ref type="bibr">Li et al. (2021b)</ref> propose to use image region features and their detected object tags produced by an object detector as pesudo-parallel pairs to bridge the gap between the two modalities. <ref type="bibr" target="#b38">Zhou et al. (2022)</ref> further enrich the training data with retrieved text pieces based on object tags and pre-train their model with multi-granular alignment tasks. These works achieve competitive results compared to several supervised VLP models, demonstrating the potential of UVLP.</p><p>However, current research on UVLP adopts a two-step training strategy that first extracts regionbased image features with an external object detector and then builds a multimodal model based on the region features. This is considered to have several limitations for VLP. First, region features may be sub-optimal for VLP because they are designed for object detection tasks rather than general cross-modal understanding and are fixed in the pre-training process <ref type="bibr" target="#b33">(Xu et al., 2021;</ref><ref type="bibr" target="#b8">Huang et al., 2021)</ref>. Second, the process of extracting region features is time-consuming, which significantly reduces the inference efficiency <ref type="bibr" target="#b11">(Kim et al., 2021)</ref>. Finally, this two-step training strategy hinders the use of vision pre-trained models (V-PTMs) such as ViT <ref type="bibr" target="#b5">(Dosovitskiy et al., 2020)</ref> and Swin Transformer <ref type="bibr">(Liu et al., 2021c)</ref>, which are not off-the-shelf object detectors but achieve promising performance on general vision tasks. Therefore, how to perform UVLP in an end-to-end manner, i.e., using raw images instead of region features, is still a valuable open question.</p><p>To explore the question, we propose an endto-end UVLP framework named E2E-UVLP. The framework consists of a vision encoder and a pretrained language model (PLM), both of which are pre-trained on unimodal data and they are connected by a linear projection layer. Taking image patches as input, our framework is capable of leveraging a wide range of V-PTMs. Without using object tags in inference, the computational cost introduced by external object detectors is eliminated. Inspired by previous works <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b38">Zhou et al., 2022;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b17">Li et al., 2020)</ref>, we derive a masked tag prediction (MTP) pre-training task which predicts the masked object tags given a raw image and the other object tags detected from it. Combining MTP with the widely used masked language modeling (MLM) task <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>, we successfully make E2E-UVLP achieve comparable or better results than existing UVLP methods, justifying end-to-end UVLP is feasible.</p><p>Although the MTP task is effective, further investigation reveals that the obtained model is less effective when dealing with complex attributes of objects, e.g., locating objects or determining the relationship between objects in an image. We argue it is due to two pitfalls of the MTP objective: (1) Discrepancy between training and inference: An object is referred to by its tag and numerically encoded position in training, while referred to only in natural language in inference. Similar discrepancies have been shown to hurt performance significantly in PLM studies <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr">Liu et al., 2021a)</ref>.</p><p>(2) Natural language expression insensitivity: As both the tag and the position of an object have been given in training, the model does not need to locate or distinguish objects by itself, not to say grounding natural language expressions on visual concepts. To alleviate the problems, we propose a novel pre-training task named referring expression matching (REM). Given an image split into patches and an object tag, we convert the tag into a referring expression heuristically (e.g., "man on the right") and predict which patches contain the referred object. By using the synthetic referring expressions in training, the discrepancy has been reduced. Moreover, thanks to the promising language processing ability of PLMs, the obtained model generalizes well from the limited heuristically selected expressions to unseen ones in training, resulting in better downstream task performance.</p><p>In summary, our contributions are three-fold:</p><p>? A novel framework E2E-UVLP is proposed to perform end-to-end unsupervised vision-andlanguage pre-training without using costly and sub-optimal region features relied heavily by previous works.</p><p>? A referring expression matching pre-training task is proposed to reduce training-inference discrepancy and improve generalization to richer natural language expressions.</p><p>? Extensive experiments on four representative vision-and-language tasks show the superiority of our proposed framework over strong unsupervised baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>As shown in Figure <ref type="figure" target="#fig_6">1</ref>, our proposed E2E-UVLP consists of a vision encoder and a pre-trained language model acting as a multimodal encoder. The vision encoder can be a vision pre-trained model such as ViT <ref type="bibr" target="#b5">(Dosovitskiy et al., 2020)</ref> and Swin Transformer <ref type="bibr">(Liu et al., 2021c)</ref>. Each image I is encoded by the vision encoder into a sequence of patch features. These patch features are then linearly projected and added with corresponding modal-type embeddings to form the vision representations V = {v 1 , . . . , v N } where N is the number of patches. For text input L, it is tokenized into a sequence of word tokens. Each token's representation t i is the sum of its word embeddings, its position or location embeddings (depending on the type of text input), and its modal-type embeddings. The resulting text representations T = {t 1 , . . . , t M } are then concatenated with V and fed into the multimodal encoder to get multimodal representations. We use BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> to initialize the multimodal encoder and word embeddings. Finally, the multimodal representations are used for different kinds of pre-training and fine-tuning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pesudo-Parallel Data Synthesis</head><p>Since there is no parallel image-text data available in unsupervised vision-and-language pretraining (UVLP), it is important to find other ways</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E2E-UVLP</head><p>Referring Expression</p><formula xml:id="formula_0">0 0 0 0 0 1 1 0 0</formula><p>running man in the middle Task-2: Referring Expression Matching        to bridge the gap between the two modalities. Inspired by <ref type="bibr">Li et al. (2021b)</ref>, we use detected object tags of each image to synthesize pesudo-parallel image-text data, which are used in the pre-training tasks for E2E-UVLP. Specifically, for each image I, we use an external object detector to generate its object proposals</p><formula xml:id="formula_1">I = " &gt; A A A B 9 X i c b V A 9 S w N B E N 3 z M 8 a v q K X N Y h C s w p 0 I W g Z t L C w i m A 9 I z r C 3 m S R L 9 v a O 3 T k 1 H P c / b C w U s f W / 2 P l v 3 C R X a O K D g c d 7 M 8 z M C 2 I p D L r u t 7 O 0 v L K 6 t l 7 Y K G 5 u b e / s l v b 2 G y Z K N I c 6 j 2 S k W w E z I I W C O g q U 0 I o 1 s D C Q 0 A x G V x O / + Q D a i E j d 4 T g G P 2 Q D J f q C M 7 T S / U 0 3 7 S A 8 Y Y p s k G X d U t m t u F P Q R e L l p E x y 1 L q l r 0 4 v 4 k k I C r l k x r Q 9 N 0 Y / Z R o F l 5 A V O 4 m B m P E R G 0 D b U s V C M H 4 6 v T q j x 1 b p 0 X 6 k b S m k U / X 3 R M p C Y 8 Z h Y D t D h k M z 7 0 3 E / 7 x 2 g v 0 L P x U q T h A U n y 3 q J 5 J i R C c R 0 J 7 Q w F G O L W F c C 3 s r 5 U O m G U c b V N G G 4 M 2 / v E g a p x X P r X i 3 Z + X q Z R 5 H g R y S I 3 J C P H J O q u S a 1 E i d c K L J M 3 k l b 8 6 j 8 + K 8 O x + z 1 i U n n z k g f + B 8 / g B C I J L + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d b I 2 O J / P C 4 Y B K t y R 2 v F P i d f H w 0 I = " &gt; A A A B 9 X i c b V A 9 S w N B E N 3 z M 8 a v q K X N Y h C s w p 0 I W g Z t L C w i m A 9 I z r C 3 m S R L 9 v a O 3 T k 1 H P c / b C w U s f W / 2 P l v 3 C R X a O K D g c d 7 M 8 z M C 2 I p D L r u t 7 O 0 v L K 6 t l 7 Y K G 5 u b e / s l v b 2 G y Z K N I c 6 j 2 S k W w E z I I W C O g q U 0 I o 1 s D C Q 0 A x G V x O / + Q D a i E j d 4 T g G P 2 Q D J f q C M 7 T S / U 0 3 7 S A 8 Y Y p s k G X d U t m t u F P Q R e L l p E x y 1 L q l r 0 4 v 4 k k I C r l k x r Q 9 N 0 Y / Z R o F l 5 A V O 4 m B m P E R G 0 D b U s V C M H 4 6 v T q j x 1 b p 0 X 6 k b S m k U / X 3 R M p C Y 8 Z h Y D t D h k M z 7 0 3 E / 7 x 2 g v 0 L P x U q T h A U n y 3 q J 5 J i R C c R 0 J 7 Q w F G O L W F c C 3 s r 5 U O m G U c b V N G G 4 M 2 / v E g a p x X P r X i 3 Z + X q Z R 5 H g R y S I 3 J C P H J O q u S a 1 E i d c K L J M 3 k l b 8 6 j 8 + K 8 O x + z 1 i U n n z k g f + B 8 / g B C I J L + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d b I 2 O J / P C 4 Y B K t y R 2 v F P i d f H w 0 I = " &gt; A A A B 9 X i c b V A 9 S w N B E N 3 z M 8 a v q K X N Y h C s w p 0 I W g Z t L C w i m A 9 I z r C 3 m S R L 9 v a O 3 T k 1 H P c / b C w U s f W / 2 P l v 3 C R X a O K D g c d 7 M 8 z M C 2 I p D L r u t 7 O 0 v L K 6 t l 7 Y K G 5 u b e / s l v b 2 G y Z K N I c 6 j 2 S k W w E z I I W C O g q U 0 I o 1 s D C Q 0 A x G V x O / + Q D a i E j d 4 T g G P 2 Q D J f q C M 7 T S / U 0 3 7 S A 8 Y Y p s k G X d U t m t u F P Q R e L l p E x y 1 L q l r 0 4 v 4 k k I C r l k x r Q 9 N 0 Y / Z R o F l 5 A V O 4 m B m P E R G 0 D b U s V C M H 4 6 v T q j x 1 b p 0 X 6 k b S m k U / X 3 R M p C Y 8 Z h Y D t D h k M z 7 0 3 E / 7 x 2 g v 0 L P x U q T h A U n y 3 q J 5 J i R C c R 0 J 7 Q w F G O L W F c C 3 s r 5 U O m G U c b V N G G 4 M 2 / v E g a p x X P r X i 3 Z + X q Z R 5 H g R y S I 3 J C P H J O q u S a 1 E i d c K L J M 3 k l b 8 6 j 8 + K 8 O x + z 1 i U n n z k g f + B 8 / g B C I J L + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d b I 2 O J / P C 4 Y B K t y R 2 v F P i d f H w 0 I = " &gt; A A A B 9 X i c b V A 9 S w N B E N 3 z M 8 a v q K X N Y h C s w p 0 I W g Z t L C w i m A 9 I z r C 3 m S R L 9 v a O 3 T k 1 H P c / b C w U s f W / 2 P l v 3 C R X a O K D g c d 7 M 8 z M C 2 I p D L r u t 7 O 0 v L K 6 t l 7 Y K G 5 u b e / s l v b 2 G y Z K N I c 6 j 2 S k W w E z I I W C O g q U 0 I o 1 s D C Q 0 A x G V x O / + Q D a i E j d 4 T g G P 2 Q D J f q C M 7 T S / U 0 3 7 S A 8 Y Y p s k G X d U t m t u F P Q R e L l p E x y 1 L q l r 0 4 v 4 k k I C r l k x r Q 9 N 0 Y / Z R o F l 5 A V O 4 m B m P E R G 0 D b U s V C M H 4 6 v T q j x 1 b p 0 X 6 k b S m k U / X 3 R M p C Y 8 Z h Y D t D h k M z 7 0 3 E / 7 x 2 g v 0 L P x U q T h A U n y 3 q J 5 J i R C c R 0 J 7 Q w F G O L W F c C 3 s r 5 U O m G U c b V N G G 4 M 2 / v E g a p x X P r X i 3 Z + X q Z R 5 H g R y S I 3 J C P H J O q u S a 1 E i d c K L J M 3 k l b 8 6 j 8 + K 8 O x + z 1 i U n n z k g f + B 8 / g B C I J L + &lt; / l a t e x i t &gt; L exp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F J 3 b 8 6 Q 3 C 6 Q y p s N U T q n v s + I v H k = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S Q i 6 L H o x Y O H C v Y D 2 l g 2 2 0 m 7 d L M J u x N t C f k f X j w o 4 t X / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 8 L r t F x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a O o o U Q w a L B K R a v t U g + A S G s h R Q D t W Q E N f Q M s f X U / 9 1 i M o z S N 5 j 5 M Y v J A O J A 8 4 o 2 i k h 9 t e 2 k U Y Y w r j O M t 6 5 Y p T d W a w l 4 m b k w r J U e + V v 7 r 9 i C U h S G S C a t 1 x n R i 9 l C r k T E B W 6 i Y a Y s p G d A A d Q y U N Q X v p 7 O r M P j F K 3 w 4 i Z U q i P V N / T 6 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r J B h c e i m X c Y I g 2 X x R k A g b I 3 s a g d 3 n C h i K i S G U K W 5 u t d m Q K s r Q B F U y I b i L L y + T 5 l n V d a r u 3 X m l d p X H U S R H 5 J i c E p d c k B q 5 I X X S I I w o 8 k x e y Z v 1 Z L 1 Y 7 9 b H v L V g 5 T O H 5 A + s z x 9 b / 5 M P &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F J 3 b 8 6 Q 3 C 6 Q y p s N U T q n v s + I v H k = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S Q i 6 L H o x Y O H C v Y D 2 l g 2 2 0 m 7 d L M J u x N t C f k f X j w o 4 t X / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 8 L r t F x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a O o o U Q w a L B K R a v t U g + A S G s h R Q D t W Q E N f Q M s f X U / 9 1 i M o z S N 5 j 5 M Y v J A O J A 8 4 o 2 i k h 9 t e 2 k U Y Y w r j O M t 6 5 Y p T d W a w l 4 m b k w r J U e + V v 7 r 9 i C U h S G S C a t 1 x n R i 9 l C r k T E B W 6 i Y a Y s p G d A A d Q y U N Q X v p 7 O r M P j F K 3 w 4 i Z U q i P V N / T 6 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r J B h c e i m X c Y I g 2 X x R k A g b I 3 s a g d 3 n C h i K i S G U K W 5 u t d m Q K s r Q B F U y I b i L L y + T 5 l n V d a r u 3 X m l d p X H U S R H 5 J i c E p d c k B q 5 I X X S I I w o 8 k x e y Z v 1 Z L 1 Y 7 9 b H v L V g 5 T O H 5 A + s z x 9 b / 5 M P &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F J 3 b 8 6 Q 3 C 6 Q y p s N U T q n v s + I v H k = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S Q i 6 L H o x Y O H C v Y D 2 l g 2 2 0 m 7 d L M J u x N t C f k f X j w o 4 t X / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 8 L r t F x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a O o o U Q w a L B K R a v t U g + A S G s h R Q D t W Q E N f Q M s f X U / 9 1 i M o z S N 5 j 5 M Y v J A O J A 8 4 o 2 i k h 9 t e 2 k U Y Y w r j O M t 6 5 Y p T d W a w l 4 m b k w r J U e + V v 7 r 9 i C U h S G S C a t 1 x n R i 9 l C r k T E B W 6 i Y a Y s p G d A A d Q y U N Q X v p 7 O r M P j F K 3 w 4 i Z U q i P V N / T 6 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r J B h c e i m X c Y I g 2 X x R k A g b I 3 s a g d 3 n C h i K i S G U K W 5 u t d m Q K s r Q B F U y I b i L L y + T 5 l n V d a r u 3 X m l d p X H U S R H 5 J i c E p d c k B q 5 I X X S I I w o 8 k x e y Z v 1 Z L 1 Y 7 9 b H v L V g 5 T O H 5 A + s z x 9 b / 5 M P &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F J 3 b 8 6 Q 3 C 6 Q y p s N U T q n v s + I v H k = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S Q i 6 L H o x Y O H C v Y D 2 l g 2 2 0 m 7 d L M J u x N t C f k f X j w o 4 t X / 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m n h 8 L r t F x v q 3 C y u r a + k Z x s 7 S 1 v b O 7 V 9 4 / a O o o U Q w a L B K R a v t U g + A S G s h R Q D t W Q E N f Q M s f X U / 9 1 i M o z S N 5 j 5 M Y v J A O J A 8 4 o 2 i k h 9 t e 2 k U Y Y w r j O M t 6 5 Y p T d W a w l 4 m b k w r J U e + V v 7 r 9 i C U h S G S C a t 1 x n R i 9 l C r k T E B W 6 i Y a Y s p G d A A d Q y U N Q X v p 7 O r M P j F K 3 w 4 i Z U q i P V N / T 6 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r J B h c e i m X c Y I g 2 X x R k A g b I 3 s a g d 3 n C h i K i S G U K W 5 u t d m Q K s r Q B F U y I b i L L y + T 5 l n V d a r u 3 X m l d p X H U S R H 5 J i c E p d c k B q 5 I X X S I I w o 8 k x e y Z v 1 Z L 1 Y 7 9 b H v L V g 5 T O H 5 A + s z x 9 b / 5 M P &lt; / l a t e x i t &gt; L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D L n N b 2 n e U x r h v C b + 3 0 Q L f 7 f T 2 E = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i w c P V a w t t K F s t p N 2 6 W Y T d j d C C f 0 H X j w o 4 t V / 5 M 1 / 4 6 b N Q V s f D D z e m 2 F m X p A I r o 3 r f j u l l d W 1 9 Y 3 y Z m V r e 2 d 3 r 7 p / 8 K j j V D F s s V j E q h N Q j Y J L b B l u B H Y S h T Q K B L a D 8 X X u t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s d H 9 b 6 V d r b t 2 d g S w T r y A 1 K N D s V 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 5 V e q j G h b E y H 2 L V U 0 g i 1 n 8 0 u n Z I T q w x I G C t b 0 p C Z + n s i o 5 H W k y i w n R E 1 I 7 3 o 5 e J / X j c 1 4 a W f c Z m k B i W b L w p T Q U x M 8 r f J g C t k R k w s o U x x e y t h I 6 o o M z a c P A R v 8 e V l 8 n h W 9 9 y 6 d 3 d e a 1 w V c Z T h C I 7 h F D y 4 g A b c Q B N a w C C E Z 3 i F N 2 f s v D j v z s e 8 t e Q U M 4 f w B 8 7 n D 9 f h j O Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D L n N b 2 n e U x r h v C b + 3 0 Q L f 7 f T 2 E = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i w c P V a w t t K F s t p N 2 6 W Y T d j d C C f 0 H X j w o 4 t V / 5 M 1 / 4 6 b N Q V s f D D z e m 2 F m X p A I r o 3 r f j u l l d W 1 9 Y 3 y Z m V r e 2 d 3 r 7 p / 8 K j j V D F s s V j E q h N Q j Y J L b B l u B H Y S h T Q K B L a D 8 X X u t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s d H 9 b 6 V d r b t 2 d g S w T r y A 1 K N D s V 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 5 V e q j G h b E y H 2 L V U 0 g i 1 n 8 0 u n Z I T q w x I G C t b 0 p C Z + n s i o 5 H W k y i w n R E 1 I 7 3 o 5 e J / X j c 1 4 a W f c Z m k B i W b L w p T Q U x M 8 r f J g C t k R k w s o U x x e y t h I 6 o o M z a c P A R v 8 e V l 8 n h W 9 9 y 6 d 3 d e a 1 w V c Z T h C I 7 h F D y 4 g A b c Q B N a w C C E Z 3 i F N 2 f s v D j v z s e 8 t e Q U M 4 f w B 8 7 n D 9 f h j O Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D L n N b 2 n e U x r h v C b + 3 0 Q L f 7 f T 2 E = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i w c P V a w t t K F s t p N 2 6 W Y T d j d C C f 0 H X j w o 4 t V / 5 M 1 / 4 6 b N Q V s f D D z e m 2 F m X p A I r o 3 r f j u l l d W 1 9 Y 3 y Z m V r e 2 d 3 r 7 p / 8 K j j V D F s s V j E q h N Q j Y J L b B l u B H Y S h T Q K B L a D 8 X X u t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s d H 9 b 6 V d r b t 2 d g S w T r y A 1 K N D s V 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 5 V e q j G h b E y H 2 L V U 0 g i 1 n 8 0 u n Z I T q w x I G C t b 0 p C Z + n s i o 5 H W k y i w n R E 1 I 7 3 o 5 e J / X j c 1 4 a W f c Z m k B i W b L w p T Q U x M 8 r f J g C t k R k w s o U x x</formula><formula xml:id="formula_2">Q L f 7 f T 2 E = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i w c P V a w t t K F s t p N 2 6 W Y T d j d C C f 0 H X j w o 4 t V / 5 M 1 / 4 6 b N Q V s f D D z e m 2 F m X p A I r o 3 r f j u l l d W 1 9 Y 3 y Z m V r e 2 d 3 r 7 p / 8 K j j V D F s s V j E q h N Q j Y J L b B l u B H Y S h T Q K B L a D 8 X X u t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s d</formula><formula xml:id="formula_3">W p Q o N m v f v U G M U s j l I Y J q n X X c x P j Z 1 Q Z z g R O K 7 1 U Y 0 L Z m A 6 x a 6 m k E W o / m 1 0 6 J S d W G Z A w V r a k I T P 1 9 0 R G I 6 0 n U W A 7 I 2 p G e t H L x f + 8 b m r C S z / j M k k N S j Z f F K a C m J j k b 5 M B V 8 i M m F h C m e L 2 V s J G V F F m b D h 5 C N 7 i y 8 v k 8 a z u u X X v 7 r z W u C r i K M M R H M M p e H A B D b i B J r S A Q Q j P 8 A p v z t</formula><formula xml:id="formula_4">W p Q o N m v f v U G M U s j l I Y J q n X X c x P j Z 1 Q Z z g R O K 7 1 U Y 0 L Z m A 6 x a 6 m k E W o / m 1 0 6 J S d W G Z A w V r a k I T P 1 9 0 R G I 6 0 n U W A 7 I 2 p G e t H L x f + 8 b m r C S z / j M k k N S j Z f F K a C m J j k b 5 M B V 8 i M m F h C m e L 2 V s J G V F F m b D h 5 C N 7 i y 8 v k 8 a z u u X X v 7 r z W u C r i K M M R H M M p e H A B D b i B J r S A Q Q j P 8 A p v z t</formula><formula xml:id="formula_5">W p Q o N m v f v U G M U s j l I Y J q n X X c x P j Z 1 Q Z z g R O K 7 1 U Y 0 L Z m A 6 x a 6 m k E W o / m 1 0 6 J S d W G Z A w V r a k I T P 1 9 0 R G I 6 0 n U W A 7 I 2 p G e t H L x f + 8 b m r C S z / j M k k N S j Z f F K a C m J j k b 5 M B V 8 i M m F h C m e L 2 V s J G V F F m b D h 5 C N 7 i y 8 v k 8 a z u u X X v 7 r z W u C r i K M M R H M M p</formula><formula xml:id="formula_6">W p Q o N m v f v U G M U s j l I Y J q n X X c x P j Z 1 Q Z z g R O K 7 1 U Y 0 L Z m A 6 x a 6 m k E W o / m 1 0 6 J S d W G Z A w V r a k I T P 1 9 0 R G I 6 0 n U W A 7 I 2 p G e t H L x f + 8 b m r C S z / j M k k N S j Z f F K a C m J j k b 5 M B V 8 i M m F h C m e L 2 V s J G V F F m b D h 5 C N 7 i y 8 v k 8 a z u u X X v 7 r z W u C r i K M M R H M M p</formula><formula xml:id="formula_7">{(o i , b i )} K i=1</formula><p>, where o i is the object tag and b i ? R 4 denotes the bounding box location. We derive two kinds of pesudo-parallel image-text data for each image and its corresponding object proposals.</p><p>Image-Tag Pair Because {o i } K i=1 are essentially a bag of text words that describe the objects detected in the image, they can be viewed as text data weakly aligned with the image. We concatenate the object tags to form a text input L tag = o 1 , . . . , o K , and compute the location embeddings for o i as a linear projection of b i . Each token in the tokenized L tag is embedded as the sum of its word embeddings and location embeddings. The reason for using location embeddings instead of position embeddings as for natural text is to distinguish between objects of the same category in the same image. We name this kind of synthetic data (I, L tag ) ? D tag as image-tag paired data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Expression Pair</head><p>The text input in imagetag paired data differs from natural text in two ways. First, it is composed of only noun words and does not conform to the grammar of natural language. Second, the location annotations do not exist in real text. We will discuss later the limitations of models trained on such data in Section 2.3. Here we describe another kind of synthetic pesudo-parallel data called image-expression paired data, which is more similar to real text. The idea is to generate a referring expression for one object in the image that can distinguish it from other objects (e.g., "smaller white sheep on the right" in Figure <ref type="figure" target="#fig_7">2</ref>). Specifically, for an image I and its detected object proposals  <ref type="formula">2021</ref>), we consider attributes generated from the object detectors for these objects, as well as the the relative size and position between the target object and other objects of the same object class. For the first example in Figure <ref type="figure" target="#fig_7">2</ref>, as the area of the bounding box of the right object is smaller than that of the left one, we add a size description "smaller" to the expression. The final referring expression L exp is the combination of size, attribute, object and position descriptions, guaranteed to refer to the target object in the image. L exp and I together form the image-expression paired data (I, L exp ) ? D exp .</p><formula xml:id="formula_8">{(o i , b i )} K i=1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-training Tasks</head><p>In this section, we introduce the pre-training tasks that enable our proposed E2E-UVLP to learn effective multimodal representations using only nonparallel image and text data without region features.</p><p>Masked Tag Prediction (MTP) This task aims to learn the alignment of the object concepts in two modalities using the image-tag paired data. Inspired by <ref type="bibr">Li et al. (2021b)</ref>, we randomly mask out the tags in L tag and predict the masked tags conditioned on the raw image and other tags. Note that region features are conditioned on instead of the raw image in <ref type="bibr">(Li et al., 2021b)</ref>. Specifically, the objective for MTP is computed as follows:</p><formula xml:id="formula_9">L MTP = -E (I,Ltag)?Dtag log P T m | T \m , V ,</formula><p>(1) where T m and T \m denote the masked tags and observed tags, respectively. For the masked tags, we keep the original bounding box locations and replace only the object tags with the special mask tokens. Different from previous works, we do not apply masked vision modeling (MVM) for the image-tag paired data, as it has been shown to cause performance degradation for end-to-end VLP <ref type="bibr" target="#b6">(Dou et al., 2022)</ref>. We also study object-guided masked vision modeling <ref type="bibr">(Liu et al., 2021b)</ref>, which aims to predict region features from grid image features, and observe no improvement compared to using MTP alone. We assume this is because L tag already carries the object information, thus making such a task trivial.</p><p>Although MTP is effective, we find that the models trained with it are less effective in dealing with complex attributes of objects. For example in Figure <ref type="figure" target="#fig_9">3</ref>, the model pre-trained with MTP provides an incorrect answer probably because it fails to focus on the correct patches of the object required in the question. We assume that this is due to two pitfalls of the MTP objective: (1) Discrepancy between training and inference. During pre-training, the text input L tag is composed of objects with their tags and bounding box locations, unlike natural language sentences used in inference, which use positional encoding. In PLM studies, similar discrepancies have been shown to significantly impair performance <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr">Liu et al.,</ref>   2021a). ( <ref type="formula" target="#formula_10">2</ref>) Natural language expression insensitivity. When trained with MTP, the model only needs to predict the tags of masked objects at given locations, rather than identifying target objects corresponding to natural language expressions from raw images. This problem is more pronounced in end-to-end UVLP because the images are encoded without using any object information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Matching (REM)</head><p>To alleviate the deficiencies of MTP, we design this novel pre-training task based on the image-expression paired data described in Section 2.2. The task of REM is to predict the position of the object referred to by the synthetic referring expression L exp .</p><p>We use the bounding box of the referred object as ground truth and convert it to a binary mask R ? {0, 1} N of the same size as the patch features, where the values corresponding to the inside of the bounding box are set to 1 and the others are set to 0. Then, given the model's prediction R = f (I, L exp ) ? [0, 1] N , we define the REM objective as</p><formula xml:id="formula_10">L REM = E (I,Lexp)?Dexp DL(R, R)+BCE(R, R),<label>(2)</label></formula><p>where DL is the soft dice loss</p><formula xml:id="formula_11">DL(R, R) = 1 - 2 N i=1 r i ? ri N i=1 r i + N i=1 ri ,<label>(3)</label></formula><p>and BCE is the binary cross entropy loss</p><formula xml:id="formula_12">BCE(R, R) = - N i=1 (1 -r i ) log(1 -ri ) +r i log(r i ) .<label>(4)</label></formula><p>We use these two losses because they have been proven to be effective in image segmentation <ref type="bibr" target="#b9">(Isensee et al., 2018)</ref>, which aims to classify each pixel in an image into a certain object class. We assume that using REM as a pre-training task can complement MTP in two ways. First, it will alleviate the discrepancy between training and inference as the model observes the text input in the form of natural language. Second, it explicitly enforces the model to localize the referred object from patch features, which strengthens the alignment between the learned visual concepts and the related linguistic expressions. As shown in Figure <ref type="figure" target="#fig_9">3</ref>, the model pre-trained with MTP and REM successfully locates the corresponding image patches and gives the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Language Modeling (MLM)</head><p>We also apply MLM on the text-only input to predict the masked tokens based on the surrounding text context. Given text input L from the text-only corpus D L , we formulate the MLM objective as</p><formula xml:id="formula_13">L MLM = -E L?D L log P T m | T \m .<label>(5)</label></formula><p>Note that no aligned images are observed in the computation of the MLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Following previous UVLP works <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b38">Zhou et al., 2022)</ref>, we take images and captions from Conceptual Captions (CC) <ref type="bibr" target="#b30">(Sharma et al., 2018)</ref> without the alignment information to construct the unsupervised image and text datasets.</p><p>We also try a more realistic setting to use images from CC and sentences from BookCorpus <ref type="bibr" target="#b39">(Zhu et al., 2015)</ref> where images and text are collected seperately from different domains. Similar to <ref type="bibr">(Li et al., 2021b)</ref>, we downsample the BookCorpus dataset to ensure the number of sentences in each training epoch is the same as the number of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our E2E-UVLP with both supervised and unsupervised vision-language pre-trained models. For supervised vision-language pre-trained models, we compare with models using different kinds of image features including region features (VisualBERT <ref type="bibr" target="#b14">(Li et al., 2019)</ref>, UNITER <ref type="bibr" target="#b3">(Chen et al., 2020)</ref> and VinVL <ref type="bibr" target="#b37">(Zhang et al., 2021)</ref>), grid features (E2E-VLP <ref type="bibr" target="#b33">(Xu et al., 2021)</ref> and sistently outperforms previous UVLP methods on all downstream tasks, which demonstrates that our end-to-end approach can learn a better cross-modal representation than the approaches using region features. When compared to supervised VLP models, our model achieves competitive results. Specifically, our model achieves a VQA score of 73.3% on the test-dev split, which is even higher than the performance of some supervised models. Finally, note that in the supervised VLP, the best model using region features (VinVL) performs better than the models using other types of image features, while in the unsupervised setting our approach using patch features outperforms the methods based on the same region features of VinVL. We attribute this to the use of the pre-training task is more suitable for patch features in unsupervised VLP, i.e., REM (Section 2.3).</p><p>We also investigate pre-training using images from CC and text from BookCorpus, and the results are shown in Table <ref type="table">2</ref>. Previous works suggest that experimental results in this setting decline notably compared to pre-training with the in-domain CC captions <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b38">Zhou et al., 2022)</ref>. In our experiments, however, we observe comparable or only slightly degraded performance on three of the four downstream tasks. On the VQA task, the model trained on BookCorpus is even slightly better than the model trained on CC by 0.2%. results demonstrate that our model is robust to the sources of text and image data, which makes it more practical in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>In this section, we conduct an ablation study on the pre-training tasks. To save experimental cost, we pre-train the models with non-parallel images and text from MSCOCO and only report results on the VQA and NLVR2 tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Visualization</head><p>In Figure <ref type="figure">4</ref>, we provide some examples of the generated image-expression pairs as described in Section 2.2. As we can see, the generated referrinng expressions are able to distinguish the target object from other objects in the image by heuristically adding discriminative size and position descriptions. For example, in the second image, there are two umbrellas both with the attribute of the color blue. Since the target object has a larger bounding box, a size description "larger" is added. Similarly, by taking into account the relative positions of the two bounding boxes, a position description of "at the bottom left" is added. The resulting expression will be able to distinguish between the two objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Vision-and-Language Pre-training Current research on visual-and-language pre-training (VLP) can be generally divided into two categories: the two-step training strategy and the end-to-end training strategy. Most works <ref type="bibr" target="#b24">(Lu et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr" target="#b37">Zhang et al., 2021)</ref> fall into the first category where they first use external object detectors such as BUTD <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> to extract region features for the images and then use them together with text embeddings to generate multimodal representations. However, the region features may be sub-optimal for VLP because they are designed for object detection tasks and are fixed during the pre-training process <ref type="bibr" target="#b33">(Xu et al., 2021;</ref><ref type="bibr" target="#b8">Huang et al., 2021)</ref>. Recently, some works integrate the encoding of images into the pretraining process, taking the raw images as input to learn the vision-and-language representations in an end-to-end fashion. These approach can be further categorized into the ones using grid features encoded with CNNs such as E2E-VLP <ref type="bibr" target="#b33">(Xu et al., 2021)</ref> and SOHO <ref type="bibr" target="#b8">(Huang et al., 2021)</ref>, and the ones using patch features encoded with ViTs such as ViLT <ref type="bibr" target="#b11">(Kim et al., 2021</ref><ref type="bibr">), Visual Parsing (Xue et al., 2021)</ref> and ALBEF <ref type="bibr">(Li et al., 2021a)</ref>. In this work, we apply a similar end-to-end approach to unsupervised visual-and-language pre-training with image patch features.</p><p>All of these works on VLP require access to large-scale parallel image-text datasets <ref type="bibr" target="#b18">(Lin et al., 2014;</ref><ref type="bibr" target="#b12">Krishna et al., 2017;</ref><ref type="bibr" target="#b30">Sharma et al., 2018;</ref><ref type="bibr" target="#b25">Ordonez et al., 2011)</ref>, which are difficult to collect due to the large amount of annotations or data cleaning efforts required. To alleviate this problem, some recent works explore unsupervised visionand-language pre-training has emerged, in which only non-parallel image and text data are utilized. Our work also belongs to this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised</head><p>Vision-and-Language Pretraining <ref type="bibr">Li et al. (2021b)</ref> first propose the idea of unsupervised vision-and-language pre-training (UVLP) without using paired image-text data. Their model, U-VisualBERT, is alternately pre-trained on both image-only and text-only data. In addition, they utilize object tags as anchor points for cross-modal alignment to compensate for the absence of aligned data and achieve similar performance to supervised models. <ref type="bibr" target="#b38">Zhou et al. (2022)</ref> suggest that using tags alone is not sufficient and propose pre-training tasks for multi-granular alignment learning with a retrieved weakly aligned image-text corpus for UVLP.</p><p>The most important difference between our work and previous UVLP works is that we use an end-toend training approach to implement UVLP, which is the first to the best of our knowledge. Besides, we identify the limitations of models trained with tags and propose a novel pre-training task, REM, to address these deficiencies. As a result, our approach significantly outperforms previous regionbased UVLP works on all downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Comprehension and Generation</head><p>The task of REM is inspired by the research lines of referring expression generation (REG) and comprehension (REC). REG is generally treat as a special case of image captioning to generate referring expressions from visual features with RNNs <ref type="bibr" target="#b19">(Liu et al., 2017;</ref><ref type="bibr" target="#b36">Zarrie? and Schlangen, 2018)</ref>, while <ref type="bibr" target="#b10">Kazakos et al. (2021)</ref> generate synthetic referring expressions heuristically from object annotations. We apply a similar generation strategy but on the detected object proposals. The task of REC is to localize an object from candidate objects given a referring expression <ref type="bibr" target="#b24">(Mao et al., 2016;</ref><ref type="bibr" target="#b21">Liu et al., 2019)</ref>, while our proposed REM directly predicts the referred object from patch features without object candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel framework that performs end-to-end unsupervised vision-and-language pretraining without using costly and sub-optimal region features. To reduce the training-inference discrepancy, we propose a new pre-training task that predicts the locations of objects with synthetic referring expressions that are more similar to real text. Experiments show that our approach consistently outperforms existing unsupervised visionand-language pre-training methods, and achieves competitive results compared to supervised visionand-language pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although our approach eliminates the dependence on object detection during inference, it still requires object proposals for pre-training, which would damage the efficiency of pre-training. In addition, our method is limited by the finite number of object tags and the lack of diversity of heuristically generated referring expressions. We hope to address this limitation by jointly training a generator with unsupervised vision-language pre-training that automatically generates referring expressions or other type of psudo-parallel text for the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Downstream Tasks</head><p>Most of our settings on downstream tasks follow the setup of ViLT <ref type="bibr" target="#b11">(Kim et al., 2021)</ref> with small adjustments, as detailed below.</p><p>VQA The VQA task involves answering the question according to the given image, which requires an understanding of both vision and language. Following <ref type="bibr" target="#b11">Kim et al. (2021)</ref>, we fine-tune the model on the train and validation sets, and 1, 000 validation image-question pairs are reserved for internal validation. We use the 3, 129 most frequent answers as answer candidates following <ref type="bibr" target="#b35">Yu et al. (2019)</ref>. We set the batch size to 256 and the peak learning rate to 5 ? 10 -5 . The model is fine-tuned for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLVR2</head><p>The task of NLVR2 is to determine whether a natural language description is true given a pair of images. Following <ref type="bibr" target="#b11">Kim et al. (2021)</ref>, we reformulate the input to two image-caption pairs, concatenating the two representations as the input of a classification head. Following <ref type="bibr">Li et al. (2021b)</ref>, we perform task-specific pre-training before fine-tuning using mask-and-predict objective for 10 epochs. The batch size is 256 and the peak learning rate is 1 ? 10 -5 . During fine-tuning, we use a batch size of 128 and set the peak learning rate to 1 ? 10 -5 . The model is fine-tuned for 10 epochs.</p><p>VE The VE task is derived from Flickr30K <ref type="bibr" target="#b26">(Plummer et al., 2015)</ref> images and Stanford Natural Language Inference (SNLI) <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> dataset. Given an image premise P and text hypothesis H, the task aims to determine whether P implies H. This task is a 3-way classification problem to output entailment, neutral, or contradiction based on the relation inferred from the input imagetext pair. The batch size is set as 256 and we set the peak learning rate as 7 ? 10 -5 to train for 5 epochs.</p><p>Image Retrieval Given a caption, the image retrieval task is to find the corresponding image from a collection of images. Following UNITER <ref type="bibr" target="#b3">(Chen et al., 2020)</ref>, we sample 31 negative image-text pairs along with a positive sample to construct a mini-batch for each GPU. The model is fine-tuned for 10 epochs with a peak learning rate of 5 ? 10 -5 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>L tag &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d b I 2 O J / P C 4 Y B K t y R 2 v F P i d f H w 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>e y t h I 6 o o M z a c P A R v 8 e V l 8 n h W 9 9 y 6 d 3 d e a 1 w V c Z T h C I 7 h F D y 4 g A b c Q B N a w C C E Z 3 i F N 2 f s v D j v z s e 8 t e Q U M 4 f w B 8 7 n D 9 f h j O Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D L n N b 2 n e U x r h v C b + 3 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>H 9 b 6 V d r b t 2 d g S w T r y A 1 K N D s V 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 5 V e q j G h b E y H 2 L V U 0 g i 1 n 8 0 u n Z I T q w x I G C t b 0 p C Z + n s i o 5 H W k y i w n R E 1 I 7 3 o 5 e J / X j c 1 4 a W f c Z m k B i W b L w p T Q U x M 8 r f J g C t k R k w s o U x x e y t h I 6 o o M z a c P A R v 8 e V l 8 n h W 9 9 y 6 d 3 d e a 1 w V c Z T h C I 7 h F D y 4 g A b c Q B N a w C C E Z 3 i F N 2 f s v D j v z s e 8 t e Q U M 4 f w B 8 7 n D 9 f h j O Q = &lt; / l a t e x i t &gt; I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 1 R m z Q 0 g Z 3 t c D 5 S V / y C w j L P O Y i 4 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 9 6 q W F t o Q 9 l s J + 3 S z S b s b o Q S + g + 8 e F D E q / / I m / / G T Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 6 9 x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p / r b S r 9 b c u j s D W S Z e Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>h 5 c d 6 d j 3 l r y S l m D u E P n M 8 f 0 1 K M 4 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "8 1 R m z Q 0 g Z 3 t c D 5 S V / y C w j L P O Y i 4 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 9 6 q W F t o Q 9 l s J + 3 S zS b s b o Q S + g + 8 e F D E q / / I m / / G T Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 6 9 x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p / r b S r 9 b c u j s D W S Z e Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>h 5 c d 6 d j 3 l r y S l m D u E P n M 8 f 0 1 K M 4 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "8 1 R m z Q 0 g Z 3 t c D 5 S V / y C w j L P O Y i 4 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 9 6 q W F t o Q 9 l s J + 3 S zS b s b o Q S + g + 8 e F D E q / / I m / / G T Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 6 9 x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p / r b S r 9 b c u j s D W S Z e Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e H A B D b i B J r S A Q Q j P 8 A p v z t h 5 c d 6 d j 3 l r y S l m D u E P n M 8 f 0 1 K M 4 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "8 1 R m z Q 0 g Z 3 t c D 5 S V / y C w j L P O Y i 4 = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 9 6 q W F t o Q 9 l s J + 3 S zS b s b o Q S + g + 8 e F D E q / / I m / / G T Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 6 9 x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p / r b S r 9 b c u j s D W S Z e Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our proposed E2E-UVLP framework. The model learns cross-modal representations from non-parallel text and image data in an end-to-end fashion (Section 2.1).To bridge the gap between the two modalities, we generate pesudo-parallel text L tag and L exp for each image I (Section 2.2). On the right side, we illustrate how to conduct pre-training tasks using different types of data (Section 2.3).</figDesc><graphic url="image-38.png" coords="3,206.66,256.62,82.99,63.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the process of synthesizing referring expressions. We randomly select one target object and heuristically generate discriminative descriptions based on the bounding boxes of all objects of the same class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Q</head><label></label><figDesc>: what food is to the left of the carrots? MTP: lettuce MTP+REM: broccoli</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison between VQA models fine-tuned from pre-trained models with different pre-training tasks. We visualize the most relevant patches for the keyword "food" in the given question. The model pretrained with only MTP fails to identify the corresponding patches, which leads to a wrong answer.</figDesc><graphic url="image-52.png" coords="5,81.77,89.75,88.77,88.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 4: Examples of the synthetic image-expression paired data. We mark the objects referred to with red bounding boxes. The generated expressions are able to distinguish the target object from other objects in the image by heuristically adding size and position descriptions to the detected object tags and attributes.</figDesc><graphic url="image-58.png" coords="7,308.22,199.43,82.03,82.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Masked Language Modeling E2E-UVLP Multimodal Encoder Text Embedding Vision Encoder E2E-UVLP</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Object Tags</cell><cell cols="2">road [mask]</cell><cell>sky</cell></row><row><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell>Locations</cell><cell></cell></row><row><cell>You never know when or if the</cell><cell>road person coat dog sky</cell><cell></cell></row><row><cell>perfect</cell><cell></cell><cell></cell></row><row><cell>person will</cell><cell></cell><cell></cell></row><row><cell>appear. The</cell><cell></cell><cell></cell></row><row><cell>fateful night I</cell><cell></cell><cell></cell></row><row><cell>set eyes on</cell><cell>running man</cell><cell></cell></row><row><cell>?</cell><cell>in the middle</cell><cell></cell></row></table><note><p><p><p><p><p><p>person Task-1: Masked Tag Prediction</p>E2E-UVLP</p>Sentence</p>You [mask] know when or if the perfect [mask] will appear .</p>never person</p>Task-3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>These Ablation study of different pre-training tasks. All models are pre-trained with non-parallel images and text from MSCOCO.</figDesc><table><row><cell>Pre-training Tasks</cell><cell>VQA2 Test-Dev</cell><cell>NLVR2 Test-P</cell></row><row><cell>None MLM MTP REM MTP + REM MLM + MTP MLM + REM MTP + MLM + REM</cell><cell>70.1 69.9 71.7 70.7 72.8 72.6 73.2 73.6</cell><cell>51.2 50.3 67.4 70.4 72.8 74.1 74.5 74.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows the</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61925601</rs>), the <rs type="funder">National Social Science Fund of China</rs> (No.</p></div>
<div><head>20&amp;ZD279), Beijing Academy of Artificial</head><p>Intel-ligence (BAAI), and a grant from the <rs type="funder">Guoqiang Institute, Tsinghua University</rs>. We thank all anony-mous reviewers for their valuable comments and suggestions on this work. We also thank <rs type="person">Xinyu Ma</rs> for his help in the experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KKX3ygn">
					<idno type="grant-number">61925601</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Visual Embed VQA2 NLVR2 VE Flickr30k Test-Dev Test-P Test R@1 R@5 R@10 Supervised (w/ Paired Image-Text Data)</p><p>VisualBERT <ref type="bibr" target="#b14">(Li et al., 2019)</ref> Region 70.9 73.9 -61.2 86.3 91.9 UNITER <ref type="bibr" target="#b3">(Chen et al., 2020)</ref> Region 72.7 77.9 78.3 72.5 92.4 96.1 VinVL <ref type="bibr">(Zhang et al.,</ref>   <ref type="bibr" target="#b6">(Dou et al., 2022)</ref>). Note that in addition to the CC dataset we use for pre-training, these models typically use other parallel data sources such as MSCOCO <ref type="bibr" target="#b18">(Lin et al., 2014)</ref>, VG <ref type="bibr" target="#b12">(Krishna et al., 2017)</ref> and SBU <ref type="bibr" target="#b25">(Ordonez et al., 2011)</ref>.</p><p>For unsupervised vision-language pre-trained models, we compare with U-VisualBERT <ref type="bibr">(Li et al., 2021b)</ref>, U-VisualBERT VinVL which is a version of U-VisualBERT with VinVL object features re-implemented by <ref type="bibr" target="#b38">Zhou et al. (2022)</ref>, and ?-VLA <ref type="bibr" target="#b38">(Zhou et al., 2022)</ref>. All of these models use region-based image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>For the model architecture of E2E-UVLP, we use a 12-layer Swin-Transformer as the image encoder and a 12-layer Transformer acting as the multimodal encoder, which are initialized with pretrained weights of Swin-B/32 and BERT-base, respectively. We utilize the widely-used object detector BUTD <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> to extract object proposals for the images as in other region-based VLP methods. We resize each image to the size of of 384 ? 384 with center-cropping for both pretraining and fine-tuning.</p><p>For the pre-training of E2E-UVLP, we set the total training iterations to 100k with a batch size of 512. We use AdamW with a peak learning rate of 3 ? 10 -5 . The learning rate is warmed-up to the peak value in the first 10% of the iterations, and then linearly decayed to 0. All the pre-training experiments are conducted on 16 NVIDIA V100s with 32GB memory per GPU.</p><p>We evaluate our model on four typical downstream tasks: Visual Question Answering (VQA) <ref type="bibr" target="#b7">(Goyal et al., 2017)</ref>, Natural Language for Visual Reasoning (NLVR2) <ref type="bibr" target="#b31">(Suhr et al., 2018)</ref>, Visual Entailment (VE) <ref type="bibr" target="#b32">(Xie et al., 2019)</ref> and Image Retrieval on Flickr30k (Flickr30k) <ref type="bibr" target="#b26">(Plummer et al., 2015)</ref>. For details of the downstream tasks, please refer to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Table <ref type="table">1</ref> shows the main results of E2E-UVLP on four downstream tasks. For each model, we list the type of image features used for pre-training. From the table we can see that E2E-UVLP con-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of training end-to-end vision-and-language transformers</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18166" to="18176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-to-end pre-training for visionlanguage representation learning</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12976" to="12985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Wasserthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Norajitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Wirkert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10486</idno>
		<title level="m">nnU-Net: Self-adapting framework for U-Net-based medical image segmentation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Syn-thRef: Generation of synthetic referring expressions for object segmentation</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier Gir?-I</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04403</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ViLT: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised vision-and-language pre-training without parallel images and captions</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5339" to="5350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2021c. UNIMO: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2592" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV 2020</title>
		<meeting>ECCV 2020<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4856" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">2021a. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>CoRR, abs/2107.13586</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1950" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Xuming He, and Nan Duan. 2021b. KD-VLP: Improving end-to-end vision-and-language pretraining with object knowledge distillation</title>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Shao-Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasudev</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><surname>Lal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10504</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2021c. Swin Transformer: Hierarchical vision Transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2019. 2016</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Generation and comprehension of unambiguous object descriptions</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Im2Text: Describing images using 1 million captioned photographs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Bryan A Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<title level="m">Girish Sastry, Amanda Askell, Pamela Mishkin</title>
		<meeting><address><addrLine>Jack Clark</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">10808</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<title level="m">A corpus for reasoning about natural language grounded in photographs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Visual entailment: A novel task for fine-grained image understanding</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">E2E-VLP: End-to-end vision-language pre-training enhanced by visual learning</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="503" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Yupan</forename><surname>Hongwei Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4514" to="4528" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decoding strategies for neural referring expression generation</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Zarrie?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised vision-and-language pre-training via retrieval-based multi-granular alignment</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16485" to="16494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
