<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Parameter-Efficient Tuning: Are We Really There Yet?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-16">16 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guanzheng</forename><surname>Chen</surname></persName>
							<email>guanzzh.chen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
							<email>zaiqiao.meng@glasgow.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
							<email>liangshangsong@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Machine Learning</orgName>
								<orgName type="institution">Mohamed bin Zayed University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Parameter-Efficient Tuning: Are We Really There Yet?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-16">16 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.07962v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parameter-efficient tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of PETuning methods. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in mid-and high-resource settings. We delve deeper into the cause of the instability and observed that model size does not explain the phenomenon but training iteration positively correlates with the stability. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained Language Models (PLMs) such as BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> have orchestrated tremendous progress in NLP in the past few years, achieving state-ofthe-art on a large variety of benchmarks such as GLUE <ref type="bibr" target="#b27">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr" target="#b26">(Wang et al., 2019)</ref>. Most successful applications of PLMs follow the pretraining-and-finetuning transfer learning paradigm <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, where PLMs are used as backbone to be combined with additional guanzhchen/PETuning. parameters and finetuned on downstream tasks in an end-to-end manner. Whilst being simple and effective, such paradigm requires task-specific tuning of the full model which consists of hundreds of millions <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., 2019)</ref>, or even billions <ref type="bibr" target="#b23">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020;</ref><ref type="bibr" target="#b24">Raffel et al., 2020)</ref> of parameters for each task, which is time-consuming and resource-intensive.</p><p>To avoid full model finetuning, there has been a surge of studies on Parameter-Efficient Tuning (PETuning) methods, such as Adapter <ref type="bibr" target="#b8">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b21">Pfeiffer et al., 2020)</ref> and Prompttuning <ref type="bibr" target="#b11">(Lester et al., 2021)</ref>, which aim to tune the PLMs by adjusting lightweight trainable parameters while keeping most pretrained parameters frozen. There are various ways to introduce the trainable parameters into PLMs. For example, Adapters inject a small portion of model-level parameters within each transformer layer of the pretrained model, and only tune these newly initialized adapter parameters to transfer knowledge to new tasks efficiently. Prompt-tuning <ref type="bibr" target="#b22">(Qin and Eisner, 2021;</ref><ref type="bibr">Liu et al., 2021c;</ref><ref type="bibr" target="#b11">Lester et al., 2021)</ref> introduces trainable continuous embeddings into the original sequences of input word embeddings while keeping the parameters of backbone pretrained model frozen, augmenting the PLMs on the feature level. Diff-pruning <ref type="bibr" target="#b4">(Guo et al., 2021)</ref> learns and updates additional sparse diff-vector for all pretrained parameters, and LoRA <ref type="bibr" target="#b9">(Hu et al., 2021)</ref> learns low-rank matrices to approximate the updated matrices, both of which update the PLMs on the full-parameter level. Moreover, BitFit <ref type="bibr" target="#b28">(Zaken et al., 2021)</ref> only tunes the bias terms of PLMs, without even introducing any new parameters.</p><p>Given that various exciting progresses of PETuning methods have been reported in the literature, demonstrating competitive performance with higher training efficiency, the idea that PETuning could be a new general paradigm in place of full finetuning for transfer learning in NLP becomes never more tempting <ref type="bibr">(Liu et al., 2021a)</ref>. We, however, argue that current evidences are insufficient to support the complete overthrown of full finetuning, and we need to proceed with more caution when compare PETuning methods with finetuning. First, some of these models are tuned using different evaluation strategies <ref type="bibr" target="#b20">(Pfeiffer et al., 2021;</ref><ref type="bibr" target="#b11">Lester et al., 2021;</ref><ref type="bibr" target="#b4">Guo et al., 2021)</ref>, making their reported results incomparable. Indeed, due to limitation of the unpublished test set in GLUE and SuperGLUE benchmarks, many existing works <ref type="bibr" target="#b11">(Lester et al., 2021;</ref><ref type="bibr" target="#b25">Vu et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b20">Pfeiffer et al., 2021)</ref> only report the performance on the development set but misuse the early stopping strategy on the same set, which is essentially a data leakage that could result in overfitting. Indeed, our pilot experiments in ?4 illustrate this inappropriate evaluation protocol only yields spuriously high performance but weakens the generalisation ability of the model. Second, our experiments in ?5.3 found that the finetuning and PETuning processes are inherently unstable and show great fluctuation dependent on different random seeds. Despite most PETuning works report their experimental results across more than 5 runs, there are few works showing that their improvement is significant.</p><p>The aforementioned experimental flaws will continue to hamper progresses of trained models among the broader NLP research community, moti-vating a more careful consideration for the comparison and evaluation on PETuning methods. Therefore, in this paper, we conduct a comprehensive and fair evaluation on the effectiveness of PETuning methods. In particular, 1) we point out the fundamental flaw of current evaluation schemes used by most PETuning studies, and how that leads to misinterpretations of the progress. 2) We offer a more reliable practice for model selection that is not prone to overfitting. 3) We conduct the first comprehensive study to investigate the stability of off-the-shelf PETuning methods, revisiting their performance not only based on the experimental metrics but also their stability. 4) We perform extensive experiments and analyses to find the factors affecting the stability of PETuning methods.</p><p>Key Findings: 1) Finetuning cannot be fully replaced so far, since there is no PETuning methods can outperform finetuning universally as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Despite the inherent efficiency of parameters and outstanding performance on low-resource tasks, compared with finetuning, PETuning methods struggle somewhat on mediumresource tasks and fall behind across the board on high-resource tasks. 2) All PETuning methods unanimously show the instability across different epochs and random seeds similar to finetuning <ref type="bibr" target="#b2">(Dodge et al., 2020)</ref>. Because of such instability and large variance, prompt-tuning cannot robustly and consistently re-produce the reported competitive or even better performance (which is usually the optimal run across multiple episodes <ref type="bibr" target="#b11">(Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021b)</ref>). 3) The size of trainable parameters cannot directly lead the stability of PETuning methods. On the one hand, tuning with fewer parameters can cause performance outliers through multiple runs; on the other hand, tuning with more parameters leads to a wider distribution of the performance scores. Both the two scenarios can lead to high variance in the results. 4) The stability of PETuning methods is proportional to the scale of training data, which essentially relies on the number of iterations.</p><p>In the rest of the paper, we first introduce the five popular PETuning methods that we focus on ( ?2), and then point out why the current evaluation protocol can be flawed ( ?4). Finally we design a new protocol for fair comparisons and conduct comprehensive experiments to benchmark the stability and performance of PETuning methods ( ?5). &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 1 k s L Z Prompt-tuning Wup &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o 1 p L x i X O z c c q 0 e F I P 2 K J w P J 2 p g A = " &gt; A</p><formula xml:id="formula_0">P f h x Q l p K x 9 f o K a + L U t D W 0 = " &gt; A A A C D H i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S J U k D J T K r o s u H F Z w T 6 g M 5 Z M m m l D M 8 m Q Z I Q y z A e 4 8 V f c u F D E r R / g z r 8 x 0 4 6 g r Q c C J + f c y 7 3 3 + B G j S t v 2 l 1 V Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H H S V i i U k b C y Z k z 0 e K M M p J W 1 P N S C + S B I U + I 1 1 / c p X 5 3 X s i F R X 8 V k 8 j 4 o V o x G l A M d J G G p Q r b o j 0 2 A 8 S P x 0 k V R c P h T 5 N 7 x K X M H b 2 8 z V V d s 2 e A S 4 T J y c V k K M 1 K H + 6 Q 4 H j k H C N G V K q 7 9 i R 9 h I k N c W M p C U 3 V i R C e I J G p G 8 o R y F R X j I 7 J o U n R h n C Q E j z u I Y z 9 X d H g k K l p q F v K r P V 1 a K X i f 9 5 / V g H l 1 5 C e R R r w v F 8 U B A z q A X M k o F D K g n W b G o I w p K a X S E e I 4 m w N v m V T A j O 4 s n L p F O v O Y 3 a + U 2 j 0 q z n c R T B E T g G V e C A C 9 A E 1 6 A F 2 g C D B / A E X s C</formula><formula xml:id="formula_1">A A C A X i c b V D L S s N A F J 3 4 r P U V d S O 4 G S y C C y m J V H R Z c O O y g n 1 A E 8 J k O m m H T h 7 M 3 I g l x I 2 / 4 s a F I m 7 9 C 3 f + j Z M 2 C 2 0 9 c O F w z r 3 c e 4 + f C K 7 A s r 6 N p e W V 1 b X 1 y k Z 1 c 2 t 7 Z 9 f c 2 + + o O J W U t W k s Y t n z i W K C R 6 w N H A T r J Z K R 0 B e s 6 4 + v C 7 9 7 z 6 T i c X Q H k 4 S 5 I R l G P O C U g J Y 8 8 9 A J C Y z 8 I O v m X u Y A e w C c p Q n O c 8 + s W X V r C r x I 7 J L U U I m W Z 3 4 5 g 5 i m I Y u A C q J U 3 7 Y S c D M i g V P B 8 q q T K p Y Q O i Z D 1 t c 0 I i F T b j b 9 I M c n W h n g I J a 6 I s B T 9 f d E R k K l J q G v O 4 t 7 1 b x X i P 9 5 / R S C K z f j U Z I C i + h s U Z A K D D E u 4 s A D L h k F M d G E U M n 1 r Z i O i C Q U d G h V H Y I 9 / / I i 6 Z z X 7 U b 9 4 r Z R a 5 6 V c V T Q E T p G p 8 h G l 6 i J b l A L t R F F j + g Z v a I 3 4 8 l 4 M d 6 N j 1 n r k l H O H K A / M D 5 / A B q 2 l z 8 = &lt; / l a t e x i t &gt;</formula><p>Wdown &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l x 6 6 k 7 L s M C c k 1 Y e 6 J z 8 t E e 7 7 b P 0 Figure <ref type="figure">2</ref> shows the difference of these PETuning methods.</p><formula xml:id="formula_2">= " &gt; A A A C A 3 i c b V B N S 8 N A E N 3 U r 1 q / o t 7 0 s l g E D 1 I S q e i x 4 M V j B f s B b S m b 7 a Z d u t m E 3 Y l a Q s C L f 8 W L B 0 W 8 + i e 8 + W / c t D l o 6 4 O B x 3 s z z M z z I s E 1 O M 6 3 V V h a X l l d K 6 6 X N j a 3 t n f s 3 b 2 m D m N F W Y O G I l R t j 2 g m u G Q N 4 C B Y O 1 K M B J 5 g L W 9 8 l f m t O 6 Y 0 D + U t T C L W C 8 h Q c p 9 T A k b q 2 w f d g M D I 8 5 N W 2 k + 6 w B 4 A J 4 P w X u I 0 7 d t l p + J M g R e J m 5 M y y l H v 2 1 / d Q U j j g E m g g m j d c Z 0 I e g l R w K l g a a k b a x Y R O i Z D 1 j F U k o D p X j L 9 I c X H R h l g P 1 S m J O C p + n s i I Y H W k 8 A z n d n F e t 7 L x P + 8 T g z + Z S / h M o q B S T p b 5 M c C Q 4 i z Q P C A K 0 Z B T A w h V H F z K 6 Y j o g g F E 1 v J h O D O v 7 x I m m c V t 1 o 5 v 6 m W a 6 d 5 H E V 0 i I 7 Q C X L R B a q h a 1 R H D U T R I 3 p G r + j N e r J e r H f r Y 9 Z a s P K Z f f Q H 1 u c P u O C Y J g = = &lt; / l a t e x i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model-Level</head><p>Adapter-Tuning. Adapters <ref type="bibr" target="#b8">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b21">Pfeiffer et al., 2020</ref><ref type="bibr" target="#b20">Pfeiffer et al., , 2021) )</ref> are a type of PETuning approaches that insert small newly initialised parameter modules on the model-level (i.e. each transformer layer) of PLMs. In particular, these adapter modules are normally moulded by a two-layer feed-foward neural network with a bottleneck: (1) a down-projection with W down ? R d?r to project the input h to a lower-dimensional space specified by bottleneck dimension r;</p><p>(2) an upprojection with W up ? R r?d to project back to the input size. Mathematically, the adapter can be defined as:</p><formula xml:id="formula_3">h a = W up f W down h ,<label>(1)</label></formula><p>where h a is the output and f (?) is the activation function. During the finetuning, the model only update the parameters of the adapter modules while keeping the underlying pretrained model fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-Level</head><p>Prompt-Tuning. Prompt-Tuning <ref type="bibr" target="#b11">(Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021b)</ref> is another type of PETuning approaches that introduce additional tunable parameters on the feature-level. Specifically, prompttuning introduces additional tunable predix vectors, namely prompts, to extend the input text features (or the input of each transformer layer <ref type="bibr">(Liu et al., 2021b)</ref>), and tunes only the prompts during finetuning. Due to its simplicity and lightness, prompttuning could achieve an on par performance, particularly in billions-size PLMs, an even better performance, comparing with the full finetuning <ref type="bibr">(Liu et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter-Level</head><p>Diff-Pruning. Diff-pruning <ref type="bibr" target="#b4">(Guo et al., 2021)</ref> works on all parameters of PLMs, which aims to learn additional trainable sparse parameters for the entire models. Speciafically, for the pretrained parameters ?, diff-pruning reparameterize the taskspecific model parameters ? ? as</p><formula xml:id="formula_4">? ? = ? + ? ? ,<label>(2)</label></formula><p>where ? ? denotes the trainable diff vector, which is regularised to be sparse.</p><p>LoRA. LoRA <ref type="bibr" target="#b9">(Hu et al., 2021)</ref> focuses on the updating procedure of the language model parameters.</p><p>For a pretrained weight matrix W ? R d?k , LoRA uses trainable low-rank matrices to approximate the updates by</p><formula xml:id="formula_5">W + ?W = W + BA,<label>(3)</label></formula><p>where B ? R d?r , A ? R r?k , and the rank r min(d, k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Partial Finetuning</head><p>BitFit. Partial finetuning aims to tune a fraction of PLMs parameters without introducing any additional ones. In particular, <ref type="bibr" target="#b10">Lee et al. (2019)</ref> only tunes the top layers or even the prediction head, however, which usually performs much worse than full finetuning. With the principle of efficiency and effectiveness, BitFit <ref type="bibr" target="#b28">(Zaken et al., 2021)</ref> turns to tunes the bias terms of PLMs to obtain competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General Experimental Setup</head><p>In this section, we illustrate the general task, compared methods, and hyperparameter settings for the following experiments. Apart from that, in ?4 and ?5, we will additionally illustrate their specific data and evaluation setups, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Setup</head><p>In order to extensively compare the performance and stability of PETuning methods with the fullmodel finetuning, we select a full set of 12 tasks across low, medium and high-resource scales of GLUE and SuperGLUE, including natural language inference (CB, RTE, MNLI, QNLI), question answering (COPA, BoolQ), paraphrasing (MRPC, QQP), sentiment analysis (SST-2), sentence similarity (STS-B), word sense disambiguation (WiC), and coreference resolution (WSC) tasks. According to the dataset sizes, we divide these tasks into three levels:</p><p>? Low-Resource: the tasks with training data size smaller than 1k, including CB, COPA, and WSC.</p><p>? Medium-Resource: the tasks with training data size between 1k and 10k, including RTE, MRPC, WiC, STS-B, and BoolQ.</p><p>? High-Resource: the tasks with training data size larger than 10k, including SST-2, QNLI, QQP, and MNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compared Methods</head><p>We have chosen four representative PETuning methods: Adapter, Prompt-tuning, LoRA, and BitFit, across all levels. For Adapter, we use the Pfeiffer architecture <ref type="bibr" target="#b21">(Pfeiffer et al., 2020)</ref> since it has reported better performance than others. For Prompt-tuning, due to the poor performance of standard prompt-tuning <ref type="bibr" target="#b11">(Lester et al., 2021)</ref> on small PLMs, e.g., base versions of Bert and Roberta, we adopt the settings of prefix tuning (Li and Liang, 2021) to add continuous prompts for each transformer layer of PLMs. For LoRA &amp; BitFit, we take the architectures from their origin papers <ref type="bibr" target="#b9">(Hu et al., 2021;</ref><ref type="bibr" target="#b28">Zaken et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyperparameter Setup</head><p>We adopt Roberta base as the base model released by Huggingface 2 . The grid search is used to select the learning rate from {1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} and batch size from {16, 32}. We search the reduction factor from {2, 16, 2 https://github.com/huggingface/ transformers 64} following <ref type="bibr" target="#b20">(Pfeiffer et al., 2021)</ref> for Adapter, the prompt length from {8, 16, 32, 64} for prefix tuning, and the scaling factor ? and rank from {8, 16} for LoRA following its origin paper. There are many studies focusing on achieving better initialization by post pretraining for PETuning methods such as Adapter <ref type="bibr" target="#b20">(Pfeiffer et al., 2021)</ref> and prompt <ref type="bibr" target="#b25">(Vu et al., 2021;</ref><ref type="bibr" target="#b3">Gu et al., 2021)</ref>, however, to be a fair comparison, the extra parameters of all PETuning methods are initialized randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Unreliability of Misused Early Stopping</head><p>Almost all PETuning methods are evaluated on GLUE and SuperGLUE, which, in nowadays, have become the de facto benchmarks for verifying model effectiveness in Natural Language Understanding (NLU). For the sake of validity and fairness of the evaluation, the test set labels in these benchmarks are not released. Instead, web portals<ref type="foot" target="#foot_0">3</ref> are provided for submitting and evaluating the prediction results. Due to the limited number of allowed evaluation submissions to these benchmarks, a large number of works have followed a common practice that only reports and compares model performance on the development sets rather than the real test sets, where the development set is treated as the "test set" <ref type="bibr" target="#b11">(Lester et al., 2021;</ref><ref type="bibr" target="#b25">Vu et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b20">Pfeiffer et al., 2021)</ref>. While this practice is a convenient approximation of model performance as it allows quicklyobtained results from large-scaled experiments, there has been a serious problem in how these studies exercise this practice: the same development set is often used for both validating and testing the model. Therefore, the reported results on the development set would come from overly-optimistic/overfitted checkpoints since early stopping is applied on this same development data. We argue that this breach of the standard train/development/test paradigm, when accompanied by inherent instability of PETuning models, compromises fair and rigorous comparison, leading to unreliable conclusions and misunderstandings of the examined models.</p><p>To verify our intuition, in this section, we scrutinise this common practice by redesigning the evaluation protocol to strictly separate the data used for validating and testing. We also conduct comprehensive experiments to understand the effects of training and evaluating models with the misused early stopping.</p><p>Data Setup. We conduct experiments on the RTE dataset, which is a textual entailment dataset included in both GLUE and SuperGLUE. While previous studies perform both validation and evaluation on the development set, we will separate the validation step (early stopping) on a different set. One way to do so is to separate part of the training data as the new development set for validation and treat the original development set as the new test set <ref type="bibr" target="#b29">(Zhang et al., 2021;</ref><ref type="bibr" target="#b17">mahabadi et al., 2021)</ref>. This strategy will be directly comparable to results reported from past studies, and it will be performed in ?5.1. One potential concern of this strategy is that the newly created development set (separated from train) may be of a different distribution from the new test set (original development set), where the comparison between using new development and new test set for early stopping may be unfair. In this section, we will adopt a more controlled setting where we create the new development and test sets from the same development set to highlight the data leakage issue from the misused early stopping.</p><p>Specifically, we divide the original development set by a 50%/50% split (denoted by dev.1 and dev.2 respectively). We report results on the second split, dev.2 (treated as the new test set), and use either dev.1 or dev.2 (treated as the new development set) for model selection for both fine-tuning (FT) and PETuning methods, i.e., Adapter, prefix tuning (PT), Lora, and BitFit, following the settings illustrated in ?3.2. Depending on whether dev.1 or dev.2 are used as the new development set, we use RTE 1-2 to denote the evaluation scheme where the model is developed on dev.1 and tested on dev.2; and use RTE 2-2 to denote when the model is developed and tested both on dev.2). Evaluation Setup. We set the number of epochs to 50 and adopt the early stopping strategy with the patience of 10 non-increasing epochs following <ref type="bibr" target="#b18">(Mao et al., 2021)</ref>, where the stopping metrics are set as the accuracy and evaluation loss, respectively. We conduct grid search for each experiment following the settings in ?3.3. same test set. Note that the development sets for early stopping are of the same scale and similar distribution since they are both divided from the original development set randomly. We can see from Table <ref type="table">1</ref> that a large gap between the experimental results on the RTE 1-2 and RTE 2-2 schemes can be observed and RTE 2-2 always obtains a significant better performance for all PETuning methods, especially with accuracy as stopping metric. Note that using a same set as both the validation and test set is clearly not a rigorous setting and easy to overfit. In order to further analyse these two sets, we choose the best hyperparameters for dev.1 and dev.2 and present their performance over training steps in Figure <ref type="figure">3</ref>. Although we keep the two datasets of the same size and similar distribution as far as possible, there is still a significant gap between the curves of their performance resulting in distinct selected epochs, which may be more pronounced between real development and test sets. Actually, it is by combating such gap that a model with strong generalisation can achieve outstanding and stable performance. However, the evaluation scheme, i.e., RTE 2-2 , is tantamount to cheating to erase this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and</head><p>Therefore, some reported results by conducting early stopping and reporting results on the same dataset are unreliable, as it amounts to a direct leak of the evaluation target on the test set during the training procedure, where the outstanding performance just originates from overfitting the test set, but without corresponding outstanding ability of generalisation.</p><p>This observation motivates us to re-examine these PETuning methods with a fairer evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments with Fair Evaluation</head><p>In this section, we use the new evaluation protocol to strictly separate validation and evaluation steps (will be discussed in ?5.1) and conduct a largescale investigation on the performance of PETuning methods on 12 downstream tasks from GLUE and SuperGLUE. We evaluate the models on their task performance ( ?5.2), and also investigate their stability across multiple runs ( ?5.3). Full-model finetuning results are also listed for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Data Setup. There are two common approaches to determine the number of iterations (epochs) for training. The first approach is to choose it from several fixed values, where the range can only be determined empirically and the different ranges significantly affect the performance of the model. A better solution is to adopt a larger number of epochs with the early stopping strategy, which not only allows the model to determine the number of iterations automatically to obtain the optimal performance, but also avoids overfitting.</p><p>Aforementioned experiments in Sec. 4 illustrate the impropriety of early stopping on the same data set where the results are reported. To obtain separate development and test sets while maintaining sufficient training data, we conduct a 90%/10% split from the original training data to create our new train set and development set for each task, while we use the original development set as our test set. <ref type="foot" target="#foot_1">4</ref>Evaluation Setup. We conduct the experiments on 12 tasks illustrated in ?3.1. All experimental results are reported across 20 runs for low and medium-resource tasks, and 10 runs for highresource tasks with different random seeds, respectively, to fully investigate the stability of finetuning and PETuning methods. Based on the hyperparameter settings in ?3.3, we additionally set the number of epochs to 50 and adopt the early stopping strategy with the patience of 10 non-increasing epochs, where the stopping metrics are set as the evaluation loss, since the more stable performance (lower standard deviation) as shown in Table <ref type="table">1</ref>.</p><p>Previous works generally compare the experimental results of their models with baselines by mean and standard deviation (std.). We argue that such comparison may not be straightforward for models that perform close to each other. To address this weakness, we introduce significance test into the comparison between each PETuning method and finetuning for each task. We define that one PETuning method outperforms finetuning on a specific task only when the mean value of the PETuning results outperforms that of finetuning, and there is a statistical significance of the difference (p-value &lt; 0.05) between the two sets of results from their separately-conducted multiple runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Performance</head><p>In this section, we aim to investigate the performance of PETuning methods compared with finetuning, unanimously on our proposed fair evaluation. We conduct this comparison not only on the overall average for all 12 tasks, but mostly focus on a fine-grained view across low, medium, and high-resource tasks, where the divergence between finetuning and PETuning varies considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PETuning Methods Outperform Finetuning?</head><p>On average for all tasks in Table <ref type="table" target="#tab_1">2</ref>, some increases are indeed detected from most PETuning methods, i.e., Adapter, LoRA, and BitFit, which match their reported competitive (even better) performance. Besides the performance, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, PETuning methods are undeniably ahead in terms of efficiency, with far fewer parameters (less than 2%). However, it may be too quick to jump onto the conclusion that PETuning methods are more advantageous, given that the overall comparison, which was used in many past studies <ref type="bibr" target="#b8">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b9">Hu et al., 2021;</ref><ref type="bibr" target="#b11">Lester et al., 2021)</ref> performs worse, which would be discussed separately.</p><p>As concluded in Table <ref type="table" target="#tab_2">3</ref>, Adapter &amp; LoRA &amp; BitFit obtain very impressive performance on the low-resource tasks that significantly outperform finetuning by large margins, which is the main reason behind these methods' overall better performance. However, the situation changes when there is more training data. Specifically, for the medium-resource tasks, only Adapter and BitFit maintain the competitive performance compared with finetuning, while LoRA lags behind finetuning. For the high-resource setting, finetuning performs consistently better than all PETuning methods. To investigate the reasons behind such performance differences under different training resources, we plot the evaluation loss over training steps for COPA, WiC, and SST-2, as the representatives of low, medium, and high-resource tasks, respectively in Figure <ref type="figure">4</ref>. We observe that finetuning always converges faster than PETuning, especially on low-resource task COPA, where the training steps are less than 100. One possible explanation the aforementioned differences is that finetuning may be more prone to overfit on low-resource settings with fewer training iterations, resulting in poorer performance.</p><formula xml:id="formula_6">C B C O P A W S C R T E M R P C W i C S T S -B B o o l Q S S T -2 Q N L I Q Q P M N L I -30<label>-</label></formula><p>Therefore, we can conclude that finetuning cannot be fully replaced so far, since there are no PETuning methods that can outperform finetuning universally. The performance gains of PETuning methods are mainly on low-resource tasks, while as the data size increases, finetuning regains dominance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prefix Tuning Performs Poorly across Multiple</head><p>Runs. As illustrated in Table <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">Table 3</ref>, we argue that, unlike what has been reported in <ref type="bibr">Liu et al. (2021b)</ref>, finetuning outperforms prefix tuning by large margins on most tasks across multiple runs. This is because previous works <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b11">Lester et al., 2021;</ref><ref type="bibr" target="#b25">Vu et al., 2021)</ref> neglected the stability factor and based their conclusions on one optimal performance rather than comparing multiple runs. In Figure <ref type="figure" target="#fig_4">5</ref>, we observe that prefix tuning achieves competitive upper bounds over multiple runs compared with finetuning on many tasks, however its competitiveness is not consistent across different runs. Such instability of prefix tuning leads to it's poor performance in our experiments, which will be discussed in more details in ?5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Stability</head><p>Besides performance, stability is also a crucial factor to consider when comparing finetuning and PETuning. However, to our best knowledge, no previous studies discussed the stability of PETuning methods. In this section, we provide the first investigation on this matter<ref type="foot" target="#foot_2">5</ref> .</p><p>PETuning Methods are Unstable. While <ref type="bibr" target="#b2">Dodge et al. (2020)</ref> proposed that finetuning has apparent instability across multiple runs when varying only the random seeds, we find all PETuning methods have large standard deviation over many tasks in a similar way, as shown by the results in Table <ref type="table" target="#tab_1">2</ref>. To visually illustrate the instability of finetuning and PETuning methods, in Fig <ref type="figure" target="#fig_5">6</ref>, we show the distribution of the multiple results on CB, COPA, WSC, and RTE, where all methods exhibit pronouncedly large standard deviation. In fact, finetuning and all PETuning methods have a similarly wide range of distribution across the multiple runs, suggesting PETuning methods are also unstable on these tasks.</p><p>Inspired by many previous studies investigating the instability in finetuning <ref type="bibr" target="#b29">(Zhang et al., 2021;</ref><ref type="bibr" target="#b10">Lee et al., 2020;</ref><ref type="bibr" target="#b19">Mosbach et al., 2021)</ref>, we conduct analysis to find out the underlying factors that lead to the instability of PETuning methods. First, we analyse which part of the randomness causes the instability by varying different random seeds. Secondly, we mainly focus on two factors that could potentially reduce instability, the number of trainable parameters, and the training data size plus training iterations.</p><p>Random Seeds: Weight Initialization or Data Order? A direct cause for instability in our experiments is randomness that is reflected in both weight initialisation (including the final classification head and extra parameters of PETuning methods) and the order of the training data. To disentangle these two factors, we use two separate random seeds to control weight initialisation and training order respectively, comparing with using one global random seeds to control these two factors together.</p><p>The results in Figure <ref type="figure" target="#fig_10">10</ref> show that all PETuning methods, including prefix tuning, are significantly affected by both training data order and weight initialisation.. Most surprisingly, varying the train data order alone can independently cause much instability. Such instability calls for questions over the findings from existing studies <ref type="bibr" target="#b20">(Pfeiffer et al., 2021;</ref><ref type="bibr" target="#b11">Lester et al., 2021;</ref><ref type="bibr" target="#b25">Vu et al., 2021)</ref> which only focus on obtaining better prior or prompt initialization without considering the robustness issue in these methods. By contrast, our experiment results can address this concern by performing significance test over multiple runs from different random seeds.</p><p>The Number of Trainable Parameters. The boxplots in Figure <ref type="figure" target="#fig_5">6</ref> show two types of instability: for finetuning and Adapter, the large instability originates from the fact that the experimental results fluctuate evenly over a wide range across multiple runs on these tasks. However, for the other PETuning methods, i.e., prefix tuning, LoRA, and BitFit, the culprit of large instability may be the outliers (some of which are even less than the lower end of the box by &gt;30 large margins). It is obvious that the outliers become the constraint on the stability of most PETuning methods, but excluding Adapter. One potential factor to explain the difference between these two types of instability is the number of trainable parameters as Adapter involves a larger number of trainable parameters about 3? more than other PETuning methods <ref type="bibr" target="#b18">(Mao et al., 2021)</ref>. To test this hypothesis, we conduct experiments to investigate how the stability of PETuning methods varies with the number of parameters.</p><p>Specifically, we compare model stability on the COPA task where PETuning methods exhibit large instability. Specifically, we compare different parameter scales of PETuning methods over the reduction factor from {64, 16, 2} for Adapter, the prompt length from {32, 64, 128} for prefix tuning, and the rank from {8, 16, 32} for LoRA.</p><p>Confirming our hypothesis, in Figure <ref type="figure">7</ref>, we observe that PETuning methods are more likely to have outliers with fewer parameters, but less so as the parameter scale increases. However, it does not mean that increasing the size of parameters can relatively increase the stability of PETuning methods, since more parameters maybe spread the results of the PETuning methods over a wider range, which also results in larger standard deviation. Consequently, the instability of PETuning methods as measured by standard deviation is always pronounced through various parameter scales, and the instability is manifested in different ways: fewer trainable parameters may result in more outliers    while more trainable parameters may produce a wider range of distribution over multiple runs.</p><p>Training Data Size and Iterations. From Table 2 and Figure <ref type="figure" target="#fig_5">6</ref>, we can easily observe that the stability (std.) of finetuning and PETuning methods is proportional to the data scale of the tasks, which is also shown in   <ref type="bibr">et al. (2021)</ref> propose a unified view to connect various PETuning methods. However, there has not been reliable validation and comparison for these off-the-shelf PETuning methods over stability and effectiveness, and this is where our paper bridges the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have conducted a rigorous re-examination on the current parameter-efficient tuning (PETuning) methods. We have demonstrated that performing early stopping and evaluation on the same dataset (the common practice of most past studies) can give misleading conclusions. This issue is more pronounced when accompanied by the instability nature of PETuning, leading to the inflated results and an overclaimed advantage of PETuning. We re-evaluate these PETuning methods by a new evaluation protocal that strictly separates validation and test sets, and found that the advantage from PETuning methods is not consistent with prefix tuning performing poorly across tasks and most PETuning methods only perform better than finetuning in low-resource settings. We also systematically investigated the effect from model size, training data size and training steps, and random seeds on the instability of PETUning methods. We found that while changing model size influences how instability is manifested, it cannot reduce instability.</p><p>In the mean time, more training iterations can usually reduce instability. Our finds open up many key challenges for PETuning over performance and instability, which we intend to tackle in future work, to allow PETuning to be really in place of finetuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The relative performance difference of PETuning methods, i.e., Adapter, prefix tuning (PT), LoRA, BitFit, comparing with the full finetuning (FT) over training data size of 12 tasks from GLUE and Super-GLUE. The tasks and split of different resources are illustrated in ?3.1. The size of each point denotes the standard deviation and the colors of PETuning methods denote the percentage of trainable parameters over different tasks compared to the full finetuning (100%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " E t E X 6 6 W / 3 S z V F 6 k a 4 p Y 5b R W o C W o = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 B I v g q S S i 6 M F D w Y v H C v Y D m l A m m 0 2 7 d L O J u 5 N C C f 0 d X j w o 4 t U f 4 8 1 / 4 7 b N Q V s f D D z e m 2 F m X p A K r t F x v q 3 S 2 v r G 5 l Z 5 u 7 K z u 7 d / U D 0 8 a u s k U 5 S 1 a C I S 1 Q 1 A M 8 E l a y F H w b q p Y h AH g n W C 0 d 3 M 7 4 y Z 0 j y R j z h J m R / D Q P K I U 0 A j + V 7 I B E I / 9 x C y a b 9 a c + r O H P Y q c Q t S I w W a / e q X F y Y 0 i 5 l E K k D r n u u k 6 O e g k F P B p h U v 0 y w F O o I B 6 x k q I W b a z + d H T + 0 z o 4 R 2 l C h T E u 2 5 + n s i h 1 j r S R y Y z h h w q J e 9 m f i f 1 8 s w u v F z L t M M m a S L R V E m b E z s W Q J 2 y B W j K C a G A F X c 3 G r T I S i g a H K q m B D c 5 Z d X S f u i 7 l 7 W r x 4 u a 4 3 b I o 4 y O S G n 5 J y 4 5 J o 0 y D 1 p k h a h 5 I k 8 k 1 f y Z o 2 t F + v d + l i 0 l q x i 5 p j 8 g f X 5 A z Y + k m E = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r 9 W g 9 W 2 / W + 7 y 0 Y O U 9 h + A P r I 9 v n a C b S Q = = &lt; / l a t e x i t &gt; B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u z + T h m + 0 B t c Q 3 1 Z 2 c b 8 H 4 X 0 W c y M = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 9 E L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k sY h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 7 Z f L L l l d w G y T r y M l C B D v V / 8 6 g 1 i l k Y o D R N U 6 6 7 n J s a f U m U 4 E z g r 9 F K N C W V j O s S u p Z J G q P 3 p 4 t A Z u b D K g I S x s i U N W a i / J 6 Y 0 0 n o S B b Y z o m a k V 7 2 5 + J / X T U 1 4 4 0 + 5 T F K D k i 0 X h a k g J i b z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u C D c F b f X m d t C p l r 1 q + a l R L t U o W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B Ce 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J I p j L 4 = &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h X / S W Q e l 3 H X d o N A F L + u k x W C u A J o = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Y J R o 8 Y L x 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j u 7 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 7 Z f L L l l d w G y T r y M l C B D v V / 8 6 g 1 i l k Y o D R N U 6 6 7 n J s a f U m U 4 E z g r 9 F K N C W V j O s S u p Z J G q P 3 p 4 t A Z u b D K g I S x s i U N W a i / J 6 Y 0 0 n o S B b Y z o m a k V 7 2 5 + J / X T U 1 4 4 0 + 5 T F K D k i 0 X h a k g J i b z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u C D c F b f X m d t C p l r 1 q + a l R L t U o W R x 7 O 4 B w u w Y N r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J C l j L 0 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2: Different PETuning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relative upper bound difference between finetuning and prefix tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The experimental results over 20 different random seeds across CB, COPA, WSC, and RTE datasets, where finetuning and PETuning methods show large instability. The dashed rhombuses denote the mean (horizontal dashed line) and standard deviation (vertical distance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Performance of Adapter, prefix tuning, and LoRA over different parameter scales on COPA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Standard deviations of data scale in {1k (solid line), 2k (dashed line)} over training steps on WiC task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>To investigate the effects of training data size, inspired by(Mosbach    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance over 20 runs on RTE, controlled by global random seeds, weight initialization random seeds, and data order random seeds, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Analyses. To highlight the data leakage issue from the misused early stopping, in Table1, we report the experimental results for finetuning and PETuning on RTE 1-2 and RTE 2-2 evaluation schemes, respectively, which share the</figDesc><table><row><cell></cell><cell cols="2">Evaluation loss</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell>RTE 1-2</cell><cell>RTE 2-2</cell><cell>RTE 1-2</cell><cell>RTE 2-2</cell></row><row><cell>FT</cell><cell cols="2">78.89 ?1.36 78.89 ?1.36</cell><cell>79.28 ?1.9</cell><cell>79.62 ?2.22</cell></row><row><cell>Adapter</cell><cell>75.1 ?1.60</cell><cell>76.3 ?4.26</cell><cell>76.55 ?3.57</cell><cell>78.42 ?3.7</cell></row><row><cell>PT</cell><cell cols="4">57.55 ?2.71 66.19 ?8.51 57.84 ?4.85 67.19 ?11.37</cell></row><row><cell>LoRA</cell><cell cols="2">75.22 ?2.77 75.94 ?3.39</cell><cell>75.11 ?3.3</cell><cell>77.7 ?4.57</cell></row><row><cell>BitFit</cell><cell cols="4">70.79 ?10.38 71.3 ?10.19 66.76 ?12.98 68.2 ?13.72</cell></row><row><cell cols="5">Table 1: Mean and standard deviation results with dif-</cell></row><row><cell cols="5">ferent dev./test splits for RTE task across 20 runs. Ac-</cell></row><row><cell cols="5">curacy and Evaluation loss are the stopping metrics for</cell></row><row><cell cols="5">early stopping. Bold denotes the highest mean value for</cell></row><row><cell cols="4">corresponding PETuning method.</cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell>dev.1: FT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adapter</cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>PT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LoRA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BitFit</cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>dev.2:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FT</cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>Adapter</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LoRA</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>BitFit</cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell></row></table><note><p><p>Accuracy</p>Figure 3: Best performance on dev.1 and dev.2 over training epochs. The markers denote the epochs selected by early stopping.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>, may neglect important divergences in the wide range of tasks with different scales of training data. Mean and standard deviation results for each of the 12 tasks across 20 runs. Avg. and All/Avg. denote the mean values for corresponding resource and all 12 tasks, respectively. The datasets are ordered by their scale of training data. We report the f1 score for CB and MRPC, Pearson correlation for STS-B, and accuracy for other tasks (matched accuracy for MNLI). Higher is better for all metrics. ? denotes the corresponding PETuning method outperforms finetuning and ? denotes the opposite. These tasks are organized from top to bottom according to the scales of their corresponding training data sets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FT</cell><cell></cell><cell>Adapter</cell><cell>PT</cell><cell>LoRA</cell><cell>BitFit</cell></row><row><cell></cell><cell></cell><cell cols="5">Low-Resource</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">CB (250) COPA (400) WSC (554) Avg.</cell><cell></cell><cell cols="5">70.00 ?13.32 77.49 ?13.20 46.55 ? ?5.74 54.70 ?3.36 65.90 ? ?5.42 55.35 ?5.07 66.4 ? 82.05 ? ?9.62 ?9.05 63.46 ?0.0 63.46 ?0.0 58.7 ? 63.46 ?0.0 ?4.69 62.72 ?4.58 68.95 ? ?4.83 53.53 ? ?3.22 70.64 ? ?4.32</cell><cell>81.12 ? ?8.94 56.65 ?3.72 63.46 ?0.0 67.08 ? ?3.57</cell></row><row><cell></cell><cell></cell><cell cols="5">Medium-Resource</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">RTE (2500) MRPC (3700) WiC (6000) STS-B (7000)</cell><cell></cell><cell cols="2">73.77 ?3.17 90.54 ?1.05 65.47 ?2.04 90.42 ?0.26</cell><cell cols="4">73.88 ?1.88 57.36 ? ?8.01 91.06 ?0.63 89.35 ? ?1.31 65.12 ?1.88 62.12 ? ?1.32 90.23 ? ?0.1 89.64 ?0.39 90.47 ?0.11 90.44 ?0.15 69.69 ? 70.67 ?10.77 ?7.89 91.03 ?0.95 91.06 ?0.71 61.29 ? 66.0 ?1.41 ?6.7</cell></row><row><cell></cell><cell></cell><cell cols="4">BoolQ (9427)</cell><cell></cell><cell cols="2">78.75 ?0.72</cell><cell cols="3">76.93 ?0.92 75.44 ?0.47 76.92 ?1.33</cell><cell>76.9 ?0.84</cell></row><row><cell></cell><cell></cell><cell cols="2">Avg.</cell><cell></cell><cell></cell><cell></cell><cell cols="2">79.79 ?0.99</cell><cell cols="2">79.44 ?0.74 74.78 ? ?1.62</cell><cell>77.88 ? ?2.02</cell><cell>79.01 ?2.22</cell></row><row><cell></cell><cell></cell><cell cols="5">High-Resource</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">SST-2 (67k)</cell><cell></cell><cell cols="2">94.15 ?0.0</cell><cell>93.34 ? ?0.31</cell><cell>94.15 ?0.0</cell><cell>94.15 ?0.0</cell><cell>93.92 ?0.07</cell></row><row><cell></cell><cell></cell><cell cols="4">QNLI (105k)</cell><cell></cell><cell cols="2">92.40 ?0.12</cell><cell cols="4">92.31 ?0.09 92.31 ?0.27 91.00 ?0.69 91.60 ?1.01</cell></row><row><cell></cell><cell></cell><cell cols="4">QQP (364k) MNLI (393k) Avg.</cell><cell></cell><cell cols="2">91.38 ?0.06 87.42 ?0.20 91.34 ?0.09</cell><cell>90.28 ?0.0 86.88 ? ?0.17 90.70 ? ?0.12</cell><cell>88.90 ? ?0.32 86.30 ? ?0.08 90.42 ? ?0.14</cell><cell cols="2">90.45 ? ?0.17 86.96 ?0.24 85.50 ? 89.28 ? ?0.0 ?0.32 90.64 ? ?0.21 90.08 ? ?0.29</cell></row><row><cell></cell><cell></cell><cell cols="3">All/Avg.</cell><cell></cell><cell></cell><cell cols="2">79.37</cell><cell>80.57</cell><cell>74.68</cell><cell>80.32</cell><cell>79.72</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">COPA</cell><cell></cell><cell></cell><cell>FT</cell><cell></cell><cell cols="2">Low Medium High</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adapter</cell><cell></cell><cell></cell></row><row><cell>Eval. Loss</cell><cell>0 0.6 0.8 1 1.2 1.4</cell><cell>100</cell><cell>5k</cell><cell>200</cell><cell>WiC SST-2</cell><cell>300 10k</cell><cell>400</cell><cell>500 15k</cell><cell>PT LoRA BitFit</cell><cell cols="2">Adapter PT LoRA BitFit</cell><cell>-? -?</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10k</cell><cell></cell><cell>20k</cell><cell cols="2">30k</cell><cell>40k</cell><cell>50k</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Steps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 4: Evaluation loss over training steps on COPA,</cell><cell></cell><cell></cell></row><row><cell cols="3">WiC, and SST-2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">PETuning Only Performs Better on Low-</cell><cell></cell><cell></cell></row><row><cell cols="10">Resource Tasks. To provide a fine-grained view</cell><cell></cell><cell></cell></row><row><cell cols="10">for the comparison of finetuning and PETuning, we</cell><cell></cell><cell></cell></row><row><cell cols="10">revisit the results in Table 2 by grouping the 12</cell><cell></cell><cell></cell></row><row><cell cols="10">tasks into low, medium, and high-resource tasks</cell><cell></cell><cell></cell></row><row><cell cols="10">as illustrated in  ?3.1. On this view, we observe</cell><cell></cell><cell></cell></row><row><cell cols="10">that all PETuning methods show noticeable diver-</cell><cell></cell><cell></cell></row><row><cell cols="10">gence compared with finetuning across different</cell><cell></cell><cell></cell></row><row><cell cols="10">scales, except for prefix tuning which consistently</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance changes of PETuning methods compared with finetuning on low, medium, and highresource settings, respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://gluebenchmark.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Ideally, the standard train-dev-test splits of GLUE and SuperGLUE should be used. However, due to the amount of experiments and evaluations need to be done in our ultralarge-scale investigation, we create our own splits instead of submitting models to the learderboards.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We measure the stability by the std. across multi runs.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<imprint>
			<pubPlace>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</pubPlace>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Minnesota. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ppt: Pre-trained prompt tuning for few-shot learning</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4884" to="4896" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust transfer learning with pretrained language models through adapters</title>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="854" to="861" />
		</imprint>
	</monogr>
	<note>Short Papers). Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2021b. On the effectiveness of adapter-based tuning for pretrained language model adaptation</title>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2208" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Jaejun</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2020. April 26-30, 2020. 2019</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>What would elsa do? freezing layers during transformer fine-tuning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">2021a. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2021b. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Gpt understands, too</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unipelt: A unified framework for parameter-efficient language model tuning</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the stability of finetuning BERT: misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview. net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying LMs with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spot: Better frozen model adaptation through soft prompt transfer</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superglue: a stickier benchmark for generalpurpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameterefficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting few-sample bert fine-tuning</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
