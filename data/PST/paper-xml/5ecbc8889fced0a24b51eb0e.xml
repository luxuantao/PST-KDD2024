<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Shot Video Object Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-07">7 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ting</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Tao</forename><surname>Mei</surname></persName>
						</author>
						<title level="a" type="main">Single Shot Video Object Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-07">7 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.03560v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Object Detection</term>
					<term>Single Shot Detection</term>
					<term>Feature Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single shot detectors that are potentially faster and simpler than two-stage detectors tend to be more applicable to object detection in videos. Nevertheless, the extension of such object detectors from image to video is not trivial especially when appearance deterioration exists in videos, e.g., motion blur or occlusion. A valid question is how to explore temporal coherence across frames for boosting detection. In this paper, we propose to address the problem by enhancing per-frame features through aggregation of neighboring frames. Specifically, we present Single Shot Video Object Detector (SSVD) -a new architecture that novelly integrates feature aggregation into a one-stage detector for object detection in videos. Technically, SSVD takes Feature Pyramid Network (FPN) as backbone network to produce multiscale features. Unlike the existing feature aggregation methods, SSVD, on one hand, estimates the motion and aggregates the nearby features along the motion path, and on the other, hallucinates features by directly sampling features from the adjacent frames in a two-stream structure. Extensive experiments are conducted on ImageNet VID dataset, and competitive results are reported when comparing to state-of-the-art approaches. More remarkably, for 448 × 448 input, SSVD achieves 79.2% mAP on ImageNet VID, by processing one frame in 85 ms on an Nvidia Titan X Pascal GPU. The code is available at https://github.com/ddjiajun/SSVD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T He development of deep learning technologies has led to the significant surge of research activities in computer vision area. In between, object detection is one of the most fundamental tasks and the recent advances in deep convolutional neural networks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have successfully achieved remarkable improvements on object detection in images <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Nevertheless, directly applying these object detectors for still images to object detection in videos is very challenging due to the fact that video is an information-intensive media with large variations and complexities, not to mention that some frames in videos may be deteriorated by motion blur or occlusion. Such facts motivate and highlight the explorations of object detectors in videos to improve detection accuracy. Furthermore, the natural existence of spatio-temporal coherence in videos also offers a fertile ground for designing video-level object detectors. J. Deng, W. Zhou and H. Li are with University of Science and Technology of China, Hefei, China (e-mail: dengjj@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn).</p><p>Y. Pan, T. Yao and T. Mei are with JD AI Research, Beijing, China (e-mail: panyw.ustc@gmail.com; tingyao.ustc@gmail.com; tmei@jd.com). Video object detection results with motion stream and sampling stream. In the upper example (a), motion stream fails to detect domestic cat accurately due to the motion blur, while sampling stream that performs feature aggregation without motion estimation can ameliorate this case. In the bottom example (b) when the airplane moves fast, an unwanted bounding box of airplane is generated for sampling stream. In contrast, motion stream can solve this problem by capturing long-range motion with explicit motion estimation. Object detection in videos therefore should take both motion and sampling streams into consideration.</p><p>There are two general directions along the exploitation of spatio-temporal coherence for object detection in videos. One common solution for video object detection is box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and another branch is featurelevel aggregation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The former often applies a tracker to per-frame bounding box proposals over multiple frames to generate dense tubelets, making this category of approaches computationally expensive. Moreover, such methods are not trained end-to-end since the processes of per-frame proposal generation and bounding box tracking are independent. Feature aggregation improves detection by enhancing per-frame features through spatio-temporal aggregation of nearby frames. In this case, feature extraction and aggregation plus detection are trained in an end-to-end manner. We follow this elegant recipe and employ feature aggregation in our work. One natural way to execute feature aggregation for a reference frame is to estimate the motion across frames and warp the feature maps from neighboring frames to the reference one based on the motion. However, the results may suffer from robustness problem when the object appearances are deteriorated by motion blur or occlusion which often exists in videos as shown in Figure <ref type="figure" target="#fig_5">1(a)</ref>. As such, we additionally introduce a stream of directly hallucinating/generating features through self-guided sampling from adjacent frames, which performs better than motion stream in this case. Nevertheless, the stream of self-guided sampling may fail to localize object accurately when object moves extremely fast (e.g., fast moving airplane in Figure <ref type="figure" target="#fig_5">1(b)</ref>). This is due to the fact that the receptive field in self-guided sampling for offset prediction is smaller than that in optical flow generation. Therefore, the range of estimated motion in sampling stream is shorter than that in motion stream, resulting in failure of motion capturing in sampling stream when object moves extremely fast. As a result, we simultaneously exploit both motion calibration and self-guided sampling in a two-stream feature aggregation structure for video object detection. More importantly, we novelly integrate feature aggregation into single shot object detection framework, which is more fit for computationally intensive video scenarios.</p><p>By consolidating the idea of two-stream feature aggregation into one-stage detection, we present a new Single Shot Video Object Detector (SSVD). Specifically, SSVD consists of three core modules: Feature Pyramid Network (FPN), two-stream feature aggregation structure and class/box subnets. FPN is taken as backbone network to output feature maps of multiple scales in a spatial pyramid. Each feature map in the pyramid is then fed into two-stream feature aggregation structure. One is motion stream, which estimates the displacements of objects across frames according to optical flow and warps the feature maps of nearby frames to the reference one along the motion paths. The other is sampling stream, which directly hallucinates the feature map of the reference frame by spatiotemporal sampling features from adjacent frames through deformable convolutions. The aggregated feature map in each stream is input into class/box subnets to classify anchor boxes and regress from anchor boxes to ground-truth object boxes. The final results are a blend of outputs on all the feature maps in the two streams. The whole SSVD is end-to-end trained by minimizing the focal loss for box classification plus the standard smooth L 1 loss for box regression.</p><p>The main contribution of this work is the proposal of a one-stage detector SSVD for addressing the issue of video object detection. Our SSVD, on one hand, takes advantages of single shot detectors (i.e., potentially faster and simpler than two-stage detectors), and on the other, leverages temporal coherence across frames to boost detection. The solution also leads to the elegant view of how temporal coherence along the motion paths and feature sampling across frames should be amended for feature aggregation, which are problems not yet fully understood in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We briefly divide the most existing algorithms for object detection into two categories: object detection in images and object detection in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Detection in Images</head><p>Inspired by the recent advances in image representation using deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, remarkable progresses have been witnessed for object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In particular, one common deep solution for object detection is based on a two-stage paradigm, i.e., first perform region proposal and then do classification. R-CNN <ref type="bibr" target="#b25">[26]</ref> is one of first attempts that tackles object detection problem in a two-stage solution, which firstly utilizes selective search to generate region proposals and then classifies each proposal. Later on, SPP-Net <ref type="bibr" target="#b32">[33]</ref> and Fast R-CNN <ref type="bibr" target="#b24">[25]</ref> extend <ref type="bibr" target="#b25">[26]</ref> by devising SPP pooling or ROI pooling to enable the sharing of features across region proposals, which significantly speed up the process of detection. Faster R-CNN <ref type="bibr" target="#b12">[13]</ref> further advances Fast R-CNN by leveraging Region Proposal Networks (RPN) instead of selective search at the first stage. Compared to the costly per-region classification subnet in Faster R-CNN, R-FCN <ref type="bibr" target="#b7">[8]</ref> capitalizes on the fully convolutional network with position-sensitive ROI pooling. In addition, inspired by domain adaptation <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> for recognition, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> focus on learning robust and domaininvariant detectors based on two-stage approaches.</p><p>Another direction mainly constructs one-stage detectors by omitting region proposal stage. One of the early successes is OverFeat <ref type="bibr" target="#b37">[38]</ref> which constructs multi-scale sliding windows to jointly classify and localize objects in deep architecture. In <ref type="bibr" target="#b27">[28]</ref>, YOLO divides feature maps into rigid grids, and objects are assigned to be detected by each grid. Technically, the prediction of objectness, confidence score of multiple classes and relative bounding boxes coordinates are devised as a regression problem in YOLO. As another genre of one-stage object detector, SSD <ref type="bibr" target="#b11">[12]</ref> further utilizes multiple feature maps at different scales with predefined default boxes to boost detection for objects in variety of scales and aspect ratios. Although methods with one-stage paradigm are potential to be faster and simpler, they trail in accuracy compared to two-stage ones. To match the performance of two-stage models, <ref type="bibr" target="#b10">[11]</ref> presents an effective dense one-stage detector, RetinaNet, and equips it with Focal Loss to alleviate the foreground-background class imbalance and successfully achieve comparable performance of state-of-the-art two-stage detectors. More recently, anchorfree one stage detectors proposed in <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> are superior in generalization ability since getting rid of limitation to the design of anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection in Videos</head><p>Generalizing the existing detectors from still image to video domain is very challenge as video is an informationintensive media with both the spatial and temporal complex variations, not to mention that frames in videos are usually deteriorated by motion blur or occlusion. The research on object detection in videos has proceeded along two different directions: box-level tracking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and feature-level aggregation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Box-level tracking employs boxlevel operations and post-processing over per-frame bounding box proposals to generate dense tubelets for identifying objects across frames. For instance, Seq-NMS <ref type="bibr" target="#b15">[16]</ref> builds a temporal graph across clips and then seeks the optimal path in this graph via dynamic programming for the selection of tubelets. Later, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> integrate per-frame proposals into tubelets for re-scoring, which further improves the robustness of video object detection. <ref type="bibr" target="#b14">[15]</ref> extends R-FCN with a tracking module for simultaneous detection and tracking. DorT <ref type="bibr" target="#b42">[43]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Stream</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling</head><p>Stream FPN FPN Fig. <ref type="figure">2</ref>. An architecture overview of our Single Shot Video Object Detector (SSVD) (better viewed in color). It consists of three modules: (1) Feature Pyramid Network (FPN), (2) two-stream feature aggregation structure, and (3) Class/Box Subnets. Given the input sequence of adjacent frames {I t+τ } K τ =−K , K preceding frames and K subsequent frames are taken as support frames for object detection in the reference frame I t . Feature Pyramid Network is first leveraged to extract multi-scale pyramidal feature maps of each frame for motion and sampling stream separately. Next, feature aggregation (FA) with motion-aware calibration is employed along motion path by warping the feature maps of support frames in the same scale into the reference frame, whilst in the sampling stream we utilize feature aggregation (FA) with self-guided sampling to directly hallucinate the feature map of reference frame through spatio-temporal sampling features from support frames. After that, the aggregated feature map on each scale is injected into class/box subnets which include two parallel branches, one for anchor boxes classification and another for bounding box regression (k: the number of classes, A: the number of anchors per spatial location). The whole system integrates the three modules in one feed-forward CNN, making our detector single shot trainable. At inference, we blend all the predicted bounding boxes from two streams to produce the final detection results. on a scheduler network to switch between detection and tracking networks. Furthermore, HQ-link <ref type="bibr" target="#b43">[44]</ref> devises the cuboid proposal networks and Tubelet NMS to enable the global post-processing for robust video object detection.</p><p>Unlike box-level tracking that associates bounding boxes across frames with independent processes of linking/tracking, feature-level aggregation naturally enhances per-frame features via spatio-temporal aggregation, enabling an end-toend detection paradigm. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> calibrate a sequence of per-frame feature maps with the guidance from optical flow and aggregate them along motion paths to enhance object detection. Next, <ref type="bibr" target="#b20">[21]</ref> extends <ref type="bibr" target="#b23">[24]</ref> by exploiting additional motion path with box-level calibration for feature aggregation. Instead of estimating the motion across frames for warping the feature map, STSN <ref type="bibr" target="#b18">[19]</ref> performs object detection in a frame by learning to spatially sample features from adjacent frames for aggregation. STMN <ref type="bibr" target="#b21">[22]</ref> adopts spatiotemporal memory module with spatial alignment mechanism to model long-term temporal appearance and motion dynamics. Besides, RDN <ref type="bibr" target="#b45">[46]</ref> and SELSA <ref type="bibr" target="#b46">[47]</ref> strengthen region-level features by exploiting the relation/affinity between region proposals across frames</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Summary</head><p>Our approach belongs to feature-level aggregation methods. The novelty of our SSVD is on the exploitation of two-stream (motion and sampling stream) feature aggregation in one-stage detection paradigm, which is seldom explored. The motion stream performs feature aggregation by estimating motion from optical flow and warping the feature maps of nearby frames to the reference one along motion paths, whilst the sampling stream is trained to directly hallucinates the feature map of the reference frame through spatio-temporal sampling from adjacent frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SINGLE SHOT VIDEO OBJECT DETECTOR</head><p>We devise our Single Shot Video Object Detector (SSVD) to facilitate object detection in videos by integrating two-stream feature aggregation via motion estimation and feature sampling into one-stage detection framework. An overview of SSVD is illustrated in Figure <ref type="figure">2</ref>. In particular, SSVD first utilizes Feature Pyramid Network to extract multi-scale pyramidal feature maps for motion and sampling stream separately. The pyramid feature maps of frames in the same scale are further aggregated by warping them into the reference one with motion-aware guidance along motion pathway or directly hallucinating the feature map of reference one with selfguided spatio-temporal sampling through sampling pathway, Next, we directly average the calibrated feature maps of all support frames as aggregated feature map in each scale to boost object detection for reference frame. Note that here we only depict how to aggregate the support feature maps with motion-aware calibration, and omit the feature aggregation from reference frame for simplicity.</p><p>respectively. For the aggregated feature maps in each stream, the class/box subnets are leveraged to simultaneously classify objects and perform bounding box regression. As such, three core modules in our detector, i.e., Feature Pyramid Network, two-stream feature aggregation structure and class/box subnets are elegantly integrated into one feed-forward CNN, enabling single shot detection. In the inference stage, the final results are a blend of outputs on all the feature maps from two streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Suppose we have a sequence of adjacent frames {I t+τ } K τ =−K , where the central frame I t is treated as the reference frame, and K preceding frames plus K subsequent frames are taken into account as the support frames. In the standard task of video object detection, the ultimate goal is to localize and recognize objects from the reference frame. Taking the inspiration from the temporal coherence exploration in video understanding <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, we aim to detect the objects in the reference frame I t by additionally leveraging the spatio-temporal coherence distilled from the support frames. Technically, we formulate our video object detection architecture in one-stage paradigm with two-stream feature aggregation structure. The aggregated features are endowed with the characteristics of both temporal coherence distilled from the motion across adjacent frames in motion stream and contextual content encoded across the sampling features from adjacent frames in sampling stream.</p><p>Formally, given the reference frame I t and each support frame I t+τ , Feature Pyramid Network (FPN) <ref type="bibr" target="#b55">[56]</ref> is leveraged to extract multi-scale pyramidal feature maps of each frame for motion and sampling stream, separately. More precisely, by feeding each frame into the FPN (i.e., N mo FPN , N sp FPN ) in motion and sampling stream, we can obtain two sets of multiscale pyramidal feature maps, i.e., {f t P i } 6 i=3 = N mo FPN (I t ) and <ref type="foot" target="#foot_0">1</ref> , for motion and sampling stream, respectively. The network structure of N mo FPN and N sp FPN in the two streams is identical but the parameters of the two FPN are not shared during training. These pyramidal feature maps decrease in scale progressively and allow predictions of detection at multiple scales. Next, motion/sampling stream takes the feature maps of support and reference frames at the same scale (i.e., {f t+τ P i , f t P i }/{g t+τ P i , g t P i }) as the inputs, and performs feature aggregation with motion-aware calibration or self-guided sampling. On the basis of the aggregated feature map of each scale in each stream, class/box subnets are utilized to simultaneously classify anchor boxes and regress from anchor boxes to ground-truth object boxes. The final results at inference are produced by combing the outputs from all scales in the two streams with late fusion scheme. The main acronyms and notations in our paper are given in Table <ref type="table" target="#tab_2">I</ref>.</p><formula xml:id="formula_0">{g t P i } 6 i=3 = N sp FPN (I t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Stream</head><p>One natural way to enhance per-frame feature is to estimate the motion across frames in the form of optical flow and warp the features from neighboring frames to the target one based on the motion. Such way of motion compensation has convincingly demonstrated high capability of modeling temporal correlation in video super-resolution <ref type="bibr" target="#b56">[57]</ref>, video object detection <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and video translation <ref type="bibr" target="#b57">[58]</ref>. As a result, we follow this elegant recipe and design the module of feature aggregation with motion-aware calibration which performs feature aggregation over the input feature maps with same scale along motion stream, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. The idea behind this feature aggregation module is to calibrate the feature map of support frame to the reference frame with the guidance from optical flow across them in the context of onestage video object detection.</p><p>Motion Estimation. Technically, with the input reference and support frames, we first estimate the motion between them in the form of optical flow. Unlike the works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref> which capitalize on FlowNet-s <ref type="bibr" target="#b58">[59]</ref> to produce optical flow, PWC-Net <ref type="bibr" target="#b59">[60]</ref> is particularly remould in our motion stream. Compared to a generic U-Net CNN built in FlowNets, the pyramid processing in PWC-Net is more flexible to the calibration of multi-scale feature maps, tailored to one-stage detection paradigm. Furthermore, the architecture of PWC-Net is smaller in size and easier to be trained when integrating into object detector.</p><p>In particular, by injecting the reference frame I t and the support frame I t+τ into PWC-Net (i.e., N pwc ), a set of multiscale flow fields describing the motion between them are obtained: {m</p><formula xml:id="formula_1">(t,t+τ ) P i } 6 i=3 = N pwc (I t , I t+τ ). {m (t,t+τ ) P i } 6</formula><p>i=3 denote the outputs from the last four groups of layers in PWC-Net by removing the refinement module and correspond to the support frame's feature map {f t+τ P i } 6 i=3 in each scale, respectively. In practice, to align the scale of each flow field m (t,t+τ ) P i with the corresponding feature map f t+τ P i , we downsample the spatial resolution of the input frames in PWC-Net by a factor 2.</p><p>Motion-aware Calibration. For each scale, given the feature map of support frame f t+τ P i and the corresponding flow field m (t,t+τ ) P i , we conduct the motion-aware calibration by warping the feature map of support frame to the reference frame with the guidance of the flow:</p><formula xml:id="formula_2">f t+τ →t P i = W(f t+τ P i , m (t,t+τ ) P i ), i ∈ {3, 4, 5, 6},<label>(1)</label></formula><p>where f t+τ →t P i</p><p>is the calibrated feature map of support frame I t+τ and W(•) denotes the function of bilinear warping.</p><p>Feature Aggregation. After obtaining the calibrated feature maps of all support frames, we directly average them as the aggregated feature map in each scale of the reference frame in motion stream:</p><formula xml:id="formula_3">f mo P i = K τ =−K f t+τ →t P i 2K + 1 , i ∈ {3, 4, 5, 6}.<label>(2)</label></formula><p>Through feature aggregation with motion-aware calibration, the aggregated feature f mo P i of reference frame is enhanced with the temporal coherence distilled in motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sampling Stream</head><p>In the motion stream, we facilitate the feature aggregation by exploring the explicit motion compensation fully conditioned on optical flow across adjacent frames. Nevertheless, such design in motion path is heavily restricted by the quality of estimated motion and may suffer from robustness problem when the object appearances are deteriorated by motion blur or occlusion. To alleviate this issue, we devise feature aggregation module with self-guided sampling in sampling stream that directly hallucinates features via spatio-temporal sampling from the support frames, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. The philosophy is originated from the idea of deformable convolution <ref type="bibr" target="#b60">[61]</ref> which performs non-rigid spatial sampling with self-learnt offsets. More precisely, deformable convolution upgrades standard convolution by adding 2D offsets to the regular grid sampling locations, where the 2D offsets are inferred by the input feature itself, without additional supervision. In our case, we extend the augmentation of spatial sampling locations in standard deformation convolution <ref type="bibr" target="#b60">[61]</ref> which is only conditioned on one feature map to the measure of deformation across two feature maps of the reference frame and the support frame. In other words, our feature aggregation module in sampling stream learns to predict the implicit correlation between support and reference frames in the form of 2D offsets depending on the input frames, without referring to the estimated optical flow. As such, the hallucinated feature of reference frame for aggregation is acquired by spatio-temporal sampling over support frames through deformable convolutions with the guidance of the self-learnt 2D offset.</p><p>Offset Predictor. To learn the deformation between reference and support frames, we uniquely present a new architecture of our offset predictor in feature aggregation module with self-guided sampling to predict the offsets. Here the predicted offsets implicitly depict the temporal correlation between the reference and support frames, which could further guide the following spatio-temporal sampling from support frames. In the architecture of our offset predictor, we adopt the "U-Net" Note that here we only take the feature map with the lowest resolution as an example to depict the detailed processing of self-guided sampling. Specifically, for each spatial resolution, we concatenate the feature map from each support frame with feature map of reference frame, which will be fed into offset predictor for predict the offsets between the reference and support frames. Next, spatiotemporal sampling is further performed to directly hallucinate the feature of reference frame via deformable convolution over the feature map of support frame and the predicted offsets.</p><p>structure <ref type="bibr" target="#b61">[62]</ref> design to process input feature maps across multiple scales and enrich the receptive field for offset prediction. Compared to <ref type="bibr" target="#b18">[19]</ref> that adopts four stacked deformable convolution module for offset prediction, the design of our offset predictor contains ∼40 times less parameters, which is more efficient and tailored to one-stage video object detector.</p><p>Concretely, given the feature maps in each scale of reference frame g t P i and support frame g t+τ P i in sampling stream, we firstly concatenate the two feature maps along channel dimension, which is taken as the input of offset predictor (N off ). Next, the concatenated feature map is fed into three groups of convolutional layers, where the first group retains the scale of the concatenated feature map and the last two groups progressively downsample the scale by a factor 2. Finally, we concatenate the output feature maps of three convolution groups in offset predictor with upsampling operation, and leverage them to predict the offsets ∆o (t,t+τ ) P i = N off (g t+τ P i , g t P i ) for g t+τ P i at each scale.</p><p>Spatio-temporal Sampling. Given the feature map of support frame g t+τ P i and its predicted offset ∆o (t,t+τ ) P i</p><p>, we hallucinate the feature of reference frame g t+τ →t P i via deformable convolution over g t+τ P i . Here, we directly exploit the predicted offset from our offset predictor as the 2D offset in deformable convolution. Such way enables the spatio-temporal sampling from support frames with respect to the reference frame and the generated feature is therefore tailored to the spatio-temporal context.</p><p>Feature Aggregation. After obtaining the hallucinated feature maps of reference frame from all support frames in each scale, we average them as the aggregated feature map of reference frame in sampling stream:</p><formula xml:id="formula_4">ĝsp P i = K τ =−K g t+τ →t P i 2K + 1 , i ∈ {3, 4, 5, 6}.<label>(3)</label></formula><p>Through feature aggregation with self-guided sampling, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Location FPN Features Aggregated Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal Sampling and Feature Aggregation</head><p>Center activation unit of the object Sampling location in support frames Intermedia Data. In Figure <ref type="figure" target="#fig_4">5</ref>, we provide some intermedia data to illustrate the effect of feature aggregation with selfguided sampling. The blue square in reference frame depicts a pixel, for which we want to compute a convolution output. The red points are the corresponding sampling locations in support frames, which are predicted by our offset predictor. As illustrated in this figure, our offset predictor learns to sample the locations from the supporting frames that belong to the corresponding object. The aggregated feature of reference frame is thus hallucinated by performing deformable convolution over the FPN feature map of support frame and the predicted offsets. From the aggregated feature map of reference frame, we can clearly observe that the features depicting the same object are strengthened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and Inference</head><p>During training, the aggregated feature in each scale (i.e., { f mo P i } 6 i=3 and {ĝ sp P i } 6 i=3 ) in both motion and sampling stream is injected into class/box subnets, which simultaneously classify anchor boxes with Focal Loss <ref type="bibr" target="#b10">[11]</ref> (L FL ) and regress from anchor boxes to ground-truth object boxes with Smooth L 1 Loss <ref type="bibr" target="#b24">[25]</ref> (L Loc ). Accordingly, the overall objective of our SSVD is computed as</p><formula xml:id="formula_5">L = 1 Nfg { j c LFL(p mo j,c , l * j ) + j c LFL(p sp j,c , l * j ) +[l * j ≥ 1] j LLoc(x mo i , t * j ) + [l * j ≥ 1] j LLoc(x sp j , t * j )},<label>(4)</label></formula><p>where j is the index of anchor in a mini-batch, l * j is the ground truth class label and [l * j ≥ 1] indicates that the j-th anchor is assigned to one of the foreground classes. t * j denotes the location size of ground-truth object boxes. N fg is the number of anchors assigned to ground-truth object boxes. p mo j,c and p sp j,c denotes the predicted confidence score of class c of the j-th anchor in motion and sampling stream, respectively. x mo j and x sp j is the predicted coordinates of the j-th anchor in motion and sampling stream. Note that not all the frames within aggregation range are taken in the training phase, we just randomly select two of them. The detailed training process is presented in Algorithm 1. end for 8: FA with Motion-aware Calibration: end for 13: Calculate objectives as in Equation <ref type="formula" target="#formula_5">4</ref>. 25: Optimizing with stochastic gradient descent.</p><formula xml:id="formula_6">f mo P i = f t+τ 1 →t P i +f t+τ 2 →t P i 2 , i ∈ {3,</formula><p>At the inference stage, we adopt the late fusion scheme to combine the detection results from motion and sampling streams. Specifically, we first accumulate all the predicted bounding boxes from motion and sampling streams and then apply Non-Maximum Suppression (NMS) with a threshold of jaccard overlap 0.45 per class to produce the final results. We detail the inference algorithm in Algorithm 2. Moreover, implementation details of training and inference are given in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset Sampling and Evaluation</head><p>We evaluate our SSVD on the large-scale benchmark for video object detection task, i.e., ImageNet <ref type="bibr" target="#b62">[63]</ref> object detection from video (VID) dataset, which contains 3,862 training and 555 validation videos in 30 classes. As the annotations of testing videos are not publicly available, we follow the widely adopted protocols in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b23">[24]</ref> to report the results on validation set in terms of the evaluation metric of mean Average Precision (mAP). The 30 object classes in ImageNet VID dataset are a subset of 200 classes of ImageNet object detection (DET) dataset. Therefore, we follow <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref> and train SSVD on the intersection of ImageNet VID and ImageNet DET. Due to the redundancy among frames in ImageNet VID and the large variations in the number of samples per class in ImageNet DET, we sample 15 frames from each video in ImageNet VID dataset and at most 2,000 images per class in ImageNet DET dataset for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Architecture Design</head><p>Feature Pyramid Network. FPN is built at the top of ResNet-101 pre-trained on ImageNet. As in <ref type="bibr" target="#b55">[56]</ref>, P3, P4, Algorithm 2 Inference Algorithm of our SSVD </p><formula xml:id="formula_7">f t+τ →t P i = W(f t+τ P i , m (t,t+τ ) P i</formula><p>), i ∈ {3, 4, 5, 6} Calibration 13:</p><p>end for 14:  </p><formula xml:id="formula_8">f mo P i = K τ =−K f t+τ →t P i 2K+1 , i ∈ {3, 4,</formula><formula xml:id="formula_9">ĝsp P i = K τ =−K g t+τ →t P i 2K+1 , i ∈ {3, 4,</formula><formula xml:id="formula_10">{f t+K+1 P i } 6 i=3 = N mo FPN (I t+K+1 ) Motion Stream 28: {g t+K+1 P i } 6 i=3 = N sp FPN (I t+K+1 ) Sampling Stream 29:</formula><p>Update feature buffers. 30: end for 31: Offline Post-processing (optional):</p><formula xml:id="formula_11">32: {Dt} = Seq-NMS({Pt})</formula><p>Applying Seq-NMS <ref type="bibr" target="#b15">[16]</ref> and P5 are computed from the outputs of ResNet residual stage (conv3 to conv5) with top-down pathway and lateral connections. P6 is obtained by attaching a 3 × 3 stride-2 convolution over the last residual block in conv5. As such, with the input size of 448 2 in SSVD, the spatial scale of P3, P4, P5, and P6 is 56 2 , 28 2 , 14 2 , and 7 2 , respectively. Two-stream Feature Aggregation. For motion stream, we utilize PWC-Net <ref type="bibr" target="#b59">[60]</ref> pre-trained on Flying Chairs dataset for optical flow estimation. For sampling stream, each conv layer in offset predictor consists of 3 × 3 kernel with 256 filters except for the final one with 72 filters. All the parameters of offset predictor are randomly initialized. The deformable group in deformable conv is set as 4.</p><p>Class/Box Subnets. The class/box subnets include two parallel branches, i.e., class and box branch. The structure in class branch starts from two 3 × 3 conv layers, each with 256 filters, followed by a 3 × 3 conv layer with kA filters plus sigmoid activations. Here k is the number of classes and A is the number of anchors per spatial location. The structure of box branch is identical to that in class branch except that box branch terminates in 4A linear outputs per spatial location. For each anchor, the four outputs denote the predicted relative offsets between the anchor and the groundtruth box. To handle different scales and aspect ratios of objects, translation-invariant anchors are assigned to P3, P4, P5, P6 with anchor areas ranging from 32 2 to 256 2 . As in [56], <ref type="bibr" target="#b10">[11]</ref>, each pyramid layer is associated with anchors at three aspect ratios of {1:2, 1:1, 2:1} and three size factors of {2 0 , 2 1/3 , 2 2/3 }. As such, there are A = 9 anchors per spatial location in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Training. In our experiments, the whole architecture of our SSVD is trained over 4 GPUs by synchronized SGD with momentum of 0.9 and weight decay of 0.0001. The batch size is set as 16. Following <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, the two-phase training strategy is adopted. In the first phase, we construct a still image detector by directly linking the class/box subnets with Feature Pyramid Network, and train this detector over the images/frames from the combined training set of ImageNet DET and VID. The learning rate is set as 0.001 in the first 80, 000 iterations and 0.0001 in the last 40, 000 iterations. In the second phase, the whole SSVD is trained on ImageNet VID, with the learning rate of 0.001 and 0.0001 in the first 60, 000 iterations and the last 30, 000 iterations, respectively. Note that Feature Pyramid Network and class/box subnets are initialized from the weights learnt in the first phase. To make the model more robust and adaptive to variant changes in videos, we perform the widely-adopted data augmentation as in SSD <ref type="bibr" target="#b11">[12]</ref>. The aggregation range K is set as 12 in all experiments. As in FGFA <ref type="bibr" target="#b23">[24]</ref>, we utilize temporal dropout in training stage by randomly selecting two support frames from the input adjacent frames {I t+τ } K τ =−K of reference frame I t . Inference. At inference, we follow <ref type="bibr" target="#b23">[24]</ref> and sequentially process each frame with a sliding feature buffer of the nearby frames. The capacity of feature buffer is maintained as 25 except for the beginning and ending 12 frames. For each feature buffer consisting of 24 support frames and one reference frame, we uniformly select 6 support frames and aggregate them to the reference frame for object detection. Note that no data augmentation is adopted during inference. For offline post-processing, we leverage a widely-adopted global linking and suppression algorithm (i.e. Seq-NMS <ref type="bibr" target="#b15">[16]</ref>) to perform box-level association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We investigate how each stream in our SSVD influences the overall performance, as summarized in Table <ref type="table" target="#tab_7">II</ref>. Note that all the variants of SSVD here are constructed over ResNet-101 for fair comparison. Besides the standard mean Average Precision (mAP) over all classes, we additionally adopt the metric protocols in <ref type="bibr" target="#b23">[24]</ref> by categorizing ground-truth objects with respect to their motion speed. The speed of an object is measured by averaged intersection-over-union (IoU) with its corresponding instances in the nearby frames. As such, the objects are classified into three classes: slow (IoU &gt; 0.9), medium (IoU ∈[0.7 ,0.9]), and fast (IoU &lt; 0.7).</p><p>Method (a) is the single-frame baseline. By directly linking class/box subnets with FPN, (a) operates object detection over each single frame in one-stage paradigm and the mAP performance achieves 74.5%. When detailing the performances for objects with different motion speeds, we can clearly observe that the faster the object moves, the lower the detection performance. The results indicate the challenge of object detection in videos, especially when the frames are with fastmoving objects and may be deteriorated by motion blur.</p><p>Method (b) integrates the feature aggregation module with motion-aware calibration into (a). The mAP increases from 74.5% to 78.2% and the performance improvement is consistently observed on objects in different speed. This validates the effectiveness of enhancing per-frame feature by aggregating features from nearby frames with the guidance of optical flow along motion stream.</p><p>Method (c) equips (a) with the feature aggregation module with self-guided sampling. (c) exhibits better performance than (a), which demonstrates the advantage of directly hallucinating features through sampling features from adjacent frames in sampling stream. Furthermore, (b) yields inferior performance to (c). The result basically indicates that relying on optical flow for feature aggregation in motion stream might more easily suffer from robustness problem (e.g., motion blur or occlusion) than self-learning of offset for feature hallucination in sampling stream.</p><p>Method (d) utilizes both motion and sampling streams with late fusion scheme, which further boosts up the performances. The results verify the merit of simultaneously exploiting feature aggregation with motion-aware calibration and selfguided sampling in one-stage paradigm.</p><p>Effect of Fusion Scheme. In general, there are three directions to fuse motion stream and sampling stream in our model. One is to integrate optical flow guidance into sampling stream for sampling coordinates prediction. Another is to perform early fusion scheme by concatenating aggregated features of two streams in each scale before feeding them into class/box subnets. And the last one is our adopted late fusion scheme to additionally perform NMS over all the predicted bounding boxes from two streams. Figure <ref type="figure" target="#fig_7">6</ref> depicts these three fusion schemes. We compare the performances of our SSVD with these three schemes. The results are 78.7%, 78.1% and 79.2% w.r.t motion-guided sampling, early fusion with concatenation and late fusion by NMS, which indicate that the adopted late fusion scheme outperforms the other two schemes in our case.</p><p>Qualitative Analysis. Figure <ref type="figure" target="#fig_8">7</ref> showcases four video examples of detection results on ImageNet VID validation set by the single-frame baseline (a) and SSVD. Four typical deteriorated object appearances in videos, i.e., rare poses, motion blur, illumination variation and part occlusion, are depicted in Figure <ref type="figure" target="#fig_8">7</ref> and our SSVD consistently shows better detection results than the single-frame baseline (a). For example, the single-frame baseline (a) fails to detect "horse" in the second and third frames of the fourth video due to part occlusion. In contrast, by leveraging feature aggregation with motion-aware calibration and self-guided sampling, SSVD can detect objects in each frame correctly.</p><p>Failure Cases Analysis. Here we further showcase some failure cases of our SSVD. As shown in Figure <ref type="figure" target="#fig_9">8</ref>, for case (a), our SSVD fails to correctly detect smaller objects, i.e., two cattle, when they are far away from the camera. Nevertheless, after a long range of time, our SSVD succeeds to detect them when they are near the camera. This problem can be alleviated by completing the detection results of smaller objects with more accurate ones when they are near camera. Therefore, how to preserve such long-range temporal coherence might strengthen our SSVD, which will be one of our future works. Moreover, in case (b), for the two dogs in each frame, we can easily detect one in the right, while our SSVD fails to correctly localize or recognize the left one. We speculate that this failure case is due to that SSVD leaves the relations between objects Methods Backbone mAP (%) R-FCN <ref type="bibr" target="#b7">[8]</ref> ResNet-101 73.6 DorT <ref type="bibr" target="#b42">[43]</ref> ResNet-101 73.9 Faster R-CNN <ref type="bibr" target="#b12">[13]</ref> ResNet-101 75.4 D (&amp; T loss) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 75.8 FGFA <ref type="bibr" target="#b23">[24]</ref> ResNet-101 76.3 LWDN <ref type="bibr" target="#b63">[64]</ref> ResNet-101 76.3 PSLA <ref type="bibr" target="#b64">[65]</ref> ResNet-101 77.1 MANet <ref type="bibr" target="#b20">[21]</ref> ResNet-101 78.1 THP <ref type="bibr" target="#b22">[23]</ref> ResNet-101+DCN 78.6 STSN <ref type="bibr" target="#b18">[19]</ref> ResNet-101+DCN 78.9 in each frame unexploited. One possible way to address it is to enable the interactions between objects in SSVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons with State-of-the-Art Methods</head><p>End-to-end Models. Table <ref type="table" target="#tab_7">III</ref> compares the performances of nine runs on ImageNet VID validation set, including several state-of-the-art techniques and our SSVD. These models purely learn video object detector by enhancing per-frame feature for detection in an end-to-end fashion without any postprocessing. Among them, the former two approaches (i.e., R-FCN <ref type="bibr" target="#b7">[8]</ref> and Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>) are two representative twostage image object detectors only exploit single frame information. D (&amp;T loss) <ref type="bibr" target="#b14">[15]</ref> exhibits better performances than the two image object detectors by exploiting temporal coherence among adjacent frames via RoI tracking module. The latter three baselines (i.e., FGFA <ref type="bibr" target="#b23">[24]</ref>, MANet <ref type="bibr" target="#b20">[21]</ref>, and STSN <ref type="bibr" target="#b18">[19]</ref>) further boost the performance for video object detection by enhancing per-frame feature. Specifically, FGFA takes optical flow as guidance for pixel-level calibration for feature aggregation. MANet outperforms FGFA by additionally utilizing box-level calibration for feature aggregation. STSN leverages spatio-temporal sampling instead of motion calibration for feature aggregation, which achieves comparable performance with FGFA under the same backbone of Deformable Convolution Network <ref type="bibr" target="#b60">[61]</ref> (DCN). Note that as reported in <ref type="bibr" target="#b22">[23]</ref>, the performance of FGFA with DCN is 78.8%. For fair comparisons, our SSVD is evaluated based on three commonly adopted basic architectures, ResNet-101, Deformable ResNet-101 (DCN), and ResNeXt-101-32×4d <ref type="bibr" target="#b65">[66]</ref> . Overall, the results with the same basic architecture INDICATES LINKING WITH SEQ-NMS <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone mAP (%) TPN+LSTM <ref type="bibr" target="#b16">[17]</ref> GoogLeNet 68.4 TCNN <ref type="bibr" target="#b40">[41]</ref> DeepID+Craft 73.8 FGFA <ref type="bibr" target="#b23">[24]</ref> ResNet-101 78.4 D&amp;T (τ = 1) <ref type="bibr" target="#b14">[15]</ref> ResNet-101 79.8 STSN <ref type="bibr" target="#b18">[19]</ref> ResNet-101+DCN 80.4 STMN <ref type="bibr" target="#b21">[22]</ref> ResNet-101 80.5 HQ-link <ref type="bibr" target="#b43">[44]</ref> ResNet-101 80.  consistently demonstrate that our proposed SSVD by integrating two-stream feature aggregation into one-stage detection paradigm exhibits better performance than all the six two-stage end-to-end models.</p><p>Post-processing. Furthermore, we equipped our SSVD with Seq-NMS <ref type="bibr" target="#b15">[16]</ref>, a larger degree of improvement is attained (e.g., from 79.2% to 80.5% for ResNet-101), achieving competitive results with state-of-the-art video object detectors plus the post-processing of box-level association. The comparison results are illustrated in Table <ref type="table" target="#tab_9">IV</ref>. Technically, both of FGFA and STSN adopt Seq-NMS as post-processing. TPN+LSTM exploits encoder-decoder LSTM to rescore generated tubelets proposal by Tubelet Proposal Networks (TPN) <ref type="bibr" target="#b16">[17]</ref> and the performance is 68.4%. TCNN performs tubelet rescoring by motion-guided propagation to stabilize detection results and achieves 73.8% mAP. D&amp;T links detection boxes between each two adjacent frames by predicted tracking boxes and Viterbi Algorithm as in <ref type="bibr" target="#b66">[67]</ref>, which boosts the performance from 75.8% to 79.8%. STMN firstly combines detection results of spatio-temporal memory module based sequence prediction with that of single-frame baseline, and then takes similar linking algorithm as D&amp;T. In addition, the performance of our SSVD with post-processing is even comparable to the winner of ILSVRC2016 <ref type="bibr" target="#b67">[68]</ref>, which is a comprehensive detection system. Specifically, by utilizing multi-scale testing and model ensemble, <ref type="bibr" target="#b67">[68]</ref> achieves 81.2% mAP, which is similar to the result of our SSVD with ResNeXt-101-32×4d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental Analysis</head><p>Effect of the Number of Sampled Support Frames. As mentioned in implementation details, we uniformly select support frames from the sliding feature buffer (25 adjacent frames) at inference stage. In order to show the relationship between the performance/run time and the number of sampled support frames, we compare the results by varying this number from 0 to 24. As shown in Table <ref type="table" target="#tab_11">V</ref>, except for the setting with 0 sampled support frame (i.e., our single-frame baseline), the performance by using different number of sampled support frames fluctuates within the range of 0.11%. That practically eases the option of the number of sampled support frames. Meanwhile, enlarging the number of sampled support generally increases run time at inference stage. Therefore, in our experiments, the number of sampled support frames is empirically set to 6, which is a good tradeoff between performance and run time. Run Time Comparison. Figure <ref type="figure" target="#fig_10">9</ref> depicts the detailed run time and performance of each approach. The results clearly indicate the advantage of SSVD against state-of-the-art twostage approaches in terms of both speed and accuracy. In particular, at inference stage, SSVD processes one frame in 85 ms, which is even faster than the single-frame baseline of two-stage video object detectors, e.g., R-FCN and Faster R-CNN. Note that here we exclude several video object detection methods ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b44">[45]</ref>) which are additionally equipped with the acceleration techniques (e.g., flow guided feature propagation and adaptive key frame selection). The similar acceleration techniques can also be adopted in our SSVD to further enhance the efficiency, and we will leave this discussion for further investigation.</p><p>Tracklet Evaluation. We additionally evaluate our SSVD over an auxiliary tracking metric (mAP track ) in ImageNet VID. The mAP track metric measures the mAP of predicted tracklet against ground truth tracklet in a snippet, which goes beyond the mAP metric that only evaluates the detection accuracy at frame level. Specifically, we exploit the Viterbi algorithm to link the per-frame detection boxes of our SSVD into tracklets. The mAP track of SSVD + Viterbi <ref type="bibr" target="#b66">[67]</ref> is 60.2%, which is higher than 57.0% of DorT <ref type="bibr" target="#b42">[43]</ref>. The results further demonstrate the effectiveness of our SSVD at tracklet level.</p><p>Take-aways. Here we summarize several take-aways as follows: (1) Motion stream often fails to estimate motion from optical flow when the object appearances are deteriorated by motion blur or occlusion, while sampling stream can hallucinate the feature map via frame sampling and thus performs better in this case. <ref type="bibr" target="#b1">(2)</ref> In contrast, sampling stream may suffer from robustness problem when object moves extremely fast. This is due to the fact that the receptive field in sampling stream for offset prediction is smaller than that in PWC-Net <ref type="bibr" target="#b59">[60]</ref> for optical flow generation. As such, the range of estimated motion in sampling stream is shorter than that in motion stream, resulting in failure of motion capturing in sampling stream when object moves extremely fast. In this case, motion stream performs better than sampling stream.</p><p>(3) In practice, considering the information redundancy across adjacent frames, we uniformly sample 6 support frames from the sliding feature buffer (25 adjacent frames) to perform detection at inference. As shown in Table V, this selected number of sampled support frames indeed seeks good tradeoff between performance and run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We have presented Single Shot Video Object Detector (SSVD), which explores temporal coherence across frames to boost video object detection. Particularly, we study the problem from the viewpoint of integrating two-stream feature aggregation into a single shot detector. To verify our claim, we utilize FPN to produce multi-scale feature maps in a spatial pyramid and feed feature maps in each scale into twostream feature aggregation structure. One is motion stream that performs feature aggregation by estimating motion from optical flow and warping the feature maps of nearby frames to the reference one along motion paths. The other is sampling stream, which directly hallucinates the feature map of the reference frame through spatio-temporal sampling from adjacent frames. The aggregated feature map in each stream is input into class/box subnets to simultaneously classify anchor boxes and perform bounding box regression. Extensive experiments conducted on ImageNet VID dataset validate our proposal and analysis. More remarkably, we achieve state-of-the-art performance of single model without post-processing: 79.2% mAP, by processing one frame in 85 ms on an Nvidia Titan X Pascal GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>T</head><label></label><figDesc>. Yao and W. Zhou are the corresponding authors. This work was supported in part to Dr. Houqiang Li by NSFC under contract No. 61836011, and in part to Dr. Wengang Zhou by NSFC under contract No. 61822208 &amp; 61632019 and Youth Innovation Promotion Association CAS (No. 2018497).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig.1. Video object detection results with motion stream and sampling stream. In the upper example (a), motion stream fails to detect domestic cat accurately due to the motion blur, while sampling stream that performs feature aggregation without motion estimation can ameliorate this case. In the bottom example (b) when the airplane moves fast, an unwanted bounding box of airplane is generated for sampling stream. In contrast, motion stream can solve this problem by capturing long-range motion with explicit motion estimation. Object detection in videos therefore should take both motion and sampling streams into consideration.</figDesc><graphic url="image-1.png" coords="1,372.01,238.00,195.25,76.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Feature aggregation with motion-aware calibration in the motion stream of our SSVD. Specifically, each support frame is paired with reference frame and injected into PWC-Net to estimate optical flow in different resolution. The optical flow fields, in turn, guide the calibration of support feature maps out from Feature Pyramid Networks (FPN) by bilinear warping.Next, we directly average the calibrated feature maps of all support frames as aggregated feature map in each scale to boost object detection for reference frame. Note that here we only depict how to aggregate the support feature maps with motion-aware calibration, and omit the feature aggregation from reference frame for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Feature aggregation with self-guided sampling in the sampling stream of our SSVD ('concat': 'channel-wise concatenate'). Note that here we only take the feature map with the lowest resolution as an example to depict the detailed processing of self-guided sampling. Specifically, for each spatial resolution, we concatenate the feature map from each support frame with feature map of reference frame, which will be fed into offset predictor for predict the offsets between the reference and support frames. Next, spatiotemporal sampling is further performed to directly hallucinate the feature of reference frame via deformable convolution over the feature map of support frame and the predicted offsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Intermedia data of our Sampling Stream (better viewed in color). The blue square in reference frame depicts a pixel, for which we want to compute a convolution output. The red points are the corresponding sampling locations in support frames, which are predicted by our offset predictor. aggregated feature ĝsp P i is endowed with the contextual content encoded in sampling.Intermedia Data. In Figure5, we provide some intermedia data to illustrate the effect of feature aggregation with selfguided sampling. The blue square in reference frame depicts a pixel, for which we want to compute a convolution output. The red points are the corresponding sampling locations in support frames, which are predicted by our offset predictor. As illustrated in this figure, our offset predictor learns to sample the locations from the supporting frames that belong to the corresponding object. The aggregated feature of reference frame is thus hallucinated by performing deformable convolution over the FPN feature map of support frame and the predicted offsets. From the aggregated feature map of reference frame, we can clearly observe that the features depicting the same object are strengthened.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Training Process of our SSVD 1: Input: a reference frame I t and two support frames I t+τ 1 , I t+τ 2 sampled from frame {I t+τ } K τ =−K . 2: Pre-processing: Data augmentation is performed as in<ref type="bibr" target="#b11">[12]</ref>. 3: Feature Extraction:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Different schemes for fusing two streams in SSVD: (1) Integrate estimated motion from motion stream into sampling stream for feature aggregation; (2) Early fusion before class/box subnets via concatenation and (3) Late fusion with NMS. The jaccard overlap of NMS is set as 0.45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Four video examples of detection results on ImageNet VID validation set by the single-frame baseline and our SSVD. The examples include four typical deteriorated object appearances in videos, i.e., rare poses, motion blur, illumination variation and part occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Failure cases in the ImageNet VID dataset (better viewed in color). The green boxes illustrate the true-positive detections, and the red one shows the prediction with wrong classes. TABLE III PERFORMANCE COMPARISONS WITH THE STATE-OF-THE-ART END-TO-END VIDEO OBJECT DETECTION METHODS ON IMAGENET VID VALIDATION SET. THE MEAN AVERAGE PRECISION (MAP) OVER ALL CLASSES IS REPORTED.</figDesc><graphic url="image-339.png" coords="9,119.26,94.97,110.50,77.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance and run time comparisons with the state-of-the-art methods at inference on ImageNet VID dataset. The mean Average Precision and average runtime for each method are detailed in the legend below.</figDesc><graphic url="image-342.png" coords="10,97.99,56.20,156.07,137.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>FA with motion-aware calibration</head><label></label><figDesc>capitalizes</figDesc><table><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>class/box subnets</cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>...</cell><cell>class/box subnets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>class/box subnets</cell></row><row><cell>ResNet</cell><cell>...</cell><cell>ResNet</cell><cell>...</cell><cell>ResNet</cell></row><row><cell></cell><cell>...</cell><cell></cell><cell>...</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>FA with self-guided sampling</head><label></label><figDesc></figDesc><table><row><cell>class/box subnets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>class/box subnets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>class/box subnets</cell><cell>class subnet</cell><cell></cell><cell></cell><cell>Combination</cell></row><row><cell></cell><cell></cell><cell>W x H W x H x256 x256</cell><cell>W x H W x H x 256 x 256</cell><cell>W x H W x H x kA x kA</cell><cell>Stream</cell></row><row><cell></cell><cell></cell><cell>W x H W x H</cell><cell>W x H W x H</cell><cell>W x H W x H</cell></row><row><cell></cell><cell>box</cell><cell>x256 x256</cell><cell>x 256 x 256</cell><cell>x 4A x 4A</cell></row><row><cell></cell><cell>subnet</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>MAIN ACRONYMS AND NOTATIONS USED IN THIS PAPER.</figDesc><table><row><cell>I t</cell><cell>a frame at time t</cell></row><row><cell>{I t }</cell><cell>a set of frames</cell></row><row><cell>K</cell><cell>temporal spanning range</cell></row><row><cell>N mo FPN (•) N sp FPN (•)</cell><cell>feature pyramid network in motion stream feature pyramid network in sampling stream</cell></row><row><cell>Npwc(•)</cell><cell>PWC-Net</cell></row><row><cell>Noff(•)</cell><cell>offset predictor</cell></row><row><cell>Ndet(•)</cell><cell>detect subnets</cell></row><row><cell>W(•)</cell><cell>bilinear warping</cell></row><row><cell>Dconv(•)</cell><cell>deformable convolution layer</cell></row><row><cell>{f t P i } 6 i=3 {g t P i } 6 i=3 f mo P i</cell><cell>pyramid features in motion stream pyramid features in sampling stream aggregated feature in motion stream</cell></row><row><cell>ĝsp P i P mo t P sp t</cell><cell>aggregated feature in sampling stream predictions in motion stream before NMS predictions in sampling stream before NMS</cell></row><row><cell>Dt</cell><cell>final detections</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>4, 5, 6} Aggregation 14: FA with Self-guided Sampling:</figDesc><table><row><cell>15:</cell><cell>for τ ∈ {τ1, τ2} do</cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell>for i ∈ {3, 4, 5, 6} do</cell><cell></cell><cell></cell></row><row><cell>17: 18:</cell><cell>∆o (t,t+τ ) P i g t+τ →t P i = DConv(g t+τ = Noff(g t+τ P i , g t P i ) P i , o (t,t+τ ) P i</cell><cell>)</cell><cell>Offset Prediction Sampling</cell></row><row><cell>19:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="3">20: 21: 22: Perform Detection: end for ĝsp P i = g t+τ 1 →t P i t+τ 2 →t +g P i 2 23: Applying detect subnets on , i ∈ {3, 4, 5, 6} f mo P i and ĝsp P i .</cell><cell>Aggregation</cell></row></table><note>24:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>1 :</head><label>1</label><figDesc>Input: video frames {I t }, temporal spanning range K. 2: Feature Buffer Initialization: 3: for t = 1 to K + 1 do</figDesc><table><row><cell>4: 5:</cell><cell cols="3">{f t+τ P i } 6 i=3 = N mo FPN (I t+τ ) {g t+τ P i } 6 i=3 = N sp FPN (I t+τ )</cell><cell>Motion Stream Sampling Stream</cell></row><row><cell cols="3">6: end for</cell><cell></cell><cell></cell></row><row><cell cols="4">7: Online Detection:</cell><cell></cell></row><row><cell cols="4">8: for t = 1 to ∞ do</cell><cell></cell></row><row><cell>9:</cell><cell cols="3">(In Motion Stream)</cell><cell></cell></row><row><cell>10:</cell><cell cols="3">for τ = max(1, t − K) to t + K do</cell><cell></cell></row><row><cell>11:</cell><cell>{m</cell><cell>(t,t+τ ) P i</cell><cell>} 6 i=3 = Npwc(I t , I t+τ )</cell><cell>Motion Estimation</cell></row><row><cell>12:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>CONTRIBUTION OF EACH STREAM IN OUR SSVD ON IMAGENET VID VALIDATION SET. THE SUBSCRIPTS DENOTE THE ABSOLUTE PERFORMANCE GAINS COMPARED TO THE SINGLE-FRAME BASELINE (A).</figDesc><table><row><cell>feature aggregation</cell><cell></cell><cell cols="3">mean average precision (%)</cell></row><row><cell cols="2">motion sampling slow stream stream</cell><cell cols="2">medium fast</cell><cell>overall</cell></row><row><cell>(a)</cell><cell>81.1</cell><cell>73.7</cell><cell>52.8</cell><cell>74.5</cell></row><row><cell>(b)</cell><cell cols="4">85.4 ↑4.3 77.5 ↑3.8 56.1 ↑3.3 78.2 ↑3.7</cell></row><row><cell>(c)</cell><cell cols="4">85.6 ↑4.5 77.6 ↑3.9 56.8 ↑4.0 78.5 ↑4.0</cell></row><row><cell>(d)</cell><cell cols="4">86.1 ↑5.0 78.2 ↑4.5 57.7 ↑4.9 79.2 ↑4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISONS WITH THE STATE-OF-THE-ART VIDEO OBJECT DETECTION SYSTEMS PLUS POST-PROCESSING ON IMAGENET VID VALIDATION SET. THE MEAN AVERAGE PRECISION (MAP) OVER ALL CLASSES IS REPORTED.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V THE</head><label>V</label><figDesc>EFFECT OF THE NUMBER OF SAMPLED SUPPORT FRAMES IN OUR SSVD AT INFERENCE ON IMAGENET VID DATASET. THE EXPERIMENTS ARE CONDUCTED ON AN NVIDIA TITAN X PASCAL GPU.</figDesc><table><row><cell># sampled support frames</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>mAP (%)</cell><cell cols="6">74.52 77.1 79.07 79.18 79.12 79.16</cell></row><row><cell>run time (ms)</cell><cell>38</cell><cell>66</cell><cell>74</cell><cell>85</cell><cell>114</cell><cell>183</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">P 3, P 4, P 5 denote the output of standard FPN, and correspond to the last residual blocks in conv3, conv4, conv5 of ResNet. P 6 is produced by attaching 3 × 3 conv with stride</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">over the last residual block in conv5.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a unified sample weighting network for object detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Seq-nms for video object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporallyaware feature maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5686" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatialtemporal memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lighthead r-cnn: In defense of two-stage object detector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive convolution for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3205" to="3217" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection via body part semantic and contextual information with dnn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3148" to="3159" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2239" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring categoryagnostic clusters for open-set domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring object relation in mean teacher for cross-domain detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">466</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A robust learning approach to domain adaptive object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khodabandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="480" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchorbased object detector</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2896" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detect or track: Towards costeffective video object detection/tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8803" to="8810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Object detection in videos by high quality object linking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09823</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unified spatio-temporal attention networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep intrinsic video representation by exploring temporal coherence and graph structure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3832" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with local and global diffusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mocycle-gan: Unpaired video-to-video translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video object detection with locally-weighted deformable neighbors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8529" to="8536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Progressive sparse local attention for video object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3909" to="3918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Ilsvrc2016 object detection from video: Team nuist</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks/2016/Imagenet%202016%20VID.pptx" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
