<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Objective based Spatio-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dae</forename><forename type="middle">Hoe</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Dae-jeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wissam</forename><forename type="middle">J</forename><surname>Baddar</surname></persName>
							<email>wisam.baddar@kaist.ac.kr.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Dae-jeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinhyeok</forename><surname>Jang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Dae-jeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Ro</surname></persName>
							<email>ymro@kaist.ac.kr.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Dae-jeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Objective based Spatio-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">329F7F06AE418411E40CB8D7B18A8F93</idno>
					<idno type="DOI">10.1109/TAFFC.2017.2695999</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression recognition (FER)</term>
					<term>expression intensity variation</term>
					<term>spatio-temporal feature representation</term>
					<term>deep learning</term>
					<term>long short-term memory (LSTM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial expression recognition (FER) is increasingly gaining importance in various emerging affective computing applications. In practice, achieving accurate FER is challenging due to the large amount of inter-personal variations such as expression intensity variations. In this paper, we propose a new spatio-temporal feature representation learning for FER that is robust to expression intensity variations. The proposed method utilizes representative expression-states (e.g., onset, apex and offset of expressions) which can be specified in facial sequences regardless of the expression intensity. The characteristics of facial expressions are encoded in two parts in this paper. As the first part, spatial image characteristics of the representative expression-state frames are learned via a convolutional neural network. Five objective terms are proposed to improve the expression class separability of the spatial feature representation. In the second part, temporal characteristics of the spatial feature representation in the first part are learned with a long short-term memory of the facial expression. Comprehensive experiments have been conducted on a deliberate expression dataset (MMI) and a spontaneous micro-expression dataset (CASME II). Experimental results showed that the proposed method achieved higher recognition rates in both datasets compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>acial expression is considered important for nonverbal communication since it can convey the internal states, emotions and intentions of humans <ref type="bibr" target="#b0">[1]</ref>. In recent years, researchers in computer vision, affective computing and human computer interaction fields have been trying to automatically recognize and interpret facial expressions <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. This is mainly attributed to the multitude of potential applications, such as interactive agents, fatigue measurement or even lie detection <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>While many research efforts have been devoted to explore facial expression recognition (FER), most of the previous researches have adopted hand-crafted features. Those methods can be broadly divided into two groups; static FER <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and dynamic FER <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Static (frame-based) FER relied only on static facial features obtained by extracting hand-crafted features from the selected peak (apex) expression frames of video sequences. The effectiveness of various types of appearance features (e.g., local binary patterns (LBPs) <ref type="bibr" target="#b15">[16]</ref>, local phase quantization (LPQ) <ref type="bibr" target="#b16">[17]</ref>, Gabor wavelets <ref type="bibr" target="#b17">[18]</ref>, etc.) has been investigated. These hand-crafted features could cause a feature confusion that could occur between the facial expression and the facial identity <ref type="bibr" target="#b18">[19]</ref>. To reduce the effect of subject identity on the FER performance, some methods adopted appearance difference <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, geometrical difference <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> or both differences <ref type="bibr" target="#b22">[23]</ref> between the query face and the neutral face of a subject. However, in real-world applications, it is generally difficult to assume that the identity of subject is known and the neutral face image of subject is available beforehand <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Moreover, the performance of such methods could be degraded when non-apex frames are used or when the expression intensity is small (i.e., subtle and micro-expressions).</p><p>Inspired by the fact that facial expressions are inherently a dynamic process <ref type="bibr" target="#b10">[11]</ref>, research efforts have utilized spatio-temporal features in order to capture the expression dynamics in facial expression sequences. Spatiotemporal features, such as local binary patterns from three orthogonal planes (LBP-TOP) <ref type="bibr" target="#b11">[12]</ref> and local phase quantization from three orthogonal planes (LPQ-TOP) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> were obtained by extending LBP <ref type="bibr" target="#b15">[16]</ref> and LPQ <ref type="bibr" target="#b16">[17]</ref> into the three orthogonal planes (XY plane: appearance, XT planes: horizontal motion, and YT plane: vertical motion) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Although the spatio-temporal features have shown an improvement on the FER performance compared to the static (frame-based) features, they suffer from the following drawbacks. First, the spatial appearance features extracted from XY plane could encode the subject identity which is not desirable for FER <ref type="bibr" target="#b13">[14]</ref>. Second, the extracted dynamic features could be negatively affected by the different characteristics of the facial ex-pression (e.g., different transition duration and/or transition type such as onset or offset <ref type="bibr" target="#b12">[13]</ref>). Added to that, these spatio-temporal features adopt temporal normalization to obtain expression sequences with a fixed number of frames. The temporal normalization methods could result in the loss of temporal scale information.</p><p>The recent successes of deep learning in different fields of computer vision have prompted researchers to investigate in utilizing deep learning methods in FER <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, local textural patterns, namely 'micro-action-patterns', were learned by a convolutional layer at apex expression frames. The learned micro-action-patterns were clustered and passed to a restricted Boltzmann machine to learn a spatial feature representation of the expression. In <ref type="bibr" target="#b25">[26]</ref>, a deep convolutional neural network (CNN) was used to learn a spatial feature representation. By relying on the spatial feature only, the method did not utilize facial expression dynamics were while performing the FER task, which can limit the performance at non-apex frames or frames of subtle expressions. In <ref type="bibr" target="#b18">[19]</ref>, a 3D CNN was utilized to learn spatio-temporal features from deformable facial action parts. In <ref type="bibr" target="#b31">[32]</ref>, a 3D CNN was also used in order to learn the spatio-temporal appearance features of the sequence. In <ref type="bibr" target="#b31">[32]</ref>, a temporal geometric feature was jointly learned in order to reduce the effect of the identity on the learned spatio-temporal appearance features. Although 3D CNNs can capture the dynamics of the expression, the 3D convolution is computationally expensive. Moreover, a large number of parameters (weights and biases) need to be learned, which makes the learning processes harder <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. To reduce complexity and the number of learned parameters, a small number of frames need to be used as an input for the 3D CNN. This could cause a loss of information on facial expression dynamics <ref type="bibr" target="#b34">[35]</ref>. Subtle expression dynamics that occur on micro-expression sequences could be missed due to the short temporal range of the 3D CNN. In other methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, a recurrent neural network (RNN) was used with the conventional CNN to encode dynamics in the sequence for classification of facial expression <ref type="bibr" target="#b26">[27]</ref> and prediction of the arousal and valence scores of emotions <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The methods showed that the architectures of CNN with RNN can improve recognition performances compared to conventional CNN.</p><p>While many research efforts have been conducted for FER, achieving accurate and effective FER is still a challenging problem. This is mainly attributed to the large amount of variations in the imaging conditions and the inter-personal variations such as expression intensity variations <ref type="bibr" target="#b30">[31]</ref>. In practice, facial expressions appear in a wide range of expression intensity, i.e., expression intensity can span from subtle micro-expression motions to exaggerated expressions <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Therefore, a robust FER method to expression intensity variation is of paramount importance for practical FER. In this paper, we propose a new spatio-temporal feature representation learning for FER that is robust to expression intensity variations. The contributions of this paper are summarized as follows 1. New spatio-temporal feature representation learning method to robust expression intensity variations is proposed. Differing from the simple CNN and RNN architecture <ref type="bibr" target="#b26">[27]</ref> which only utilized expression class information, the proposed method utilizes the representative expression-state information (e.g., onset, apex and offset of expressions) in the network training with proposed objective terms. It has been widely accepted that the expression-states is useful for facial analysis <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. By incorporating the representative expression-states, the proposed method mitigates the problem of the expression intensity variations and varying expression duration, and at the same time, focuses on improving expression class separability of the learned feature representation. The proposed learning method consists of two parts. In the first part, spatial feature representation of the facial expression is learned using the CNN with representative expression-states (i.e., onset, onset to apex transition, apex, apex to offset transition and offset). The CNN adopts the expression-states in the objective function to regulate the learning of the spatial feature representation. So, the expression class separability of the learned spatial feature representation is improved. In the second part, temporal characteristics of the spatial feature representation in the first part are learned with a long short-term memory (LSTM) <ref type="bibr" target="#b37">[37]</ref>. The temporal sequence learning is devised to learn time scale dependent information in facial expression sequences that have varying number of frames. To preserve the expression-state continuity for further sequence learning, the fifth objective function is devised. The objective term for the expressionstate continuity is useful to efficiently encode intermediate expression-states (e.g., it is useful when some of expression-states are missed). To the best of our knowledge, this work is the first attempt to devise objective terms to utilize the expression state information in deep learning for facial expression recognition. Extensive and comprehensive experiments have been conducted on the MMI dataset <ref type="bibr" target="#b35">[36]</ref> for deliberate expressions and the CASME II dataset <ref type="bibr" target="#b34">[35]</ref> for microexpressions. Experimental results from both datasets show that the expression class separability of the learned feature representation is improved in both types of expression intensity (i.e., deliberate expressions and microexpressions). As a result, the proposed method outperforms state-of-the-art methods on both datasets in terms of the recognition rate. Moreover, an inter-dataset experiment shows that the FER with the learned feature representation maintains a comparable performance under intra-dataset conditions. These results indicate that the proposed method can learn versatile spatio-temporal feature representation for both deliberate expression and micro-expression.</p><p>The remainder of this paper is organized as follows. Section 2 details the proposed facial expression recognition with learned spatio-temporal feature representation. Section 3 presents experimental results to verify the effectiveness of the proposed spatio-temporal feature representation. Finally, conclusions are drawn in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FACIAL EXPRESSION RECOGNITION WITH LEARNED SPATIO-TEMPORAL FEATURE REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the Proposed Method</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the overview of the proposed spatiotemporal feature representation learning for FER. The proposed learning for spatio-temporal feature representation consists of two parts: 1) spatial feature representation learning with expression-state constraints and 2) consecutive temporal-feature representation learning with LSTM.</p><p>In the first part, representative frames of different expression-states (i.e., onset, onset to apex transition, apex, apex to offset transition and offset) of the facial expression sequence are utilized in the learning. From the ground truth of representative expression-state frames, spatial feature representations are learned by a CNN with an objective function with multiple objective terms, which are robust to expression intensity variations. The objective terms are devised to regulate the learning process by 1) minimizing expression classification error, 2) minimizing intra-class variation within the same expression class, 3) minimizing expression-state classification error, 4) minimizing expression-state variation, and 5) preserving the expressionstate continuity, which is necessary for consecutive temporal-feature representation learning in the second part.</p><p>In order to capture the facial expression dynamics, temporal feature representation of the facial expression is learned via the LSTM of the second part. The CNN model learned in the first part is utilized to extract spatial feature representation of each frame for training the LSTM. The expression class separability of the learned spatial feature representation is improved due to the expression-state constraints. On top of the spatial feature representation, the LSTM can consecutively learn the temporal dynamics of the facial expression. As a result, the proposed method can generate discriminative spatio-temporal feature representations that improve the recognition of facial expression at different expression intensities. The details of each step of the proposed method are described in the following subsections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial Feature Representation Learning with Expression-State Constraints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Representative Expression-State Frame Extraction</head><p>In practice, facial expressions appear in a wide range of expression intensities. The expression intensities can be measured by determining deformation of facial parts, which result in difference of facial images <ref type="bibr" target="#b38">[38]</ref>. The facial expression intensity can be measured by calculating correlation between features of the reference frame and other frames <ref type="bibr" target="#b39">[39]</ref>. In this paper, normalized cross correlation <ref type="bibr" target="#b40">[40]</ref> of each frame with respect to the apex frame is used to measure the changes between facial frames, which is a widely used image similarity metric due to its robustness to linear changes in the amplitude of illumination <ref type="bibr" target="#b41">[41]</ref>. Fig. <ref type="figure">2</ref> shows examples of the change of expression state, which is represented by the normalized cross correlations.</p><p>As shown in the figure, the expression intensity varies in a wide range across subjects and expression classes. Such expression intensity variations make the practical FER a challenging problem <ref type="bibr" target="#b13">[14]</ref>. To overcome this challenge, representative expression-states are utilized, which are specified in facial sequences regardless of the expression intensity variations and different expression duration <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Representative expression-state frames are selected from an expression sequence as  <ref type="figure" target="#fig_1">3</ref>. By utilizing the expression-state as a learning constraint of the CNN, the proposed method improves expression class separability of the learned spatial feature representation. In addition, the use of the representative expression-states enables the spatial feature representation learning to handle a variety of expression transition duration.  </p><formula xml:id="formula_0">, , 2 , , 2 , '                     off off</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input shape</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stride</head><p>Pad </p><formula xml:id="formula_1">Output shape Input 64643 - - - 64643 C1 64643 33 1 0 626232 S1 626232 33 2 0 313132 C2 313132 33 1 0 292964 S2 292964 33 2 0 141464 C3 141464 33 1 0 121264 S3 121264 33 2 0 6664 F4 6664 - - - 5121 F5 ** 5121 - - - 5121 Fc *** 5121 (out- put of F5) - - - Nc1 Fp *** 5121 (out- put of F5) - - - Np1 * C, S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Learning Spatial Feature Representation with the Proposed Objective Function Constraints to Expression-States</head><p>Table <ref type="table" target="#tab_4">1</ref> shows the architecture used for the spatial feature representation learning with expression-state constraints.</p><p>The architecture is based on LeNet architecture <ref type="bibr" target="#b42">[42]</ref>. The parameters on the architecture in Table <ref type="table" target="#tab_4">1</ref> were experimentally chosen for computational efficiency and classification accuracy. Note that in the network, Fc layer is used for expression classification and Fp layer is used for expression state classification. After training the network, the proposed spatial feature representation is extracted at the layer F5. To improve the expression class separability of the spatial feature representation during the training of the CNN, five objective terms illustrated in Fig. <ref type="figure" target="#fig_6">4</ref> are devised. In addition, gradients of the objective terms used for back-propagation algorithm <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref> are accordingly derived. Note that Table <ref type="table">B</ref> in Appendix B summarizes variables used in the proposed objective terms. The first objective term (E1) of the objective function (Fig. <ref type="figure" target="#fig_6">4 (a)</ref>) is devised for minimizing the expression classification error at the layer Fc. To that end, a cross entropy error <ref type="bibr" target="#b45">[45]</ref> can be readily utilized to enforce the learning of discriminative feature representation, which is defined as</p><formula xml:id="formula_2">, log ) ( , ) ( , 1     i k F k i F k i c c y y E (2)</formula><p>where</p><formula xml:id="formula_3">) ( , c F k i y</formula><p>is the expression ground truth of the i-th sample (1 if k is the correct class and 0 otherwise), and</p><formula xml:id="formula_4">) ( , ˆc F k i y</formula><p>is the predicted probability that the sample belongs to the expression class k calculated at the layer Fc with Softmax function. The objective term E1 makes samples with different expression classes be separable in the feature space as shown in Fig. <ref type="figure" target="#fig_6">4 (a)</ref>.</p><p>To perform back-propagation, the gradients of the objective term E1 for the layer Fc are written as follows (please see Appendix A for the detailed derivation):</p><p>  , and ˆ) ( </p><formula xml:id="formula_5">T F F F i F i F i F c c c c c E E E                 b h W y y b<label>1 ) 5 ( ) ( 1 ) ( ) ( ) ( 1</label></formula><p>is the expression ground truth vector of the i-th training sample,    <ref type="table" target="#tab_4">1</ref>). As shown in (3), the gradient with respect to the weight is dependent on the gradient with respect to the bias. Therefore, the gradients with respect to the bias are sufficient to be described in the objective term. The gradients of the other layers can also be computed by the same backpropagation algorithm <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>.</p><p>In addition, the intra-class variations in the same expression class could occur due to factors such as the subject appearance. To mitigate those intra-class variations of spatial feature representations at the last hidden layer (F5 in Table <ref type="table" target="#tab_4">1</ref>), the term E2 (Fig. <ref type="figure" target="#fig_6">4 (b)</ref>) is devised as</p><formula xml:id="formula_8">, ) ( 2 1 , , 2 min 2 2 , , 2           i p c c c i p c d g E m f (4)</formula><p>where f c,p,i is the spatial feature representation vector of the i-th training sample of class c and the p-th expressionstate extracted at the last hidden layer (F5 in Table <ref type="table" target="#tab_4">1</ref>), m c is the mean feature vector of the training samples in class c and c d min is half of the minimum distance between m c and m j for c j  . The function <ref type="bibr" target="#b46">[46]</ref> where  is a sharpness parameter. As shown in Fig. <ref type="figure" target="#fig_6">4</ref> (b), the minimization of (4) makes f c,p,i be in a multi-dimensional sphere which radius is c d min and the center point is m c , thus resulting in the minimization of intra-class variation.</p><formula xml:id="formula_9">   / )) exp( 1 log( ) (   g is a smoothed approximation of ) , 0 max( ] [    </formula><p>For simplicity when obtaining the gradient of the second objective term (E2) in ( <ref type="formula" target="#formula_15">4</ref>), the mean feature vector m c is assumed to be a constant vector. Thus the gradient of E2 with respect to the bias of the last hidden layer (F5 in Table <ref type="table" target="#tab_4">1</ref>) can be written as</p><formula xml:id="formula_10">, ) ( , ) ( ' ) )( ( ' 2 min 2 2 , ,<label>2 , , ) 5 ( , , 2 ) 5 ( 2</label></formula><formula xml:id="formula_11">c i p c c i p c F c i p c F d e e g E         f m h m f b   (5)</formula><p>where '  ' denotes the Hadamard product, As can be observed from Fig. <ref type="figure" target="#fig_1">3</ref>, expression-state frames could appear similar to other expression-state frames, due to small facial motion changes. Such similarity between the frames could result in similar spatial feature representation along the sequence of frames. As a result, temporal changes become harder to model, especially in micro-expression sequences. To improve the distinction between the learned spatial feature representations of different expression-state frames, two objective terms (E3 and E4) are devised. E3 (Fig. <ref type="figure" target="#fig_6">4 (c)</ref>) is added to distinguish the expression-state by minimizing the expression-state classification error at the layer Fp as</p><formula xml:id="formula_12">, log ) ( , ) ( , 3     i j F j i F j i p p y y E (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where j is the expression-state index, </p><p>) (  y is the expression-state ground truth vector of the i-th training sample.  As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, even same expression-state of the expression class could contain appearance variations (e.g., subject appearance variations, expression intensity variations, etc.). To minimize the variations of expression-state in spatial feature representations at the last hidden layer (F5 in Table <ref type="table" target="#tab_4">1</ref>), the term E4 (Fig. <ref type="figure" target="#fig_6">4 (d)</ref>) is devised as As a result, it helps cluster the spatial feature representation of the same expression-state, which minimizes the effect of the expression-state variations on the FER task.</p><p>By considering the feature means m c,p as constant vectors, the gradient of E4 with respect to bias can be written as The terms E3 and E4 are devised to improve the distinction between expression-states. However, spatial feature representations from adjacent frames in an expression sequence are not guaranteed to be continuous to each other in the feature space. The continuity of spatial feature representations in adjacent frames is useful to expression sequence classification in the following temporal feature representation learning (described in section 2.3). Thus, the last term, E5 (Fig. <ref type="figure" target="#fig_6">4 (e)</ref>), is devised to regulate the expression-state continuity in the feature space. As a result, spatial feature representations of frames between adjacent expression-states are enforced to reside in between the spatial feature representations in the feature space. The term E5 is defined as  <ref type="bibr" target="#b9">(10)</ref> where p apex is the index of apex expression-state, q represents the index of the adjacent expression-state in the learning stage. As shown in Fig. <ref type="figure" target="#fig_6">4</ref> (e), the term E5 is devised to feature means of two adjacent expression-states m c,p and m c,q become continuous to each other in the feature space.</p><formula xml:id="formula_15">. , ) ( ' ) )( ( ' 2 , min 2 2 , ,<label>, 4 , , ) 5 ( , , , 4 ) ( 4 </label></formula><formula xml:id="formula_16">                              , if , 1 if , 1 , 2<label>2 1 , 2 , min 2 2 ,</label></formula><p>Similar to <ref type="bibr" target="#b8">(9)</ref>, by considering the feature means m c,p and m c,q as constant vectors, the gradients of E5 with respect to bias of the last hidden layer (F5 in Table <ref type="table" target="#tab_4">1</ref>) can be written as where N c,p is the number of samples from all subjects with expression-states for the training set.</p><formula xml:id="formula_17">, 2 , ) ( ' ) ( ) ( ' 2 , min 2 2 , ,<label>5 0 , , ) 5 ( , , , 5 ) ( 5 </label></formula><p>To validate the improvement of the expression class separability according to the proposed objective terms, Fisher's discriminant ratio <ref type="bibr" target="#b47">[47]</ref> is employed as a class separability score. The Fisher's discriminant ratio (J) measures the ratio of the between-class scatter (S b ) and the within-class scatter (S w ) as follows  </p><formula xml:id="formula_18">Input Input Nf  (64643) Nf  (64643) Spatial feature representation C1~F5 in Table 1 Nf  (64643) Nf  (5121) Temporal feature representation L6 Nf  (5121) Nf  (5121) L7 Nf  (5121) Nf  (5121) L8 Nf  (5121) Nf  (5121) F9 Nf  (5121) Nc  1 * L is an abbreviation of LSTM layer.</formula><p>Nf is the number of frames and Nc is the number of expression classes.</p><p>where N c is the number of classes, N i is the number of samples in the i-th expression class, m i is the mean feature vector of samples in the i-th class, m is the mean feature vector of all samples and f ij is the spatial feature representation of the j-th sample in the i-th class. The spatial feature representation was extracted from the last hidden layer of the trained CNN (F5 in Table <ref type="table" target="#tab_4">1</ref>). Higher Fisher's discriminant ratio is achieved with larger between-class scatter and smaller within-class scatter which results in better expression class separability. Fig. <ref type="figure" target="#fig_15">5</ref> shows the class separability scores with different objective terms in the objective function. As can be seen in the figure, by employing E2, which focuses on reducing the intra-class variations of each expression, the class separability score in feature space is improved. Significant improvement is achieved by including terms related to the expression-state (i.e., E3, E4 and E5) in the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal Feature Representation Learning with LSTM</head><p>In this section, temporal feature representation learning with LSTM is described. LSTM is employed to model facial dynamics of the variable length facial expression sequences.          Fig. <ref type="figure" target="#fig_19">6</ref> (a) illustrates the structure of the LSTM defined in <ref type="bibr" target="#b12">(13)</ref>. Fig. <ref type="figure" target="#fig_19">6</ref> (b) and (c) visualize the evolution of the states of three gates, a memory cell and an output (i.e., latent feature representation) for two LSTM dimensions that are activated on sad expression (Fig. <ref type="figure" target="#fig_19">6 (b</ref>)) or a surprise expression (Fig. <ref type="figure" target="#fig_19">6 (c</ref>)) at the last LSTM layer L8 in the MMI dataset. As shown in the figures, the input gate determines which information should be added to the memory cell. The forget gate decides which information stored in the memory cell is important and should be retained (i.e., larger values are activated at the forget gate to retain information in its memory cell). As the network processes more frames, the memory cell states gradually absorb the useful information related to expression classes. And the output gate makes a latent feature representation of the output data related to expression classes. Note that the LSTM layer illustrated in Fig. <ref type="figure" target="#fig_19">6</ref> (a) operates as follows <ref type="bibr" target="#b37">[37]</ref>   is the memory cell, and</p><formula xml:id="formula_19">E 1 +E 2 E 1 +E 2 +E 3 E 1 +E 2 +E 3 +E 4 +E 5</formula><formula xml:id="formula_20">E 1 +E 2 E 1 +E 2 +E 3 E 1 +E 2 +E 3 +E 4 +E 5 E 1 +E 2 +E 3 +E 4 (a) (b) (c)</formula><formula xml:id="formula_21">σ h σ σ ) 1 (  l t h ) ( l t h ) ( l t h ) ( 1 l t  h ) ( 1 l t  c ) ( l t c ) ( , l t f g ) ( , l t i n g ) ( ,</formula><formula xml:id="formula_22">, ) tanh( , ) ] , [ tanh( ), ] , [ sigm( ), ] , [ sigm( ), ] , [ sigm( ) ( , ) ( ) ( ) ( , ) ( ) 1 ( ) ( 1 ) ( ) ( , )<label>( 1 )</label></formula><formula xml:id="formula_23">( ) ( ) 1 ( ) ( 1 ) ( ) ( , ) ( ) 1 ( ) ( 1 ) ( ) ( , ) ( ) 1 ( ) (</formula><formula xml:id="formula_24">g c h g b h h W g c c b h h W g b h h W g b h h W g                       (13)</formula><formula xml:id="formula_25">) ( l t h</formula><p>is the output of the l-th LSTM layer with a given t-th input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>To verify the effectiveness of the proposed FER with learned spatio-temporal feature representation, experiments have been conducted on the MMI dataset <ref type="bibr" target="#b35">[36]</ref> for deliberate expressions, and the CASME II dataset <ref type="bibr" target="#b34">[35]</ref> for micro-expressions. In the experiments, the face region in each frame was automatically cropped and aligned based on the eye landmarks <ref type="bibr" target="#b50">[50]</ref> which were automatically detected <ref type="bibr" target="#b51">[51]</ref>. The construction of the utilized MMI and CASME II datasets was performed as follows 1. MMI dataset: A total of 205 deliberate expression sequences with frontal faces were collected from 30 subjects. The expression sequences were recorded at a temporal resolution of 24 fps. Each expression sequence of the dataset was labeled with one of the six basic expression classes (i.e., angry, disgust, fear, happy, sad, and surprise). The exsequences were collected such that, the first frame in the sequence was the onset frame and last frame was the offset frame. The indexes of the apex were located manually <ref type="bibr" target="#b13">[14]</ref>. 2. CASME II dataset: A total of 246 spontaneous micro-expression sequences were collected from 26 subjects. The expression sequences were recorded at a temporal resolution of 200 fps. The microexpression sequences in the dataset were labeled as one of the five expression classes (i.e., happiness, disgust, repression, surprise, and others). The onset, apex and offset frame indexes were also provided for each micro-expression sequence. In the experiments, micro-expression sequences from onset-to-offset were used <ref type="bibr" target="#b52">[52]</ref>. All the experiments in this paper were conducted in a subject independent manner, such that the subjects in the training set were excluded from the test set. Particularly, the experiments were conducted with a leave-one-subjectout (LOSO) cross validation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>. To avoid overfitting due to the limited number of samples in the utilized datasets (MMI and CASME II), expression sequences were augmented during the training <ref type="bibr" target="#b53">[53]</ref>. For each training expression sequence, 150 augmented sequences were obtained by: 1) horizontal flipping of the sequence frames, 2) rotating the frames between the angles [-10º, 10º] with an increment of 5º, 3) translating the frames along [(0, 0), (-2, -2), (-2, +2), (+2, -2), (+2, +2)] pixels in x and y axis, and 4) scaling the frames with scaling factors of 0.9, 1.0 and 1.1. As a result, about 29k and 35k training sequences were generated for each cross validation fold on average in MMI and CASME II datasets, respectively. For training the CNN, 5 representative expression-state frames were selected from each augmented training sequence as described in section 2.2.1, resulting in 148k and 177k training frames for each cross validation fold on average in MMI and CASME II datasets, respectively. After that, the learned CNN model was utilized to extract the spatial feature representation of all frames of training sequences for training the LSTM. The learning of the CNN (shown in Table <ref type="table" target="#tab_4">1</ref>) with the proposed objective terms was implemented by ConvNet <ref type="bibr" target="#b54">[54]</ref>. The temporal feature representation learning with LSTM (Table <ref type="table" target="#tab_6">2</ref>) was implemented by Keras <ref type="bibr" target="#b55">[55]</ref>. For the activation function, rectified linear unit (ReLU) <ref type="bibr" target="#b56">[56]</ref> was used. In this paper, the initial learning rate was set to 0.08 for the MMI dataset and 0.01 for the CASME II dataset. The training epochs were set to 40 and 50 for the CNN and LSTM, respectively. The training of the CNN and LSTM for the given number of epoch took approximately 150 minutes and 100 minutes on a NVIDIA Titan X GPU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effectiveness of the Proposed Spatio-Temporal Feature Representation</head><p>We demonstrated the effectiveness of the proposed spatio-temporal feature representation for the FER task under a subject-independent recognition scenario. For the MMI dataset, the comparison was conducted with stateof-the-art methods including hand-crafted features and deep learning. As hand-crafted feature based methods, sparse representation classifier (SRC) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref> with static (frame-based) features (i.e., LBP <ref type="bibr" target="#b7">[8]</ref>, LPQ <ref type="bibr" target="#b9">[10]</ref>, Gabor wavelet <ref type="bibr" target="#b8">[9]</ref>, and collaborative expression representation (CER) <ref type="bibr" target="#b2">[3]</ref> extracted from the apex frames) and spatiotemporal features (i.e., LBP-TOP <ref type="bibr" target="#b11">[12]</ref> and LPQ-TOP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>) were used. In addition, deep learning based methods with spatial feature representation (i.e., AURF <ref type="bibr" target="#b29">[30]</ref>, AUDN <ref type="bibr" target="#b30">[31]</ref>, and deep CNN <ref type="bibr" target="#b25">[26]</ref>) and spatio-temporal feature representation (i.e., 3D CNN-DAP <ref type="bibr" target="#b18">[19]</ref> and DTAGN <ref type="bibr" target="#b31">[32]</ref>) were used in the comparison.   The comparative recognition rates and the confusion matrix of the proposed method are shown in Table <ref type="table" target="#tab_11">3</ref>. As shown in the table, the proposed method outperformed existing state-of-the-art FER methods. Specifically, the proposed spatio-temporal feature representation showed better recognition rates compared to the deep learning based methods with spatial feature representation (AURF <ref type="bibr" target="#b29">[30]</ref>, AUDN <ref type="bibr" target="#b30">[31]</ref>, deep CNN <ref type="bibr" target="#b25">[26]</ref>) and the proposed spatial feature representation. This can be attributed to the efficient encoding of the expression dynamics in the proposed spatio-temporal feature representation learning. Moreover, by utilizing the expression-state in the proposed objective function, the proposed method achieved a superior performance to methods that encoded the spatio-temporal dynamics with a 3D CNN (3D CNN-DAP <ref type="bibr" target="#b18">[19]</ref> and DTAGN <ref type="bibr" target="#b31">[32]</ref>).</p><p>In the spontaneous micro-expression experiments, the FER performance of the proposed method was compared with four spatio-temporal hand-crafted feature based methods devised for the micro-expression recognition; 1) Support vector machine (SVM) classifier with LBP-TOP <ref type="bibr" target="#b11">[12]</ref>; 2) SVM with LBP-TOP extracted from motion magnified sequences <ref type="bibr" target="#b52">[52]</ref>; 3) SVM with LBP-Three Mean Orthogonal Planes (LBP-MOP) <ref type="bibr" target="#b14">[15]</ref>; and 4) SVM with Monogenic Riesz wavelet representation <ref type="bibr" target="#b57">[57]</ref>.</p><p>The comparative recognition rates and the confusion matrix of the proposed method on the CASMI II dataset are shown in Table <ref type="table" target="#tab_12">4</ref>. As shown in the table, the proposed spatio-temporal feature representation outperformed existing state-of-the-art micro-expression recognition methods and the proposed spatial feature representation. The results indicate that the proposed method is also effective for recognizing micro-expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-Dataset Evaluation of the Proposed Spatio-temporal Feature Representation</head><p>To investigate the generalization performance of the proposed method, an inter-dataset evaluation was conducted.</p><p>In deep learning, it is known that lower layers normally extract general low-level features such as edges from the input images while higher layers extract high-level features which are more specific to different tasks <ref type="bibr" target="#b58">[58]</ref>. Thus, it can be expected that the network that learned subtle facial dynamic patterns on micro-expression could operate well on recognizing deliberate expression that includes apparent facial dynamics. For this experiment, the network was trained on the micro-expression dataset (CASME II) and tested on the deliberate expression dataset (MMI). In training the network, due to the different expression classes in both datasets, fine tuning was performed in order to represent all the expressions in the test dataset (MMI). To that end, weights of the first two convolutional layers (C1 and C2) were initialized with pretrained weights trained from CASME II dataset. Remaining higher layers were initialized by the Xavier's initialization <ref type="bibr" target="#b59">[59]</ref>. During the training of the network, weights of the first two convolutional layers (C1 and C2) were fixed, while higher layers (from C3 to F9) were trained. In this experiment, leave-one-subject-out cross validation scheme was also used, as described in section 3.1.</p><p>Table <ref type="table" target="#tab_13">5</ref> shows the recognition rates of the proposed method under intra and inter-dataset evaluation. As shown in the table, the proposed method was able to maintain a feasible recognition rate under inter-dataset evaluation. Furthermore, the proposed method under inter-dataset evaluation showed comparable performance to state-of-the-art methods under intra-dataset evaluation shown in Table <ref type="table" target="#tab_12">4</ref>. This result indicates that the proposed method could learn versatile spatio-temporal feature representation of micro-expression and deliberate expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effectiveness of the Proposed Spatial Feature Representation Learning with Expression-State Constraints</head><p>In this section, the effects of the proposed objective terms were evaluated in terms of the expression recognition rate in the CNN (the part 1 in Fig. <ref type="figure" target="#fig_0">1</ref>). For the evaluation, the proposed objective terms were incrementally added to the objective function for improving the class separability of the feature space in a coarse to fine approach. First, expression class related objective terms (i.e., E1 and E2) were used, because the primary goal is the expression classification. Then, expression-state related objective terms (i.e., E3 and E4) were followed. If expression-state related objective terms are used first, the training may suffer from local minima, because expression-state class could be fine-grained class of expression class. In addition, minimizing intra-class variation related terms (i.e., E2 and E4) should be followed by minimizing classification error related terms (i.e., E1 and E3), because minimizing intraclass variation does not guarantee minimization of classification error. Lastly, preserving the expression-state continuity term (i.e., E5) should be followed by expressionstate related terms (i.e., E3 and E4). For applying the expression-state continuity term, expression-states need to be differentiable in feature space. In case of the spatial feature representation which is a static (frame-based) feature extracted from a frame, apex frame has the most information for the expression classification. Therefore, the performance was measured using apex frames of each sequence. The recognition rates were calculated with an output of layer Fc which predicts the probability of each expression class. The t-test was employed to measure the statistical significance of the recognition rate improvement of each objective term. For the t-test, the leave-onesubject-out cross validation scheme described in section 3.1 was repeated 20 times with the different random initialization of the network weights, resulting in different recognition rates. Table <ref type="table" target="#tab_14">6</ref> shows recognition rates according to different objective terms and the significance of the improvement. In the MMI dataset, the proposed spatial feature representation learning with expression-state constraints improved recognition rate of 19.07% compared to the conventional CNN (using E1 only). In the CASME II dataset, the recognition rate of the micro-expressions was improved about 17.89 % compared to the conventional CNN. Specifically, utilizing the objective term E2, which focuses on reducing the intra-class variations of each expression, showed high improvement in the MMI dataset. This is mainly attributed to the characteristics of the expression of MMI dataset, in which there exists a large intra-class variation between different subjects. Furthermore, in recognizing micro-expressions, the results show that the ex-pression-state related objective terms (E3 and E5) have a high impact on the recognition rate. The t-test results showed that the recognition rate improvements of each objective term were significant in both dataset, except the objective term E4 in CASME II dataset, which focuses on minimizing expression-state variation. It could be thought that the loss induced by the objective term E4 in CASME II dataset was small, due to the small variation of frames in the same expression-state class. Note that the recognition rate improvement is significant if the p-value is less than 0.05. These results indicate that the proposed objective terms and the frame extraction (selection) of representative expression-state improve classification performance by learning of discriminative spatial feature representation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Visualizing the Spatio-Temporal Feature Representation Learned with the Proposed Method</head><p>To visualize the effectiveness of the proposed spatiotemporal feature representation, t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b60">[60]</ref> was utilized. For the comparison, spatial feature representation of the conventional CNN was learned with the apex frames which have strong expression (the conventional CNN is learned by using the objective term E1). The visualization was performed on both the MMI dataset and the CASME II dataset. In this experiment, for the visualization, subjects of each dataset were randomly divided into two sets; 50% of the training set and 50% of the test set. Fig. <ref type="figure" target="#fig_25">7</ref> shows the t-SNE visualization of both the spatial feature representation obtained with a conventional CNN and the proposed spatio-temporal feature representation on both the MMI dataset and the CASME II dataset. As shown in the figure, the proposed spatio-temporal feature representation led to more separated classes compared to the spatial feature representation obtained with conventional CNN. These results show that the proposed spatiotemporal representation learning with expression-state constraints produces discriminative feature representation that can be utilized for accurate FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Convergence of the Learning</head><p>In this section, the convergence of the proposed learning method was analyzed. Fig. <ref type="figure" target="#fig_27">8</ref> shows the training loss values along the epochs for the CNN and the LSTM used in the proposed method. As can be seen in the figure, the training in MMI dataset converged faster compared to the training in CASME II dataset. This implies facial dynamics in a deliberate expression is easier to learn compared to a spontaneous micro-expression. The result showed that the CNN and the LSTM used in the proposed method converged well on both the MMI dataset and the CASME II dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, spatio-temporal feature representation learning for FER that is robust to the expression intensity variations was proposed. For the robust FER, the proposed method utilized representative expression-states in the network trianing which could be specified in facial sequences regardless of the expression intensity variations or different expression durations. In the proposed method, characteristics of facial expressions were encoded in two parts. In the first part, spatial image characteristics of the representative expression-state frames were learned by the CNN. In the first part, four objective terms were proposed to deal with variations in expression classes and expression-states. Moreover, a new objective term was proposed to preserve the expression-state continuity of spatial feature representation for further sequence learning. In the second part, temporal characteristics of spatial feature representation were consecutively learned with an LSTM. By adopting an LSTM recurrent neural network, the proposed method could learn time scale dependent information in facial expression sequences which have different number of frames.</p><p>Extensive and comprehensive experiments have been conducted on the deliberate expression dataset (MMI) and the micro-expression dataset (CASME II). Experiments showed that the proposed method achieved a higher level of FER rates in both datasets compared to the performances with hand-crafted feature based methods as well as existing deep learning based methods.</p><p>For the practical FER, the proposed method can be applied to real-world environments in which the expression-state information is not available. As showed in the inter-dataset evaluation in Section 3.3, the proposed method could learn versatile spatio-temporal feature representation of micro-expression and deliberate expression in different datasets. In real-world setting, existing datasets (e.g., MMI and CASME II) which have annotations of expression-states can be used for pre-training the proposed framework. To further improve the recognition performance, LSTM layers in the proposed network could be fine-tuned using real-world datasets which do not have annotations of expression-states. Note that the temporal feature representation learning does not use the expression-state annotations. Thus, the proposed method could be used in real-world environments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed spatio-temporal feature representation learning for facial expression recognition robust to expression intensity variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of the extracted representative expression-state frames. (a) Happy and (b) disgust expressions from MMI dataset. (c) Happy and (d) disgust expressions from CASME II dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the true class of the sample is k-th class, and ) ( ˆc F i y denotes the predicted probability calculated at the layer Fc with Softmax function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weights of the layer Fc, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>of the last hidden layer (F5 in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the objective terms of the proposed objective function for the proposed spatial feature representation learning with expression-state constraints. Color represents the expression class of samples and shape represents expression-state of samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the last hidden layer (F5 in Table1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>sion-state ground truth of the i-th sample (1 if j is the correct expression-state and 0 otherwise), and expression-state of that sample calculated at the layer Fp with Softmax function which generates predicted probability of each expression-state. The term E3 makes samples with different expression-states in an expression class be separable in the feature space as shown in Fig.4 (c).Similar to (3), the gradients of the third objective term (E3) are given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>if the true expression-state of the sample is j-th expression-state. ed probability calculated at the layer Fp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>and weights of the layer Fp, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>where m c,p is the mean feature vector of samples in the cth class and the p-th expression-state and p c d , min is half of the minimum distance between m c,p and m c,j for p j .  is a parameter for determining the range of the expression-state distribution. As shown in Fig.4 (d), the minimization of (8) makes f c,p,i be in a multi-dimensional sphere which radius is p c d , min and the center point is m c,p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of class separability score of the spatial feature representation learned with different objective terms in MMI dataset and CASME II dataset. Note that higher value means more separable classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>extracted from the learned CNN model are fed to the LSTM. As a result, the last LSTM layer (L8) produces the proposed spatio-temporal feature representation. Then, the classification result is drawn at F9 with a Softmax classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Illustration of the LSTM (defined in (13)) used for temporal feature representation learning. (b) and (c) Corresponding visualization of gates, cell state and output of last LSTM layer (i.e., l=L8). The horizontal axis t corresponds to the number of processed frames. The vertical axis is the index of the video sequence samples in the MMI dataset. The number of frames in x-axis and the number of samples in y-axis of the figure are 20 and 197, respectively. Here we visualize the gates and cell state of two LSTM dimensions that successfully learn patterns of (b) sad expression and (c) surprise expression in temporal domain, respectively. As shown in (a), input gate ( ) ( ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>layer ( (l-1)-th layer) to the cell state ( includes expression class related information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>and biases of the l-th LSTM layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A comparative t-SNE visualization [60] of the spatial feature representation extracted by a conventional CNN (a, c) and the proposed spatio-temporal feature representation (b, d). (a) and (b) are the t-SNE visualizations obtained from the deliberate dataset (MMI), (c) and (d) are the t-SNE visualizations obtained from the spontaneous micro-expression dataset (CASME II). Note that, the small dots denote augmented training data and large circles which thick border lines denote testing data. Best viewed in color.</figDesc><graphic coords="12,122.63,378.41,195.38,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>This work was partially supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2015R1A2A2A01005724). Yong Man Ro is the corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Training loss curves of the proposed method in MMI dataset and CASME II dataset. (a) CNN in the proposed spatial feature representation learning, (b) LSTM in the proposed temporal feature representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Temporal feature representation learning on LSTM</head><label></label><figDesc>Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Onset</cell><cell>Apex (Peak frame)</cell><cell>Offset</cell><cell></cell><cell></cell><cell>Onset</cell><cell>Apex (Peak frame)</cell><cell>Offset</cell></row><row><cell>Normalized cross correlation</cell><cell>to apex (peak frame)</cell><cell>0.750 0.800 0.850 0.900 0.950 1.000</cell><cell cols="3">] 1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 , , [ off apex on t t t</cell><cell>Normalized cross correlation</cell><cell>to apex (apex frame)</cell><cell>0.750 0.800 0.850 0.900 0.950 1.000</cell><cell>1 1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 I</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frame Index</cell><cell></cell><cell></cell><cell></cell><cell>Frame Index</cell></row><row><cell cols="9">Output: Trained CNN model (C1~F5) apex t Set of selected frames ' I (c) (e) (g) Fig. 2. Examples of the change of expression state, which is represented as normalized cross correlations between apex (peak frame) Expression intensity Frame on t off t ' I L6 L7 L8 (b) (d) (f) (h) and onset, apex and offset frames. (a) Happy, (b) disgust, (c) angry and (d) surprise expressions from MMI dataset. (e) Happy, (f) disgust, (g) repression and (h) surprise expressions from CASME II dataset. 0.750 0.800 0.850 0.900 0.950 1.000 1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 Normalized cross correlation to apex (peak frame) Frame Index Onset Offset Apex (Peak frame) 0.750 0.800 0.850 0.900 0.950 1.000 1 6 11 16 21 26 31 36 41 46 51 Normalized cross correlation to apex (peak frame) Frame Index Onset Offset Apex (Peak frame) 0.980 0.985 0.990 0.995 1.000 1949-3045 (c) 2016 IEEE. (a) 1 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 Normalized cross correlation to apex (peak frame)</head><label></label><figDesc>Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Onset</cell><cell></cell><cell></cell><cell cols="3">Apex (Peak frame)</cell><cell></cell><cell></cell><cell></cell><cell>Offset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Onset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Peak frame) Apex</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Offset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Normalized cross correlation</cell><cell>to apex (peak frame)</cell><cell></cell><cell>0.980 0.985 0.990 0.995 1.000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>6</cell><cell>11</cell><cell>16</cell><cell>21</cell><cell>26</cell><cell>31</cell><cell>36</cell><cell>41</cell><cell>46</cell><cell>51</cell><cell>56</cell><cell>61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frame Index</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Frame Index</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Apex</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Apex</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Onset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Offset</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Onset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Offset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(Peak frame)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(Peak frame)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized cross correlation</cell><cell>to apex (peak frame)</cell><cell>0.980 0.985 0.990 0.995 1.000</cell><cell>1</cell><cell>11</cell><cell>21</cell><cell>31</cell><cell>41</cell><cell>51</cell><cell>61</cell><cell>71</cell><cell>81</cell><cell>91</cell><cell>101 111</cell><cell>Normalized cross correlation</cell><cell>to apex (peak frame)</cell><cell>0.980 0.985 0.990 0.995 1.000</cell><cell>1</cell><cell>6</cell><cell>11</cell><cell>16</cell><cell></cell><cell>21</cell><cell>26</cell><cell>31</cell><cell>36</cell><cell></cell><cell>41</cell><cell>46</cell><cell>51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frame Index</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Frame Index</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1949-3045 (c) 2016 IEEE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 THE</head><label>1</label><figDesc></figDesc><table /><note><p>ARCHITECTURE OF THE CNN FOR SPATIAL FEATURE REPRESENTATION LEARNING WITH EXPRESSION-STATE CON-STRAINTS Layer *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 THE</head><label>2</label><figDesc>ARCHITECTURE OF THE NETWORK FOR LEARNING THE SPA-TIO-TEMPORAL FEATURE REPRESENTATION OF THE EXPRESSION DYNAMICS</figDesc><table><row><cell>Type</cell><cell>Layer *</cell><cell>Input shape</cell><cell>Output shape</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>[</cell><cell>,</cell><cell>,</cell><cell>...,</cell><cell>]</cell></row></table><note><p><p><p><p><p><p><p>summarizes the overall architecture of the proposed network. Motivated by the recent deep learning approaches</p><ref type="bibr" target="#b48">[48]</ref></p>,</p><ref type="bibr" target="#b49">[49]</ref></p>, deep LSTM network is constructed by stacking multiple LSTM layers on top of each other. This stacked LSTM network can combine multiple representations with flexible use of face sequences</p><ref type="bibr" target="#b48">[48]</ref></p>. In the network, the spatial feature representations of all the frames</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</figDesc><table><row><cell>Sample</cell><cell></cell><cell></cell><cell cols="3">Sample</cell><cell></cell><cell cols="3">Sample</cell><cell></cell><cell cols="3">Sample</cell><cell></cell><cell cols="3">Sample</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>Angry</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell></row><row><cell>Disgust</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Happy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sad</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Surprise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t</cell><cell></cell><cell></cell><cell>t</cell><cell></cell><cell></cell><cell></cell><cell>t</cell><cell></cell><cell></cell><cell cols="2">t</cell><cell></cell><cell></cell><cell></cell><cell>t</cell></row><row><cell>Input gate</cell><cell>g</cell><cell>( i n l</cell><cell>) ,</cell><cell>t</cell><cell>Forget gate</cell><cell>g</cell><cell>( f l</cell><cell>) , t</cell><cell>Cell state</cell><cell>c</cell><cell>( l t</cell><cell>)</cell><cell>Output gate</cell><cell>g</cell><cell>( o l ,</cell><cell>) t</cell><cell>Output</cell><cell>( l t</cell><cell>)</cell></row></table><note><p>h 1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</figDesc><table><row><cell>10</cell><cell>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, TAFFC-2016-06-0098.R2</cell></row></table><note><p>1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">RECOGNITION PERFORMANCE WITH MMI DATASET</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell></cell><cell>Method</cell><cell></cell><cell>Cross validation</cell><cell>Input</cell><cell cols="2">Recognition rate (%)</cell></row><row><cell></cell><cell>LBP + SRC [8]</cell><cell></cell><cell></cell><cell>LOSO</cell><cell>Apex frame</cell><cell></cell><cell>59.18</cell></row><row><cell></cell><cell>LPQ + SRC [10]</cell><cell></cell><cell></cell><cell>LOSO</cell><cell>Apex frame</cell><cell></cell><cell>62.72</cell></row><row><cell>Hand-crafted feature based methods</cell><cell cols="2">Gabor + SRC [9] LBP-TOP [12]+SRC</cell><cell></cell><cell>LOSO LOSO</cell><cell>Apex frame Sequence</cell><cell></cell><cell>61.89 61.19</cell></row><row><cell></cell><cell cols="2">LPQ-TOP [11], [13] + SRC</cell><cell></cell><cell>LOSO</cell><cell>Sequence</cell><cell></cell><cell>64.11</cell></row><row><cell></cell><cell>CER [3]</cell><cell></cell><cell></cell><cell>LOSO</cell><cell>Apex frame</cell><cell></cell><cell>70.12</cell></row><row><cell></cell><cell>3D CNN-DAP [19]</cell><cell></cell><cell></cell><cell>LOSO</cell><cell>Sequence</cell><cell></cell><cell>63.40</cell></row><row><cell></cell><cell>DTAGN [32]</cell><cell></cell><cell></cell><cell>10-fold</cell><cell>Sequence</cell><cell></cell><cell>70.24</cell></row><row><cell></cell><cell>AURF [30]</cell><cell></cell><cell></cell><cell>10-fold</cell><cell>Apex frame</cell><cell></cell><cell>69.88</cell></row><row><cell>Deep learning based methods</cell><cell>AUDN [31]</cell><cell></cell><cell></cell><cell>10-fold</cell><cell>Apex frame</cell><cell></cell><cell>75.85</cell></row><row><cell></cell><cell>Deep CNN [26]</cell><cell></cell><cell></cell><cell>5-fold</cell><cell>Static frame</cell><cell></cell><cell>77.90</cell></row><row><cell></cell><cell cols="2">Proposed spatial feature representation</cell><cell></cell><cell>LOSO</cell><cell>Apex frame</cell><cell></cell><cell>69.94</cell></row><row><cell></cell><cell cols="3">Proposed spatio-temporal feature representation</cell><cell>LOSO</cell><cell>Sequence</cell><cell></cell><cell>78.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Actual</cell><cell>Predicted</cell><cell>Angry</cell><cell>Disgust</cell><cell>Fear</cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell></row><row><cell>Angry</cell><cell></cell><cell>77.78</cell><cell>14.81</cell><cell>0.00</cell><cell>3.70</cell><cell>3.70</cell><cell>0.00</cell></row><row><cell>Disgust</cell><cell></cell><cell>4.17</cell><cell>83.33</cell><cell>4.17</cell><cell>4.17</cell><cell>4.17</cell><cell>0.00</cell></row><row><cell>Fear</cell><cell></cell><cell>0.00</cell><cell>0.00</cell><cell>65.22</cell><cell>13.04</cell><cell>8.70</cell><cell>13.04</cell></row><row><cell>Happy</cell><cell></cell><cell>0.00</cell><cell>3.03</cell><cell>0.00</cell><cell>93.94</cell><cell>0.00</cell><cell>3.03</cell></row><row><cell>Sad</cell><cell></cell><cell>10.71</cell><cell>0.00</cell><cell>10.71</cell><cell>3.57</cell><cell>71.43</cell><cell>3.57</cell></row><row><cell>Surprise</cell><cell></cell><cell>5.26</cell><cell>10.53</cell><cell>2.63</cell><cell>2.63</cell><cell>2.63</cell><cell>76.32</cell></row><row><cell cols="5">(a) Comparisons with state-of-the-art FER methods. (b) Confusion matrix of the proposed method.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 4</head><label>4</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</figDesc><table><row><cell></cell><cell></cell><cell cols="4">RECOGNITION PERFORMANCE WITH CASME II DATASET</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="2">Recognition rates (%)</cell></row><row><cell cols="2">LBP-TOP [12]</cell><cell></cell><cell></cell><cell></cell><cell>44.12</cell><cell></cell></row><row><cell cols="3">LBP-TOP with adaptive motion magnification [52]</cell><cell></cell><cell></cell><cell>51.91</cell><cell></cell></row><row><cell cols="2">LBP-MOP [15]</cell><cell></cell><cell></cell><cell></cell><cell>45.75</cell><cell></cell></row><row><cell cols="2">Riesz wavelet [57]</cell><cell></cell><cell></cell><cell></cell><cell>46.15</cell><cell></cell></row><row><cell cols="3">Proposed spatial feature representation</cell><cell></cell><cell></cell><cell>58.54</cell><cell></cell></row><row><cell cols="3">Proposed spatio-temporal feature representation</cell><cell></cell><cell></cell><cell>60.98</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Actual</cell><cell>Predicted</cell><cell>Happy</cell><cell>Disgust</cell><cell>Surprise</cell><cell>Repression</cell><cell>Others</cell></row><row><cell></cell><cell>Happy</cell><cell>37.50</cell><cell>12.50</cell><cell>3.13</cell><cell>0.00</cell><cell>46.88</cell></row><row><cell></cell><cell>Disgust</cell><cell>1.59</cell><cell>50.79</cell><cell>3.17</cell><cell>0.00</cell><cell>44.44</cell></row><row><cell></cell><cell>Surprise</cell><cell>0.00</cell><cell>8.00</cell><cell>60.00</cell><cell>0.00</cell><cell>32.00</cell></row><row><cell cols="2">Repression</cell><cell>14.81</cell><cell>7.41</cell><cell>3.70</cell><cell>22.22</cell><cell>51.85</cell></row><row><cell></cell><cell>Others</cell><cell>0.00</cell><cell>12.12</cell><cell>0.00</cell><cell>2.02</cell><cell>85.86</cell></row></table><note><p><p>(a) Comparisons with state-of-the-art FER methods. (b) Confusion matrix of the proposed method.</p>1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 5</head><label>5</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2017.2695999, IEEE Transactions on Affective Computing</figDesc><table><row><cell cols="2">RECOGNITION RATES OF THE PROPOSED METHOD UNDER</cell></row><row><cell cols="2">INTRA AND INTER-DATASET EVALUATION</cell></row><row><cell>Evaluation method (Training set / test set)</cell><cell>Recognition rates (%)</cell></row><row><cell>Intra-dataset (MMI / MMI)</cell><cell>78.61</cell></row><row><cell>Inter-dataset (CASME II / MMI)</cell><cell>72.83</cell></row></table><note><p>1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 6 COMPARISON</head><label>6</label><figDesc>OF RECOGNITION RATE USING SPATIAL FEATURE REPRESENTATION WITH THE CNNS (THE PART 1 IN FIG. 1) LEARNED WITH DIFFERENT OBJECTIVE TERMS IN MMI DATASET AND CASME II DATASET E4: minimizing expression-state variation, E5: preserving expression-state continuity. The p-value indicates the significance of the improvement of recognition rate induced by the added objective term.</figDesc><table><row><cell>Objective terms</cell><cell>MMI</cell><cell></cell><cell>CASME II</cell><cell></cell></row><row><cell></cell><cell>Recognition rates (%)</cell><cell>p-value</cell><cell>Recognition rates (%)</cell><cell>p-value</cell></row><row><cell>E1</cell><cell>50.87</cell><cell>-</cell><cell>40.65</cell><cell>-</cell></row><row><cell>E1+E2</cell><cell>65.90</cell><cell>&lt;0.0001</cell><cell>48.37</cell><cell>0.0005</cell></row><row><cell>E1+E2+E3</cell><cell>67.63</cell><cell>0.0032</cell><cell>54.07</cell><cell>0.0006</cell></row><row><cell>E1+E2+E3+E4</cell><cell>68.79</cell><cell>0.0071</cell><cell>54.88</cell><cell>0.4909</cell></row><row><cell>All (E1+E2+E3+E4+E5)</cell><cell>69.94</cell><cell>0.0129</cell><cell>58.54</cell><cell>0.0100</cell></row></table><note><p>Note: E1: minimizing expression classification error, E2: minimizing intra-class variation, E3: minimizing expression-state classification error,</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The computer expression recognition toolbox (CERT)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. and Workshops on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A real-time automated system for the recognition of human facial expressions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="96" to="105" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collaborative expression representation using peak expression and intra class variation face images for practical subject-independent emotion recognition in videos</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Baddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully automatic facial action unit detection and temporal analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="149" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local directional number pattern for face analysis: Face and expression recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Ramirez</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Rojas</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1740" to="1752" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition using extended ar-lbp</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wireless Networks and Computational Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new method for facial expression recognition based on sparse representation plus LBP</title>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust facial expression recognition via compressive sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3747" to="3761" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local phase quantization and sparse representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zilu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int&apos;l Conf. Natural Computation</title>
		<imprint>
			<biblScope unit="page" from="222" to="225" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. and Workshops on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dynamic appearance descriptor approach to facial actions temporal modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="161" to="174" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Partial Matching of Facial Expression Sequence Using Over-complete Transition Dictionary for Emotion Recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Spatio-Temporal Local Binary Patterns for Spontaneous Facial Micro-Expression Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>-W. Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>p. e0124674</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">(Multiscale) Local Phase Quantisation histogram discriminant analysis with score normalisation for robust face recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gabor wavelets and general discriminant analysis for face identification and verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fairhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="553" to="563" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse representations for facial expressions recognition via l1 optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifying facial actions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="974" to="989" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial expression recognition in image sequences using geometric deformation features and support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="172" to="187" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial expression recognition using geometric and appearance features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Internet Multimedia Computing and Service</title>
		<meeting>IEEE Int&apos;l Conf. Internet Multimedia Computing and Service</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Intra-Class Variation Reduction Using Training Expression Images for Sparse Representation Based Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="340" to="351" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of face recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="247" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM on Int&apos;l Conf. on Multimodal Interaction</title>
		<imprint>
			<biblScope unit="page" from="467" to="474" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short term memory recurrent neural network based multimodal dimensional emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>Int&apos;l Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How Deep Neural Networks Can Improve Emotion Recognition on Video Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07377</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AU-inspired Deep Networks for Facial Expression Feature Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="246" to="253" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new 2.5 D representation for lymph node detection using random sets of deep convolutional neural network observations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CASME II: An improved spontaneous micro-expression database and the baseline evaluation</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="317" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Personal use is permitted, but republication/redistribution requires IEEE permission</title>
		<ptr target="http://www.ieee.org/publications_standards/publications/rights/index.html" />
	</analytic>
	<monogr>
		<title level="m">for more information</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1949" to="3045" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantifying micro-expressions with constraint local model and local binary pattern</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at the European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast normalized cross-correlation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision interface</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="120" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast normalized cross correlation for defect detection</title>
		<author>
			<persName><forename type="first">D.-M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2625" to="2631" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural networks for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford university press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Notes on convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cross-entropy vs. squared error training: a theoretical and experimental comparison</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="1756" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face recognition by independent component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action Recognition using Visual Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS workshop on Time Series</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluation of face resolution for expression analysis</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="82" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Subtle Facial Expression Recognition Using Adaptive Magnification of Discriminative Facial Motion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Annual ACM Conf. on Multimedia</title>
		<meeting>23rd Annual ACM Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="911" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Demyanov</surname></persName>
		</author>
		<ptr target="https://github.com/sdemyanov/ConvNet" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<title level="m">Keras. Available</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning</title>
		<meeting>Int&apos;l Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monogenic Riesz wavelet representation for microexpression recognition</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Le Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>-W. Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int&apos;l Conf. on Digital Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1237" to="1241" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="3320" to="3328" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
